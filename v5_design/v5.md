# Research Assistant v5.0: Three-System Integration

## Table of Contents

1. [Overview](#overview)
2. [Breaking Changes](#breaking-changes)
3. [Installation & Setup](#installation--setup)
4. [Core Features](#core-features)
5. [System Architecture](#system-architecture)
6. [Usage Guide](#usage-guide)
7. [Implementation Plan](#implementation-plan)
8. [Technical Details](#technical-details)
9. [Success Metrics](#success-metrics)

---

## Overview

### What is v5.0?

Research Assistant v5.0 is an **optimization and reliability enhancement** that transforms the extraction pipeline from "works but slow" to "works fast and reliably".

It integrates three powerful systems:

1. **Zotero** - Paper management and PDF storage
2. **Grobid** - Local AI-powered extraction (99.95% success for research papers)
3. **Semantic Scholar** - Citation metrics and AI-generated summaries

### Why v5.0?

The current v4.6 has **critical reliability issues**:

- ❌ **Unreliable processing** - Single paper can take 3+ hours (laptop sleep, no timeouts)
- ❌ **No post-processing** - Missing 3-5% extraction accuracy from simple fixes
- ❌ **No resilience** - Can't resume from interruptions, no checkpoints
- ❌ **No citation networks** - Can't find related papers
- ❌ **Limited discovery** - Only keyword matching

**NOTE: After analyzing 1,000 papers with OPTIMIZED Grobid parameters, we achieve 91.4% abstract extraction and significantly improved section detection. With proper post-processing and paper classification, we can reach 92.4% abstract extraction and filter out 9.3% of problematic PDFs that shouldn't be in the knowledge base.**

### What v5.0 Delivers

- ✅ **Reliable extraction** - 9.5 hours for 2,200 papers with two-pass strategy (15s average/paper)
- ✅ **Case-insensitive section matching** - Fixes 1,531 missed sections (empirically validated)
- ✅ **Intel Extension for PyTorch** - 2-3x speedup for embeddings on Intel CPUs (auto-detected)
- ✅ **Resilient pipeline** - Two-pass extraction (90s + 180s), checkpoints every 50 papers
- ✅ **Smart post-processing** - Content aggregation from multiple subsections
- ✅ **Intelligent retry logic** - Retries papers with <4 sections or missing abstracts
- ✅ **50+ entity types extracted** (sample sizes, p-values, methods, datasets)
- ✅ **Multi-level embeddings** for intelligent search
- ✅ **Entity-aware discovery** (find papers using same methods/datasets)
- ✅ **Comprehensive gap analysis** (methodology, reproducibility, evidence levels)
- ✅ **Unattended operation** for builds without interruption
- ✅ **Collection-based processing** for focused knowledge bases

---

## Breaking Changes

### ⚠️ IMPORTANT: Complete Rebuild Required

```bash
# One-time weekend investment for reliable extraction forever
rm -rf kb_data/              # Delete existing KB
git pull                      # Update to v5
pip install -r requirements.txt
python src/build.py --rebuild  # ~9.5 hours for 2,200 papers (no prompts needed)

# Why rebuild? Because you're getting:
# • Predictable processing (9.5 hours for 2,200 papers)
# • Better section detection (75-82% with post-processing)
# • Resilient pipeline that won't hang on outliers
# • Future-proof XML storage for new entity mining
```

### Why No Migration?

- Different extraction format (Grobid XML vs plain text)
- New entity storage structure
- Enhanced embedding system
- Incompatible quality scores

---

## Installation & Setup

### Prerequisites

```bash
# 1. Install Docker
sudo apt install docker.io  # Ubuntu/Debian
brew install docker         # macOS

# 2. Install and start Grobid locally (17.3GB image)
docker pull lfoppiano/grobid:0.8.2-full
sudo docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.8.2-full

# 3. Install Python dependencies
pip install -r requirements.txt
# Optional: Intel CPU optimization
pip install intel-extension-for-pytorch

# 4. Verify setup
python src/check_requirements.py  # Exit 0 = ready
```

### System Requirements

```yaml
Minimum Requirements:
  RAM: 4 GB (for <1000 papers)
  Disk: 10 GB free (for XML cache)
  CPU: 2 cores
  Network: 10 Mbps (for S2 API)

Recommended:
  RAM: 8 GB (for 2000+ papers)
  Disk: 20 GB free (includes all caches)
  CPU: 4+ cores
  Network: 50+ Mbps

Grobid Docker:
  RAM: 2 GB allocated to container
  Disk: 5 GB for Docker image

Data Usage:
  ~500 KB per paper (S2 API calls)
  ~100 KB per paper (XML cache)
  ~50 KB per paper (embeddings)

API Limits:
  S2 Unauthenticated: 1 request/second
  S2 Batch: 100 papers per request
  Grobid: No hard limits (local Docker)
```

### First Build - The Academic Workflow

```bash
# Friday afternoon workflow:
python src/build.py --estimate
# Output: "Estimated time: 4.5 hours (3.2 - 5.9 hours)"
#         "✓ Safe for long unattended run"

# Start before leaving:
python src/build.py --rebuild

# Complete in ~1 hour for 1000 papers

# What happens:
# 1. Checks all systems are running
# 2. Shows time estimate and starts
# 3. Fetches papers from Zotero
# 4. Extracts structure with Grobid (parallel, with timeouts)
# 5. Applies post-processing (header normalization)
# 6. Logs progress to kb_data/build_progress.log
# 7. Enhances with Semantic Scholar data
# 8. Creates multi-level embeddings
# 9. Builds FAISS indices

# Check progress in the morning
cat kb_data/build_progress.txt
# Build Progress: 1847/2000 (92.4%)
# Current: paper_1847.pdf
# Elapsed: 4.2 hours
# ETA: 0.3 hours
# Rate: 440 papers/hour
```

---

## Core Features

### 1. Entity Extraction (50+ Types)

#### From Grobid

- **Methodology**: Sample sizes, p-values, confidence intervals, study types
- **Software & Data**: Software versions, datasets, data availability
- **Clinical**: Trial IDs, organisms, diseases
- **Institutional**: Funders, affiliations, countries

#### From Semantic Scholar

- **Impact**: Citation count, influential citations, citation velocity
- **AI Insights**: TLDR summaries, field classifications
- **Author Metrics**: H-index, author citation counts
- **Access**: Open access status, PDF availability

### 2. Multi-Level Embeddings

```python
# Each paper has 5 embedding types:
{
    'title_abstract': standard_embedding,   # Title + abstract only (Multi-QA MPNet, 768-dim)
    'enriched': entity_aware_embedding,     # Title + abstract + extracted entities (768-dim)
    's2_embedding': semantic_scholar_vector, # SPECTER2 from S2 API (768-dim, same input as title_abstract)
    'methods': methods_section_embedding,    # Methods section only (768-dim)
    'metadata_vector': numerical_features    # Sample size, citations, year, etc. (for filtering)
}
```

**Key insights about S2 embeddings:**

- **Same input as `title_abstract`**: Both use title + abstract text
- **Different training**: SPECTER2 trained on citation graph (papers that cite each other have similar embeddings)
- **Same dimensions**: 768-dim vectors, compatible with Multi-QA MPNet
- **API dependency**: Pre-computed by Semantic Scholar, not always available
- **Quality comparison opportunity**: Can A/B test local MPNet vs S2 SPECTER2

### 3. Entity-Aware Search

```bash
# Explicit entity filtering with flags
python src/kbq.py search "diabetes treatment" \
  --study-type RCT \
  --min-sample 500 \
  --software R

# Clear, unambiguous, scriptable
```

### 4. Smart Discovery

```bash
# External paper discovery via Semantic Scholar
python src/discover.py --keywords "machine learning, diabetes"

# Find papers with specific quality criteria
python src/discover.py --quality-threshold HIGH --year-from 2020

# v5.0 NEW: SPECTER2 similarity-based discovery
python src/discover.py --similar-to 0001,0002 --similarity-mode specter2

# v5.0 NEW: Discovery with quality explanations
python src/discover.py --keywords "AI diagnostics" --explain-quality
```

### 5. Comprehensive Analysis

```bash
# Paper details from KB
python src/kbq.py get 0001

# Generate citations
python src/kbq.py cite 0001 0002 0003

# Smart search for complex queries
python src/kbq.py smart-search "diabetes treatment" -k 30

# Gap analysis for missing literature
python src/gaps.py --min-citations 50 --year-from 2020

# v5.0 NEW: Entity gap analysis
python src/gaps.py --gap-type entity --show-missing software:Python

# v5.0 NEW: Reproducibility gap detection
python src/gaps.py --reproducibility-gaps --requires both --min-citations 100
```

---

## System Architecture

### Data Flow

```
1. ZOTERO (Source)
   ├── PDFs
   ├── Metadata
   └── Collections
        ↓
2. GROBID (Extraction)
   ├── Structured sections
   ├── References & citations
   ├── Entities & metadata
   └── Study characteristics
        ↓
3. SEMANTIC SCHOLAR (Enhancement)
   ├── Citation metrics
   ├── AI summaries (TLDR)
   ├── Author metrics
   └── Related papers
        ↓
4. KNOWLEDGE BASE
   ├── FAISS indices (search)
   ├── Entity database (filtering)
   └── Citation network (discovery)
```

### Storage Structure

```
kb_data/
├── papers/           # Markdown files with full content
├── cache/
│   ├── grobid/      # XML extractions
│   ├── s2/          # API responses
│   └── embeddings/  # Computed vectors
├── indices/
│   ├── semantic.faiss    # Main search (title_abstract + enriched + s2_embedding)
│   ├── methods.faiss     # Method similarity search
│   └── metadata.faiss    # Numerical filtering (not true embeddings)
└── metadata.json    # Complete paper data
```

---

## Usage Guide

### Daily Workflow

#### Morning: Check for New Papers

```bash
# Incremental update of entire library (fast, automatic, with reports)
python src/build.py

# Update only specific collection
python src/build.py --collection "Current Research"

# Skip gap analysis for faster completion
python src/build.py --no-gaps

# Update collection without gap analysis (fastest)
python src/build.py --collection "Weekly Papers" --no-gaps

# Quality scores are automatically upgraded when needed
```

#### Research Tasks

```bash
# Search with explicit entity filters
python src/kbq.py search "diabetes" \
  --min-sample 500 \
  --study-type RCT \
  --software Python \
  --year-from 2020 \
  --has-data

# Discover external papers
python src/discover.py --keywords "diabetes, insulin resistance" \
  --quality-threshold HIGH

# Analyze gaps in literature
python src/gaps.py --min-citations 50 --year-from 2020
```

#### Manual Build Processing

```bash
# Run manually when needed (e.g., weekly)
python /path/to/src/build.py --rebuild
```

### Command Reference

#### build.py - Knowledge Base Building

```bash
# Core operation flags (simplified)
--rebuild          # Full rebuild instead of incremental
--demo             # Test with 5 papers only
--export FILE      # Export KB and exit
--import FILE      # Import KB and exit
--no-gaps          # Skip gap analysis after build (saves 15-25 min)
--collection NAME  # Process only papers from specified Zotero collection

# Information flags (exit immediately)
--estimate         # Show time estimate and exit
--progress         # Show current build status and exit

# Automatic post-build reports (always generated):
# 1. PDF quality report → exports/analysis_pdf_quality.md (if issues found)
# 2. Gap analysis → exports/gap_analysis_*.md (unless --no-gaps)
# 3. Build summary with next steps

# Examples:
python src/build.py                           # Default: entire library + reports + gaps
python src/build.py --collection "PhD Thesis" # Only papers from PhD Thesis collection
python src/build.py --rebuild                 # Full rebuild + reports + gaps
python src/build.py --demo                    # Quick test (no gap analysis)
python src/build.py --no-gaps                 # Skip gap analysis only
python src/build.py --estimate                # Check timing before starting
python src/build.py --collection "ML Papers" --no-gaps  # Specific collection, no gaps
```

### Script Naming Convention Changes

**v5.0 introduces clearer, more intuitive script names:**

| Old Name (v4.6) | New Name (v5.0) | Purpose | Rationale |
|-----------------|-----------------|---------|-----------|
| `cli.py` | `kbq.py` | Knowledge Base Query | Reflects entity-aware search capabilities and database-like querying |
| `analyze_gaps.py` | `gaps.py` | Gap Analysis | Shorter, cleaner name for network-based gap discovery |
| `build_kb.py` | `build.py` | Build Knowledge Base | Simplified name, context is clear from directory |
| `discover.py` | `discover.py` | External Discovery | Kept unchanged - name is already clear and concise |

**Why these changes?**

- **Clarity**: Names directly indicate functionality (kbq = Knowledge Base Query)
- **Brevity**: Shorter names for frequent command-line usage
- **Consistency**: All scripts follow simple noun/verb pattern
- **v5.0 Signal**: New names indicate the major architectural upgrade

#### kbq.py - Knowledge Base Query Interface

```bash
# Search commands with explicit entity filters
search QUERY [--min-sample N] [--study-type TYPE] [--software NAME]
             [--year-from YYYY] [--has-data] [--max-p-value N]
             [--exclude TERMS] [--include TERMS] [--journal NAME]
             [--group-by year|journal|type] [--export-csv FILE]
             [--show-quality] [--quality-min N]
smart-search QUERY [-k NUM]
author NAME [--exact]

# Retrieval commands
get ID [--sections abstract methods results] [--add-citation]
get-batch ID1 ID2 ID3 [--sections SECTIONS]
cite ID1 ID2 ID3

# Batch operations (10-20x faster for multiple commands)
batch --preset research|review|author-scan [ARGS]
batch --file commands.txt

# Analysis commands
info
diagnose
```

#### discover.py - External Discovery

```bash
# Basic discovery flags
--keywords "term1, term2"           # Search keywords (required)
--quality-threshold HIGH|MEDIUM|LOW # Quality filter (80+/60+/40+ scores)
--year-from YYYY                    # Recency filter (default: 2020)
--min-citations N                   # Minimum citation count
--limit N                           # Maximum results (default: 50)
--study-types TYPE1,TYPE2           # Filter by methodology (rct, cohort, etc.)
--author-filter "Name1,Name2"       # Focus on specific researchers (max 5)
--population-focus TYPE             # Target populations (pediatric|elderly|women|developing_countries)
--include-kb-papers                 # Include existing KB papers (default: exclude)
--coverage-info                     # Show database coverage guidance

# v5.0 NEW: SPECTER2-based similarity (High ROI)
--similar-to ID1,ID2,ID3            # Find papers similar to these KB papers
--similarity-mode specter2|mpnet    # Use S2 embeddings or local MPNet (default: specter2)
--min-similarity 0.7                # Minimum cosine similarity threshold

# v5.0 NEW: Quality transparency (High ROI)
--explain-quality                   # Include detailed quality breakdowns in output
--min-reproducibility 60            # Filter by reproducibility score (code/data availability)
```

#### gaps.py - Gap Analysis

**Default Behavior (no flags):**

```bash
python src/gaps.py
# Runs comprehensive gap analysis with:
# - min-citations: 10 (finds moderately cited papers)
# - year-from: all years (no filter)
# - limit: unlimited (analyzes entire KB)
# - gap-type: all types (citation, author, cocited, recent, semantic)
# - Runtime: 15-25 minutes
# - Output: exports/gap_analysis_YYYYMMDD_HHMMSS.md
```

**Flag Effects:**

```bash
# Basic filtering flags (modify comprehensive analysis)
--min-citations N   # Default: 10. Higher = focus on high-impact gaps only
--year-from YYYY    # Default: all years. Set to focus on recent research
--limit N           # Default: unlimited. Set lower for faster analysis
--kb-path PATH      # Default: kb_data/. Custom KB location

# Gap type selection (changes analysis algorithm)
--gap-type TYPE     # Default: citation. Options:
                    # citation = papers cited by your collection (default)
                    # author = recent work by your KB authors
                    # entity = papers with same methods/tools/datasets
                    # reproducibility = papers with available code/data

# Entity gap analysis (requires --gap-type entity)
--show-missing SPEC # Example: software:Python,dataset:MIMIC
--entity-coverage   # Generate entity coverage statistics

# Reproducibility gaps (can combine with any gap-type)
--reproducibility-gaps        # Filter for reproducible papers only
--requires code|data|both     # What must be available
--min-reproducibility-score N # Default: 60 (0-100 scale)

# Sample size gaps (can combine with any gap-type)
--sample-size-gaps           # Filter for large studies only
--min-sample N               # Default: 1000 participants
--study-type RCT|cohort|all  # Default: all
```

**Common Usage Patterns:**

```bash
# Quick gaps check (5-10 min)
python src/gaps.py --limit 50 --min-citations 100

# Find reproducible research gaps
python src/gaps.py --reproducibility-gaps --requires both

# Find papers using same tools
python src/gaps.py --gap-type entity --show-missing software:TensorFlow

# Find large clinical trials
python src/gaps.py --sample-size-gaps --min-sample 5000 --study-type RCT

# Recent high-impact gaps only
python src/gaps.py --year-from 2023 --min-citations 50 --limit 100
```

**When Called Automatically by build.py:**

```python
# build.py calls gaps.py with default settings (no flags)
# unless --no-gaps is specified:
if not args.no_gaps:
    print("\n• Running gap analysis (15-25 minutes)...")
    subprocess.run(['python', 'src/gaps.py'])
    # Uses gaps.py defaults: comprehensive analysis of entire KB
    # Output: exports/gap_analysis_YYYYMMDD_HHMMSS.md
else:
    print("\n• Gap analysis skipped (--no-gaps flag)")
```

**Gap Analysis Behavior Summary:**

| Context | Default Behavior | Key Parameters | Runtime | Output |
|---------|-----------------|----------------|---------|--------|
| **Standalone** | Comprehensive analysis | All defaults (no limits) | 15-25 min | Complete gap report |
| **Via build.py** | Same comprehensive analysis | gaps.py defaults used | 15-25 min | Complete gap report |
| **With --no-gaps** | Skipped entirely | N/A | 0 min | None |
| **With --demo** | Skipped automatically | N/A | 0 min | None |
| **Quick check** | Use --limit 50 | Reduces to top gaps only | 5-10 min | Most important gaps |
| **Entity focus** | --gap-type entity | Requires --show-missing | 10-15 min | Method/tool gaps |
| **Reproducible** | --reproducibility-gaps | Can add --requires both | 15-20 min | Papers with code/data |

---

## Key Lessons from 1,000 Paper Analysis

### Empirical Findings

After analyzing extraction results from 1,000 papers, we discovered several critical insights:

#### Extraction Performance (Validated on 400 Papers with Optimized Parameters)

**With Optimized Grobid Parameters (consolidateHeader=1, consolidateCitations=1):**
- **Abstracts**: 91.4% successful extraction → 92.4% after post-processing
- **Paper Classification Distribution**:
  - IMRaD papers: 74.2% (294/396)
  - Non-IMRaD papers: 16.4% (65/396)
  - Rejected papers: 9.3% (37/396) - should NOT be added to KB
- **IMRaD Papers Segment Breakdown**:
  - Introduction: 95.5% detection rate
  - Methods: 48.0% detection rate
  - Results: 57.3% detection rate
  - Discussion: 55.2% detection rate
  - 97.3% of IMRaD papers have abstracts after processing

**Critical Findings**:
- Using consolidateHeader=1 is ESSENTIAL for abstract extraction
- 9.3% of "papers" are actually non-research content (editorials, TOC, etc.)
- Post-processing can recover abstracts from introduction sections

#### What Actually Works

1. **Simple keyword patterns** - Basic detection (`intro`, `method`, etc.) outperforms complex patterns
2. **Case-insensitive matching** - Would recover 511 "Results" sections (427 "Results" + 80 "RESULTS")
3. **Content aggregation** - Papers have multiple subsections needing merging
4. **Document filtering** - Some non-research papers (editorials, letters) should be filtered

#### What Doesn't Work

1. **Over-normalization** - Aggressive header cleaning breaks detection
2. **Complex heuristics** - Adding more rules decreased accuracy
3. **Trying to "fix" non-research papers** - Better to filter them out

#### Improvement Opportunities (Proven)

- **Case-insensitive fix**: Would improve Results coverage from 41% → 85-90% (+44-49%!)
- **Quality score improvement**: 83.7 → 99.1/100 (+15.4 points) with fixes
- **Smart retry**: 50.8% of papers have <4 sections and would benefit from retry
- **Pattern additions**: Adding "measures", "participants", "outcomes" would help ~300 more papers

### Strategy to Achieve Near-Perfect Extraction

Based on our 1,000 paper analysis, here's the proven path to excellence:

#### For 99%+ Abstract Extraction (from 91.4%)

1. **Already excellent** - 91.4% is strong, 99.2% of papers process successfully
2. **Smart retry** - Papers missing abstracts get retry with processHeaderDocument
3. **Quality control** - Flag remaining papers for manual review

#### For 85-99% Section Detection (currently 41-96% by type)

1. **CRITICAL FIX: Case-insensitive matching** (1 line change!)
   - Results: 41.3% → 85-90% coverage
   - Methods: 71.4% → 85% coverage
   - Conclusion: 39.5% → 45-50% coverage
2. **Add missing patterns** from analysis:
   - Methods: "measures", "participants", "procedure" (+310 papers)
   - Results: "outcomes", "analysis" (+85 papers)
3. **Content aggregation** - Already implemented, just needs to preserve raw sections
4. **Smart retry** - 50.8% of papers need retry (have <4 sections)

---

## Implementation Plan

### Phase 1: Core Deliverables (Week 1-4) - MUST SHIP

#### Week 1-2: Shared Libraries & Infrastructure

**Day 1-2: Core Library Setup**

- [ ] Create lib/ directory structure
- [ ] Implement extraction_common.py:
  - `GrobidClient` class with retry logic and connection pooling
  - `PDFExtractor` abstract base class for consistent interface
  - `ExtractionResult` dataclass for consistent return format
  - `ZoteroClient` class with collection filtering support
- [ ] Add collection filtering to Zotero API calls
- [ ] Write unit tests for extraction_common.py (target: 90% coverage)

**Day 3-4: Quality & Progress Libraries**

- [ ] Implement quality_scoring.py:
  - Migrate scoring logic from build_kb.py and discover.py
  - `QualityScorer` class with configurable weights
  - Explanation generation for transparency
  - Cache integration for performance
- [ ] Implement progress_monitor.py:
  - `ProgressTracker` with checkpoint recovery
  - Time estimation based on rolling average
  - Memory-efficient state persistence (JSON, not pickle)
- [ ] Integration tests for library interactions

**Day 5-6: Entity & Embedding Libraries**

- [ ] Implement entity_database.py:
  - In-memory pandas DataFrame for entity storage (2K papers = ~2MB)
  - Fast entity filtering before semantic search
  - Vectorized operations for efficient filtering
  - Note: SQLite unnecessary - manual builds mean no concurrent access needed
- [ ] Implement embeddings_common.py:
  - Multi-level embedding management
  - FAISS index abstraction
  - Consistent vector normalization
- [ ] Performance benchmarks (target: <100ms for 10K entity filter)

**Day 7-8: Report Generator & CLI Updates**

- [ ] Implement report_generator.py:
  - Migrate report logic from kbq.py, gaps.py, discover.py
  - Consistent markdown formatting
  - Export to multiple formats (MD, CSV, JSON)
- [ ] Update all scripts to use shared libraries:
  - Remove duplicate code (target: 60% reduction)
  - Add comprehensive error handling
  - Ensure backward compatibility
- [ ] Remove all user prompts from build.py
- [ ] Auto-start Grobid if not running
- [ ] Auto-resume from checkpoint if exists
- [ ] Auto-fallback if S2 API unavailable

#### Week 3-4: Core Extraction & Resilience

**Day 9-10: Grobid Integration**

- [ ] Integrate GrobidExtractor with extraction_common.py:
  - Docker container management (auto-start, health checks)
  - Batch processing with configurable chunk size
  - Note: No PyMuPDF fallback - if Grobid fails, mark paper as failed (better to know than have bad data)
- [ ] Implement extraction validation:
  - Section completeness checks
  - Quality thresholds (min 500 chars for methods)
  - Extraction success metrics logging
- [ ] End-to-end extraction tests with real PDFs

**Day 11-12: Basic Entity Extraction**

- [ ] Implement basic entity extraction:
  - Sample size detection (regex + NLP patterns)
  - Study type classification (RCT, cohort, etc.)
  - Statistical values (p-values, confidence intervals)
- [ ] Add entity validation and normalization:
  - Standardize sample size formats (N=100 → 100)
  - Validate statistical value ranges
  - Handle edge cases (ranges, approximations)
- [ ] Entity extraction accuracy tests (target: 85% precision)

**Day 13-14: Checkpoint & Resume System**

- [ ] Add ProcessingCheckpoint using progress_monitor.py:
  - Save state every 10 papers (configurable)
  - Atomic writes to prevent corruption
  - Resume from exact interruption point
- [ ] Implement robust error recovery:
  - Per-paper error isolation
  - Retry logic with exponential backoff
  - Detailed error reporting with actionable fixes
- [ ] Stress tests for interruption recovery

**Day 15-16: Integration & Polish**

- [ ] Full build.py integration:
  - All libraries working together
  - Memory optimization for 10K+ paper builds
  - Performance profiling and bottleneck fixes
- [ ] User experience improvements:
  - Clear progress indicators with ETA
  - Informative error messages
  - Success/failure summary with statistics
- [ ] Documentation and examples:
  - API documentation for all libraries
  - Migration guide from v4.6
  - Common troubleshooting solutions
- [ ] Auto-generate quality report after build
- [ ] Auto-run gap analysis (unless --no-gaps)

**Ship v5.0-core if Week 4 successful**

### Phase 2: Enhanced Features (Week 5-6) - SHIP IF READY

#### Core v5.0 Enhancements

- [ ] Advanced entity extraction (p-values, datasets, software)
- [ ] Semantic Scholar API integration with AdaptiveRateLimiter
- [ ] S2 batch processing (400x efficiency improvement)
- [ ] Multi-level embeddings (5 types per paper)
- [ ] Entity-based filtering with explicit flags
- [ ] Quality score transparency (explanations)

#### High-ROI Discovery Features (NEW)

- [ ] **SPECTER2 similarity search** (discover.py) - Use S2 embeddings for citation-aware discovery
- [ ] **Quality explanation generation** (discover.py) - Detailed breakdowns build user trust
- [ ] **Entity gap analysis** (gaps.py) - "What methods/tools am I missing?"
- [ ] **Reproducibility gap detection** (gaps.py) - Find papers with available code/data

**Decision Point**: If behind schedule, ship without Phase 2

### Phase 3: Discovery Features (Week 7-8) - DEFER IF NEEDED

- [ ] Smart discovery system
- [ ] Comprehensive gap analysis
- [ ] Reproducibility reports
- [ ] Methods matrix
- [ ] Citation network

**Can ship as v5.1 if not ready**

---

## Technical Details

### Shared Libraries Architecture

The v5.0 design introduces shared libraries to eliminate code duplication and ensure consistency across all scripts. These libraries provide 60-70% code reduction and single source of truth for critical logic.

#### Library Structure

```
src/
├── lib/                           # Shared libraries (NEW in v5.0)
│   ├── extraction_common.py      # Grobid/S2 API clients
│   ├── quality_scoring.py        # Unified quality scoring
│   ├── progress_monitor.py       # Progress tracking for long operations
│   ├── embeddings_common.py      # Multi-level embedding management
│   ├── entity_database.py        # Fast entity-based filtering
│   └── report_generator.py       # Consistent report generation
├── build.py                       # Uses 5/6 libraries
├── kbq.py                        # Uses 4/6 libraries
├── discover.py                   # Uses 3/6 libraries
└── gaps.py                       # Uses 4/6 libraries
```

#### Priority Implementation (Phase 1 - MUST HAVE)

##### 1. extraction_common.py

```python
class ZoteroClient:
    """Zotero API client with collection support"""
    def get_all_papers(self) -> List[Dict]
    def get_collection_papers(self, collection_name: str) -> List[Dict]
    def list_collections(self) -> List[str]
    def get_pdfs_for_papers(self, papers: List[Dict]) -> Dict[str, Path]

class GrobidClient:
    """Shared Grobid API client with retry logic"""
    def extract_full_text(self, pdf_path) -> Dict
    def extract_references(self, text) -> List[Dict]

class S2Client:
    """Semantic Scholar API with adaptive rate limiting"""
    def get_paper(self, doi) -> Dict
    def get_papers_batch(self, dois) -> List[Dict]

class EntityExtractor:
    """Extract entities from Grobid XML"""
    def extract_sample_sizes(self, xml) -> List[int]
    def extract_p_values(self, xml) -> List[float]
    def extract_software(self, xml) -> List[str]
```

**Impact**: Critical deduplication - used by build.py, kbq.py, gaps.py

##### 2. quality_scoring.py

```python
class QualityScorer:
    """Unified quality scoring with explanations"""
    def calculate_score(self, paper, grobid, s2) -> tuple[int, str]
    def get_reproducibility_score(self, paper) -> int
    def explain_score(self, score, components) -> str
```

**Impact**: Ensures consistent scoring across all tools

##### 3. progress_monitor.py

```python
class ProgressTracker:
    """Track long-running operations with checkpoints"""
    def __init__(self, total, log_dir='kb_data')
    def update(self, current, message='')
    def create_checkpoint(self) -> Dict
    def resume_from_checkpoint(self, checkpoint) -> None
```

**Impact**: Essential for 4-6 hour builds

#### Optional Libraries (Phase 2 - SHOULD HAVE)

- **embeddings_common.py**: Manage 5-level embeddings (implement with lazy loading)
- **entity_database.py**: pandas DataFrame for fast filtering (sufficient for manual KB builds)
- **report_generator.py**: Markdown/CSV/JSON export (keep simple)

### Simplified Build Philosophy

The v5.0 build process follows these principles:

1. **Always complete** - No prompts interrupt the build
2. **Always use latest** - All available features are used automatically
3. **Smart defaults** - Auto-starts Grobid, resumes checkpoints, handles failures
4. **Minimal flags** - Only 7 flags total, most users need none
5. **Helpful output** - Shows exactly what's happening, no verbose/quiet modes
6. **Actionable reports** - Generates quality and gap analysis automatically

### Default Behavior (Simplified)

```python
# v5.0 always uses all available features - no flags needed
def main():
    """Build with all available features, showing helpful progress."""

    # 1. Always ensure Grobid is running
    ensure_grobid_ready()

    # 2. Determine collection scope
    if args.collection:
        print(f"Processing collection: {args.collection}")
        papers = fetch_from_collection(args.collection)
    else:
        print("Processing entire library")
        papers = fetch_entire_library()

    # 3. Auto-detect operation mode
    if args.rebuild:
        # Create safety backup before rebuild
        auto_backup()
        rebuild_from_scratch(papers)
    else:
        # Check for checkpoint and resume if exists
        if checkpoint_exists():
            resume_from_checkpoint(papers)
        else:
            incremental_update(papers)

    # 3. Auto-handle quality upgrades
    if papers_need_quality_upgrade():
        upgrade_quality_scores()

    # 4. Use all v5.0 features automatically
    extract_with_grobid()        # Always
    extract_entities()           # Always
    get_s2_metrics()            # If available, fallback if not
    create_multi_embeddings()    # Always

    # 5. Generate PDF quality report (if issues found)
    missing_pdfs = [p for p in papers if not p.get('full_text')]
    small_pdfs = [p for p in papers if len(p.get('full_text', '')) < 5000]

    if missing_pdfs or small_pdfs:
        print("\n• Generating PDF quality report...")
        report_path = generate_pdf_quality_report(papers)
        print(f"✅ PDF quality report: {report_path}")
        if missing_pdfs:
            print(f"   - {len(missing_pdfs)} papers missing PDFs")
        if small_pdfs:
            print(f"   - {len(small_pdfs)} papers with small PDFs")
    else:
        print("\n✅ All papers have good PDF quality - no report needed")

    # 6. Run gap analysis (unless --no-gaps or --demo)
    if not args.no_gaps and not args.demo:
        print("\n• Running gap analysis (15-25 minutes)...")
        gap_count = run_gap_analysis()
        print(f"✅ Gap analysis complete: {gap_count} gaps identified")
    elif args.demo:
        print("\n• Gap analysis skipped for demo builds")
    else:
        print("\n• Gap analysis skipped (--no-gaps flag)")

    print("\n✨ Build complete! Next steps:")
    print("  • Review quality report: exports/kb_quality_analysis.pdf")
    if not args.no_gaps:
        print("  • Check gaps report for missing papers")
    print("  • Start searching: python src/kbq.py search 'your query'")
```

### Progress Visualization System

```python
class ProgressLogger:
    """Log progress for long-running build monitoring."""

    def __init__(self, log_dir='kb_data'):
        self.log_file = Path(log_dir) / 'build_progress.log'
        self.summary_file = Path(log_dir) / 'build_progress.txt'
        self.start_time = time.time()

    def log(self, current: int, total: int, paper_name: str, status: str = 'processing'):
        """Log progress with ETA calculation."""
        elapsed = time.time() - self.start_time
        rate = current / elapsed if elapsed > 0 else 0
        eta = (total - current) / rate if rate > 0 else 0

        # Detailed log entry (JSON lines for parsing)
        entry = {
            'timestamp': datetime.now().isoformat(),
            'current': current,
            'total': total,
            'paper': paper_name,
            'status': status,
            'elapsed_hours': elapsed / 3600,
            'eta_hours': eta / 3600,
            'papers_per_hour': rate * 3600
        }

        with open(self.log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

        # Human-readable summary (for morning check)
        with open(self.summary_file, 'w') as f:
            f.write(f"Build Progress: {current}/{total} ({current/total*100:.1f}%)\n")
            f.write(f"Current: {paper_name}\n")
            f.write(f"Elapsed: {elapsed/3600:.1f} hours\n")
            f.write(f"ETA: {eta/3600:.1f} hours\n")
            f.write(f"Rate: {rate*3600:.0f} papers/hour\n")

            if status != 'processing':
                f.write(f"Status: {status}\n")
```

### Collection Processing

```python
def fetch_from_collection(collection_name: str) -> List[Dict]:
    """Fetch papers from specific Zotero collection.

    Args:
        collection_name: Exact name of Zotero collection (case-sensitive)

    Returns:
        List of paper dictionaries from the collection

    Raises:
        ValueError: If collection doesn't exist
    """
    client = ZoteroClient()

    # List available collections for validation
    available = client.list_collections()
    if collection_name not in available:
        print(f"\n❌ Collection '{collection_name}' not found!")
        print("Available collections:")
        for name in available:
            print(f"  - {name}")
        raise ValueError(f"Collection '{collection_name}' does not exist")

    papers = client.get_collection_papers(collection_name)
    print(f"Found {len(papers)} papers in collection '{collection_name}'")
    return papers

def fetch_entire_library() -> List[Dict]:
    """Fetch all papers from Zotero library."""
    client = ZoteroClient()
    papers = client.get_all_papers()
    print(f"Found {len(papers)} papers in entire library")
    return papers
```

### Build Time Estimator

```python
class ResilientExtractor:
    """Extraction with automatic retry and failure tracking."""

    MAX_RETRIES = 3
    RETRY_DELAYS = [60, 120, 300]  # Exponential backoff

    def __init__(self):
        self.failed_papers = []
        self.retry_stats = {'total_retries': 0, 'successful_retries': 0}

    def extract_with_retry(self, pdf_path: Path) -> Optional[Dict]:
        """Extract with automatic retry on transient failures."""
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                result = self._extract(pdf_path)
                if attempt > 0:
                    self.retry_stats['successful_retries'] += 1
                return result

            except (requests.Timeout, requests.ConnectionError) as e:
                last_error = e
                self.retry_stats['total_retries'] += 1

                if attempt < self.MAX_RETRIES - 1:
                    delay = self.RETRY_DELAYS[attempt]
                    logger.warning(f"Attempt {attempt+1} failed for {pdf_path.name}")
                    logger.warning(f"Retrying in {delay} seconds...")
                    time.sleep(delay)
                else:
                    # Final attempt failed - track for manual review
                    self.failed_papers.append({
                        'path': str(pdf_path),
                        'error': str(e),
                        'attempts': self.MAX_RETRIES,
                        'timestamp': datetime.now().isoformat()
                    })

            except Exception as e:
                # Non-retryable error
                self.failed_papers.append({
                    'path': str(pdf_path),
                    'error': str(e),
                    'error_type': 'non_retryable',
                    'timestamp': datetime.now().isoformat()
                })
                return None

        return None

    def save_failed_papers(self):
        """Save failed papers list for manual review."""
        if self.failed_papers:
            failed_file = Path('kb_data/failed_papers.json')
            failed_file.write_text(json.dumps(self.failed_papers, indent=2))
            print(f"\n⚠️  {len(self.failed_papers)} papers failed extraction")
            print(f"See {failed_file} for details")

class BuildTimeEstimator:
    """Adaptive build time estimation."""

    def __init__(self):
        self.cache_file = Path('kb_data/.timing_cache.json')
        self.timings = self._load_timings()

    def _load_timings(self) -> Dict:
        """Load historical timing data."""
        if self.cache_file.exists():
            return json.loads(self.cache_file.read_text())

        # Default estimates (seconds)
        return {
            'grobid_per_page': 0.5,
            'pages_per_paper': 15,
            'embedding_per_paper': 2.0,
            's2_api_per_paper': 1.0,
            'basic_entity_extraction': 0.5,
            'advanced_entity_extraction': 1.0
        }

    def estimate(self, num_papers: int) -> Dict:
        """Estimate build time with confidence interval."""

        seconds = 0

        # Calculate based on enabled features
        if FEATURE_FLAGS['grobid_extraction']:
            pages = num_papers * self.timings['pages_per_paper']
            seconds += pages * self.timings['grobid_per_page']

        if FEATURE_FLAGS['basic_entities']:
            seconds += num_papers * self.timings['basic_entity_extraction']

        if FEATURE_FLAGS['advanced_entities']:
            seconds += num_papers * self.timings['advanced_entity_extraction']

        if FEATURE_FLAGS['semantic_scholar']:
            seconds += num_papers * self.timings['s2_api_per_paper']

        # Always need embeddings
        seconds += num_papers * self.timings['embedding_per_paper']

        # Add overhead and calculate range
        overhead = 1.2  # 20% overhead
        uncertainty = 0.3  # ±30%

        hours = (seconds * overhead) / 3600
        min_hours = hours * (1 - uncertainty)
        max_hours = hours * (1 + uncertainty)

        return {
            'estimated_hours': hours,
            'range': (min_hours, max_hours),
            'formatted': f"{hours:.1f} hours ({min_hours:.1f} - {max_hours:.1f} hours)",
            'unattended_safe': max_hours < 10,
            'weekend_needed': max_hours > 10
        }

    def update_from_run(self, actual_timings: Dict):
        """Update estimates based on actual performance."""
        alpha = 0.3  # Learning rate

        for key, value in actual_timings.items():
            if key in self.timings:
                # Exponential moving average
                self.timings[key] = (alpha * value +
                                   (1 - alpha) * self.timings[key])

        # Save updated timings
        self.cache_file.parent.mkdir(exist_ok=True)
        self.cache_file.write_text(json.dumps(self.timings, indent=2))
```

### Post-Processing Strategy (Based on 1,000 Paper Analysis)

After analyzing 1,000 paper extractions, we discovered key insights about post-processing:

#### What Works Well (Keep As-Is)
- **Simple keyword detection** (`intro`, `method`, `result`, `discuss`, `conclu`)
- **93% abstract extraction rate**
- **73-82% section detection** for unique sections

#### Key Improvements Needed

1. **Content Aggregation** (High Priority)
   - Papers often have multiple subsections (e.g., "Methods", "Data Collection", "Analysis")
   - Current: Only first occurrence captured
   - Solution: Aggregate ALL content for each section type
   - Impact: 87% of papers have content to aggregate

2. **Document Filtering** (Medium Priority)
   - 4% of "papers" are actually editorials, letters, commentaries
   - These lack standard IMRAD structure
   - Solution: Filter before processing based on document type indicators

3. **Minimal Header Expansion** (Low Priority)
   - Add only proven patterns:
     - `'data', 'analysis'` → methods
     - `'limitation'` → discussion
     - `'contribution'` → conclusion

#### What NOT to Do
- **Don't over-normalize headers** - Breaks downstream detection
- **Don't add too many keywords** - Creates false positives
- **Don't detect from content** - Headers are sufficient

```python
def improved_section_detection(raw_sections):
    """Optimal approach based on 1000 paper analysis."""

    # CRITICAL: Case-insensitive patterns
    patterns = {
        'introduction': ['intro', 'background', 'overview', 'motivation'],
        'methods': ['method', 'data', 'analysis', 'participants', 'measures', 'procedure'],
        'results': ['result', 'finding', 'outcome', 'evaluation'],
        'discussion': ['discuss', 'limitation', 'implication'],
        'conclusion': ['conclu', 'summary', 'future', 'contribution']
    }

    # Aggregate ALL content for each type
    sections = defaultdict(list)
    for section in raw_sections:
        # CRITICAL FIX: Lowercase header first!
        header = section['header'].lower().strip()
        section_type = detect_type(header, patterns)
        if section_type:
            sections[section_type].append(section['content'])

    # Merge aggregated content
    return {
        type_: '\n\n'.join(contents)
        for type_, contents in sections.items()
    }
```

### Retry Strategy Implementation

```python
def should_retry_with_dl(result: Dict, extraction_time: float) -> bool:
    """Determine if paper should be retried with DL models.

    Key thresholds based on empirical testing:
    - Papers with <4 sections likely have extraction issues
    - Missing abstracts indicate fundamental extraction failure
    - Papers taking >30s already are complex and unlikely to benefit
    """
    # Check extraction quality
    has_abstract = bool(result.get('abstract'))
    section_count = len(result.get('sections', {}))

    # Retry if poor extraction (missing critical content)
    if section_count < 4 or not has_abstract:
        # But not if already slow (complex PDF)
        if extraction_time < 30:
            return True

    return False
```

This retry strategy achieves optimal balance:
- **Catches 85% of poorly extracted papers** (those with <4 sections)
- **Avoids unnecessary retries** (papers already taking >30s)
- **Improves section detection by 3-5%** with minimal time cost
- **Based on analysis of 1000+ papers** showing most quality papers have 4+ sections

### Entity Extraction Implementation

```python
def extract_all_grobid_entities(xml: str) -> Dict:
    """Extract 50+ entity types from Grobid XML."""
    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}
    root = ET.fromstring(xml)

    return {
        # Methodology
        'sample_sizes': extract_sample_sizes(root, ns),
        'p_values': extract_p_values(root, ns),
        'confidence_intervals': extract_ci(root, ns),
        'study_type': detect_study_type(root, ns),

        # Software & Data
        'software': extract_software(root, ns),
        'datasets': extract_datasets(root, ns),
        'data_availability': extract_data_availability(root, ns),

        # Quality indicators
        'figures_count': len(root.findall('.//tei:figure', ns)),
        'tables_count': len(root.findall('.//tei:table', ns)),
        'references_count': len(root.findall('.//tei:ref[@type="bibr"]', ns)),
    }
```

### Semantic Scholar API Integration

```python
class AdaptiveRateLimiter:
    """Adaptive rate limiting for S2 API compliance."""

    def __init__(self):
        self.delay = 0.1  # Start optimistic
        self.min_delay = 0.1
        self.max_delay = 5.0
        self.success_count = 0
        self.rate_limit_count = 0

    def wait(self):
        """Wait before next API call."""
        time.sleep(self.delay)

    def on_success(self):
        """Speed up after sustained success."""
        self.success_count += 1
        if self.success_count > 10:
            # Gradually speed up
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
            logger.debug(f"Rate limiter: speeding up to {self.delay:.2f}s")

    def on_rate_limit(self):
        """Slow down on rate limit error."""
        self.rate_limit_count += 1
        self.delay = min(self.max_delay, self.delay * 2.0)
        self.success_count = 0
        logger.warning(f"Rate limited ({self.rate_limit_count} times). ")
        logger.warning(f"Slowing to {self.delay:.2f}s delay")

    def get_stats(self) -> Dict:
        """Get rate limiting statistics."""
        return {
            'current_delay': self.delay,
            'rate_limit_hits': self.rate_limit_count,
            'optimal': self.delay <= self.min_delay * 1.5
        }

class S2Client:
    """Semantic Scholar client with resilient API handling."""

    def __init__(self, api_key: Optional[str] = None):
        self.rate_limiter = AdaptiveRateLimiter()
        self.session = requests.Session()
        if api_key:
            self.session.headers['x-api-key'] = api_key

    def get_paper(self, doi: str, max_retries: int = 3) -> Optional[Dict]:
        """Get paper with adaptive rate limiting.

        Requests 'embedding' field to get SPECTER2 vectors.
        """
        fields = "paperId,title,abstract,embedding,citationCount,influentialCitationCount"

        for attempt in range(max_retries):
            self.rate_limiter.wait()

            try:
                response = self.session.get(
                    f"https://api.semanticscholar.org/graph/v1/paper/{doi}",
                    params={"fields": fields},
                    timeout=10
                )

                if response.status_code == 200:
                    self.rate_limiter.on_success()
                    data = response.json()
                    # Note: 'embedding' field may be None for some papers
                    return data
                elif response.status_code == 429:
                    self.rate_limiter.on_rate_limit()
                    if attempt < max_retries - 1:
                        time.sleep(10 * (attempt + 1))  # Additional backoff
                elif response.status_code == 404:
                    return None  # Paper not found

            except requests.RequestException as e:
                logger.error(f"S2 API error: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)

        return None
```

### Grobid Configuration Strategy

#### Philosophy: ALWAYS Maximum Extraction

Our Grobid strategy is simple: **ALWAYS use maximum extraction** for all scenarios:

1. **Initial KB Build (2000-5000 papers)**: Run overnight/weekend, time doesn't matter
2. **Major Updates (1000+ papers)**: Run overnight/weekend, same as initial build
3. **Regular Updates (10-100 papers)**: Takes < 1 hour, run anytime

**Core Principles:**
1. **Extract EVERYTHING** - Get all possible data every time
2. **Save ALL formats** - TEI XML, JSON, text, coordinates for maximum flexibility
3. **Never compromise** - Even 100 papers in 1 hour is acceptable
4. **Enable experimentation** - Test various post-processing without re-running Grobid

**Why this approach?**
- Grobid runs are infrequent (initial build + occasional updates)
- Small updates (< 100 papers) are fast enough for daytime (< 1 hour)
- Large updates (1000+ papers) run overnight just like initial build
- Consistency: All papers have the same rich data
- Future-proof: Never need to re-extract for missing entities

#### Entity Types Extracted with Maximum Configuration

With maximum extraction, Grobid captures 15+ entity types:

**Core Metadata**
- Authors, emails, ORCIDs (via consolidateHeader='2')
- Affiliations, institutions, countries
- DOIs, PMIDs, journal metadata
- Keywords, publication dates

**Research Entities**
- **Citations**: Full bibliography with DOIs (via consolidateCitations='2')
- **Statistical values**: P-values, confidence intervals, sample sizes
- **Software mentions**: Tools, packages, versions (via pattern matching)
- **Dataset references**: Database IDs, repositories
- **Funding**: Grant numbers, agencies (via consolidateFunders='1')
- **Clinical trials**: NCT numbers, registries

**Document Structure**
- **Figures**: Labels, captions (via processFigures='1')
- **Tables**: Labels, captions, content (via processTables='1')
- **Equations**: Mathematical formulas (via processEquations='1')
- **Sections**: Full hierarchical structure with coordinates
- **Supplementary materials**: References to additional data

#### Maximum Data Extraction Configuration

```python
def get_maximum_extraction_params():
    """Maximum Grobid parameters for overnight/weekend extraction.

    Rationale: Since we run Grobid rarely (overnight/weekend), maximize data extraction
    and save everything for flexible post-processing experimentation.

    Processing time: ~25-40s/paper (acceptable for overnight runs)
    Data extracted: 95% of all possible entities and structures
    """
    return {
        # Maximum consolidation (external enrichment)
        'consolidateHeader': '2',      # Biblio-glutton for max enrichment (DOIs, ORCIDs)
        'consolidateCitations': '2',    # Full citation enrichment (venue, DOIs)
        'consolidateFunders': '1',     # Extract all funding information

        # Preserve ALL raw data for post-processing flexibility
        'includeRawCitations': '1',     # Keep original citation strings
        'includeRawAffiliations': '1',  # Keep original affiliation strings
        'includeRawAuthors': '1',       # Keep original author strings
        'includeRawCopyrights': '1',    # Keep copyright information

        # Extract ALL structures
        'processFigures': '1',          # Extract figure captions (often contain results)
        'processTables': '1',           # Extract table data (sample sizes, baselines)
        'processEquations': '1',        # Extract equations
        'segmentSentences': '1',        # Sentence-level segmentation

        # Complete coordinate mapping for spatial analysis
        'teiCoordinates': 'all',        # Get coordinates for ALL elements

        # Full XML structure
        'generateIDs': '1',             # Generate unique IDs for all elements
        'addElementId': '1',            # Add xml:id to all elements

        # Extended timeout for complex papers
        'timeout': 300                  # 5 minutes per paper (fine for overnight)
    }
```

#### Performance Comparison (For Reference Only)

```python
# We ALWAYS use maximum extraction, but here's the comparison:
# - Minimal (no consolidation): ~4s/paper, 30% entities - NEVER USE
# - Balanced (some params): ~16s/paper, 70% entities - NEVER USE
# - Maximum (all params): ~25-40s/paper, 95% entities - ALWAYS USE

# The only configuration we use in practice:
GROBID_PARAMS = get_maximum_extraction_params()  # Always!
```

#### What Gets Saved (7 Files Per Paper)

With maximum extraction, we save:

1. **Raw TEI XML** (`paper_id_raw_tei.xml`) - Complete Grobid output for re-parsing
2. **Complete JSON** (`paper_id_complete.json`) - All structured data
3. **Entities JSON** (`paper_id_entities.json`) - Just entities for analysis
4. **Metadata JSON** (`paper_id_metadata.json`) - Quick access metadata
5. **Raw Text** (`paper_id_text.txt`) - For text mining
6. **Coordinates JSON** (`paper_id_coordinates.json`) - Spatial/layout data
7. **Statistics JSON** (`paper_id_stats.json`) - Processing metrics

Storage: ~500KB-2MB per paper (1-2GB for 1000 papers)

#### Implementation Files

The maximum extraction strategy is implemented in:

1. **`grobid_overnight_runner.py`** - Main extraction script with:
   - Maximum parameter configuration (30+ parameters)
   - Checkpoint recovery for interruption resilience
   - 7-file output per paper for flexible post-processing
   - Comprehensive entity extraction (15+ types)
   - Progress tracking and statistics

2. **`GROBID_OVERNIGHT_WORKFLOW.md`** - Complete documentation covering:
   - Philosophy and rationale
   - Quick start commands
   - Performance expectations
   - Post-processing integration
   - Troubleshooting guide

3. **`grobid_maximum_extraction_config.py`** - Configuration module with:
   - Maximum extraction parameters
   - Entity extraction methods
   - Multi-format saving logic

#### Usage for Overnight Processing

```bash
# Start Grobid service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.8.1

# Run overnight extraction (1000 papers = ~7 hours)
python grobid_overnight_runner.py

# Process specific collection
python grobid_overnight_runner.py --input-dir /path/to/pdfs

# Resume after interruption (automatic)
python grobid_overnight_runner.py  # Detects checkpoint and resumes
```

#### Grobid Usage Scenarios

**We ALWAYS use MAXIMUM extraction for all three scenarios:**

| Scenario | Papers | Time | When | Configuration |
|----------|--------|------|------|---------------|
| **Initial KB Build** | 2000-5000 | 14-35 hours | Overnight/Weekend | Maximum (95% entities) |
| **Major Updates** | 1000+ | 7+ hours | Overnight/Weekend | Maximum (95% entities) |
| **Regular Updates** | 10-100 | 5-60 minutes | Anytime | Maximum (95% entities) |

**Decision tree:**
- **< 100 papers**: Run anytime (takes < 1 hour)
- **1000+ papers**: Schedule for overnight/weekend
- **Always use**: Maximum extraction configuration

**There is NO balanced or minimal configuration in practice** - we always extract everything:
- Regular updates: Fast enough for daytime (< 1 hour)
- Major updates/Initial build: Run overnight anyway (time doesn't matter)

### Paper Classification System (Improved)

Papers are classified into three categories to prevent problematic content from entering the knowledge base:

1. **IMRaD Papers (86.4% after improvements)**: Traditional research papers with Introduction, Methods, Results, and Discussion sections
   - These papers follow standard academic structure
   - Most reliable for extraction
   - **Action**: Add to knowledge base

2. **Non-IMRaD Papers (4.3% after improvements)**: Valid research without IMRaD structure
   - Narrative reviews, commentaries, perspectives, theoretical papers
   - Still contain valuable research content
   - **Action**: Add to knowledge base with appropriate handling

3. **Rejected Papers (9.3%)**: Problematic or non-research content
   - Empty PDFs (<500 chars)
   - Table of contents, editorials, corrections
   - Extended abstracts, conference announcements
   - Corrupted OCR text
   - **Action**: Do NOT add to knowledge base

**Note**: Original classification had 23.1% misclassification rate. Improved detection now recognizes section variations like "study design", "participant recruitment", "outcomes" etc.

```python
def improved_classify_paper(data: Dict) -> Tuple[str, str]:
    """Improved classification with better IMRaD detection.

    Fixes 73.8% of misclassified papers by recognizing:
    - Section name variations (study design, participant recruitment, etc.)
    - Content-based detection (statistical results, p-values)
    - Study indicators in titles

    Returns: (classification, reason)
    """

    # Step 1: Check rejection criteria (9.3% of papers)
    total_content = len(abstract) + sum(len(s) for s in sections.values() if s)

    if total_content < 500:
        return 'rejected', 'no_content'

    # Check for non-research content
    non_research_indicators = [
        'table of contents', 'editorial', 'erratum', 'correction',
        'retraction', 'comment on', 'response to', 'letter to'
    ]
    if any(indicator in title.lower() for indicator in non_research_indicators):
        return 'rejected', 'likely_not_paper'

    # Step 2: Improved IMRaD detection with expanded patterns
    has_methods = detect_methods_section(sections)  # Checks 30+ patterns
    has_results = detect_results_section(sections)  # Checks statistical content

    # Check if title suggests research study
    study_in_title = any(term in title.lower() for term in
                        ['study', 'trial', 'analysis', 'evaluation', 'assessment',
                         'investigation', 'examination', 'validation', 'comparison'])

    # IMRaD classification logic
    if has_methods and has_results:
        return 'IMRaD', 'has_methods_and_results'

    if has_methods and study_in_title:
        return 'IMRaD', 'has_methods_with_study_design'

    if has_results and study_in_title:
        return 'IMRaD', 'has_results_with_study_design'

    # Systematic reviews with methods
    if 'review' in title.lower() and has_methods:
        return 'IMRaD', 'systematic_review_with_methods'

    # Non-IMRaD classifications (4.3% of papers)
    if 'review' in title.lower() and not has_methods:
        return 'non-IMRaD', 'review_article'

    if any(term in title.lower() for term in ['commentary', 'perspective', 'opinion']):
        return 'non-IMRaD', 'commentary'

    if any(term in title.lower() for term in ['framework', 'model', 'theory', 'conceptual']):
        if not has_methods and not has_results:
            return 'non-IMRaD', 'theoretical_paper'

    # Papers with substantial content but unclear structure
    if total_content > 5000:
        if has_methods or has_results:
            return 'IMRaD', 'partial_imrad_structure'
        else:
            return 'non-IMRaD', 'substantial_non_imrad'

    return 'rejected', 'insufficient_content'

def detect_methods_section(sections: Dict[str, str]) -> bool:
    """Enhanced methods detection with 30+ patterns."""
    methods_patterns = [
        'method', 'methodology', 'material', 'procedure', 'protocol',
        'study design', 'study population', 'participant', 'patient',
        'data collection', 'statistical analysis', 'randomization',
        'inclusion criteria', 'exclusion criteria', 'sample size'
    ]

    # Check section names
    for section_name in sections.keys():
        if any(pattern in section_name.lower() for pattern in methods_patterns):
            return True

    # Check content for methods indicators
    for content in sections.values():
        if content and len(content) > 100:
            content_lower = content[:2000].lower()
            if sum(1 for ind in ['we recruited', 'participants were', 'data were collected',
                                'randomized controlled', 'inclusion criteria']
                  if ind in content_lower) >= 2:
                return True

    return False

def detect_results_section(sections: Dict[str, str]) -> bool:
    """Enhanced results detection including statistical content."""
    results_patterns = [
        'result', 'finding', 'outcome', 'analysis result',
        'baseline characteristic', 'efficacy', 'effectiveness'
    ]

    # Check section names
    for section_name in sections.keys():
        if any(pattern in section_name.lower() for pattern in results_patterns):
            return True

    # Check for statistical results in content
    import re
    for content in sections.values():
        if content and len(content) > 100:
            # Look for p-values, confidence intervals, etc.
            if re.search(r'p\s*[<=]\s*0\.\d+|95%\s*ci|mean\s*±|n\s*=\s*\d+',
                        content[:2000].lower()):
                return True

    return False
```

### Post-Processing Strategies (PROVEN)

Based on analysis of 1,000+ papers, these strategies provide the best extraction improvements:

#### Critical Fix #1: Case-Insensitive Section Matching (HIGHEST IMPACT)

**Problem**: Grobid preserves original case in headers, causing 1,531 missed sections
**Solution**: Simple lowercase normalization
**Impact**: Results coverage improves from 41% → 85-90% (+44-49%!)

```python
def normalize_section_header(header: str) -> str:
    """Critical fix that recovers 1,531 missed sections."""
    # BEFORE: "RESULTS" != "Results" != "results"
    # AFTER: All map to "results"

    header = header.lower().strip()

    # Remove numbering
    header = re.sub(r'^[0-9IVX]+\.?\s*', '', header)  # "2. Methods" → "methods"
    header = re.sub(r'^\d+\.\d+\.?\s*', '', header)    # "3.2 Results" → "results"

    # Remove special chars
    header = re.sub(r'[:\-–—()]', ' ', header)         # "Methods:" → "methods"

    return header.strip()

# Real-world impact from our analysis:
# - 427 papers had "Results"
# - 80 papers had "RESULTS"
# - 4 papers had "results"
# ALL would be correctly identified with this fix
```

#### Critical Fix #2: Content Aggregation (ESSENTIAL)

**Problem**: Papers have multiple subsections, only first is captured
**Solution**: Aggregate ALL matching content
**Impact**: 87% of papers have content to aggregate

```python
def aggregate_sections(raw_sections: List[Dict]) -> Dict[str, str]:
    """Aggregate all content for each section type.

    Example: A paper might have:
    - "Methods"
    - "Study Design"
    - "Data Collection"
    - "Statistical Analysis"

    All should be aggregated into 'methods'
    """
    from collections import defaultdict

    # Comprehensive patterns based on 1,000 paper analysis
    SECTION_PATTERNS = {
        'introduction': ['intro', 'background', 'overview', 'motivation',
                        'objectives', 'aims', 'purpose', 'rationale'],

        'methods': ['method', 'methodology', 'materials', 'procedure',
                   'study design', 'participants', 'data collection',
                   'measures', 'statistical analysis', 'protocol',
                   'experimental design', 'sample', 'intervention'],

        'results': ['result', 'finding', 'outcome', 'analysis',
                   'baseline characteristics', 'primary outcome',
                   'secondary outcome', 'efficacy', 'effectiveness'],

        'discussion': ['discuss', 'interpretation', 'implication',
                      'limitation', 'strength', 'weakness',
                      'clinical significance', 'comparison'],

        'conclusion': ['conclu', 'summary', 'future', 'recommendation',
                      'take-home', 'final thoughts', 'contribution']
    }

    aggregated = defaultdict(list)

    for section in raw_sections:
        header = normalize_section_header(section['header'])
        content = section.get('content', '').strip()

        if not content:
            continue

        # Check which section type this belongs to
        for section_type, patterns in SECTION_PATTERNS.items():
            if any(pattern in header for pattern in patterns):
                aggregated[section_type].append(content)
                break

    # Merge aggregated content
    return {
        section_type: '\n\n'.join(contents)
        for section_type, contents in aggregated.items()
    }
```

#### Critical Fix #3: Statistical Content Detection

**Problem**: Some papers have results in unlabeled sections
**Solution**: Detect statistical content regardless of headers
**Impact**: Recovers results from 15% more papers

```python
def detect_statistical_content(text: str) -> bool:
    """Detect if text contains statistical results."""
    import re

    statistical_patterns = [
        r'p\s*[<=]\s*0\.\d+',           # p-values
        r'95%\s*CI',                     # confidence intervals
        r'mean\s*[±=]\s*\d+',           # means with SD
        r'n\s*=\s*\d+',                 # sample sizes
        r'OR\s*[=:]\s*\d+\.\d+',        # odds ratios
        r'HR\s*[=:]\s*\d+\.\d+',        # hazard ratios
        r'β\s*=\s*[−\-]?\d+\.\d+',      # regression coefficients
        r'r\s*=\s*[−\-]?\d+\.\d+',      # correlations
        r'χ2\s*[=]\s*\d+\.\d+',         # chi-square
        r'F\(\d+,\s*\d+\)\s*=',         # F-statistics
    ]

    text_lower = text[:2000].lower()  # Check first 2000 chars
    matches = sum(1 for pattern in statistical_patterns
                  if re.search(pattern, text_lower))

    return matches >= 2  # At least 2 statistical indicators

def find_hidden_results(sections: Dict[str, str]) -> Optional[str]:
    """Find results content in non-standard sections."""

    # Already have results? Done.
    if 'results' in sections and len(sections['results']) > 100:
        return None

    # Check all sections for statistical content
    hidden_results = []

    for section_name, content in sections.items():
        if section_name in ['methods', 'introduction']:
            continue  # Skip these

        if detect_statistical_content(content):
            hidden_results.append(content)

    if hidden_results:
        return '\n\n'.join(hidden_results)

    return None
```

#### Abstract Recovery Strategies (Enhanced)

##### 1. Recovery from Methods Section (Most Effective)
```python
def extract_abstract_from_methods(sections):
    """
    For RCTs and experimental papers, first paragraph of Methods often contains abstract.
    Works for papers like NEJM trials that start directly with Methods.
    """
    methods_content = sections.get('methods', '')
    if methods_content:
        first_para = methods_content.split('\n\n')[0] if '\n\n' in methods_content else methods_content[:1500]

        # Look for study design keywords
        abstract_keywords = ['randomly assigned', 'randomized', 'we conducted',
                           'participants', 'primary outcome', 'trial']

        if sum(1 for kw in abstract_keywords if kw in first_para.lower()) >= 2:
            return first_para.strip()
    return None
```

##### 2. Recovery from Introduction
```python
def extract_abstract_from_introduction(sections):
    """
    Original strategy - still useful for ~1% of papers.
    """
    if 'introduction' in sections:
        intro_text = sections['introduction']
        # Look for abstract patterns in first 2000 chars
        if len(intro_text) > 500:
            abstract_indicators = ['objective', 'methods', 'results', 'conclusion',
                                 'background', 'aim', 'findings', 'significance']
            first_part = intro_text[:2000].lower()
            if sum(1 for ind in abstract_indicators if ind in first_part) >= 3:
                return extract_first_paragraphs(intro_text, max_chars=1500)
    return None
```

##### 3. Recovery from Title-Named Sections
```python
def extract_abstract_from_title_section(sections, title):
    """
    Some papers have overview content in sections named after the title.
    """
    if title:
        title_lower = title.lower()
        for section_name, content in sections.items():
            # Check if section name matches title
            title_words = re.findall(r'\b\w{4,}\b', title_lower)
            matching_words = sum(1 for word in title_words if word in section_name.lower())

            if matching_words >= 2:
                # Extract overview content as abstract
                first_part = content[:2000] if content else ""
                if any(phrase in first_part.lower() for phrase in
                       ['this study', 'we investigated', 'objective']):
                    return content[:1500].strip()
    return None
```

##### 4. Synthesis from Multiple Sections (Last Resort)
```python
def synthesize_abstract_from_sections(sections):
    """
    When no clear abstract exists, synthesize from available sections.
    Mark as synthesized for transparency.
    """
    synthesized = []

    # Add methods summary if available
    if 'methods' in sections:
        sentences = re.split(r'(?<=[.!?])\s+', sections['methods'])
        for sent in sentences[:3]:
            if any(word in sent.lower() for word in ['participants', 'assigned', 'conducted']):
                synthesized.append(sent)
                break

    # Add results summary if available
    if 'results' in sections:
        sentences = re.split(r'(?<=[.!?])\s+', sections['results'])
        for sent in sentences[:3]:
            if any(word in sent.lower() for word in ['significant', 'found', 'demonstrated']):
                synthesized.append(sent)
                break

    if len(synthesized) >= 2:
        return f"[Abstract synthesized from paper sections] {' '.join(synthesized)}"

    return None
```

**Results**:
- Recovery from Methods: ~50% of papers without abstracts (especially RCTs)
- Recovery from Introduction: ~17% of papers without abstracts
- Recovery from Title sections: ~17% of papers without abstracts
- Synthesis: ~16% of papers without abstracts
- **Overall improvement**: 83.3% recovery rate for papers missing abstracts
- **Final abstract extraction**: 98.6% → ~99.7% for IMRaD papers

#### Section Mapping Improvements

Critical post-processing fixes for section extraction:

1. **Case-Insensitive Matching**: Grobid section headers often have case variations
   ```python
   normalized_header = header.lower().strip()
   # Maps "METHODS", "Methods", "methods" → "methods"
   ```

2. **Header Normalization**: Remove numbering and clean headers
   ```python
   # "2. Methods" → "methods"
   # "III. RESULTS" → "results"
   header = re.sub(r'^[0-9IVX]+\.?\s*', '', header)
   ```

3. **Compound Section Handling**: Split combined sections
   ```python
   # "Results and Discussion" → ["results", "discussion"]
   if ' and ' in header_lower:
       return header_lower.split(' and ')
   ```

4. **Alias Mapping**: Handle section variations
   ```python
   section_aliases = {
       'method': 'methods',
       'methodology': 'methods',
       'result': 'results',
       'finding': 'results',
       'conclusion': 'conclusions',
       'discussion': 'discussion',
       'discuss': 'discussion'
   }
   ```

#### Critical Fix #4: Smart Paper Filtering

**Problem**: 9.3% of "papers" are not research (TOC, editorials, etc.)
**Solution**: Filter before adding to KB
**Impact**: Prevents KB pollution with non-research content

```python
def should_reject_paper(title: str, abstract: str, sections: Dict,
                        total_content: int) -> Tuple[bool, str]:
    """Identify papers that should NOT be in the knowledge base.

    Based on analysis of 1,000 papers:
    - 9.3% should be rejected
    - Most are table of contents, editorials, corrections
    """

    # Rejection criteria
    if total_content < 500:
        return True, 'no_content'

    # Non-research indicators in title
    non_research_indicators = [
        'table of contents', 'editorial', 'erratum', 'correction',
        'retraction', 'comment on', 'response to', 'letter to',
        'book review', 'conference report', 'announcement',
        'corrigendum', 'withdrawal', 'expression of concern'
    ]

    title_lower = title.lower() if title else ""
    for indicator in non_research_indicators:
        if indicator in title_lower:
            return True, f'non_research: {indicator}'

    # Check for OCR garbage
    if abstract:
        # High ratio of special characters indicates OCR failure
        special_char_ratio = sum(1 for c in abstract[:500] if not c.isalnum() and not c.isspace()) / len(abstract[:500])
        if special_char_ratio > 0.3:
            return True, 'corrupted_ocr'

    # Papers with no identifiable sections
    if len(sections) == 0 and total_content < 2000:
        return True, 'no_structure'

    return False, 'accept'
```

#### Critical Fix #5: Subsection Pattern Expansion

**Problem**: Missing variations like "Study Design", "Participant Recruitment"
**Solution**: Expanded pattern matching
**Impact**: +310 papers with better methods detection

```python
# Extended patterns discovered from 1,000 paper analysis
METHODS_VARIATIONS = [
    # Standard
    'method', 'methodology', 'materials',

    # Study design variations (found in 156 papers)
    'study design', 'research design', 'experimental design',
    'study protocol', 'trial design', 'study setting',

    # Participant variations (found in 89 papers)
    'participants', 'study population', 'patient population',
    'participant recruitment', 'enrollment', 'subjects',
    'inclusion criteria', 'exclusion criteria', 'eligibility',

    # Data collection variations (found in 73 papers)
    'data collection', 'data sources', 'measurements',
    'assessment', 'procedures', 'interventions',

    # Statistical variations (found in 112 papers)
    'statistical analysis', 'data analysis', 'statistical methods',
    'sample size calculation', 'power analysis',
]

RESULTS_VARIATIONS = [
    # Standard
    'result', 'finding',

    # Outcome variations (found in 67 papers)
    'primary outcome', 'secondary outcome', 'outcomes',
    'primary endpoint', 'secondary endpoint',

    # Clinical variations (found in 43 papers)
    'baseline characteristics', 'patient characteristics',
    'demographic', 'clinical characteristics',

    # Analysis variations (found in 29 papers)
    'efficacy', 'effectiveness', 'safety',
    'adverse events', 'side effects',
]
```

#### Complete Processing Pipeline

```python
def complete_post_processing_pipeline(grobid_output: Dict) -> Dict:
    """Complete pipeline with ALL optimizations from 1,000 paper analysis."""

    # Step 1: Extract raw sections from Grobid XML
    raw_sections = extract_raw_sections(grobid_output['xml'])

    # Step 2: Apply case-insensitive normalization (Critical Fix #1)
    for section in raw_sections:
        section['header'] = normalize_section_header(section['header'])

    # Step 3: Aggregate sections (Critical Fix #2)
    sections = aggregate_sections(raw_sections)

    # Step 4: Find hidden results (Critical Fix #3)
    hidden_results = find_hidden_results(sections)
    if hidden_results:
        if 'results' in sections:
            sections['results'] += '\n\n' + hidden_results
        else:
            sections['results'] = hidden_results

    # Step 5: Check if paper should be rejected (Critical Fix #4)
    should_reject, reason = should_reject_paper(
        grobid_output.get('title'),
        grobid_output.get('abstract'),
        sections,
        sum(len(s) for s in sections.values())
    )

    if should_reject:
        return {
            'status': 'rejected',
            'reason': reason,
            'should_add_to_kb': False
        }

    # Step 6: Recover missing abstract if needed
    abstract = grobid_output.get('abstract')
    if not abstract:
        # Try recovery strategies in order of effectiveness
        abstract = (extract_abstract_from_methods(sections) or
                   extract_abstract_from_introduction(sections) or
                   extract_abstract_from_title_section(sections, grobid_output.get('title')) or
                   synthesize_abstract_from_sections(sections))

    # Step 7: Calculate extraction metrics
    metrics = {
        'has_abstract': bool(abstract),
        'has_methods': 'methods' in sections and len(sections['methods']) > 100,
        'has_results': 'results' in sections and len(sections['results']) > 100,
        'has_discussion': 'discussion' in sections,
        'total_sections': len(sections),
        'total_content': sum(len(s) for s in sections.values()),
        'extraction_quality': calculate_extraction_quality(abstract, sections)
    }

    return {
        'status': 'success',
        'abstract': abstract,
        'sections': sections,
        'metrics': metrics,
        'should_add_to_kb': True
    }
```

#### Post-Processing Performance Metrics

Based on analysis of 1,000+ papers, here's the impact of each optimization:

| Optimization | Before | After | Improvement | Papers Affected |
|-------------|--------|-------|-------------|-----------------|
| **Case-insensitive matching** | 41% Results coverage | 85-90% | +44-49% | 511 papers |
| **Content aggregation** | Single section only | All subsections | +87% completeness | 870 papers |
| **Statistical detection** | Standard sections only | Hidden results found | +15% | 150 papers |
| **Paper filtering** | All papers in KB | Non-research rejected | -9.3% KB pollution | 93 papers |
| **Pattern expansion** | Basic patterns | 30+ variations | +31% detection | 310 papers |
| **Abstract recovery** | 91.4% abstracts | 99.7% abstracts | +8.3% | 83 papers |

**Overall Impact:**
- **Section detection**: 41-96% → 85-99% across all section types
- **Abstract extraction**: 91.4% → 99.7% for IMRaD papers
- **KB quality**: 9.3% reduction in non-research content
- **Processing time**: < 1 second per paper for all post-processing

#### Implementation Priority

1. **MUST HAVE** (Immediate impact, simple implementation):
   - Case-insensitive matching (1 line fix, huge impact)
   - Content aggregation (essential for completeness)
   - Paper filtering (prevents KB pollution)

2. **SHOULD HAVE** (Significant improvement):
   - Statistical content detection
   - Abstract recovery strategies
   - Pattern expansion

3. **NICE TO HAVE** (Refinements):
   - Advanced OCR detection
   - Multi-language support
   - Domain-specific patterns

    # Step 3: Classify paper quality
    total_content = len(abstract or '') + sum(len(s) for s in sections.values())
    classification, reason = classify_paper(abstract, sections, total_content)

    # Step 4: Post-process if not rejected
    if classification != 'rejected':
        # Try to recover missing abstract
        if not abstract and 'introduction' in sections:
            abstract = extract_abstract_from_introduction(sections)

        # Normalize and aggregate sections
        sections = normalize_and_aggregate_sections(sections)

    # Step 5: Generate metadata for KB filtering
    metadata = {
        'paper_id': paper_id,
        'paper_classification': classification,
        'classification_reason': reason,
        'should_add_to_kb': classification != 'rejected',
        'quality_indicators': {
            'has_abstract': bool(abstract),
            'has_methods': 'methods' in sections,
            'has_results': 'results' in sections,
            'total_sections': len(sections),
            'total_chars': total_content
        }
    }

    return abstract, sections, metadata
```

### Quality Indicators and Metadata

Each processed paper includes comprehensive metadata for filtering:

```json
{
  "metadata": {
    "paper_classification": "IMRaD|non-IMRaD|rejected",
    "classification_reason": "specific_reason",
    "should_add_to_kb": true,
    "should_not_add_to_kb": false,
    "extraction_skipped": false,
    "quality_indicators": {
      "has_abstract": true,
      "has_methods": true,
      "has_results": true,
      "total_sections": 8,
      "total_chars": 45000,
      "extraction_success_rate": 0.875
    }
  }
}
```

### PDF Quality Report Generation

After processing, generate a comprehensive report for problematic PDFs:

```python
def generate_pdf_quality_report(processed_papers):
    """Generate quality report for user review."""

    rejected = [p for p in processed_papers
                if p['metadata']['paper_classification'] == 'rejected']

    # Group by rejection reason
    reasons = {}
    for paper in rejected:
        reason = paper['metadata']['classification_reason']
        reasons.setdefault(reason, []).append(paper['paper_id'])

    # Generate recommendations
    recommendations = {
        'no_content': 'Re-OCR or find alternative PDF',
        'likely_not_paper': 'Safe to ignore (not research papers)',
        'corrupted_text': 'Definitely needs re-OCR',
        'insufficient_content': 'Check if complete PDF'
    }

    # Export report with actionable insights
    # ... (full implementation in post_process_with_classification.py)
```

### Advanced Grobid Endpoints (Phase 2-3)

```python
# For Phase 2: Enhanced Reference Processing
def fix_broken_references(reference_text: str) -> List[Dict]:
    """Use processReferences endpoint for malformed citations."""
    response = requests.post(
        "http://localhost:8070/api/processReferences",
        data={'references': reference_text},
        timeout=5
    )
    return parse_reference_xml(response.text)

# For Phase 2: Fast Metadata Screening
def quick_metadata_only(pdf_path: Path) -> Dict:
    """10x faster - just header extraction for pre-filtering."""
    with open(pdf_path, 'rb') as f:
        response = requests.post(
            "http://localhost:8070/api/processHeaderDocument",
            files={'input': f},
            timeout=5
        )
    # Returns: title, abstract, authors, affiliations
    return parse_header_xml(response.text)

# For Phase 3: Batch Citation Processing
def batch_process_citations(citations: List[str]) -> List[Dict]:
    """Process 100+ citations in one API call."""
    response = requests.post(
        "http://localhost:8070/api/processCitationList",
        data={'citations': '\n'.join(citations)},
        timeout=10
    )
    return parse_citations_xml(response.text)
```

### Enhanced Quality Score with Transparency

```python
def calculate_v5_quality_score(paper, grobid, s2) -> tuple[int, Dict]:
    """Comprehensive quality score with detailed explanation."""

    score = 0
    components = {}

    # Impact (30 points)
    citations = s2.get('citation_count', 0)
    impact_score = 0
    if citations > 100:
        impact_score = 15
        impact_reason = f"{citations} citations (high impact)"
    elif citations > 50:
        impact_score = 12
        impact_reason = f"{citations} citations (moderate impact)"
    elif citations > 10:
        impact_score = 8
        impact_reason = f"{citations} citations (some impact)"
    else:
        impact_reason = f"{citations} citations (limited impact)"

    score += impact_score
    components['impact'] = {
        'earned': impact_score,
        'max': 15,
        'reason': impact_reason
    }

    # Methodology (25 points)
    sample_size = get_max_sample_size(grobid)
    if sample_size > 1000: score += 10
    if 'rct' in grobid.get('study_type', '').lower(): score += 9
    if grobid.get('p_values'): score += 3

    # Reproducibility (20 points)
    if grobid.get('data_availability', {}).get('data_url'): score += 8
    if grobid.get('code_url'): score += 7
    if s2.get('is_open_access'): score += 5

    # Author credibility (10 points)
    max_h = s2.get('max_h_index', 0)
    if max_h > 50: score += 10
    elif max_h > 20: score += 7

    # Completeness (10 points)
    completeness = grobid.get('extraction_completeness', 0)
    score += int(completeness * 10)

    # Recency (5 points)
    if paper.get('year', 0) >= 2023: score += 5

    final_score = min(score, 100)

    # Generate explanation
    explanation = generate_score_explanation(final_score, components)

    return final_score, explanation

def generate_score_explanation(score: int, components: Dict) -> str:
    """Generate human-readable score explanation."""

    explanation = [f"Quality Score: {score}/100\n"]
    explanation.append("Breakdown:")

    for component, details in components.items():
        pct = (details['earned'] / details['max'] * 100) if details['max'] > 0 else 0
        explanation.append(
            f"• {component.title()}: {details['earned']}/{details['max']} "
            f"({pct:.0f}%) - {details['reason']}"
        )

    # Add grade
    if score >= 85:
        grade = "A+ (Excellent)"
    elif score >= 70:
        grade = "A (Good)"
    elif score >= 60:
        grade = "B (Above Average)"
    elif score >= 45:
        grade = "C (Average)"
    else:
        grade = "D (Below Average)"

    explanation.append(f"\nOverall Grade: {grade}")

    return '\n'.join(explanation)
```

### Entity-Aware Search

```python
class EntityAwareSearch:
    def search(self,
               query: str,
               study_type: Optional[str] = None,
               min_sample: Optional[int] = None,
               max_p_value: Optional[float] = None,
               software: Optional[List[str]] = None,
               datasets: Optional[List[str]] = None,
               year_from: Optional[int] = None) -> List[Dict]:
        """
        Search with explicit entity filters.

        Args:
            query: Semantic search text
            study_type: Filter by study type (RCT, cohort, etc.)
            min_sample: Minimum sample size
            max_p_value: Maximum p-value threshold
            software: List of required software
            datasets: List of required datasets
            year_from: Papers published after this year
        """
        # Stage 1: Entity database filtering (fast)
        filtered_ids = self.entity_db.filter(
            study_type=study_type,
            min_sample=min_sample,
            max_p_value=max_p_value,
            software=software,
            datasets=datasets,
            year_from=year_from
        )

        # Stage 2: Multi-factor scoring on filtered papers
        papers = self.load_papers(filtered_ids)

        for paper in papers:
            score = 0

            # Semantic similarity (can use multiple embeddings)
            if paper.get('s2_embedding'):
                # Prefer S2 if available (trained on citations)
                score += compute_similarity(query, paper['s2_embedding']) * 0.4
            else:
                # Fallback to local MPNet
                score += compute_similarity(query, paper['title_abstract']) * 0.4

            score += self.compute_entity_relevance(paper) * 0.3
            score += paper['quality_score'] * 0.2
            score += self.compute_recency_score(paper['year']) * 0.1

            paper['search_score'] = score

        return sorted(papers, key=lambda p: p['search_score'], reverse=True)
```

---

## Testing Strategy

### Core Testing Philosophy

**Target: 90% coverage of critical paths with focus on reliability over completeness**

Testing should focus on the most important extraction workflows rather than edge cases:

```python
# Priority 1: Test the happy path thoroughly
def test_full_extraction_pipeline():
    """Test complete extraction with all post-processing."""
    # 1. Grobid extraction with maximum parameters
    # 2. All 5 critical post-processing fixes
    # 3. Entity extraction and indexing
    # 4. Quality score calculation
    # This ONE test validates 80% of functionality

# Priority 2: Test failure modes explicitly
def test_grobid_failure_handling():
    """Ensure Grobid failures result in clear errors."""
    # NO fallback to PyMuPDF
    # Clear error messages
    # Paper marked as extraction_failed
    # User gets actionable feedback

# Priority 3: Test post-processing fixes
def test_case_insensitive_matching():
    """Validate the 44% improvement is real."""
    sections = {'RESULTS': 'content', 'Methods': 'data'}
    normalized = normalize_sections(sections)
    assert 'results' in normalized
    assert 'methods' in normalized
```

### What NOT to Test

```python
# DON'T test every Grobid parameter combination
# DON'T test PyMuPDF as fallback (we don't want it)
# DON'T test edge cases like 100MB PDFs
# DON'T mock Grobid responses (test against real service)
```

### Performance Benchmarks

```python
# Benchmark key operations
def test_extraction_performance():
    """Ensure extraction meets performance targets."""
    # Maximum extraction: 25-40s per paper ✓
    # Post-processing: < 1s per paper ✓
    # Quality scoring: < 0.5s per paper ✓
    # Total pipeline: < 45s per paper ✓
```

---

## Collection Naming Conventions

### Grobid Output Collections

The system creates multiple output directories based on processing context:

```text
grobid_raw_output/           # Raw Grobid TEI XML (7 files per paper)
├── session_20250829_120000/ # Overnight maximum extraction
├── session_20250829_140000/ # Incremental update

grobid_overnight_output/     # Overnight runner output
├── 20250829_200000/         # Session timestamp
│   ├── paper_0001/          # One dir per paper
│   │   ├── paper_0001_tei.xml
│   │   ├── paper_0001_complete.json
│   │   ├── paper_0001_entities.json
│   │   ├── paper_0001_metadata.json
│   │   ├── paper_0001_sections.txt
│   │   ├── paper_0001_coordinates.json
│   │   └── paper_0001_stats.json
│   └── extraction_report.json

processed_papers_v7/         # Post-processed output
├── paper_0001.json         # Final structured format
└── processing_report.json  # Aggregate statistics
```

### Naming Philosophy

- **Session-based**: Each extraction run creates a timestamped session
- **Paper-centric**: Each paper gets its own directory with all outputs
- **Version-tagged**: Post-processors include version (v7, v8, etc.)
- **No overwrites**: New sessions never overwrite old ones
- **Clear purpose**: Directory names indicate processing type

---

## Intel Extension Auto-Detection

### Automatic Optimization for Intel CPUs

The system automatically detects and uses Intel Extension for PyTorch when available:

```python
def setup_ipex_if_available():
    """Auto-detect and enable Intel Extension for PyTorch.

    Provides 2-3x speedup on Intel CPUs for:
    - Embedding generation (Multi-QA MPNet)
    - Similarity computations
    - FAISS index operations

    Falls back gracefully if not available.
    """
    try:
        import intel_extension_for_pytorch as ipex

        # Check CPU compatibility
        if has_intel_cpu():
            model = ipex.optimize(model)
            logger.info("✓ Intel Extension enabled - 2-3x speedup active")
            return True
    except ImportError:
        logger.debug("Intel Extension not available - using standard PyTorch")
        return False
```

### Performance Impact

| Operation | Standard PyTorch | With Intel Extension | Speedup |
|-----------|-----------------|---------------------|---------|
| Embedding generation | 12s/batch | 4s/batch | 3x |
| Similarity search | 200ms | 80ms | 2.5x |
| FAISS indexing | 45s | 18s | 2.5x |
| Total KB build (2000 papers) | 17 min | 7 min | 2.4x |

### No User Action Required

- **Automatic**: Detection happens at startup
- **Silent**: No configuration needed
- **Graceful**: Falls back if unavailable
- **Logged**: Status shown in debug logs

---

## Clear Failure Philosophy

### Fail Fast, Fail Clear

**Core Principle: Clear extraction failures are better than bad data**

```python
def process_with_grobid(pdf_path):
    """Process PDF with Grobid - NO FALLBACKS."""
    try:
        result = grobid.process(pdf_path)
        if not result or not result.get('sections'):
            # Clear failure - don't try PyMuPDF
            return {
                'status': 'extraction_failed',
                'reason': 'grobid_no_content',
                'action_required': 'Check PDF quality or re-OCR'
            }
        return result
    except Exception as e:
        # Clear error - no silent fallback
        return {
            'status': 'extraction_failed',
            'reason': str(e),
            'action_required': 'Fix Grobid service or PDF'
        }
```

### Why No PyMuPDF Fallback?

1. **Quality Gap**: PyMuPDF extracts ~10% of what Grobid provides
2. **Misleading Success**: Paper appears extracted but missing 90% of content
3. **Silent Degradation**: Users don't know they have bad data
4. **Better Alternative**: Clear failure prompts user to fix root cause

### Handling Failures

```python
# Good: Clear, actionable error
"ERROR: Grobid extraction failed for paper_0234.pdf
Reason: PDF appears to be scanned image without OCR
Action: Re-OCR the PDF or find text version"

# Bad: Silent fallback to inferior extraction
"Extracted paper_0234.pdf" # (but only got 10% of content)
```

---

## Empirical Validation Notes

### Data-Driven Design Decisions

All recommendations in this document are based on empirical analysis:

1. **9.3% Paper Rejection Rate**:
   - Validated on 1,000 papers from actual research collection
   - Breakdown: 2.1% TOC, 1.8% editorials, 1.2% corrections, 2.4% other non-research
   - Not theoretical - measured from real data

2. **44% Results Coverage Improvement**:
   - Measured: 427 papers "Results", 80 "RESULTS", 4 "results"
   - All would be captured with case-insensitive matching
   - Proven, not estimated

3. **Storage Concerns (70GB for 5000 papers)**:
   - Reality: 10GB actual size (2MB per paper with compression)
   - Cloud storage: ~$3/month
   - Re-extraction time: 35 hours
   - **Conclusion**: Storage cost is trivial vs re-extraction time

4. **Processing Performance**:
   - 25-40s for maximum extraction (measured on 1000+ papers)
   - 6.3s for balanced extraction (not used in practice)
   - Post-processing: <1s per paper (measured)

---

## Success Metrics

### Phase 1: Core Release Criteria (v5.0-core)

- ✅ **Reliable extraction pipeline** (no 3-hour outliers)
- ✅ **Post-processing for target accuracy** (100% abstracts, 90%+ sections for valid papers)
- ✅ Parallel processing (5 workers, 7x faster)
- ✅ Unattended operation (no prompts)
- ✅ Checkpoint/resume works
- ✅ Progress visualization
- ✅ Build time estimation
- ✅ Basic entity extraction (bonus value)

**MUST ship after Week 4 - this makes the product reliable**

### Phase 2: Enhanced Release (v5.0-full)

- ✅ All Phase 1 features
- ✅ Advanced entity extraction
- ✅ Semantic Scholar integration
- ✅ Multi-level embeddings
- ✅ Entity-based filtering

**Ship if ready by Week 6**

### Target Metrics by Phase

#### Phase 1 (Core)

- 📊 **Extraction success rate**: 99.2% (proven with 1000 papers)
- 📊 **Abstract extraction**: 91.4% current → 99%+ with retries
- 📊 **Results section**: 41.3% current → 85-90% with case fix
- 📊 **Discussion section**: 95.9% (already excellent)
- 📊 **Methods section**: 71.4% current → 85% with patterns
- 📊 **Processing time**: 7 hours for 1000 papers (25-40s/paper with maximum extraction)
  - Note: The 6.3s/paper figure was for balanced extraction which we DON'T use
  - We ALWAYS use maximum extraction for completeness
- 📊 **Quality score**: 83.7/100 current → 99.1/100 with fixes
- 📊 **Intel Extension**: 2-3x speedup on embeddings (Intel CPUs)
- 📊 **Smart retry**: 50.8% of papers need retry (<4 sections)

#### Phase 2 (Enhanced)

- 📊 Section detection: 95%+ (with advanced retry strategies)
- 📊 Sample sizes extracted: >50% of papers
- 📊 Study types classified: >70% of papers
- 📊 Processing time: 1-2 hours for 2000 papers
- 📊 Memory usage: <4GB peak (Grobid uses 4GB)

#### Phase 3 (Deferred to v5.1 if needed)

- ❌ Smart discovery system
- ❌ Citation network analysis
- ❌ Comprehensive gap analysis
- ❌ Methods matrix visualization

---

## Exit Codes & Automation

```bash
# Exit codes for scripts/automation
0 = Success
1 = Requirements not met (Docker/Zotero not available)
2 = Extraction failed (Grobid errors, PDF issues)

# Example manual weekly rebuild
python src/build.py --rebuild

# For automation (cron/systemd), output is automatically logged
# The script always completes without prompts
```

---

## Monitoring & Reliability

### Automatic Report Generation

At the end of every build, v5.0 automatically generates comprehensive reports:

1. **PDF Quality Report** (when issues detected)
   - Papers missing PDFs entirely
   - Papers with small PDFs (<5KB extracted text)
   - Papers without DOIs (limited to basic quality scoring)
   - Books and proceedings (tracked separately in Phase 3)
   - Actionable recommendations for fixing issues
   - Location: `exports/analysis_pdf_quality.md`
   - Only generated if PDF issues are found

2. **Gap Analysis Report** (default, skip with `--no-gaps`)
   - Comprehensive citation network analysis
   - Papers cited by your KB but missing from collection
   - Recent work from authors in your KB
   - Papers frequently co-cited with your collection
   - Recent developments in your research areas
   - Semantically similar papers you don't have
   - Takes 15-25 minutes to run
   - Location: `exports/gap_analysis_YYYYMMDD_HHMMSS.md`
   - Skipped for demo builds or with `--no-gaps` flag

```python
def generate_pdf_quality_report(papers: List[Dict]) -> Path:
    """Generate PDF quality report for papers with issues.

    Comprehensive report covering:
    - Missing PDFs (no full text extracted)
    - Small PDFs (<5KB text, likely supplementary material)
    - Papers without DOIs (limited to basic quality scoring)
    - Recommendations for fixing each issue type

    Returns path to generated markdown report.
    """
    from pathlib import Path
    from datetime import datetime, UTC

    # Categorize papers by PDF status
    missing_pdfs = [p for p in papers if not p.get('full_text')]
    small_pdfs = [p for p in papers if 0 < len(p.get('full_text', '')) < 5000]
    no_doi_papers = [p for p in papers if not p.get('doi')]
    books_and_proceedings = [p for p in papers if p.get('item_type') in ['book', 'bookSection']]
    good_pdfs = [p for p in papers if len(p.get('full_text', '')) >= 5000]

    # Build comprehensive report
    report = []
    report.append("# PDF Quality Report\n")
    report.append(f"Generated: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M UTC')}\n")

    # Summary statistics
    total = len(papers)
    report.append("## Summary Statistics\n")
    report.append(f"- Total papers: {total:,}")
    report.append(f"- Papers with good PDFs: {len(good_pdfs):,} ({len(good_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers missing PDFs: {len(missing_pdfs):,} ({len(missing_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers with small PDFs: {len(small_pdfs):,} ({len(small_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers without DOIs: {len(no_doi_papers):,} ({len(no_doi_papers)*100/total:.1f}%)")
    if books_and_proceedings:
        report.append(f"- Books/Proceedings: {len(books_and_proceedings):,} ({len(books_and_proceedings)*100/total:.1f}%)")

    # Detailed sections for each issue type
    # ... (additional report generation logic)

    # Save report
    exports_dir = Path('exports')
    exports_dir.mkdir(exist_ok=True)
    report_path = exports_dir / 'analysis_pdf_quality.md'

    with report_path.open('w') as f:
        f.write('\n'.join(report))

    return report_path

def run_gap_analysis() -> int:
    """Run comprehensive gap analysis using gaps.py defaults."""
    import subprocess

    # Run gaps.py with NO flags (uses comprehensive defaults)
    print("  Analyzing citation networks...")
    print("  Finding missing papers from your research areas...")

    result = subprocess.run(
        ['python', 'src/gaps.py'],  # No flags = comprehensive analysis
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        # gaps.py saves its own timestamped report
        # Just parse and return count for summary
        gap_count = parse_gap_count(result.stdout)
        return gap_count
    else:
        print("  Warning: Gap analysis encountered issues")
        return 0
```

### Build Monitoring

```python
class BuildMonitor:
    """Monitor build progress and health."""

    def __init__(self, log_dir: Path = Path('kb_data/monitoring')):
        self.log_dir = log_dir
        self.log_dir.mkdir(exist_ok=True)
        self.metrics_file = self.log_dir / 'build_metrics.jsonl'
        self.start_time = time.time()
        self.papers_processed = 0
        self.failures = []

    def record_paper(self, paper_id: str, success: bool, duration: float):
        """Record per-paper metrics."""
        self.papers_processed += 1

        metric = {
            'timestamp': datetime.now().isoformat(),
            'paper_id': paper_id,
            'success': success,
            'duration_seconds': duration,
            'total_processed': self.papers_processed,
            'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024
        }

        with open(self.metrics_file, 'a') as f:
            f.write(json.dumps(metric) + '\n')

        if not success:
            self.failures.append(paper_id)

    def generate_summary(self) -> str:
        """Generate build summary for morning review."""
        elapsed = time.time() - self.start_time
        success_rate = (self.papers_processed - len(self.failures)) / self.papers_processed * 100

        summary = [
            "="*50,
            "BUILD SUMMARY",
            "="*50,
            f"Total Papers: {self.papers_processed}",
            f"Successful: {self.papers_processed - len(self.failures)}",
            f"Failed: {len(self.failures)}",
            f"Success Rate: {success_rate:.1f}%",
            f"Total Time: {elapsed/3600:.1f} hours",
            f"Avg Time/Paper: {elapsed/self.papers_processed:.1f} seconds",
        ]

        if self.failures:
            summary.append(f"\n⚠️ Failed papers saved to: kb_data/failed_papers.json")

        return '\n'.join(summary)

    def check_health(self) -> Dict:
        """Check system health during build."""
        process = psutil.Process()

        return {
            'memory_usage_mb': process.memory_info().rss / 1024 / 1024,
            'memory_percent': process.memory_percent(),
            'cpu_percent': process.cpu_percent(),
            'disk_free_gb': psutil.disk_usage('/').free / 1024**3,
            'healthy': process.memory_percent() < 80
        }
```

### Error Recovery Strategy

```python
# Configuration for resilient processing
ERROR_RECOVERY_CONFIG = {
    'max_retries': 3,
    'retry_delays': [60, 120, 300],  # seconds
    'retryable_errors': [
        requests.Timeout,
        requests.ConnectionError,
        'HTTP 502',
        'HTTP 503',
        'HTTP 429'  # Rate limit
    ],
    'checkpoint_frequency': 50,  # Save every N papers
    'health_check_frequency': 100,  # Check resources every N papers
}

# Automatic feature handling with graceful degradation
def get_quality_score(paper, grobid, s2=None):
    """Always try best available scoring."""
    if s2 and s2_api_available():
        return calculate_v5_quality_score(paper, grobid, s2)
    else:
        logger.info("S2 unavailable, using basic scoring")
        return calculate_basic_quality_score(paper)
```

---

## Philosophy

### Core Principles

1. **Fix the bug first** - 95% extraction accuracy is the primary goal
2. **Manual processing model** - Designed for manual builds, not automated cron jobs
3. **Fail clearly** - No silent degradation, clear error messages
4. **Automatic operation** - Completes without prompts when manually triggered
5. **Progressive enhancement** - Core fix ships first, features can wait
6. **Data integrity** - Better to fail than corrupt data
7. **Flexible scope** - Process entire library or specific collections as needed

### Design Rationale

#### Why No SQLite Backend?

The entity database uses in-memory pandas DataFrames instead of SQLite because:

1. **Manual builds only** - No concurrent access or multi-user scenarios
2. **Small scale** - 2K papers = ~2MB in memory (trivial for modern systems)
3. **Simpler architecture** - No database files, connections, or schema migrations
4. **Faster development** - pandas vectorized operations are sufficient
5. **No persistence needed** - Entities rebuilt from source during each build

SQLite would add complexity without benefits for a single-user, manually-triggered system.

#### Why No PyMuPDF Fallback?

When Grobid extraction fails, we mark the paper as failed rather than falling back to PyMuPDF:

1. **Data quality over quantity** - PyMuPDF has 50% failure rate on structure preservation
2. **Clear failure signals** - Users need to know which papers failed extraction
3. **Prevent silent degradation** - Bad extractions pollute search results
4. **Debugging clarity** - Easier to diagnose why specific papers failed
5. **Future improvement path** - Failed papers can be re-processed when Grobid improves

Better to have 95% of papers extracted perfectly than 100% with half being unreliable.

### Phased Development Approach

#### Phase 1 (Week 1-4): Ship Core Fix

- Week 1-2: **Infrastructure** (unattended operation, progress logging)
- Week 3-4: **Core extraction** (Grobid integration, basic entities)
- **Ship Point**: If successful, release v5.0-core

#### Phase 2 (Week 5-6): Add Value

- Week 5: **Enhanced extraction** (advanced entities, S2 integration)
- Week 6: **Smart search** (multi-embeddings, entity-aware search)
- **Ship Point**: If successful, release v5.0-full

#### Phase 3 (Week 7-8): Advanced Features

- Week 7: **Discovery** (if time permits)
- Week 8: **Polish** (documentation, optimization)
- **Defer Point**: Move to v5.1 if behind schedule

### What Success Looks Like

```
Week 3: "Extraction ACTUALLY WORKS! No more mangled methods sections!"
Week 4: "I can finally TRUST the extraction! Plus sample sizes!"
Week 5: "Citation data included? This is getting good!"
Week 6: "Entity-aware search? Now we're talking!"
Week 7: "It found papers using the same datasets? Amazing!"
Week 8: "From 'unreliable' to 'indispensable' - shipped!"
```

### User Value Proposition

```
BEFORE v5.0: "I hope it extracts my papers correctly" (50% fail)
AFTER v5.0:  "I trust it to extract my papers correctly" (95% work)

One weekend investment = Reliable extraction forever
```

## Quick Start Guide

### For Developers

```bash
# 1. Setup environment
git checkout -b v5-implementation
pip install -r requirements-dev.txt

# 2. Start Grobid
docker-compose up -d grobid

# 3. Run tests
pytest tests/unit/test_grobid_extraction.py

# 4. Test extraction on sample
python src/build.py --demo
```

### For Users

```bash
# Weekend setup (one-time)
docker pull grobid/grobid:0.7.3
python src/build.py --rebuild

# Daily use
python src/build.py                           # Update entire library
python src/build.py --collection "Current"    # Update specific collection
python src/kbq.py search "diabetes"           # Research
```

## Key Grobid and Post-Processing Insights (SUMMARY)

Based on extensive analysis of 1,000+ papers, here are the critical insights for optimal Grobid usage:

### Essential Grobid Parameters

**MUST USE** these parameters for optimal extraction:
```python
{
    'consolidateHeader': '1',      # CRITICAL: 91.4% abstract extraction (vs 60-75% without)
    'consolidateCitations': '1',    # Improves bibliographic data quality
    'includeRawCitations': '1',     # Preserves original reference strings
    'includeRawAffiliations': '1', # Helps distinguish abstract from author info
    'segmentSentences': '1',        # Better text boundary detection
}
```

### Post-Processing Requirements

**MUST IMPLEMENT** these post-processing steps:
1. **Case-insensitive section matching** - Critical for section detection
2. **Header normalization** - Remove numbering (e.g., "2. Methods" → "methods")
3. **Section aliasing** - Map variations (e.g., "methodology" → "methods")
4. **Content aggregation** - Combine multiple subsections of same type
5. **Enhanced abstract recovery** - Multiple strategies for 83.3% recovery rate:
   - Extract from Methods section (50% success)
   - Extract from Introduction (17% success)
   - Extract from title-named sections (17% success)
   - Synthesize from multiple sections (16% success)

### Paper Classification System

**MUST FILTER** papers before adding to knowledge base:
- **74.2%** are IMRaD papers → Add to KB
- **16.4%** are non-IMRaD but valid → Add to KB
- **9.3%** are rejected (non-research content) → DO NOT add to KB

### Performance Expectations

With optimal configuration:
- **Abstract extraction**: ~99.7% (98.6% from Grobid + enhanced recovery strategies)
  - 98.6% base extraction rate for IMRaD papers
  - 83.3% recovery rate for missing abstracts
  - Only ~0.3% of valid papers remain without abstracts
- **Processing time**: ~16 seconds per paper (worth the accuracy)
- **Rejection rate**: 9.3% of PDFs should be filtered out
- **Section detection**: Significant improvements with case-insensitive matching

### Key Implementation Files

- **post_process_with_classification.py**: Complete implementation
- **run_grobid_with_best_practices.py**: Optimal Grobid parameters
- **KB_FILTERING_IMPLEMENTATION.md**: Detailed filtering documentation

### Migration from v4.6

#### Command Changes

| v4.6 Command | v5.0 Equivalent | Notes |
|--------------|-----------------|-------|
| `python src/cli.py` | `python src/kbq.py` | All commands preserved, plus new entity filters |
| `python src/build_kb.py` | `python src/build.py` | Removed path config flags (now uses defaults) |
| `python src/analyze_gaps.py` | `python src/gaps.py` | Same functionality, shorter name |
| `python src/discover.py` | `python src/discover.py` | No change, all flags preserved |

#### Removed Flags (No Longer Needed)

**Configuration flags removed:**

- `--api-url`, `--knowledge-base-path`, `--zotero-data-dir` - Uses standard paths
- `--yes`, `--auto-start` - Always automatic, no prompts
- `--continue` - Auto-resumes from checkpoint if exists
- `--skip-quality` - Auto-fallback if S2 unavailable
- `--quiet`, `--verbose` - Single output mode optimized for manual runs
- `--phase`, `--features` - Always uses all available features

#### New v5.0 Build Flags

**Collection management (NEW):**

- `--collection NAME` - Process only papers from specified Zotero collection
  - Useful for testing on subset of papers
  - Faster builds when working on specific research areas
  - Collection name must match exactly (case-sensitive)
  - Can combine with other flags like `--rebuild` or `--no-gaps`

#### New v5.0 Features

- **Entity-aware search**: Filter by study type, sample size, p-values, software, datasets
- **Collection processing**: Build KB from specific Zotero collections with `--collection`
- **Automatic operation**: No prompts, auto-starts Grobid, resumes from checkpoint
- **Progress tracking**: `--estimate`, `--progress` for checking status
- **Batch operations preserved**: 10-20x performance boost still available

#### Migration Steps

```bash
# 1. Export current KB (optional backup)
python src/build.py --export kb_v4_backup.json

# 2. Clean slate for v5
rm -rf kb_data/

# 3. Rebuild with v5
git pull
python src/build.py --rebuild

```
