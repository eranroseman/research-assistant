# Research Assistant v5.0: Three-System Integration

## Table of Contents

1. [Overview](#overview)
2. [Breaking Changes](#breaking-changes)
3. [Installation & Setup](#installation--setup)
4. [Core Features](#core-features)
5. [System Architecture](#system-architecture)
6. [Usage Guide](#usage-guide)
7. [Implementation Plan](#implementation-plan)
8. [Technical Details](#technical-details)
9. [Success Metrics](#success-metrics)

---

## Overview

### What is v5.0?

Research Assistant v5.0 is a **critical bug fix and enhancement** that transforms the product from "might work" (50% extraction success) to "will work" (95% extraction success).

It integrates three powerful systems:

1. **Zotero** - Paper management and PDF storage
2. **Grobid** - AI-powered structure and entity extraction (95% accuracy)
3. **Semantic Scholar** - Citation metrics and AI-generated summaries

### Why v5.0?

The current v4.6 has a **product-breaking bug**:

- ❌ **50% extraction failure rate** - Current extraction cannot preserve document structure
- ❌ **Unreliable extraction** - Users cannot trust the output
- ❌ **No citation networks** - Can't find related papers
- ❌ **Limited discovery** - Only keyword matching

**This is not a feature addition - it's fixing a critical defect.**

### What v5.0 Delivers

- ✅ **95% extraction accuracy** with Grobid
- ✅ **50+ entity types extracted** (sample sizes, p-values, methods, datasets)
- ✅ **Multi-level embeddings** for intelligent search
- ✅ **Entity-aware discovery** (find papers using same methods/datasets)
- ✅ **Comprehensive gap analysis** (methodology, reproducibility, evidence levels)
- ✅ **Unattended operation** for long-running manual builds
- ✅ **Collection-based processing** for focused knowledge bases

---

## Breaking Changes

### ⚠️ IMPORTANT: Complete Rebuild Required

```bash
# One-time weekend investment for reliable extraction forever
rm -rf kb_data/              # Delete existing KB
git pull                      # Update to v5
pip install -r requirements.txt
python src/build.py --rebuild  # 4-6 hour rebuild (no prompts needed)

# Why rebuild? Because you're getting:
# • 50% → 95% extraction accuracy
# • Structured data that won't need re-extraction
# • Future-proof XML storage for new entity mining
```

### Why No Migration?

- Different extraction format (Grobid XML vs plain text)
- New entity storage structure
- Enhanced embedding system
- Incompatible quality scores

---

## Installation & Setup

### Prerequisites

```bash
# 1. Install Docker
sudo apt install docker.io  # Ubuntu/Debian
brew install docker         # macOS

# 2. Install Grobid (one-time)
docker pull lfoppiano/grobid:0.8.2
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.8.2

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Verify setup
python src/check_requirements.py  # Exit 0 = ready
```

### System Requirements

```yaml
Minimum Requirements:
  RAM: 4 GB (for <1000 papers)
  Disk: 10 GB free (for XML cache)
  CPU: 2 cores
  Network: 10 Mbps (for S2 API)

Recommended:
  RAM: 8 GB (for 2000+ papers)
  Disk: 20 GB free (includes all caches)
  CPU: 4+ cores
  Network: 50+ Mbps

Grobid Docker:
  RAM: 2 GB allocated to container
  Disk: 5 GB for Docker image

Data Usage:
  ~500 KB per paper (S2 API calls)
  ~100 KB per paper (XML cache)
  ~50 KB per paper (embeddings)

API Limits:
  S2 Unauthenticated: 1 request/second
  S2 Batch: 100 papers per request
  Grobid: No hard limits (local Docker)
```

### First Build - The Academic Workflow

```bash
# Friday afternoon workflow:
python src/build.py --estimate
# Output: "Estimated time: 4.5 hours (3.2 - 5.9 hours)"
#         "✓ Safe for long unattended run"

# Start before leaving:
python src/build.py --rebuild

# Monday morning: Complete KB with quality and gap reports ready

# What happens:
# 1. Checks all systems are running
# 2. Shows time estimate and starts
# 3. Fetches papers from Zotero
# 4. Extracts structure with Grobid (4-6 hours)
# 5. Logs progress to kb_data/build_progress.log
# 6. Enhances with Semantic Scholar data
# 7. Creates multi-level embeddings
# 8. Builds FAISS indices

# Check progress in the morning
cat kb_data/build_progress.txt
# Build Progress: 1847/2000 (92.4%)
# Current: paper_1847.pdf
# Elapsed: 4.2 hours
# ETA: 0.3 hours
# Rate: 440 papers/hour
```

---

## Core Features

### 1. Entity Extraction (50+ Types)

#### From Grobid

- **Methodology**: Sample sizes, p-values, confidence intervals, study types
- **Software & Data**: Software versions, datasets, data availability
- **Clinical**: Trial IDs, organisms, diseases
- **Institutional**: Funders, affiliations, countries

#### From Semantic Scholar

- **Impact**: Citation count, influential citations, citation velocity
- **AI Insights**: TLDR summaries, field classifications
- **Author Metrics**: H-index, author citation counts
- **Access**: Open access status, PDF availability

### 2. Multi-Level Embeddings

```python
# Each paper has 5 embedding types:
{
    'title_abstract': standard_embedding,   # Title + abstract only (Multi-QA MPNet, 768-dim)
    'enriched': entity_aware_embedding,     # Title + abstract + extracted entities (768-dim)
    's2_embedding': semantic_scholar_vector, # SPECTER2 from S2 API (768-dim, same input as title_abstract)
    'methods': methods_section_embedding,    # Methods section only (768-dim)
    'metadata_vector': numerical_features    # Sample size, citations, year, etc. (for filtering)
}
```

**Key insights about S2 embeddings:**

- **Same input as `title_abstract`**: Both use title + abstract text
- **Different training**: SPECTER2 trained on citation graph (papers that cite each other have similar embeddings)
- **Same dimensions**: 768-dim vectors, compatible with Multi-QA MPNet
- **API dependency**: Pre-computed by Semantic Scholar, not always available
- **Quality comparison opportunity**: Can A/B test local MPNet vs S2 SPECTER2

### 3. Entity-Aware Search

```bash
# Explicit entity filtering with flags
python src/kbq.py search "diabetes treatment" \
  --study-type RCT \
  --min-sample 500 \
  --software R

# Clear, unambiguous, scriptable
```

### 4. Smart Discovery

```bash
# External paper discovery via Semantic Scholar
python src/discover.py --keywords "machine learning, diabetes"

# Find papers with specific quality criteria
python src/discover.py --quality-threshold HIGH --year-from 2020

# v5.0 NEW: SPECTER2 similarity-based discovery
python src/discover.py --similar-to 0001,0002 --similarity-mode specter2

# v5.0 NEW: Discovery with quality explanations
python src/discover.py --keywords "AI diagnostics" --explain-quality
```

### 5. Comprehensive Analysis

```bash
# Paper details from KB
python src/kbq.py get 0001

# Generate citations
python src/kbq.py cite 0001 0002 0003

# Smart search for complex queries
python src/kbq.py smart-search "diabetes treatment" -k 30

# Gap analysis for missing literature
python src/gaps.py --min-citations 50 --year-from 2020

# v5.0 NEW: Entity gap analysis
python src/gaps.py --gap-type entity --show-missing software:Python

# v5.0 NEW: Reproducibility gap detection
python src/gaps.py --reproducibility-gaps --requires both --min-citations 100
```

---

## System Architecture

### Data Flow

```
1. ZOTERO (Source)
   ├── PDFs
   ├── Metadata
   └── Collections
        ↓
2. GROBID (Extraction)
   ├── Structured sections
   ├── References & citations
   ├── Entities & metadata
   └── Study characteristics
        ↓
3. SEMANTIC SCHOLAR (Enhancement)
   ├── Citation metrics
   ├── AI summaries (TLDR)
   ├── Author metrics
   └── Related papers
        ↓
4. KNOWLEDGE BASE
   ├── FAISS indices (search)
   ├── Entity database (filtering)
   └── Citation network (discovery)
```

### Storage Structure

```
kb_data/
├── papers/           # Markdown files with full content
├── cache/
│   ├── grobid/      # XML extractions
│   ├── s2/          # API responses
│   └── embeddings/  # Computed vectors
├── indices/
│   ├── semantic.faiss    # Main search (title_abstract + enriched + s2_embedding)
│   ├── methods.faiss     # Method similarity search
│   └── metadata.faiss    # Numerical filtering (not true embeddings)
└── metadata.json    # Complete paper data
```

---

## Usage Guide

### Daily Workflow

#### Morning: Check for New Papers

```bash
# Incremental update of entire library (fast, automatic, with reports)
python src/build.py

# Update only specific collection
python src/build.py --collection "Current Research"

# Skip gap analysis for faster completion
python src/build.py --no-gaps

# Update collection without gap analysis (fastest)
python src/build.py --collection "Weekly Papers" --no-gaps

# Quality scores are automatically upgraded when needed
```

#### Research Tasks

```bash
# Search with explicit entity filters
python src/kbq.py search "diabetes" \
  --min-sample 500 \
  --study-type RCT \
  --software Python \
  --year-from 2020 \
  --has-data

# Discover external papers
python src/discover.py --keywords "diabetes, insulin resistance" \
  --quality-threshold HIGH

# Analyze gaps in literature
python src/gaps.py --min-citations 50 --year-from 2020
```

#### Manual Build Processing

```bash
# Run manually when needed (e.g., weekly)
python /path/to/src/build.py --rebuild
```

### Command Reference

#### build.py - Knowledge Base Building

```bash
# Core operation flags (simplified)
--rebuild          # Full rebuild instead of incremental
--demo             # Test with 5 papers only
--export FILE      # Export KB and exit
--import FILE      # Import KB and exit
--no-gaps          # Skip gap analysis after build (saves 15-25 min)
--collection NAME  # Process only papers from specified Zotero collection

# Information flags (exit immediately)
--estimate         # Show time estimate and exit
--progress         # Show current build status and exit

# Automatic post-build reports (always generated):
# 1. PDF quality report → exports/analysis_pdf_quality.md (if issues found)
# 2. Gap analysis → exports/gap_analysis_*.md (unless --no-gaps)
# 3. Build summary with next steps

# Examples:
python src/build.py                           # Default: entire library + reports + gaps
python src/build.py --collection "PhD Thesis" # Only papers from PhD Thesis collection
python src/build.py --rebuild                 # Full rebuild + reports + gaps
python src/build.py --demo                    # Quick test (no gap analysis)
python src/build.py --no-gaps                 # Skip gap analysis only
python src/build.py --estimate                # Check timing before starting
python src/build.py --collection "ML Papers" --no-gaps  # Specific collection, no gaps
```

### Script Naming Convention Changes

**v5.0 introduces clearer, more intuitive script names:**

| Old Name (v4.6) | New Name (v5.0) | Purpose | Rationale |
|-----------------|-----------------|---------|-----------|
| `cli.py` | `kbq.py` | Knowledge Base Query | Reflects entity-aware search capabilities and database-like querying |
| `analyze_gaps.py` | `gaps.py` | Gap Analysis | Shorter, cleaner name for network-based gap discovery |
| `build_kb.py` | `build.py` | Build Knowledge Base | Simplified name, context is clear from directory |
| `discover.py` | `discover.py` | External Discovery | Kept unchanged - name is already clear and concise |

**Why these changes?**

- **Clarity**: Names directly indicate functionality (kbq = Knowledge Base Query)
- **Brevity**: Shorter names for frequent command-line usage
- **Consistency**: All scripts follow simple noun/verb pattern
- **v5.0 Signal**: New names indicate the major architectural upgrade

#### kbq.py - Knowledge Base Query Interface

```bash
# Search commands with explicit entity filters
search QUERY [--min-sample N] [--study-type TYPE] [--software NAME] 
             [--year-from YYYY] [--has-data] [--max-p-value N]
             [--exclude TERMS] [--include TERMS] [--journal NAME]
             [--group-by year|journal|type] [--export-csv FILE]
             [--show-quality] [--quality-min N]
smart-search QUERY [-k NUM]
author NAME [--exact]

# Retrieval commands
get ID [--sections abstract methods results] [--add-citation]
get-batch ID1 ID2 ID3 [--sections SECTIONS]
cite ID1 ID2 ID3

# Batch operations (10-20x faster for multiple commands)
batch --preset research|review|author-scan [ARGS]
batch --file commands.txt

# Analysis commands
info
diagnose
```

#### discover.py - External Discovery

```bash
# Basic discovery flags
--keywords "term1, term2"           # Search keywords (required)
--quality-threshold HIGH|MEDIUM|LOW # Quality filter (80+/60+/40+ scores)
--year-from YYYY                    # Recency filter (default: 2020)
--min-citations N                   # Minimum citation count
--limit N                           # Maximum results (default: 50)
--study-types TYPE1,TYPE2           # Filter by methodology (rct, cohort, etc.)
--author-filter "Name1,Name2"       # Focus on specific researchers (max 5)
--population-focus TYPE             # Target populations (pediatric|elderly|women|developing_countries)
--include-kb-papers                 # Include existing KB papers (default: exclude)
--coverage-info                     # Show database coverage guidance

# v5.0 NEW: SPECTER2-based similarity (High ROI)
--similar-to ID1,ID2,ID3            # Find papers similar to these KB papers
--similarity-mode specter2|mpnet    # Use S2 embeddings or local MPNet (default: specter2)
--min-similarity 0.7                # Minimum cosine similarity threshold

# v5.0 NEW: Quality transparency (High ROI)
--explain-quality                   # Include detailed quality breakdowns in output
--min-reproducibility 60            # Filter by reproducibility score (code/data availability)
```

#### gaps.py - Gap Analysis

**Default Behavior (no flags):**

```bash
python src/gaps.py
# Runs comprehensive gap analysis with:
# - min-citations: 10 (finds moderately cited papers)
# - year-from: all years (no filter) 
# - limit: unlimited (analyzes entire KB)
# - gap-type: all types (citation, author, cocited, recent, semantic)
# - Runtime: 15-25 minutes
# - Output: exports/gap_analysis_YYYYMMDD_HHMMSS.md
```

**Flag Effects:**

```bash
# Basic filtering flags (modify comprehensive analysis)
--min-citations N   # Default: 10. Higher = focus on high-impact gaps only
--year-from YYYY    # Default: all years. Set to focus on recent research
--limit N           # Default: unlimited. Set lower for faster analysis
--kb-path PATH      # Default: kb_data/. Custom KB location

# Gap type selection (changes analysis algorithm)
--gap-type TYPE     # Default: citation. Options:
                    # citation = papers cited by your collection (default)
                    # author = recent work by your KB authors
                    # entity = papers with same methods/tools/datasets
                    # reproducibility = papers with available code/data

# Entity gap analysis (requires --gap-type entity)
--show-missing SPEC # Example: software:Python,dataset:MIMIC
--entity-coverage   # Generate entity coverage statistics

# Reproducibility gaps (can combine with any gap-type)
--reproducibility-gaps        # Filter for reproducible papers only
--requires code|data|both     # What must be available
--min-reproducibility-score N # Default: 60 (0-100 scale)

# Sample size gaps (can combine with any gap-type)
--sample-size-gaps           # Filter for large studies only
--min-sample N               # Default: 1000 participants
--study-type RCT|cohort|all  # Default: all
```

**Common Usage Patterns:**

```bash
# Quick gaps check (5-10 min)
python src/gaps.py --limit 50 --min-citations 100

# Find reproducible research gaps
python src/gaps.py --reproducibility-gaps --requires both

# Find papers using same tools
python src/gaps.py --gap-type entity --show-missing software:TensorFlow

# Find large clinical trials
python src/gaps.py --sample-size-gaps --min-sample 5000 --study-type RCT

# Recent high-impact gaps only
python src/gaps.py --year-from 2023 --min-citations 50 --limit 100
```

**When Called Automatically by build.py:**

```python
# build.py calls gaps.py with default settings (no flags)
# unless --no-gaps is specified:
if not args.no_gaps:
    print("\n• Running gap analysis (15-25 minutes)...")
    subprocess.run(['python', 'src/gaps.py'])
    # Uses gaps.py defaults: comprehensive analysis of entire KB
    # Output: exports/gap_analysis_YYYYMMDD_HHMMSS.md
else:
    print("\n• Gap analysis skipped (--no-gaps flag)")
```

**Gap Analysis Behavior Summary:**

| Context | Default Behavior | Key Parameters | Runtime | Output |
|---------|-----------------|----------------|---------|--------|
| **Standalone** | Comprehensive analysis | All defaults (no limits) | 15-25 min | Complete gap report |
| **Via build.py** | Same comprehensive analysis | gaps.py defaults used | 15-25 min | Complete gap report |
| **With --no-gaps** | Skipped entirely | N/A | 0 min | None |
| **With --demo** | Skipped automatically | N/A | 0 min | None |
| **Quick check** | Use --limit 50 | Reduces to top gaps only | 5-10 min | Most important gaps |
| **Entity focus** | --gap-type entity | Requires --show-missing | 10-15 min | Method/tool gaps |
| **Reproducible** | --reproducibility-gaps | Can add --requires both | 15-20 min | Papers with code/data |

---

## Implementation Plan

### Phase 1: Core Deliverables (Week 1-4) - MUST SHIP

#### Week 1-2: Shared Libraries & Infrastructure

**Day 1-2: Core Library Setup**

- [ ] Create lib/ directory structure
- [ ] Implement extraction_common.py:
  - `GrobidClient` class with retry logic and connection pooling
  - `PDFExtractor` abstract base class for consistent interface
  - `ExtractionResult` dataclass for consistent return format
  - `ZoteroClient` class with collection filtering support
- [ ] Add collection filtering to Zotero API calls
- [ ] Write unit tests for extraction_common.py (target: 90% coverage)

**Day 3-4: Quality & Progress Libraries**  

- [ ] Implement quality_scoring.py:
  - Migrate scoring logic from build_kb.py and discover.py
  - `QualityScorer` class with configurable weights
  - Explanation generation for transparency
  - Cache integration for performance
- [ ] Implement progress_monitor.py:
  - `ProgressTracker` with checkpoint recovery
  - Time estimation based on rolling average
  - Memory-efficient state persistence (JSON, not pickle)
- [ ] Integration tests for library interactions

**Day 5-6: Entity & Embedding Libraries**

- [ ] Implement entity_database.py:
  - In-memory pandas DataFrame for entity storage (2K papers = ~2MB)
  - Fast entity filtering before semantic search  
  - Vectorized operations for efficient filtering
  - Note: SQLite unnecessary - manual builds mean no concurrent access needed
- [ ] Implement embeddings_common.py:
  - Multi-level embedding management
  - FAISS index abstraction
  - Consistent vector normalization
- [ ] Performance benchmarks (target: <100ms for 10K entity filter)

**Day 7-8: Report Generator & CLI Updates**

- [ ] Implement report_generator.py:
  - Migrate report logic from kbq.py, gaps.py, discover.py
  - Consistent markdown formatting
  - Export to multiple formats (MD, CSV, JSON)
- [ ] Update all scripts to use shared libraries:
  - Remove duplicate code (target: 60% reduction)
  - Add comprehensive error handling
  - Ensure backward compatibility
- [ ] Remove all user prompts from build.py
- [ ] Auto-start Grobid if not running
- [ ] Auto-resume from checkpoint if exists
- [ ] Auto-fallback if S2 API unavailable

#### Week 3-4: Core Extraction & Resilience

**Day 9-10: Grobid Integration**

- [ ] Integrate GrobidExtractor with extraction_common.py:
  - Docker container management (auto-start, health checks)
  - Batch processing with configurable chunk size
  - Note: No PyMuPDF fallback - if Grobid fails, mark paper as failed (better to know than have bad data)
- [ ] Implement extraction validation:
  - Section completeness checks
  - Quality thresholds (min 500 chars for methods)
  - Extraction success metrics logging
- [ ] End-to-end extraction tests with real PDFs

**Day 11-12: Basic Entity Extraction**

- [ ] Implement basic entity extraction:
  - Sample size detection (regex + NLP patterns)
  - Study type classification (RCT, cohort, etc.)
  - Statistical values (p-values, confidence intervals)
- [ ] Add entity validation and normalization:
  - Standardize sample size formats (N=100 → 100)
  - Validate statistical value ranges
  - Handle edge cases (ranges, approximations)
- [ ] Entity extraction accuracy tests (target: 85% precision)

**Day 13-14: Checkpoint & Resume System**

- [ ] Add ProcessingCheckpoint using progress_monitor.py:
  - Save state every 10 papers (configurable)
  - Atomic writes to prevent corruption
  - Resume from exact interruption point
- [ ] Implement robust error recovery:
  - Per-paper error isolation
  - Retry logic with exponential backoff
  - Detailed error reporting with actionable fixes
- [ ] Stress tests for interruption recovery

**Day 15-16: Integration & Polish**

- [ ] Full build.py integration:
  - All libraries working together
  - Memory optimization for 10K+ paper builds
  - Performance profiling and bottleneck fixes
- [ ] User experience improvements:
  - Clear progress indicators with ETA
  - Informative error messages
  - Success/failure summary with statistics
- [ ] Documentation and examples:
  - API documentation for all libraries
  - Migration guide from v4.6
  - Common troubleshooting solutions
- [ ] Auto-generate quality report after build
- [ ] Auto-run gap analysis (unless --no-gaps)

**Ship v5.0-core if Week 4 successful**

### Phase 2: Enhanced Features (Week 5-6) - SHIP IF READY

#### Core v5.0 Enhancements

- [ ] Advanced entity extraction (p-values, datasets, software)
- [ ] Semantic Scholar API integration with AdaptiveRateLimiter
- [ ] S2 batch processing (400x efficiency improvement)
- [ ] Multi-level embeddings (5 types per paper)
- [ ] Entity-based filtering with explicit flags
- [ ] Quality score transparency (explanations)

#### High-ROI Discovery Features (NEW)

- [ ] **SPECTER2 similarity search** (discover.py) - Use S2 embeddings for citation-aware discovery
- [ ] **Quality explanation generation** (discover.py) - Detailed breakdowns build user trust
- [ ] **Entity gap analysis** (gaps.py) - "What methods/tools am I missing?"
- [ ] **Reproducibility gap detection** (gaps.py) - Find papers with available code/data

**Decision Point**: If behind schedule, ship without Phase 2

### Phase 3: Discovery Features (Week 7-8) - DEFER IF NEEDED

- [ ] Smart discovery system
- [ ] Comprehensive gap analysis
- [ ] Reproducibility reports
- [ ] Methods matrix
- [ ] Citation network

**Can ship as v5.1 if not ready**

---

## Technical Details

### Shared Libraries Architecture

The v5.0 design introduces shared libraries to eliminate code duplication and ensure consistency across all scripts. These libraries provide 60-70% code reduction and single source of truth for critical logic.

#### Library Structure

```
src/
├── lib/                           # Shared libraries (NEW in v5.0)
│   ├── extraction_common.py      # Grobid/S2 API clients
│   ├── quality_scoring.py        # Unified quality scoring
│   ├── progress_monitor.py       # Progress tracking for long operations
│   ├── embeddings_common.py      # Multi-level embedding management
│   ├── entity_database.py        # Fast entity-based filtering
│   └── report_generator.py       # Consistent report generation
├── build.py                       # Uses 5/6 libraries
├── kbq.py                        # Uses 4/6 libraries
├── discover.py                   # Uses 3/6 libraries
└── gaps.py                       # Uses 4/6 libraries
```

#### Priority Implementation (Phase 1 - MUST HAVE)

##### 1. extraction_common.py

```python
class ZoteroClient:
    """Zotero API client with collection support"""
    def get_all_papers(self) -> List[Dict]
    def get_collection_papers(self, collection_name: str) -> List[Dict]
    def list_collections(self) -> List[str]
    def get_pdfs_for_papers(self, papers: List[Dict]) -> Dict[str, Path]

class GrobidClient:
    """Shared Grobid API client with retry logic"""
    def extract_full_text(self, pdf_path) -> Dict
    def extract_references(self, text) -> List[Dict]

class S2Client:
    """Semantic Scholar API with adaptive rate limiting"""
    def get_paper(self, doi) -> Dict
    def get_papers_batch(self, dois) -> List[Dict]
    
class EntityExtractor:
    """Extract entities from Grobid XML"""
    def extract_sample_sizes(self, xml) -> List[int]
    def extract_p_values(self, xml) -> List[float]
    def extract_software(self, xml) -> List[str]
```

**Impact**: Critical deduplication - used by build.py, kbq.py, gaps.py

##### 2. quality_scoring.py

```python
class QualityScorer:
    """Unified quality scoring with explanations"""
    def calculate_score(self, paper, grobid, s2) -> tuple[int, str]
    def get_reproducibility_score(self, paper) -> int
    def explain_score(self, score, components) -> str
```

**Impact**: Ensures consistent scoring across all tools

##### 3. progress_monitor.py

```python
class ProgressTracker:
    """Track long-running operations with checkpoints"""
    def __init__(self, total, log_dir='kb_data')
    def update(self, current, message='')
    def create_checkpoint(self) -> Dict
    def resume_from_checkpoint(self, checkpoint) -> None
```

**Impact**: Essential for 4-6 hour builds

#### Optional Libraries (Phase 2 - SHOULD HAVE)

- **embeddings_common.py**: Manage 5-level embeddings (implement with lazy loading)
- **entity_database.py**: pandas DataFrame for fast filtering (sufficient for manual KB builds)
- **report_generator.py**: Markdown/CSV/JSON export (keep simple)

### Simplified Build Philosophy

The v5.0 build process follows these principles:

1. **Always complete** - No prompts interrupt the build
2. **Always use latest** - All available features are used automatically
3. **Smart defaults** - Auto-starts Grobid, resumes checkpoints, handles failures
4. **Minimal flags** - Only 7 flags total, most users need none
5. **Helpful output** - Shows exactly what's happening, no verbose/quiet modes
6. **Actionable reports** - Generates quality and gap analysis automatically

### Default Behavior (Simplified)

```python
# v5.0 always uses all available features - no flags needed
def main():
    """Build with all available features, showing helpful progress."""
    
    # 1. Always ensure Grobid is running
    ensure_grobid_ready()
    
    # 2. Determine collection scope
    if args.collection:
        print(f"Processing collection: {args.collection}")
        papers = fetch_from_collection(args.collection)
    else:
        print("Processing entire library")
        papers = fetch_entire_library()
    
    # 3. Auto-detect operation mode
    if args.rebuild:
        # Create safety backup before rebuild
        auto_backup()
        rebuild_from_scratch(papers)
    else:
        # Check for checkpoint and resume if exists
        if checkpoint_exists():
            resume_from_checkpoint(papers)
        else:
            incremental_update(papers)
    
    # 3. Auto-handle quality upgrades
    if papers_need_quality_upgrade():
        upgrade_quality_scores()
    
    # 4. Use all v5.0 features automatically
    extract_with_grobid()        # Always
    extract_entities()           # Always
    get_s2_metrics()            # If available, fallback if not
    create_multi_embeddings()    # Always
    
    # 5. Generate PDF quality report (if issues found)
    missing_pdfs = [p for p in papers if not p.get('full_text')]
    small_pdfs = [p for p in papers if len(p.get('full_text', '')) < 5000]
    
    if missing_pdfs or small_pdfs:
        print("\n• Generating PDF quality report...")
        report_path = generate_pdf_quality_report(papers)
        print(f"✅ PDF quality report: {report_path}")
        if missing_pdfs:
            print(f"   - {len(missing_pdfs)} papers missing PDFs")
        if small_pdfs:
            print(f"   - {len(small_pdfs)} papers with small PDFs")
    else:
        print("\n✅ All papers have good PDF quality - no report needed")
    
    # 6. Run gap analysis (unless --no-gaps or --demo)
    if not args.no_gaps and not args.demo:
        print("\n• Running gap analysis (15-25 minutes)...")
        gap_count = run_gap_analysis()
        print(f"✅ Gap analysis complete: {gap_count} gaps identified")
    elif args.demo:
        print("\n• Gap analysis skipped for demo builds")
    else:
        print("\n• Gap analysis skipped (--no-gaps flag)")
    
    print("\n✨ Build complete! Next steps:")
    print("  • Review quality report: exports/kb_quality_analysis.pdf")
    if not args.no_gaps:
        print("  • Check gaps report for missing papers")
    print("  • Start searching: python src/kbq.py search 'your query'")
```

### Progress Visualization System

```python
class ProgressLogger:
    """Log progress for long-running build monitoring."""

    def __init__(self, log_dir='kb_data'):
        self.log_file = Path(log_dir) / 'build_progress.log'
        self.summary_file = Path(log_dir) / 'build_progress.txt'
        self.start_time = time.time()

    def log(self, current: int, total: int, paper_name: str, status: str = 'processing'):
        """Log progress with ETA calculation."""
        elapsed = time.time() - self.start_time
        rate = current / elapsed if elapsed > 0 else 0
        eta = (total - current) / rate if rate > 0 else 0

        # Detailed log entry (JSON lines for parsing)
        entry = {
            'timestamp': datetime.now().isoformat(),
            'current': current,
            'total': total,
            'paper': paper_name,
            'status': status,
            'elapsed_hours': elapsed / 3600,
            'eta_hours': eta / 3600,
            'papers_per_hour': rate * 3600
        }

        with open(self.log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

        # Human-readable summary (for morning check)
        with open(self.summary_file, 'w') as f:
            f.write(f"Build Progress: {current}/{total} ({current/total*100:.1f}%)\n")
            f.write(f"Current: {paper_name}\n")
            f.write(f"Elapsed: {elapsed/3600:.1f} hours\n")
            f.write(f"ETA: {eta/3600:.1f} hours\n")
            f.write(f"Rate: {rate*3600:.0f} papers/hour\n")

            if status != 'processing':
                f.write(f"Status: {status}\n")
```

### Collection Processing

```python
def fetch_from_collection(collection_name: str) -> List[Dict]:
    """Fetch papers from specific Zotero collection.
    
    Args:
        collection_name: Exact name of Zotero collection (case-sensitive)
    
    Returns:
        List of paper dictionaries from the collection
    
    Raises:
        ValueError: If collection doesn't exist
    """
    client = ZoteroClient()
    
    # List available collections for validation
    available = client.list_collections()
    if collection_name not in available:
        print(f"\n❌ Collection '{collection_name}' not found!")
        print("Available collections:")
        for name in available:
            print(f"  - {name}")
        raise ValueError(f"Collection '{collection_name}' does not exist")
    
    papers = client.get_collection_papers(collection_name)
    print(f"Found {len(papers)} papers in collection '{collection_name}'")
    return papers

def fetch_entire_library() -> List[Dict]:
    """Fetch all papers from Zotero library."""
    client = ZoteroClient()
    papers = client.get_all_papers()
    print(f"Found {len(papers)} papers in entire library")
    return papers
```

### Build Time Estimator

```python
class ResilientExtractor:
    """Extraction with automatic retry and failure tracking."""

    MAX_RETRIES = 3
    RETRY_DELAYS = [60, 120, 300]  # Exponential backoff

    def __init__(self):
        self.failed_papers = []
        self.retry_stats = {'total_retries': 0, 'successful_retries': 0}

    def extract_with_retry(self, pdf_path: Path) -> Optional[Dict]:
        """Extract with automatic retry on transient failures."""
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                result = self._extract(pdf_path)
                if attempt > 0:
                    self.retry_stats['successful_retries'] += 1
                return result

            except (requests.Timeout, requests.ConnectionError) as e:
                last_error = e
                self.retry_stats['total_retries'] += 1

                if attempt < self.MAX_RETRIES - 1:
                    delay = self.RETRY_DELAYS[attempt]
                    logger.warning(f"Attempt {attempt+1} failed for {pdf_path.name}")
                    logger.warning(f"Retrying in {delay} seconds...")
                    time.sleep(delay)
                else:
                    # Final attempt failed - track for manual review
                    self.failed_papers.append({
                        'path': str(pdf_path),
                        'error': str(e),
                        'attempts': self.MAX_RETRIES,
                        'timestamp': datetime.now().isoformat()
                    })

            except Exception as e:
                # Non-retryable error
                self.failed_papers.append({
                    'path': str(pdf_path),
                    'error': str(e),
                    'error_type': 'non_retryable',
                    'timestamp': datetime.now().isoformat()
                })
                return None

        return None

    def save_failed_papers(self):
        """Save failed papers list for manual review."""
        if self.failed_papers:
            failed_file = Path('kb_data/failed_papers.json')
            failed_file.write_text(json.dumps(self.failed_papers, indent=2))
            print(f"\n⚠️  {len(self.failed_papers)} papers failed extraction")
            print(f"See {failed_file} for details")

class BuildTimeEstimator:
    """Adaptive build time estimation."""

    def __init__(self):
        self.cache_file = Path('kb_data/.timing_cache.json')
        self.timings = self._load_timings()

    def _load_timings(self) -> Dict:
        """Load historical timing data."""
        if self.cache_file.exists():
            return json.loads(self.cache_file.read_text())

        # Default estimates (seconds)
        return {
            'grobid_per_page': 0.5,
            'pages_per_paper': 15,
            'embedding_per_paper': 2.0,
            's2_api_per_paper': 1.0,
            'basic_entity_extraction': 0.5,
            'advanced_entity_extraction': 1.0
        }

    def estimate(self, num_papers: int) -> Dict:
        """Estimate build time with confidence interval."""

        seconds = 0

        # Calculate based on enabled features
        if FEATURE_FLAGS['grobid_extraction']:
            pages = num_papers * self.timings['pages_per_paper']
            seconds += pages * self.timings['grobid_per_page']

        if FEATURE_FLAGS['basic_entities']:
            seconds += num_papers * self.timings['basic_entity_extraction']

        if FEATURE_FLAGS['advanced_entities']:
            seconds += num_papers * self.timings['advanced_entity_extraction']

        if FEATURE_FLAGS['semantic_scholar']:
            seconds += num_papers * self.timings['s2_api_per_paper']

        # Always need embeddings
        seconds += num_papers * self.timings['embedding_per_paper']

        # Add overhead and calculate range
        overhead = 1.2  # 20% overhead
        uncertainty = 0.3  # ±30%

        hours = (seconds * overhead) / 3600
        min_hours = hours * (1 - uncertainty)
        max_hours = hours * (1 + uncertainty)

        return {
            'estimated_hours': hours,
            'range': (min_hours, max_hours),
            'formatted': f"{hours:.1f} hours ({min_hours:.1f} - {max_hours:.1f} hours)",
            'unattended_safe': max_hours < 10,
            'weekend_needed': max_hours > 10
        }

    def update_from_run(self, actual_timings: Dict):
        """Update estimates based on actual performance."""
        alpha = 0.3  # Learning rate

        for key, value in actual_timings.items():
            if key in self.timings:
                # Exponential moving average
                self.timings[key] = (alpha * value +
                                   (1 - alpha) * self.timings[key])

        # Save updated timings
        self.cache_file.parent.mkdir(exist_ok=True)
        self.cache_file.write_text(json.dumps(self.timings, indent=2))
```

### Entity Extraction Implementation

```python
def extract_all_grobid_entities(xml: str) -> Dict:
    """Extract 50+ entity types from Grobid XML."""
    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}
    root = ET.fromstring(xml)

    return {
        # Methodology
        'sample_sizes': extract_sample_sizes(root, ns),
        'p_values': extract_p_values(root, ns),
        'confidence_intervals': extract_ci(root, ns),
        'study_type': detect_study_type(root, ns),

        # Software & Data
        'software': extract_software(root, ns),
        'datasets': extract_datasets(root, ns),
        'data_availability': extract_data_availability(root, ns),

        # Quality indicators
        'figures_count': len(root.findall('.//tei:figure', ns)),
        'tables_count': len(root.findall('.//tei:table', ns)),
        'references_count': len(root.findall('.//tei:ref[@type="bibr"]', ns)),
    }
```

### Semantic Scholar API Integration

```python
class AdaptiveRateLimiter:
    """Adaptive rate limiting for S2 API compliance."""

    def __init__(self):
        self.delay = 0.1  # Start optimistic
        self.min_delay = 0.1
        self.max_delay = 5.0
        self.success_count = 0
        self.rate_limit_count = 0

    def wait(self):
        """Wait before next API call."""
        time.sleep(self.delay)

    def on_success(self):
        """Speed up after sustained success."""
        self.success_count += 1
        if self.success_count > 10:
            # Gradually speed up
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
            logger.debug(f"Rate limiter: speeding up to {self.delay:.2f}s")

    def on_rate_limit(self):
        """Slow down on rate limit error."""
        self.rate_limit_count += 1
        self.delay = min(self.max_delay, self.delay * 2.0)
        self.success_count = 0
        logger.warning(f"Rate limited ({self.rate_limit_count} times). ")
        logger.warning(f"Slowing to {self.delay:.2f}s delay")

    def get_stats(self) -> Dict:
        """Get rate limiting statistics."""
        return {
            'current_delay': self.delay,
            'rate_limit_hits': self.rate_limit_count,
            'optimal': self.delay <= self.min_delay * 1.5
        }

class S2Client:
    """Semantic Scholar client with resilient API handling."""

    def __init__(self, api_key: Optional[str] = None):
        self.rate_limiter = AdaptiveRateLimiter()
        self.session = requests.Session()
        if api_key:
            self.session.headers['x-api-key'] = api_key

    def get_paper(self, doi: str, max_retries: int = 3) -> Optional[Dict]:
        """Get paper with adaptive rate limiting.
        
        Requests 'embedding' field to get SPECTER2 vectors.
        """
        fields = "paperId,title,abstract,embedding,citationCount,influentialCitationCount"
        
        for attempt in range(max_retries):
            self.rate_limiter.wait()

            try:
                response = self.session.get(
                    f"https://api.semanticscholar.org/graph/v1/paper/{doi}",
                    params={"fields": fields},
                    timeout=10
                )

                if response.status_code == 200:
                    self.rate_limiter.on_success()
                    data = response.json()
                    # Note: 'embedding' field may be None for some papers
                    return data
                elif response.status_code == 429:
                    self.rate_limiter.on_rate_limit()
                    if attempt < max_retries - 1:
                        time.sleep(10 * (attempt + 1))  # Additional backoff
                elif response.status_code == 404:
                    return None  # Paper not found

            except requests.RequestException as e:
                logger.error(f"S2 API error: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)

        return None
```

### Advanced Grobid Endpoints (Phase 2-3)

```python
# For Phase 2: Enhanced Reference Processing
def fix_broken_references(reference_text: str) -> List[Dict]:
    """Use processReferences endpoint for malformed citations."""
    response = requests.post(
        "http://localhost:8070/api/processReferences",
        data={'references': reference_text},
        timeout=5
    )
    return parse_reference_xml(response.text)

# For Phase 2: Fast Metadata Screening
def quick_metadata_only(pdf_path: Path) -> Dict:
    """10x faster - just header extraction for pre-filtering."""
    with open(pdf_path, 'rb') as f:
        response = requests.post(
            "http://localhost:8070/api/processHeaderDocument",
            files={'input': f},
            timeout=5
        )
    # Returns: title, abstract, authors, affiliations
    return parse_header_xml(response.text)

# For Phase 3: Batch Citation Processing
def batch_process_citations(citations: List[str]) -> List[Dict]:
    """Process 100+ citations in one API call."""
    response = requests.post(
        "http://localhost:8070/api/processCitationList",
        data={'citations': '\n'.join(citations)},
        timeout=10
    )
    return parse_citations_xml(response.text)
```

### Enhanced Quality Score with Transparency

```python
def calculate_v5_quality_score(paper, grobid, s2) -> tuple[int, Dict]:
    """Comprehensive quality score with detailed explanation."""

    score = 0
    components = {}

    # Impact (30 points)
    citations = s2.get('citation_count', 0)
    impact_score = 0
    if citations > 100:
        impact_score = 15
        impact_reason = f"{citations} citations (high impact)"
    elif citations > 50:
        impact_score = 12
        impact_reason = f"{citations} citations (moderate impact)"
    elif citations > 10:
        impact_score = 8
        impact_reason = f"{citations} citations (some impact)"
    else:
        impact_reason = f"{citations} citations (limited impact)"

    score += impact_score
    components['impact'] = {
        'earned': impact_score,
        'max': 15,
        'reason': impact_reason
    }

    # Methodology (25 points)
    sample_size = get_max_sample_size(grobid)
    if sample_size > 1000: score += 10
    if 'rct' in grobid.get('study_type', '').lower(): score += 9
    if grobid.get('p_values'): score += 3

    # Reproducibility (20 points)
    if grobid.get('data_availability', {}).get('data_url'): score += 8
    if grobid.get('code_url'): score += 7
    if s2.get('is_open_access'): score += 5

    # Author credibility (10 points)
    max_h = s2.get('max_h_index', 0)
    if max_h > 50: score += 10
    elif max_h > 20: score += 7

    # Completeness (10 points)
    completeness = grobid.get('extraction_completeness', 0)
    score += int(completeness * 10)

    # Recency (5 points)
    if paper.get('year', 0) >= 2023: score += 5

    final_score = min(score, 100)

    # Generate explanation
    explanation = generate_score_explanation(final_score, components)

    return final_score, explanation

def generate_score_explanation(score: int, components: Dict) -> str:
    """Generate human-readable score explanation."""

    explanation = [f"Quality Score: {score}/100\n"]
    explanation.append("Breakdown:")

    for component, details in components.items():
        pct = (details['earned'] / details['max'] * 100) if details['max'] > 0 else 0
        explanation.append(
            f"• {component.title()}: {details['earned']}/{details['max']} "
            f"({pct:.0f}%) - {details['reason']}"
        )

    # Add grade
    if score >= 85:
        grade = "A+ (Excellent)"
    elif score >= 70:
        grade = "A (Good)"
    elif score >= 60:
        grade = "B (Above Average)"
    elif score >= 45:
        grade = "C (Average)"
    else:
        grade = "D (Below Average)"

    explanation.append(f"\nOverall Grade: {grade}")

    return '\n'.join(explanation)
```

### Entity-Aware Search

```python
class EntityAwareSearch:
    def search(self, 
               query: str,
               study_type: Optional[str] = None,
               min_sample: Optional[int] = None,
               max_p_value: Optional[float] = None,
               software: Optional[List[str]] = None,
               datasets: Optional[List[str]] = None,
               year_from: Optional[int] = None) -> List[Dict]:
        """
        Search with explicit entity filters.
        
        Args:
            query: Semantic search text
            study_type: Filter by study type (RCT, cohort, etc.)
            min_sample: Minimum sample size
            max_p_value: Maximum p-value threshold
            software: List of required software
            datasets: List of required datasets
            year_from: Papers published after this year
        """
        # Stage 1: Entity database filtering (fast)
        filtered_ids = self.entity_db.filter(
            study_type=study_type,
            min_sample=min_sample,
            max_p_value=max_p_value,
            software=software,
            datasets=datasets,
            year_from=year_from
        )
        
        # Stage 2: Multi-factor scoring on filtered papers
        papers = self.load_papers(filtered_ids)
        
        for paper in papers:
            score = 0
            
            # Semantic similarity (can use multiple embeddings)
            if paper.get('s2_embedding'):
                # Prefer S2 if available (trained on citations)
                score += compute_similarity(query, paper['s2_embedding']) * 0.4
            else:
                # Fallback to local MPNet
                score += compute_similarity(query, paper['title_abstract']) * 0.4
            
            score += self.compute_entity_relevance(paper) * 0.3
            score += paper['quality_score'] * 0.2
            score += self.compute_recency_score(paper['year']) * 0.1
            
            paper['search_score'] = score

        return sorted(papers, key=lambda p: p['search_score'], reverse=True)
```

---

## Success Metrics

### Phase 1: Core Release Criteria (v5.0-core)

- ✅ **95% extraction accuracy** (THE critical fix)
- ✅ Unattended operation (no prompts)
- ✅ Checkpoint/resume works
- ✅ Progress visualization for morning check
- ✅ Build time estimation
- ✅ Basic entity extraction (bonus value)

**MUST ship after Week 4 - this fixes the product**

### Phase 2: Enhanced Release (v5.0-full)

- ✅ All Phase 1 features
- ✅ Advanced entity extraction
- ✅ Semantic Scholar integration
- ✅ Multi-level embeddings
- ✅ Entity-based filtering

**Ship if ready by Week 6**

### Target Metrics by Phase

#### Phase 1 (Core)

- 📊 Extraction accuracy: >95%
- 📊 Sample sizes extracted: >30% of papers
- 📊 Study types classified: >50% of papers
- 📊 Processing time: <8 hours for 2000 papers
- 📊 Progress logging: 100% coverage

#### Phase 2 (Enhanced)

- 📊 Sample sizes extracted: >50% of papers
- 📊 Study types classified: >70% of papers
- 📊 Processing time: 4-6 hours for 2000 papers
- 📊 Memory usage: <3GB peak

#### Phase 3 (Deferred to v5.1 if needed)

- ❌ Smart discovery system
- ❌ Citation network analysis
- ❌ Comprehensive gap analysis
- ❌ Methods matrix visualization

---

## Exit Codes & Automation

```bash
# Exit codes for scripts/automation
0 = Success
1 = Requirements not met (Docker/Zotero not available)
2 = Extraction failed (Grobid errors, PDF issues)

# Example manual weekly rebuild
python src/build.py --rebuild

# For automation (cron/systemd), output is automatically logged
# The script always completes without prompts
```

---

## Monitoring & Reliability

### Automatic Report Generation

At the end of every build, v5.0 automatically generates comprehensive reports:

1. **PDF Quality Report** (when issues detected)
   - Papers missing PDFs entirely
   - Papers with small PDFs (<5KB extracted text)
   - Papers without DOIs (limited to basic quality scoring)
   - Books and proceedings (tracked separately in Phase 3)
   - Actionable recommendations for fixing issues
   - Location: `exports/analysis_pdf_quality.md`
   - Only generated if PDF issues are found

2. **Gap Analysis Report** (default, skip with `--no-gaps`)
   - Comprehensive citation network analysis
   - Papers cited by your KB but missing from collection
   - Recent work from authors in your KB
   - Papers frequently co-cited with your collection
   - Recent developments in your research areas
   - Semantically similar papers you don't have
   - Takes 15-25 minutes to run
   - Location: `exports/gap_analysis_YYYYMMDD_HHMMSS.md`
   - Skipped for demo builds or with `--no-gaps` flag

```python
def generate_pdf_quality_report(papers: List[Dict]) -> Path:
    """Generate PDF quality report for papers with issues.
    
    Comprehensive report covering:
    - Missing PDFs (no full text extracted)
    - Small PDFs (<5KB text, likely supplementary material)
    - Papers without DOIs (limited to basic quality scoring)
    - Recommendations for fixing each issue type
    
    Returns path to generated markdown report.
    """
    from pathlib import Path
    from datetime import datetime, UTC
    
    # Categorize papers by PDF status
    missing_pdfs = [p for p in papers if not p.get('full_text')]
    small_pdfs = [p for p in papers if 0 < len(p.get('full_text', '')) < 5000]
    no_doi_papers = [p for p in papers if not p.get('doi')]
    books_and_proceedings = [p for p in papers if p.get('item_type') in ['book', 'bookSection']]
    good_pdfs = [p for p in papers if len(p.get('full_text', '')) >= 5000]
    
    # Build comprehensive report
    report = []
    report.append("# PDF Quality Report\n")
    report.append(f"Generated: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M UTC')}\n")
    
    # Summary statistics
    total = len(papers)
    report.append("## Summary Statistics\n")
    report.append(f"- Total papers: {total:,}")
    report.append(f"- Papers with good PDFs: {len(good_pdfs):,} ({len(good_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers missing PDFs: {len(missing_pdfs):,} ({len(missing_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers with small PDFs: {len(small_pdfs):,} ({len(small_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers without DOIs: {len(no_doi_papers):,} ({len(no_doi_papers)*100/total:.1f}%)")
    if books_and_proceedings:
        report.append(f"- Books/Proceedings: {len(books_and_proceedings):,} ({len(books_and_proceedings)*100/total:.1f}%)")
    
    # Detailed sections for each issue type
    # ... (additional report generation logic)
    
    # Save report
    exports_dir = Path('exports')
    exports_dir.mkdir(exist_ok=True)
    report_path = exports_dir / 'analysis_pdf_quality.md'
    
    with report_path.open('w') as f:
        f.write('\n'.join(report))
    
    return report_path

def run_gap_analysis() -> int:
    """Run comprehensive gap analysis using gaps.py defaults."""
    import subprocess
    
    # Run gaps.py with NO flags (uses comprehensive defaults)
    print("  Analyzing citation networks...")
    print("  Finding missing papers from your research areas...")
    
    result = subprocess.run(
        ['python', 'src/gaps.py'],  # No flags = comprehensive analysis
        capture_output=True,
        text=True
    )
    
    if result.returncode == 0:
        # gaps.py saves its own timestamped report
        # Just parse and return count for summary
        gap_count = parse_gap_count(result.stdout)
        return gap_count
    else:
        print("  Warning: Gap analysis encountered issues")
        return 0
```

### Build Monitoring

```python
class BuildMonitor:
    """Monitor build progress and health."""

    def __init__(self, log_dir: Path = Path('kb_data/monitoring')):
        self.log_dir = log_dir
        self.log_dir.mkdir(exist_ok=True)
        self.metrics_file = self.log_dir / 'build_metrics.jsonl'
        self.start_time = time.time()
        self.papers_processed = 0
        self.failures = []

    def record_paper(self, paper_id: str, success: bool, duration: float):
        """Record per-paper metrics."""
        self.papers_processed += 1

        metric = {
            'timestamp': datetime.now().isoformat(),
            'paper_id': paper_id,
            'success': success,
            'duration_seconds': duration,
            'total_processed': self.papers_processed,
            'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024
        }

        with open(self.metrics_file, 'a') as f:
            f.write(json.dumps(metric) + '\n')

        if not success:
            self.failures.append(paper_id)

    def generate_summary(self) -> str:
        """Generate build summary for morning review."""
        elapsed = time.time() - self.start_time
        success_rate = (self.papers_processed - len(self.failures)) / self.papers_processed * 100

        summary = [
            "="*50,
            "BUILD SUMMARY",
            "="*50,
            f"Total Papers: {self.papers_processed}",
            f"Successful: {self.papers_processed - len(self.failures)}",
            f"Failed: {len(self.failures)}",
            f"Success Rate: {success_rate:.1f}%",
            f"Total Time: {elapsed/3600:.1f} hours",
            f"Avg Time/Paper: {elapsed/self.papers_processed:.1f} seconds",
        ]

        if self.failures:
            summary.append(f"\n⚠️ Failed papers saved to: kb_data/failed_papers.json")

        return '\n'.join(summary)

    def check_health(self) -> Dict:
        """Check system health during build."""
        process = psutil.Process()

        return {
            'memory_usage_mb': process.memory_info().rss / 1024 / 1024,
            'memory_percent': process.memory_percent(),
            'cpu_percent': process.cpu_percent(),
            'disk_free_gb': psutil.disk_usage('/').free / 1024**3,
            'healthy': process.memory_percent() < 80
        }
```

### Error Recovery Strategy

```python
# Configuration for resilient processing
ERROR_RECOVERY_CONFIG = {
    'max_retries': 3,
    'retry_delays': [60, 120, 300],  # seconds
    'retryable_errors': [
        requests.Timeout,
        requests.ConnectionError,
        'HTTP 502',
        'HTTP 503',
        'HTTP 429'  # Rate limit
    ],
    'checkpoint_frequency': 50,  # Save every N papers
    'health_check_frequency': 100,  # Check resources every N papers
}

# Automatic feature handling with graceful degradation
def get_quality_score(paper, grobid, s2=None):
    """Always try best available scoring."""
    if s2 and s2_api_available():
        return calculate_v5_quality_score(paper, grobid, s2)
    else:
        logger.info("S2 unavailable, using basic scoring")
        return calculate_basic_quality_score(paper)
```

---

## Philosophy

### Core Principles

1. **Fix the bug first** - 95% extraction accuracy is the primary goal
2. **Manual processing model** - Designed for manual builds, not automated cron jobs
3. **Fail clearly** - No silent degradation, clear error messages
4. **Automatic operation** - Completes without prompts when manually triggered
5. **Progressive enhancement** - Core fix ships first, features can wait
6. **Data integrity** - Better to fail than corrupt data
7. **Flexible scope** - Process entire library or specific collections as needed

### Design Rationale

#### Why No SQLite Backend?

The entity database uses in-memory pandas DataFrames instead of SQLite because:

1. **Manual builds only** - No concurrent access or multi-user scenarios
2. **Small scale** - 2K papers = ~2MB in memory (trivial for modern systems)
3. **Simpler architecture** - No database files, connections, or schema migrations
4. **Faster development** - pandas vectorized operations are sufficient
5. **No persistence needed** - Entities rebuilt from source during each build

SQLite would add complexity without benefits for a single-user, manually-triggered system.

#### Why No PyMuPDF Fallback?

When Grobid extraction fails, we mark the paper as failed rather than falling back to PyMuPDF:

1. **Data quality over quantity** - PyMuPDF has 50% failure rate on structure preservation
2. **Clear failure signals** - Users need to know which papers failed extraction
3. **Prevent silent degradation** - Bad extractions pollute search results
4. **Debugging clarity** - Easier to diagnose why specific papers failed
5. **Future improvement path** - Failed papers can be re-processed when Grobid improves

Better to have 95% of papers extracted perfectly than 100% with half being unreliable.

### Phased Development Approach

#### Phase 1 (Week 1-4): Ship Core Fix

- Week 1-2: **Infrastructure** (unattended operation, progress logging)
- Week 3-4: **Core extraction** (Grobid integration, basic entities)
- **Ship Point**: If successful, release v5.0-core

#### Phase 2 (Week 5-6): Add Value

- Week 5: **Enhanced extraction** (advanced entities, S2 integration)
- Week 6: **Smart search** (multi-embeddings, entity-aware search)
- **Ship Point**: If successful, release v5.0-full

#### Phase 3 (Week 7-8): Advanced Features

- Week 7: **Discovery** (if time permits)
- Week 8: **Polish** (documentation, optimization)
- **Defer Point**: Move to v5.1 if behind schedule

### What Success Looks Like

```
Week 3: "Extraction ACTUALLY WORKS! No more mangled methods sections!"
Week 4: "I can finally TRUST the extraction! Plus sample sizes!"
Week 5: "Citation data included? This is getting good!"
Week 6: "Entity-aware search? Now we're talking!"
Week 7: "It found papers using the same datasets? Amazing!"
Week 8: "From 'unreliable' to 'indispensable' - shipped!"
```

### User Value Proposition

```
BEFORE v5.0: "I hope it extracts my papers correctly" (50% fail)
AFTER v5.0:  "I trust it to extract my papers correctly" (95% work)

One weekend investment = Reliable extraction forever
```

## Quick Start Guide

### For Developers

```bash
# 1. Setup environment
git checkout -b v5-implementation
pip install -r requirements-dev.txt

# 2. Start Grobid
docker-compose up -d grobid

# 3. Run tests
pytest tests/unit/test_grobid_extraction.py

# 4. Test extraction on sample
python src/build.py --demo
```

### For Users

```bash
# Weekend setup (one-time)
docker pull grobid/grobid:0.7.3
python src/build.py --rebuild

# Daily use
python src/build.py                           # Update entire library
python src/build.py --collection "Current"    # Update specific collection
python src/kbq.py search "diabetes"           # Research
```

### Migration from v4.6

#### Command Changes

| v4.6 Command | v5.0 Equivalent | Notes |
|--------------|-----------------|-------|
| `python src/cli.py` | `python src/kbq.py` | All commands preserved, plus new entity filters |
| `python src/build_kb.py` | `python src/build.py` | Removed path config flags (now uses defaults) |
| `python src/analyze_gaps.py` | `python src/gaps.py` | Same functionality, shorter name |
| `python src/discover.py` | `python src/discover.py` | No change, all flags preserved |

#### Removed Flags (No Longer Needed)

**Configuration flags removed:**

- `--api-url`, `--knowledge-base-path`, `--zotero-data-dir` - Uses standard paths
- `--yes`, `--auto-start` - Always automatic, no prompts
- `--continue` - Auto-resumes from checkpoint if exists
- `--skip-quality` - Auto-fallback if S2 unavailable
- `--quiet`, `--verbose` - Single output mode optimized for manual runs
- `--phase`, `--features` - Always uses all available features

#### New v5.0 Build Flags

**Collection management (NEW):**

- `--collection NAME` - Process only papers from specified Zotero collection
  - Useful for testing on subset of papers
  - Faster builds when working on specific research areas
  - Collection name must match exactly (case-sensitive)
  - Can combine with other flags like `--rebuild` or `--no-gaps`

#### New v5.0 Features

- **Entity-aware search**: Filter by study type, sample size, p-values, software, datasets
- **Collection processing**: Build KB from specific Zotero collections with `--collection`
- **Automatic operation**: No prompts, auto-starts Grobid, resumes from checkpoint
- **Progress tracking**: `--estimate`, `--progress` for checking status
- **Batch operations preserved**: 10-20x performance boost still available

#### Migration Steps

```bash
# 1. Export current KB (optional backup)
python src/build.py --export kb_v4_backup.json

# 2. Clean slate for v5
rm -rf kb_data/

# 3. Rebuild with v5
git pull
python src/build.py --rebuild

```
