# Research Assistant v5.0: Three-System Integration

## Table of Contents
1. [Overview](#overview)
2. [Breaking Changes](#breaking-changes)
3. [Installation & Setup](#installation--setup)
4. [Core Features](#core-features)
5. [System Architecture](#system-architecture)
6. [Usage Guide](#usage-guide)
7. [Implementation Plan](#implementation-plan)
8. [Technical Details](#technical-details)
9. [Success Metrics](#success-metrics)

---

## Overview

### What is v5.0?
Research Assistant v5.0 is a **critical bug fix and enhancement** that transforms the product from "might work" (50% extraction success) to "will work" (95% extraction success).

It integrates three powerful systems:

1. **Zotero** - Paper management and PDF storage
2. **Grobid** - AI-powered structure and entity extraction (95% accuracy)
3. **Semantic Scholar** - Citation metrics and AI-generated summaries

### Why v5.0?
The current v4.6 has a **product-breaking bug**:
- ❌ **50% extraction failure rate** - PyMuPDF fundamentally cannot preserve document structure
- ❌ **Unreliable extraction** - Users cannot trust the output
- ❌ **No citation networks** - Can't find related papers
- ❌ **Limited discovery** - Only keyword matching

**This is not a feature addition - it's fixing a critical defect.**

### What v5.0 Delivers
- ✅ **95% extraction accuracy** with Grobid
- ✅ **50+ entity types extracted** (sample sizes, p-values, methods, datasets)
- ✅ **Multi-level embeddings** for intelligent search
- ✅ **Entity-aware discovery** (find papers using same methods/datasets)
- ✅ **Comprehensive gap analysis** (methodology, reproducibility, evidence levels)
- ✅ **Unattended operation** for overnight processing

---

## Breaking Changes

### ⚠️ IMPORTANT: Complete Rebuild Required
```bash
# One-time weekend investment for reliable extraction forever
rm -rf kb_data/              # Delete existing KB
git pull                      # Update to v5
pip install -r requirements.txt
python src/build_kb.py --rebuild --yes --auto-start  # 4-6 hour rebuild

# Why rebuild? Because you're getting:
# • 50% → 95% extraction accuracy
# • Structured data that won't need re-extraction
# • Future-proof XML storage for new entity mining
```

### Why No Migration?
- Different extraction format (Grobid XML vs plain text)
- New entity storage structure
- Enhanced embedding system
- Incompatible quality scores

---

## Installation & Setup

### Prerequisites
```bash
# 1. Install Docker
sudo apt install docker.io  # Ubuntu/Debian
brew install docker         # macOS

# 2. Install Grobid (one-time)
docker pull grobid/grobid:0.7.3
docker run -d \
  -p 8070:8070 \
  --name grobid \
  --restart unless-stopped \
  --mem="2g" \
  grobid/grobid:0.7.3

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Verify setup
python src/check_requirements.py  # Exit 0 = ready
```

### System Requirements
```yaml
Minimum Requirements:
  RAM: 4 GB (for <1000 papers)
  Disk: 10 GB free (for XML cache)
  CPU: 2 cores
  Network: 10 Mbps (for S2 API)

Recommended:
  RAM: 8 GB (for 2000+ papers)
  Disk: 20 GB free (includes all caches)
  CPU: 4+ cores
  Network: 50+ Mbps

Grobid Docker:
  RAM: 2 GB allocated to container
  Disk: 5 GB for Docker image

Data Usage:
  ~500 KB per paper (S2 API calls)
  ~100 KB per paper (XML cache)
  ~50 KB per paper (embeddings)

API Limits:
  S2 Unauthenticated: 1 request/second
  S2 Batch: 100 papers per request
  Grobid: No hard limits (local Docker)
```

### First Build - The Academic Workflow
```bash
# Friday afternoon workflow:
python src/build_kb.py --estimate
# Output: "Estimated time: 4.5 hours (3.2 - 5.9 hours)"
#         "✓ Safe for overnight run"

# Start before leaving:
python src/build_kb.py --rebuild --yes --auto-start

# Monday morning: Complete KB ready for research

# What happens:
# 1. Checks all systems are running
# 2. Shows time estimate and starts
# 3. Fetches papers from Zotero
# 4. Extracts structure with Grobid (4-6 hours)
# 5. Logs progress to kb_data/build_progress.log
# 6. Enhances with Semantic Scholar data
# 7. Creates multi-level embeddings
# 8. Builds FAISS indices

# Check progress in the morning
cat kb_data/build_progress.txt
# Build Progress: 1847/2000 (92.4%)
# Current: paper_1847.pdf
# Elapsed: 4.2 hours
# ETA: 0.3 hours
# Rate: 440 papers/hour
```

---

## Core Features

### 1. Entity Extraction (50+ Types)

#### From Grobid
- **Methodology**: Sample sizes, p-values, confidence intervals, study types
- **Software & Data**: Software versions, datasets, data availability
- **Clinical**: Trial IDs, organisms, diseases
- **Institutional**: Funders, affiliations, countries

#### From Semantic Scholar
- **Impact**: Citation count, influential citations, citation velocity
- **AI Insights**: TLDR summaries, field classifications
- **Author Metrics**: H-index, author citation counts
- **Access**: Open access status, PDF availability

### 2. Multi-Level Embeddings

```python
# Each paper has 5 embedding types:
{
    'full_text': standard_embedding,        # Title + abstract
    'enriched': entity_aware_embedding,     # Includes all entities
    's2_embedding': semantic_scholar_vector, # Pre-computed by S2
    'methods': methods_section_embedding,    # For methodology similarity
    'metadata_vector': numerical_features    # For filtering
}
```

### 3. Entity-Aware Search

```bash
# Understands research requirements
python src/research.py search "RCT with n>500 using R"

# Parsed as:
# - Study type: RCT
# - Minimum sample: 500
# - Software: R
```

### 4. Smart Discovery

```bash
# Find papers using your methods
python src/research.py discover --similar-methods

# Find potential replications
python src/research.py discover --same-datasets

# Fill methodology gaps
python src/research.py discover --fill-gaps
```

### 5. Comprehensive Analysis

```bash
# Paper dashboard with all metrics
python src/research.py paper-dashboard 0001

# Reproducibility assessment
python src/research.py reproducibility-report

# Methods matrix (papers × tools)
python src/research.py methods-matrix

# Multi-dimensional gap analysis
python src/research.py gaps --comprehensive
```

---

## System Architecture

### Data Flow
```
1. ZOTERO (Source)
   ├── PDFs
   ├── Metadata
   └── Collections
        ↓
2. GROBID (Extraction)
   ├── Structured sections
   ├── References & citations
   ├── Entities & metadata
   └── Study characteristics
        ↓
3. SEMANTIC SCHOLAR (Enhancement)
   ├── Citation metrics
   ├── AI summaries (TLDR)
   ├── Author metrics
   └── Related papers
        ↓
4. KNOWLEDGE BASE
   ├── FAISS indices (search)
   ├── Entity database (filtering)
   └── Citation network (discovery)
```

### Storage Structure
```
kb_data/
├── papers/           # Markdown files with full content
├── cache/
│   ├── grobid/      # XML extractions
│   ├── s2/          # API responses
│   └── embeddings/  # Computed vectors
├── indices/
│   ├── semantic.faiss    # Main search
│   ├── methods.faiss     # Method similarity
│   └── metadata.faiss    # Entity filtering
└── metadata.json    # Complete paper data
```

---

## Usage Guide

### Daily Workflow

#### Morning: Check for New Papers
```bash
# Incremental update (fast, no prompts)
python src/build_kb.py

# If quality scores need upgrading
python src/build_kb.py --quality-upgrade --yes
```

#### Research Tasks
```bash
# Search with filters
python src/research.py search "diabetes" \
  --min-sample 500 \
  --study-type RCT \
  --has-data

# Discover related work
python src/research.py discover \
  --similar-methods \
  --min-quality 80

# Analyze gaps
python src/research.py gaps --comprehensive
```

#### Overnight Processing
```bash
# Add to crontab for weekly rebuild
0 2 * * 0 python /path/to/src/build_kb.py --rebuild --yes --auto-start --quiet
```

### Command Reference

#### build_kb.py - Knowledge Base Management
```bash
# Flags for unattended operation
--rebuild           # Full rebuild (requires --yes)
--yes, -y          # Skip all confirmations
--auto-start       # Auto-start Docker/Grobid
--continue         # Resume from checkpoint
--estimate         # Show time estimate only
--skip-quality     # Skip S2 if unavailable
--quiet, -q        # Silent mode for cron
--demo             # 5-paper test mode
--export FILE      # Export KB
--import FILE      # Import KB
--features         # Show enabled features
--progress         # Show current progress
--phase [1|2|3]     # Limit to specific phase features

# Examples:
python src/build_kb.py --rebuild --yes --phase 1  # Core only
python src/build_kb.py --estimate                  # Check time
python src/build_kb.py --continue                  # Resume failed build
python src/build_kb.py --features                  # Show what's enabled
```

#### research.py - Research Interface
```bash
# Search commands
search QUERY [--min-sample N] [--study-type TYPE] [--has-data]
smart-search QUERY [-k NUM]

# Discovery commands
discover [--similar-methods] [--same-datasets] [--fill-gaps]

# Analysis commands
gaps [--comprehensive]
reproducibility-report
methods-matrix
paper-dashboard ID

# Citation commands
cite ID1 ID2 ID3
```

---

## Implementation Plan

### Phase 1: Core Deliverables (Week 1-4) - MUST SHIP
- [ ] Remove all user prompts from build_kb.py
- [ ] Implement GrobidExtractor with proper XML parsing
- [ ] Add ProcessingCheckpoint for resume capability
- [ ] Create build time estimator with adaptive learning
- [ ] Add progress visualization for overnight runs
- [ ] Extract basic entities (sample sizes, study types)
- [ ] Implement ResilientExtractor with retry logic
- [ ] Add BuildMonitor for health tracking
- [ ] Unattended CLI interface with all flags

**Ship v5.0-core if Week 4 successful**

### Phase 2: Enhanced Features (Week 5-6) - SHIP IF READY
- [ ] Advanced entity extraction (p-values, datasets, software)
- [ ] Semantic Scholar API integration with AdaptiveRateLimiter
- [ ] S2 batch processing (400x efficiency improvement)
- [ ] Multi-level embeddings (5 types per paper)
- [ ] Entity-aware search parsing ("RCT with n>500")
- [ ] Quality score transparency (explanations)

**Decision Point**: If behind schedule, ship without Phase 2

### Phase 3: Discovery Features (Week 7-8) - DEFER IF NEEDED
- [ ] Smart discovery system
- [ ] Comprehensive gap analysis
- [ ] Reproducibility reports
- [ ] Methods matrix
- [ ] Citation network

**Can ship as v5.1 if not ready**

---

## Technical Details

### Feature Flags for Partial Deliverables

```python
FEATURE_FLAGS = {
    # Phase 1: Core (Week 1-4) - MUST SHIP
    'grobid_extraction': True,      # Critical fix
    'basic_entities': True,          # Sample sizes, study types
    'checkpoint_resume': True,       # Required for long builds
    'progress_logging': True,        # Progress visualization
    'time_estimation': True,         # Build time estimates

    # Phase 2: Enhanced (Week 5-6) - SHIP IF READY
    'advanced_entities': False,      # P-values, software, datasets
    'semantic_scholar': False,       # Citation metrics, TLDR
    'multi_embeddings': False,       # 5-level embedding system
    'entity_search': False,          # Parse "RCT with n>500"

    # Phase 3: Discovery (Week 7-8) - DEFER IF NEEDED
    'smart_discovery': False,        # Find similar papers
    'gap_analysis': False,           # Comprehensive gaps
    'citation_network': False,       # NetworkX integration
    'reproducibility': False,        # Reproducibility reports
}

def check_feature(feature: str) -> bool:
    """Check if feature is enabled in this build."""
    if not FEATURE_FLAGS.get(feature, False):
        print(f"Feature '{feature}' not available in this version")
        print(f"Available in: v5.1")
        return False
    return True
```

### Progress Visualization System

```python
class ProgressLogger:
    """Log progress for overnight monitoring."""

    def __init__(self, log_dir='kb_data'):
        self.log_file = Path(log_dir) / 'build_progress.log'
        self.summary_file = Path(log_dir) / 'build_progress.txt'
        self.start_time = time.time()

    def log(self, current: int, total: int, paper_name: str, status: str = 'processing'):
        """Log progress with ETA calculation."""
        elapsed = time.time() - self.start_time
        rate = current / elapsed if elapsed > 0 else 0
        eta = (total - current) / rate if rate > 0 else 0

        # Detailed log entry (JSON lines for parsing)
        entry = {
            'timestamp': datetime.now().isoformat(),
            'current': current,
            'total': total,
            'paper': paper_name,
            'status': status,
            'elapsed_hours': elapsed / 3600,
            'eta_hours': eta / 3600,
            'papers_per_hour': rate * 3600
        }

        with open(self.log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

        # Human-readable summary (for morning check)
        with open(self.summary_file, 'w') as f:
            f.write(f"Build Progress: {current}/{total} ({current/total*100:.1f}%)\n")
            f.write(f"Current: {paper_name}\n")
            f.write(f"Elapsed: {elapsed/3600:.1f} hours\n")
            f.write(f"ETA: {eta/3600:.1f} hours\n")
            f.write(f"Rate: {rate*3600:.0f} papers/hour\n")

            if status != 'processing':
                f.write(f"Status: {status}\n")
```

### Build Time Estimator

```python
class ResilientExtractor:
    """Extraction with automatic retry and failure tracking."""

    MAX_RETRIES = 3
    RETRY_DELAYS = [60, 120, 300]  # Exponential backoff

    def __init__(self):
        self.failed_papers = []
        self.retry_stats = {'total_retries': 0, 'successful_retries': 0}

    def extract_with_retry(self, pdf_path: Path) -> Optional[Dict]:
        """Extract with automatic retry on transient failures."""
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                result = self._extract(pdf_path)
                if attempt > 0:
                    self.retry_stats['successful_retries'] += 1
                return result

            except (requests.Timeout, requests.ConnectionError) as e:
                last_error = e
                self.retry_stats['total_retries'] += 1

                if attempt < self.MAX_RETRIES - 1:
                    delay = self.RETRY_DELAYS[attempt]
                    logger.warning(f"Attempt {attempt+1} failed for {pdf_path.name}")
                    logger.warning(f"Retrying in {delay} seconds...")
                    time.sleep(delay)
                else:
                    # Final attempt failed - track for manual review
                    self.failed_papers.append({
                        'path': str(pdf_path),
                        'error': str(e),
                        'attempts': self.MAX_RETRIES,
                        'timestamp': datetime.now().isoformat()
                    })

            except Exception as e:
                # Non-retryable error
                self.failed_papers.append({
                    'path': str(pdf_path),
                    'error': str(e),
                    'error_type': 'non_retryable',
                    'timestamp': datetime.now().isoformat()
                })
                return None

        return None

    def save_failed_papers(self):
        """Save failed papers list for manual review."""
        if self.failed_papers:
            failed_file = Path('kb_data/failed_papers.json')
            failed_file.write_text(json.dumps(self.failed_papers, indent=2))
            print(f"\n⚠️  {len(self.failed_papers)} papers failed extraction")
            print(f"See {failed_file} for details")

class BuildTimeEstimator:
    """Adaptive build time estimation."""

    def __init__(self):
        self.cache_file = Path('kb_data/.timing_cache.json')
        self.timings = self._load_timings()

    def _load_timings(self) -> Dict:
        """Load historical timing data."""
        if self.cache_file.exists():
            return json.loads(self.cache_file.read_text())

        # Default estimates (seconds)
        return {
            'grobid_per_page': 0.5,
            'pages_per_paper': 15,
            'embedding_per_paper': 2.0,
            's2_api_per_paper': 1.0,
            'basic_entity_extraction': 0.5,
            'advanced_entity_extraction': 1.0
        }

    def estimate(self, num_papers: int) -> Dict:
        """Estimate build time with confidence interval."""

        seconds = 0

        # Calculate based on enabled features
        if FEATURE_FLAGS['grobid_extraction']:
            pages = num_papers * self.timings['pages_per_paper']
            seconds += pages * self.timings['grobid_per_page']

        if FEATURE_FLAGS['basic_entities']:
            seconds += num_papers * self.timings['basic_entity_extraction']

        if FEATURE_FLAGS['advanced_entities']:
            seconds += num_papers * self.timings['advanced_entity_extraction']

        if FEATURE_FLAGS['semantic_scholar']:
            seconds += num_papers * self.timings['s2_api_per_paper']

        # Always need embeddings
        seconds += num_papers * self.timings['embedding_per_paper']

        # Add overhead and calculate range
        overhead = 1.2  # 20% overhead
        uncertainty = 0.3  # ±30%

        hours = (seconds * overhead) / 3600
        min_hours = hours * (1 - uncertainty)
        max_hours = hours * (1 + uncertainty)

        return {
            'estimated_hours': hours,
            'range': (min_hours, max_hours),
            'formatted': f"{hours:.1f} hours ({min_hours:.1f} - {max_hours:.1f} hours)",
            'overnight_safe': max_hours < 10,
            'weekend_needed': max_hours > 10
        }

    def update_from_run(self, actual_timings: Dict):
        """Update estimates based on actual performance."""
        alpha = 0.3  # Learning rate

        for key, value in actual_timings.items():
            if key in self.timings:
                # Exponential moving average
                self.timings[key] = (alpha * value +
                                   (1 - alpha) * self.timings[key])

        # Save updated timings
        self.cache_file.parent.mkdir(exist_ok=True)
        self.cache_file.write_text(json.dumps(self.timings, indent=2))
```

### Entity Extraction Implementation

```python
def extract_all_grobid_entities(xml: str) -> Dict:
    """Extract 50+ entity types from Grobid XML."""
    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}
    root = ET.fromstring(xml)

    return {
        # Methodology
        'sample_sizes': extract_sample_sizes(root, ns),
        'p_values': extract_p_values(root, ns),
        'confidence_intervals': extract_ci(root, ns),
        'study_type': detect_study_type(root, ns),

        # Software & Data
        'software': extract_software(root, ns),
        'datasets': extract_datasets(root, ns),
        'data_availability': extract_data_availability(root, ns),

        # Quality indicators
        'figures_count': len(root.findall('.//tei:figure', ns)),
        'tables_count': len(root.findall('.//tei:table', ns)),
        'references_count': len(root.findall('.//tei:ref[@type="bibr"]', ns)),
    }
```

### Semantic Scholar API Integration

```python
class AdaptiveRateLimiter:
    """Adaptive rate limiting for S2 API compliance."""

    def __init__(self):
        self.delay = 0.1  # Start optimistic
        self.min_delay = 0.1
        self.max_delay = 5.0
        self.success_count = 0
        self.rate_limit_count = 0

    def wait(self):
        """Wait before next API call."""
        time.sleep(self.delay)

    def on_success(self):
        """Speed up after sustained success."""
        self.success_count += 1
        if self.success_count > 10:
            # Gradually speed up
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
            logger.debug(f"Rate limiter: speeding up to {self.delay:.2f}s")

    def on_rate_limit(self):
        """Slow down on rate limit error."""
        self.rate_limit_count += 1
        self.delay = min(self.max_delay, self.delay * 2.0)
        self.success_count = 0
        logger.warning(f"Rate limited ({self.rate_limit_count} times). ")
        logger.warning(f"Slowing to {self.delay:.2f}s delay")

    def get_stats(self) -> Dict:
        """Get rate limiting statistics."""
        return {
            'current_delay': self.delay,
            'rate_limit_hits': self.rate_limit_count,
            'optimal': self.delay <= self.min_delay * 1.5
        }

class S2Client:
    """Semantic Scholar client with resilient API handling."""

    def __init__(self, api_key: Optional[str] = None):
        self.rate_limiter = AdaptiveRateLimiter()
        self.session = requests.Session()
        if api_key:
            self.session.headers['x-api-key'] = api_key

    def get_paper(self, doi: str, max_retries: int = 3) -> Optional[Dict]:
        """Get paper with adaptive rate limiting."""

        for attempt in range(max_retries):
            self.rate_limiter.wait()

            try:
                response = self.session.get(
                    f"https://api.semanticscholar.org/v1/paper/{doi}",
                    timeout=10
                )

                if response.status_code == 200:
                    self.rate_limiter.on_success()
                    return response.json()
                elif response.status_code == 429:
                    self.rate_limiter.on_rate_limit()
                    if attempt < max_retries - 1:
                        time.sleep(10 * (attempt + 1))  # Additional backoff
                elif response.status_code == 404:
                    return None  # Paper not found

            except requests.RequestException as e:
                logger.error(f"S2 API error: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)

        return None
```

### Advanced Grobid Endpoints (Phase 2-3)

```python
# For Phase 2: Enhanced Reference Processing
def fix_broken_references(reference_text: str) -> List[Dict]:
    """Use processReferences endpoint for malformed citations."""
    response = requests.post(
        "http://localhost:8070/api/processReferences",
        data={'references': reference_text},
        timeout=5
    )
    return parse_reference_xml(response.text)

# For Phase 2: Fast Metadata Screening
def quick_metadata_only(pdf_path: Path) -> Dict:
    """10x faster - just header extraction for pre-filtering."""
    with open(pdf_path, 'rb') as f:
        response = requests.post(
            "http://localhost:8070/api/processHeaderDocument",
            files={'input': f},
            timeout=5
        )
    # Returns: title, abstract, authors, affiliations
    return parse_header_xml(response.text)

# For Phase 3: Batch Citation Processing
def batch_process_citations(citations: List[str]) -> List[Dict]:
    """Process 100+ citations in one API call."""
    response = requests.post(
        "http://localhost:8070/api/processCitationList",
        data={'citations': '\n'.join(citations)},
        timeout=10
    )
    return parse_citations_xml(response.text)
```

### Enhanced Quality Score with Transparency

```python
def calculate_v5_quality_score(paper, grobid, s2) -> tuple[int, Dict]:
    """Comprehensive quality score with detailed explanation."""

    score = 0
    components = {}

    # Impact (30 points)
    citations = s2.get('citation_count', 0)
    impact_score = 0
    if citations > 100:
        impact_score = 15
        impact_reason = f"{citations} citations (high impact)"
    elif citations > 50:
        impact_score = 12
        impact_reason = f"{citations} citations (moderate impact)"
    elif citations > 10:
        impact_score = 8
        impact_reason = f"{citations} citations (some impact)"
    else:
        impact_reason = f"{citations} citations (limited impact)"

    score += impact_score
    components['impact'] = {
        'earned': impact_score,
        'max': 15,
        'reason': impact_reason
    }

    # Methodology (25 points)
    sample_size = get_max_sample_size(grobid)
    if sample_size > 1000: score += 10
    if 'rct' in grobid.get('study_type', '').lower(): score += 9
    if grobid.get('p_values'): score += 3

    # Reproducibility (20 points)
    if grobid.get('data_availability', {}).get('data_url'): score += 8
    if grobid.get('code_url'): score += 7
    if s2.get('is_open_access'): score += 5

    # Author credibility (10 points)
    max_h = s2.get('max_h_index', 0)
    if max_h > 50: score += 10
    elif max_h > 20: score += 7

    # Completeness (10 points)
    completeness = grobid.get('extraction_completeness', 0)
    score += int(completeness * 10)

    # Recency (5 points)
    if paper.get('year', 0) >= 2023: score += 5

    final_score = min(score, 100)

    # Generate explanation
    explanation = generate_score_explanation(final_score, components)

    return final_score, explanation

def generate_score_explanation(score: int, components: Dict) -> str:
    """Generate human-readable score explanation."""

    explanation = [f"Quality Score: {score}/100\n"]
    explanation.append("Breakdown:")

    for component, details in components.items():
        pct = (details['earned'] / details['max'] * 100) if details['max'] > 0 else 0
        explanation.append(
            f"• {component.title()}: {details['earned']}/{details['max']} "
            f"({pct:.0f}%) - {details['reason']}"
        )

    # Add grade
    if score >= 85:
        grade = "A+ (Excellent)"
    elif score >= 70:
        grade = "A (Good)"
    elif score >= 60:
        grade = "B (Above Average)"
    elif score >= 45:
        grade = "C (Average)"
    else:
        grade = "D (Below Average)"

    explanation.append(f"\nOverall Grade: {grade}")

    return '\n'.join(explanation)
```

### Entity-Aware Search

```python
class EntityAwareSearch:
    def search(self, query: str, papers: List[Dict]) -> List[Dict]:
        # Parse query for requirements
        entities = self._extract_query_entities(query)

        # Multi-factor scoring
        for paper in papers:
            score = 0
            score += semantic_similarity * 0.4
            score += entity_match_score * 0.3
            score += quality_score * 0.2
            score += recency_score * 0.1

        return sorted_by_score

    def _extract_query_entities(self, query: str) -> Dict:
        # Parse "RCT with n>500" → {study_type: 'rct', min_sample: 500}
        entities = {}

        # Sample size
        if match := re.search(r'n\s*[><=]\s*(\d+)', query):
            entities['min_sample'] = int(match.group(1))

        # Study type
        if 'RCT' in query.upper():
            entities['study_type'] = 'rct'

        return entities
```

---

## Success Metrics

### Phase 1: Core Release Criteria (v5.0-core)
- ✅ **95% extraction accuracy** (THE critical fix)
- ✅ Unattended operation (no prompts)
- ✅ Checkpoint/resume works
- ✅ Progress visualization for morning check
- ✅ Build time estimation
- ✅ Basic entity extraction (bonus value)

**MUST ship after Week 4 - this fixes the product**

### Phase 2: Enhanced Release (v5.0-full)
- ✅ All Phase 1 features
- ✅ Advanced entity extraction
- ✅ Semantic Scholar integration
- ✅ Multi-level embeddings
- ✅ Entity-aware search

**Ship if ready by Week 6**

### Target Metrics by Phase

#### Phase 1 (Core)
- 📊 Extraction accuracy: >95%
- 📊 Sample sizes extracted: >30% of papers
- 📊 Study types classified: >50% of papers
- 📊 Processing time: <8 hours for 2000 papers
- 📊 Progress logging: 100% coverage

#### Phase 2 (Enhanced)
- 📊 Sample sizes extracted: >50% of papers
- 📊 Study types classified: >70% of papers
- 📊 Processing time: 4-6 hours for 2000 papers
- 📊 Memory usage: <3GB peak

#### Phase 3 (Deferred to v5.1 if needed)
- ❌ Smart discovery system
- ❌ Citation network analysis
- ❌ Comprehensive gap analysis
- ❌ Methods matrix visualization

---

## Exit Codes & Automation

```bash
# Exit codes for scripts/automation
0 = Success
1 = Requirements not met (Docker/Grobid/Zotero)
2 = Extraction failed
3 = User abort (only without --yes flag)

# Example cron job for weekly rebuild
0 2 * * 0 cd /path/to/project && python src/build_kb.py --rebuild --yes --auto-start --quiet || echo "KB rebuild failed with code $?"

# Example systemd timer for daily incremental
[Timer]
OnCalendar=daily
Persistent=true

[Service]
ExecStart=/usr/bin/python /path/to/src/build_kb.py --yes --auto-start --quiet
```

---

## Monitoring & Reliability

### Build Monitoring

```python
class BuildMonitor:
    """Monitor build progress and health."""

    def __init__(self, log_dir: Path = Path('kb_data/monitoring')):
        self.log_dir = log_dir
        self.log_dir.mkdir(exist_ok=True)
        self.metrics_file = self.log_dir / 'build_metrics.jsonl'
        self.start_time = time.time()
        self.papers_processed = 0
        self.failures = []

    def record_paper(self, paper_id: str, success: bool, duration: float):
        """Record per-paper metrics."""
        self.papers_processed += 1

        metric = {
            'timestamp': datetime.now().isoformat(),
            'paper_id': paper_id,
            'success': success,
            'duration_seconds': duration,
            'total_processed': self.papers_processed,
            'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024
        }

        with open(self.metrics_file, 'a') as f:
            f.write(json.dumps(metric) + '\n')

        if not success:
            self.failures.append(paper_id)

    def generate_summary(self) -> str:
        """Generate build summary for morning review."""
        elapsed = time.time() - self.start_time
        success_rate = (self.papers_processed - len(self.failures)) / self.papers_processed * 100

        summary = [
            "="*50,
            "BUILD SUMMARY",
            "="*50,
            f"Total Papers: {self.papers_processed}",
            f"Successful: {self.papers_processed - len(self.failures)}",
            f"Failed: {len(self.failures)}",
            f"Success Rate: {success_rate:.1f}%",
            f"Total Time: {elapsed/3600:.1f} hours",
            f"Avg Time/Paper: {elapsed/self.papers_processed:.1f} seconds",
        ]

        if self.failures:
            summary.append(f"\n⚠️ Failed papers saved to: kb_data/failed_papers.json")

        return '\n'.join(summary)

    def check_health(self) -> Dict:
        """Check system health during build."""
        process = psutil.Process()

        return {
            'memory_usage_mb': process.memory_info().rss / 1024 / 1024,
            'memory_percent': process.memory_percent(),
            'cpu_percent': process.cpu_percent(),
            'disk_free_gb': psutil.disk_usage('/').free / 1024**3,
            'healthy': process.memory_percent() < 80
        }
```

### Error Recovery Strategy

```python
# Configuration for resilient processing
ERROR_RECOVERY_CONFIG = {
    'max_retries': 3,
    'retry_delays': [60, 120, 300],  # seconds
    'retryable_errors': [
        requests.Timeout,
        requests.ConnectionError,
        'HTTP 502',
        'HTTP 503',
        'HTTP 429'  # Rate limit
    ],
    'checkpoint_frequency': 50,  # Save every N papers
    'health_check_frequency': 100,  # Check resources every N papers
}

# Feature flag runtime checking with graceful degradation
def safe_feature_check(feature: str) -> bool:
    """Check feature availability with fallback."""
    if not FEATURE_FLAGS.get(feature, False):
        logger.warning(f"Feature '{feature}' not available, using fallback")
        return False
    return True

# Example usage:
if safe_feature_check('semantic_scholar'):
    quality_score, explanation = calculate_v5_quality_score(paper, grobid, s2)
else:
    quality_score, explanation = calculate_basic_quality_score(paper)
```

---

## Philosophy

### Core Principles
1. **Fix the bug first** - 95% extraction accuracy is the primary goal
2. **Overnight processing model** - Designed for "set Friday, use Monday" workflow
3. **Fail clearly** - No silent degradation, clear error messages
4. **Unattended operation** - Must work overnight without prompts
5. **Progressive enhancement** - Core fix ships first, features can wait
6. **Data integrity** - Better to fail than corrupt data

### Phased Development Approach

#### Phase 1 (Week 1-4): Ship Core Fix
- Week 1-2: **Infrastructure** (unattended operation, progress logging)
- Week 3-4: **Core extraction** (Grobid integration, basic entities)
- **Ship Point**: If successful, release v5.0-core

#### Phase 2 (Week 5-6): Add Value
- Week 5: **Enhanced extraction** (advanced entities, S2 integration)
- Week 6: **Smart search** (multi-embeddings, entity-aware search)
- **Ship Point**: If successful, release v5.0-full

#### Phase 3 (Week 7-8): Advanced Features
- Week 7: **Discovery** (if time permits)
- Week 8: **Polish** (documentation, optimization)
- **Defer Point**: Move to v5.1 if behind schedule

### What Success Looks Like
```
Week 3: "Extraction ACTUALLY WORKS! No more mangled methods sections!"
Week 4: "I can finally TRUST the extraction! Plus sample sizes!"
Week 5: "Citation data included? This is getting good!"
Week 6: "Entity-aware search? Now we're talking!"
Week 7: "It found papers using the same datasets? Amazing!"
Week 8: "From 'unreliable' to 'indispensable' - shipped!"
```

### User Value Proposition
```
BEFORE v5.0: "I hope it extracts my papers correctly" (50% fail)
AFTER v5.0:  "I trust it to extract my papers correctly" (95% work)

One weekend investment = Reliable extraction forever
```

## Quick Start Guide

### For Developers
```bash
# 1. Setup environment
git checkout -b v5-implementation
pip install -r requirements-dev.txt

# 2. Start Grobid
docker-compose up -d grobid

# 3. Run tests
pytest tests/unit/test_grobid_extraction.py

# 4. Test extraction on sample
python src/build_kb.py --demo --phase 1
```

### For Users
```bash
# Weekend setup (one-time)
docker pull grobid/grobid:0.7.3
python src/build_kb.py --rebuild --yes --auto-start

# Daily use
python src/build_kb.py                    # Incremental update
python src/research.py search "diabetes"  # Research
```

### Migration from v4.6
```bash
# 1. Export current KB (optional backup)
python src/build_kb.py --export kb_v4_backup.json

# 2. Clean slate for v5
rm -rf kb_data/

# 3. Rebuild with v5
git pull
python src/build_kb.py --rebuild --yes --auto-start
```
