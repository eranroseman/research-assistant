# Research Assistant v5.0: Three-System Integration

## Table of Contents

1. [Overview](#overview)
2. [Breaking Changes](#breaking-changes)
3. [Installation & Setup](#installation--setup)
4. [Core Features](#core-features)
5. [System Architecture](#system-architecture)
6. [Usage Guide](#usage-guide)
7. [Implementation Plan](#implementation-plan)
8. [Technical Details](#technical-details)
9. [Success Metrics](#success-metrics)

---

## Overview

### What is v5.0?

Research Assistant v5.0 is a **critical bug fix and enhancement** that transforms the product from "might work" (50% extraction success) to "will work" (95% extraction success).

It integrates three powerful systems:

1. **Zotero** - Paper management and PDF storage
2. **Grobid** - AI-powered structure and entity extraction (95% accuracy)
3. **Semantic Scholar** - Citation metrics and AI-generated summaries

### Why v5.0?

The current v4.6 has a **product-breaking bug**:

- ‚ùå **50% extraction failure rate** - Current extraction cannot preserve document structure
- ‚ùå **Unreliable extraction** - Users cannot trust the output
- ‚ùå **No citation networks** - Can't find related papers
- ‚ùå **Limited discovery** - Only keyword matching

**This is not a feature addition - it's fixing a critical defect.**

### What v5.0 Delivers

- ‚úÖ **95% extraction accuracy** with Grobid
- ‚úÖ **50+ entity types extracted** (sample sizes, p-values, methods, datasets)
- ‚úÖ **Multi-level embeddings** for intelligent search
- ‚úÖ **Entity-aware discovery** (find papers using same methods/datasets)
- ‚úÖ **Comprehensive gap analysis** (methodology, reproducibility, evidence levels)
- ‚úÖ **Unattended operation** for long-running manual builds
- ‚úÖ **Collection-based processing** for focused knowledge bases

---

## Breaking Changes

### ‚ö†Ô∏è IMPORTANT: Complete Rebuild Required

```bash
# One-time weekend investment for reliable extraction forever
rm -rf kb_data/              # Delete existing KB
git pull                      # Update to v5
pip install -r requirements.txt
python src/build.py --rebuild  # 4-6 hour rebuild (no prompts needed)

# Why rebuild? Because you're getting:
# ‚Ä¢ 50% ‚Üí 95% extraction accuracy
# ‚Ä¢ Structured data that won't need re-extraction
# ‚Ä¢ Future-proof XML storage for new entity mining
```

### Why No Migration?

- Different extraction format (Grobid XML vs plain text)
- New entity storage structure
- Enhanced embedding system
- Incompatible quality scores

---

## Installation & Setup

### Prerequisites

```bash
# 1. Install Docker
sudo apt install docker.io  # Ubuntu/Debian
brew install docker         # macOS

# 2. Install Grobid (one-time)
docker pull lfoppiano/grobid:0.8.2
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.8.2

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Verify setup
python src/check_requirements.py  # Exit 0 = ready
```

### System Requirements

```yaml
Minimum Requirements:
  RAM: 4 GB (for <1000 papers)
  Disk: 10 GB free (for XML cache)
  CPU: 2 cores
  Network: 10 Mbps (for S2 API)

Recommended:
  RAM: 8 GB (for 2000+ papers)
  Disk: 20 GB free (includes all caches)
  CPU: 4+ cores
  Network: 50+ Mbps

Grobid Docker:
  RAM: 2 GB allocated to container
  Disk: 5 GB for Docker image

Data Usage:
  ~500 KB per paper (S2 API calls)
  ~100 KB per paper (XML cache)
  ~50 KB per paper (embeddings)

API Limits:
  S2 Unauthenticated: 1 request/second
  S2 Batch: 100 papers per request
  Grobid: No hard limits (local Docker)
```

### First Build - The Academic Workflow

```bash
# Friday afternoon workflow:
python src/build.py --estimate
# Output: "Estimated time: 4.5 hours (3.2 - 5.9 hours)"
#         "‚úì Safe for long unattended run"

# Start before leaving:
python src/build.py --rebuild

# Monday morning: Complete KB with quality and gap reports ready

# What happens:
# 1. Checks all systems are running
# 2. Shows time estimate and starts
# 3. Fetches papers from Zotero
# 4. Extracts structure with Grobid (4-6 hours)
# 5. Logs progress to kb_data/build_progress.log
# 6. Enhances with Semantic Scholar data
# 7. Creates multi-level embeddings
# 8. Builds FAISS indices

# Check progress in the morning
cat kb_data/build_progress.txt
# Build Progress: 1847/2000 (92.4%)
# Current: paper_1847.pdf
# Elapsed: 4.2 hours
# ETA: 0.3 hours
# Rate: 440 papers/hour
```

---

## Core Features

### 1. Entity Extraction (50+ Types)

#### From Grobid

- **Methodology**: Sample sizes, p-values, confidence intervals, study types
- **Software & Data**: Software versions, datasets, data availability
- **Clinical**: Trial IDs, organisms, diseases
- **Institutional**: Funders, affiliations, countries

#### From Semantic Scholar

- **Impact**: Citation count, influential citations, citation velocity
- **AI Insights**: TLDR summaries, field classifications
- **Author Metrics**: H-index, author citation counts
- **Access**: Open access status, PDF availability

### 2. Multi-Level Embeddings

```python
# Each paper has 5 embedding types:
{
    'title_abstract': standard_embedding,   # Title + abstract only (Multi-QA MPNet, 768-dim)
    'enriched': entity_aware_embedding,     # Title + abstract + extracted entities (768-dim)
    's2_embedding': semantic_scholar_vector, # SPECTER2 from S2 API (768-dim, same input as title_abstract)
    'methods': methods_section_embedding,    # Methods section only (768-dim)
    'metadata_vector': numerical_features    # Sample size, citations, year, etc. (for filtering)
}
```

**Key insights about S2 embeddings:**

- **Same input as `title_abstract`**: Both use title + abstract text
- **Different training**: SPECTER2 trained on citation graph (papers that cite each other have similar embeddings)
- **Same dimensions**: 768-dim vectors, compatible with Multi-QA MPNet
- **API dependency**: Pre-computed by Semantic Scholar, not always available
- **Quality comparison opportunity**: Can A/B test local MPNet vs S2 SPECTER2

### 3. Entity-Aware Search

```bash
# Explicit entity filtering with flags
python src/kbq.py search "diabetes treatment" \
  --study-type RCT \
  --min-sample 500 \
  --software R

# Clear, unambiguous, scriptable
```

### 4. Smart Discovery

```bash
# External paper discovery via Semantic Scholar
python src/discover.py --keywords "machine learning, diabetes"

# Find papers with specific quality criteria
python src/discover.py --quality-threshold HIGH --year-from 2020

# v5.0 NEW: SPECTER2 similarity-based discovery
python src/discover.py --similar-to 0001,0002 --similarity-mode specter2

# v5.0 NEW: Discovery with quality explanations
python src/discover.py --keywords "AI diagnostics" --explain-quality
```

### 5. Comprehensive Analysis

```bash
# Paper details from KB
python src/kbq.py get 0001

# Generate citations
python src/kbq.py cite 0001 0002 0003

# Smart search for complex queries
python src/kbq.py smart-search "diabetes treatment" -k 30

# Gap analysis for missing literature
python src/gaps.py --min-citations 50 --year-from 2020

# v5.0 NEW: Entity gap analysis
python src/gaps.py --gap-type entity --show-missing software:Python

# v5.0 NEW: Reproducibility gap detection
python src/gaps.py --reproducibility-gaps --requires both --min-citations 100
```

---

## System Architecture

### Data Flow

```
1. ZOTERO (Source)
   ‚îú‚îÄ‚îÄ PDFs
   ‚îú‚îÄ‚îÄ Metadata
   ‚îî‚îÄ‚îÄ Collections
        ‚Üì
2. GROBID (Extraction)
   ‚îú‚îÄ‚îÄ Structured sections
   ‚îú‚îÄ‚îÄ References & citations
   ‚îú‚îÄ‚îÄ Entities & metadata
   ‚îî‚îÄ‚îÄ Study characteristics
        ‚Üì
3. SEMANTIC SCHOLAR (Enhancement)
   ‚îú‚îÄ‚îÄ Citation metrics
   ‚îú‚îÄ‚îÄ AI summaries (TLDR)
   ‚îú‚îÄ‚îÄ Author metrics
   ‚îî‚îÄ‚îÄ Related papers
        ‚Üì
4. KNOWLEDGE BASE
   ‚îú‚îÄ‚îÄ FAISS indices (search)
   ‚îú‚îÄ‚îÄ Entity database (filtering)
   ‚îî‚îÄ‚îÄ Citation network (discovery)
```

### Storage Structure

```
kb_data/
‚îú‚îÄ‚îÄ papers/           # Markdown files with full content
‚îú‚îÄ‚îÄ cache/
‚îÇ   ‚îú‚îÄ‚îÄ grobid/      # XML extractions
‚îÇ   ‚îú‚îÄ‚îÄ s2/          # API responses
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/  # Computed vectors
‚îú‚îÄ‚îÄ indices/
‚îÇ   ‚îú‚îÄ‚îÄ semantic.faiss    # Main search (title_abstract + enriched + s2_embedding)
‚îÇ   ‚îú‚îÄ‚îÄ methods.faiss     # Method similarity search
‚îÇ   ‚îî‚îÄ‚îÄ metadata.faiss    # Numerical filtering (not true embeddings)
‚îî‚îÄ‚îÄ metadata.json    # Complete paper data
```

---

## Usage Guide

### Daily Workflow

#### Morning: Check for New Papers

```bash
# Incremental update of entire library (fast, automatic, with reports)
python src/build.py

# Update only specific collection
python src/build.py --collection "Current Research"

# Skip gap analysis for faster completion
python src/build.py --no-gaps

# Update collection without gap analysis (fastest)
python src/build.py --collection "Weekly Papers" --no-gaps

# Quality scores are automatically upgraded when needed
```

#### Research Tasks

```bash
# Search with explicit entity filters
python src/kbq.py search "diabetes" \
  --min-sample 500 \
  --study-type RCT \
  --software Python \
  --year-from 2020 \
  --has-data

# Discover external papers
python src/discover.py --keywords "diabetes, insulin resistance" \
  --quality-threshold HIGH

# Analyze gaps in literature
python src/gaps.py --min-citations 50 --year-from 2020
```

#### Manual Build Processing

```bash
# Run manually when needed (e.g., weekly)
python /path/to/src/build.py --rebuild
```

### Command Reference

#### build.py - Knowledge Base Building

```bash
# Core operation flags (simplified)
--rebuild          # Full rebuild instead of incremental
--demo             # Test with 5 papers only
--export FILE      # Export KB and exit
--import FILE      # Import KB and exit
--no-gaps          # Skip gap analysis after build (saves 15-25 min)
--collection NAME  # Process only papers from specified Zotero collection

# Information flags (exit immediately)
--estimate         # Show time estimate and exit
--progress         # Show current build status and exit

# Automatic post-build reports (always generated):
# 1. PDF quality report ‚Üí exports/analysis_pdf_quality.md (if issues found)
# 2. Gap analysis ‚Üí exports/gap_analysis_*.md (unless --no-gaps)
# 3. Build summary with next steps

# Examples:
python src/build.py                           # Default: entire library + reports + gaps
python src/build.py --collection "PhD Thesis" # Only papers from PhD Thesis collection
python src/build.py --rebuild                 # Full rebuild + reports + gaps
python src/build.py --demo                    # Quick test (no gap analysis)
python src/build.py --no-gaps                 # Skip gap analysis only
python src/build.py --estimate                # Check timing before starting
python src/build.py --collection "ML Papers" --no-gaps  # Specific collection, no gaps
```

### Script Naming Convention Changes

**v5.0 introduces clearer, more intuitive script names:**

| Old Name (v4.6) | New Name (v5.0) | Purpose | Rationale |
|-----------------|-----------------|---------|-----------|
| `cli.py` | `kbq.py` | Knowledge Base Query | Reflects entity-aware search capabilities and database-like querying |
| `analyze_gaps.py` | `gaps.py` | Gap Analysis | Shorter, cleaner name for network-based gap discovery |
| `build_kb.py` | `build.py` | Build Knowledge Base | Simplified name, context is clear from directory |
| `discover.py` | `discover.py` | External Discovery | Kept unchanged - name is already clear and concise |

**Why these changes?**

- **Clarity**: Names directly indicate functionality (kbq = Knowledge Base Query)
- **Brevity**: Shorter names for frequent command-line usage
- **Consistency**: All scripts follow simple noun/verb pattern
- **v5.0 Signal**: New names indicate the major architectural upgrade

#### kbq.py - Knowledge Base Query Interface

```bash
# Search commands with explicit entity filters
search QUERY [--min-sample N] [--study-type TYPE] [--software NAME] 
             [--year-from YYYY] [--has-data] [--max-p-value N]
             [--exclude TERMS] [--include TERMS] [--journal NAME]
             [--group-by year|journal|type] [--export-csv FILE]
             [--show-quality] [--quality-min N]
smart-search QUERY [-k NUM]
author NAME [--exact]

# Retrieval commands
get ID [--sections abstract methods results] [--add-citation]
get-batch ID1 ID2 ID3 [--sections SECTIONS]
cite ID1 ID2 ID3

# Batch operations (10-20x faster for multiple commands)
batch --preset research|review|author-scan [ARGS]
batch --file commands.txt

# Analysis commands
info
diagnose
```

#### discover.py - External Discovery

```bash
# Basic discovery flags
--keywords "term1, term2"           # Search keywords (required)
--quality-threshold HIGH|MEDIUM|LOW # Quality filter (80+/60+/40+ scores)
--year-from YYYY                    # Recency filter (default: 2020)
--min-citations N                   # Minimum citation count
--limit N                           # Maximum results (default: 50)
--study-types TYPE1,TYPE2           # Filter by methodology (rct, cohort, etc.)
--author-filter "Name1,Name2"       # Focus on specific researchers (max 5)
--population-focus TYPE             # Target populations (pediatric|elderly|women|developing_countries)
--include-kb-papers                 # Include existing KB papers (default: exclude)
--coverage-info                     # Show database coverage guidance

# v5.0 NEW: SPECTER2-based similarity (High ROI)
--similar-to ID1,ID2,ID3            # Find papers similar to these KB papers
--similarity-mode specter2|mpnet    # Use S2 embeddings or local MPNet (default: specter2)
--min-similarity 0.7                # Minimum cosine similarity threshold

# v5.0 NEW: Quality transparency (High ROI)
--explain-quality                   # Include detailed quality breakdowns in output
--min-reproducibility 60            # Filter by reproducibility score (code/data availability)
```

#### gaps.py - Gap Analysis

**Default Behavior (no flags):**

```bash
python src/gaps.py
# Runs comprehensive gap analysis with:
# - min-citations: 10 (finds moderately cited papers)
# - year-from: all years (no filter) 
# - limit: unlimited (analyzes entire KB)
# - gap-type: all types (citation, author, cocited, recent, semantic)
# - Runtime: 15-25 minutes
# - Output: exports/gap_analysis_YYYYMMDD_HHMMSS.md
```

**Flag Effects:**

```bash
# Basic filtering flags (modify comprehensive analysis)
--min-citations N   # Default: 10. Higher = focus on high-impact gaps only
--year-from YYYY    # Default: all years. Set to focus on recent research
--limit N           # Default: unlimited. Set lower for faster analysis
--kb-path PATH      # Default: kb_data/. Custom KB location

# Gap type selection (changes analysis algorithm)
--gap-type TYPE     # Default: citation. Options:
                    # citation = papers cited by your collection (default)
                    # author = recent work by your KB authors
                    # entity = papers with same methods/tools/datasets
                    # reproducibility = papers with available code/data

# Entity gap analysis (requires --gap-type entity)
--show-missing SPEC # Example: software:Python,dataset:MIMIC
--entity-coverage   # Generate entity coverage statistics

# Reproducibility gaps (can combine with any gap-type)
--reproducibility-gaps        # Filter for reproducible papers only
--requires code|data|both     # What must be available
--min-reproducibility-score N # Default: 60 (0-100 scale)

# Sample size gaps (can combine with any gap-type)
--sample-size-gaps           # Filter for large studies only
--min-sample N               # Default: 1000 participants
--study-type RCT|cohort|all  # Default: all
```

**Common Usage Patterns:**

```bash
# Quick gaps check (5-10 min)
python src/gaps.py --limit 50 --min-citations 100

# Find reproducible research gaps
python src/gaps.py --reproducibility-gaps --requires both

# Find papers using same tools
python src/gaps.py --gap-type entity --show-missing software:TensorFlow

# Find large clinical trials
python src/gaps.py --sample-size-gaps --min-sample 5000 --study-type RCT

# Recent high-impact gaps only
python src/gaps.py --year-from 2023 --min-citations 50 --limit 100
```

**When Called Automatically by build.py:**

```python
# build.py calls gaps.py with default settings (no flags)
# unless --no-gaps is specified:
if not args.no_gaps:
    print("\n‚Ä¢ Running gap analysis (15-25 minutes)...")
    subprocess.run(['python', 'src/gaps.py'])
    # Uses gaps.py defaults: comprehensive analysis of entire KB
    # Output: exports/gap_analysis_YYYYMMDD_HHMMSS.md
else:
    print("\n‚Ä¢ Gap analysis skipped (--no-gaps flag)")
```

**Gap Analysis Behavior Summary:**

| Context | Default Behavior | Key Parameters | Runtime | Output |
|---------|-----------------|----------------|---------|--------|
| **Standalone** | Comprehensive analysis | All defaults (no limits) | 15-25 min | Complete gap report |
| **Via build.py** | Same comprehensive analysis | gaps.py defaults used | 15-25 min | Complete gap report |
| **With --no-gaps** | Skipped entirely | N/A | 0 min | None |
| **With --demo** | Skipped automatically | N/A | 0 min | None |
| **Quick check** | Use --limit 50 | Reduces to top gaps only | 5-10 min | Most important gaps |
| **Entity focus** | --gap-type entity | Requires --show-missing | 10-15 min | Method/tool gaps |
| **Reproducible** | --reproducibility-gaps | Can add --requires both | 15-20 min | Papers with code/data |

---

## Implementation Plan

### Phase 1: Core Deliverables (Week 1-4) - MUST SHIP

#### Week 1-2: Shared Libraries & Infrastructure

**Day 1-2: Core Library Setup**

- [ ] Create lib/ directory structure
- [ ] Implement extraction_common.py:
  - `GrobidClient` class with retry logic and connection pooling
  - `PDFExtractor` abstract base class for consistent interface
  - `ExtractionResult` dataclass for consistent return format
  - `ZoteroClient` class with collection filtering support
- [ ] Add collection filtering to Zotero API calls
- [ ] Write unit tests for extraction_common.py (target: 90% coverage)

**Day 3-4: Quality & Progress Libraries**  

- [ ] Implement quality_scoring.py:
  - Migrate scoring logic from build_kb.py and discover.py
  - `QualityScorer` class with configurable weights
  - Explanation generation for transparency
  - Cache integration for performance
- [ ] Implement progress_monitor.py:
  - `ProgressTracker` with checkpoint recovery
  - Time estimation based on rolling average
  - Memory-efficient state persistence (JSON, not pickle)
- [ ] Integration tests for library interactions

**Day 5-6: Entity & Embedding Libraries**

- [ ] Implement entity_database.py:
  - In-memory pandas DataFrame for entity storage (2K papers = ~2MB)
  - Fast entity filtering before semantic search  
  - Vectorized operations for efficient filtering
  - Note: SQLite unnecessary - manual builds mean no concurrent access needed
- [ ] Implement embeddings_common.py:
  - Multi-level embedding management
  - FAISS index abstraction
  - Consistent vector normalization
- [ ] Performance benchmarks (target: <100ms for 10K entity filter)

**Day 7-8: Report Generator & CLI Updates**

- [ ] Implement report_generator.py:
  - Migrate report logic from kbq.py, gaps.py, discover.py
  - Consistent markdown formatting
  - Export to multiple formats (MD, CSV, JSON)
- [ ] Update all scripts to use shared libraries:
  - Remove duplicate code (target: 60% reduction)
  - Add comprehensive error handling
  - Ensure backward compatibility
- [ ] Remove all user prompts from build.py
- [ ] Auto-start Grobid if not running
- [ ] Auto-resume from checkpoint if exists
- [ ] Auto-fallback if S2 API unavailable

#### Week 3-4: Core Extraction & Resilience

**Day 9-10: Grobid Integration**

- [ ] Integrate GrobidExtractor with extraction_common.py:
  - Docker container management (auto-start, health checks)
  - Batch processing with configurable chunk size
  - Note: No PyMuPDF fallback - if Grobid fails, mark paper as failed (better to know than have bad data)
- [ ] Implement extraction validation:
  - Section completeness checks
  - Quality thresholds (min 500 chars for methods)
  - Extraction success metrics logging
- [ ] End-to-end extraction tests with real PDFs

**Day 11-12: Basic Entity Extraction**

- [ ] Implement basic entity extraction:
  - Sample size detection (regex + NLP patterns)
  - Study type classification (RCT, cohort, etc.)
  - Statistical values (p-values, confidence intervals)
- [ ] Add entity validation and normalization:
  - Standardize sample size formats (N=100 ‚Üí 100)
  - Validate statistical value ranges
  - Handle edge cases (ranges, approximations)
- [ ] Entity extraction accuracy tests (target: 85% precision)

**Day 13-14: Checkpoint & Resume System**

- [ ] Add ProcessingCheckpoint using progress_monitor.py:
  - Save state every 10 papers (configurable)
  - Atomic writes to prevent corruption
  - Resume from exact interruption point
- [ ] Implement robust error recovery:
  - Per-paper error isolation
  - Retry logic with exponential backoff
  - Detailed error reporting with actionable fixes
- [ ] Stress tests for interruption recovery

**Day 15-16: Integration & Polish**

- [ ] Full build.py integration:
  - All libraries working together
  - Memory optimization for 10K+ paper builds
  - Performance profiling and bottleneck fixes
- [ ] User experience improvements:
  - Clear progress indicators with ETA
  - Informative error messages
  - Success/failure summary with statistics
- [ ] Documentation and examples:
  - API documentation for all libraries
  - Migration guide from v4.6
  - Common troubleshooting solutions
- [ ] Auto-generate quality report after build
- [ ] Auto-run gap analysis (unless --no-gaps)

**Ship v5.0-core if Week 4 successful**

### Phase 2: Enhanced Features (Week 5-6) - SHIP IF READY

#### Core v5.0 Enhancements

- [ ] Advanced entity extraction (p-values, datasets, software)
- [ ] Semantic Scholar API integration with AdaptiveRateLimiter
- [ ] S2 batch processing (400x efficiency improvement)
- [ ] Multi-level embeddings (5 types per paper)
- [ ] Entity-based filtering with explicit flags
- [ ] Quality score transparency (explanations)

#### High-ROI Discovery Features (NEW)

- [ ] **SPECTER2 similarity search** (discover.py) - Use S2 embeddings for citation-aware discovery
- [ ] **Quality explanation generation** (discover.py) - Detailed breakdowns build user trust
- [ ] **Entity gap analysis** (gaps.py) - "What methods/tools am I missing?"
- [ ] **Reproducibility gap detection** (gaps.py) - Find papers with available code/data

**Decision Point**: If behind schedule, ship without Phase 2

### Phase 3: Discovery Features (Week 7-8) - DEFER IF NEEDED

- [ ] Smart discovery system
- [ ] Comprehensive gap analysis
- [ ] Reproducibility reports
- [ ] Methods matrix
- [ ] Citation network

**Can ship as v5.1 if not ready**

---

## Technical Details

### Shared Libraries Architecture

The v5.0 design introduces shared libraries to eliminate code duplication and ensure consistency across all scripts. These libraries provide 60-70% code reduction and single source of truth for critical logic.

#### Library Structure

```
src/
‚îú‚îÄ‚îÄ lib/                           # Shared libraries (NEW in v5.0)
‚îÇ   ‚îú‚îÄ‚îÄ extraction_common.py      # Grobid/S2 API clients
‚îÇ   ‚îú‚îÄ‚îÄ quality_scoring.py        # Unified quality scoring
‚îÇ   ‚îú‚îÄ‚îÄ progress_monitor.py       # Progress tracking for long operations
‚îÇ   ‚îú‚îÄ‚îÄ embeddings_common.py      # Multi-level embedding management
‚îÇ   ‚îú‚îÄ‚îÄ entity_database.py        # Fast entity-based filtering
‚îÇ   ‚îî‚îÄ‚îÄ report_generator.py       # Consistent report generation
‚îú‚îÄ‚îÄ build.py                       # Uses 5/6 libraries
‚îú‚îÄ‚îÄ kbq.py                        # Uses 4/6 libraries
‚îú‚îÄ‚îÄ discover.py                   # Uses 3/6 libraries
‚îî‚îÄ‚îÄ gaps.py                       # Uses 4/6 libraries
```

#### Priority Implementation (Phase 1 - MUST HAVE)

##### 1. extraction_common.py

```python
class ZoteroClient:
    """Zotero API client with collection support"""
    def get_all_papers(self) -> List[Dict]
    def get_collection_papers(self, collection_name: str) -> List[Dict]
    def list_collections(self) -> List[str]
    def get_pdfs_for_papers(self, papers: List[Dict]) -> Dict[str, Path]

class GrobidClient:
    """Shared Grobid API client with retry logic"""
    def extract_full_text(self, pdf_path) -> Dict
    def extract_references(self, text) -> List[Dict]

class S2Client:
    """Semantic Scholar API with adaptive rate limiting"""
    def get_paper(self, doi) -> Dict
    def get_papers_batch(self, dois) -> List[Dict]
    
class EntityExtractor:
    """Extract entities from Grobid XML"""
    def extract_sample_sizes(self, xml) -> List[int]
    def extract_p_values(self, xml) -> List[float]
    def extract_software(self, xml) -> List[str]
```

**Impact**: Critical deduplication - used by build.py, kbq.py, gaps.py

##### 2. quality_scoring.py

```python
class QualityScorer:
    """Unified quality scoring with explanations"""
    def calculate_score(self, paper, grobid, s2) -> tuple[int, str]
    def get_reproducibility_score(self, paper) -> int
    def explain_score(self, score, components) -> str
```

**Impact**: Ensures consistent scoring across all tools

##### 3. progress_monitor.py

```python
class ProgressTracker:
    """Track long-running operations with checkpoints"""
    def __init__(self, total, log_dir='kb_data')
    def update(self, current, message='')
    def create_checkpoint(self) -> Dict
    def resume_from_checkpoint(self, checkpoint) -> None
```

**Impact**: Essential for 4-6 hour builds

#### Optional Libraries (Phase 2 - SHOULD HAVE)

- **embeddings_common.py**: Manage 5-level embeddings (implement with lazy loading)
- **entity_database.py**: pandas DataFrame for fast filtering (sufficient for manual KB builds)
- **report_generator.py**: Markdown/CSV/JSON export (keep simple)

### Simplified Build Philosophy

The v5.0 build process follows these principles:

1. **Always complete** - No prompts interrupt the build
2. **Always use latest** - All available features are used automatically
3. **Smart defaults** - Auto-starts Grobid, resumes checkpoints, handles failures
4. **Minimal flags** - Only 7 flags total, most users need none
5. **Helpful output** - Shows exactly what's happening, no verbose/quiet modes
6. **Actionable reports** - Generates quality and gap analysis automatically

### Default Behavior (Simplified)

```python
# v5.0 always uses all available features - no flags needed
def main():
    """Build with all available features, showing helpful progress."""
    
    # 1. Always ensure Grobid is running
    ensure_grobid_ready()
    
    # 2. Determine collection scope
    if args.collection:
        print(f"Processing collection: {args.collection}")
        papers = fetch_from_collection(args.collection)
    else:
        print("Processing entire library")
        papers = fetch_entire_library()
    
    # 3. Auto-detect operation mode
    if args.rebuild:
        # Create safety backup before rebuild
        auto_backup()
        rebuild_from_scratch(papers)
    else:
        # Check for checkpoint and resume if exists
        if checkpoint_exists():
            resume_from_checkpoint(papers)
        else:
            incremental_update(papers)
    
    # 3. Auto-handle quality upgrades
    if papers_need_quality_upgrade():
        upgrade_quality_scores()
    
    # 4. Use all v5.0 features automatically
    extract_with_grobid()        # Always
    extract_entities()           # Always
    get_s2_metrics()            # If available, fallback if not
    create_multi_embeddings()    # Always
    
    # 5. Generate PDF quality report (if issues found)
    missing_pdfs = [p for p in papers if not p.get('full_text')]
    small_pdfs = [p for p in papers if len(p.get('full_text', '')) < 5000]
    
    if missing_pdfs or small_pdfs:
        print("\n‚Ä¢ Generating PDF quality report...")
        report_path = generate_pdf_quality_report(papers)
        print(f"‚úÖ PDF quality report: {report_path}")
        if missing_pdfs:
            print(f"   - {len(missing_pdfs)} papers missing PDFs")
        if small_pdfs:
            print(f"   - {len(small_pdfs)} papers with small PDFs")
    else:
        print("\n‚úÖ All papers have good PDF quality - no report needed")
    
    # 6. Run gap analysis (unless --no-gaps or --demo)
    if not args.no_gaps and not args.demo:
        print("\n‚Ä¢ Running gap analysis (15-25 minutes)...")
        gap_count = run_gap_analysis()
        print(f"‚úÖ Gap analysis complete: {gap_count} gaps identified")
    elif args.demo:
        print("\n‚Ä¢ Gap analysis skipped for demo builds")
    else:
        print("\n‚Ä¢ Gap analysis skipped (--no-gaps flag)")
    
    print("\n‚ú® Build complete! Next steps:")
    print("  ‚Ä¢ Review quality report: exports/kb_quality_analysis.pdf")
    if not args.no_gaps:
        print("  ‚Ä¢ Check gaps report for missing papers")
    print("  ‚Ä¢ Start searching: python src/kbq.py search 'your query'")
```

### Progress Visualization System

```python
class ProgressLogger:
    """Log progress for long-running build monitoring."""

    def __init__(self, log_dir='kb_data'):
        self.log_file = Path(log_dir) / 'build_progress.log'
        self.summary_file = Path(log_dir) / 'build_progress.txt'
        self.start_time = time.time()

    def log(self, current: int, total: int, paper_name: str, status: str = 'processing'):
        """Log progress with ETA calculation."""
        elapsed = time.time() - self.start_time
        rate = current / elapsed if elapsed > 0 else 0
        eta = (total - current) / rate if rate > 0 else 0

        # Detailed log entry (JSON lines for parsing)
        entry = {
            'timestamp': datetime.now().isoformat(),
            'current': current,
            'total': total,
            'paper': paper_name,
            'status': status,
            'elapsed_hours': elapsed / 3600,
            'eta_hours': eta / 3600,
            'papers_per_hour': rate * 3600
        }

        with open(self.log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

        # Human-readable summary (for morning check)
        with open(self.summary_file, 'w') as f:
            f.write(f"Build Progress: {current}/{total} ({current/total*100:.1f}%)\n")
            f.write(f"Current: {paper_name}\n")
            f.write(f"Elapsed: {elapsed/3600:.1f} hours\n")
            f.write(f"ETA: {eta/3600:.1f} hours\n")
            f.write(f"Rate: {rate*3600:.0f} papers/hour\n")

            if status != 'processing':
                f.write(f"Status: {status}\n")
```

### Collection Processing

```python
def fetch_from_collection(collection_name: str) -> List[Dict]:
    """Fetch papers from specific Zotero collection.
    
    Args:
        collection_name: Exact name of Zotero collection (case-sensitive)
    
    Returns:
        List of paper dictionaries from the collection
    
    Raises:
        ValueError: If collection doesn't exist
    """
    client = ZoteroClient()
    
    # List available collections for validation
    available = client.list_collections()
    if collection_name not in available:
        print(f"\n‚ùå Collection '{collection_name}' not found!")
        print("Available collections:")
        for name in available:
            print(f"  - {name}")
        raise ValueError(f"Collection '{collection_name}' does not exist")
    
    papers = client.get_collection_papers(collection_name)
    print(f"Found {len(papers)} papers in collection '{collection_name}'")
    return papers

def fetch_entire_library() -> List[Dict]:
    """Fetch all papers from Zotero library."""
    client = ZoteroClient()
    papers = client.get_all_papers()
    print(f"Found {len(papers)} papers in entire library")
    return papers
```

### Build Time Estimator

```python
class ResilientExtractor:
    """Extraction with automatic retry and failure tracking."""

    MAX_RETRIES = 3
    RETRY_DELAYS = [60, 120, 300]  # Exponential backoff

    def __init__(self):
        self.failed_papers = []
        self.retry_stats = {'total_retries': 0, 'successful_retries': 0}

    def extract_with_retry(self, pdf_path: Path) -> Optional[Dict]:
        """Extract with automatic retry on transient failures."""
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                result = self._extract(pdf_path)
                if attempt > 0:
                    self.retry_stats['successful_retries'] += 1
                return result

            except (requests.Timeout, requests.ConnectionError) as e:
                last_error = e
                self.retry_stats['total_retries'] += 1

                if attempt < self.MAX_RETRIES - 1:
                    delay = self.RETRY_DELAYS[attempt]
                    logger.warning(f"Attempt {attempt+1} failed for {pdf_path.name}")
                    logger.warning(f"Retrying in {delay} seconds...")
                    time.sleep(delay)
                else:
                    # Final attempt failed - track for manual review
                    self.failed_papers.append({
                        'path': str(pdf_path),
                        'error': str(e),
                        'attempts': self.MAX_RETRIES,
                        'timestamp': datetime.now().isoformat()
                    })

            except Exception as e:
                # Non-retryable error
                self.failed_papers.append({
                    'path': str(pdf_path),
                    'error': str(e),
                    'error_type': 'non_retryable',
                    'timestamp': datetime.now().isoformat()
                })
                return None

        return None

    def save_failed_papers(self):
        """Save failed papers list for manual review."""
        if self.failed_papers:
            failed_file = Path('kb_data/failed_papers.json')
            failed_file.write_text(json.dumps(self.failed_papers, indent=2))
            print(f"\n‚ö†Ô∏è  {len(self.failed_papers)} papers failed extraction")
            print(f"See {failed_file} for details")

class BuildTimeEstimator:
    """Adaptive build time estimation."""

    def __init__(self):
        self.cache_file = Path('kb_data/.timing_cache.json')
        self.timings = self._load_timings()

    def _load_timings(self) -> Dict:
        """Load historical timing data."""
        if self.cache_file.exists():
            return json.loads(self.cache_file.read_text())

        # Default estimates (seconds)
        return {
            'grobid_per_page': 0.5,
            'pages_per_paper': 15,
            'embedding_per_paper': 2.0,
            's2_api_per_paper': 1.0,
            'basic_entity_extraction': 0.5,
            'advanced_entity_extraction': 1.0
        }

    def estimate(self, num_papers: int) -> Dict:
        """Estimate build time with confidence interval."""

        seconds = 0

        # Calculate based on enabled features
        if FEATURE_FLAGS['grobid_extraction']:
            pages = num_papers * self.timings['pages_per_paper']
            seconds += pages * self.timings['grobid_per_page']

        if FEATURE_FLAGS['basic_entities']:
            seconds += num_papers * self.timings['basic_entity_extraction']

        if FEATURE_FLAGS['advanced_entities']:
            seconds += num_papers * self.timings['advanced_entity_extraction']

        if FEATURE_FLAGS['semantic_scholar']:
            seconds += num_papers * self.timings['s2_api_per_paper']

        # Always need embeddings
        seconds += num_papers * self.timings['embedding_per_paper']

        # Add overhead and calculate range
        overhead = 1.2  # 20% overhead
        uncertainty = 0.3  # ¬±30%

        hours = (seconds * overhead) / 3600
        min_hours = hours * (1 - uncertainty)
        max_hours = hours * (1 + uncertainty)

        return {
            'estimated_hours': hours,
            'range': (min_hours, max_hours),
            'formatted': f"{hours:.1f} hours ({min_hours:.1f} - {max_hours:.1f} hours)",
            'unattended_safe': max_hours < 10,
            'weekend_needed': max_hours > 10
        }

    def update_from_run(self, actual_timings: Dict):
        """Update estimates based on actual performance."""
        alpha = 0.3  # Learning rate

        for key, value in actual_timings.items():
            if key in self.timings:
                # Exponential moving average
                self.timings[key] = (alpha * value +
                                   (1 - alpha) * self.timings[key])

        # Save updated timings
        self.cache_file.parent.mkdir(exist_ok=True)
        self.cache_file.write_text(json.dumps(self.timings, indent=2))
```

### Entity Extraction Implementation

```python
def extract_all_grobid_entities(xml: str) -> Dict:
    """Extract 50+ entity types from Grobid XML."""
    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}
    root = ET.fromstring(xml)

    return {
        # Methodology
        'sample_sizes': extract_sample_sizes(root, ns),
        'p_values': extract_p_values(root, ns),
        'confidence_intervals': extract_ci(root, ns),
        'study_type': detect_study_type(root, ns),

        # Software & Data
        'software': extract_software(root, ns),
        'datasets': extract_datasets(root, ns),
        'data_availability': extract_data_availability(root, ns),

        # Quality indicators
        'figures_count': len(root.findall('.//tei:figure', ns)),
        'tables_count': len(root.findall('.//tei:table', ns)),
        'references_count': len(root.findall('.//tei:ref[@type="bibr"]', ns)),
    }
```

### Semantic Scholar API Integration

```python
class AdaptiveRateLimiter:
    """Adaptive rate limiting for S2 API compliance."""

    def __init__(self):
        self.delay = 0.1  # Start optimistic
        self.min_delay = 0.1
        self.max_delay = 5.0
        self.success_count = 0
        self.rate_limit_count = 0

    def wait(self):
        """Wait before next API call."""
        time.sleep(self.delay)

    def on_success(self):
        """Speed up after sustained success."""
        self.success_count += 1
        if self.success_count > 10:
            # Gradually speed up
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
            logger.debug(f"Rate limiter: speeding up to {self.delay:.2f}s")

    def on_rate_limit(self):
        """Slow down on rate limit error."""
        self.rate_limit_count += 1
        self.delay = min(self.max_delay, self.delay * 2.0)
        self.success_count = 0
        logger.warning(f"Rate limited ({self.rate_limit_count} times). ")
        logger.warning(f"Slowing to {self.delay:.2f}s delay")

    def get_stats(self) -> Dict:
        """Get rate limiting statistics."""
        return {
            'current_delay': self.delay,
            'rate_limit_hits': self.rate_limit_count,
            'optimal': self.delay <= self.min_delay * 1.5
        }

class S2Client:
    """Semantic Scholar client with resilient API handling."""

    def __init__(self, api_key: Optional[str] = None):
        self.rate_limiter = AdaptiveRateLimiter()
        self.session = requests.Session()
        if api_key:
            self.session.headers['x-api-key'] = api_key

    def get_paper(self, doi: str, max_retries: int = 3) -> Optional[Dict]:
        """Get paper with adaptive rate limiting.
        
        Requests 'embedding' field to get SPECTER2 vectors.
        """
        fields = "paperId,title,abstract,embedding,citationCount,influentialCitationCount"
        
        for attempt in range(max_retries):
            self.rate_limiter.wait()

            try:
                response = self.session.get(
                    f"https://api.semanticscholar.org/graph/v1/paper/{doi}",
                    params={"fields": fields},
                    timeout=10
                )

                if response.status_code == 200:
                    self.rate_limiter.on_success()
                    data = response.json()
                    # Note: 'embedding' field may be None for some papers
                    return data
                elif response.status_code == 429:
                    self.rate_limiter.on_rate_limit()
                    if attempt < max_retries - 1:
                        time.sleep(10 * (attempt + 1))  # Additional backoff
                elif response.status_code == 404:
                    return None  # Paper not found

            except requests.RequestException as e:
                logger.error(f"S2 API error: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)

        return None
```

### Advanced Grobid Endpoints (Phase 2-3)

```python
# For Phase 2: Enhanced Reference Processing
def fix_broken_references(reference_text: str) -> List[Dict]:
    """Use processReferences endpoint for malformed citations."""
    response = requests.post(
        "http://localhost:8070/api/processReferences",
        data={'references': reference_text},
        timeout=5
    )
    return parse_reference_xml(response.text)

# For Phase 2: Fast Metadata Screening
def quick_metadata_only(pdf_path: Path) -> Dict:
    """10x faster - just header extraction for pre-filtering."""
    with open(pdf_path, 'rb') as f:
        response = requests.post(
            "http://localhost:8070/api/processHeaderDocument",
            files={'input': f},
            timeout=5
        )
    # Returns: title, abstract, authors, affiliations
    return parse_header_xml(response.text)

# For Phase 3: Batch Citation Processing
def batch_process_citations(citations: List[str]) -> List[Dict]:
    """Process 100+ citations in one API call."""
    response = requests.post(
        "http://localhost:8070/api/processCitationList",
        data={'citations': '\n'.join(citations)},
        timeout=10
    )
    return parse_citations_xml(response.text)
```

### Enhanced Quality Score with Transparency

```python
def calculate_v5_quality_score(paper, grobid, s2) -> tuple[int, Dict]:
    """Comprehensive quality score with detailed explanation."""

    score = 0
    components = {}

    # Impact (30 points)
    citations = s2.get('citation_count', 0)
    impact_score = 0
    if citations > 100:
        impact_score = 15
        impact_reason = f"{citations} citations (high impact)"
    elif citations > 50:
        impact_score = 12
        impact_reason = f"{citations} citations (moderate impact)"
    elif citations > 10:
        impact_score = 8
        impact_reason = f"{citations} citations (some impact)"
    else:
        impact_reason = f"{citations} citations (limited impact)"

    score += impact_score
    components['impact'] = {
        'earned': impact_score,
        'max': 15,
        'reason': impact_reason
    }

    # Methodology (25 points)
    sample_size = get_max_sample_size(grobid)
    if sample_size > 1000: score += 10
    if 'rct' in grobid.get('study_type', '').lower(): score += 9
    if grobid.get('p_values'): score += 3

    # Reproducibility (20 points)
    if grobid.get('data_availability', {}).get('data_url'): score += 8
    if grobid.get('code_url'): score += 7
    if s2.get('is_open_access'): score += 5

    # Author credibility (10 points)
    max_h = s2.get('max_h_index', 0)
    if max_h > 50: score += 10
    elif max_h > 20: score += 7

    # Completeness (10 points)
    completeness = grobid.get('extraction_completeness', 0)
    score += int(completeness * 10)

    # Recency (5 points)
    if paper.get('year', 0) >= 2023: score += 5

    final_score = min(score, 100)

    # Generate explanation
    explanation = generate_score_explanation(final_score, components)

    return final_score, explanation

def generate_score_explanation(score: int, components: Dict) -> str:
    """Generate human-readable score explanation."""

    explanation = [f"Quality Score: {score}/100\n"]
    explanation.append("Breakdown:")

    for component, details in components.items():
        pct = (details['earned'] / details['max'] * 100) if details['max'] > 0 else 0
        explanation.append(
            f"‚Ä¢ {component.title()}: {details['earned']}/{details['max']} "
            f"({pct:.0f}%) - {details['reason']}"
        )

    # Add grade
    if score >= 85:
        grade = "A+ (Excellent)"
    elif score >= 70:
        grade = "A (Good)"
    elif score >= 60:
        grade = "B (Above Average)"
    elif score >= 45:
        grade = "C (Average)"
    else:
        grade = "D (Below Average)"

    explanation.append(f"\nOverall Grade: {grade}")

    return '\n'.join(explanation)
```

### Entity-Aware Search

```python
class EntityAwareSearch:
    def search(self, 
               query: str,
               study_type: Optional[str] = None,
               min_sample: Optional[int] = None,
               max_p_value: Optional[float] = None,
               software: Optional[List[str]] = None,
               datasets: Optional[List[str]] = None,
               year_from: Optional[int] = None) -> List[Dict]:
        """
        Search with explicit entity filters.
        
        Args:
            query: Semantic search text
            study_type: Filter by study type (RCT, cohort, etc.)
            min_sample: Minimum sample size
            max_p_value: Maximum p-value threshold
            software: List of required software
            datasets: List of required datasets
            year_from: Papers published after this year
        """
        # Stage 1: Entity database filtering (fast)
        filtered_ids = self.entity_db.filter(
            study_type=study_type,
            min_sample=min_sample,
            max_p_value=max_p_value,
            software=software,
            datasets=datasets,
            year_from=year_from
        )
        
        # Stage 2: Multi-factor scoring on filtered papers
        papers = self.load_papers(filtered_ids)
        
        for paper in papers:
            score = 0
            
            # Semantic similarity (can use multiple embeddings)
            if paper.get('s2_embedding'):
                # Prefer S2 if available (trained on citations)
                score += compute_similarity(query, paper['s2_embedding']) * 0.4
            else:
                # Fallback to local MPNet
                score += compute_similarity(query, paper['title_abstract']) * 0.4
            
            score += self.compute_entity_relevance(paper) * 0.3
            score += paper['quality_score'] * 0.2
            score += self.compute_recency_score(paper['year']) * 0.1
            
            paper['search_score'] = score

        return sorted(papers, key=lambda p: p['search_score'], reverse=True)
```

---

## Success Metrics

### Phase 1: Core Release Criteria (v5.0-core)

- ‚úÖ **95% extraction accuracy** (THE critical fix)
- ‚úÖ Unattended operation (no prompts)
- ‚úÖ Checkpoint/resume works
- ‚úÖ Progress visualization for morning check
- ‚úÖ Build time estimation
- ‚úÖ Basic entity extraction (bonus value)

**MUST ship after Week 4 - this fixes the product**

### Phase 2: Enhanced Release (v5.0-full)

- ‚úÖ All Phase 1 features
- ‚úÖ Advanced entity extraction
- ‚úÖ Semantic Scholar integration
- ‚úÖ Multi-level embeddings
- ‚úÖ Entity-based filtering

**Ship if ready by Week 6**

### Target Metrics by Phase

#### Phase 1 (Core)

- üìä Extraction accuracy: >95%
- üìä Sample sizes extracted: >30% of papers
- üìä Study types classified: >50% of papers
- üìä Processing time: <8 hours for 2000 papers
- üìä Progress logging: 100% coverage

#### Phase 2 (Enhanced)

- üìä Sample sizes extracted: >50% of papers
- üìä Study types classified: >70% of papers
- üìä Processing time: 4-6 hours for 2000 papers
- üìä Memory usage: <3GB peak

#### Phase 3 (Deferred to v5.1 if needed)

- ‚ùå Smart discovery system
- ‚ùå Citation network analysis
- ‚ùå Comprehensive gap analysis
- ‚ùå Methods matrix visualization

---

## Exit Codes & Automation

```bash
# Exit codes for scripts/automation
0 = Success
1 = Requirements not met (Docker/Zotero not available)
2 = Extraction failed (Grobid errors, PDF issues)

# Example manual weekly rebuild
python src/build.py --rebuild

# For automation (cron/systemd), output is automatically logged
# The script always completes without prompts
```

---

## Monitoring & Reliability

### Automatic Report Generation

At the end of every build, v5.0 automatically generates comprehensive reports:

1. **PDF Quality Report** (when issues detected)
   - Papers missing PDFs entirely
   - Papers with small PDFs (<5KB extracted text)
   - Papers without DOIs (limited to basic quality scoring)
   - Books and proceedings (tracked separately in Phase 3)
   - Actionable recommendations for fixing issues
   - Location: `exports/analysis_pdf_quality.md`
   - Only generated if PDF issues are found

2. **Gap Analysis Report** (default, skip with `--no-gaps`)
   - Comprehensive citation network analysis
   - Papers cited by your KB but missing from collection
   - Recent work from authors in your KB
   - Papers frequently co-cited with your collection
   - Recent developments in your research areas
   - Semantically similar papers you don't have
   - Takes 15-25 minutes to run
   - Location: `exports/gap_analysis_YYYYMMDD_HHMMSS.md`
   - Skipped for demo builds or with `--no-gaps` flag

```python
def generate_pdf_quality_report(papers: List[Dict]) -> Path:
    """Generate PDF quality report for papers with issues.
    
    Comprehensive report covering:
    - Missing PDFs (no full text extracted)
    - Small PDFs (<5KB text, likely supplementary material)
    - Papers without DOIs (limited to basic quality scoring)
    - Recommendations for fixing each issue type
    
    Returns path to generated markdown report.
    """
    from pathlib import Path
    from datetime import datetime, UTC
    
    # Categorize papers by PDF status
    missing_pdfs = [p for p in papers if not p.get('full_text')]
    small_pdfs = [p for p in papers if 0 < len(p.get('full_text', '')) < 5000]
    no_doi_papers = [p for p in papers if not p.get('doi')]
    books_and_proceedings = [p for p in papers if p.get('item_type') in ['book', 'bookSection']]
    good_pdfs = [p for p in papers if len(p.get('full_text', '')) >= 5000]
    
    # Build comprehensive report
    report = []
    report.append("# PDF Quality Report\n")
    report.append(f"Generated: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M UTC')}\n")
    
    # Summary statistics
    total = len(papers)
    report.append("## Summary Statistics\n")
    report.append(f"- Total papers: {total:,}")
    report.append(f"- Papers with good PDFs: {len(good_pdfs):,} ({len(good_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers missing PDFs: {len(missing_pdfs):,} ({len(missing_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers with small PDFs: {len(small_pdfs):,} ({len(small_pdfs)*100/total:.1f}%)")
    report.append(f"- Papers without DOIs: {len(no_doi_papers):,} ({len(no_doi_papers)*100/total:.1f}%)")
    if books_and_proceedings:
        report.append(f"- Books/Proceedings: {len(books_and_proceedings):,} ({len(books_and_proceedings)*100/total:.1f}%)")
    
    # Detailed sections for each issue type
    # ... (additional report generation logic)
    
    # Save report
    exports_dir = Path('exports')
    exports_dir.mkdir(exist_ok=True)
    report_path = exports_dir / 'analysis_pdf_quality.md'
    
    with report_path.open('w') as f:
        f.write('\n'.join(report))
    
    return report_path

def run_gap_analysis() -> int:
    """Run comprehensive gap analysis using gaps.py defaults."""
    import subprocess
    
    # Run gaps.py with NO flags (uses comprehensive defaults)
    print("  Analyzing citation networks...")
    print("  Finding missing papers from your research areas...")
    
    result = subprocess.run(
        ['python', 'src/gaps.py'],  # No flags = comprehensive analysis
        capture_output=True,
        text=True
    )
    
    if result.returncode == 0:
        # gaps.py saves its own timestamped report
        # Just parse and return count for summary
        gap_count = parse_gap_count(result.stdout)
        return gap_count
    else:
        print("  Warning: Gap analysis encountered issues")
        return 0
```

### Build Monitoring

```python
class BuildMonitor:
    """Monitor build progress and health."""

    def __init__(self, log_dir: Path = Path('kb_data/monitoring')):
        self.log_dir = log_dir
        self.log_dir.mkdir(exist_ok=True)
        self.metrics_file = self.log_dir / 'build_metrics.jsonl'
        self.start_time = time.time()
        self.papers_processed = 0
        self.failures = []

    def record_paper(self, paper_id: str, success: bool, duration: float):
        """Record per-paper metrics."""
        self.papers_processed += 1

        metric = {
            'timestamp': datetime.now().isoformat(),
            'paper_id': paper_id,
            'success': success,
            'duration_seconds': duration,
            'total_processed': self.papers_processed,
            'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024
        }

        with open(self.metrics_file, 'a') as f:
            f.write(json.dumps(metric) + '\n')

        if not success:
            self.failures.append(paper_id)

    def generate_summary(self) -> str:
        """Generate build summary for morning review."""
        elapsed = time.time() - self.start_time
        success_rate = (self.papers_processed - len(self.failures)) / self.papers_processed * 100

        summary = [
            "="*50,
            "BUILD SUMMARY",
            "="*50,
            f"Total Papers: {self.papers_processed}",
            f"Successful: {self.papers_processed - len(self.failures)}",
            f"Failed: {len(self.failures)}",
            f"Success Rate: {success_rate:.1f}%",
            f"Total Time: {elapsed/3600:.1f} hours",
            f"Avg Time/Paper: {elapsed/self.papers_processed:.1f} seconds",
        ]

        if self.failures:
            summary.append(f"\n‚ö†Ô∏è Failed papers saved to: kb_data/failed_papers.json")

        return '\n'.join(summary)

    def check_health(self) -> Dict:
        """Check system health during build."""
        process = psutil.Process()

        return {
            'memory_usage_mb': process.memory_info().rss / 1024 / 1024,
            'memory_percent': process.memory_percent(),
            'cpu_percent': process.cpu_percent(),
            'disk_free_gb': psutil.disk_usage('/').free / 1024**3,
            'healthy': process.memory_percent() < 80
        }
```

### Error Recovery Strategy

```python
# Configuration for resilient processing
ERROR_RECOVERY_CONFIG = {
    'max_retries': 3,
    'retry_delays': [60, 120, 300],  # seconds
    'retryable_errors': [
        requests.Timeout,
        requests.ConnectionError,
        'HTTP 502',
        'HTTP 503',
        'HTTP 429'  # Rate limit
    ],
    'checkpoint_frequency': 50,  # Save every N papers
    'health_check_frequency': 100,  # Check resources every N papers
}

# Automatic feature handling with graceful degradation
def get_quality_score(paper, grobid, s2=None):
    """Always try best available scoring."""
    if s2 and s2_api_available():
        return calculate_v5_quality_score(paper, grobid, s2)
    else:
        logger.info("S2 unavailable, using basic scoring")
        return calculate_basic_quality_score(paper)
```

---

## Philosophy

### Core Principles

1. **Fix the bug first** - 95% extraction accuracy is the primary goal
2. **Manual processing model** - Designed for manual builds, not automated cron jobs
3. **Fail clearly** - No silent degradation, clear error messages
4. **Automatic operation** - Completes without prompts when manually triggered
5. **Progressive enhancement** - Core fix ships first, features can wait
6. **Data integrity** - Better to fail than corrupt data
7. **Flexible scope** - Process entire library or specific collections as needed

### Design Rationale

#### Why No SQLite Backend?

The entity database uses in-memory pandas DataFrames instead of SQLite because:

1. **Manual builds only** - No concurrent access or multi-user scenarios
2. **Small scale** - 2K papers = ~2MB in memory (trivial for modern systems)
3. **Simpler architecture** - No database files, connections, or schema migrations
4. **Faster development** - pandas vectorized operations are sufficient
5. **No persistence needed** - Entities rebuilt from source during each build

SQLite would add complexity without benefits for a single-user, manually-triggered system.

#### Why No PyMuPDF Fallback?

When Grobid extraction fails, we mark the paper as failed rather than falling back to PyMuPDF:

1. **Data quality over quantity** - PyMuPDF has 50% failure rate on structure preservation
2. **Clear failure signals** - Users need to know which papers failed extraction
3. **Prevent silent degradation** - Bad extractions pollute search results
4. **Debugging clarity** - Easier to diagnose why specific papers failed
5. **Future improvement path** - Failed papers can be re-processed when Grobid improves

Better to have 95% of papers extracted perfectly than 100% with half being unreliable.

### Phased Development Approach

#### Phase 1 (Week 1-4): Ship Core Fix

- Week 1-2: **Infrastructure** (unattended operation, progress logging)
- Week 3-4: **Core extraction** (Grobid integration, basic entities)
- **Ship Point**: If successful, release v5.0-core

#### Phase 2 (Week 5-6): Add Value

- Week 5: **Enhanced extraction** (advanced entities, S2 integration)
- Week 6: **Smart search** (multi-embeddings, entity-aware search)
- **Ship Point**: If successful, release v5.0-full

#### Phase 3 (Week 7-8): Advanced Features

- Week 7: **Discovery** (if time permits)
- Week 8: **Polish** (documentation, optimization)
- **Defer Point**: Move to v5.1 if behind schedule

### What Success Looks Like

```
Week 3: "Extraction ACTUALLY WORKS! No more mangled methods sections!"
Week 4: "I can finally TRUST the extraction! Plus sample sizes!"
Week 5: "Citation data included? This is getting good!"
Week 6: "Entity-aware search? Now we're talking!"
Week 7: "It found papers using the same datasets? Amazing!"
Week 8: "From 'unreliable' to 'indispensable' - shipped!"
```

### User Value Proposition

```
BEFORE v5.0: "I hope it extracts my papers correctly" (50% fail)
AFTER v5.0:  "I trust it to extract my papers correctly" (95% work)

One weekend investment = Reliable extraction forever
```

## Quick Start Guide

### For Developers

```bash
# 1. Setup environment
git checkout -b v5-implementation
pip install -r requirements-dev.txt

# 2. Start Grobid
docker-compose up -d grobid

# 3. Run tests
pytest tests/unit/test_grobid_extraction.py

# 4. Test extraction on sample
python src/build.py --demo
```

### For Users

```bash
# Weekend setup (one-time)
docker pull grobid/grobid:0.7.3
python src/build.py --rebuild

# Daily use
python src/build.py                           # Update entire library
python src/build.py --collection "Current"    # Update specific collection
python src/kbq.py search "diabetes"           # Research
```

### Migration from v4.6

#### Command Changes

| v4.6 Command | v5.0 Equivalent | Notes |
|--------------|-----------------|-------|
| `python src/cli.py` | `python src/kbq.py` | All commands preserved, plus new entity filters |
| `python src/build_kb.py` | `python src/build.py` | Removed path config flags (now uses defaults) |
| `python src/analyze_gaps.py` | `python src/gaps.py` | Same functionality, shorter name |
| `python src/discover.py` | `python src/discover.py` | No change, all flags preserved |

#### Removed Flags (No Longer Needed)

**Configuration flags removed:**

- `--api-url`, `--knowledge-base-path`, `--zotero-data-dir` - Uses standard paths
- `--yes`, `--auto-start` - Always automatic, no prompts
- `--continue` - Auto-resumes from checkpoint if exists
- `--skip-quality` - Auto-fallback if S2 unavailable
- `--quiet`, `--verbose` - Single output mode optimized for manual runs
- `--phase`, `--features` - Always uses all available features

#### New v5.0 Build Flags

**Collection management (NEW):**

- `--collection NAME` - Process only papers from specified Zotero collection
  - Useful for testing on subset of papers
  - Faster builds when working on specific research areas
  - Collection name must match exactly (case-sensitive)
  - Can combine with other flags like `--rebuild` or `--no-gaps`

#### New v5.0 Features

- **Entity-aware search**: Filter by study type, sample size, p-values, software, datasets
- **Collection processing**: Build KB from specific Zotero collections with `--collection`
- **Automatic operation**: No prompts, auto-starts Grobid, resumes from checkpoint
- **Progress tracking**: `--estimate`, `--progress` for checking status
- **Batch operations preserved**: 10-20x performance boost still available

#### Migration Steps

```bash
# 1. Export current KB (optional backup)
python src/build.py --export kb_v4_backup.json

# 2. Clean slate for v5
rm -rf kb_data/

# 3. Rebuild with v5
git pull
python src/build.py --rebuild

```
