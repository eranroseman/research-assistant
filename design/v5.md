# Research Assistant v5.0: Three-System Integration

## Table of Contents
1. [Overview](#overview)
2. [Breaking Changes](#breaking-changes)
3. [Installation & Setup](#installation--setup)
4. [Core Features](#core-features)
5. [System Architecture](#system-architecture)
6. [Usage Guide](#usage-guide)
7. [Implementation Plan](#implementation-plan)
8. [Technical Details](#technical-details)
9. [Success Metrics](#success-metrics)

---

## Overview

### What is v5.0?
Research Assistant v5.0 is a **critical bug fix and enhancement** that transforms the product from "might work" (50% extraction success) to "will work" (95% extraction success).

It integrates three powerful systems:

1. **Zotero** - Paper management and PDF storage
2. **Grobid** - AI-powered structure and entity extraction (95% accuracy)
3. **Semantic Scholar** - Citation metrics and AI-generated summaries

### Why v5.0?
The current v4.6 has a **product-breaking bug**:
- ‚ùå **50% extraction failure rate** - PyMuPDF fundamentally cannot preserve document structure
- ‚ùå **Unreliable extraction** - Users cannot trust the output
- ‚ùå **No citation networks** - Can't find related papers
- ‚ùå **Limited discovery** - Only keyword matching

**This is not a feature addition - it's fixing a critical defect.**

### What v5.0 Delivers
- ‚úÖ **95% extraction accuracy** with Grobid
- ‚úÖ **50+ entity types extracted** (sample sizes, p-values, methods, datasets)
- ‚úÖ **Multi-level embeddings** for intelligent search
- ‚úÖ **Entity-aware discovery** (find papers using same methods/datasets)
- ‚úÖ **Comprehensive gap analysis** (methodology, reproducibility, evidence levels)
- ‚úÖ **Unattended operation** for overnight processing

---

## Breaking Changes

### ‚ö†Ô∏è IMPORTANT: Complete Rebuild Required
```bash
# One-time weekend investment for reliable extraction forever
rm -rf kb_data/              # Delete existing KB
git pull                      # Update to v5
pip install -r requirements.txt
python src/build_kb.py --rebuild --yes --auto-start  # 4-6 hour rebuild

# Why rebuild? Because you're getting:
# ‚Ä¢ 50% ‚Üí 95% extraction accuracy
# ‚Ä¢ Structured data that won't need re-extraction
# ‚Ä¢ Future-proof XML storage for new entity mining
```

### Why No Migration?
- Different extraction format (Grobid XML vs plain text)
- New entity storage structure
- Enhanced embedding system
- Incompatible quality scores

---

## Installation & Setup

### Prerequisites
```bash
# 1. Install Docker
sudo apt install docker.io  # Ubuntu/Debian
brew install docker         # macOS

# 2. Install Grobid (one-time)
docker pull grobid/grobid:0.7.3
docker run -d \
  -p 8070:8070 \
  --name grobid \
  --restart unless-stopped \
  --mem="2g" \
  grobid/grobid:0.7.3

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Verify setup
python src/check_requirements.py  # Exit 0 = ready
```

### System Requirements
```yaml
Minimum Requirements:
  RAM: 4 GB (for <1000 papers)
  Disk: 10 GB free (for XML cache)
  CPU: 2 cores
  Network: 10 Mbps (for S2 API)

Recommended:
  RAM: 8 GB (for 2000+ papers)
  Disk: 20 GB free (includes all caches)
  CPU: 4+ cores
  Network: 50+ Mbps

Grobid Docker:
  RAM: 2 GB allocated to container
  Disk: 5 GB for Docker image

Data Usage:
  ~500 KB per paper (S2 API calls)
  ~100 KB per paper (XML cache)
  ~50 KB per paper (embeddings)

API Limits:
  S2 Unauthenticated: 1 request/second
  S2 Batch: 100 papers per request
  Grobid: No hard limits (local Docker)
```

### First Build - The Academic Workflow
```bash
# Friday afternoon workflow:
python src/build_kb.py --estimate
# Output: "Estimated time: 4.5 hours (3.2 - 5.9 hours)"
#         "‚úì Safe for overnight run"

# Start before leaving:
python src/build_kb.py --rebuild --yes --auto-start

# Monday morning: Complete KB ready for research

# What happens:
# 1. Checks all systems are running
# 2. Shows time estimate and starts
# 3. Fetches papers from Zotero
# 4. Extracts structure with Grobid (4-6 hours)
# 5. Logs progress to kb_data/build_progress.log
# 6. Enhances with Semantic Scholar data
# 7. Creates multi-level embeddings
# 8. Builds FAISS indices

# Check progress in the morning
cat kb_data/build_progress.txt
# Build Progress: 1847/2000 (92.4%)
# Current: paper_1847.pdf
# Elapsed: 4.2 hours
# ETA: 0.3 hours
# Rate: 440 papers/hour
```

---

## Core Features

### 1. Entity Extraction (50+ Types)

#### From Grobid
- **Methodology**: Sample sizes, p-values, confidence intervals, study types
- **Software & Data**: Software versions, datasets, data availability
- **Clinical**: Trial IDs, organisms, diseases
- **Institutional**: Funders, affiliations, countries

#### From Semantic Scholar
- **Impact**: Citation count, influential citations, citation velocity
- **AI Insights**: TLDR summaries, field classifications
- **Author Metrics**: H-index, author citation counts
- **Access**: Open access status, PDF availability

### 2. Multi-Level Embeddings

```python
# Each paper has 5 embedding types:
{
    'full_text': standard_embedding,        # Title + abstract
    'enriched': entity_aware_embedding,     # Includes all entities
    's2_embedding': semantic_scholar_vector, # Pre-computed by S2
    'methods': methods_section_embedding,    # For methodology similarity
    'metadata_vector': numerical_features    # For filtering
}
```

### 3. Entity-Aware Search

```bash
# Understands research requirements
python src/research.py search "RCT with n>500 using R"

# Parsed as:
# - Study type: RCT
# - Minimum sample: 500
# - Software: R
```

### 4. Smart Discovery

```bash
# Find papers using your methods
python src/research.py discover --similar-methods

# Find potential replications
python src/research.py discover --same-datasets

# Fill methodology gaps
python src/research.py discover --fill-gaps
```

### 5. Comprehensive Analysis

```bash
# Paper dashboard with all metrics
python src/research.py paper-dashboard 0001

# Reproducibility assessment
python src/research.py reproducibility-report

# Methods matrix (papers √ó tools)
python src/research.py methods-matrix

# Multi-dimensional gap analysis
python src/research.py gaps --comprehensive
```

---

## System Architecture

### Data Flow
```
1. ZOTERO (Source)
   ‚îú‚îÄ‚îÄ PDFs
   ‚îú‚îÄ‚îÄ Metadata
   ‚îî‚îÄ‚îÄ Collections
        ‚Üì
2. GROBID (Extraction)
   ‚îú‚îÄ‚îÄ Structured sections
   ‚îú‚îÄ‚îÄ References & citations
   ‚îú‚îÄ‚îÄ Entities & metadata
   ‚îî‚îÄ‚îÄ Study characteristics
        ‚Üì
3. SEMANTIC SCHOLAR (Enhancement)
   ‚îú‚îÄ‚îÄ Citation metrics
   ‚îú‚îÄ‚îÄ AI summaries (TLDR)
   ‚îú‚îÄ‚îÄ Author metrics
   ‚îî‚îÄ‚îÄ Related papers
        ‚Üì
4. KNOWLEDGE BASE
   ‚îú‚îÄ‚îÄ FAISS indices (search)
   ‚îú‚îÄ‚îÄ Entity database (filtering)
   ‚îî‚îÄ‚îÄ Citation network (discovery)
```

### Storage Structure
```
kb_data/
‚îú‚îÄ‚îÄ papers/           # Markdown files with full content
‚îú‚îÄ‚îÄ cache/
‚îÇ   ‚îú‚îÄ‚îÄ grobid/      # XML extractions
‚îÇ   ‚îú‚îÄ‚îÄ s2/          # API responses
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/  # Computed vectors
‚îú‚îÄ‚îÄ indices/
‚îÇ   ‚îú‚îÄ‚îÄ semantic.faiss    # Main search
‚îÇ   ‚îú‚îÄ‚îÄ methods.faiss     # Method similarity
‚îÇ   ‚îî‚îÄ‚îÄ metadata.faiss    # Entity filtering
‚îî‚îÄ‚îÄ metadata.json    # Complete paper data
```

---

## Usage Guide

### Daily Workflow

#### Morning: Check for New Papers
```bash
# Incremental update (fast, no prompts)
python src/build_kb.py

# If quality scores need upgrading
python src/build_kb.py --quality-upgrade --yes
```

#### Research Tasks
```bash
# Search with filters
python src/research.py search "diabetes" \
  --min-sample 500 \
  --study-type RCT \
  --has-data

# Discover related work
python src/research.py discover \
  --similar-methods \
  --min-quality 80

# Analyze gaps
python src/research.py gaps --comprehensive
```

#### Overnight Processing
```bash
# Add to crontab for weekly rebuild
0 2 * * 0 python /path/to/src/build_kb.py --rebuild --yes --auto-start --quiet
```

### Command Reference

#### build_kb.py - Knowledge Base Management
```bash
# Flags for unattended operation
--rebuild           # Full rebuild (requires --yes)
--yes, -y          # Skip all confirmations
--auto-start       # Auto-start Docker/Grobid
--continue         # Resume from checkpoint
--estimate         # Show time estimate only
--skip-quality     # Skip S2 if unavailable
--quiet, -q        # Silent mode for cron
--demo             # 5-paper test mode
--export FILE      # Export KB
--import FILE      # Import KB
--features         # Show enabled features
--progress         # Show current progress
--phase [1|2|3]     # Limit to specific phase features

# Examples:
python src/build_kb.py --rebuild --yes --phase 1  # Core only
python src/build_kb.py --estimate                  # Check time
python src/build_kb.py --continue                  # Resume failed build
python src/build_kb.py --features                  # Show what's enabled
```

#### research.py - Research Interface
```bash
# Search commands
search QUERY [--min-sample N] [--study-type TYPE] [--has-data]
smart-search QUERY [-k NUM]

# Discovery commands
discover [--similar-methods] [--same-datasets] [--fill-gaps]

# Analysis commands
gaps [--comprehensive]
reproducibility-report
methods-matrix
paper-dashboard ID

# Citation commands
cite ID1 ID2 ID3
```

---

## Implementation Plan

### Phase 1: Core Deliverables (Week 1-4) - MUST SHIP
- [ ] Remove all user prompts from build_kb.py
- [ ] Implement GrobidExtractor with proper XML parsing
- [ ] Add ProcessingCheckpoint for resume capability
- [ ] Create build time estimator with adaptive learning
- [ ] Add progress visualization for overnight runs
- [ ] Extract basic entities (sample sizes, study types)
- [ ] Implement ResilientExtractor with retry logic
- [ ] Add BuildMonitor for health tracking
- [ ] Unattended CLI interface with all flags

**Ship v5.0-core if Week 4 successful**

### Phase 2: Enhanced Features (Week 5-6) - SHIP IF READY
- [ ] Advanced entity extraction (p-values, datasets, software)
- [ ] Semantic Scholar API integration with AdaptiveRateLimiter
- [ ] S2 batch processing (400x efficiency improvement)
- [ ] Multi-level embeddings (5 types per paper)
- [ ] Entity-aware search parsing ("RCT with n>500")
- [ ] Quality score transparency (explanations)

**Decision Point**: If behind schedule, ship without Phase 2

### Phase 3: Discovery Features (Week 7-8) - DEFER IF NEEDED
- [ ] Smart discovery system
- [ ] Comprehensive gap analysis
- [ ] Reproducibility reports
- [ ] Methods matrix
- [ ] Citation network

**Can ship as v5.1 if not ready**

---

## Technical Details

### Feature Flags for Partial Deliverables

```python
FEATURE_FLAGS = {
    # Phase 1: Core (Week 1-4) - MUST SHIP
    'grobid_extraction': True,      # Critical fix
    'basic_entities': True,          # Sample sizes, study types
    'checkpoint_resume': True,       # Required for long builds
    'progress_logging': True,        # Progress visualization
    'time_estimation': True,         # Build time estimates

    # Phase 2: Enhanced (Week 5-6) - SHIP IF READY
    'advanced_entities': False,      # P-values, software, datasets
    'semantic_scholar': False,       # Citation metrics, TLDR
    'multi_embeddings': False,       # 5-level embedding system
    'entity_search': False,          # Parse "RCT with n>500"

    # Phase 3: Discovery (Week 7-8) - DEFER IF NEEDED
    'smart_discovery': False,        # Find similar papers
    'gap_analysis': False,           # Comprehensive gaps
    'citation_network': False,       # NetworkX integration
    'reproducibility': False,        # Reproducibility reports
}

def check_feature(feature: str) -> bool:
    """Check if feature is enabled in this build."""
    if not FEATURE_FLAGS.get(feature, False):
        print(f"Feature '{feature}' not available in this version")
        print(f"Available in: v5.1")
        return False
    return True
```

### Progress Visualization System

```python
class ProgressLogger:
    """Log progress for overnight monitoring."""

    def __init__(self, log_dir='kb_data'):
        self.log_file = Path(log_dir) / 'build_progress.log'
        self.summary_file = Path(log_dir) / 'build_progress.txt'
        self.start_time = time.time()

    def log(self, current: int, total: int, paper_name: str, status: str = 'processing'):
        """Log progress with ETA calculation."""
        elapsed = time.time() - self.start_time
        rate = current / elapsed if elapsed > 0 else 0
        eta = (total - current) / rate if rate > 0 else 0

        # Detailed log entry (JSON lines for parsing)
        entry = {
            'timestamp': datetime.now().isoformat(),
            'current': current,
            'total': total,
            'paper': paper_name,
            'status': status,
            'elapsed_hours': elapsed / 3600,
            'eta_hours': eta / 3600,
            'papers_per_hour': rate * 3600
        }

        with open(self.log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

        # Human-readable summary (for morning check)
        with open(self.summary_file, 'w') as f:
            f.write(f"Build Progress: {current}/{total} ({current/total*100:.1f}%)\n")
            f.write(f"Current: {paper_name}\n")
            f.write(f"Elapsed: {elapsed/3600:.1f} hours\n")
            f.write(f"ETA: {eta/3600:.1f} hours\n")
            f.write(f"Rate: {rate*3600:.0f} papers/hour\n")

            if status != 'processing':
                f.write(f"Status: {status}\n")
```

### Build Time Estimator

```python
class ResilientExtractor:
    """Extraction with automatic retry and failure tracking."""

    MAX_RETRIES = 3
    RETRY_DELAYS = [60, 120, 300]  # Exponential backoff

    def __init__(self):
        self.failed_papers = []
        self.retry_stats = {'total_retries': 0, 'successful_retries': 0}

    def extract_with_retry(self, pdf_path: Path) -> Optional[Dict]:
        """Extract with automatic retry on transient failures."""
        last_error = None

        for attempt in range(self.MAX_RETRIES):
            try:
                result = self._extract(pdf_path)
                if attempt > 0:
                    self.retry_stats['successful_retries'] += 1
                return result

            except (requests.Timeout, requests.ConnectionError) as e:
                last_error = e
                self.retry_stats['total_retries'] += 1

                if attempt < self.MAX_RETRIES - 1:
                    delay = self.RETRY_DELAYS[attempt]
                    logger.warning(f"Attempt {attempt+1} failed for {pdf_path.name}")
                    logger.warning(f"Retrying in {delay} seconds...")
                    time.sleep(delay)
                else:
                    # Final attempt failed - track for manual review
                    self.failed_papers.append({
                        'path': str(pdf_path),
                        'error': str(e),
                        'attempts': self.MAX_RETRIES,
                        'timestamp': datetime.now().isoformat()
                    })

            except Exception as e:
                # Non-retryable error
                self.failed_papers.append({
                    'path': str(pdf_path),
                    'error': str(e),
                    'error_type': 'non_retryable',
                    'timestamp': datetime.now().isoformat()
                })
                return None

        return None

    def save_failed_papers(self):
        """Save failed papers list for manual review."""
        if self.failed_papers:
            failed_file = Path('kb_data/failed_papers.json')
            failed_file.write_text(json.dumps(self.failed_papers, indent=2))
            print(f"\n‚ö†Ô∏è  {len(self.failed_papers)} papers failed extraction")
            print(f"See {failed_file} for details")

class BuildTimeEstimator:
    """Adaptive build time estimation."""

    def __init__(self):
        self.cache_file = Path('kb_data/.timing_cache.json')
        self.timings = self._load_timings()

    def _load_timings(self) -> Dict:
        """Load historical timing data."""
        if self.cache_file.exists():
            return json.loads(self.cache_file.read_text())

        # Default estimates (seconds)
        return {
            'grobid_per_page': 0.5,
            'pages_per_paper': 15,
            'embedding_per_paper': 2.0,
            's2_api_per_paper': 1.0,
            'basic_entity_extraction': 0.5,
            'advanced_entity_extraction': 1.0
        }

    def estimate(self, num_papers: int) -> Dict:
        """Estimate build time with confidence interval."""

        seconds = 0

        # Calculate based on enabled features
        if FEATURE_FLAGS['grobid_extraction']:
            pages = num_papers * self.timings['pages_per_paper']
            seconds += pages * self.timings['grobid_per_page']

        if FEATURE_FLAGS['basic_entities']:
            seconds += num_papers * self.timings['basic_entity_extraction']

        if FEATURE_FLAGS['advanced_entities']:
            seconds += num_papers * self.timings['advanced_entity_extraction']

        if FEATURE_FLAGS['semantic_scholar']:
            seconds += num_papers * self.timings['s2_api_per_paper']

        # Always need embeddings
        seconds += num_papers * self.timings['embedding_per_paper']

        # Add overhead and calculate range
        overhead = 1.2  # 20% overhead
        uncertainty = 0.3  # ¬±30%

        hours = (seconds * overhead) / 3600
        min_hours = hours * (1 - uncertainty)
        max_hours = hours * (1 + uncertainty)

        return {
            'estimated_hours': hours,
            'range': (min_hours, max_hours),
            'formatted': f"{hours:.1f} hours ({min_hours:.1f} - {max_hours:.1f} hours)",
            'overnight_safe': max_hours < 10,
            'weekend_needed': max_hours > 10
        }

    def update_from_run(self, actual_timings: Dict):
        """Update estimates based on actual performance."""
        alpha = 0.3  # Learning rate

        for key, value in actual_timings.items():
            if key in self.timings:
                # Exponential moving average
                self.timings[key] = (alpha * value +
                                   (1 - alpha) * self.timings[key])

        # Save updated timings
        self.cache_file.parent.mkdir(exist_ok=True)
        self.cache_file.write_text(json.dumps(self.timings, indent=2))
```

### Entity Extraction Implementation

```python
def extract_all_grobid_entities(xml: str) -> Dict:
    """Extract 50+ entity types from Grobid XML."""
    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}
    root = ET.fromstring(xml)

    return {
        # Methodology
        'sample_sizes': extract_sample_sizes(root, ns),
        'p_values': extract_p_values(root, ns),
        'confidence_intervals': extract_ci(root, ns),
        'study_type': detect_study_type(root, ns),

        # Software & Data
        'software': extract_software(root, ns),
        'datasets': extract_datasets(root, ns),
        'data_availability': extract_data_availability(root, ns),

        # Quality indicators
        'figures_count': len(root.findall('.//tei:figure', ns)),
        'tables_count': len(root.findall('.//tei:table', ns)),
        'references_count': len(root.findall('.//tei:ref[@type="bibr"]', ns)),
    }
```

### Semantic Scholar API Integration

```python
class AdaptiveRateLimiter:
    """Adaptive rate limiting for S2 API compliance."""

    def __init__(self):
        self.delay = 0.1  # Start optimistic
        self.min_delay = 0.1
        self.max_delay = 5.0
        self.success_count = 0
        self.rate_limit_count = 0

    def wait(self):
        """Wait before next API call."""
        time.sleep(self.delay)

    def on_success(self):
        """Speed up after sustained success."""
        self.success_count += 1
        if self.success_count > 10:
            # Gradually speed up
            self.delay = max(self.min_delay, self.delay * 0.9)
            self.success_count = 0
            logger.debug(f"Rate limiter: speeding up to {self.delay:.2f}s")

    def on_rate_limit(self):
        """Slow down on rate limit error."""
        self.rate_limit_count += 1
        self.delay = min(self.max_delay, self.delay * 2.0)
        self.success_count = 0
        logger.warning(f"Rate limited ({self.rate_limit_count} times). ")
        logger.warning(f"Slowing to {self.delay:.2f}s delay")

    def get_stats(self) -> Dict:
        """Get rate limiting statistics."""
        return {
            'current_delay': self.delay,
            'rate_limit_hits': self.rate_limit_count,
            'optimal': self.delay <= self.min_delay * 1.5
        }

class S2Client:
    """Semantic Scholar client with resilient API handling."""

    def __init__(self, api_key: Optional[str] = None):
        self.rate_limiter = AdaptiveRateLimiter()
        self.session = requests.Session()
        if api_key:
            self.session.headers['x-api-key'] = api_key

    def get_paper(self, doi: str, max_retries: int = 3) -> Optional[Dict]:
        """Get paper with adaptive rate limiting."""

        for attempt in range(max_retries):
            self.rate_limiter.wait()

            try:
                response = self.session.get(
                    f"https://api.semanticscholar.org/v1/paper/{doi}",
                    timeout=10
                )

                if response.status_code == 200:
                    self.rate_limiter.on_success()
                    return response.json()
                elif response.status_code == 429:
                    self.rate_limiter.on_rate_limit()
                    if attempt < max_retries - 1:
                        time.sleep(10 * (attempt + 1))  # Additional backoff
                elif response.status_code == 404:
                    return None  # Paper not found

            except requests.RequestException as e:
                logger.error(f"S2 API error: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)

        return None
```

### Advanced Grobid Endpoints (Phase 2-3)

```python
# For Phase 2: Enhanced Reference Processing
def fix_broken_references(reference_text: str) -> List[Dict]:
    """Use processReferences endpoint for malformed citations."""
    response = requests.post(
        "http://localhost:8070/api/processReferences",
        data={'references': reference_text},
        timeout=5
    )
    return parse_reference_xml(response.text)

# For Phase 2: Fast Metadata Screening
def quick_metadata_only(pdf_path: Path) -> Dict:
    """10x faster - just header extraction for pre-filtering."""
    with open(pdf_path, 'rb') as f:
        response = requests.post(
            "http://localhost:8070/api/processHeaderDocument",
            files={'input': f},
            timeout=5
        )
    # Returns: title, abstract, authors, affiliations
    return parse_header_xml(response.text)

# For Phase 3: Batch Citation Processing
def batch_process_citations(citations: List[str]) -> List[Dict]:
    """Process 100+ citations in one API call."""
    response = requests.post(
        "http://localhost:8070/api/processCitationList",
        data={'citations': '\n'.join(citations)},
        timeout=10
    )
    return parse_citations_xml(response.text)
```

### Enhanced Quality Score with Transparency

```python
def calculate_v5_quality_score(paper, grobid, s2) -> tuple[int, Dict]:
    """Comprehensive quality score with detailed explanation."""

    score = 0
    components = {}

    # Impact (30 points)
    citations = s2.get('citation_count', 0)
    impact_score = 0
    if citations > 100:
        impact_score = 15
        impact_reason = f"{citations} citations (high impact)"
    elif citations > 50:
        impact_score = 12
        impact_reason = f"{citations} citations (moderate impact)"
    elif citations > 10:
        impact_score = 8
        impact_reason = f"{citations} citations (some impact)"
    else:
        impact_reason = f"{citations} citations (limited impact)"

    score += impact_score
    components['impact'] = {
        'earned': impact_score,
        'max': 15,
        'reason': impact_reason
    }

    # Methodology (25 points)
    sample_size = get_max_sample_size(grobid)
    if sample_size > 1000: score += 10
    if 'rct' in grobid.get('study_type', '').lower(): score += 9
    if grobid.get('p_values'): score += 3

    # Reproducibility (20 points)
    if grobid.get('data_availability', {}).get('data_url'): score += 8
    if grobid.get('code_url'): score += 7
    if s2.get('is_open_access'): score += 5

    # Author credibility (10 points)
    max_h = s2.get('max_h_index', 0)
    if max_h > 50: score += 10
    elif max_h > 20: score += 7

    # Completeness (10 points)
    completeness = grobid.get('extraction_completeness', 0)
    score += int(completeness * 10)

    # Recency (5 points)
    if paper.get('year', 0) >= 2023: score += 5

    final_score = min(score, 100)

    # Generate explanation
    explanation = generate_score_explanation(final_score, components)

    return final_score, explanation

def generate_score_explanation(score: int, components: Dict) -> str:
    """Generate human-readable score explanation."""

    explanation = [f"Quality Score: {score}/100\n"]
    explanation.append("Breakdown:")

    for component, details in components.items():
        pct = (details['earned'] / details['max'] * 100) if details['max'] > 0 else 0
        explanation.append(
            f"‚Ä¢ {component.title()}: {details['earned']}/{details['max']} "
            f"({pct:.0f}%) - {details['reason']}"
        )

    # Add grade
    if score >= 85:
        grade = "A+ (Excellent)"
    elif score >= 70:
        grade = "A (Good)"
    elif score >= 60:
        grade = "B (Above Average)"
    elif score >= 45:
        grade = "C (Average)"
    else:
        grade = "D (Below Average)"

    explanation.append(f"\nOverall Grade: {grade}")

    return '\n'.join(explanation)
```

### Entity-Aware Search

```python
class EntityAwareSearch:
    def search(self, query: str, papers: List[Dict]) -> List[Dict]:
        # Parse query for requirements
        entities = self._extract_query_entities(query)

        # Multi-factor scoring
        for paper in papers:
            score = 0
            score += semantic_similarity * 0.4
            score += entity_match_score * 0.3
            score += quality_score * 0.2
            score += recency_score * 0.1

        return sorted_by_score

    def _extract_query_entities(self, query: str) -> Dict:
        # Parse "RCT with n>500" ‚Üí {study_type: 'rct', min_sample: 500}
        entities = {}

        # Sample size
        if match := re.search(r'n\s*[><=]\s*(\d+)', query):
            entities['min_sample'] = int(match.group(1))

        # Study type
        if 'RCT' in query.upper():
            entities['study_type'] = 'rct'

        return entities
```

---

## Success Metrics

### Phase 1: Core Release Criteria (v5.0-core)
- ‚úÖ **95% extraction accuracy** (THE critical fix)
- ‚úÖ Unattended operation (no prompts)
- ‚úÖ Checkpoint/resume works
- ‚úÖ Progress visualization for morning check
- ‚úÖ Build time estimation
- ‚úÖ Basic entity extraction (bonus value)

**MUST ship after Week 4 - this fixes the product**

### Phase 2: Enhanced Release (v5.0-full)
- ‚úÖ All Phase 1 features
- ‚úÖ Advanced entity extraction
- ‚úÖ Semantic Scholar integration
- ‚úÖ Multi-level embeddings
- ‚úÖ Entity-aware search

**Ship if ready by Week 6**

### Target Metrics by Phase

#### Phase 1 (Core)
- üìä Extraction accuracy: >95%
- üìä Sample sizes extracted: >30% of papers
- üìä Study types classified: >50% of papers
- üìä Processing time: <8 hours for 2000 papers
- üìä Progress logging: 100% coverage

#### Phase 2 (Enhanced)
- üìä Sample sizes extracted: >50% of papers
- üìä Study types classified: >70% of papers
- üìä Processing time: 4-6 hours for 2000 papers
- üìä Memory usage: <3GB peak

#### Phase 3 (Deferred to v5.1 if needed)
- ‚ùå Smart discovery system
- ‚ùå Citation network analysis
- ‚ùå Comprehensive gap analysis
- ‚ùå Methods matrix visualization

---

## Exit Codes & Automation

```bash
# Exit codes for scripts/automation
0 = Success
1 = Requirements not met (Docker/Grobid/Zotero)
2 = Extraction failed
3 = User abort (only without --yes flag)

# Example cron job for weekly rebuild
0 2 * * 0 cd /path/to/project && python src/build_kb.py --rebuild --yes --auto-start --quiet || echo "KB rebuild failed with code $?"

# Example systemd timer for daily incremental
[Timer]
OnCalendar=daily
Persistent=true

[Service]
ExecStart=/usr/bin/python /path/to/src/build_kb.py --yes --auto-start --quiet
```

---

## Monitoring & Reliability

### Build Monitoring

```python
class BuildMonitor:
    """Monitor build progress and health."""

    def __init__(self, log_dir: Path = Path('kb_data/monitoring')):
        self.log_dir = log_dir
        self.log_dir.mkdir(exist_ok=True)
        self.metrics_file = self.log_dir / 'build_metrics.jsonl'
        self.start_time = time.time()
        self.papers_processed = 0
        self.failures = []

    def record_paper(self, paper_id: str, success: bool, duration: float):
        """Record per-paper metrics."""
        self.papers_processed += 1

        metric = {
            'timestamp': datetime.now().isoformat(),
            'paper_id': paper_id,
            'success': success,
            'duration_seconds': duration,
            'total_processed': self.papers_processed,
            'memory_mb': psutil.Process().memory_info().rss / 1024 / 1024
        }

        with open(self.metrics_file, 'a') as f:
            f.write(json.dumps(metric) + '\n')

        if not success:
            self.failures.append(paper_id)

    def generate_summary(self) -> str:
        """Generate build summary for morning review."""
        elapsed = time.time() - self.start_time
        success_rate = (self.papers_processed - len(self.failures)) / self.papers_processed * 100

        summary = [
            "="*50,
            "BUILD SUMMARY",
            "="*50,
            f"Total Papers: {self.papers_processed}",
            f"Successful: {self.papers_processed - len(self.failures)}",
            f"Failed: {len(self.failures)}",
            f"Success Rate: {success_rate:.1f}%",
            f"Total Time: {elapsed/3600:.1f} hours",
            f"Avg Time/Paper: {elapsed/self.papers_processed:.1f} seconds",
        ]

        if self.failures:
            summary.append(f"\n‚ö†Ô∏è Failed papers saved to: kb_data/failed_papers.json")

        return '\n'.join(summary)

    def check_health(self) -> Dict:
        """Check system health during build."""
        process = psutil.Process()

        return {
            'memory_usage_mb': process.memory_info().rss / 1024 / 1024,
            'memory_percent': process.memory_percent(),
            'cpu_percent': process.cpu_percent(),
            'disk_free_gb': psutil.disk_usage('/').free / 1024**3,
            'healthy': process.memory_percent() < 80
        }
```

### Error Recovery Strategy

```python
# Configuration for resilient processing
ERROR_RECOVERY_CONFIG = {
    'max_retries': 3,
    'retry_delays': [60, 120, 300],  # seconds
    'retryable_errors': [
        requests.Timeout,
        requests.ConnectionError,
        'HTTP 502',
        'HTTP 503',
        'HTTP 429'  # Rate limit
    ],
    'checkpoint_frequency': 50,  # Save every N papers
    'health_check_frequency': 100,  # Check resources every N papers
}

# Feature flag runtime checking with graceful degradation
def safe_feature_check(feature: str) -> bool:
    """Check feature availability with fallback."""
    if not FEATURE_FLAGS.get(feature, False):
        logger.warning(f"Feature '{feature}' not available, using fallback")
        return False
    return True

# Example usage:
if safe_feature_check('semantic_scholar'):
    quality_score, explanation = calculate_v5_quality_score(paper, grobid, s2)
else:
    quality_score, explanation = calculate_basic_quality_score(paper)
```

---

## Philosophy

### Core Principles
1. **Fix the bug first** - 95% extraction accuracy is the primary goal
2. **Overnight processing model** - Designed for "set Friday, use Monday" workflow
3. **Fail clearly** - No silent degradation, clear error messages
4. **Unattended operation** - Must work overnight without prompts
5. **Progressive enhancement** - Core fix ships first, features can wait
6. **Data integrity** - Better to fail than corrupt data

### Phased Development Approach

#### Phase 1 (Week 1-4): Ship Core Fix
- Week 1-2: **Infrastructure** (unattended operation, progress logging)
- Week 3-4: **Core extraction** (Grobid integration, basic entities)
- **Ship Point**: If successful, release v5.0-core

#### Phase 2 (Week 5-6): Add Value
- Week 5: **Enhanced extraction** (advanced entities, S2 integration)
- Week 6: **Smart search** (multi-embeddings, entity-aware search)
- **Ship Point**: If successful, release v5.0-full

#### Phase 3 (Week 7-8): Advanced Features
- Week 7: **Discovery** (if time permits)
- Week 8: **Polish** (documentation, optimization)
- **Defer Point**: Move to v5.1 if behind schedule

### What Success Looks Like
```
Week 3: "Extraction ACTUALLY WORKS! No more mangled methods sections!"
Week 4: "I can finally TRUST the extraction! Plus sample sizes!"
Week 5: "Citation data included? This is getting good!"
Week 6: "Entity-aware search? Now we're talking!"
Week 7: "It found papers using the same datasets? Amazing!"
Week 8: "From 'unreliable' to 'indispensable' - shipped!"
```

### User Value Proposition
```
BEFORE v5.0: "I hope it extracts my papers correctly" (50% fail)
AFTER v5.0:  "I trust it to extract my papers correctly" (95% work)

One weekend investment = Reliable extraction forever
```

## Quick Start Guide

### For Developers
```bash
# 1. Setup environment
git checkout -b v5-implementation
pip install -r requirements-dev.txt

# 2. Start Grobid
docker-compose up -d grobid

# 3. Run tests
pytest tests/unit/test_grobid_extraction.py

# 4. Test extraction on sample
python src/build_kb.py --demo --phase 1
```

### For Users
```bash
# Weekend setup (one-time)
docker pull grobid/grobid:0.7.3
python src/build_kb.py --rebuild --yes --auto-start

# Daily use
python src/build_kb.py                    # Incremental update
python src/research.py search "diabetes"  # Research
```

### Migration from v4.6
```bash
# 1. Export current KB (optional backup)
python src/build_kb.py --export kb_v4_backup.json

# 2. Clean slate for v5
rm -rf kb_data/

# 3. Rebuild with v5
git pull
python src/build_kb.py --rebuild --yes --auto-start
```
