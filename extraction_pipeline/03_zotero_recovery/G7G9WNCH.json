{
  "paper_id": "G7G9WNCH",
  "title": "553 (18.4%) Primary ICD-9 diagnosis (n (%)) Sepsis, including pneumonia",
  "abstract": "Characteristics of hospitals, per number of ICU admissions Teaching tertiary hospital Nonteaching: 37,146 (47.0%) Teaching: 29,388 (37.2%) Unknown: 12,539 (15.9%) Age, years (mean (s.d.)) 64.4 (16.9) 65.0 (16.7) Male gender (n (%)) 9,604 (56.2%) 40,949 (51.8%) Premorbid status (n (%)) Hypertension Diabetes CHF Cancer COPD or RLD CKD",
  "year": 2018,
  "date": "2018-10-22",
  "journal": "Cardiovascular Respiratory Neurological Renal Others",
  "publication": "Cardiovascular Respiratory Neurological Renal Others",
  "authors": [
    {
      "affiliation": "1  Department of Surgery and Cancer , Imperial College London , London , UK. \n\t\t\t\t\t\t\t\t Department of Surgery and Cancer \n\t\t\t\t\t\t\t\t Imperial College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t UK"
    },
    {
      "affiliation": "2  Department of Bioengineering , Imperial College London , London , UK. \n\t\t\t\t\t\t\t\t Department of Bioengineering \n\t\t\t\t\t\t\t\t Imperial College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t UK"
    },
    {
      "affiliation": "3  Laboratory of Computational Physiology , Harvard-MIT Division of Health Sciences & Technology , Cambridge , MA , USA. \n\t\t\t\t\t\t\t\t Laboratory of Computational Physiology \n\t\t\t\t\t\t\t\t Harvard-MIT Division of Health Sciences & Technology \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Cambridge \n\t\t\t\t\t\t\t\t\t MA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "4  Beth Israel Deaconess Medical Center , Boston , MA , USA. \n\t\t\t\t\t\t\t\t Beth Israel Deaconess Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Boston \n\t\t\t\t\t\t\t\t\t MA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "5  Department of eICU Research and Development , Philips Healthcare , Baltimore , MD , USA. \n\t\t\t\t\t\t\t\t Department of eICU Research and Development \n\t\t\t\t\t\t\t\t Philips Healthcare \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Baltimore \n\t\t\t\t\t\t\t\t\t MD \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "6  Department of Pharmacy Practice and Science , University of Maryland , School of Pharmacy , Baltimore , MD , USA. \n\t\t\t\t\t\t\t\t Department of Pharmacy Practice and Science \n\t\t\t\t\t\t\t\t School of Pharmacy \n\t\t\t\t\t\t\t\t University of Maryland \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Baltimore \n\t\t\t\t\t\t\t\t\t MD \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "7  Department of Computer Science , Imperial College London , London , UK. \n\t\t\t\t\t\t\t\t Department of Computer Science \n\t\t\t\t\t\t\t\t Imperial College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t UK"
    },
    {
      "affiliation": "8  Medical Research Council London Institute of Medical Sciences , London , UK. \n\t\t\t\t\t\t\t\t Medical Research Council \n\t\t\t\t\t\t\t\t London Institute of Medical Sciences \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t UK"
    },
    {
      "affiliation": "9  Behaviour Analytics Lab , Data Science Institute , London , UK. \n\t\t\t\t\t\t\t\t Behaviour Analytics Lab \n\t\t\t\t\t\t\t\t Data Science Institute \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t UK"
    }
  ],
  "doi": "",
  "sections": [
    {
      "title": "S",
      "text": "epsis is defined as severe infection leading to life-threatening acute organ dysfunction  7  . The management of intravenous fluids and vasopressors in sepsis is a key clinical challenge and a top research priority  1, 4  . Besides general guidelines, such as the Surviving Sepsis Campaign, no tool currently exists to personalize treatment of sepsis and assist clinicians in making decisions in realtime and at the patient level  [4] [5] [6]  . As a consequence, clinical variability in sepsis treatment is extreme, with consistent evidence that suboptimal decisions lead to poorer outcomes  [8] [9] [10]  .\n\nWe developed the AI Clinician, a computational model using reinforcement learning, which is able to dynamically suggest optimal treatments for adult patients with sepsis in the intensive care unit (ICU). Reinforcement learning is a category of AI tools in which a virtual agent learns from trial-and-error an optimized set of rules-a policy-that maximizes an expected return  11, 12  . Similarly, a clinician's goal is to make therapeutic decisions in order to maximize a patient's probability of a good outcome  12, 13  . Reinforcement learning has many desirable properties that may help medical decision-making. The intrinsic design of models using reinforcement learning can handle sparse reward signals, which makes them well-suited to overcome the complexity related to the heterogeneity of patient responses to medical interventions and the delayed indications of the efficacy of treatments  11  . Importantly, these algorithms are able to infer optimal decisions from suboptimal training examples. Reinforcement learning has been successfully applied in the past to medical problems, such as diabetes and mechanical ventilation in the ICU  [14] [15] [16] [17]  .\n\nOur AI Clinician was built and validated on two large nonoverlapping ICU databases containing data routinely collected from adult patients in the United States. The Medical Information Mart for Intensive Care version III (MIMIC-III)  18  was used for model development, and the eICU Research Institute Database (eRI) for model testing. In both datasets, we included adult patients fulfilling the international consensus sepsis-3 criteria  7  . After exclusion of ineligible cases, we included 17,083 admissions (88.4% of eligible patients with sepsis) from five separate ICUs in one tertiary teaching hospital and 79,073 admissions (73.6% of eligible patients with sepsis) from 128 different hospitals from MIMIC-III and eRI, respectively (Supplementary Fig.  1 ). Patient demographics and clinical characteristics are shown in Table  1  and Supplementary Table  1 .\n\nIn both datasets, we extracted a set of 48 variables, including demographics, Elixhauser premorbid status  19  , vital signs, laboratory values, fluids and vasopressors received (Supplementary Table  2 ). Patients' data were coded as multidimensional discrete time series with 4-h time steps, and for each patient, we included up to 72 h of measurements taken around the estimated time of onset of sepsis. The total volume of intravenous fluids and maximum dose of vasopressors administered over each 4-h period defined the medical treatments of interest. The model aims at optimizing patient mortality, so a reward was associated to survival and a penalty to death.\n\nA Markov decision process (MDP) was used to model the patient environment and trajectories  20, 21  . The various elements of the model were defined using patient data time series from the training set (a random sample of 80% of MIMIC-III; Fig.  1 ). We deployed the AI Clinician to solve the MDP and predict outcomes of treatment strategies. First, we evaluated the actual treatments of clinicians by analyzing all the prescriptions and computing the average return of each treatment option, which can take values from -100 to + 100 in our model. Then, the MDP was solved using policy iteration, which identified the treatments that maximized return, that is, the expected 90-d survival of patients in the MIMIC-III cohort  11  . The resultant policy is referred to hereafter as the ' AI policy' .\n\nEvaluating the performance of this new AI policy using the trajectories of patients generated by another policy (the clinicians' policy)\n\nThe Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care Matthieu Komorowski 1,2,3 , Leo A. Celi 3,4 , Omar Badawi 3,5,6 , Anthony C. Gordon 1 * and A. Aldo Faisal 2,7,8,9 * Sepsis is the third leading cause of death worldwide and the main cause of mortality in hospitals  [1] [2] [3]  , but the best treatment strategy remains uncertain. In particular, evidence suggests that current practices in the administration of intravenous fluids and vasopressors are suboptimal and likely induce harm in a proportion of patients 1,4-6 . To tackle this sequential decision-making problem, we developed a reinforcement learning agent, the Artificial Intelligence (AI) Clinician, which extracted implicit knowledge from an amount of patient data that exceeds by many-fold the life-time experience of human clinicians and learned optimal treatment by analyzing a myriad of (mostly suboptimal) treatment decisions. We demonstrate that the value of the AI Clinician's selected treatment is on average reliably higher than human clinicians. In a large validation cohort independent of the training data, mortality was lowest in patients for whom clinicians' actual doses matched the AI decisions. Our model provides individualized and clinically interpretable treatment decisions for sepsis that could improve patient outcomes."
    },
    {
      "title": "Articles",
      "text": "NATure MedICINe is termed off-policy evaluation  [22] [23] [24]  . It was crucial to obtain reliable estimates of the performance of this new policy without deploying it, as executing a bad policy would be dangerous for patients  22, 23  . Therefore, we implemented a type of high-confidence off-policy evaluation (HCOPE) method (weighted importance sampling (WIS)), and we used bootstrapping to estimate the true distribution of the policy value in the MIMIC-III 20% validation set (Fig.  2b  and Supplementary Fig.  1 )  23, 24  . We built 500 different models using 500 different clustering solutions of the training data, and the selected final model maximized the 95% confidence lower bound of the AI policy  23  . Fig.  2a  shows that this bound consistently exceeded the 95% confidence upper bound of the clinicians' policy, provided that enough models were built. This model selection method maximizes the theoretical statistical safety of the new AI policy. The chosen AI policy was then tested on the independent eRI dataset.\n\nGood model calibration was confirmed by plotting the relationship between the return of the clinicians' policy and patients' 90-day mortality (Fig.  2c ). In Fig.  2d , we show the average return measured in survivors and nonsurvivors.\n\nFig.  3a  shows the distribution of the estimated value of the clinicians' policy and the AI policy in the selected final model tested on the eRI cohort. Using bootstrapping with 2,000 resamplings, the median value of clinicians' policy and the AI policy were estimated at 56.9 (interquartile range, 54.7-58.8) and 84.5 (interquartile range, 84.3-87.7), respectively. Fig.  3b , c  shows the distribution of treatment doses according to clinicians' and AI policies. On average, the AI Clinician recommended lower doses of intravenous fluids and higher doses of vasopressors than the clinicians' actual treatments. The proportion of time the eRI patients received vasopressors was only 17%, but this would have been 30% if the AI Clinician's recommendation was followed.\n\nWe further validated the model by analyzing patient mortality when the dose actually administered corresponded to or differed from the dose suggested by the AI Clinician. Fifty-eight percent of the time, the patients received a dose of vasopressor very close to the suggested dose, within 0.02 \u00b5 g/kg body weight/min (\u00b5 g/kg/min) or 10% (whichever was smaller). For fluids, patients received the suggested dose approximately 36% of the time, within 10 mL/hour or 10%. These patients, who received doses similar to the doses recommended by the AI Clinician, had the lowest mortality. When the actual dose given was different from the suggested dose, clinicians gave more or less fluids in similar proportions and less vasopressor 75% of the time. Administering more or less of either treatment than the AI policy was associated with increasing mortality rates in a dose-dependent fashion. Fig.  3d ,e depicts this association, with the dose gap averaged at the patient level. The median dose deficit in patients who received too little vasopressor was 0.13 \u00b5 g/kg/min (interquartile range, 0.04-0.27 \u00b5 g/kg/min). Using a random forest classification model, we gained some insight into the model representations and interpretability by estimating the relative importance of the model parameters for predicting the administration of both medications (Supplementary Fig.  2 ). This confirmed that the decisions suggested by the AI Clinician were clinically interpretable and relied primarily on sensible clinical and biological parameters.\n\nHere we demonstrate how reinforcement learning could be applied to solve a complex medical problem and suggest individualized and clinically interpretable treatment strategies for sepsis. In an independent cohort, the patients who received the treatments suggested by the AI Clinician had the lowest mortality rate.\n\nWhen clinicians' actual treatments varied from the AI Clinician's suggested policy, this was most commonly administration of too little vasopressor. Early use of low-dose vasopressor has been suggested to play a role in sepsis;  4, 5, 8, 9  this may avoid administration of an excessive amounts of fluids, which has been linked with a poorer outcome  1, 4, 5, 25  . Our results support this strategy but importantly allow the treatment to be individualized for each patient.\n\nWe envision that this system would be used in real-time, with patient data obtained from different streams being fed into electronic health record software fitted with our algorithm, which would suggest a course of action. Physicians will always need to make subjective clinical judgments about treatment strategies, but computational models can provide additional insight about optimal decisions, avoiding targeting short-term resuscitation goals and instead following trajectories toward longer-term survival  [26] [27] [28]  . The reinforcement learning approach that we have developed is agnostic to data used and could in principle be applied to any data-rich clinical environment and many medical interventions. In the future, it is likely that as '-omic' technologies develop, this additional information will be added to the AI Clinician to improve state definition and guide more therapies in selected patient groups.\n\nHowever, there are limitations to our study. Although the datasets we used are large and comprise routinely collected clinical data, some sites and patients had to be excluded owing to poor-quality data recording or missing data. Because of differences between the two datasets, slightly different implementations of the sepsis-3 criteria were used, and hospital mortality was used in eRI instead of 90-d mortality. Finally, some laboratory values would not have been immediately available to clinicians to inform decision-making but were available to the AI Clinician. This work will clearly require prospective evaluation using realtime data and decision-making in clinical trials and also testing in different healthcare settings, but a reduction in mortality from sepsis by only a small percentage would represent several tens of thousands of lives saved annually worldwide  3  . In the last 10-15 years, attempts to develop new treatments to reduce sepsis mortality have uniformly been unsuccessful  29, 30  . The use of computer decision support systems to better guide treatments and improve outcomes is therefore a much needed approach."
    },
    {
      "title": "Online content",
      "text": "Any methods, additional references, Nature Research reporting summaries, source data, statements of data availability and associated accession codes are available at  https://doi.org/10.1038/  s41591-018-0213-5 ."
    },
    {
      "title": "Articles"
    },
    {
      "title": "NATure MedICINe"
    },
    {
      "title": "Methods",
      "text": "Study design and databases. We built and then validated a computational clinicaldecision support model based on the retrospective analysis of two nonoverlapping intensive care databases containing data collected from adult patients. The databases were: MIMIC-III was used for model development, and eRI for model validation. Both databases contain high-resolution patient data including demographics, vital signs time series, laboratory tests, illness severity scores, medications and procedures, fluid intake and outputs, clinician notes, and diagnostic coding."
    },
    {
      "title": "Patient cohorts.",
      "text": "In both datasets, MIMIC-III and eRI, we included adult patients fulfilling the sepsis-3 criteria  7  . Sepsis was defined as a suspected infection (prescription of antibiotics and sampling of bodily fluids for microbiological culture) combined with evidence of organ dysfunction, defined by a SOFA score \u2265 2 (refs  7, 31  ). We adhered to the original temporal criteria for diagnosis of sepsis: when the antibiotic was given first, the microbiological sample must have been collected within 24 h; when the microbiological sampling occurred first, the antibiotic must have been administered within 72 h 31 . The earlier event defined the onset of sepsis. In line with previous research, we assumed a baseline SOFA of zero for all patients 31,32 ."
    },
    {
      "title": "Exclusion criteria.",
      "text": "\u2022 In both databases:\n\n\u2022 Age < 18 years old at the time of ICU admission \u2022 Mortality not documented \u2022 Withdrawal of treatment (see below)\n\n\u2022 In MIMIC-III:\n\n\u2022 Intravenous fluid intake not documented\n\n\u2022 In eRI:\n\n\u2022 ICU readmissions, because of the potential risk in this database of mixing up data from subsequent ICU admissions."
    },
    {
      "title": "\u2022 Patient admitted in an ICU with insufficient data collection (see below).",
      "text": "We excluded patients whose treatment was withdrawn because in this case clinical decisions are no longer made aiming to optimize survival, which would have led to spurious actions in the AI policy. Withdrawal of treatment often involves patients with high severity of illness and who are on high doses of vasopressors, in which treatment is withdrawn as it is considered futile. Therefore, we defined withdrawal as patients who died within 24 h of the end of the data collection period and received vasopressors at any point and whose vasopressors were stopped at the end of the data collection.\n\nIn the eRI, the data was recorded heterogeneously across ICUs. To avoid any systematic bias in our analysis (for example, when no medication appears in the database, where in reality it was actually administered to the patient), we excluded hospitals for the years in which the availability of data was not sufficient, as data recording practices could vary over time. We defined two indicators of data availability for vasopressors and intravenous fluids, averaged per day, per patient, per hospital and per year. Given that our analysis resolution was 4 h, we expected at least six records per day, even if the dose was constant. Hospital-years with less than six daily records on average were excluded. In total, 331 ICUs out of 459 were excluded with the combined data-quality-selection approach. For comparison, the data quality in MIMIC-III was high, with a weighted daily average over the five ICUs of 20.4 intravenous fluids records and 31.1 vasopressor records."
    },
    {
      "title": "Data extraction and preprocessing.",
      "text": "In MIMIC-III, data were included from up to 24 h preceding until 48 h following the estimated onset of sepsis, in order to capture the early phase of its management, including initial resuscitation. The outcome was 90-day mortality. Owing to the size of the eRI database (over 2.4 terabytes), a simplified data-extraction process had to be employed. Therefore, we identified all adult patients who had sepsis during the first 36 h after admission and extracted the data obtained from these patients over the first 72 h after admission. Survival at 90 d was not available in eRI, so hospital mortality defined the outcome of interest in this cohort.\n\nFrom both datasets, we extracted a set of 48 variables, including demographics, Elixhauser premorbid status  19  , vital signs, laboratory values, fluids and vasopressors received and fluid balance (Supplementary Table  2 ). Patients' data were coded as multidimensional discrete time series with 4-h time steps. Data variables with multiple measurements within a 4-h time step were averaged (for example, heart rate) or summed (for example, urine output) as appropriate.\n\nAll features were checked for outliers and errors using a frequency histogram method and univariate statistical approaches (Tukey's method). Errors were corrected when possible (for example, conversion of temperature from Fahrenheit to Celsius degrees). Parameters were capped to clinically plausible values.\n\nTo address the problem of missing or irregularly sampled data, we used a timelimited parameter-specific sample-and-hold approach in both datasets, a common practice with health time series data that intuitively matches the cognitive process of clinicians  33  . The remaining missing data were interpolated in MIMIC-III using multivariable nearest-neighbor imputation as the clustering algorithm did not accept missing values  34  . We did not interpolate the remaining missing data in eRI as it was not required for model validation.\n\nBuilding the computational model. The true patient physiological state is only partially represented by the data available, and therefore the disease process could be formulated as a partially observable MDP. A MDP was used to approximate patient trajectory and to model the decision-making process  12, 20, 21  . The MDP is defined by the tuple {S, A,T,R,\u03b3 }, with:\n\n\u2022 S, a finite set of states (in our model, the health states of patients).\n\n\u2022 A, the finite set of actions available from state s (in our model, the dose prescribed of intravenous fluids and vasopressors converted into discrete decisions). \u2022 T(s\u2032 ,s,a), the transition matrix, containing the probability that action a in state s at time t will lead to state s\u2032 at time t + 1, which describes the dynamics of the system. \u2022 R(s\u2032 ), the immediate reward received for transitioning to state s\u2032 . Transitions to desirable states yield a positive reward, and reaching undesirable states generates a penalty. \u2022 \u03b3 , the discount factor, which allows modelling of the fact that a future reward is worth less than an immediate reward.\n\nA sample of 80% of the MIMIC-III cohort was used for model training, and the remaining 20% was used for model validation. The state space was defined by clustering all patient time series from the MIMIC-III development set. A good cluster hierarchy is one in which individuals that are in the same cluster are similar with respect to their observable properties. Specifically, the state space was constructed by k-means + + clustering of the patients' data, resulting in 750 discrete mutually exclusive patient states  35  . We used Bayesian and Akaike information criteria to determine the optimal number of clusters (Supplementary Fig.  3e )  36  . We chose a high value of k to ensure a highly granular model while avoiding using too large a state space ( > 1,000), which would have led to very sparsely populated states (Supplementary Fig.  3a ). Two absorbing states were added to the state space, corresponding to death and discharge of the patient. To further assess the validity of our state aggregation, we used the distribution of International Classification of Diseases codes in the states and demonstrated that past medical history and diagnoses are encapsulated to some extent within our chosen state definition (Supplementary Fig.  3b ).\n\nPrior to clustering and to account for unequal means and variances in data, normally distributed data was standardized, log-normal distributed variables were log-transformed before standardization, and binary data was centered on zero. The normality of each variable was tested with visual methods: quantile-quantile plots and frequency histograms.\n\nThe management of ICU patients with sepsis is extremely complex and includes several principles such as rapid control of the source of infection, correction of hypovolaemia, and management of secondary organ failures. Including all these potential interventions as actions in the MDP would have required a much larger dataset. A key challenge is arguably the management of intravenous fluids and vasopressors. Consequently, we focused on medical decisions regarding total volume of intravenous fluids and maximum dose of vasopressors administered over each 4-h period. Intravenous fluids included boluses and background infusions of crystalloids, colloids and blood products, normalized by tonicity as previously described  8  . The vasopressors included norepinephrine, epinephrine, vasopressin, dopamine and phenylephrine and were converted when necessary to norepinephrine-equivalent using previously published dose correspondence  37  . To define the action space, the dose of each treatment was represented as one of five possible choices, choice 1 being 'no drug given' and the remaining non-null doses divided into four quartiles (Supplementary Table  3 ). The combination of the two treatments produced 25 possible discrete actions. We expressed the suggested dose as the median of each dose bin matching a suggested action.\n\nThe sequences of successive states and actions are referred to as patients' trajectories. In our models, we used either hospital mortality or 90-d mortality as the sole defining factor for the system-defined penalty and reward. When a patient survived, a positive reward was released at the end of each patient's trajectory (a 'reward' of + 100); a negative reward (a 'penalty' of -100) was issued if the patient died.\n\nWe estimated the transition matrix T(s\u2032 ,s,a) by counting how many times each transition was observed in the MIMIC-III training dataset and converting the transition counts to a stochastic matrix  32  . In high-risk environments (where executing a bad policy could cause harm) limiting the action space to known options is a sensible choice to increase the safety of the model. We restricted the set of actions to choose from to frequently observed actions taken by clinicians and excluded transitions seen fewer than five times. As such, the resulting AI policy suggests the best possible treatment among all the options chosen (relatively frequently) by clinicians.\n\nMarkov models rely on the Markov property, which is that the transitions (given state and action) are memoryless. The probability to leave a state in a Markov chain remains constant, no matter how long the agent has been in the state. Thus, the probability to remain in a state follows an exponential decay  38  . We measured the empirical state persistence probability for each state and found a high goodness-of-fit between the data and exponential decay distributions for virtually all states (Supplementary Fig.  3 ).\n\nThe discount factor \u03b3 defines the horizon of the reinforcement learning agent, which is how much importance is given to future rewards compared to the reward in the current state. It can take values between 0 and 1 (ref.  21 ). We chose a \u03b3 value of 0.99, which means that we put nearly as much importance on late deaths as opposed to early deaths.\n\nIn reinforcement learning, a policy \u03c0 corresponds to a set of rules dictating which action is taken while in a particular state  21  . Each MDP determines a stateaction value function Q \u03c0 , that reflects the expected sum of discounted rewards for choosing an action while in a particular state, and following a policy \u03c0 thereafter  21  . In our model, Q \u03c0 summarizes the effect of the treatment decisions on the patient's mortality risk, with beneficial decisions having positive Q \u03c0 values and harmful decisions negative Q \u03c0 values  12, 13  ."
    },
    {
      "title": "Evaluation of clinicians' actions.",
      "text": "We performed an evaluation of the actual actions (the policy) of clinicians using temporal difference learning (TD-learning) of the Q function by observing all the prescriptions of intravenous fluids and vasopressors in existing records (offline sampling) and computing the average value of each treatment option at the state level  21  . The advantage of TD-learning over policy iteration is that it does not require knowledge of the MDP (model-free) and makes it possible to learn simply from sample trajectories  21  . It was computed iteratively from actual patient episodes of successive state-action pairs, with resampling, using the following Q update formula:\n\nWith Q \u03c0 (s,a) the current {state, action} tuple considered, Q \u03c0 (s\u2032 ,a\u2032 ) the next {state, action} tuple, \u03b1 the learning rate and r the immediate reward.\n\nWe stopped the evaluation after processing 500,000 patient trajectories with resampling, which is when the value of the estimated policy reached an asymptote.\n\nEstimation of the AI policy. We learned a theoretical optimal policy (which we call the ' AI' policy) for the MDP using in-place policy iteration, which identified the decisions that maximize the long-term sum of rewards, hence the expected survival of patients  21  . Policy iteration started with a random policy that was iteratively evaluated and then improved until converging to an optimal solution. After convergence, the AI policy \u03c0*corresponded to the actions with the highest state-action value in each state:\n\n* a\n\nThe value V of a policy \u03c0 was computed using the Bellman equation for V \u03c0 and represented the expected return when starting in s and following \u03c0 thereafter:\n\na s\n\nBecause 90-d mortality was not available in the eRI, hospital mortality was used as the outcome of interest. We verified first that the model performed well in the MIMIC-III database, when the model was trained using hospital mortality (Supplementary Fig.  4 ) and 90-d mortality (data not shown). This sanity check supported the extension of the framework into the eRI data.\n\nModel evaluation. Our objective is to evaluate the value of a newly learnt AI policy using trajectories of patients generated by another policy (the clinicians')  [21] [22] [23]  . This is termed off-policy evaluation (OPE). Using direct, model-based estimates of the policy value are known to reduce variance at the cost of adding bias to the estimate  22, 39  . Therefore, we implemented a type of HCOPE method, WIS, and used bootstrapping to estimate the true distribution of the policy value in the test sets  22, 23, 40  . WIS may be a biased although consistent policy estimator, so the bootstrap confidence interval may also be biased, even though the literature suggests that consistency is a more desirable property than unbiasedness  22, 39, 41  . It is accepted that bootstrapping produces accurate confidence intervals with less data than exact HCOPE methods and is safe enough in high-risk applications, such as healthcare  22, 23  . Of note, the use of bootstrap confidence intervals around WIS estimates has not been previously described in biomedical research, but the approach is suggested in reinforcement learning research  22, 23  .\n\nWe define \u03c0 0 as the behavior policy (the clinicians'), from which actual patient data was generated, and \u03c0 1 as the evaluation, or AI policy. In OPE tasks, importance sampling is a simple way to correct for the discrepancy between \u03c0 0 and \u03c0 1 (ref.  42 ). Weighting the estimate allows reducing its variance  39  . Using importance sampling (IS) methods with a deterministic evaluation policy is problematic, as only a few {s,a} pairs and short sequences can be used for policy evaluation. Indeed, the IS weights become zero as soon as the two policies diverge. We softened \u03c0 1 , so it now recommends taking the suggested action 99% of the time and any of the other actions a total of 1% of the time. This allows assessment of the entirety of the patient trajectories. Our goal was to estimate the value of \u03c0 1 from data trajectories. We defined \u03c0 \u03c0 \u03c1 = | \u2215 | a s a s : ( ) ( ) t t t t t 1 0 as the per-step importance ratio, \u03c1 = \u220f \u03c1 \u2032 \u2032 = : t t t t 1: 1 as the cumulative importance ratio up to step t, = \u2211 \u03c1 \u2215| | = | | w D t i D t i 1 1: ( )\n\nas the average cumulative importance ratio at horizon t in dataset D and |D| as the number of trajectories in D  21, 39  . The trajectory-wise WIS estimator is given by:"
    },
    {
      "text": "Fig. 1 | data flow of the AI Clinician. Eighty percent of the MIMIC-III dataset was used to define the elements of the MDP. Time series of patient data were clustered into finite states. The dose of intravenous (i.v.) fluids and vasopressors were discretized into 25 possible actions. Patient survival at 90 d after ICU admission defined reward. Reinforcement learning was used to estimate optimal treatment strategies-the AI policy. The remaining 20% of MIMIC-III data was used to identify the best model among 500 candidates, which was then tested on an independent dataset from the eRI database."
    },
    {
      "text": "Fig.2| Selection of the best AI policy and model calibration. a, Evolution of the 95% lower bound (LB) of the best AI policy and of the 95% upper bound (UB) of highest-valued clinician policy during the building of 500 models. After only a few models, a higher value for the AI policy than the clinician treatment, within the accepted risk, is guaranteed. n = 13,666 patients in the MIMIC-III development dataset, n = 3,417 in the MIMIC-III test set and n = 79,073 in the eRI set. b, Distribution of the estimated value of the clinicians' actual treatments, the AI policy, a random policy and a zero-drug policy across the 500 models in the MIMIC-III test set (n = 500 models in each boxplot). The chosen AI policy maximizes the 95% confidence lower bound. On each boxplot, the central line indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. The whiskers extend to 1.5 times the interquartile range. Points beyond the whiskers are considered outliers and are plotted individually using the + symbol. c, The relationship between the return of clinicians' treatments and patient 90-d mortality in the MIMIC-III training set (n = 13,666 patients). Return of actions were sorted into 100 bins, and the mean observed mortality (blue line for raw, red line for smoothed) was computed in each of these bins. The shaded blue area represents the s.e.m. Treatments with a low return were associated with a high risk of mortality, whereas treatments with a high return led to better survival rates. d, Average return in survivors (n = 11,031) and nonsurvivors (n = 2,635) in the MIMIC-III training set. c and d were generated by bootstrapping in the training data with 2,000 resamplings."
    },
    {
      "text": "Fig.3| Comparison of clinician and AI policies in erI and average dose excess received per patient of both drugs in erI with corresponding mortality. a, Distribution of the estimated value of the clinician and AI policies in the selected model, built by bootstrapping with 2,000 resamplings. b,c, Visualization of the clinician and AI policies. All actions were aggregated over all time steps for the five dose bins of both medications. On average, patients were administered more intravenous fluid (b) and less vasopressor (c) medications than recommended by the AI policy. Vasopressor dosage is shown in \u00b5 g/kg/min of norepinephrine equivalent, and intravenous fluid dosage is shown in mL/4 h. d, The dose excess, referring to the difference between the given and suggested dose averaged over all time points per patient, for intravenous fluids (left) and vasopressor (right). The figure was generated by bootstrapping with 2,000 resamplings. The shaded area represents the s.e.m. In both plots, the smallest dose difference was associated with the best survival rates (vertical dotted line). The further away the dose received was from the suggested dose, the worse the outcome."
    },
    {
      "text": "(i) The MIMIC-III, an open-access, anonymized database of 61,532 admissions from 2001-2012 in six ICUs at a Boston teaching hospital 18 . (ii) The Philips eRI, containing more than 3.3 million admissions from 2003-2016 in 459 ICUs across the United States."
    }
  ],
  "references": [
    {
      "title": "553 (18.4%) Primary ICD-9 diagnosis (n (%)) Sepsis, including pneumonia",
      "year": 2003,
      "doi": "10.1080/109158199225170"
    },
    {
      "title": "Sepsis: pathophysiology and clinical management",
      "authors": [
        "J Gotts",
        "M Matthay"
      ],
      "year": 2016,
      "doi": "10.1136/bmj.i1585"
    },
    {
      "title": "Statistical Brief #160. Healthcare Cost and Utilization Project (HCUP) Statistical Briefs",
      "authors": [
        "C Torio",
        "R Andrews"
      ],
      "year": 2011
    },
    {
      "title": "Hospital deaths in patients with sepsis from 2 independent cohorts",
      "authors": [
        "V Liu"
      ],
      "year": 2014
    },
    {
      "title": "Fluid resuscitation in human sepsis: time to rewrite history?",
      "authors": [
        "L Byrne",
        "F Van Haren"
      ],
      "year": 2017
    },
    {
      "title": "The demise of early goal-directed therapy for severe sepsis and septic shock",
      "authors": [
        "P Marik"
      ],
      "year": 2015,
      "doi": "10.1111/aas.12479"
    },
    {
      "title": "A rational approach to fluid therapy in sepsis",
      "authors": [
        "P Marik",
        "R Bellomo"
      ],
      "year": 2016,
      "doi": "10.1093/bja/aev349"
    },
    {
      "title": "The third international consensus definitions for sepsis and septic shock (sepsis-3)",
      "authors": [
        "M Singer"
      ],
      "year": 2016
    },
    {
      "title": "Interaction between fluids and vasoactive agents on mortality in septic shock: a multicenter, observational study",
      "authors": [
        "J Waechter"
      ],
      "year": 2014,
      "doi": "10.1097/ccm.0000000000000520"
    },
    {
      "title": "Early versus delayed administration of norepinephrine in patients with septic shock",
      "authors": [
        "X Bai"
      ],
      "year": 2014,
      "doi": "10.1186/s13054-014-0532-y"
    },
    {
      "title": "Fluid administration in severe sepsis and septic shock, patterns and outcomes: an analysis of a large national database",
      "authors": [
        "P Marik",
        "W Linde-Zwirble",
        "E Bittner",
        "J Sahatjian",
        "D Hansell"
      ],
      "year": 2017,
      "doi": "10.1007/s00134-016-4675-y"
    },
    {
      "title": "Reinforcement Learning: An Introduction",
      "authors": [
        "R Sutton",
        "A Barto"
      ],
      "year": 1998
    },
    {
      "title": "Artificial intelligence framework for simulating clinical decision-making: a Markov decision process approach",
      "authors": [
        "C Bennett",
        "K Hauser"
      ],
      "year": 2013,
      "doi": "10.1016/j.artmed.2012.12.003"
    },
    {
      "title": "Modeling Medical Treatment Using Markov Decision Processes",
      "authors": [
        "A Schaefer",
        "M Bailey",
        "S Shechter",
        "M Roberts"
      ],
      "year": 2005,
      "doi": "10.1007/1-4020-8066-2_23"
    },
    {
      "title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs",
      "authors": [
        "V Gulshan"
      ],
      "year": 2016
    },
    {
      "title": "A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units",
      "authors": [
        "N Prasad",
        "L.-F Cheng",
        "C Chivers",
        "M Draugelis",
        "B Engelhardt"
      ],
      "year": 2017,
      "doi": "10.3390/jpm12050661"
    },
    {
      "title": "The use of reinforcement learning algorithms to meet the challenges of an artificial pancreas",
      "authors": [
        "M Bothe"
      ],
      "year": 2013,
      "doi": "10.1586/17434440.2013.827515"
    },
    {
      "title": "Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control",
      "authors": [
        "C Lowery",
        "A Faisal"
      ],
      "year": 2013
    },
    {
      "title": "MIMIC-III, a freely accessible critical care database",
      "authors": [
        "A Johnson"
      ],
      "year": 2016
    },
    {
      "title": "Comorbidity measures for use with administrative data",
      "authors": [
        "A Elixhauser",
        "C Steiner",
        "D Harris",
        "R Coffey"
      ],
      "year": 1998,
      "doi": "10.1097/00005650-199801000-00004"
    },
    {
      "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
      "authors": [
        "M Puterman"
      ],
      "year": 2014,
      "doi": "10.1002/9780470316887"
    },
    {
      "title": "Reinforcement Learning: An Introduction",
      "authors": [
        "R Sutton",
        "A Barto"
      ],
      "year": 2018
    },
    {
      "title": "High-Confidence Off-Policy Evaluation",
      "authors": [
        "P Thomas",
        "G Theocharous",
        "M Ghavamzadeh"
      ],
      "year": 2015,
      "doi": "10.1609/aaai.v29i1.9541"
    },
    {
      "title": "Bootstrapping with Models: Confidence Intervals for Off-Policy Evaluation",
      "authors": [
        "J Hanna",
        "P Stone",
        "S Niekum"
      ],
      "year": 2016,
      "doi": "10.1609/aaai.v31i1.11123"
    },
    {
      "title": "High confidence policy improvement",
      "authors": [
        "P Thomas",
        "G Theocharous",
        "M Ghavamzadeh"
      ],
      "year": 2015
    },
    {
      "title": "A positive fluid balance is an independent prognostic factor in patients with sepsis",
      "authors": [
        "A Acheampong",
        "J.-L Vincent"
      ],
      "year": 2015,
      "doi": "10.1186/s13054-015-0970-1"
    },
    {
      "title": "Machine learning and decision support in critical care",
      "authors": [
        "A Johnson"
      ],
      "year": 2016,
      "doi": "10.1109/jproc.2015.2501978"
    },
    {
      "title": "The future of critical care medicine: integration and personalization",
      "authors": [
        "J.-L Vincent"
      ],
      "year": 2016,
      "doi": "10.1097/ccm.0000000000001530"
    },
    {
      "title": "Machine learning and prediction in medicinebeyond the peak of inflated expectations",
      "authors": [
        "J Chen",
        "S Asch"
      ],
      "year": 2017
    },
    {
      "title": "levosimendan for the prevention of acute organ dysfunction in sepsis",
      "authors": [
        "A Gordon"
      ],
      "year": 2016,
      "doi": "10.1056/nejmoa1609409"
    },
    {
      "title": "Drotrecogin alfa (activated) in adults with septic shock",
      "authors": [
        "V Ranieri"
      ],
      "year": 2012,
      "doi": "10.1056/nejmoa1202290"
    },
    {
      "title": "Prognostic accuracy of the SOFA Score, SIRS Criteria, and qSOFA score for in-hospital mortality among adults with suspected infection admitted to the intensive care unit",
      "authors": [
        "E Raith"
      ],
      "year": 2017
    },
    {
      "title": "Detecting hazardous intensive care patient episodes using real-time mortality models",
      "authors": [
        "C Hug"
      ],
      "year": 2009
    },
    {
      "title": "Improved methods for the imputation of missing data by nearest neighbor methods",
      "authors": [
        "G Tutz",
        "S Ramzan"
      ],
      "year": 2015
    },
    {
      "title": "K-means+ + : The Advantages of Careful Seeding",
      "authors": [
        "D Arthur",
        "S Vassilvitskii"
      ],
      "year": 2007
    },
    {
      "title": "Bayesian information criterion for longitudinal and clustered data",
      "authors": [
        "R Jones"
      ],
      "year": 2011
    },
    {
      "title": "Survival after shock requiring high-dose vasopressor therapy",
      "authors": [
        "S Brown"
      ],
      "year": 2013,
      "doi": "10.1378/chest.12-1106"
    },
    {
      "title": "Discrete-time Markov chains",
      "authors": [
        "J Norris"
      ],
      "year": 1997
    },
    {
      "title": "Doubly robust off-policy value evaluation for reinforcement learning",
      "authors": [
        "N Jiang",
        "L Li"
      ],
      "year": 2015,
      "doi": "10.2514/6.2023-0967.vid"
    },
    {
      "title": "Data-efficient off-policy policy evaluation for reinforcement learning",
      "authors": [
        "P Thomas",
        "E Brunskill"
      ],
      "year": 2016
    },
    {
      "title": "Eligibility Traces for off-policy policy evaluation",
      "authors": [
        "D Precup",
        "R Sutton",
        "S Singh"
      ],
      "year": 2000
    },
    {
      "title": "Safe and efficient off-policy reinforcement learning",
      "authors": [
        "R Munos",
        "T Stepleton",
        "A Harutyunyan",
        "M Bellemare"
      ],
      "year": 2016
    }
  ],
  "num_references": 42,
  "original_doi": "https://doi.org/10.13039/501100000266"
}
