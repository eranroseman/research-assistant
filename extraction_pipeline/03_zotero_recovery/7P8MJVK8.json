{
  "paper_id": "7P8MJVK8",
  "title": "Multi-domain Clinical Natural Language Processing with MedCAT: the Medical Concept Annotation Toolkit",
  "abstract": "Electronic health records (EHR) contain large volumes of unstructured text, requiring the application of Information Extraction (IE) technologies to enable clinical analysis. We present the open source Medical Concept Annotation Toolkit (MedCAT) that provides: a) a novel self-supervised machine learning algorithm for extracting concepts using any concept vocabulary including UMLS/SNOMED-CT; b) a feature-rich annotation interface for customising and training IE models; and c) integrations to the broader CogStack ecosystem for vendor-agnostic health system deployment. We show improved performance in extracting UMLS concepts from open datasets (F1:0.448-0.738 vs 0.429-0.650). Further real-world validation demonstrates SNOMED-CT extraction at 3 large London hospitals with self-supervised training over \u223c8.8B words from \u223c17M clinical records and further fine-tuning with 6K clinician annotated examples. We show strong transferability (F1>0.94) between hospitals, datasets and concept types indicating cross-domain EHR-agnostic utility for accelerated clinical and research use cases.",
  "year": 2021,
  "date": "2021-03-25",
  "journal": "Eur. J. Heart Fail",
  "publication": "Eur. J. Heart Fail",
  "authors": [
    {
      "forename": "Zeljko",
      "surname": "Kraljevic",
      "name": "Zeljko Kraljevic",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Thomas",
      "surname": "Searle",
      "name": "Thomas Searle",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Anthony",
      "surname": "Shek",
      "name": "Anthony Shek",
      "affiliation": "c  Department of Clinical Neuroscience , Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Clinical Neuroscience \n\t\t\t\t\t\t\t\t Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Lukasz",
      "surname": "Roguski",
      "name": "Lukasz Roguski",
      "affiliation": "b  Health Data Research UK London , University College London , London , U.K. \n\t\t\t\t\t\t\t\t Health Data Research \n\t\t\t\t\t\t\t\t UK London \n\t\t\t\t\t\t\t\t University College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Kawsar",
      "surname": "Noor",
      "name": "Kawsar Noor",
      "affiliation": "b  Health Data Research UK London , University College London , London , U.K. \n\t\t\t\t\t\t\t\t Health Data Research \n\t\t\t\t\t\t\t\t UK London \n\t\t\t\t\t\t\t\t University College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Daniel",
      "surname": "Bean",
      "name": "Daniel Bean",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Aurelie",
      "surname": "Mascio",
      "name": "Aurelie Mascio",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Leilei",
      "surname": "Zhu",
      "name": "Leilei Zhu",
      "affiliation": "d  Institute of Health Informatics , University College London , cityLondon , U.K. \n\t\t\t\t\t\t\t\t Institute of Health Informatics \n\t\t\t\t\t\t\t\t University College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t cityLondon \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Amos",
      "surname": "Folarin",
      "name": "Amos Folarin",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Angus",
      "surname": "Roberts",
      "name": "Angus Roberts",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Rebecca",
      "surname": "Bendayan",
      "name": "Rebecca Bendayan",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Mark",
      "surname": "Richardson",
      "name": "Mark Richardson",
      "affiliation": "c  Department of Clinical Neuroscience , Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Clinical Neuroscience \n\t\t\t\t\t\t\t\t Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Robert",
      "surname": "Stewart",
      "name": "Robert Stewart",
      "affiliation": "e  Department of Psychological Medicine , Institute of Psychiatry , Psychology and Neuroscience , King's College London , London ,, U.K. \n\t\t\t\t\t\t\t\t Department of Psychological Medicine \n\t\t\t\t\t\t\t\t Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Anoop",
      "surname": "Shah",
      "name": "Anoop Shah",
      "affiliation": "b  Health Data Research UK London , University College London , London , U.K. \n\t\t\t\t\t\t\t\t Health Data Research \n\t\t\t\t\t\t\t\t UK London \n\t\t\t\t\t\t\t\t University College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Wai",
      "surname": "Wong",
      "name": "Wai Wong",
      "affiliation": "d  Institute of Health Informatics , University College London , cityLondon , U.K. \n\t\t\t\t\t\t\t\t Institute of Health Informatics \n\t\t\t\t\t\t\t\t University College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t cityLondon \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Zina",
      "surname": "Ibrahim",
      "name": "Zina Ibrahim",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "James",
      "surname": "Teo",
      "name": "James Teo",
      "affiliation": "c  Department of Clinical Neuroscience , Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Clinical Neuroscience \n\t\t\t\t\t\t\t\t Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K"
    },
    {
      "forename": "Richard",
      "surname": "Dobson",
      "name": "Richard Dobson",
      "affiliation": "a  Department of Biostatistics and Health Informatics Institute of Psychiatry , Psychology and Neuroscience , King's College London , London , U.K. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Health Informatics Institute of Psychiatry \n\t\t\t\t\t\t\t\t Psychology and Neuroscience \n\t\t\t\t\t\t\t\t King's College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t U.K",
      "email": "richard.j.dobson@kcl.ac.uk"
    }
  ],
  "doi": "",
  "arxiv": "arXiv:2010.01165v2[cs.CL]",
  "keywords": [
    "Electronic Health Record Information Extraction",
    "Clinical Natural Language Processing",
    "Clinical Concept Embeddings",
    "Clinical Ontology Embeddings"
  ],
  "sections": [
    {
      "title": "Introduction",
      "text": "Electronic Health Records (EHR) are large repositories of clinical and operational data that have a variety of use cases from population health, clinical decision support, risk factor stratification and clinical research. However, health record systems store large portions of clinical information in unstructured format or proprietary structured formats, resulting in data that is hard to manipulate, extract and analyse. There is a need for a platform to accurately extract information from freeform health text in a scalable manner that is agnostic to underlying health informatics architectures.\n\nWe present the Medical Concept Annotation Toolkit (MedCAT): an open-source Named Entity Recognition + Linking (NER+L) and contextualization library, an annotation tool and online learning training interface, and integration service for broader CogStack[1] ecosystem integration for easy deployment into health systems. The MedCAT library can learn to extract concepts (e.g. disease, symptoms, medications) from free-text and link them to any biomedical ontology such as SNOMED-CT[2] and UMLS  [3] . MedCATtrainer  [4] , the annotation tool, enables clinicians to inspect, improve and customize the extracted concepts via a web interface built for training MedCAT information extraction pipelines. This work outlines the technical contributions of MedCAT and compares the effectiveness of these technologies with existing biomedical NER+L tools. We further present real clinical usage of our work in the analysis of multiple EHRs across various NHS hospital sites including running the system over 20 years of collected data pre-dating even the usage of modern EHRs at one site. MedCAT has been deployed and contributed to clinical research findings in multiple NHS trusts throughout England  [5, 6] ."
    },
    {
      "title": "Problem Definition",
      "text": "Recently NER models based on Deep Learning (DL), notably Transformers  [7]  and Long-Short Term Memory Networks  [8]  have achieved considerable improvements in accuracy  [9] . However, both approaches require explicit supervised training. In the case of biomedical concept extraction, there is little publicly available labelled data due to the personal and sensitive nature of the text. Building such a corpus can be onerous and expensive due to the need for direct EHR access and domain expert annotators. In addition, medical vocabularies can contain millions of different named entities with overlaps (see Fig.  1 ). Extracted entities will also often require further classification to ensure they are contextually relevant; for example extracted concepts may need to be ignored if they occurred in the past or are negated. We denote this further classification as meta-annotation or a 'contextualisation' of a recognised entity. Overall, using data-intensive methods such as DL can be extremely challenging in real clinical settings.\n\nThis work is positioned to improve on current tools such as the Open Biomedical Annotator (OBA) service  [10]  that have been used in tools such as DeepPatient  [11]  and ConvAE  [12]  to structure and infer clinically meaningful outputs from EHRs. MedCAT allows for continual improvement of annotated concepts through a novel self-supervised machine learning algorithm, customisation of concept vocabularies, and downstream contextualisation of extracted concepts. All of which are either partially or not addressed by current tools."
    },
    {
      "title": "NER+L in a Biomedical Context",
      "text": "Due to the limited availability of training data in biomedical NER+L, existing tools often employ a dictionary-based approach. This involves the usage of a vocabulary of all possible terms of interest and the associated linked concept as specified in the clinical database e.g. UMLS or SNOMED-CT. This approach allows the detection of concepts without providing manual annotations. However, it poses several challenges that occur frequently in EHR text. These include: spelling mistakes, form variability (e.g. kidney failure vs failure of kidneys), recognition and disambiguation (e.g. does 'hr' refer to the concept for 'hour' or 'heart rate' or neither)."
    },
    {
      "title": "Existing Biomedical NER+L Tools",
      "text": "We compare prior NER+L tools for biomedical documents that are capable of handling extremely large concept databases (completely and not a small subset). MetaMap  [13]  was developed to map biomedical text to the UMLS Metathesaurus. MetaMap cannot handle spelling mistakes and has limited capabilities to handle ambiguous concepts. It offers an opaque additional 'Word-Sense-Disambiguation' system that attempts to disambiguate candidate concepts that consequently slows extraction. Bio-YODIE  [14]  improves upon the speed of extraction compared to MetaMap and includes improved disambiguation capabilities, but requires an annotated corpus or supervised training. SemEHR  [15]  builds upon Bio-YODIE to somewhat address these shortcomings by applying manual rules to the output of Bio-YODIE to improve the results. Manual rules can be labour-intensive, brittle and time-consuming, but they can produce good results  [16] . cTAKES  [17] , builds on existing open-source technologies-the Unstructured Information Management Architecture  [18]  framework and OpenNLP  [19]  the natural language processing toolkit. The core cTAKES library does not handle any of the previously mentioned challenges without additional plugins. ScispaCy  [20]  is a practical biomedical/scientific text processing tool, which heavily leverages the spaCy 2 library. Figure  1:  A fictitious example of biomedical NER+L with nested entities and further 'meta-annotations'; a further classification or 'context' applied to an already extracted concept e.g. 'time current' indicates extracted concepts are mentioned in a temporally present context. This context may also be referred to as an attribute of a recognised entity. Each one of the detected boxes (nested) has multiple candidates in the Unified Medical Language System (UMLS). The goal is to detect the entity and annotate it with the most appropriate concept ID, e.g. for the span Status, we have at least three candidates in UMLS, namely C0449438, C1444752, C1546481.\n\nIn contrast to other tools mentioned, ScispaCy is primarily a supervised model for NER with limited linking capabilities. CLAMP  [21]  is a comprehensive clinical NLP software that enables recognition and automatic encoding of clinical information in narrative patient reports. Similar to ScispaCy it is a supervised approach and not directly comparable to other tools mentioned here. MetaMap, BioYODIE, SemEHR, cTakes and ScispaCy only support extraction of UMLS concepts. BioPortal  [22]  offers a web hosted annotation API for 880 distinct ontologies. This is important for use cases that are not well supported by only the UMLS concept vocabulary  [23]  or are better suited to alternative terminologies  [24] . However, transmitting sensitive hospital data to an externally hosted annotation web API may be prohibited under data protection legislation  [25] . The BioPortal annotator is a 'fixed' algorithm so does not allow customisation or improvements through machine learning or support of non-english language corpora  [26] .\n\nCLAMP, and in a limited capacity cTakes and Se-mEHR, support further contextualisation of extracted concepts. MetaMap, BioYODIE and scispaCy treat this as a downstream task although it is often required before extracted concepts can be used in clinical research. Med-CAT addresses these shortcomings of prior tools allowing for flexibly clinician driven definition of concept contextualisation, supporting modern information extraction requirements for biomedical text."
    },
    {
      "title": "Methods",
      "text": "MedCAT presents a set of decoupled technologies for developing IE pipelines for varied health informatics use cases. Fig.  2  shows a typical MedCAT workflow within a wider typical CogStack deployment. CogStack queries selectively extract relevant documents from the EHR including the structured and unstructured (freetext) notes. With MedCAT we firstly agree with clinical partners the relevant terms within a clinical terminology(1) and train MedCAT self-supervised(2). We load the model into the MedCATtrainer annotation tool(3) alongside a random sample of the extracted EHR documents(4). Clinical domain experts validate and improve the model using supervised online learning  (5) . Metrics demonstrate the quality of a fine-tuned MedCAT model  (6)  and once desired performance is reached the fine-tuned model is exported  (7)  and run upon the wider free-text EHR dataset  (8, 9) , facilitating downstream clinical research through the newly structured data  (10) .\n\nThis section presents the MedCAT platform technologies, its method for learning to extract and contextualise biomedical concepts through self-supervised and supervised learning. Integrations with the broader CogStack ecosystem are presented alongside source code foot_1  . Finally, we present our experimental methodology for assessing MedCAT in real clinical scenarios."
    },
    {
      "title": "The MedCAT Core Library",
      "text": "We now outline the technical details of the NER+L algorithm, the self-supervised and supervised training procedures and methods for flexibly contextualising linked entities."
    },
    {
      "title": "Vocabulary and Concept Database",
      "text": "MedCAT NER+L relies on two core components:\n\n\u2022 Vocabulary (VCB): the list of all possible words that can appear in the documents to be annotated. It is primarily used for the spell checking features of the algorithm. We have compiled our own VCB by scraping Wikipedia and enriching it with words from UMLS.\n\nOnly the Wikipedia VCB is made public, but the full VCB can be built with scripts provided in the MedCAT repository ( https://github.com/CogStack/MedCAT ). The scripts require access to the UMLS Metathesaurus ( https://www.nlm.nih.gov/research/umls ).\n\n\u2022 Concept Database (CDB): a table representing a biomedical concept dictionary (e.g. UMLS, SNOMED-CT). Each new concept added to the CDB is represented by an ID and Name. A concept ID can be referred to through multiple names with identical conceptual meanings such as heart failure, myocardial failure, weak heart and cardiac failure."
    },
    {
      "title": "The NER+L Algorithm",
      "text": "With a prepared CDB and VCB, we perform a first pass NER+L pipeline then run a trainable disambiguation algorithm. The initial NER+L pipeline starts with cleaning and spell-checking the input text. We employ a fast and lightweight spell checker ( http://www.norvig.com/spell- correct.html ) that uses word frequency and edit distance between misspelled and correct words to fix mistakes. We use the following rules:\n\n\u2022 A word is spelled against the VCB, but corrected only against the CDB.\n\n\u2022 The spelling is never corrected in the case of abbreviations.\n\n\u2022 An increase in the word length corresponds to an increase in character correction allowance.\n\nNext, the document is tokenized and lemmatized to ensure a broader coverage of all the different forms of a concept name. We used SciSpaCy  [20] , a tool tuned for these tasks in the biomedical domain. Finally, to detect entity candidates we use a dictionary-based approach with a moving expanding window:\n\n1. Given a document d 1 2. Set window length = 1 and word position = 0 3. There are three possible cases:\n\n(a) The text in the current window is a concept in our CDB (the concept dictionary), mark it and go to 4. Note that MedCAT can ignore token order, but only for up-to two tokens (stopwords are not counted in the two token limit).\n\n(b) The text is a substring of a longer concept name, if so go to 4.\n\n(c) Otherwise reset window length to 1, increase word position by 1 and repeat step 3 4. Expand the window size by 1 and repeat 3.\n\nSteps 3 and 4 help us solve the problem of overlapping entities shown in Fig.  1 ."
    },
    {
      "title": "Self-Supervised Training Procedure",
      "text": "For concept recognition and disambiguation, we use context similarity. Initially, we find and annotate mentions of concepts that are unambiguous, (e.g. step 3. a. in the previous expanding window algorithm) then we learn the context of marked text spans. For new documents, when a concept candidate is detected and is ambiguous its context is compared to the currently learned one, if the similarity is above a threshold the candidate is annotated and linked. The similarity between the context embeddings also serves as a confidence score of the annotation and can be later used for filtering and further analysis. The self-supervised training procedure is defined as follows:\n\n1. Given a corpus of biomedical documents and a CDB.\n\n2. For each concept in the CDB ignore all names that are not unique (ambiguous) or that are known abbreviations.\n\n3. Iterate over the documents and annotate all of the concepts using the approach described earlier. The filtering applied in the previous steps guarantee the entity can be annotated."
    },
    {
      "title": "4.",
      "text": "For each annotated entity calculate the context embedding V cntx .\n\n5. Update the concept embedding V concept with the context embedding V cntx .\n\nThe self-supervised training relies upon one of the names assigned to each concept to be unique in the CDB. The unique name is a reference point for training to learn concept context, so when an ambiguous name appears (a name that is used for more than one concept in the CDB) it can be disambiguated. For example, the UMLS concept id:C0024117 has the unique name Chronic Obstructive Airway Disease. This name is unique in UMLS. If we find a text span with this name we can use the surrounding text of this span for training, because it uniquely links to C0024117. \u223c 95% of the concepts in UMLS have at least one unique name.\n\nThe context of a concept is represented by vector embeddings. Given a document d 1 where C x is a detected concept candidate (Equation. 1) we calculate the context embedding. This is a vector representation of the context for that concept candidate (Equation. 2). That includes a pre-set (s) number of words to the left and right of the concept candidate words. Importantly, the concept candidate words are also included in context embedding calculation as the model is assisted by knowing what words the surrounding context words relate to.\n\nWhere: d 1 -An example of a document w 1..n -Words in the document, or to be more specific tokens C x -The detected concept candidate that matches the words w k and w k+1\n\nWhere:\n\nV cntx -Calculated context embedding V w k -Word embedding s -Words from left and right that are included in the context of a detected concept candidate. Typically, s is set to 9 for long context and 2 for short context.\n\nTo calculate context embeddings we use the word embedding method Word2Vec  [27] . Contextualised embedding approaches such as BERT  [28]  were also tested alongside fastText  [29]  and GloVe  [30] . Results presented in Section 3.1 show the BERT embeddings (the Med-CAT U/MI/B configuration) perform worse on average compared to the simpler Word2Vec embeddings. Fast-Text and GloVe perform similarly to Word2Vec, therefore our default implementation uses Word2Vec for ease of implementation. We trained 300 dimensional Word2Vec embeddings using the entire MIMIC-III  [31]  dataset of 53,423 admissions.\n\nOnce a correct annotation is found (a word uniquely links to a CDB name), a context embedding V cntx is calculated, and the corresponding V concept is updated using the following formula:\n\nWhere: C concept -Number of times this concept appeared during training sim -Similarity between V concept and V cntx lr -Learning rate The update rule is based on the Word2Vec model and aims to make the concept embedding V concept similar to the context in which the concept was presently found V cntx . The scaling which is achieved via the cosine similarity is used to favour new contexts in which a concept appears over contexts that frequently appeared in the past.\n\nTo prevent the context embedding for each concept being dominated by most frequent words, we used negative sampling as defined in  [27] . Whenever we update the V concept with V cntx we also generate a negative context by randomly choosing K words from the vocabulary consisting of all words in our dataset. Here K is equal to 2s i.e. twice the window size for the context (s is the context size from one side of the detected concept, meaning in the positive cycle we will have s words from the left and s words from the right). The probability of choosing each word and the update function for vector embeddings is defined as:\n\nWhere: n -Size of the vocabulary P(w i ) -Probability of choosing the word w i K -Number of randomly chosen words for the negative context V ncntx -Negative context"
    },
    {
      "title": "Supervised Training Procedure",
      "text": "The supervised training process is similar to the selfsupervised process but given the correct concept for the extracted term we update the V concept using the calculated V ctx as defined in Eq. 3-10. This no longer relies upon the self-supervised constraint that at least one name in the set of possible names for a concept is unique as the correct term is provided by human annotators."
    },
    {
      "title": "Contextualisation of Identified and Linked Concepts: Meta-Annotations",
      "text": "Once a span of text is recognised and linked to a concept, further contextualisation or meta-annotation is often required. For example, a simple task of identifying all patients with a fever can entail classifying the located fever text spans that are current mentions (e.g. the patient reports a fever vs the patient reported a fever but . . . ), are positive mentions (e.g. patient has a high fever vs patient has no sign of fever), are actual mentions (e.g. patient is feverish vs monitoring needed if fever reappears), or are experienced by the patient (e.g. pts family all had high fevers). We treat each of these contextualization tasks as distinct binary or multiclass classification tasks Metaannotations are equivalent to 'attributes' in cTakes parlance.\n\nThe MedCAT library provides a 'MetaCAT' component that wraps a Bidirectional-Long-Short-Term-Memory (Bi-LSTM) model trainable directly from Med-CATtrainer project exports. Bi-LSTM models have consistently demonstrated strong performance in biomedical text classification task  [32, 33, 34]  and our own recent work  [35]  demonstrated a Bi-LSTM based model outperforms all other assessed approaches, including Transformer models. MetaCAT models replace the specific concept of interest for example 'diabetes mellitus' with a generic parent term of the concept '[concept]'. The forward / backward pass of the model then learns a concept agnostic context representation of the concept allowing MetaCAT models to be used across concepts as observed in our results (Section. 3.3.3). The MetaCAT API follows standard neural network training methods but are abstracted away from end users whilst still maintaining enough visibility for users to understand when MetaCAT models have been trained effectively. Each training epoch displays training and test set loss and metrics such as precision, recall and F1. An open-source tutorial showcasing the MetaCAT features are available as part of the se-ries of wider MedCAT tutorials  4  . Once trained, MetaCAT models can be exported and reused for further usage outside of initial classification tasks similarly to the MedCAT NER+L models."
    },
    {
      "title": "MedCATTrainer: Annotation Tool",
      "text": "MedCATtrainer allows domain experts to inspect, modify and improve a configured MedCAT NER+L model. The tool either actively trains the underlying model after each reviewed document (facilitating live model improvements as feedback is provided by human users) or simply collects and validates concepts extracted by a static MedCAT model. The active learning is done on a concept level and MedCATtrainer will automatically mark some concepts as correct/incorrect and ask for user input for others where it is not confident enough. Version 0.1  [4]  presented a proof-of-concept annotation tool that has been rewritten and tightly integrated with the Med-CAT library, whilst providing a wealth of new features supporting clinical informatics workflows. We also provide extensive documentation  5  and pre-built containers  6 updated with each new release facilitating easy setup by informatics teams."
    },
    {
      "title": "Datasets and",
      "text": "Experimental Setup 2.4.1. Named Entity Recognition and Linking Open Datasets MedCAT concept recognition and linking was validated on the following publicly datasets: 1. MedMentions[36] -consists of 4,392 titles and abstracts randomly selected from papers released on PubMed in 2016 in the biomedical field, published in the English language, and with both a Title and Abstract. The text was manually annotated for UMLS concepts resulting in 352,496 mentions. We calculate that \u223c 40% of concepts in MedMentions require disambiguation, suggesting a detected span of text can be linked to multiple UMLS concepts if only the span of text is considered. 2. ShARe/CLEF 2014 Task 2[37] -we used the development set containing 300 documents of 4 types -discharge summaries, radiology, electrocardiograms, and echocardiograms. We've used the UMLS annotations and ignored the attribute annotations. 3. MIMIC-III[31] -consists of \u223c 58, 000 de-identified EHRs from critical care patients collected between 2001-2012. MIMIC-III includes demographic, vital sign, and laboratory test data alongside unstructured free-text notes.\n\nWe attempted to use the SemEval 2019 shared task for the evaluation of the NER+L task 7 , but dataset access is currently under review for all requests to i2b2."
    },
    {
      "title": "Clinical Use Case Datasets",
      "text": "Our further experiments used real world EHR data from the following UK NHS hospital Trusts:\n\n\u2022 King's College Hospital Foundation Trust (KCH) Dataset: -300 free text inpatient notes for Covid-19 positive patients, 121 Epilepsy clinic letters 2018-2019, 100 Cardiac Clinic letters, 200 echocardiographic reports, 100 CT pulmonary angiograms, 700 10k character chunks of clinical notes of patients with Diabetes Mellitus/ Gastroenteritis/ Inflammatory bowel disease/ Crohn's disease/ Ulcerative colitis for supervised training. -\u223c 17M documents with \u223c 8.8B tokens (entire KCH electronic health record from 1999 to 2020 consisting documents from 'multi-era', multi-vendor electronic health records (including iSoft iCM, EMIS Symphony and AllScripts) and multiple geographically-distributed hospital sites (Kings College Hospital, Princess Royal University Hospital and Orpington Hospital) were processed for self-supervised training. \u2022 South London and Maudsley Foundation Trust (SLaM): 2200 free text notes for patients with a primary or secondary diagnosis of severe mental illness 7  https://competitions.codalab.org/competitions/1935  0  between 2007 and 2018 with each document reviewed for only a specific physical health comorbidity that may or may not appear in the note. \u2022 University College London Hospitals Foundation Trust (UCLH) Covid-19 Datasets: 300 Free text clinical notes for Covid-19 positive or suspected patients from Jan -Apr 2020 from single-vendor electronic health record (Epic).\n\nWe used two large biomedical concept databases and prepared them as described in our source-code repository  8  , the databases are:\n\n\u2022 UMLS 2018AB: 3.82 million concepts and 14 million unique concept names from 207 source vocabularies.\n\n\u2022 SNOMED CT UK edition: >659K concepts. The UK SNOMED CT clinical extension 20200401 and UK Drug Extension 20200325 with ICD-10 and OPCS-4 mappings."
    },
    {
      "title": "Named Entity Recognition and Linking Experimental Setup",
      "text": "We use MedMentions  [36] , ShARe/CLEF  [37]  and MIMIC-III  [31]  datasets in our experiments. We denote the 'MedMentions' dataset (i.e. all concepts) and 'Med-Mentions Disorders Only' (i.e. only concepts grouped under the Disorder group as shown in  [38] ). We train MedCAT self-supervised on MIMIC-III configured with the UMLS database. We denote the version using Word2Vec embeddings as 'MedCAT' and the one using Bio ClinicalBERT  [39]  embeddings as 'MedCAT BERT'.\n\nAn annotation by MedCAT is considered correct only if the exact text value was found and the annotation was linked to the correct concept in the CDB. We contrast our performance with the performance of tools presented in Section. 1.3. Appendix C provides self-supervised training configuration details."
    },
    {
      "title": "Clinical Use Case NER+L Experimental Setup",
      "text": "For our clinical use cases we extracted SNOMED-CT terms, the official terminology across primary and sec-ondary care for the UK National Health Health service, as this was preferred by our clinical teams over UMLS.\n\nFig.  3  shows our process of model training and distribution to partner hospital Trusts. Initially, we built our untrained MedCAT model using the SNOMED-CT concept vocabulary (M1), we then trained it self-supervised on the MIMIC-III dataset (M2). Next, the entire KCH EPR (17M documents with 8.8B tokens) is used for selfsupervised training (M3). We collect annotations with clinician experts at KCH and train supervised (M4). We share this model with each partner hospital site where further self-supervised training (M5, M7) and specific supervised training with their respective annotation datasets (M6, M8).\n\nSite-specific models (M3, M5, M7) are loaded into deployed instances of MedCATtrainer and configured with annotation projects to collect SNOMED-CT annotations for a range of site specific disorders, findings, symptoms, procedures and medications that our clinical teams are interested in for further research (i.e. already published work on Covid-19  [5, 6] ). These included chronic (i.e. diabetes mellitus, ischaemic heart disease, heart failure) and acute (cerebrovascular accident, transient ischemic attack) disorders. For comparison between sites we find 14 common extracted concept groups (Appendix A) and calculate F1 scores for each concept group and reporting average, standard deviation (SD), and interquartile-range (IQR).\n\nWe shared fine-tuned MedCAT models between KCH and 2 NHS partner Trusts UCLH and SLaM. This was a collaborative effort with each hospital team only having access to their respective hospital EHR / CogStack instance. Each site collected annotated data using MedCATtrainer, tested the original base model, a self-supervised only trained model and a final supervised trained model with the MedCATtrainer collected annotations."
    },
    {
      "title": "Clinical Use Case Contextualisation Model Exper-",
      "text": "imental Setup From ongoing and published work  [5, 6]  we configured and collected meta-annotation training examples and trained a variety of contextualisation models per site as defined in Table. 1. Our experiments test the effectiveness of our meta annotation modelling approach to flexibly learn contextual Site Task Values KCH Presence Affirmed / Negated / Hypothetical Experiencer Patient / Family / Other Temporality Past / Present / Future UCLH Negation Yes / No Experiencer Yes / No Problem Temporality Past Medical Issue / Current Problem Certainty Confirmed / Suspected Irrelevant Yes / No SLaM Status Patient / Other / NA Diagnosis Yes / No Table 1: Meta Annotation Tasks Defined Per Site, KCH = King's College Hospital NHS Foundation Trust, UCLH = University College London Hospitals NHS Foundation Trust, SLaM = South London and Maudsley NHS Foundation Trust cues by assessing cross-disorder and cross-site transferability (Section. 3.3.3. To assess cross-disorder transferability of each of the 11 disorder groups (as specified in Appendix A) we use the SLaM collected 'Diagnosis' dataset that consists of 100 annotations for each disorder group. We stratify our train/test sets by disorder, placing all examples for one disorder group in the test set and use the remaining disorder examples as a train set. We run this procedure 11 times so that each disorder group is tested once. We average all scores of each fold and report results.\n\nTo demonstrate cross-site transferability we derive an equivalent meta-annotation dataset from the 'Presence' (KCH) and 'Status' (SLaM) datasets as they are semantically equivalent despite having different possible annotation values. We merge 'Presence' annotations from Affirmed/Hypothetical/False to Affirmed/Other to match classes available in SLaM. We then train and test new meta annotation models between sites and datasets report average results."
    },
    {
      "title": "Results",
      "text": "We firstly present our concept recognition and linking results, comparing performance across previously described tools in Section. 1.3 using the UMLS concept database and openly available datasets presented in Section. 2.  4  We then present a qualitative analysis of learnt concept embeddings demonstrating the captured semantics of MedCAT concepts. Finally, we show real world clinical usage of the deployed platform to extract, link and contextualise SNOMED-CT concepts across multiple NHS hospital trusts in the UK."
    },
    {
      "title": "Entity Extraction and Linking",
      "text": "Table  2  presents our results for self-supervised training of MedCAT and NER+L performance compared with prior tools using openly available datasets. Metrics for all the tools were calculated consistently. Bold indicates best performance. For each manual annotation we check whether it was detected and linked to the correct Unified Medical Language System (UMLS) concept. The metrics are precision (P), recall (R) and the harmonic mean of precision and recall (F1). MedCAT mod-els were configured with UMLS concepts and trained (self-supervised) on MIMIC-III: the base version (Med-CAT) uses Word2Vec embeddings (trained on MIMIC-III), while (MedCAT BERT) uses static word embeddings from Bio ClinicalBERT  [39] . For the BERT version of MedCAT we do not use the full BERT model to calculate context representations, but only the pre-trained static word embeddings.\n\nOur results show MedCAT improves performance compared to all prior tools across all tested metrics (excluding precision when compared to ScispaCy/CLAMP -which are supervised models). We observe that the best performance across all tools is achieved on the ShARe/CLEF dataset. However, MedCAT still improves F1 performance by 9 percentage points over the next best system. We note the simpler Word2Vec embedding (base Med-CAT) on average performs better than the more expressive Bio ClinicalBERT (BERT) embeddings. We provide a further breakdown of the range of performances by MedCAT across MedMentions and ShARe/CLEF split by UMLS semantic type in Table 3: MedCAT performance for different UMLS semantic types on MedMentions and ShARe/CLEF"
    },
    {
      "title": "Qualitative Analysis",
      "text": "For concept disambiguation the MedCAT core library learns vector embeddings from the contexts in which a concept appears. This is similar to prior work  [40] , although we also present a novel self-supervised training algorithm, annotation system and wider workflow. Using our learnt concept embeddings we perform a qualitative analysis by inspecting concept similarities, with the expectation that similar concepts have similar embeddings. Table . 4 shows the learnt context embeddings capture medical knowledge including relations between diseases, medications and symptoms. We train MedCAT self-supervised over MIMIC-III  [31]  using the entirety of UMLS, 3.82 Million concepts from 207 separate vocabularies. Training configuration details are provided in Appendix C."
    },
    {
      "title": "Clinical Use Cases across Multiple Hospitals",
      "text": "The MedCAT platform was used in a number of clinical use cases providing evidence for its applicability to answer relevant, data intensive research questions. For example, we extracted relevant comorbid health conditions in individuals with severe mental illness and patients hospitalized after Covid-19 infection  [5, 6, 41] . These use cases analysed data sources from 2 acute secondary/tertiary care services at King's College Hospital (KCH), University College London Hospitals (UCLH) and mental health care services South London and Maudsley (SLaM) NHS Foundation Trusts in London, UK.\n\nThe following results focus on providing an aggregate view of MedCAT performance over real NER+L clinical use-cases, meta-annotation or context classification tasks and model transferability across clinical domains (physical health vs mental health), EHR systems and concepts."
    },
    {
      "title": "Entity Extraction and Linking",
      "text": "Table . 5 shows our results for NER+L across hospital sites, model and training configurations as described in Section 2.4.2 Our KCH annotations were collected across a range of clinicians, clinical research questions and therefore MedCATtrainer projects. This unfortunately led to a lack of resourcing to enable double annotations and calculation of inter-annotator-agreement (IIA) scores. SLaM annotations were collected by clinician / non-clinician pairs with average inter-annotator agreement (IIA) at 0.88, disagreements were discarded before results were calculated to ensure a gold-standard. UCLH IIA was at 0.85 between two medical students with annotation disagreements arbitrated by an experienced clinician providing the final gold-standard dataset. For our KCH results we use all annotations collected across various MedCATtrainer projects within our 14 concept groups as described in Section. 2.4.4 Both KCH and UCLH annotations contained occurrences of all 14 concept groups, SLaM annotated notes did not contain any occurrences of Dyspnea (SCTID:267036007), Pulmonary embolism (SCTID:59282003) and Chest pain (SCTID:29857009). Table 5: NER+L Results Across Hospitals. MedCAT NER+L performance for common disorder concepts defined in Appendix A by clinical teams. Annotations for supervised learning are used as test sets for models M1, M2, M3, M5, M7. Average performance on a 10 fold cross-validation with a held out test set is reported for models M4, M6, M8. KCH: Kings College Hospital; UCLH: University College Hospital; SLaM: South London and The Maudsley NHS Foundation Trusts."
    },
    {
      "title": "Entity Extraction and",
      "text": "comparing results across studies / sites is difficult as the definitions of tasks and concepts collected are different and therefore output trained models are bespoke. Table . 6a shows aggregate performance at each site, and Table . 6b-6c show further experiments for cross-site and crossconcept model transferability.\n\nWe achieve strong weighted (0.892-0.977) / macro (0.841-0.860) F1 performance across all tasks and sites, with breakdown of each metric per site/task available in Appendix D. We report average macro and weighted F1 score demonstrating the variation in performance due to unbalanced datasets across most tasks.\n\nFor cross-concept transferability, Table . 6b shows a decrease in performance when stratifying by concept. However, we still observe a relatively high 0.82-0.85 score suggesting the model is capable of learning disorder independent representations that distinguish the classification boundary for the 'Diagnosis' task, not just the disorder specific contexts.\n\nOur cross-site transferability results, Table  6c , suggest the 'Status' context model that is trained on cross site (i.e. KCH) data then fine-tuned on site specific data (i.e. SLaM) performs better (+ 0. 0.08 Macro / + 0.09 Weighted F1) compared with training on only the SLaM site specific training only (i.e. comparing row 3 and 4)."
    },
    {
      "title": "Discussion"
    },
    {
      "title": "Named Entity Recognition and Linking",
      "text": "Our evaluation of MedCAT's NER+L method using self-supervised training was bench-marked against existing tools that are able to work with large biomedical databases and are not use-case specific. Our datasets and methods are publicly available making the experiments transparent, replicable, and extendable. With the Med-Mentions dataset, using only self-supervised learning, our results in 3.1, demonstrate an improvement on the prior tools for both disorder detection (F1=0.495 vs 0.464) and general concept detection (F1=0.448 vs. 0.429). We observe all tools perform best with the ShARe/CLEF dataset. We suggest this broadly due to the lack of ambiguity and the more clinical setting allowing alternative systems to also perform reasonably well.\n\nWe now discuss the result between our BERT and regular (Word2Vec) configured MedCAT models. Generally BERT, a deep neural embedding model, performs well for a range of downstream tasks  [28]  better than older approaches such as Word2Vec, i.e. a shallow neural embedding. We believe this due to our use of pre-trained static BERT embeddings that: 1) are not specifically trained to produce similar values for words appearing in a similar context, 2) Sub-word tokenization might be problematic if the tokenizer was trained on a non-medical dataset (no matter whether it was fine-tuned later on MIMIC-III, pubmed or similar).\n\nThe general concept detection task with MedMentions is difficult due to: the larger number of entities to be extracted, the rarity of certain concepts and the often highly context dependent nature of some occurrences. Recent work  [42]  highlights examples of ambiguous texts within the MedMentions dataset such as 'probe' with 7 possible labels ('medical device', 'indicator reagent or diagnostic aid' etc.) Further work  [40]  also showed a deep learning approach (BioBERT+) that achieved F1=0.56. When MedCAT is provided with the same supervised training data we achieve F1=0.71. We find our improved performance is due to the long tail of entities in MedMentions that lack sufficient training data for methods such BioBERT to perform well.\n\nOur qualitative inspection of the learnt concept embeddings, 3.2 indicate learnt semantics of the target medical domain. This result mirrors similar findings reported in fields such as materials science  [43] . Recent work has suggested an approach to quantity the effectiveness of learnt embeddings  [38]  in representing the source ontology. However, this relies on concept relationships to be curated before assessment requiring clinical guidance that may be subjective in the clinical domain. We leave a full quantitative assessment of the learnt embeddings to future work for this reason.\n\nAs more concepts are extracted the likelihood of concepts requiring disambiguation increases, particularly in biomedical text  [44] . Estimating the number of training samples for successful disambiguation is difficult but based on our experiments we need at least 30 occurrences of a concept in the free text to perform disambiguation. We provide more details in Appendix B.\n\nFinally, we note that there are no limitations algorithmically for MedCAT to support languages other than our tested language, English. As MedCAT uses a concept dictionary/vocab for NER+L, if there are existing re- sources (e.g. SNOMED-CT has already been translated into Spanish, Dutch, Swedish and Danish) they can be used directly for these languages with likely similar results. Alternatively, users could build their own custom concept dictionary (CDB) for their language of choice. Meta-annotation or contextualisation models also do not have language specific features, i.e. English, and would also likely perform well as they only rely on bi-directional context from supervised examples to make predictions."
    },
    {
      "title": "Clinical Use Cases",
      "text": "MedCAT models and annotated training data have been implemented to be easily shared and reused, facilitating a federated learning approach to model improvement and specialisation with models brought to sensitive data silos. Our results in Section. 3.3 demonstrate that we are able directly apply models trained at one hospital site (KCH) to multiple other sites, and clinical domains (physical vs mental health datasets) with only a small drop in average F1 (0.044 at UCLH, 0.062 at SLaM), and after small amount of additional site specific training, we observe comparable performance (-0.021 at UCLH, -0.002 at SLaM).\n\nWe also highlight that separate teams were able to deploy, extract and analyse real clinical data using the tools as is by following provided examples, documentation and integrations with the wider CogStack ecosystem. Academic engineering projects are often built to support a single research project, however MedCAT and the CogStack ecosystem are scalable fit-for-purpose locally-tunable solutions for teams to derive value from their data instead of being stalled by poor quality code or lack of documentation. This means the model is broadly useful with top-up tuning also available for specific scenarios, domains and hospitals.\n\nEach hospital site and clinical team freely defined the set of contextualisation tasks and associated values for each task. On aggregate our results show performance is consistently strong across all sites and tasks (Macro F1: 0.841-0.860, Weighted F1: 0.892-0.977). With many of the tasks the annotated datasets are highly unbalanced. For example, the 'Presence' task at KCH, disorders are often only mentioned in the EHR if they are affirmed (e.g. \"...pmhx: TIA...\"), and only rarely are hypothetical (e.g. \"...patient had possible TIA...\") or negated terms (e.g. \"...no sign of TIA. . . \") encountered. This explains the differences in performance when reporting macro vs weighted average F1 score. We would expect generalization performance to lie between these reported metrics."
    },
    {
      "title": "Limitations",
      "text": "MedCAT is able to employ a self-supervised training method as the initial pass of the algorithm uses a given unique name to learn and improve an initial concept embedding. However, if the input vocabulary linked to the concepts inadequately specifies possible names or the given names of a concept rarely appear in the text then improvements can only occur during standard supervised learning. The main limitation of our approach is that it greatly depends on the quality of the concept database. Large biomedical concept databases (e.g. UMLS) however have a well specified vocabulary offering many synonyms, acronyms and differing forms of a given concept.\n\nA limitation of our concept embedding approach is if different concepts appear in similar contexts disambiguation and linking to the correct concept can be difficult. For example, 'OD' can link to 'overdose' or 'once daily', both referring to medications with very different implications. We have rarely seen this problem during real-world corpus. Our approach can also struggle if concepts appear in many varying contexts that are rarely seen or annotated for. With each new context updating the underlying concept embedding this may decrease performance of the embedding.\n\nSupervised learning requires training data to be consistently labelled. This is a problem in the clinical domain that consists of specialised language that can be open to interpretation. We recommend using detailed annotation guidelines that enumerate ambiguous scenarios for annotators."
    },
    {
      "title": "Future Work",
      "text": "MedCAT uses a vocabulary based approach to detect entity candidates. Future work could investigate the expansion of such an approach with a supervised learning model like BERT  [28] . The supervised learning model would then be used for detection of entity candidates that have enough training data and to overcome the challenge of detecting new unseen forms of concept names. The vocabulary based approach would cover cases with insufficient annotated training data or concepts that have few different names (forms). The linking process for both approaches would remain the same self-supervised.\n\nOur self-supervised training over the \u223c20 year KCH EHR, as described in Section. 2.4, took over two weeks to complete. Future work could improve the training speed by parallelizing this process since concepts in a CDB are mostly independent of one another. Further work could address effective model sharing, allowing subsequent users/sites to benefit from prior work, where only model validation and fine-tuning is required instead of training from scratch.\n\nFinally, ongoing work aims to extend the MedCAT library to address relation identification and extraction. For example, linking the extracted drug dosage / frequency with the associated drug concept, or identifying relations between administered procedures and following clinical events."
    },
    {
      "title": "Conclusions",
      "text": "This paper presents MedCAT a multi-domain clinical natural language processing toolkit within a wider ecosystem of open-source technologies namely CogStack.\n\nThe biomedical community is unique in that considerable efforts have produced comprehensive concept databases such as UMLS and SNOMED-CT amongst many others. MedCAT flexibly leverages these efforts in the extraction of relevant data from a corpus of biomedical documents (e.g. EHRs). Each concept can have one or more equivalent names, such as abbreviations or synonyms. Many of these names are ambiguous between concepts. The MedCAT library is based upon a simple idea: at least one of the names for each concept is unique and given a large enough corpus that name will be used in a number of contexts. As the context is learned from the unique name, when an ambiguous name is later detected, its context is compared to the learnt context, allowing us to find the correct concept to link. By comparing the context similarity we can also calculate confidence scores for a provided linked concept.\n\nWith MedCAT we have built an effective, high performance IE algorithm demonstrating improved performance over prior solutions on open access datasets. We Overall, MedCAT is built to enable clinical research and potential improvements of care delivery by leveraging data in existing clinical text. Currently, MedCAT is deployed in a number of hospitals in the UK in silo or as part of the wider CogStack ecosystem, with wide-ranging use cases to inform clinical decisions with real-time alerting, patient stratification, clinical trial recruitment and clinical coding. The large volume of medical information that is captured solely in free text is now accessible using stateof-the-art healthcare specific NLP. that represents MedCAT's annotation of the records. Access to the medical records will not be possible given their confidential nature. SLaM: This project was approved by the CRIS Oversight Committee which is responsible for ensuring all research applications comply with ethical and legal guidelines. The CRIS system enables access to anonymised electronic patient records for secondary analysis from SLaM and has full ethical approvals. CRIS was developed with extensive involvement from service users and adheres to strict governance frameworks managed by service users. It has passed a robust ethics approval process acutely attentive to the use of patient data. Specifically, this system was approved as a dataset for secondary data analysis on this basis by Oxfordshire Research Ethics Committee C (08/H06060/71). The data is de-identified and used in a data-secure format and all patients have the choice to opt-out of their anonymized data being used. Approval for data access can only be provided from the CRIS Oversight Committee at SLaM.\n\nTable A.1: SNOMED-CT concept level groupings for clinical use cases Container Concept Concepts"
    },
    {
      "text": "Figure 2: An example MedCAT workflow using the MedCAT core library and MedCATtrainer technologies to support clinical research."
    },
    {
      "text": "Figure 3: Model provenance for NER+L clinical use case results between datasets and sites. M1-8, showing the MedCAT model instances, the data and method of training and base model used across all sites."
    },
    {
      "text": "have commoditised the development, deployment and implementation of IE pipelines with supporting technologies MedCATtrainer / MedCATservice supporting the transfer, validation, re-use and fine-tuning of MedCAT models across sites, clinical domains and concept vocabularies. MedCAT deployments are enabled by extensive documentation, examples, APIs and supporting real world clinical use cases outlined in prior published work."
    },
    {
      "text": "Table. 3. Comparison of NER+L tools for the extraction of UMLS concepts. *The results for ScispaCy/CLAMP are not directly comparable to other tools as they are supervised models."
    },
    {
      "text": "Linking Model Transferability Table.5 demonstrates the improved NER+L performance that arises from using domain specific data first self-supervised in MIMIC-III, then KCH.We observe further improvements with clinician expertise with supervised training using the KCH data.With model sharing to UCLH we observe a 0.044 average drop in F1 performance compared to KCH.Further self-supervised training directly on UCLH data offers minimal average performance gains but does reduce the F1 SD and IQR suggesting there is less variability in performance across concepts.Supervised training on a small (499) annotations from UCLH delivers comparable performance to our KCH trained model.For our experiments at SLaM we see average F1 performance drop initially by 0.062 using the KCH model directly on SLaM data.SLaM is a large mental health service provider where EHRs are markedly different to acute care hospitals KCH and UCLH.Interestingly, successive self-supervised (M7) and supervised training (M8) show benefits across all measures with final performance largely similar to final KCH performance.Importantly, this suggests performance is transferred to the different hospital sites and initially only drops by \u223c0.04. With self-supervised training and further supervised training we are able to reach KCH performance with \u223c 7\u00d7 fewer manually collected examples at UCLH or \u223c 2\u00d7 fewer examples at SLaM. Qualitative Analysis of Learnt Concept Embeddings. UMLS concepts that have highest cosine similarity between learnt vector embeddings of concepts in bold. The first row defines the chosen concept and the target concept type. We have randomly chosen the most frequent concepts and presented the 8 most similar concepts for each target concept type. For example, Neoplastic Process (C0006826) and the following rows show the top 8 most similar Procedure concepts."
    },
    {
      "text": "Contextualisation Model ResultsSite Specific Contextualisation Model Performance.Weighted / Macro average F1 Meta annotation model performance custom defined and trained per site -detailed definitions are provided in Appendix D. Task definitions are uniquely defined at each site, e.g.Experiencer at KCH considers the values patient / family / other whereas Experiencer at UCLH only considers the value patient / other.Status at SLaM considers the values affirmed / other and Certainty at UCLH considers the values confirmed / suspected.We include all concepts of interest as defined under clinician guidance at each site, therefore site-to-site comparison in performance cannot be made.Cross Site Transferability Performance.11foldconcept stratified CV vs randomized CV for SLaM 'Diagnosis' contextualisation task performance.The 11 concepts were selected from NER+L experiment concepts available at SLaM (Supplementary Table1).The 'Diagnosis' task at SLaM was used as this was our most balanced dataset between all tasks and concepts collected.Cross-site transferability of the MetaCAT model for Presence (at KCH) / Status (at SLaM converted to values of Affirmed/Other) -as that was the only task that existed across sites. Results show 10 fold CV where applicable -e.g. row 2 is direct testing of the KCH model on SLaM data, so no training is performed on the SLaM side."
    }
  ],
  "references": [
    {
      "title": "-709044004 -Chronic kidney disease (disorder) S-230690007 -Cerebrovascular accident",
      "doi": "10.1007/springerreference_167678"
    },
    {
      "title": "MedCATTrainer: A biomedical free text annotation interface with active learning and research use case specific customisation",
      "authors": [
        "T Searle",
        "Z Kraljevic",
        "R Bendayan",
        "D Bean",
        "R Dobson"
      ],
      "year": 2019,
      "doi": "10.18653/v1/d19-3024"
    },
    {
      "title": "ACE-inhibitors and angiotensin-2 receptor blockers are not associated with severe SARS-COVID19 infection in a multi-site UK acute hospital trust",
      "authors": [
        "D Bean",
        "Z Kraljevic",
        "T Searle",
        "R Bendayan",
        "G Kevin",
        "A Pickles",
        "A Folarin",
        "L Roguski",
        "K Noor",
        "A Shek",
        "R Zakeri",
        "A Shah",
        "J Teo",
        "R Dobson"
      ],
      "year": 2020,
      "doi": "10.1002/ejhf.1924"
    },
    {
      "title": "Evaluation and improvement of the national early warning score (NEWS2) for COVID-19: a multi-hospital study",
      "authors": [
        "E Carr",
        "R Bendayan",
        "D Bean",
        "M Stammers",
        "W Wang",
        "H Zhang",
        "T Searle",
        "Z Kraljevic",
        "A Shek",
        "H Phan",
        "W Muruet",
        "A Shinton",
        "T Shi",
        "X Zhang",
        "A Pickles",
        "D Stahl",
        "R Zakeri",
        "K O'gallagher",
        "A Folarin",
        "L Roguski",
        "F Borca",
        "J Batchelor",
        "X Wu",
        "J Sun",
        "A Pinto",
        "B Guthrie",
        "C Breen",
        "A Douiri",
        "H Wu",
        "V Curcin",
        "J Teo",
        "A Shah",
        "R Dobson"
      ],
      "year": 2020,
      "doi": "10.1101/2020.04.24.20078006"
    },
    {
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "\u0141 Kaiser"
      ],
      "year": 2017
    },
    {
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": 1997,
      "doi": "10.1162/neco.1997.9.8.1735"
    },
    {
      "title": "Universal language model finetuning for text classification",
      "authors": [
        "J Howard",
        "S Ruder"
      ],
      "year": 2018,
      "doi": "10.18653/v1/p18-1031"
    },
    {
      "title": "The open biomedical annotator",
      "authors": [
        "C Jonquet",
        "N Shah",
        "M Musen"
      ],
      "year": 2009
    },
    {
      "title": "Deep patient: An unsupervised representation to predict the future of patients from the electronic health records",
      "authors": [
        "R Miotto",
        "L Li",
        "B Kidd",
        "J Dudley"
      ],
      "year": 2016,
      "doi": "10.1038/srep26094"
    },
    {
      "title": "Deep representation learning of electronic health records to unlock patient stratification at scale",
      "authors": [
        "I Landi",
        "B Glicksberg",
        "H.-C Lee",
        "S Cherng",
        "G Landi",
        "M Danieletto",
        "J Dudley",
        "C Furlanello",
        "R Miotto"
      ],
      "year": 2020,
      "doi": "10.1038/s41746-020-0301-z"
    },
    {
      "title": "An overview of MetaMap: historical perspective and recent advances",
      "authors": [
        "A Aronson",
        "F.-M Lang"
      ],
      "year": 2010,
      "doi": "10.1136/jamia.2009.002733"
    },
    {
      "title": "Bio-YODIE: A named entity linking system for biomedical text",
      "authors": [
        "G Gorrell",
        "X Song",
        "A Roberts"
      ],
      "year": 2018
    },
    {
      "title": "Se-mEHR: A general-purpose semantic search system to surface semantic data from clinical notes for tailored care, trial recruitment, and clinical research",
      "authors": [
        "H Wu",
        "G Toti",
        "K Morley",
        "Z Ibrahim",
        "A Folarin",
        "R Jackson",
        "I Kartoglu",
        "A Agrawal",
        "C Stringer",
        "D Gale",
        "G Gorrell",
        "A Roberts",
        "M Broadbent",
        "R Stewart",
        "R Dobson"
      ],
      "year": 2018,
      "doi": "10.1093/jamia/ocx160"
    },
    {
      "title": "Named entity recognition for electronic health records: A comparison of rule-based and machine learning approaches",
      "authors": [
        "P Gorinski",
        "H Wu",
        "C Grover",
        "R Tobin",
        "C Talbot",
        "H Whalley",
        "C Sudlow",
        "W Whiteley",
        "B Alex"
      ],
      "year": 2019
    },
    {
      "title": "Mayo clinical text analysis and knowledge extraction system (cTAKES): architecture, component evaluation and applications",
      "authors": [
        "G Savova",
        "J Masanz",
        "P Ogren",
        "J Zheng",
        "S Sohn",
        "K Kipper-Schuler",
        "C Chute"
      ],
      "year": 2010,
      "doi": "10.1136/jamia.2009.001560"
    },
    {
      "title": "UIMA: an architectural approach to unstructured information processing in the corporate research environment",
      "authors": [
        "D Ferrucci",
        "A Lally"
      ],
      "year": 2004,
      "doi": "10.1017/s1351324904003523"
    },
    {
      "title": "Opennlp: A java-based nlp toolkit",
      "authors": [
        "T Morton",
        "J Kottmann",
        "J Baldridge",
        "G Bierner"
      ],
      "year": 2005
    },
    {
      "title": "Scis-paCy: Fast and robust models for biomedical natural language processing",
      "authors": [
        "M Neumann",
        "D King",
        "I Beltagy",
        "W Ammar"
      ],
      "year": 2019,
      "doi": "10.18653/v1/w19-5034"
    },
    {
      "title": "Clamp -a toolkit for efficiently building customized clinical natural language processing pipelines",
      "authors": [
        "E Soysal",
        "J Wang",
        "M Jiang",
        "Y Wu",
        "S Pakhomov",
        "H Liu",
        "H Xu"
      ],
      "year": 2017,
      "doi": "10.1093/jamia/ocx132"
    },
    {
      "title": "Bio-Portal: enhanced functionality via new web services from the national center for biomedical ontology to access and use ontologies in software applications",
      "authors": [
        "P Whetzel",
        "N Noy",
        "N Shah",
        "P Alexander",
        "C Nyulas",
        "T Tudorache",
        "M Musen"
      ],
      "year": 2011,
      "doi": "10.1093/nar/gkr469"
    },
    {
      "title": "Consumer health concepts that do not map to the UMLS: Where do they fit?",
      "authors": [
        "A Keselman",
        "C Smith",
        "G Divita",
        "H Kim",
        "A Browne",
        "G Leroy",
        "Q Zeng-Treitler"
      ],
      "year": 2008,
      "doi": "10.1197/jamia.m2599"
    },
    {
      "title": "Standard lexicons, coding systems and ontologies for interoperability and semantic computation in imaging",
      "authors": [
        "K Wang"
      ],
      "year": 2018,
      "doi": "10.1007/s10278-018-0069-8"
    },
    {
      "title": "Data protection and information governance",
      "year": 2020,
      "doi": "10.1201/9781315385488-27"
    },
    {
      "title": "Fostering multilinguality in the UMLS: A computational approach to terminology expansion for multiple languages",
      "authors": [
        "J Hellrich",
        "U Hahn"
      ],
      "year": 2014
    },
    {
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": 2013
    },
    {
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": 2018
    },
    {
      "title": "Enriching word vectors with subword information",
      "authors": [
        "P Bojanowski",
        "E Grave",
        "A Joulin",
        "T Mikolov"
      ],
      "year": 2017,
      "doi": "10.1162/tacl_a_00051"
    },
    {
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": 2014,
      "doi": "10.3115/v1/d14-1162"
    },
    {
      "title": "MIMIC-III, a freely accessible critical care database",
      "authors": [
        "A Johnson",
        "T Pollard",
        "L Shen",
        "L.-W Lehman",
        "M Feng",
        "M Ghassemi",
        "B Moody",
        "P Szolovits",
        "L Celi",
        "R Mark"
      ],
      "year": 2016,
      "doi": "10.1038/sdata.2016.35"
    },
    {
      "title": "An attention-based BiLSTM-CRF approach to document-level chemical named entity recognition",
      "authors": [
        "L Luo",
        "Z Yang",
        "P Yang",
        "Y Zhang",
        "L Wang",
        "H Lin",
        "J Wang"
      ],
      "year": 2018,
      "doi": "10.1093/bioinformatics/btx761"
    },
    {
      "title": "Cross-type biomedical named entity recognition with deep multi-task learning",
      "authors": [
        "X Wang",
        "Y Zhang",
        "X Ren",
        "Y Zhang",
        "M Zitnik",
        "J Shang",
        "C Langlotz",
        "J Han"
      ],
      "year": 2019,
      "doi": "10.1093/bioinformatics/bty869"
    },
    {
      "title": "Leveraging biomedical resources in Bi-LSTM for Drug-Drug interaction extraction",
      "authors": [
        "B Xu",
        "X Shi",
        "Z Zhao",
        "W Zheng"
      ],
      "year": 2018,
      "doi": "10.1109/access.2018.2845840"
    },
    {
      "title": "Comparative analysis of text classification approaches in electronic health records",
      "authors": [
        "A Mascio",
        "Z Kraljevic",
        "D Bean",
        "R Dobson",
        "R Stewart",
        "R Bendayan",
        "A Roberts"
      ],
      "year": 2020,
      "doi": "10.18653/v1/2020.bionlp-1.9"
    },
    {
      "title": "MedMentions: A large biomedical corpus annotated with UMLS concepts",
      "authors": [
        "S Mohan",
        "D Li"
      ],
      "year": 2019
    },
    {
      "title": "Others, Task 2: ShARe/CLEF ehealth evaluation lab",
      "authors": [
        "D Mowery",
        "S Velupillai",
        "B South",
        "L Christensen",
        "D Martinez",
        "L Kelly",
        "L Goeuriot",
        "N Elhadad",
        "S Pradhan",
        "G Savova"
      ],
      "year": 2014
    },
    {
      "title": "Exploring semantic groups through visual approaches",
      "authors": [
        "O Bodenreider",
        "A Mccray"
      ],
      "year": 2003
    },
    {
      "title": "Publicly available clinical BERT embeddings",
      "authors": [
        "E Alsentzer",
        "J Murphy",
        "W Boag",
        "W.-H Weng",
        "D Jin",
        "T Naumann",
        "M Mcdermott"
      ],
      "year": 2019,
      "doi": "10.18653/v1/w19-1909"
    },
    {
      "title": "Clinical concept embeddings learned from massive sources of multimodal medical data",
      "authors": [
        "A Beam",
        "B Kompa",
        "A Schmaltz",
        "I Fried",
        "G Weber",
        "N Palmer",
        "X Shi",
        "T Cai",
        "I Kohane"
      ],
      "year": 2018,
      "doi": "10.1142/9789811215636_0027"
    },
    {
      "title": "A case-control and cohort study to determine the relationship between ethnic background and severe",
      "authors": [
        "R Zakeri",
        "R Bendayan",
        "M Ashworth",
        "D Bean",
        "H Dodhia",
        "S Durbaba",
        "K Gallagher",
        "C Palmer",
        "V Curcin",
        "E Aitken",
        "W Bernal",
        "R Barker",
        "S Norton",
        "M Gulliford",
        "J Teo",
        "J Galloway",
        "R Dobson",
        "A Shah"
      ],
      "year": 2020,
      "doi": "10.1101/2020.07.08.20148965"
    },
    {
      "title": "Extracting UMLS concepts from medical text using general and Domain-Specific deep learning models",
      "authors": [
        "K Fraser",
        "I Nejadgholi",
        "B De Bruijn",
        "M Li",
        "A Laplante",
        "K El Abidine"
      ],
      "year": 2019
    },
    {
      "title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
      "authors": [
        "V Tshitoyan",
        "J Dagdelen",
        "L Weston",
        "A Dunn",
        "Z Rong",
        "O Kononova",
        "K Persson",
        "G Ceder",
        "A Jain"
      ],
      "year": 2019,
      "doi": "10.1038/s41586-019-1335-8"
    },
    {
      "title": "Term identification in the biomedical literature",
      "authors": [
        "M Krauthammer",
        "G Nenadic"
      ],
      "year": 2004,
      "doi": "10.1016/j.jbi.2004.08.004"
    }
  ],
  "num_references": 42,
  "original_doi": "https://doi.org/10.13039/501100000265"
}
