{
  "paper_id": "M5Y76GSL",
  "title": "A reinforcement learning based algorithm for personalization of digital, just-in-time, adaptive interventions",
  "abstract": "A C T Suboptimal health related behaviors and habits; and resulting chronic diseases are responsible for majority of deaths globally. Studies show that providing personalized support to patients yield improved results by preventing and/or timely treatment of these problems. Digital, just-in-time and adaptive interventions are mobile phone-based notifications that are being utilized to support people wherever and whenever necessary in coping with their health problems. In this research, we propose a reinforcement learning-based mechanism to personalize interventions in terms of timing, frequency and preferred type(s). We simultaneously employ two reinforcement learning models, namely intervention-selection and opportune-moment-identification; capturing and exploiting changes in people's long-term and momentary contexts respectively. While the intervention-selection model adapts the intervention delivery with respect to type and frequency, the opportune-moment-identification model tries to find the most opportune moments to deliver interventions throughout a day. We propose two accelerator techniques over the standard reinforcement learning algorithms to boost learning performance. First, we propose a customized version of eligibility traces for rewarding past actions throughout an agent's trajectory. Second, we utilize the transfer learning method to reuse knowledge across multiple learning environments. We validate the proposed approach in a simulated experiment where we simulate four personas differing in their daily activities, preferences on specific intervention types and attitudes towards the targeted behavior. Our experiments show that the proposed approach yields better results compared to the standard reinforcement learning algorithms and successfully capture the simulated variations associated with the personas.",
  "year": 2015,
  "date": "2015",
  "journal": "Transl Behav Med",
  "publication": "Transl Behav Med",
  "authors": [
    {
      "forename": "Suat",
      "surname": "G\u00f6n\u00fcl",
      "name": "Suat G\u00f6n\u00fcl",
      "affiliation": "a  SRDC Corp. , Silikon Blok Kat : \n\t\t\t\t\t\t\t\t SRDC Corp \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Silikon Blok Kat"
    },
    {
      "forename": "Tuncay",
      "surname": "Naml\u0131",
      "name": "Tuncay Naml\u0131",
      "affiliation": "a  SRDC Corp. , Silikon Blok Kat : \n\t\t\t\t\t\t\t\t SRDC Corp \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Silikon Blok Kat"
    },
    {
      "forename": "Ahmet",
      "surname": "Cos \u00b8ar",
      "name": "Ahmet Cos \u00b8ar",
      "affiliation": "b  Department of Computer Engineering , Middle East Technical University , Orta Dogu \n\t\t\t\t\t\t\t\t Department of Computer Engineering \n\t\t\t\t\t\t\t\t Middle East Technical University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Orta Dogu"
    },
    {
      "forename": "Hakk\u0131",
      "surname": "Toroslu",
      "name": "Hakk\u0131 Toroslu",
      "affiliation": "b  Department of Computer Engineering , Middle East Technical University , Orta Dogu \n\t\t\t\t\t\t\t\t Department of Computer Engineering \n\t\t\t\t\t\t\t\t Middle East Technical University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Orta Dogu"
    },
    {
      "affiliation": "1  No:"
    },
    {
      "affiliation": "16  SRDC Teknokent ODT\u00dc , Ankara , Turkey \n\t\t\t\t\t\t\t\t SRDC Teknokent ODT\u00dc \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Ankara \n\t\t\t\t\t\t\t\t\t Turkey"
    },
    {
      "affiliation": "Teknik \u00dcniversitesi Universiteler Mah. Dumlupinar Blv. No:1 06800 , Ankara Turkey \n\t\t\t\t\t\t\t\t Teknik \u00dcniversitesi Universiteler \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Mah. Dumlupinar Blv. No:1 \n\t\t\t\t\t\t\t\t\t 06800 \n\t\t\t\t\t\t\t\t\t Ankara \n\t\t\t\t\t\t\t\t\t Turkey"
    }
  ],
  "doi": "10.1016/j.artmed.2021.102062",
  "arxiv": "arXiv:1104.4664",
  "sections": [
    {
      "title": "Introduction"
    },
    {
      "title": "Motivation",
      "text": "Suboptimal health behaviors and chronic diseases are responsible for approximately 40% of preventable deaths, in addition to their undesirable effects on quality of life and economy  [1] . Chronic diseases are so prevalent nowadays that they cause deaths of approximately 40 million people yearly, corresponding to 70% of the deaths globally  [2] . People's self-management abilities are of critical importance with regards to treatment of suboptimal health behaviors, habits; and chronic diseases. Some common self-management activities include, but not limited to, complying with dietary plans, adherence to prescribed medications or performing physical exercises  [3] . Supporting people in such activities necessitates consideration of individuals' unique lifestyles and priorities; their psychological and psychosocial contexts; and environmental factors  [4] . It is proven in many studies that providing personalized self-management support related to chronic diseases or common health problems yield better results compared to person-invariant approaches  [5] [6] [7] [8] .\n\nDigital interventions are very suitable to support people in their selfmanagement activities. Recent technological advances in mobile and health sensors (wearable, implanted and digestible) areas  [9]  have provided means to deliver adaptive interventions via mobile devices momentarily, when required. In this respect, the main objective of this work is to deliver an algorithm that learns personalized intervention delivery strategies with consideration to people's long-term and momentary contexts. The aim, in turn, as related to real-life care programs, 's to maximize adherence by preventing avoidable burden of these interventions and thus achieve better clinical outcomes eventually."
    },
    {
      "title": "Reinforcement Learning (RL) for personalization of JITAIs",
      "text": "The just-in-time adaptive intervention (JITAI) concept is introduced by Nahum-Shani et al. to define momentary interventions via a number of components such as decision points or intervention options  [10] . We propose an RL  [11]  based algorithm for personalization of JITAI components. Using RL for this purpose is convenient for a number of reasons: Firstly, an RL agent learns through experience. Therefore, it does not require an initial data set, as used by other forms of learning methods to train an internal model or policy. However, it is still possible to bootstrap the learning process to make sensible choices of interventions by configuring the policy based on external expert knowledge  [12, 13] .\n\nThe RL framework is convenient for modeling people's varying contexts (e.g. environmental or psychosocial) and devising a learning algorithm based on each individual's continuously changing context values. Having an iterative nature, an RL agent performs actions based on its environment while continuously observing state changes of the environment; and updates its learning model based on the rewards. Similarly, in our case, the learning algorithm reacts to the changing contexts of an individual and updates its internal policy. By doing so, the algorithm is expected to construct a (near-) optimal personalized intervention delivery policy.\n\nIn principle, traditional RL already provides mechanisms to learn solutions for any task without the need of human supervision. However, if RL agents begin learning tabula rasa, i.e. no prior knowledge from a domain expert is available, the number of samples needed to learn a nearly optimal solution is often prohibitively large for real-world problems  [14, 15] . So, learning a near-optimal policy is often slow or infeasible, especially if the experiment involves real-world subjects. In addition to long learning times, learning performance in a traditional RL system is usually asymptotic to a logarithmic curve, meaning that mostly random actions would be taken during the beginning phase of the learning process when the state space is scarcely explored  [16] . Addressing these challenges about the application of RL in problems with real subjects, our objective is to deliver an algorithm that learns a personalized policy quickly, which produces acceptible results even when the state space is scarcely explored. The algorithm is supposed to adapt its intervention delivery strategy dynamically and systematically based on people's ever-changing, unique momentary and long-term contexts."
    },
    {
      "title": "Main contributions",
      "text": "In our recent work  [17] , a single RL model was developed in order to optimize all aspects of JITAIs. Several extensions are built in this research based on this work. Main contributions of this work are listed below:\n\n\u2022 Modeling the personalized JITAI delivery problem by simultaneously employing two RL models, namely intervention-selection and opportune-moment-identification. They learn personalized patterns with respect to two intrinsic characterics of JITAIs: adaptiveness and justin-timeness respectively. The intervention-selection model adapts intervention delivery considering that a person still needs to be supported towards forming habit on a targeted behavior. The model learns personal preferences on intervention types and frequency throughout the learning process. The opportune-moment-identification model, learns personalized patterns to deliver interventions at the most opportune moments during the day. \u2022 Integration of an empirically validated habit formation model  [18]  for realistic simulation of habit related parameters included in people's long terms contexts. \u2022 Mathematical background for the main RL models as well as the optimization techniques utilized by these models. The interventionselection model utilizes habit formation model in order to simulate people's habitual behaviors and habit formation processes; the opportune-moment-identification model utilizes selective eligibility traces and transfer learning techniques to accelerate the learning process to learn personalized patterns and devise tailored intervention delivery strategies. \u2022 Simulation-based validation where personas are simulated with variations in their daily activities, preferences on specific intervention types and attitudes towards a targeted behavior. \u2022 Analysis of the performance of the two RL-models individually and validation of their hyperparameter configurations."
    },
    {
      "title": "Organization of the paper",
      "text": "In the rest of the manuscript, Section 2 presents related works related to personalized intervention delivery and methods beyond the traditional RL. Section 3 includes the main concepts and approach. Section 4 presents the learning algorithm after elaborating accelerator techniques applied on the standard methods. Section 5 presents an instantiation of the conceptual approach by elaborating two Markov Decision Process (MDP)-based RL models. Section 6 presents the simulation-based validation approach by giving details about the variations of the simulated personas. After presenting the results of the validation approach and discussing their alignment with the simulation configurations in Section 7, we conclude the paper."
    },
    {
      "title": "Related work",
      "text": "We investigate related works from two main perspectives. We first analyze studies providing personalized support (in the form of mobile phone-based interventions) to people suffering from chronic diseases or health problems. We are specifically interested in their system design and personalization methods. Second, we present studies proposing alternative ways of rewarding past actions and applying transfer learning between learning environments."
    },
    {
      "title": "Personalization of interventions via adaptivity and just-in-timeness.:",
      "text": "Personalized guidance to people suffering from health problems have been studied in many studies  [19] [20] [21] . Gustafson et al. developed a framework with several self-management support modules for overcoming alcohol addiction  [22] . GPS monitoring module, one of the modules of their system, warns people when they are close to a pre-defined place. The work proposed by Waki et al. processes patients' inputs about food and physical exercise, collected twice a day, and generates an intervention accordingly  [23] . Personalization via RL is applied in other domains such as personalized medicine  [24]  in critical care or education  [25] . We consider such systems as static considering their decision making methods on delivering interventions. In contrary to such approaches, our approach keeps a dynamic set of decision rules inside the RL algorithm's policy. This provides a systematic way to improve/adapt intervention delivery dynamically in time according to the changing person contexts.\n\nOptimization of intervention delivery with respect to just-intimeness and adaptivity were investigated by many studies in the literature. Some of them tackle specific health problems. For example, the agent-based model developed by Chih et al. discovers the predictors of food choice and obesity  [26] . Similarly, the system proposed by Goldstein et al. predicts dietary lapses and delivers interventions tailored for individuals  [27] . Their system uses supervised machine learning techniques working on physiological context variables and identifies strong predictors of potential lapses. On the other hand, there are also studies devising more generic systems that optimize JITAI components dynamically. These systems implement mathematical models of existing behavior change theories by capturing long-term changes in the model parameters and adapting the intervention parameters accordingly  [28] . For instance, the system proposed by Navarro-Barrientos et al.  [29]  realizes the Theory of Planned Behavior  [30] . The system targets adaptation of intervention parameters so that users of the system can lose weight. Martin et al.'s system benefits from the Social Cognitive Theory  [31]  in supporting people with their physical activity tasks  [32] . Spruitz et al. introduce a generic conceptual approach to devise data-driven mathematical models for JITAIs  [33] .\n\nThe study conducted by Lei et al. is closest to ours in terms of utilization of RL for optimization of JITAI parameters  [34] . They reduce the intervention delivery problem into a contextual bandit problem by modelling the states of the RL environment with various momentary context parameters. As the learning algorithm, the referred study benefits from a native actor-critic approach while ours employs an actor-only approach learning in an online setting. They also present a simulation-based validation approach to investigate the performance of their approach. They assume a person context composed of i.i.d. variables. As described in  [35] , we utilize a template-based context simulation where contexts are composed of individual variables that are randomized with Gaussian distribution. Like us, Lei et al. also considers intervention burden on people where the burden effect is reflected to the reward function of the RL setup. Our simulation approach also includes the habit concept and incorporates the habit formation model to simulate relevant parameters.\n\nRewarding past actions.: Eligibility traces are one of the basic mechanisms for rewarding past actions. Basic eligibility tracing has two variations, namely accumulating traces and replacing traces. In the accumulating traces, eligibility value of a revisited state is increased by 1, which allows eligibility values larger than 1. In the replacing traces case, though, eligibility values of revisited states are reset to 1. This means that frequency and recency are primary heuristics in accumulating and replacing traces respectively. In our approach, we utilize accumulating traces as we want a fast learning rate for frequently visited states.\n\nSome studies are similar to ours in terms of selective, postponed rewarding of past actions considering that some of the actions taken could be suboptimal. For example, Bloch et al. propose the Temporal Second Difference Traces method so that trace updates can be performed after apparently suboptimal actions are taken  [36] . In addition to keeping track of a single eligibility value for visited states, the method keeps track of all updates on the eligilibility values. Using this information, it is able to optimize the updates as more information becomes available. Other relevant rewarding mechanisms include postponing the update of a visited state by recording the agents' recent experiences until the the state is visited again. The aim is to improve the quality of the update with more accurate information  [37] . Kormushev et al.  propose a concept, called Time Hopping, to prevent failures (i.e. taking wrong actions) in scarcely explored states  [16] . The approach, though, is only applicable for simulation-based studies as setting the environment state as desired in real-life is not feasible.\n\nTransfer learning strategies between learning environments.: From a pure RL perspective, the survey study conducted by Taylor et al. provides a comprehensive summary about application of transfer learning in the RL domain  [38] . The survey classifies related studies into groups based on similarities and differences between environments among which knowledge to be transferred. Referring to this survey, our study is classified into the group where state and action spaces of the environments are the same. Studies in this group that also transfer whole policies among tasks either learn from easy missions or break down the task to easier missions. However, the JITAI personalization problem does not have a modular structure. So, approaches working on hierarchical or modular tasks would not work on our problem. Lazaric follows a similar approach with us where state transitions are saved as tuples of source state, taken action, target state and emitted reward. His method checks similarity of information collected in an environment with information obtained in other environments to decide transfer the knowledge or not  [39] . In our study, only state and action pairs are stored at each transition. Such pairs are aggregated in a single common policy indicating the action to be selected in the corresponding state. Nevertheless, partial policy transfers or criteria-based knowledge transfer as in  [40]  might be appropriate for our case as well. Source environments might have certain regions with relatively higher similarities with the other policies.\n\nData associated with a single person is deemed insufficient when it comes to devise an intervention delivery policy tailored to an individual via RL methods  [41] . In this sense, Tabatabaei et al. propose a narrowing reinforcement learning approach to overcome the cold-start problem  [42] . They use a neighborhood function to group similar users and generated suitable policies based on the reactions of the users to the delivered interventions. The size of the neighborhood is inversely proportional with the collected experiences. This means that the policy becomes more individualized as the amount of the collected experience increases. Similarly, Zhu et al. introduce a group-driven RL for personalized intervention delivery  [41] . K-means clustering is utilized to group users based on similarity of activity trajectories and learn a shared RL policy for each group. Y\u00fcr\u00fcten applies timeseries analysis on activity data to recognize common behavioral profiles  [43] . In addition to daily activity patterns, his system tries to group users based on their reactions to the delivered interventions. In our approach, we seek similarity between states across environments i.e. persons. We use random forests for classifying states that are composed of contextual variables where the states are labeled with the actions as described previously."
    },
    {
      "title": "Problem formulation",
      "text": "The main challenge targeted by the study is to discover optimal values for intervention parameters such as type, timing or frequency that can change per individual. As the real-life experiment opportunities are limited, we propose a mechanism for simulating personas having different characteristics. The simulation mechanism accepts a set of inputs, such as daily action plans or personal preferences, as tailoring factors for the proposed learning algorithm. In addition, a mathematical habit formation model is used to simulate personas' habit formation process in a realistic way in terms of the duration of habit formation.\n\nThe proposed learning algorithm generates signals for necessity and timing the intervention by tracking and processing the simulated data. An intervention along with its metadata like timing or content is represented with the JITAI concept. Informally, a JITAI is defined as a method to capture a person's changing contextual state and provide an appropriate type of intervention wherever and whenever necessary. In our context, JITAI parameters (frequency, type and timing) are optimized via the two RL models.\n\nOverall, the proposed algorithm is an RL-based, iterative algorithm that aims to select the best action at each iteration. The algorithm runs in sync with people's self-management action plans and momentary contexts where each iteration of the RL models correspond to a decision point as described below:\n\nThe intervention-selection and opportune-moment-identification models have their own set of points to evaluate the relevant person context and take corresponding action. Decision point sets (DP AP and DP RW of the intervention-selection and opportune-momentidentification models respectively) are driven by action plans as described below:\n\nAn action plan contains a list of planned activities to be performed by the person for achieving the targeted behavior change and associated clinical outcomes  [44] . An action plan partitions a day, as depicted in Fig.  1 , into time frames. Each activity is associated with two sequential time frames. The first time frame is the period through which the activity is supposed to be performed. Only certain types of interventions can be delivered at each frame e.g. reminders in the first one and motivation or warning interventions in the second.\n\nDefinition 1 (DP AP ): An action plan yields a set of points in time where a decision is made for delivering an intervention. Let AP be the set of activities such that a \u2208 AP. Then, the set of the points generated for the action plan, DP AP , can be represented as,\n\nThe set definition above implies that there would be a corresponding decision point for both time frames [dp i ,dp i+1 ] associated with a planned activity.\n\nOpportune moments for engagement with an intervention are affected by a person's contextual states throughout the daily life. For instance, a state change from physical activeness to sedentary state might be an indicator of an opportune moment.\n\nDefinition 2 (DP RW ): The set, DP RW contains all time points where a contextual change occurs in a person's real world. Therefore, this set differs for each person; even for each day of a particular person."
    },
    {
      "title": "Definition 3 (JITAI):",
      "text": "A JITAI is represented with two mappings. For the intervention selection, it is a mapping from a state (is s n ) representing the person's long-term context to a decision (i.e. action) (is a n ) on whether to generate an intervention (for simplicity no intervention decision can also be considered as a type of intervention with no action) and its type. For the opportune-moment-identification case, the mapping is from a state (omi s n ) representing the person's momentary context to a decision (omi a n ) on when exactly to deliver the intervention.\n\nDefinition 4 (JITAI using RL): Fig.  2  shows how the execution of the RL models are projected onto the time frames of an action plan. \u2200dp, dp \u2208 DP AP , i.e. for all decision points of the intervention selection model, a state transition occurs such that the learning agent travels from is s 1 to is s n . At each decision point, the intervention selection model selects an intervention type for delivery. Similarly, \u2200dp,dp \u2208 DP RW , the opportunemoment-identification model makes state transitions starting from omi s 1 to omi s n . A state transition occurs when there is a change in the momentary context of the person. However, the opportune-momentidentification is disabled in cases no particular intervention type is selected by the intervention-selection model. For example, considering Fig.  2 , no intervention type is selected in is s 2 . Therefore, there is no state transition of the opportune-moment-identification model throughout the second time frame. When it is active, the opportunemoment-identification model decides to deliver the selected intervention or not at each decision point.\n\nRL models learn tailored intervention delivery strategies aiming maximum engagement with interventions, to achieve the expected behavior change and improved clinical indicators eventually. Considering that suboptimal support via interventions would lead to undesired results, our aim in this work is to achieve a near-optimal delivery strategy for an individual as soon as possible by maximizing the rewards collected by each model."
    },
    {
      "title": "Proposed approach",
      "text": "In this section, following a schematic diagram describing the overall approach, we present the accelerator techniques as an expansion on the base RL algorithms to boost the learning performance. Then, the overall algorithm shows how the two RL models interact with each other and these two techniques."
    },
    {
      "title": "Overall structure of the approach",
      "text": "As shown in Fig.  3 , the approach is composed of two main steps. Basically, in the first step i.e. the training phase, the algorithm trains a State Classifier to be used in the actual experiment to be able to reduce the number of random actions that are taken in unknown states, i.e. states that are not visited previously. As mentioned earlier, the core algorithm contains two RL models that run simultaneously as shown in Fig.  2 . The opportune-moment-identification model monitors the momentary changes of the simulated persona and take actions accordingly. In this phase, the opportune-moment-identification model utilizes only the selective eligibility traces method as the State Classifier is not available yet. On the other hand, the intervention-selection model monitors the habitual context of the persona via the habit formation model. As a result of the training phase, the State Classifier becomes available for the actual experiment where the opportune-momentidentification model uses this technique also for further improving the learning process. Fig. 2. Execution of the intervention-selection and opportune-moment-identification models in sync with the time frames associated to action plan activities."
    },
    {
      "title": "Selective eligibility traces",
      "text": "Fig.  4  depicts how the environment and agent interact with each other concerning the opportune-moment-identification-model. a i denotes the action taken when the environment is in state s i and r i is the reward received for a i . The boxes labeled with j indicate that an intervention is delivered to the person as an action i.e. Deliver_Intervention has been taken; otherwise means that the Deliver_Nothing action has been taken. \u2200dp, dp \u2208 DP RW , i.e. for each decision point of the opportunemoment-identification model, the algorithm makes a decision on delivering the intervention (i.e. the one selected by the interventionselection model) or not at each step until the behavior is performed or the person engages with the intervention. As seen in the figure, throughout a learning episode, the algorithm might deliver an intervention several times. However, the person might not see or engage with interventions immediately. One or more interventions might be processed or discarded after a certain amount of time by the person when s/ he interacts with the delivered interventions. In the example, there would be two notifications on the mobile phone, i.e. j 1 and j 2 , at state s 7 .\n\nDelayed response to the delivered intervention necessitates rewarding past actions. The RL literature has a solution for such cases: eligibility traces. However, the standard eligibility trace approach is not suitable for the problem we target. For example, assume that the person would only engage with j 2 and discards j 1 . r 7 would be a positive reward as the person has seen/engaged with j 2 . In this case, the standard eligibility tracing mechanism gives credit to all previous actions, including a 1 , a 3 and a 5 . However, rewarding a 1 and a 3 is an inconsistency. Because, those actions are suboptimal as interventions were sent in inappropriate moments without any engagement.\n\nWe address this inconsistency by customizing the eligibility criteria of retrospective rewarding. Instead of rewarding all past state-action pairs in the agent's trajectory, only the actions resulting with engaged interventions are positively rewarded. Complementary to this modification, we ensure that the action before the engagement with the intervention would be Deliver_Action even if it were originally Deliv-er_Nothing action. For example, in Fig.  4 , although a 7 is a Deliver_Nothing action originally, it was indeed an opportune moment for engaging with the intervention. So, we modify the eligibility trace as if a  Deliver_Intervention action were taken in s 7 . This modification favors taking the Delivery_Intervention action in future visits to s 7 . The algorithm described in Fig.  5  shows the two modifications we perform on the standard eligibility trace mechanism."
    },
    {
      "title": "Transfer learning across opportune-moment-identification environments",
      "text": "To be able to apply transfer learning in an RL setup, we first analyze similarities and differences between the environments among which knowledge to be shared. We apply transfer learning in the opportunemoment-identification model. In this respect, while the state space, action space and reward functions have the same configuration, only the transition functions differ, which is obvious as they reflect people's unique contexts.\n\nHaving these similarities and differences, we decided to have complete policies as the knowledge to be transferred. Having the policy of others, a learning agent could perform better in new states. It may benefit from the other agents' experiences that visited the same or similar state previously, instead of choosing a random action. Fewer random actions provide a warm-start for the learning agent which does a jump-start in terms of learning performance. However, this requires identification of other agents, whose policies to be transferred to the current agent. To deal with this problem, we decided to keep a common policy (CP) that accumulates successful experiences of all agents. In this way, we build a case repository accumulating the transitions happening in any available environment  [45] ."
    },
    {
      "title": "Definition 5 (Common Policy -CP):",
      "text": "where CP is a function that maps states to action-number tuples. Each tuple includes the action along with the number of agents taking that action in the given state with a successful outcome. We do not transfer CP as it is but use it as raw data to train a supervised-learning based State Classifier (SC).\n\nDefinition 6 (State Classifier -SC): SC: S OMI \u2192 A OMI , where SC is a function from the state space to the action space. SC provides classification capability for states that have not been discovered by the current agent. It might even be the case that a state might have not been discovered by any of the agents. Even so, the classifier might predict the action considering the similarity of the states that were previously discovered.\n\nCP is updated after each episode, based on the activities of the agent throughout the episode. The algorithm to update CP is explained in Fig.  6 . SC is trained with the data accumulated in CP. Each training data item is a tuple with a state and label as follows: data_item = < s, l>, where s \u2208 S OMI and l \u2208 A OMI . The label of each data item is determined by the total number of times of actions being selected by the agents. Simply, the action with the highest number of selection for the given state is set as the label.\n\nAs presented before, the state parameters of the opportune-momentidentification model are enumerated values, which makes decision trees an appropriate method to perform classification on the data items. Specifically, we decided to use the Random Forests  [46]  as they are more suitable to realize customized transfer learning approaches, such as transferring partial policies, by modifying the tree generation accordingly. In this study, though, we used the default tree generation and classification mechanism provided by the Apache Spark library  [47] ."
    },
    {
      "title": "Overall algorithm",
      "text": "In this section, we present the overall learning algorithm. As depicted in Fig.  7 , the algorithm describes the execution of a single learning episode, which corresponds to a simulated day in the trials. S. G\u00f6n\u00fcl et al. Fig. 7. Overall learning algorithm.\n\nFirst, we present inputs, which are complex data structures encompassing specific data elements used in the algorithm. The first four inputs are the main components of the RL environments, namely environment and agent objects of the intervention-selection and opportune-moment-identification models. The environment objects keep track of the current state of the environment and transition history. Next, an action plan includes daily planned activities for the person. Lastly, while the Common Policy (CP) contains the accumulated selection counts of actions along with the states, the State Classifier(SC) is the trained model that is used to predict actions to be taken in unknown states.\n\nThe actual procedure starts with the first phase, initiating the outermost loop of the algorithm. This loop runs for each decision point of the intervention-selection model throughout a learning episode, i.e. \u2200 dp,dp \u2208 DP AP . The first step is to determine the action (is_a t ) to be taken in the current state (is_s t ) of the intervention-selection model, by the learning agent of the intervention-selection model. Internally, we use a greedy policy such that the action leading to the highest long-term reward is selected. The policy selects the action from EA DP , i.e. the eligible set of interventions as defined in Section 5.2.\n\nIn the second phase, the opportune-moment-identification model runs only if a certain intervention type is selected (is a t \u2215 = No I ntervention). The first operation performed by the opportune-moment-identification model is to determine the action (omi_a t ) based on the current state (omi_s t ). Greedy logic is used by this model also. However, the learning algorithm utilizes SC in case it has to select a random action (omi a s .selectMode = RANDOM) in an unknown state. In other cases, it utilizes the greedy policy and selects the action with the highest q-value. Following action selection, the environment makes transition from the current state to the next state by taking the selected action.\n\nIn the third phase, two simulations take place. If an intervention is delivered (omi a t = Deliver I ntervention), the person's reaction to the delivered intervention is simulated. The result of the simulation is either to discard the intervention or engage with it. The second simulation is about performing the targeted behavior. Whether the behaviour will be performed or not is determined by the habit formation model. Details about the two simulation activities will be given in the next section.\n\nThe fourth phase starts with obtaining the reward (omi_r t ) for the action taken by the opportune-moment-identification model via Eq. (1) and Eq. (2). Then, eligibility traces are updated (Algorithm1), and the transition is recorded into the episode analysis object. For each selected intervention, the opportune-moment-identification model runs through all the time frames associated with the current planned activity of the action plan (context c hange occurs within action p lan a ctivity.time f rame).\n\nFinally, execution context switches back to the interventionselection model by advancing the habit formation model one step. The next step for the intervention-selection model (is s t+1 ) is obtained using the updated parameters of the habit formation model and the reward (is r t ) is generated (Eq. (  3 )). The loop is iterated for all time frames generated by the action plan. Once the episode is over, CP is updated with the collected data for the episode (Algorithm2)."
    },
    {
      "title": "RL models",
      "text": "In this section, we instantiate the intervention-selection and opportune-moment-identification models by providing formal definitions for each of them. We assume that people are fully observable thanks to the directly sensed or inferred contextual parameters required by the learning models. This makes using MDPs appropriate to formally describe the RL environments. An MDP M is characterized with a tuple consisting of four parameters: state set, action set, probability matrix and reward function, which is represented as M = < S, A, P, R>. We present MDP configurations for the intervention-selection and opportunemoment-identification models in the following sections."
    },
    {
      "title": "The opportune-moment-identification model",
      "text": "The MDP instantiated for the opportune-moment-identification model is represented with: M OMI = < S OMI , A OMI , P OMI , R OMI >.\n\nConsidering that the opportune-moment-identification model aims to capture person-specific patterns over momentary context, the person state includes parameters pertaining to physiological, psychological or environmental context of the person. Each state s OMI \u2208 S OMI is composed of six parameters as defined below:\n\ns OMI = < time, location, physical_activity_status, phone_screen_status, emotional_status, number_of_interventions_sent_for_planned_activity>\n\nThe time required to reach a (near) optimal policy with an RL solution is inversely proportional with the size of state and action sets. Therefore, we avoid state parameters with continuous values and modeled them as discrete parameters as described below:\u2022 time= {x: x >= 0 and x < 95, x \u2208\u2115}. This parameter is used to represent the time during a day. The action set of the opportune-moment-identification model includes two elements: Deliver_Intervention and Deliver_Nothing that are respectively used for delivering the selected intervention or not. The action set is formally described as follows:\n\nA OMI = {Deliver_Intervention, Deliver_Nothing} Reward function of the opportune-moment-identification model, R OMI , first considers the action taken at step t (omi a t ). The algorithm generates a relatively small negative reward (-1) as shown in Eq. (  1 ) for cases where no intervention is delivered (omi a t = Deliver N othing). Although choosing 0 reward seems reasonable for such cases, this setting leads to getting stuck in a local minima as not sending any intervention is favored throughout the whole learning process.\n\nIn cases where an intervention is delivered (omi a t = Deliver I ntervention), the algorithm initiates the reward by considering the person's reaction to the intervention. The reward is initially set to 1000 and -2 for reacting (reacted t = true) and not-reacting (reacted t = false) cases respectively. These initial values reflect the higher importance given to an action leading to engagement. Consecutive notreacting cases generate multiplied negative rewards in proportion with the number of cases. If the intervention was reacted, the reward is updated according to the temporal proximity of the intervention delivery to the actual engagement time with the intervention (difference t ). Three ranges are defined into which the temporal difference (td) of delivery and engagement can fall. These ranges can be represented as: 0 <= td <= 30, 30 < td <= 60 and td > 60. The lower the temporal difference, the higher the reward. While Eq. (  2 ) shows calcuation of the reward generated for the temporal proximity, Eq. (  1 ) shows calculation of the eventual reward generated at each step of the opportune-momentidentification model.\n\nThe opportune-moment-identification model is a stochastic environment considering the way state transitions occur. As presented earlier, S OMI is mainly composed of parameters pertaining to people's varying momentary contexts. Therefore, concerning transitions from state omi s t to omi s t+1 , the values of omi s t+1 are observed from people's daily activities. This means that this model does not have a well-defined transition probability matrix."
    },
    {
      "title": "The intervention-selection model",
      "text": "The MDP for the intervention-selection model is represented with:\n\nS IS is the state set, where each state s IS \u2208 S IS is represented with a tuple composed of four parameters as follows:\n\nAs in the opportune-moment-identification case, we avoided state parameters with continuous values for this model as described below:\u2022 habit_strength= {x: x >= 0 and x < 10, x \u2208\u2115}. Habit strength represents the strength of automaticity of the behavior without occupying the mind to remember to perform the behavior. In other words, the higher the habit strength is, the less need is required for providing cues in the form of interventions for reminding the behavior.\u2022 behavior_frequency= {x: x >= 0 and x < 10, x \u2208\u2115}. Behavior frequency is the ratio of number of times executing the behavior to the total number of opportunities to perform a behavior, within a certain time frame in the past.\u2022 day_type: Day type is an enumeration that can take either weekend or weekday values. Its value is extracted from the date of the active learning episode i.e. the simulated day.\u2022 remember_behavior:Remembering behavior is also an enumeration that can take either true or false values. The specific value of this parameter indicates whether the person would remember performing the behavior as planned in the action plan.\n\nThe action set of the intervention selection model is defined as follows:\n\nA IS = {J 1 , J 2 ,..., J n , No_Intervention} The set of intervention types include J 1 ,..., J n that are supposed to be identified by domain experts addressing a particular disease/health problem  [35] . In addition to the specific intervention types, not delivering any intervention, represented with No_Intervention label, is also a possible action. An internal mapping (M) keeps an association between each activity (a \u2208 AP) defined in an action plan and an eligible set of interventions (EA a ) such that:\n\nEA a is further narrowed considering the intervention types eligible for the decision points. For decision points prior to the planned activity, only the reminder interventions are considered. Otherwise, only the motivation interventions are eligible. We define the final set of eligibile actions at a decision point as EA DP . At each decision point of the intervention-selection model, an action is a t is selected from this set i.e. is a t \u2208 EA DP .\n\nR IS function is conditioned on 3 variables: remembering the behavior, delivering an intervention or not and engagement with the delivered intervention, if any. The reward is calculated based on these variables at each state change (is s t ), step t. Varying combinations of conditions on these variables form the reward function as presented in Eq. (  3 ).\n\nThe values for the six respective cases in the equation are instantiated as -50, -10, -5, -3, -1 and 10. The reasoning behind such an instantiation is to generate relatively higher rewards in cases when the person does not remember performing the behavior and an intervention type is selected as a counter action (remember b ehavior = false and is a \u2215 = No I ntervention). Engaging with the intervention is rewarded positively (reacted = true) so that the interventions that are not engaged by the person would be selected less. The worst decision of the algorithm would be not to select any intervention even if the person would not remember performing the behavior (remember b ehavior = false and is a = No I ntervention). So, this is the case where the largest reward in magnitude, but a negative one, is generated. (3)\n\n1), if omi a t = Deliver N othing, reacted t = false sent n ot r eacted(-2) * number o f a ttempts , if omi a t = Deliver I ntervention, reacted t = false sent r eacted(1000) +temporal r eward < ce : inf > t < /ce : inf > , if omi a t = Deliver I ntervention, reacted t = true (1)\n\nP IS is a matrix of probabilities, P(is s t+1 |is s, is a t ), indicating the probability of transition from state is s to state is s t+1 by taking the action is a t . The intervention-selection model follows an episodic learning approach such that each day corresponds to an episode. State transitions throughout an episode, i.e. a day, is laid out by the action plan associated to the person. Subsequent state parameters are calculated by the habit formation model embedded into the intervention-selection model. Therefore, state Transition dynamics from state is s t to is s t+1 are deterministic. We provide details about state transition and calculation of parameters of next state in Appendix A."
    },
    {
      "title": "Experiment and validation setup",
      "text": "This section presents the simulation-based validation approach. First, we provide details about the simulated concepts, which are the action plan and personas, by elaborating the differentiating factors of these concepts. We introduce a set of hypotheses, capturing the personas' differentiating factors, against which the proposed approach is validated. The validation approach continues with the metrics that are being used to validate the two accelerator methods. Afterwards, we describe a benchmarking methodology on the reward function parameters. Lastly, we provide details about the experiments of which results are discussed in the results section."
    },
    {
      "title": "Simulations",
      "text": "As briefly mentioned in the introduction, we simulate a number of concepts as distinguishing characteristics of people that should be captured by the proposed learning algorithm. The overall simulation approach has already been discussed in  [35] . Thus, we only summarize the simulation set up and provide details about the expansions on the initial approach. For more details, please see the Simulation Testbed section of the referred study. In summary, the simulation setup can be explained under two main topics: the action plan and personas.\n\nSimulating Action Plan. We assume a simple action plan with 3 predefined decision points (morning, noon and evening) at which 2 types of reminder and one type of motivational interventions can be sent.\n\nSimulating Personas. We consider four distinguishing factors for four simulated personas. The first one is habit formation, which is an indicator of strength of automaticity of performing the targeted behavior without external cues. To simulate habit-related concepts in a realistic way, we use the habit formation model  [18] . The main parameter of the model that we are interested in is commitment intensity, which is an indicator of the time required to turn a behaviour into habit. Considering the parameter can take values between 0 and 1, we assign 0.2, 0.4, 0.6 and 0.8 values to Person -1, Person -2, Person -3 and Person -4 respectively. Having a relatively higher commitment intensity shows more appreciation or having more desire for the targeted behavior by the person.\n\nThe second factor is daily activities of people. Daily activities vary for each individual. We introduce a concept called activity timeline to represent all daily activities of a person during the day from wake up to sleep. The aim of having such timelines is to simulate states that are (or not) suitable for engaging with interventions and for performing the activities described in action plans. A timeline is a placeholder for a sequence of activities to be performed throughout the day.\n\nWe populate timelines with predefined activities that can be semi-randomized for each person for each learning episode i.e. simulated day. This semi-randomization is realized in the following way: First, a subset of the activities are selected among the complete set of activities.\n\nThen, each activity is associated with a start time and duration. The sequence, start time and duration of activities differ for each learning episode and for each person. Randomization of activity selection, start time identification and duration calculation are achieved with a Gaussion distribution within predefined limits relevant for the randomized variable. For instance, duration of an activity might be randomized between 30 and 60 minutes. The third factor is simulation of reactions to the delivered interventions. Reaction to an intervention has two prerequisites: the person should be favoring that type of intervention and the daily activity should be suitable for engaging the intervention. We represent personal preferences on intervention types simply with a percentage that is used as a probability of reacting to an intervention. Persona-specific preferences on intervention types are given in Table  1 . Note that the preferences on the intervention types are not disjoint. So, the sum of the preference probabilities do not necessarily sum up to 1.\n\nThe last factor is simulation of the actual behavior performance. It is determined by the prediction on remembering the behavior by the habit formation model and suitability of the daily activity for performing the behavior. If the prediction of the model is positive, we assume that the person would perform the behavior during the activity if the activity is set as suitable for performing the activity in the scope of corresponding activity timeline."
    },
    {
      "title": "Validation methodology 6.2.1. Hypotheses for the simulated personas",
      "text": "The proposed algorithm is supposed to adapt the intervention delivery respecting to factors that differ for each person. We introduce a set of hypotheses driven by these factors against which the algorithm is validated. Specifically, we introduce the following four human factors, each of which has a corresponding hypothesis:\n\n\u2022 Commitment intensity to adopt a targeted lifestyle behavior \u2022 Habit strength facilitating the performance of the targeted behavior \u2022 Preference on intervention types \u2022 Daily activities Respective hypotheses corresponding to these factors are as follows:\u2022 Hypothesis 1: The habit formation model implies that the higher commitment intensity of a person is, the less time s/he requires to reach the maximum habit strength. Therefore, duration of the simulated care process through which interventions are sent should increase as the commitment intensity decreases.\u2022 Hypothesis 2: A high habit strength indicates that the person would perform the behavior without a need for external cues. Therefore, the number of delivered interventions should be inversely proportional with the perceived habit strength and become more and more intermittent throughout the simulated care process.\u2022 Hypothesis 3: The ratio of selected interventions should be consistent with the preferences of the persons.\u2022 Hypothesis 4: Daily activities differ for each simulated person as they have different activity timelines. Even, the activities change for the same person at each simulated day since the activities are selected in a semi-random manner. Therefore, the proposed approach should deliver interventions respecting to people's daily life patterns by respecting the timing and suitability of activities for engaging with the intervention and performing the behavior."
    },
    {
      "title": "Metrics for validating the accelerator techniques",
      "text": "In order to validate the effectiveness of the accelerator techniques, we use performance metrics introduced by the transfer learning methodology, namely: jump-start, asymptotic performance, total reward, transfer ratio and time-to-threshold  [38] .\u2022 Jump-startrepresents the improved performance of a learning agent at the beginning of the learning process"
    },
    {
      "title": "Table 1",
      "text": "Preferences associated to simulated personas on intervention types."
    },
    {
      "title": "Reminder-1",
      "text": "Reminder-\n\n2 Motivation Person 1 0% 50% 10% Person 2 20% 80% 80% Person 3 80% 10% 80% Person 4 50% 50% 50% S. G\u00f6n\u00fcl et al.\n\ncompared to the base algorithm without using transferred knowledge.\u2022 Asymptotic performanceis the maximum performance that is achieved by the base algorithm and the algorithm utilizing transferred knowledge.\u2022 Total rewardis the amount of reward collected throughout the learning process.\u2022 Transfer ratiois the total amount of rewards accumulated thanks to the transferred knowledge divided by the total amount of rewards gathered with the base algorithm without any transferred knowledge.\u2022 Time to thresholdindicates the number of steps to reach a pre-defined performance level."
    },
    {
      "title": "Baseline delivery strategy",
      "text": "Fixed intervention delivery policy (FP): A fixed intervention delivery strategy is defined as a baseline method in addition to the dynamic and adaptive intervention delivery policies as aimed by the proposed approach. This fixed policy is used as a baseline algorithm against which the results of the opportune-moment-identification model will be compared. Intervention delivery schedule of FP is as follows: Interventions are sent at 3 fixed time frames as long as the person has not engaged with the intervention and has not performed the behavior yet."
    },
    {
      "title": "Fine-tuning the reward functions parameters",
      "text": "The reward functions of the RL models are critical as they guide the learning agent in the correct direction by generating a proper reward representing the appropriateness of the taken action. Initial values for the reward function parameters are set heuristically considering the relative importance of the factor being rewarded towards achieving the ultimate goal. We validate the parameters by benchmarking alternative values and observing them leading to sub-optimal results."
    },
    {
      "title": "Experiments",
      "text": "We conduct two experiments such that each experiment contains 100 trials and each trial contains 100 learning episodes. The first experiment does not utilize the State Classifier as it has not been trained yet. The data generated by the first experiment are used to train the State Classifier. The second experiment utilizes the State Classifier for selecting actions. The results presented in the next section are obtained in the second experiment."
    },
    {
      "title": "Results",
      "text": "In the light of the experimental setup presented in the previous section, we first show how the obtained results validate the hypotheses.\n\nThen, we analyze the results with respect to the transfer learning-based metrics. Finally, we present the results of benchmarking on the reward function parameters."
    },
    {
      "title": "Validation of the hypotheses Overall duration of intervention delivery (Hypothesis 1):",
      "text": "We have set commitment intensities in an increasing manner from Person-1 to Person-4. This implies that the time required to form habit for the targeted behavior should be the shortest for Person-4 and the longest for Person-1. The left-y versus x-axis of Fig.  8   Right-y axis versus x-axis of Fig.  8  represents the habit strength per episode as simulated by the habit formation model. Time elapsed to reach the maximum habit strength for each person is also consistent with the commitment intensities set for the personas. According to the model, the person with highest commitment intensity should reach to the maximum habit strength first. In line with this relationship, Person-1 ranked last and Person-4 ranked first in reaching the maximum habit strength. Considering the length of the duration to reach the maximum habit strength, the values obtained in our study are consistent also with the results obtained by Lally et al.  [48] . They have developed a habit formation model on empirical data regarding performance of the targeted behavior and habit strength perceived by subjects. Their model outputs duration for forming habit on daily behaviors where the behaviors are classified with a set of difficulty classes. According to Lally et al., the duration for forming habit varies from almost a month to a few months where difficulty of behavior is the main determinant. In our study, we consider only one behavior type. However, studies show that the perceived difficulty for the same behavior might differ for different people  [49, 50] . Considering all these factors, we argue that the simulation is realistic in terms of generation of habit strength values i.e. performing the targeted behavior without a need to extrinsic reminders.\n\nTotal number of delivered interventions (Hypothesis 2): Looking again at the dappled lines of Fig.  8  showing the number of delivered interventions, the averages are relatively high in the starting phase of the Fig.  8 . Episode vs. intervention count vs. habit strength plot."
    },
    {
      "title": "S. G\u00f6n\u00fcl et al.",
      "text": "simulations for each persona where the habit strength is relatively low especially for Person-1 and Person-2. According to the habit formation model, lower commitment intensity implies lower habit strength. Considering this fact, during the beginning phase, it can be observed that the average numbers of delivered interventions for Person-1, the dapped blue line, are higher than the others most of the time. That is, Person-1 who has the lowest commitment intensity receives the highest number of interventions. As shown in Fig.  9 (a), the total number of interventions delivered for Person-1 is higher than the rest. This is expected as Person-1 is supposed to be reminded more to perform the behavior throughout the whole process. After a while, though, the number of delivered interventions gets closer to zero. The number of delivered interventions reacts to the changing habit strength such that as the habit strength increases the number of delivered interventions decreases and become more and more intermittent throughout the trial."
    },
    {
      "title": "Number of delivered interventions per type (Hypothesis 3):",
      "text": "The three simulated intervention types are clustered into two disjoint sets according to their categories. That is, only the interventions of the same set can be alternatives to each other. Therefore, the results should be evaluated independently for each set. Being reminder interventions, Reminder-1 and Reminder-2 belongs to the reminder set, whereas Motivation belongs the motivation set. As stated earlier, each specific preference value is used as a probability of engagement when the user is presented with the corresponding intervention type. For example, Person-1 never reacts to Reminder-1; Person-2 reacts to Reminder-2 or Motivation with an 80% chance when s/he is presented with a Reminder-2 or Motivation. Because of the disjoint preference probabilities, we cannot expect a linear correlation across individuals for a particular intervention type and delivery ratios. Even, ratios for the same person may not be correlated even if the corresponding preference probabilities are the same. For example, for Person-4, Reminder-2 and Motivation have the same percentages but it is not expected to get the same ratio for these two interventions since Reminder-1 can also be sent as a reminder intervention.\n\nWe compare intervention ratios among each distinct set per individual basis in Fig.  9(b ). Ratios of Reminder-1 and Reminder-2 reflect the initial prefences per person basis. For example, the ratio of Reminder-1 is higher than the ratio of Reminder-2 for Person-3, considering that preferences of Person-3 is 80% and 10% for Reminder-1 and Reminder-2 respectively. The ratios of Reminder-1 and Reminder-2 are almost equal each other.\n\nAt the first glance, though, it can still be said that there are inconsistencies between preferences and the obtained results. For example, Person-1 has no interest at all for Reminder-1 and little interest for Motivation. But, the results show that the ratios for Reminder-1 and Motivation are 15% and 40% respectively. In case of Reminder-1, the main reason of this inconsistency is random selection of interventions in unexplored states i.e. the states that have not encountered previously by the learning agent. That is, the learning agent makes random decisions on selecting Reminder-1 or Reminder-2. This problem applies also to Motivation. In addition to that, the algorithm favors intervention delivery regardless of its type, in case the person is predicted not to perform the behavior. In such cases, any type of intervention is considered as a reminder.\n\nTemporal proximity of intervention delivery and behaviour performance (Hypothesis 4): According to the design of the simulation, intervention delivery and behavior performance do not necessarily happen at the same time. Most of the time, there is a temporal difference between the two. The desired behavior is to send a reminder as close as possible to a time point when the conditions are suitable to perform the behavior so that the effect of the reminder is maximized. Fig.  10  shows the ratio of interventions that fall into the pre-defined ranges of temporal differences. Each range is represented with a bar in the chart. In general, the ratio of delivered interventions decreases as the time difference Fig. 10. Difference between JITAI delivery and behavior performance times.\n\nincreases. The pattern is only violated at the second bar, i.e. the 31-60 minute bar. The reason is that daily activities generated at that time slot are not suitable for engaging with the intervention. This is an indicator of the algorithm's capability on learning the personalized patterns as it does not deliver interventions in states that are not suitable for engagement."
    },
    {
      "title": "Validation of the accelerator techniques",
      "text": "The metrics that are used for validation of the accelerator techniques were introduced in Section 6.2.2. Fig.  11  addresses all these metrics simultaneously. The figure shows the rewards aggregated per episode. We present rewards obtained with 3 versions of the learning algorithm. The green, blue and red lines show the results obtained with the base Q-Learning (QL) algorithm, the base algorithm expanded with the selective eligibility traces (QL-SET) and the base algorithm expanded with both the selective eligibility traces and transfer learning techniques (QL-SET-TL).\n\nThe blue line laying over the green one, indicates a better jump-start achieved by the selective eligibility traces compared to the base algorithm. Nevertheless, the red line outperforms the other two with a notable highest jump start at the beginning of the simulation. The areas under the lines correspond to the total rewards collected by the respective algorithm. According to the figure, QL-SET-TL collects more reward than the others throughout the experiment. This also implies that QL-SET-TL is more effective than the others considering the ratio of number of engaged interventions to the total of number interventions sent. With respect to the asymptotic performance, QL-SET-TL has a higher course than the other two during the active intervention delivery periods. Concerning the time-to-threshold metric, we do not introduce a predefined threshold. However, it can be said that all versions stabilize almost at the same time. Lastly, the area between the red line (QL-SET-TL) and blue line (QL-SET) correspond to the additional rewards that were collected thanks to the transfer learning. In this respect, considering the active intervention delivery period, the area between the blue and red lines indicate a notable contribution of transfer learning on the collected rewards. QL-SET-TL performs better than FP for each metric.\n\nThe results presented in Fig.  11  shows that QL-SET-TL performs the best at the beginning of the simulation. In this respect, Fig.  12  shows the ratio of actions determined by the State Classifier (SC) versus the agent's internal policy. SC selects most of the actions at the beginning of trials. The number of actions selected by the agent's internal policy increases gradually as the agent itself learns more and more about its environment."
    },
    {
      "title": "Validation of the reward function parameters",
      "text": "While specifying reward values for each condition, zero (0) has been accepted as the neutral point and the magnitude of the rewards have been determined according to their perceived benefit. In the two subsequent sections, we present details about this approach along with experimental results obtained for benchmarked reward values. Fig. 12. Ratio of actions per action selection mechanism."
    },
    {
      "title": "Validation of the opportune-moment-identification reward function",
      "text": "The reward function of the opportune-moment-identification model includes two equations (Eq. (  1 ) and Eq.(  2 )). Eq. (  1 ) is conditioned on two variables namely the action taken (omi a t ) and reaction to the delivered intervention (reacted t ). Its main aim is to produce a reward indicating whether the state, where an intervention is delivered, is an appropriate state for engaging with the intervention or not. Thus, the most desired case is the case where an intervention is delivered and the person engages with the intervention i.e. (omi a t = Deliver I ntervention and reaction t = true). Therefore, in case an intervention is delivered but the reaction is negative i.e. (omi a t = Deliver I ntervention and reaction t = true), a relatively smaller reward should be generated. Difference in magnitudes of these two rewards shows that discovering a state that is appropriate for engaging with an intervention is more critical. However, there might be cases that are appropriate for engagement but no engagement occurs because of the randomization factors. Such cases might be observed successively. The negative reward is multiplied in proportion to the length of such cases. Thus, to neutralize or even revert the incorrect knowledge  learnt in such situations quickly, we chose a relatively higher value (1000) for the sent r eacted case. Nevertheless, different values result in a similar performance as long as the value for this parameter would be the largest one among the other cases of the equation. Fig.  13 , in (a), (b), (c) and (d) parts, show the rewards obtained for Person-1 when sent_reacted is set to 50, 500, 2500 and 10000 respectively while the other reward values are fixed. The results obtained for 500 and 2500 provide similar results with the original value i.e. 1000. The pattern changes towards to the edge cases. In case an even larger reward is specified, the learning performances of QL-SET and QL-SET-TL overlap since the effect of negative cases become negligible. On the other side, setting a smaller value for sent r eacted weakens its effect. As a result, performance plots of these two algorithms diverge considering also that the number of desired actions taken by QL-SET is fewer compared to QL-SET-TL.\n\nThe intiutive choice for the not s ent reward specified for the cases with no intervention delivery would be 0 since there is no feedback available from the person. However, in this case, the learning agent gets stuck at a local minima by always selecting the Deliver N othing action. In order to prevent this, a relatively small negative reward is generated to force the agent take the Deliver I ntervention after some time.\n\nThe second equation, i.e. Eq.(  2 ), generates a value to fine-tune the reward generated for the sent_reacted case. In this equation, the temporal difference between intervention delivery and behavior performance is considered. The aim is to have this difference as small as possible to increase the effectiveness of the intervention. Therefore, we set rewards in a decreasing manner as the temporal difference increases. Preserving this pattern while specifying reward values did not affect the performance of QL-SET-TL, QL-SET and QL. Fig.  14 , in (a), (b), (c) and (d) parts, show the results obtained when the reward values are multiplied by 10, 25, 50 and 125. As can be seen from the figure, the results obtained for the first three scales reflect the initial patterns, even though the lines get closer as the scaling increases. However, increasing the scale even further causes all the results to overlap. This is because the rewards generated in Eq.(  2 )) become negligible."
    },
    {
      "title": "Validation of the intervention-selection reward function",
      "text": "Similarly, the reward function of the intervention-selection model is also a conditional equation (Eq. (  3 )) depending on 3 variables namely, delivered action (is a t ), remembering the behavior (remember behavior t ) and reaction to the delivered intervention (reacted t ).\n\nConsidering the habit formation model, situational cues should be provided to people if they forget performing the behavior. Thus, the cases where the person does not remember performing the behavior i.e. (remember behavior t = false) is critical. Among the cases satisfying this primary condition, the case where no intervention is delivered (is a t = No I ntervention) should be avoided because it leads to non-performance of the behavior at all. Therefore, a relatively large, negative reward is set for this case (-50). When the person does not remember performing the behavior but an intervention is sent i.e. (remember behavior t = false and is a t \u2215 = No I ntervention), engagement may or may not occur. The case where an engagement occurs is a desired case. So, a positive reward  (10)  is set for this case. Since the non-engagement cases are considered as burden on the person, they are negatively rewarded. Despite non-engagement, sending an intervention can still be seen as a desired behavior. Therefore, we set the reward to -5 for this case. Cases where behavior is not remembered and no intervention is sent are the most undesirable cases (remember behavior t = false and is a t = No I nterventio). Therefore, the reward for this case is -10.\n\nThe person might remember the behavior and also react to the delivered intervention as well(remember behavior t = true and reacted t = true). However, this is still accepted as a burden on the person since s/he does not need to be reminded. Therefore, the reward is specified as 3 for this case.\n\nLastly, -1 reward is specified for the case where the person remembers the behavior and no intervention is delivered (remember behavior t = true and is a t = No I ntervention). Although choosing 0 reward for such cases seems reasonable, we chose -1 to prevent getting stuck at a local minima as in the case of opportunemoment-identification model.\n\nScaling the reward values all together does not change the algorithm results. Also, similar performance plots are obtained when the reward settings are changed by preserving the magnitudes and signs of rewards across Eq. (3)."
    },
    {
      "title": "Discussion",
      "text": "Even though the results capture the rules and conditions associated with the simulated parameters, there is still room for potential improvements e.g. selection of intervention types. Specifically, the intervention-selection model could be improved further. Although we currently follow a model-free approach, the intervention-selection model can be designed as a model-based system. This enables training of the value-function of the RL environment via intermediate simulations before taking an action. Such simulations could still run on the habit formation model. In such a structure, the intervention-selection model could even be split into two separate models that would adjust the type and frequency factors separately.\n\nWe believe that the proposed learning mechanism is an innovative approach, modeling the momentary and long-term contexts of people utilizing the RL methodology, where the eventual aim is to personalize different JITAI components; namely type, frequency and timing. However, we provided only an initial basis concerning the model parameters that are utilized to capture the personalized patterns in relative contexts and we do not claim that those are the necessary/sufficient set of parameters. On the contrary, the models could be enriched with additional context parameters pertaining to environment, mobile phone or person him/herself. Such an improvement would in turn require further optimization of the algorithm e.g. by better state generalization. In such a case deep reinforcement learning methods might be utilized as function approximators mapping the state space to the action space  [51] .\n\nState space and action space sizes determine the complexity of the approach such that the complexity increases as the state or action space gets larger. Although the algorithm runs through sequential episodes where only a small portion of the overall state space is visited, the whole state space is usually much larger. For instance, while the state space of the opportune-moment-identification model contains all combinations of time of day, physical status, phone status and so on, a person encounters only some specific states during a single episode i.e. a day. Furthermore, complexity of the actual experiment phase of the approach could be seen relatively lower considering a human subject can be involved in the experiment for a limited time. So, the complexity of the approach is not driven by the real experiment phase but the training phase. Number of episodes to train the classifier increases as the state spaces get larger. The selective eligibility traces and transfer learning techniques that we integrated into the learning process reduce the learning time both in live and simulation compared to the existing algorithms by enabling the reuse of the learnt knowledge.\n\nIn relation to the healthcare domain, we argue that the proposed approach would lead to improved care programs. The evidence is overwhelming that healthy lifestyle behaviors like perfoming physical activity or dieting can reduce the risk of developing numerous chronic diseases and in many cases even reverse the existing disease  [3] . Therefore, we can argue that the proposed approach would improve the effectiveness of behavior change programs and contribute people's health via a smart intervention delivery strategy tailored to individuals.\n\nWe think the scope of the proposed simulation approach can be expanded. In this work, we consider preferences on intervention types, commitment intensities and daily activities as differentiating factors in the persona simulation at the moment. However, a more realistic simulation could be achieved by also considering factors like selfefficacy, motivation, prior experience changing the behavior or outcome expectancies."
    },
    {
      "title": "Conclusion",
      "text": "In this study, a reinforcement learning (RL)-based algorithm is presented for personalization of intervention delivery in terms of timing, frequency and type of interventions. The algorithm optimizes these parameters by employing two dedicated RL models. The standard eligibility tracing mechanism is customized for selective rewarding of past actions as well as manipulation of the trajectory of actions considering suitability of the states for engaging with interventions. Furthermore, transfer learning technique is integrated with the algorithm in order to reuse knowledge across learning environments.\n\nThe proposed approach is validated by using a simulation-based approach since human experiments are costly, risky and timeconsuming and unreliable. During the validation, simulated personas are provided with tailored interventions aiming to promote forming habit on a targeted health-related behavior. Four personas are simulated with differences in their daily activities, preferences on specific intervention types and attitude towards the targeted health behavior. An empirically validated habit formation model is used for realistic simulation of people's intrinsic habit-related parameters and their engagement with interventions.\n\nThe proposed algorithm is able to capture the rules and conditions associated with the simulated concepts. Also better results are obtained in comparison to the standard RL algorithms.\n\nAs a future work, the proposed approach will be validated on a realworld case study in the scope of an ongoing multi-national project, POWER2DM  1  aiming to develop a self-management support system to support diabetes patients in their daily lives."
    },
    {
      "text": "Fig. 1. Partitioning of the daily time frame according to an action plan."
    },
    {
      "text": "Fig. 3. Schematic diagram of the overall approach."
    },
    {
      "text": "Fig. 4. Agent-environment interactions within the opportune-moment-identification model."
    },
    {
      "text": "Fig. 5. Modifications on the standard eligibility traces."
    },
    {
      "text": "Fig. 6. The algorithm for updating CP after each episode."
    },
    {
      "text": "The time value is generalized to represent only the quarter hour periods. Its value is calculated with the following equation: time = h*4 + \u230am / 15\u230b, where hour h is defined as h = {h: h >= 0 and h < 24, h \u2208\u2115} and minute m is defined as m = {m: m >= 0 and h < 60, m \u2208\u2115}.\u2022 location: Location represents whereabouts of a person with three enumerated values namely Home, Office and Other.\u2022 physical_activity_status: This parameter is used to enumerate physical activity of a person with Sedentary, Indoor_Activity, Walking, Running and Driving alternatives.\u2022 phone_screen_status: Enumerated value of phone screen status that can be On or Off.\u2022 emotional_status: Enumerated value representing the emotions that a person may have including Neutral, Happy, Excited, Relaxed, Angry and Stressed alternatives.\u2022 number_of_interventions_sent_for_planned_activity: Number of delivered interventions for the active activity defined in the action plan."
    },
    {
      "text": "not s ent n ot r emembered(-50), if is a = No I ntervention, remember b ehavior = true sent r emembered n ot r eacted(-10), if is a \u2215 = No I ntervention, remember b ehavior = true, reacted = false sent n ot r emembered n ot r eacted(-5), if is a \u2215 = No I ntervention, remember b ehavior = false, reacted = false sent r emembered r eacted(-3), if is a \u2215 = No I ntervention, remember b ehavior = true, reacted = true not s ent r emembered(-1), if is a = No I ntervention, remember b ehavior = true sent n ot r emembered r eacted(10), if is a \u2215 = No I ntervention, remember b ehavior = false, reacted = true"
    },
    {
      "text": "represents the number of interventions delivered in each episode. The dappled lines with square shapes show the values obtained for this metric. As implied by the habit formation model, the number of interventions gets closer to zero last for Person-1, i.e. the algorithm delivers interventions for a longer time for Person-1 compared to the other persons. In general, the length of intervention delivery period decreases for each person inversely proportional to the commitment intensity values set for the associated personas."
    },
    {
      "text": "Fig. 9. Person vs. intervention type ratio plot."
    },
    {
      "text": "Fig. 11. Rewards collected per episode in the opportune-moment-identification model."
    },
    {
      "text": "Fig. 13. Rewards obtained with the modified the sent-reacted variable in Eq. (1)."
    },
    {
      "text": "Fig. 14. Rewards obtained with varying scales of temporal rewards."
    }
  ],
  "references": [
    {
      "title": "Building new computational models to support health behavior change and maintenance: new opportunities in behavioral research",
      "authors": [
        "D Spruijt-Metz",
        "E Hekler",
        "N Saranummi",
        "S Intille",
        "I Korhonen",
        "W Nilsen"
      ],
      "year": 2015,
      "doi": "10.1007/s13142-015-0324-1"
    },
    {
      "title": "Noncommunicable diseases",
      "year": 2018,
      "doi": "10.2471/b09509"
    },
    {
      "title": "Effects of low-energy diet or exercise on cardiovascular function in working-age adults with type 2 diabetes: a prospective, randomized, open-label, blinded end point trial",
      "authors": [
        "G Gulsin",
        "D Swarbrick",
        "L Athithan",
        "E Brady",
        "J Henson",
        "E Baldry"
      ],
      "year": 2020,
      "doi": "10.2337/dc20-0129"
    },
    {
      "title": "Success rates in smoking cessation: psychological preparation plays a critical role and interacts with other factors such as psychoactive substances",
      "authors": [
        "B Joly",
        "J Perriot",
        "P Athis",
        "E Chazard",
        "G Brousse",
        "C Quantin"
      ],
      "year": 2017
    },
    {
      "title": "Effectiveness of mobile phone messaging in prevention of type 2 diabetes by lifestyle modification in men in India: a prospective, parallel-group, randomised controlled trial",
      "authors": [
        "A Ramachandran",
        "C Snehalatha",
        "J Ram",
        "S Selvam",
        "M Simon",
        "A Nanditha"
      ],
      "year": 2013,
      "doi": "10.1016/s2213-8587(13)70067-6"
    },
    {
      "title": "A randomized clinical trial of a tailored lifestyle intervention for obese, sedentary, primary care patients",
      "authors": [
        "C Eaton",
        "S Hartman",
        "E Perzanowski",
        "G Pan",
        "M Roberts",
        "P Risica"
      ],
      "year": 2016
    },
    {
      "title": "Near-optimal insulin treatment for diabetes patients: a machine learning approach",
      "authors": [
        "M Shifrin",
        "H Siegelmann"
      ],
      "year": 2020,
      "doi": "10.1016/j.artmed.2020.101917"
    },
    {
      "title": "Reinforcement learning application in diabetes blood glucose control: a systematic review",
      "authors": [
        "M Tejedor",
        "A Woldaregay",
        "F Godtliebsen"
      ],
      "year": 2020
    },
    {
      "title": "A comprehensive overview of smart wearables: the state of the art literature, recent advances, and future challenges",
      "authors": [
        "N Niknejad",
        "W Ismail",
        "A Mardani",
        "H Liao",
        "I Ghani"
      ],
      "year": 2020,
      "doi": "10.1016/j.engappai.2020.103529"
    },
    {
      "title": "Just-in-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support",
      "authors": [
        "Nahum-Shani I Smith",
        "S Spring",
        "B Collins",
        "L Witkiewitz",
        "K Tewari"
      ],
      "year": 2017
    },
    {
      "title": "Reinforcement learning: an introduction",
      "authors": [
        "R Sutton",
        "A Barto"
      ],
      "year": 1998
    },
    {
      "title": "Anticipatory mobile computing for behaviour change interventions",
      "authors": [
        "Pejovic Veljko",
        "Musolesi Mirco"
      ],
      "year": 2014
    },
    {
      "title": "Scalable inverse reinforcement learning through multifidelity bayesian optimization",
      "authors": [
        "M Imani",
        "S Ghoreishi"
      ],
      "year": 2021
    },
    {
      "title": "Transfer in reinforcement learning: a framework and a survey",
      "authors": [
        "A Lazaric"
      ],
      "year": 2012
    },
    {
      "title": "Transfer learning via inter-task mappings for temporal difference learning",
      "authors": [
        "M Taylor",
        "P Stone",
        "Y Liu"
      ],
      "year": 2007,
      "doi": "10.1145/1329125.1329170"
    },
    {
      "title": "Time hopping technique for faster reinforcement learning in simulations",
      "authors": [
        "P Kormushev",
        "K Nomoto",
        "F Dong",
        "K Hirota"
      ],
      "year": 2011,
      "doi": "10.20965/jaciii.2009.p0600"
    },
    {
      "title": "Optimization of justin-time adaptive interventions using reinforcement learning",
      "authors": [
        "S Gonul",
        "T Namli",
        "M Baskaya",
        "A Sinaci",
        "A Cosar",
        "I Toroslu"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-319-92058-0_32"
    },
    {
      "title": "Changing behavior by memory aids: a social psychological model of prospective memory and habit development tested with dynamic field data",
      "authors": [
        "R Tobias"
      ],
      "year": 2009
    },
    {
      "title": "Feasibility, acceptability, and preliminary efficacy of a smartphone intervention for schizophrenia",
      "authors": [
        "D Ben-Zeev",
        "C Brenner",
        "M Begale",
        "J Duffecy",
        "D Mohr",
        "K Mueser"
      ],
      "year": 2014,
      "doi": "10.1093/schbul/sbu033"
    },
    {
      "title": "Automatic messaging for improving patients engagement in diabetes management: an exploratory study",
      "authors": [
        "A Fioravanti",
        "G Fico",
        "D Salvi",
        "R Garc\u00eda-Betances",
        "M Arredondo"
      ],
      "year": 2015
    },
    {
      "title": "Effectiveness of a smartphone application for weight loss compared with usual care in overweight primary care patients: a randomized controlled trial",
      "authors": [
        "B Laing",
        "C Mangione",
        "C Tseng",
        "M Leng",
        "E Vaisberg",
        "M Mahida"
      ],
      "year": 2014,
      "doi": "10.7326/m13-3005"
    },
    {
      "title": "A smartphone application to support recovery from alcoholism: a randomized clinical trial",
      "authors": [
        "D Gustafson",
        "F Mctavish",
        "M Chih",
        "A Atwood",
        "R Johnson",
        "M Boyle"
      ],
      "year": 2014
    },
    {
      "title": "DialBetics: a novel smartphone-based self-management support system for type 2 diabetes patients",
      "authors": [
        "K Waki",
        "H Fujita",
        "Y Uchimura",
        "K Omae",
        "E Aramaki",
        "S Kato"
      ],
      "year": 2014
    },
    {
      "title": "Personalised medicine in critical care using Bayesian reinforcement learning",
      "authors": [
        "C Utomo",
        "H Kurniawati",
        "X Li",
        "S Pokharel"
      ],
      "year": 2019,
      "doi": "10.1007/978-3-030-35231-8_47"
    },
    {
      "title": "A model-free affective reinforcement learning approach to personalization of an autonomous social robot companion for early literacy education",
      "authors": [
        "H Park",
        "I Grover",
        "S Spaulding",
        "L Gomez",
        "C Breazeal"
      ],
      "year": 2019,
      "doi": "10.1609/aaai.v33i01.3301687"
    },
    {
      "title": "Predictive modeling of addiction lapses in a mobile health application",
      "authors": [
        "M Chih",
        "T Patton",
        "F Mctavish",
        "A Isham",
        "C Judkins-Fisher",
        "A Atwood"
      ],
      "year": 2014
    },
    {
      "title": "Return of the JITAI: applying a just-in-time adaptive intervention framework to the development of m-health solutions for addictive behaviors",
      "authors": [
        "S Goldstein",
        "B Evans",
        "D Flack",
        "A Juarascio",
        "S Manasse",
        "F Zhang"
      ],
      "year": 2017,
      "doi": "10.1007/s12529-016-9627-y"
    },
    {
      "title": "The importance of behavior theory in control system modeling of physical activity sensor data",
      "authors": [
        "W Riley",
        "Cesar Rivera"
      ],
      "year": 2014,
      "doi": "10.1109/embc.2014.6945209"
    },
    {
      "title": "A dynamical model for describing behavioural interventions for weight loss and body composition change",
      "authors": [
        "J Navarro-Barrientos",
        "D Rivera",
        "L Collins"
      ],
      "year": 2011
    },
    {
      "title": "From intentions to actions: a theory of planned behavior",
      "authors": [
        "I Ajzen"
      ],
      "year": 1985
    },
    {
      "title": "Social foundations of thought and action: a social cognitive theory",
      "authors": [
        "A Bandura"
      ],
      "year": 1986
    },
    {
      "title": "A dynamical systems model of social cognitive theory",
      "authors": [
        "C Martin",
        "D Rivera",
        "W Riley",
        "E Hekler",
        "M Buman",
        "M Adams"
      ],
      "year": 2014
    },
    {
      "title": "Dynamic models of behavior for just-in-time adaptive interventions",
      "authors": [
        "D Spruijt-Metz",
        "W Nilsen"
      ],
      "year": 2014
    },
    {
      "title": "An actor-critic contextual bandit algorithm for personalized interventions using mobile devices",
      "authors": [
        "H Lei",
        "A Tewari",
        "S Murphy"
      ],
      "year": 2014
    },
    {
      "title": "An expandable approach for design and personalization of digital, just-in-time adaptive interventions",
      "authors": [
        "S Gonul",
        "T Namli",
        "S Huisman",
        "Laleci Erturkmen",
        "G Toroslu",
        "I Cosar"
      ],
      "year": 2019
    },
    {
      "title": "Temporal second difference traces",
      "authors": [
        "M Bloch"
      ],
      "year": 2011
    },
    {
      "title": "Postponed updates for temporal-difference reinforcement learning. Intelligent systems design and applications",
      "authors": [
        "H Van Seijen",
        "S Whiteson"
      ],
      "year": 2009,
      "doi": "10.1109/isda.2009.76"
    },
    {
      "title": "Transfer learning for reinforcement learning domains: a survey",
      "authors": [
        "M Taylor",
        "P Stone"
      ],
      "year": 2009,
      "doi": "10.1007/978-3-642-01882-4_7"
    },
    {
      "title": "Knowledge transfer in reinforcement learning",
      "authors": [
        "A Lazaric"
      ],
      "year": 2008,
      "doi": "10.58286/29685"
    },
    {
      "title": "Multi-criteria expertness based cooperative method for SARSA and eligibility trace algorithms",
      "authors": [
        "E Pakizeh",
        "M Pedram",
        "M Palhang"
      ],
      "year": 2015
    },
    {
      "title": "Group-driven reinforcement learning for personalized mhealth intervention",
      "authors": [
        "F Zhu",
        "J Guo",
        "Z Xu",
        "P Liao",
        "L Yang",
        "J Huang"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-030-00928-1_67"
    },
    {
      "title": "Narrowing reinforcement learning: overcoming the cold start problem for personalized health interventions",
      "authors": [
        "S Tabatabaei",
        "M Hoogendoorn",
        "A Van Halteren"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-030-03098-8_19"
    },
    {
      "title": "Recommender systems for healthy behavior change",
      "authors": [
        "O Y\u00fcr\u00fcten"
      ],
      "year": 2017,
      "doi": "10.3940/rina.innovsail.2010.14"
    },
    {
      "title": "Care plans and care planning in long term conditions: a conceptual model",
      "authors": [
        "J Burt",
        "J Rick",
        "T Blakeman",
        "J Protheroe",
        "M Roland",
        "P Bower"
      ],
      "year": 2014,
      "doi": "10.1017/S1463423613000327"
    },
    {
      "title": "Supplemental observation acquisition for learning by observation agents",
      "authors": [
        "M Floyd",
        "B Esfandiari"
      ],
      "year": 2018,
      "doi": "10.1007/s10489-018-1191-5"
    },
    {
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": 2001,
      "doi": "10.1023/a:1010933404324"
    },
    {
      "title": "Mllib: machine learning in apache spark",
      "authors": [
        "X Meng",
        "J Bradley",
        "B Yavuz",
        "E Sparks",
        "S Venkataraman",
        "D Liu"
      ],
      "year": 2016
    },
    {
      "title": "How are habits formed: modelling habit formation in the real world",
      "authors": [
        "P Lally",
        "C Van Jaarsveld",
        "H Potts",
        "J Wardle"
      ],
      "year": 2010,
      "doi": "10.1002/ejsp.674"
    },
    {
      "title": "Self-efficacy in activities of daily living and symptom management in people with dizziness: a focus group study",
      "authors": [
        "H Fridberg",
        "C Gustavsson"
      ],
      "year": 2019
    },
    {
      "title": "Perceived difficulty with physical tasks, lifestyle, and physical performance in obese children",
      "authors": [
        "G Valerio",
        "V Gallarato",
        "D 'amico",
        "O Sticco",
        "M Tortorelli",
        "P Zito"
      ],
      "year": 2014
    },
    {
      "title": "Deep reinforcement learning: an overview",
      "authors": [
        "Y Li"
      ],
      "year": 2017
    },
    {
      "title": "Mobile intervention design in diabetes: review and recommendations",
      "authors": [
        "S Mulvaney",
        "L Ritterband",
        "L Bosslet"
      ],
      "year": 2011,
      "doi": "10.1007/s11892-011-0230-y"
    }
  ],
  "num_references": 52
}
