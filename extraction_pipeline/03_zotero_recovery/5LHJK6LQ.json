{
  "paper_id": "5LHJK6LQ",
  "title": "A guide to deep learning in healthcare",
  "year": 2022,
  "date": "2022-09-15",
  "journal": "Nat. Med",
  "publication": "Nat. Med",
  "authors": [
    {
      "forename": "Eric",
      "surname": "Topol",
      "name": "Eric Topol",
      "affiliation": "1  Department of Neurology , Yale School of Medicine , New Haven , CT , USA. \n\t\t\t\t\t\t\t\t Department of Neurology \n\t\t\t\t\t\t\t\t Yale School of Medicine \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t New Haven \n\t\t\t\t\t\t\t\t\t CT \n\t\t\t\t\t\t\t\t\t USA",
      "email": "etopol@scripps.edu"
    }
  ],
  "doi": "",
  "sections": [
    {
      "title": "W",
      "text": "hile artificial intelligence (AI) tools have transformed several domains (for example, language translation, speech recognition and natural image recognition), medicine has lagged behind. This is partly due to complexity and high dimensionality-in other words, a large number of unique features or signals contained in the data-leading to technical challenges in developing and validating solutions that generalize to diverse populations. However, there is now widespread use of wearable sensors and improved capabilities for data capture, aggregation and analysis, along with decreasing costs of genome sequencing and related 'omics' technologies. Collectively, this sets the foundation and need for novel tools that can meaningfully process this wealth of data from multiple sources, and provide value across biomedical discovery, diagnosis, prognosis, treatment and prevention.\n\nMost of the current applications of AI in medicine have addressed narrowly defined tasks using one data modality, such as a computed tomography (CT) scan or retinal photograph. In contrast, clinicians process data from multiple sources and modalities when diagnosing, making prognostic evaluations and deciding on treatment plans. Furthermore, current AI assessments are typically one-off snapshots, based on a moment of time when the assessment is performed, and therefore not 'seeing' health as a continuous state. In theory, however, AI models should be able to use all data sources typically available to clinicians, and even those unavailable to most of them (for example, most clinicians do not have a deep understanding of genomic medicine). The development of multimodal AI models that incorporate data across modalitiesincluding biosensors, genetic, epigenetic, proteomic, microbiome, metabolomic, imaging, text, clinical, social determinants and environmental data-is poised to partially bridge this gap and enable broad applications that include individualized medicine, integrated, real-time pandemic surveillance, digital clinical trials and virtual health coaches (Fig.  1 ). In this Review, we explore the opportunities for such multimodal datasets in healthcare; we then discuss the key challenges and promising strategies for overcoming these. Basic concepts in AI and machine learning will not be discussed here but are reviewed in detail elsewhere  [1] [2] [3]  ."
    },
    {
      "title": "Opportunities for leveraging multimodal data",
      "text": "Personalized 'omics' for precision health. With the remarkable progress in sequencing over the past two decades, there has been a revolution in the amount of fine-grained biological data that can be obtained using novel technical developments. These are collectively referred to as the 'omes' , and includes the genome, proteome, transcriptome, immunome, epigenome, metabolome and microbiome  4  . These can be analyzed in bulk or at the single-cell level, which is relevant because many medical conditions such as cancer are quite heterogeneous at the tissue level, and much of biology shows cell and tissue specificity.\n\nEach of the omics has shown value in different clinical and research settings individually. Genetic and molecular markers of malignant tumors have been integrated into clinical practice  5, 6  , with the US Food and Drug Administration (FDA) providing approval for several companion diagnostic devices and nucleic acid-based tests  7, 8  . As an example, Foundation Medicine and Oncotype IQ (Genomic Health) offer comprehensive genomic profiling tailored to the main classes of genomic alterations across a broad panel of genes, with the final goal of identifying potential therapeutic targets  9, 10  . Beyond these molecular markers, liquid biopsy sampleseasily accessible biological fluids such as blood and urine-are becoming a widely used tool for analysis in precision oncology, with some tests based on circulating tumor cells and circulating tumor DNA already approved by the FDA  11  . Beyond oncology, there has been a remarkable increase in the last 15 years in the availability and sharing of genetic data, which enabled genome-wide association studies  12  and characterization of the genetic architecture of complex human conditions and traits  13  . This has improved our understanding of biological pathways and produced tools such as polygenic risk scores  14  (which capture the overall genetic propensity to complex traits for each individual), and may be useful for risk stratification and individualized treatment, as well as in clinical research to enrich the recruitment of participants most likely to benefit from interventions  15, 16  .\n\nThe integration of these very distinct types of data remains challenging. Yet, overcoming this problem is paramount, as the successful integration of omics data, in addition to other types such as electronic health record (EHR) and imaging data, is expected to increase our understanding of human health even further and allow for precise and individualized preventive, diagnostic and therapeutic strategies  4  . Several approaches have been proposed for multi-omics data integration in precision health contexts  17  . Graph neural networks are one example;  18, 19  these are deep learning model"
    },
    {
      "title": "Multimodal biomedical AI",
      "text": "Juli\u00e1n N. Acosta 1 , Guido J. Falcone 1 , Pranav Rajpurkar 2,4 \u2709 and Eric J.  Topol 3, 4 \u2709  The increasing availability of biomedical data from large biobanks, electronic health records, medical imaging, wearable and ambient biosensors, and the lower cost of genome and microbiome sequencing have set the stage for the development of multimodal artificial intelligence solutions that capture the complexity of human health and disease. In this Review, we outline the key applications enabled, along with the technical and analytical challenges. We explore opportunities in personalized medicine, digital clinical trials, remote monitoring and care, pandemic surveillance, digital twin technology and virtual health assistants. Further, we survey the data, modeling and privacy challenges that must be overcome to realize the full potential of multimodal artificial intelligence in health.\n\narchitectures that process computational graphs-a well-known data structure comprising nodes (representing concepts or entities) and edges (representing connections or relationships between nodes)-thereby allowing scientists to account for the known interrelated structure of multiple types of omics data, which can improve performance of a model  20  . Another approach is dimensionality reduction, including novel methods such as PHATE and Multiscale PHATE, which can learn abstract representations of biological and clinical data at different levels of granularity, and have been shown to predict clinical outcomes, for example, in people with coronavirus disease 2019 (COVID-19)  21, 22  .\n\nIn the context of cancer, overcoming challenges related to data access, sharing and accurate labeling could potentially lead to impactful tools that leverage the combination of personalized omics data with histopathology, imaging and clinical data to inform clinical trajectories and improve patient outcomes  23  . The integration of histopathological morphology data with transcriptomics data, resulting in spatially resolved transcriptomics  24  , constitutes a novel and promising methodological advancement that will enable finer-grained research into gene expression within a spatial context. Of note, researchers have utilized deep learning to leverage histopathology images to predict spatial gene expression from these images alone, pointing to morphological features in these images not captured by human experts that could potentially enhance the utility and lower the costs of this technology  25, 26  .\n\nGenetic data are increasingly cost effective, requiring only a one-in-a-lifetime ascertainment, but they also have limited predictive ability on their own  27  . Integrating genomics data with other omics data may capture more dynamic and real-time information on how each particular combination of genetic background and environmental exposures interact to produce the quantifiable continuum of health status. As an example, Kellogg et al.  28  conducted an N-of-1 study performing whole-genome sequencing (WGS) and periodic measurements of other omics layers (transcriptome, proteome, metabolome, antibodies and clinical biomarkers); polygenic risk scoring showed an increased risk of type II diabetes mellitus, and comprehensive profiling of other omics enabled early detection and dissection of signaling network changes during the transition from health to disease.\n\nAs the scientific field advances, the cost-effectiveness profile of WGS will become increasingly favorable, facilitating the combination of clinical and biomarker data with already available genetic data to arrive at a rapid diagnosis of conditions that were previously difficult to detect  29  . Ultimately, the capability to develop multimodal AI that includes many layers of omics data will get us to the desired goal of deep phenotyping of an individual; in other words, a true understanding of each person's biological uniqueness and how that affects health."
    },
    {
      "title": "Digital clinical trials.",
      "text": "Randomized clinical trials are the gold standard study design to investigate causation and provide evidence to support the use of novel diagnostic, prognostic and therapeutic interventions in clinical medicine. Unfortunately, planning and executing a high-quality clinical trial is not only time consuming (usually taking many years to recruit enough participants and follow them in time) but also financially very costly  30, 31  . In addition, geographic, sociocultural and economic disparities in access to enrollment, have led to a remarkable underrepresentation of several groups in these studies. This limits the generalizability of results and leads to a scenario whereby widespread underrepresentation in biomedical research further perpetuates existing disparities  32  . Digitizing clinical trials could provide an unprecedented opportunity to overcome these limitations, by reducing barriers to participant enrollment and retainment, promoting engagement and optimizing trial measurements and interventions. At the same time, the use of digital technologies can enhance the granularity of the information obtained from participants, thereby increasing the value of these studies  33  . Data from wearable technology (including heart rate, sleep, physical activity, electrocardiography, oxygen saturation and glucose monitoring) and smartphone-enabled self-reported questionnaires can be useful for monitoring clinical trial patients, identifying adverse events or ascertaining trial outcomes  34  . Additionally, a recent study highlighted the potential of data from wearable sensors to predict laboratory results  35  . Consequently, the number of studies using digital products has been growing rapidly in the last few years, with a compound annual growth rate of around 34%  36  . Most of these studies utilize data from a single wearable device. One pioneering trial used a 'band-aid' patch sensor for detecting atrial fibrillation; the sensor was mailed to participants who were enrolled remotely, without the use of any clinical sites, and set the foundation for digitized clinical trials  37  . Many remote, site-less trials using wearables were conducted during the COVID-19 pandemic to detect coronavirus  38  .\n\nEffectively combining data from different wearable sensors with clinical data remains a challenge and an opportunity. Digital clinical trials could leverage multiple sources of participants' data to enable automatic phenotyping and subgrouping  34  , which could be useful for adaptive clinical trial designs that use ongoing results to modify the trial in real time  39, 40  . In the future, we expect that the increased availability of these data and novel multimodal learning techniques will improve our capabilities in digital clinical trials. Of note, recent work in a time-series analysis by Google has demonstrated the promise of attention-based model architectures to combine both static and time-dependent inputs to achieve interpretable time-series forecasting. As a hypothetical example, these models could understand whether to focus on static features such as genetic background, known time-varying features such as time of the day or observed features such as current glycemic levels, to make predictions on future risk of hypoglycemia or hyperglycemia  41  . Graph neural networks have been recently proposed to overcome the problem of missing or irregularly sampled data from multiple health sensors, by leveraging information from the interconnection between these  42  .\n\nPatient recruitment and retention in clinical trials are essential but remain a challenge. In this setting, there is an increasing interest in the utilization of synthetic control methods (that is, using external data to create controls). Although synthetic control trials are still relatively novel  43  , the FDA has already approved medications based on historical controls  44  and has developed a framework for the utilization of real-world evidence  45  . AI models utilizing data from different modalities can potentially help identify or generate the most optimal synthetic controls  46, 47  .\n\nRemote monitoring: the 'hospital-at-home' . Recent progress with biosensors, continuous monitoring and analytics raises the possibility of simulating the hospital setting in a person's home. This offers the promise of marked reduction of cost, less requirement for healthcare workforce, avoidance of nosocomial infections and medical errors that occur in medical facilities, along with the comfort, convenience and emotional support of being with family members  48  .\n\nIn this context, wearable sensors have a crucial role in remote patient monitoring. The availability of relatively affordable noninvasive devices (smartwatches or bands) that can accurately measure several physiological metrics is increasing rapidly  49, 50  . Combining these data with those derived from EHRs-using standards such as the Fast Healthcare Interoperability Resources, a global industry standard for exchanging healthcare data  51  -to query relevant information about a patient's underlying disease risk could create a more personalized remote monitoring experience for patients and caregivers. Ambient wireless sensors offer an additional opportunity to collect valuable data. Ambient sensors are devices located within the environment (for example, a room, a wall or a mirror) ranging from video cameras and microphones to depth cameras and radio signals. These ambient sensors can potentially improve remote care systems at home and in healthcare institutions  52  .\n\nThe integration of data from these multiple modalities and sensors represents a promising opportunity to improve remote patient monitoring, and some studies have already demonstrated the potential of multimodal data in these scenarios. For example, the combination of ambient sensors (such as depth cameras and microphones) with wearables data (for example, accelerometers, which measure physical activity) has the potential to improve the reliability of fall detection systems while keeping a low false alarm rate  53  , and to improve gait analysis performance  54  . Early detection of impairments in physical functional status via activities of daily living such as bathing, dressing and eating is remarkably important to provide timely clinical care, and the utilization of multimodal data from wearable devices and ambient sensors can potentially help with accurate detection and classification of difficulties in these activities  55  .\n\nBeyond management of chronic or degenerative disorders, multimodal remote patient monitoring could also be useful in the setting of acute disease. A recent program conducted by the Mayo Clinic showcased the feasibility and safety of remote monitoring in people with COVID-19 (ref.  56 ). Remote patient monitoring for hospital-at-home applications-not yet validated-requires randomized trials of multimodal AI-based remote monitoring versus hospital admission to show no impairment of safety. We need to be able to predict impending deterioration and have a system to intervene, and this has not been achieved yet.\n\nPandemic surveillance and outbreak detection. The current COVID-19 pandemic has highlighted the need for effective infectious disease surveillance at national and state levels  57  , with some countries successfully integrating multimodal data from migration maps, mobile phone utilization and health delivery data to forecast the spread of the outbreak and identify potential cases  58, 59  .\n\nOne study has also demonstrated the utilization of resting heart rate and sleep minutes tracked using wearable devices to improve surveillance of influenza-like illness in the USA  60  . This initial success evolved into the Digital Engagement and Tracking for Early Control and Treatment (DETECT) Health study, launched by the Scripps Research Translational Institute as an app-based research program aiming to analyze a diverse set of data from wearables to allow for rapid detection of the emergence of influenza, coronavirus and other fast-spreading viral illnesses. A follow-up study from this program showed that jointly considering participant self-reported symptoms and sensor metrics improved performance relative to either modality alone, reaching an area under the receiver operating curve value of 0.80 (95% confidence interval 0.73-0.86) for classifying COVID-19-positive versus COVID-19-negative status  61  .\n\nSeveral other use cases for multimodal AI models in pandemic preparedness and response have been tested with promising results, but further validation and replication of these results are needed  62, 63  .\n\nDigital twins. We currently rely on clinical trials as the best evidence to identify successful interventions. Interventions that help 10 of 100 people may be considered successful, but these are applied to the other 90 without proven or likely benefit. A complementary approach known as 'digital twins' can fill the knowledge gaps by leveraging large amounts of data to model and predict with high precision how a certain therapeutic intervention would benefit or harm a particular patient.\n\nDigital twin technology is a concept borrowed from engineering that uses computational models of complex systems (for example, cities, airplanes or patients) to develop and test different strategies or approaches more quickly and economically than in real-life scenarios  64  . In healthcare, digital twins are a promising tool for drug target discovery  65, 66  .\n\nIntegrating data from multiple sources to develop digital twin models using AI tools has already been proposed in precision oncology and cardiovascular health  67, 68  . An open-source modular framework has also been proposed for the development of medical digital twin models  69  . From a commercial point of view, Unlearn.AI has developed and tested digital twin models that leverage diverse sets of clinical data to enhance clinical trials for Alzheimer's disease and multiple sclerosis  70, 71  .\n\nConsidering the complexity of human organisms, the development of accurate and useful digital twin technology in medicine will depend on the ability to collect large and diverse multimodal data ranging from omics data and physiological sensors to clinical and sociodemographic data. This will likely require large collaborations across health systems, research groups and industry, such as the Swedish Digital Twins Consortium  65, 72  . The American Society of Clinical Oncology, through its subsidiary called CancerLinQ, developed a platform that enables researchers to utilize a wealth of data from patients with cancer to help guide optimal treatment and improve outcomes  73  . The development of AI models capable of effectively learning from all these data modalities together, to make real-time predictions, is paramount.\n\nVirtual health assistant. More than one-third of US consumers have acquired a smart speaker in the last few years. However, virtual health assistants-digital AI-enabled coaches that can advise people on their health needs-have not been developed widely to date, and those currently in the market often target a particular condition or use case. In addition, a recent review of health-focused conversational agents apps found that most of these rely on rule-based approaches and predefined app-led dialog  74  .\n\nOne of the most popular, although not multimodal AI-based, current applications of these narrowly focused virtual health assistants is in diabetes care. Virta health, Accolade and Onduo by Verily (Alphabet) have all developed applications that aim to improve diabetes control, with some demonstrating improvement in hemoglobin A1c levels in individuals who followed the programs  75  . Many of these companies have expanded or are in the process of expanding to other use cases such as hypertension control and weight loss. Other examples of virtual health coaches have tackled common conditions such as migraine, asthma and chronic obstructive pulmonary disease, among others  76  . Unfortunately, most of these applications have been tested only on small observational studies, and much more research, including randomized clinical trials, are needed to evaluate their benefits.\n\nLooking into the future, the successful integration of multiple data sources in AI models will facilitate the development of broadly focused personalized virtual health assistants  77  . These virtual health assistants can leverage individualized profiles based on genome sequencing, other omics layers, continuous monitoring of blood biomarkers and metabolites, biosensors and other relevant biomedical data-to promote behavior change, answer health-related questions, triage symptoms or communicate with healthcare providers when appropriate. Importantly, these AI-enabled medical coaches will need to demonstrate beneficial effects on clinical outcomes via randomized trials to achieve widespread acceptance in the medical field. As most of these applications are focused on improving health choices, they will need to provide evidence of influencing health behavior, which represents the ultimate pathway for the successful translation of most interventions  78  .\n\nWe still have a long way to go to achieve the full potential of AI and multimodal data integration into virtual health assistants, including the technical challenges, data-related challenges and privacy challenges discussed below. Given the rapid advances in conversational AI  79  , coupled with the development of increasingly sophisticated multimodal learning approaches, we expect future digital health applications to embrace the potential of AI to deliver accurate and personalized health coaching."
    },
    {
      "title": "Multimodal data collection",
      "text": "The first requirement for the successful development of multimodal data-enabled applications is the collection, curation and harmonization of well-phenotyped and large annotated datasets, as no amount of technical sophistication can derive information not present in the data  80  . In the last 20 years, many national and international studies have collected multimodal data with the ultimate goal of accelerating precision health (Table  1 ). In the UK, the UK Biobank initiated enrollment in 2006, reaching a final participant count of over 500,000, and plans to follow participants for at least 30 years after enrollment  81  . This large biobank has collected multiple layers of data from participants, including sociodemographic and lifestyle information, physical measurements, biological samples, 12-lead electrocardiograms and EHR data  82  . Further, almost all participants underwent genome-wide array genotyping and, more recently, proteome, whole-exome sequencing  83  and WGS  84  . A subset of individuals also underwent brain magnetic resonance imaging (MRI), cardiac MRI, abdominal MRI, carotid ultrasound and dual-energy X-ray absorptiometry, including repeat imaging across at least two time points  85  .\n\nSimilar initiatives have been conducted in other countries, such as the China Kadoorie Biobank  86  and Biobank Japan  87  . In the USA, the Department of Veteran Affairs launched the Million Veteran Program  88  in 2011, aiming to enroll 1 million veterans to contribute to scientific discovery. Two important efforts funded by the National Institutes of Health (NIH) include the Trans-Omics for Precision Medicine (TOPMed) program and the All of Us Research Program. TOPMed collects WGS with the aim to integrate this genetic information with other omics data  89  . The All of Us Research Program  90  constitutes another novel and ambitious initiative by the NIH that has enrolled about 400,000 diverse participants of the 1 million people planned across the USA, and is focused on enrolling individuals from broadly defined underrepresented groups in biomedical research, which is especially needed in medical AI  91, 92  .\n\nBesides these large national initiatives, independent institutional and multi-institutional efforts are also building deep, multimodal data resources in smaller numbers of people. The Project Baseline Health Study, funded by Verily and managed in collaboration with Stanford University, Duke University and the California Health and Longevity Institute, aims to enroll at least 10,000 individuals, starting with an initial 2,500 participants from whom a broad range of multimodal data are collected, with the aim of evolving into a combined virtual-in-person research effort  93  . As another example, the American Gut Project collects microbiome data from self-selected participants across several countries  94  . These participants also complete surveys about general health status, disease history, lifestyle data and food frequency. The Medical Information Mart for Intensive Care (MIMIC) database  95  , organized by the Massachusetts Institute of Technology, represents another example of multidimensional data collection and harmonization. Currently in its fourth version, MIMIC is an open-source database that contains de-identified data from thousands of patients who were admitted to the critical care units of the Beth Israel Deaconess Medical Center, including demographic information, EHR data (for example, diagnosis codes, medications ordered and administered, laboratory data and physiological data such as blood pressure or intracranial pressure values), imaging data (for example, chest radiographs)  96  and, in some versions, natural language text such as radiology reports and medical notes. This granularity of data is particularly useful for the data science and machine learning community, and MIMIC has become one of the benchmark datasets for AI models aiming to predict the development of clinical events such as kidney failure, or outcomes such as survival or readmissions  97, 98  .\n\nThe availability of multimodal data in these datasets may help achieve better diagnostic performance across a range of different tasks. As an example, recent work has demonstrated that the combination of imaging and EHR data outperforms each of these modalities alone to identify pulmonary embolism  99  , and to differentiate between common causes of acute respiratory failure, such as heart failure, pneumonia or chronic obstructive pulmonary disease 100 . The Michigan Predictive Activity & Clinical Trajectories in Health (MIPACT) study constitutes another example, with participants contributing data from wearables, physiological data (blood pressure), clinical information (EHR and surveys) and laboratory data  101  . The North American Prodrome Longitudinal Study is yet another example. This multisite program recruited individuals, and collected demographic, clinical and blood biomarker data with the goal of understanding the prodromal stages of psychosis  102, 103  . Other studies focusing on psychiatric disorders such as the Personalised Prognostic Tools for Early Psychosis Management also collected several types of data and have already empowered the development of multimodal machine learning workflows 104 ."
    },
    {
      "title": "Technical challenges",
      "text": "Implementation and modeling challenges. Health data are inherently multimodal. Our health status encompasses many domains (social, biological and environmental) that influence well-being in complex ways. Additionally, each of these domains is hierarchically organized, with data being abstracted from the big picture macro level (for example, disease presence or absence) to the in-depth micro level (for example, biomarkers, proteomics and genomics). Furthermore, current healthcare systems add to this multimodal approach by generating data in multiple ways: radiology and pathology images are, for example, paired with natural language data from their respective reports, while disease states are also documented in natural language and tabular data in the EHR.\n\nTable 1 | Examples of studies with multimodal data available Study country Year started data modalities Access Sample size UK Biobank UK 2006 Questionnaires EHR/clinical Laboratory Genome-wide genotyping WES WGS Imaging Metabolites Open access ~500,000 China Kadoorie Biobank China 2004 Questionnaires Physical measurements Biosamples Genome-wide genotyping Restricted access ~500,000 Biobank Japan Japan 2003 Questionnaires Clinical Laboratroy Genome-wide genotyping Restricted access ~200,000\n\nMultimodal machine learning (also referred to as multimodal learning) is a subfield of machine learning that aims to develop and train models that can leverage multiple different types of data and learn to relate these multiple modalities or combine them, with the goal of improving prediction performance  105  . A promising approach is to learn accurate representations that are similar for different modalities (for example, a picture of an apple should be represented similarly to the word 'apple'). In early 2021, OpenAI released an architecture termed Contrastive Language Image Pretraining (CLIP), which, when trained on millions of image-text pairs, matched the performance of competitive, fully supervised models without fine-tuning 106 . CLIP was inspired by a similar approach developed in the medical imaging domain termed Contrastive Visual Representation Learning from Text (ConVIRT)  107  . With ConVIRT, an image encoder and a text encoder are trained to generate image and text representations by maximizing the similarity of correctly paired image and text examples and minimizing the similarity of incorrectly paired examples-this is called contrastive learning. This approach for paired image-text co-learning has been used recently to learn from chest X-rays and their associated text reports, outperforming other self-supervised and fully supervised methods 108 . Other architectures have also been developed to integrate multimodal data from images, audio and text, such as the Video-Audio-Text Transformer, which uses videos to obtain paired multimodal image, text and audio and to train accurate multimodal representations able to generalize with good performance on many tasks-such as recognizing actions in videos, classifying audio events, classifying images, and selecting the most adequate video for an input text  109  .\n\nAnother desirable feature for multimodal learning frameworks is the ability to learn from different modalities without the need for different model architectures. Ideally, a unified multimodal model would incorporate different types of data (images, physiological sensor data and structured and unstructured text data, among others), codify concepts contained in these different types of data in a flexible and sparse way (that is, a unique task activates only a small part of the network, with the model learning which parts of the network should handle each unique task) 110 , produce aligned representations for similar concepts across modalities (for example, the picture of a dog, and the word 'dog' should produce similar internal representations), and provide any arbitrary type of output as required by the task  111  .\n\nIn the last few years, there has been a transition from architectures with strong modality-specific biases-such as convolutional neural networks for images, or recurrent neural networks for text and physiological signals-to a relatively novel architecture called the Transformer, which has demonstrated good performance across a wide variety of input and output modalities and tasks 112 . The key strategy behind transformers is to allow neural networks-which are artificial learning models that loosely mimic the behavior of the human brain-to dynamically pay attention to different parts of the input when processing and ultimately making decisions. Originally proposed for natural language processing, thus providing a way to capture the context of each word by attending to other words of the input sentence, this architecture has been successfully extended to other modalities  113  .\n\nWhile each input token (that is, the smallest unit for processing) in natural language processing corresponds to a specific word, other modalities have generally used segments of images or video clips as tokens  114  . Transformer architectures allow us to unify the framework for learning across modalities but may still need modality-specific tokenization and encoding. A recent study by Meta AI (Meta Platforms) proposed a unified framework for self-supervised learning that is independent of the modality of interest, but still requires modality-specific preprocessing and training  115  . Benchmarks for self-supervised multimodal learning allow us to measure the progress of methods across modalities: for instance, the Domain-Agnostic Benchmark for Self-supervised learning (DABS) is a recently proposed benchmark that includes chest X-rays, sensor data and natural image and text data  116  .\n\nRecent advances proposed by DeepMind (Alphabet), including Perceiver 117 and Perceiver IO 118 , propose a framework for learning across modalities with the same backbone architecture. Importantly, the input to the Perceiver architectures are modality-agnostic byte arrays, which are condensed through an attention bottleneck (that is, an architecture feature that restricts the flow of information, forcing models to condense the most relevant) to avoid size-dependent large memory costs (Fig.  2a ). After processing these inputs, the Perceiver can then feed the representations to a final classification layer to obtain the probability of each output category, while the Perceiver IO can decode these representations directly into arbitrary outputs such as pixels, raw audio and classification labels, through a query vector that specifies the task of interest; for example, the model could output the predicted imaging appearance of an evolving brain tumor, in addition to the probability of successful treatment response.\n\nA promising aspect of transformers is the ability to learn meaningful representations with unlabeled data, which is paramount in biomedical AI given the limited and expensive resources needed to obtain high-quality labels. Many of the approaches mentioned above require aligned data from different modalities (for example, image-text pairs). A study from DeepMind, in fact, suggested that curating higher-quality image-text datasets may be more important than generating large single-modality datasets, and other aspects of algorithm development and training 119 . However, these data may not be readily available in the setting of biomedical AI. One possible solution to this problem is to leverage available data from one modality to help learning with another-a multimodal learning task termed 'co-learning' 105 . As an example, some studies suggest that transformers pretrained on unlabeled language data might be able to generalize well to a broad range of other tasks 120 . In medicine, a model architecture called 'CycleGANs' , trained on unpaired contrast and non-contrast CT scans, has been used to generate synthetic non-contrast or contrast CT scans 121 , with this approach showing improvements, for instance, in COVID-19 diagnosis 122 . While promising, this approach has not been tested widely in the biomedical setting and requires further exploration.\n\nAnother important modeling challenge relates to the exceedingly high number of dimensions contained in multimodal health data, collectively termed 'the curse of dimensionality' . As the number of dimensions (that is, variables or features contained in a dataset) increases, the number of people carrying some specific combinations of these features decreases (or for some combinations, even disappears), leading to 'dataset blind spots' , that is, portions of the feature space (the set of all possible combinations of features or variables) that do not have any observation. These dataset blind spots can hurt model performance in terms of real-life prediction and should therefore be considered early in the model development and evaluation process  123  . Several strategies can be used to mitigate this issue, and have been described in detail elsewhere  123  . In brief, these include collecting data using maximum performance tasks (for example, rapid finger tapping for motor control, as opposed to passively collected data during everyday movement), ensuring large and diverse sample sizes (that is, with the conditions matching those expected at clinical deployment of the model), using domain knowledge to guide feature engineering and selection (with a focus on feature repeatability), appropriate model training and regularization, rigorous model validation and comprehensive model monitoring (including monitoring the difference between the distributions of training data and data found after deployment). Looking to the future, developing models able to incorporate previous knowledge (for example, known gene regulatory pathways and protein interactions) might be another promising approach to overcome the curse of dimensionality. Along these lines, recent studies demonstrated that models augmented by retrieving information from large databases outperform larger models trained on larger datasets, effectively leveraging available information and also providing added benefits such as interpretability  124, 125  .\n\nAn increasingly used approach in multimodal learning is to combine the data from different modalities, as opposed to simply inputting several modalities separately into a model, to increase prediction performance-process termed 'multimodal fusion'  126, 127  . Fusion of different data modalities can be performed at different stages of the process. The simplest approach involves concatenating input modalities or features before any processing (early fusion). While simple, this approach is not suitable for many complex data modalities. A more sophisticated approach is to combine and co-learn representations of these different modalities during the training process (joint fusion), allowing for modality-specific preprocessing while still capturing the interaction between data modalities. Finally, an alternative approach is to train separate models for each modality and combine the output probabilities (late fusion), a simple and robust approach, but at the cost of missing any information that could be abstracted from the interaction between modalities. Early work on fusion focused on allowing time-series models to leverage information from structured covariates for tasks such as forecasting osteoarthritis progression and predicting surgical outcomes in patients with cerebral palsy 128 . As another example of fusion, a group from DeepMind used a high-dimensional EHR-based dataset comprising 620,000 dimensions that were projected into a continuous embedding space with only 800 dimensions, capturing a wide array of information in a 6-h time frame for each patient, and built a recurrent neural network to predict acute kidney injury over time 129 . A lot of studies have used fusion of two modalities (bimodal fusion) to improve predictive performance. Imaging and EHR-based data have been fused to improve detection of pulmonary embolism, outperforming single-modality models  99  . Another bimodal study fused imaging features from chest X-rays with clinical covariates, improving the diagnosis of tuberculosis in individuals with HIV 130 . Optical coherence tomography and infrared reflectance optic disc imaging have been combined to better predict visual field maps compared to using either of those modalities alone  131  .\n\nMultimodal fusion is a general concept that can be tackled using any architectural choice. Although not biomedical, we can learn from some AI imaging work; modern guided image generation models such as DALL-E 132 and GLIDE 133 often concatenate information from different modalities into the same encoder. This approach has demonstrated success in a recent study conducted by DeepMind (using Gato, a generalist agent) showing that concatenating a wide variety of tokens created from text, images and button presses, among others, can be used to teach a model to perform several distinct tasks ranging from captioning images and playing Atari games to stacking blocks with a robot arm (Fig.  2b )  134  . Importantly, a recent study titled Align Before Fuse suggested that aligning representations across modalities before fusing them might result in better performance in downstream tasks, such as for creating text captions for images  135  . A recent study from Google Research proposed using attention bottlenecks for multimodal fusion, thereby restricting the flow of cross-modality information to force models"
    },
    {
      "title": "AI:",
      "text": "This is probably an allergic reaction"
    },
    {
      "title": "Images and questions"
    },
    {
      "title": "Masked (and shifted) target",
      "text": "Fig.  2  | Simplified illustration of the novel technical concepts in multimodal AI. a, Simplified schematic of the Perceiver-like architecture: images, text and other inputs are converted agnostically into byte arrays that are concatenated (that is, fused) and passed through cross-attention mechanisms (that is, a mechanism to project or condense information into a fixed-dimensional representation) to feed information into the network. b, Simplified illustration of the conceptual framework behind the multimodal multitask architectures (for example, Gato), within a hypothetical medical example: distinct input modalities ranging from images, text and actions are tokenized and fed to the network as input sequences, with masked shifted versions of these sequences fed as targets (that is, the network only sees information from previous time points to predict future actions, only previous words to predict the next or only the image to predict text); the network then learns to handle multiple modalities and tasks.\n\nto share the most relevant information across modalities and hence improving computational performance 136 . Another paradigm of using two modalities together is to 'translate' from one to the other. In many cases, one data modality may be strongly associated with clinical outcomes but be less affordable, accessible or require specialized equipment or invasive procedures. Deep learning-enabled computer vision has been shown to capture information typically requiring a higher-fidelity modality for human interpretation. As an example, one study developed a convolutional neural network that uses echocardiogram videos to predict laboratory values of interest such as cardiac biomarkers (troponin I and brain natriuretic peptide) and other commonly obtained biomarkers, and found that predictions from the model were accurate, with some of them even having more prognostic performance for heart failure admissions than conventional laboratory testing  137  . Deep learning has also been widely studied in cancer pathology to make predictions beyond typical pathologist interpretation tasks with H&E stains, with several applications including prediction of genotype and gene expression, response to treatment and survival using only pathology images as inputs  138  .\n\nMany other important challenges relating to multimodal model architectures remain. For some modalities (for example, three-dimensional imaging), even models using only a single time point require large computing capabilities, and the prospect of implementing a model that also processes large-scale omics or text data represents an important infrastructural challenge.\n\nWhile multimodal learning has improved at an accelerated rate for the past few years, we expect that current methods are unlikely to be sufficient to overcome all the major challenges mentioned above. Therefore, further innovation will be required to fully enable effective, multimodal AI models.\n\nData challenges. The multidimensional data underpinning health leads to a broad range of challenges in terms of collecting, linking and annotating these data. Medical datasets can be described along several axes 139 , including the sample size, depth of phenotyping, the length and intervals of follow-up, the degree of interaction between participants, the heterogeneity and diversity of the participants, the level of standardization and harmonization of the data and the amount of linkage between data sources. While science and technology have advanced remarkably to facilitate data collection and phenotyping, there are inevitable trade-offs among these features of biomedical datasets. For example, although large sample sizes (in the range of hundreds of thousands to millions) are desirable in most cases for the training of AI models (especially multimodal AI models), the costs of achieving deep phenotyping and good longitudinal follow-up scales rapidly with larger numbers of participants, becoming financially unsustainable unless automated methods of data collection are put in place.\n\nThere are large-scale efforts to provide meaningful harmonization to biomedical datasets, such as the Observational Medical Outcomes Partnership Common Data Model developed by the Observational Health Data Sciences and Informatics collaboration  140  . Harmonization enormously facilitates research efforts and enhances reproducibility and translation into clinical practice. However, harmonization may obscure some relevant pathophysiological processes underlying certain diseases. As an example, ischemic stroke subtypes tend not to be accurately captured by existing ontologies 141 , but utilizing raw data from EHRs or radiology reports could allow for the use of natural language processing for phenotyping  142  . Similarly, the Diagnostic and Statistical Manual of Mental Disorders categorizes diagnoses based on clinical manifestations, which might not fully represent underlying pathophysiological processes  143  .\n\nAchieving diversity across race/ethnicity, ancestry, income level, education level, healthcare access, age, disability status, geographic locations, gender and sexual orientation has proven difficult in practice. Genomics research is a prominent example, with the vast majority of studies focusing on individuals from European ancestry 144 . However, diversity of biomedical datasets is paramount as it constitutes the first step to ensure generalizability to the broader population  145  . Beyond these considerations, a required step for multimodal AI is the appropriate linking of all data types available in the datasets, which represents another challenge owing to the increasing risk of identification of individuals and regulatory constraints  146  .\n\nAnother frequent problem with biomedical data is the usually high proportion of missing data. While simply excluding patients with missing data before training is an option in some cases, selection bias can arise when other factors influence missing data 147 , and it is often more appropriate to address these gaps with statistical tools, such as multiple imputation 148 . As a result, imputation is a pervasive preprocessing step in many biomedical scientific fields, ranging from genomics to clinical data. Imputation has remarkably improved the statistical power of genome-wide association studies to identify novel genetic risk loci, and is facilitated by large reference datasets with deep genotypic coverage such as 1000 Genomes 149 , the UK10K 150 , the Haplotype reference consortium 151 and, recently, TOPMed  89  . Beyond genomics, imputation has also demonstrated utility for other types of medical data 152 . Different strategies have been suggested to make fewer assumptions. These include carry-forward imputation, with imputed values flagged and information added on when they were last measured 153 , and more complex strategies such as capturing the presence of missing data and time intervals using learnable decay terms  154  .\n\nThe risk of incurring several biases is important when conducting studies that collect health data, and multiple approaches are necessary to monitor and mitigate these biases 155 . The risk of these biases is amplified when combining data from multiple sources, as the bias toward individuals more likely to consent to each data modality could be amplified when considering the intersection between these potentially biased populations. This complex and unsolved problem is more important in the setting of multimodal health data (compared to unimodal data) and would warrant its own in-depth review. Medical AI algorithms using demographic features such as race as inputs can learn to perpetuate historical human biases, thereby resulting in harm when deployed 156 . Importantly, recent work has demonstrated that AI models can identify such features solely from imaging data, which highlights the need for deliberate efforts to detect racial bias and equalize racial outcomes during data quality control and model development  157  . In particular, selection bias is a common type of bias in large biobank studies, and has been reported as a problem, for example, in the UK Biobank 158 . This problem has also been pervasive in the scientific literature regarding COVID-19 (ref.  159 ). For example, patients using allergy medications were more likely to be tested for COVID-19, which leads to an artificially lower rate of positive tests, and an apparent protective effect among those tested-probably due to selection bias 160 . Importantly, selection bias can result in AI models trained on a sample that differs considerably from the general population 161 , thus hurting these models at inference time 162 . Privacy challenges. The successful development of multimodal AI in health requires breadth and depth of data, which encompasses higher privacy challenges than single-modality AI models. For example, previous studies have demonstrated that by utilizing only a little background information about participants, an adversary could re-identify those in large datasets (for example, the Netflix prize dataset), uncovering sensitive information about the individuals  163  .\n\nIn the USA, the Health Insurance Portability and Accountability Act (HIPAA) Privacy Rule is the fundamental legislation to protect privacy of health data. However, some types of health data-such as user-generated and de-identified health data-are not covered by this regulation, which poses a risk of reidentification by combining information from multiple sources. In contrast, the more recent General Data Protection Regulation (GDPR) from the European Union has a much broader scope regarding the definition of health data, and even goes beyond data protection to also require the release of information about automated decision-making using these data  164  .\n\nGiven the challenges, multiple technical solutions have been proposed and explored to ensure security and privacy while training multimodal AI models, including differential privacy, federated learning, homomorphic encryption and swarm learning  165, 166  . Differential privacy proposes a systematic random perturbation of the data with the ultimate goal of obscuring individual-level information while maintaining the global distribution of the dataset 167 . As expected, this approach constitutes a trade-off between the level of privacy obtained and the expected performance of the models. Federated learning, on the other hand, allows several individuals or health systems to collectively train a model without transferring raw data. In this approach, a trusted central server distributes a model to each of the individuals/organizations; each individual or organization then trains the model for a certain number of iterations and shares the model updates back to the trusted central server  165  . Finally, the trusted central server aggregates the model updates from all individuals/organizations and starts another round. Federated multimodal learning has been implemented in a multi-institutional collaboration for predicting clinical outcomes in people with COVID-19 (ref.  168 ). Homomorphic encryption is a cryptographic technique that allows mathematical operations on encrypted input data, therefore providing the possibility of sharing model weights without leaking information 169 . Finally, swarm learning is a relatively novel approach that, similarly to federated learning, is also based on several individuals or organizations training a model on local data, but does not require a trusted central server because it replaces it with the use of blockchain smart contracts  170  .\n\nImportantly, these approaches are often complementary and they can and should be used together. A recent study demonstrated the potential of coupling federated learning with homomorphic encryption to train a model to predict a COVID-19 diagnosis from chest CT scans, with the aggregate model outperforming all of the locally trained models 122 . While these methods are promising, multimodal health data are usually spread across several distinct organizations, ranging from healthcare institutions and academic centers to pharmaceutical companies. Therefore, the development of new methods to incentivize data sharing across sectors while preserving patient privacy is crucial.\n\nAn additional layer of safety can be obtained by leveraging novel developments in edge computing 171 . Edge computing, as opposed to cloud computing, refers to the idea of bringing computation closer to the sources of data (for example, close to ambient sensors or wearable devices). In combination with other methods such as federated learning, edge computing provides more security by avoiding the transmission of sensitive data to centralized servers. Furthermore, edge computing provides other benefits, such as reducing storage costs, latency and bandwidth usage. For example, some X-ray systems now run optimized versions of deep learning models directly in their hardware, instead of transferring images to cloud servers for identification of life-threatening conditions  172  .\n\nAs a result of the expanding healthcare AI market, biomedical data are increasingly valuable, leading to another challenge pertaining to data ownership. To date, this constitutes an open issue of debate. Some voices advocate for private patient ownership of the data, arguing that this approach would ensure the patients' right to self-determination, support health data transactions and maximize patients' benefit from data markets; while others suggest a non-property, regulatory model would better protect secure and transparent data use  173, 174  . Independent of the framework, appropriate incentives should be put in place to facilitate data sharing while ensuring security and privacy  175, 176  . conclusion Multimodal medical AI unlocks key applications in healthcare and many other opportunities exist beyond those described here. The field of drug discovery is a pertinent example, with many tasks that could leverage multidimensional data including target identification and validation, prediction of drug interactions and prediction of side effects 177 . While we addressed many important challenges to the use of multimodal AI, others that were outside the scope of this review are just as important, including the potential for false positives and how clinicians should interpret and explain the risks to patients.\n\nWith the ability to capture multidimensional biomedical data, we confront the challenge of deep phenotyping-understanding each individual's uniqueness. Collaboration across industries and sectors is needed to collect and link large and diverse multimodal health data (Box 1). Yet, as this juncture, we are far better at collating and storing such data, than we are at data analysis. To meaningfully process such high-dimensional data and actualize the many exciting use cases, it will take a concentrated joint effort of the medical community and AI researchers to build and validate new models, and ultimately demonstrate their utility to improve health outcomes."
    },
    {
      "title": "Box 1 | Priorities for future development of multimodal biomedical AI",
      "text": "\u2022 Discover and formulate key medical AI tasks for which multimodal data will add value over single modalities. \u2022 Develop approaches that can pretrain models using large amounts of unlabeled data across modalities and only require fine-tuning on limited labeled data. \u2022 Benchmark the effect of model architectures and multimodal approaches when working with previously underexplored high-dimensional data, such as omics data. \u2022 Collect paired (for example, image-text) multimodal data that could be used to train and test the generalizability of multimodal medical AI algorithms."
    },
    {
      "text": "Fig. 1 | data modalities and opportunities for multimodal biomedical AI. Created with BioRender.com."
    },
    {
      "text": "What caused this rash?"
    }
  ],
  "references": [
    {
      "title": "A guide to deep learning in healthcare",
      "authors": [
        "A Esteva"
      ],
      "year": 2019
    },
    {
      "title": "Deep learning-enabled medical computer vision",
      "authors": [
        "A Esteva"
      ],
      "year": 2021
    },
    {
      "title": "AI in health and medicine",
      "authors": [
        "P Rajpurkar",
        "E Chen",
        "O Banerjee",
        "E Topol"
      ],
      "year": 2022
    },
    {
      "title": "Integrative omics for health and disease",
      "authors": [
        "K Karczewski",
        "M Snyder"
      ],
      "year": 2018,
      "doi": "10.1038/nrg.2018.4"
    },
    {
      "title": "Emerging molecular markers of cancer",
      "authors": [
        "D Sidransky"
      ],
      "year": 2002
    },
    {
      "title": "An integrated genomic analysis of human glioblastoma multiforme",
      "authors": [
        "D Parsons"
      ],
      "year": 2008
    },
    {
      "title": "List of cleared or approved companion diagnostic devices (in vitro and imaging tools",
      "year": 2021,
      "doi": "10.1093/cid/cir046"
    },
    {
      "title": "Nucleic acid-based tests",
      "year": 2020,
      "doi": "10.1128/9781555818814.ch12.6"
    },
    {
      "title": "Why comprehensive genomic profiling?",
      "authors": [
        "Foundation Medicine"
      ],
      "year": 2018,
      "doi": "10.1200/adn.23.201428"
    },
    {
      "title": "Oncotype IQ. Oncotype MAP pan-cancer tissue test",
      "year": 2020
    },
    {
      "title": "Current and future perspectives of liquid biopsies in genomics-driven oncology",
      "authors": [
        "E Heitzer",
        "I Haque",
        "C Roberts",
        "M Speicher"
      ],
      "year": 2018
    },
    {
      "title": "Genome-wide association studies",
      "authors": [
        "E Uffelmann"
      ],
      "year": 2021
    },
    {
      "title": "A global overview of pleiotropy and genetic architecture in complex traits",
      "authors": [
        "K Watanabe"
      ],
      "year": 2019,
      "doi": "10.1038/s41588-019-0481-0"
    },
    {
      "title": "Tutorial: a guide to performing polygenic risk score analyses",
      "authors": [
        "S Choi",
        "T Mak",
        "-H O'reilly"
      ],
      "year": 2020,
      "doi": "10.1038/s41596-020-0353-1"
    },
    {
      "title": "Patients with high genome-wide polygenic risk scores for coronary artery disease may receive greater clinical benefit from alirocumab treatment in the ODYSSEY OUTCOMES trial",
      "authors": [
        "A Damask"
      ],
      "year": 2020
    },
    {
      "title": "Predicting benefit from evolocumab therapy in patients with atherosclerotic disease using a genetic risk score: results from the FOURIER trial",
      "authors": [
        "N Marston"
      ],
      "year": 2020,
      "doi": "10.1161/circulationaha.119.043805"
    },
    {
      "title": "Evaluation and comparison of multi-omics data integration methods for cancer subtyping",
      "authors": [
        "R Duan"
      ],
      "year": 2021
    },
    {
      "title": "A roadmap for multi-omics data integration using deep learning",
      "authors": [
        "M Kang",
        "E Ko",
        "T Mersha"
      ],
      "year": 2022,
      "doi": "10.1093/bib/bbab454"
    },
    {
      "title": "MOGONET integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification",
      "authors": [
        "T Wang"
      ],
      "year": 2021,
      "doi": "10.1038/s41467-021-23774-w"
    },
    {
      "title": "Graph neural networks and their current applications in bioinformatics",
      "authors": [
        "X.-M Zhang",
        "L Liang",
        "L Liu",
        "M.-J Tang"
      ],
      "year": 2021
    },
    {
      "title": "Visualizing structure and transitions in high-dimensional biological data",
      "authors": [
        "K Moon"
      ],
      "year": 2019
    },
    {
      "title": "Multiscale PHATE identifies multimodal signatures of COVID-19",
      "authors": [
        "M Kuchroo"
      ],
      "year": 2022,
      "doi": "10.1038/s41587-021-01186-x"
    },
    {
      "title": "Harnessing multimodal data integration to advance precision oncology",
      "authors": [
        "K Boehm",
        "P Khosravi",
        "R Vanguri",
        "J Gao",
        "S Shah"
      ],
      "year": 2021,
      "doi": "10.1038/s41568-021-00408-3"
    },
    {
      "title": "Method of the year: spatially resolved transcriptomics",
      "authors": [
        "V Marx"
      ],
      "year": 2021,
      "doi": "10.1038/s41592-020-01033-y"
    },
    {
      "title": "Integrating spatial gene expression and breast tumour morphology via deep learning",
      "authors": [
        "B He"
      ],
      "year": 2020
    },
    {
      "title": "Super-resolved spatial transcriptomics by deep data fusion",
      "authors": [
        "L Bergenstr\u00e5hle"
      ],
      "year": 2021,
      "doi": "10.1038/s41587-021-01075-3"
    },
    {
      "title": "Validity of polygenic risk scores: are we measuring what we think we are?",
      "authors": [
        "A Janssens"
      ],
      "year": 2019
    },
    {
      "title": "Personal omics for precision health",
      "authors": [
        "R Kellogg",
        "J Dunn",
        "M Snyder"
      ],
      "year": 2018,
      "doi": "10.1161/circresaha.117.310909"
    },
    {
      "title": "Rapid sequencing-based diagnosis of thiamine metabolism dysfunction syndrome",
      "authors": [
        "M Owen"
      ],
      "year": 2021
    },
    {
      "title": "Estimated costs of pivotal trials for novel therapeutic agents approved by the US food and drug administration, 2015-2016",
      "authors": [
        "T Moore",
        "H Zhang",
        "G Anderson",
        "G Alexander"
      ],
      "year": 2018,
      "doi": "10.1001/jamainternmed.2018.3931"
    },
    {
      "title": "Key cost drivers of pharmaceutical clinical trials in the United States",
      "authors": [
        "A Sertkaya",
        "H. -H Wong",
        "A Jessup",
        "T Beleche"
      ],
      "year": 2016,
      "doi": "10.1177/1740774515625964"
    },
    {
      "title": "Disparity of race reporting and representation in clinical trials leading to cancer drug approvals from 2008 to 2018",
      "authors": [
        "J Loree"
      ],
      "year": 2019,
      "doi": "10.1001/jamaoncol.2019.1870"
    },
    {
      "title": "Digital clinical trials: creating a vision for the future",
      "authors": [
        "S Steinhubl",
        "D Wolff-Hughes",
        "W Nilsen",
        "E Iturriaga",
        "R Califf"
      ],
      "year": 2019
    },
    {
      "title": "Digitizing clinical trials",
      "authors": [
        "O Inan"
      ],
      "year": 2020,
      "doi": "10.1038/s41746-020-0302-y"
    },
    {
      "title": "Wearable sensors enable personalized predictions of clinical laboratory measurements",
      "authors": [
        "J Dunn"
      ],
      "year": 2021
    },
    {
      "title": "Quantifying the use of connected digital products in clinical research",
      "authors": [
        "C Marra",
        "J Chen",
        "A Coravos",
        "A Stern"
      ],
      "year": 2020
    },
    {
      "title": "Effect of a home-based wearable continuous ECG monitoring patch on detection of undiagnosed atrial fibrillation: the mSToPS randomized clinical trial",
      "authors": [
        "S Steinhubl"
      ],
      "year": 2018
    },
    {
      "title": "Smartphone apps in the COVID-19 pandemic",
      "authors": [
        "J Pandit",
        "J Radin",
        "G Quer",
        "E Topol"
      ],
      "year": 2022,
      "doi": "10.1038/s41587-022-01350-x"
    },
    {
      "title": "Adaptive designs in clinical trials: why use them, and how to run and report them",
      "authors": [
        "P Pallmann"
      ],
      "year": 2018,
      "doi": "10.1186/s12916-018-1017-7"
    },
    {
      "title": "Clinical utility of polygenic risk scores for coronary artery disease",
      "authors": [
        "D Klarin",
        "P Natarajan"
      ],
      "year": 2021,
      "doi": "10.1038/s41569-021-00638-w"
    },
    {
      "title": "Temporal fusion transformers for interpretable multi-horizon time series forecasting",
      "authors": [
        "B Lim",
        "S Ar\u0131k",
        "N Loeff",
        "T Pfister"
      ],
      "year": 2021
    },
    {
      "title": "Graph-guided network for irregularly sampled multivariate time series",
      "authors": [
        "X Zhang",
        "M Zeman",
        "T Tsiligkaridis",
        "M Zitnik"
      ],
      "year": 2022
    },
    {
      "title": "Synthetic and external controls in clinical trials-a primer for researchers",
      "authors": [
        "K Thorlund",
        "L Dron",
        "J Park",
        "E Mills"
      ],
      "year": 2020
    },
    {
      "title": "FDA approves first treatment for a form of Batten disease",
      "year": 2017
    },
    {
      "title": "Real-world evidence",
      "year": 2022
    },
    {
      "title": "Synthetic control arm: the end of placebos?",
      "authors": [
        "Abbvie"
      ],
      "year": 2019
    },
    {
      "title": "Generating synthetic control subjects using machine learning for clinical trials in Alzheimer's disease (DIA",
      "authors": [
        "Ai Unlearn"
      ],
      "year": 2019
    },
    {
      "title": "Impact of remote patient monitoring on clinical outcomes: an updated meta-analysis of randomized controlled trials",
      "authors": [
        "B Noah"
      ],
      "year": 2018,
      "doi": "10.1038/s41746-017-0002-4"
    },
    {
      "title": "Wearable-device-measured physical activity and future health risk",
      "authors": [
        "T Strain"
      ],
      "year": 2020
    },
    {
      "title": "Advances in healthcare wearable devices",
      "authors": [
        "S Iqbal",
        "I Mahgoub",
        "E Du",
        "M Leavitt",
        "W Asghar"
      ],
      "year": 2021
    },
    {
      "title": "SMART on FHIR: a standards-based, interoperable apps platform for electronic health records",
      "authors": [
        "J Mandel",
        "D Kreda",
        "K Mandl",
        "I Kohane",
        "R Ramoni"
      ],
      "year": 2016
    },
    {
      "title": "Illuminating the dark spaces of healthcare with ambient intelligence",
      "authors": [
        "A Haque",
        "A Milstein",
        "L Fei-Fei"
      ],
      "year": 2020
    },
    {
      "title": "Human fall detection on embedded platform using depth maps and wireless accelerometer",
      "authors": [
        "B Kwolek",
        "M Kepski"
      ],
      "year": 2014
    },
    {
      "title": "Multimodal gait analysis based on wearable inertial and microphone sensors",
      "authors": [
        "C Wang"
      ],
      "year": 2017,
      "doi": "10.1109/uic-atc.2017.8397481"
    },
    {
      "title": "Computer vision-based descriptive analytics of seniors' daily activities for long-term health monitoring",
      "authors": [
        "Z Luo"
      ],
      "year": 2018
    },
    {
      "title": "Implementation of a multisite, interdisciplinary remote patient monitoring program for ambulatory management of patients with COVID-19",
      "authors": [
        "J Coffey"
      ],
      "year": 2021,
      "doi": "10.1038/s41746-021-00490-9"
    },
    {
      "title": "Applications of digital technology in COVID-19 pandemic planning and response",
      "authors": [
        "S Whitelaw",
        "M Mamas",
        "E Topol",
        "H Van Spall"
      ],
      "year": 2020,
      "doi": "10.1016/s2589-7500(20)30142-4"
    },
    {
      "title": "Nowcasting and forecasting the potential domestic and international spread of the 2019-nCoV outbreak originating in Wuhan, China: a modelling study",
      "authors": [
        "J Wu",
        "K Leung",
        "G Leung"
      ],
      "year": 2020,
      "doi": "10.1016/s0140-6736(20)30260-9"
    },
    {
      "title": "Response to COVID-19 in Taiwan: big data analytics, new technology, and proactive testing",
      "authors": [
        "Jason Wang",
        "C Ng",
        "C Brook"
      ],
      "year": 2020,
      "doi": "10.1001/jama.2020.3151"
    },
    {
      "title": "Harnessing wearable device data to improve state-level real-time surveillance of influenza-like illness in the USA: a population-based study",
      "authors": [
        "J Radin",
        "N Wineinger",
        "E Topol",
        "S Steinhubl"
      ],
      "year": 2020
    },
    {
      "title": "Wearable sensor data and self-reported symptoms for COVID-19 detection",
      "authors": [
        "G Quer"
      ],
      "year": 2020
    },
    {
      "title": "Leveraging artificial intelligence for pandemic preparedness and response: a scoping review to identify key use cases",
      "authors": [
        "A Syrowatka"
      ],
      "year": 2021,
      "doi": "10.1038/s41746-021-00459-8"
    },
    {
      "title": "A multimodal deep fusion graph framework to detect social distancing violations and FCGs in pandemic surveillance",
      "authors": [
        "E Varghese",
        "S Thampi"
      ],
      "year": 2021,
      "doi": "10.1016/j.engappai.2021.104305"
    },
    {
      "title": "The digital twin revolution",
      "authors": [
        "O San"
      ],
      "year": 2021,
      "doi": "10.1038/s43588-021-00077-0"
    },
    {
      "title": "Digital twins to personalize medicine",
      "authors": [
        "B Bj\u00f6rnsson"
      ],
      "year": 2019,
      "doi": "10.1186/s13073-019-0701-3"
    },
    {
      "title": "Digital twins: from personalised medicine to precision public health",
      "authors": [
        "M Kamel Boulos",
        "P Zhang"
      ],
      "year": 2021
    },
    {
      "title": "Digital twins for predictive oncology will be a paradigm shift for precision cancer care",
      "authors": [
        "T Hernandez-Boussard"
      ],
      "year": 2021
    },
    {
      "title": "The health digital twin: advancing precision cardiovascular medicine",
      "authors": [
        "G Coorey",
        "G Figtree",
        "D Fletcher",
        "J Redfern"
      ],
      "year": 2021
    },
    {
      "title": "A modular computational framework for medical digital twins",
      "authors": [
        "J Masison"
      ],
      "year": 2021,
      "doi": "10.1073/pnas.2024287118"
    },
    {
      "title": "Machine learning for comprehensive forecasting of Alzheimer's disease progression",
      "authors": [
        "C Fisher",
        "A Smith",
        "J Walsh"
      ],
      "year": 2019
    },
    {
      "title": "Generating digital twins with multiple sclerosis using probabilistic neural networks",
      "authors": [
        "J Walsh"
      ],
      "year": 2020
    },
    {
      "title": "Development of CancerLinQ, a health information learning platform from multiple electronic health record systems to support improved quality of care",
      "authors": [
        "D Potter"
      ],
      "year": 2020
    },
    {
      "title": "Health-focused conversational agents in person-centered care: a review of apps",
      "authors": [
        "P Parmar",
        "J Ryu",
        "S Pandya",
        "J Sedoc",
        "S Agarwal"
      ],
      "year": 2022
    },
    {
      "title": "A virtual type 2 diabetes clinic using continuous glucose monitoring and endocrinology visits",
      "authors": [
        "R Dixon"
      ],
      "year": 2020
    },
    {
      "title": "Identifying acute exacerbations of chronic obstructive pulmonary disease using patient-reported symptoms and cough feature analysis",
      "authors": [
        "S Claxton"
      ],
      "year": 2021
    },
    {
      "title": "High-performance medicine: the convergence of human and artificial intelligence",
      "authors": [
        "E Topol"
      ],
      "year": 2019
    },
    {
      "title": "Nudge units to improve the delivery of health care",
      "authors": [
        "M Patel",
        "K Volpp",
        "D Asch"
      ],
      "year": 2018
    },
    {
      "title": "Recipes for building an open-domain Chatbot",
      "authors": [
        "S Roller"
      ],
      "year": 2021
    },
    {
      "title": "Machine learning and prediction in medicine -beyond the peak of inflated expectations",
      "authors": [
        "J Chen",
        "S Asch"
      ],
      "year": 2017
    },
    {
      "title": "The UK Biobank resource with deep phenotyping and genomic data",
      "authors": [
        "C Bycroft"
      ],
      "year": 2018
    },
    {
      "title": "Accuracy of electronic health record data for identifying stroke cases in large-scale epidemiological studies: a systematic review from the UK biobank stroke outcomes group",
      "authors": [
        "R Woodfield",
        "I Grant",
        "Biobank Uk",
        "Outcomes Stroke",
        "Group",
        "Biobank Uk",
        "Outcomes Follow-Up",
        "Group",
        "C Sudlow"
      ],
      "year": 2015
    },
    {
      "title": "Advancing human genetics research and drug discovery through exome sequencing of the UK Biobank",
      "authors": [
        "J Szustakowski"
      ],
      "year": 2021
    },
    {
      "title": "The sequences of 150,119 genomes in the UK Biobank",
      "authors": [
        "B Halldorsson"
      ],
      "year": 2022
    },
    {
      "title": "The UK Biobank imaging enhancement of 100,000 participants: rationale, data collection, management and future directions",
      "year": 2020
    },
    {
      "title": "China Kadoorie Biobank of 0.5 million people: survey methods, baseline characteristics and long-term follow-up",
      "authors": [
        "Z Chen"
      ],
      "year": 2011
    },
    {
      "title": "Overview of the BioBank Japan Project: study design and profile",
      "authors": [
        "A Nagai"
      ],
      "year": 2017
    },
    {
      "title": "Million Veteran Program: a mega-biobank to study genetic influences on health and disease",
      "authors": [
        "J Gaziano"
      ],
      "year": 2016
    },
    {
      "title": "Sequencing of 53,831 diverse genomes from the NHLBI TOPMed Program",
      "authors": [
        "D Taliun"
      ],
      "year": 2021
    },
    {
      "title": "The ' All of Us' Research Program",
      "year": 2019
    },
    {
      "title": "Diversity and inclusion for the All of Us research program: a scoping review",
      "authors": [
        "B Mapes"
      ],
      "year": 2020
    },
    {
      "title": "Geographic distribution of US cohorts used to train deep learning algorithms",
      "authors": [
        "A Kaushal",
        "R Altman",
        "C Langlotz"
      ],
      "year": 2020
    },
    {
      "title": "The Project Baseline Health Study: a step towards a broader mission to map human health",
      "authors": [
        "K Arges"
      ],
      "year": 2020,
      "doi": "10.1038/s41746-020-0290-y"
    },
    {
      "title": "American Gut: an open platform for citizen science microbiome research",
      "authors": [
        "D Mcdonald"
      ],
      "year": 2018
    },
    {
      "title": "MIMIC-III, a freely accessible critical care database",
      "authors": [
        "A Johnson"
      ],
      "year": 2016
    },
    {
      "title": "MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports",
      "authors": [
        "A Johnson"
      ],
      "year": 2019
    },
    {
      "title": "Dynamic survival prediction in intensive care units from heterogeneous time series without the need for variable selection or curation",
      "authors": [
        "J Deasy",
        "P Li\u00f2",
        "A Ercole"
      ],
      "year": 2020
    },
    {
      "title": "Benchmarking deep learning architectures for predicting readmission to the ICU and describing patients-at-risk",
      "authors": [
        "S Barbieri"
      ],
      "year": 2020,
      "doi": "10.1038/s41598-020-58053-z"
    },
    {
      "title": "Multimodal fusion with deep neural networks for leveraging CT imaging and electronic health record: a case-study in pulmonary embolism detection",
      "authors": [
        "S.-C Huang",
        "A Pareek",
        "R Zamanian",
        "I Banerjee",
        "M Lungren"
      ],
      "year": 2020
    },
    {
      "title": "Wearable device signals and home blood pressure data across age, sex, race, ethnicity, and clinical phenotypes in the Michigan Predictive Activity & Clinical Trajectories in Health (MIPACT) study: a prospective, communitybased observational study",
      "authors": [
        "J Golbus",
        "N Pescatore",
        "B Nallamothu",
        "N Shah",
        "S Kheterpal"
      ],
      "year": 2021
    },
    {
      "title": "North American Prodrome Longitudinal Study (NAPLS 2): overview and recruitment",
      "authors": [
        "J Addington"
      ],
      "year": 2012
    },
    {
      "title": "Towards a psychosis risk blood diagnostic for persons experiencing high-risk symptoms: preliminary results from the NAPLS project",
      "authors": [
        "D Perkins"
      ],
      "year": 2015,
      "doi": "10.1093/schbul/sbu099"
    },
    {
      "title": "Multimodal machine learning workflows for prediction of psychosis in patients with clinical high-risk syndromes and recent-onset depression",
      "authors": [
        "N Koutsouleris"
      ],
      "year": 2021
    },
    {
      "title": "Multimodal machine learning: a survey and taxonomy",
      "authors": [
        "T Baltrusaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": 2019,
      "doi": "10.1109/tpami.2018.2798607"
    },
    {
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford"
      ]
    },
    {
      "authors": [
        "M Meila",
        "T Zhang"
      ],
      "year": 2021
    },
    {
      "title": "Contrastive learning of medical visual representations from paired images and text",
      "authors": [
        "Y Zhang",
        "H Jiang",
        "Y Miura",
        "C Manning",
        "C Langlotz"
      ],
      "year": 2020
    },
    {
      "title": "Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports",
      "authors": [
        "H. -Y Zhou"
      ],
      "year": 2022,
      "doi": "10.1038/s42256-021-00425-9"
    },
    {
      "title": "VATT: transformers for multimodal self-supervised learning from raw video, audio and text",
      "authors": [
        "H Akbari"
      ]
    },
    {
      "title": "VLMo: unified vision-language pre-training with mixture-of-modality-experts",
      "authors": [
        "H Bao"
      ],
      "year": 2022
    },
    {
      "title": "Introducing Pathways: a next-generation AI architecture",
      "authors": [
        "J Dean"
      ],
      "year": 2021
    },
    {
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": 2017
    },
    {
      "title": "An image is worth 16x16 words: transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "year": 2021
    },
    {
      "title": "Object-semantics aligned pre-training for vision-language tasks",
      "authors": [
        "Li"
      ],
      "year": 2020,
      "doi": "10.48550/arXiv.2004.06165"
    },
    {
      "title": "data2vec: a general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski"
      ]
    },
    {
      "title": "DABS: a Domain-Agnostic Benchmark for Self-Supervised Learning",
      "authors": [
        "A Tamkin"
      ],
      "year": 2021
    },
    {
      "title": "Perceiver: general perception with iterative attention",
      "authors": [
        "A Jaegle"
      ]
    },
    {
      "authors": [
        "M Meila",
        "T Zhang"
      ],
      "year": 2021
    },
    {
      "title": "Decoupling the role of data, attention, and losses in multimodal transformers",
      "authors": [
        "A Jaegle"
      ],
      "year": 2021
    },
    {
      "title": "Pretrained transformers as universal computation engines",
      "authors": [
        "K Lu",
        "A Grover",
        "P Abbeel",
        "I Mordatch"
      ],
      "year": 2021
    },
    {
      "title": "Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks",
      "authors": [
        "V Sandfort",
        "K Yan",
        "P Pickhardt",
        "R Summers"
      ],
      "year": 2019
    },
    {
      "title": "Advancing COVID-19 diagnosis with privacy-preserving collaboration in artificial intelligence",
      "authors": [
        "X Bai"
      ],
      "year": 2021
    },
    {
      "title": "Digital medicine and the curse of dimensionality",
      "authors": [
        "V Berisha"
      ],
      "year": 2021
    },
    {
      "title": "Retrieval augmented language model pre-training",
      "authors": [
        "K Guu",
        "K Lee",
        "Z Tung",
        "P Pasupat",
        "M Chang"
      ]
    },
    {
      "authors": [
        "H Iii",
        "Singh"
      ],
      "year": 2020
    },
    {
      "title": "Improving language models by retrieving from trillions of tokens",
      "authors": [
        "S Borgeaud"
      ]
    },
    {
      "authors": [
        "K Chaudhuri"
      ],
      "year": 2022
    },
    {
      "title": "Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines",
      "authors": [
        "S. -C Huang",
        "A Pareek",
        "S Seyyedi",
        "I Banerjee",
        "M Lungren"
      ],
      "year": 2020
    },
    {
      "title": "A comprehensive survey on multimodal medical signals fusion for smart healthcare systems",
      "authors": [
        "G Muhammad"
      ],
      "year": 2021
    },
    {
      "title": "ShortFuse: Biomedical time series representations in the presence of structured information",
      "authors": [
        "M Fiterau"
      ]
    },
    {
      "authors": [
        "F Doshi-Velez"
      ],
      "year": 2017
    },
    {
      "title": "A clinically applicable approach to continuous prediction of future acute kidney injury",
      "authors": [
        "N Toma\u0161ev"
      ],
      "year": 2019
    },
    {
      "title": "CheXaid: deep learning assistance for physician diagnosis of tuberculosis using chest X-rays in patients with HIV",
      "authors": [
        "P Rajpurkar"
      ],
      "year": 2020
    },
    {
      "title": "Policy-driven, multimodal deep learning for predicting visual fields from the optic disc and optical coherence tomography imaging",
      "authors": [
        "Y Kihara"
      ],
      "year": 2022,
      "doi": "10.1016/j.ophtha.2022.02.017"
    },
    {
      "title": "Zero-shot text-to-image generation",
      "authors": [
        "A Ramesh"
      ]
    },
    {
      "authors": [
        "M Meila",
        "T Zhang"
      ],
      "year": 2021
    },
    {
      "title": "GLIDE: towards photorealistic image generation and editing with text-guided diffusion models",
      "authors": [
        "A Nichol"
      ],
      "year": 2022
    },
    {
      "title": "A generalist agent",
      "authors": [
        "S Reed"
      ],
      "year": 2022
    },
    {
      "title": "Align before fuse: vision and language representation learning with momentum distillation",
      "authors": [
        "J Li"
      ],
      "year": 2021
    },
    {
      "title": "Attention bottlenecks for multimodal fusion",
      "authors": [
        "A Nagrani"
      ]
    },
    {
      "title": "Deep learning evaluation of biomarkers from echocardiogram videos",
      "authors": [
        "J Hughes"
      ],
      "year": 2021
    },
    {
      "title": "Deep learning in cancer pathology: a new generation of clinical biomarkers",
      "authors": [
        "A Echle"
      ],
      "year": 2020
    },
    {
      "title": "Axes of a revolution: challenges and promises of big data in healthcare",
      "authors": [
        "S Shilo",
        "H Rossman",
        "E Segal"
      ],
      "year": 2020
    },
    {
      "title": "Observational Health Data Sciences and Informatics (OHDSI): opportunities for observational researchers",
      "authors": [
        "G Hripcsak"
      ],
      "year": 2015
    },
    {
      "title": "Accuracy of identifying incident stroke cases from linked health care data in UK Biobank",
      "authors": [
        "K Rannikm\u00e4e"
      ],
      "year": 2020
    },
    {
      "title": "Automating ischemic stroke subtype classification using machine learning and natural language processing",
      "authors": [
        "R Garg",
        "E Oh",
        "A Naidech",
        "K Kording",
        "S Prabhakaran"
      ],
      "year": 2019
    },
    {
      "title": "DSM-5 and RDoC: progress in psychiatry research?",
      "authors": [
        "B Casey"
      ],
      "year": 2013
    },
    {
      "title": "The missing diversity in human genetic studies",
      "authors": [
        "G Sirugo",
        "S Williams",
        "S Tishkoff"
      ],
      "year": 2019
    },
    {
      "title": "Ensuring that biomedical AI benefits diverse populations",
      "authors": [
        "J Zou",
        "L Schiebinger"
      ],
      "year": 2021
    },
    {
      "title": "Estimating the success of re-identifications in incomplete datasets using generative models",
      "authors": [
        "L Rocher",
        "J Hendrickx",
        "Y. -A De Montjoye"
      ],
      "year": 2019
    },
    {
      "title": "Assessing missing data assumptions in EHR-based studies: a complex and underappreciated task",
      "authors": [
        "S Haneuse",
        "D Arterburn",
        "M Daniels"
      ],
      "year": 2021
    },
    {
      "title": "Approaches to addressing missing values, measurement error, and confounding in epidemiologic studies",
      "authors": [
        "M Van Smeden",
        "B Penning De Vries",
        "L Nab",
        "R Groenwold"
      ],
      "year": 2015
    },
    {
      "title": "A reference panel of 64,976 haplotypes for genotype imputation",
      "authors": [
        "S Mccarthy"
      ],
      "year": 2016
    },
    {
      "title": "Imputation of missing values for electronic health record laboratory data",
      "authors": [
        "J Li"
      ],
      "year": 2021
    },
    {
      "title": "Democratizing EHR analyses with FIDDLE: a flexible data-driven preprocessing pipeline for structured clinical data",
      "authors": [
        "S Tang"
      ],
      "year": 2020
    },
    {
      "title": "Recurrent neural networks for multivariate time series with missing values",
      "authors": [
        "Z Che"
      ],
      "year": 2018
    },
    {
      "title": "Mitigating bias in machine learning for medicine",
      "authors": [
        "K Vokinger",
        "S Feuerriegel",
        "A Kesselheim"
      ],
      "year": 2021
    },
    {
      "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
      "authors": [
        "Z Obermeyer",
        "B Powers",
        "C Vogeli",
        "S Mullainathan"
      ],
      "year": 2019
    },
    {
      "title": "AI recognition of patient race in medical imaging: a modelling study",
      "authors": [
        "J Gichoya"
      ],
      "year": 2022
    },
    {
      "title": "The UK Biobank and selection bias",
      "authors": [
        "J Swanson"
      ],
      "year": 2012
    },
    {
      "title": "Collider bias undermines our understanding of COVID-19 disease risk and severity",
      "authors": [
        "G Griffith"
      ],
      "year": 2020
    },
    {
      "title": "The influence of selection bias on identifying an association between allergy medication use and SARS-CoV-2 infection",
      "authors": [
        "L Thompson"
      ],
      "year": 2021
    },
    {
      "title": "Comparison of sociodemographic and health-related characteristics of UK biobank participants with those of the general population",
      "authors": [
        "A Fry"
      ],
      "year": 2017
    },
    {
      "title": "UK Biobank, big data, and the consequences of non-representativeness",
      "authors": [
        "K Keyes",
        "D Westreich"
      ],
      "year": 2019
    },
    {
      "title": "Robust de-anonymization of large sparse datasets",
      "authors": [
        "A Narayanan",
        "V Shmatikov"
      ],
      "year": 2008
    },
    {
      "title": "Ethical and legal challenges of artificial intelligence-driven healthcare",
      "authors": [
        "S Gerke",
        "T Minssen",
        "G Cohen"
      ],
      "year": 2020
    },
    {
      "title": "Secure, privacy-preserving and federated machine learning in medical imaging",
      "authors": [
        "G Kaissis",
        "M Makowski",
        "D R\u00fcckert",
        "R Braren"
      ],
      "year": 2020
    },
    {
      "title": "The future of digital health with federated learning",
      "authors": [
        "N Rieke"
      ],
      "year": 2020
    },
    {
      "title": "Medical imaging deep learning with differential privacy",
      "authors": [
        "A Ziller"
      ],
      "year": 2021
    },
    {
      "title": "Federated learning for predicting clinical outcomes in patients with COVID-19",
      "authors": [
        "I Dayan"
      ],
      "year": 2021
    },
    {
      "title": "Homomorphic encryption for machine learning in medicine and bioinformatics",
      "authors": [
        "A Wood",
        "K Najarian",
        "D Kahrobaei"
      ],
      "year": 2020
    },
    {
      "title": "Swarm learning for decentralized and confidential clinical machine learning",
      "authors": [
        "S Warnat-Herresthal"
      ],
      "year": 2021
    },
    {
      "title": "Edge intelligence: paving the last mile of artificial intelligence with edge computing",
      "authors": [
        "Z Zhou"
      ],
      "year": 2019
    },
    {
      "title": "Intel. How edge computing is driving advancements in healthcare analytics",
      "year": 2022
    },
    {
      "title": "How should we think about clinical data ownership?",
      "authors": [
        "A Ballantyne"
      ],
      "year": 2020
    },
    {
      "title": "Patient data ownership: who owns your health?",
      "authors": [
        "K Liddell",
        "D Simon",
        "A Lucassen"
      ],
      "year": 2021
    },
    {
      "title": "Data authorship as an incentive to data sharing",
      "authors": [
        "B Bierer",
        "M Crosas",
        "H Pierce"
      ],
      "year": 2017
    },
    {
      "title": "Revolutionizing medical data sharing using advanced privacy-enhancing technologies: technical, legal, and ethical synthesis",
      "authors": [
        "J Scheibner"
      ],
      "year": 2021
    },
    {
      "title": "Applications of machine learning in drug discovery and development",
      "authors": [
        "J Vamathevan"
      ],
      "year": 2019
    }
  ],
  "num_references": 178,
  "original_doi": "https://doi.org/10.13039/100000002"
}
