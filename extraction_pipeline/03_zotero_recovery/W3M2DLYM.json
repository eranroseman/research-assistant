{
  "paper_id": "W3M2DLYM",
  "title": "Ethical and regulatory challenges of large language models in medicine",
  "abstract": "With the rapid growth of interest in and use of large language models (LLMs) across various industries, we are facing some crucial and profound ethical concerns, especially in the medical field. The unique technical architecture and purported emergent abilities of LLMs differentiate them substantially from other artificial intelligence (AI) models and natural language processing techniques used, necessitating a nuanced understanding of LLM ethics. In this Viewpoint, we highlight ethical concerns stemming from the perspectives of users, developers, and regulators, notably focusing on data privacy and rights of use, data provenance, intellectual property contamination, and broad applications and plasticity of LLMs. A comprehensive framework and mitigating strategies will be imperative for the responsible integration of LLMs into medical practice, ensuring alignment with ethical principles and safeguarding against potential societal risks.",
  "year": 2023,
  "date": "2023-03-29",
  "journal": "Nat Med",
  "publication": "Nat Med",
  "authors": [
    {
      "forename": "J",
      "surname": "Ong",
      "name": "J Ong"
    },
    {
      "forename": "Nan",
      "surname": "Liu",
      "name": "Nan Liu"
    },
    {
      "forename": "S",
      "surname": "Ting",
      "name": "S Ting"
    },
    {
      "forename": "S Y-H",
      "surname": "Chang",
      "name": "S Y-H Chang"
    },
    {
      "forename": "Finale",
      "surname": "Doshi-Velez",
      "name": "Finale Doshi-Velez"
    },
    {
      "forename": "Jasmine",
      "surname": "Chiat",
      "name": "Jasmine Chiat"
    },
    {
      "forename": "Ling",
      "surname": "Ong",
      "name": "Ling Ong"
    },
    {
      "forename": "Yin-Hsi",
      "surname": "Shelley",
      "name": "Yin-Hsi Shelley"
    },
    {
      "surname": "Chang",
      "name": "Chang"
    },
    {
      "forename": "Wasswa",
      "surname": "William",
      "name": "Wasswa William"
    },
    {
      "forename": "Atul",
      "surname": "Butte",
      "name": "Atul Butte"
    },
    {
      "forename": "Nigam",
      "surname": "Shah",
      "name": "Nigam Shah"
    },
    {
      "forename": "Lita",
      "surname": "Sui",
      "name": "Lita Sui"
    },
    {
      "forename": "Tjien",
      "surname": "Chew",
      "name": "Tjien Chew"
    },
    {
      "forename": "Wei",
      "surname": "Lu",
      "name": "Wei Lu"
    },
    {
      "forename": "Julian",
      "surname": "Savulescu",
      "name": "Julian Savulescu"
    },
    {
      "forename": "Daniel",
      "surname": "Shu",
      "name": "Daniel Shu"
    },
    {
      "forename": "Wei",
      "surname": "Ting",
      "name": "Wei Ting"
    },
    {
      "affiliation": "Division of Pharmacy , Singapore General Hospital , Singapore \n\t\t\t\t\t\t\t\t Division of Pharmacy \n\t\t\t\t\t\t\t\t Singapore General Hospital \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Singapore"
    },
    {
      "affiliation": "Duke-NUS Medical School Department of Pharmacy (L S T Chew MMedSc) , National University of Singapore , Singapore ; \n\t\t\t\t\t\t\t\t Department of Pharmacy (L S T Chew MMedSc) \n\t\t\t\t\t\t\t\t Duke-NUS Medical School \n\t\t\t\t\t\t\t\t National University of Singapore \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Singapore"
    },
    {
      "affiliation": "Department of Ophthalmology , Chang Gung Memorial Hospital , Linkou Medical Center , Taoyuan , Taiwan (S Y-H \n\t\t\t\t\t\t\t\t Department of Ophthalmology \n\t\t\t\t\t\t\t\t Linkou Medical Center \n\t\t\t\t\t\t\t\t Chang Gung Memorial Hospital \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Taoyuan \n\t\t\t\t\t\t\t\t\t Y-H \n\t\t\t\t\t\t\t\t\t Taiwan (S"
    },
    {
      "affiliation": "Chang MD) ; College of Medicine , Chang Gung University , Taoyuan , Taiwan \n\t\t\t\t\t\t\t\t Chang MD) \n\t\t\t\t\t\t\t\t College of Medicine \n\t\t\t\t\t\t\t\t Chang Gung University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Taoyuan \n\t\t\t\t\t\t\t\t\t Taiwan"
    },
    {
      "affiliation": "Department of Biomedical Sciences and Engineering , Mbarara University of Science and Technology , Mbarara , Uganda ( \n\t\t\t\t\t\t\t\t Department of Biomedical Sciences and Engineering \n\t\t\t\t\t\t\t\t Mbarara University of Science and Technology \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Mbarara \n\t\t\t\t\t\t\t\t\t Uganda"
    },
    {
      "affiliation": "W William PhD) ; Bakar Computational Health Sciences Institute , and Department of Pediatrics , University of California , San Francisco , San Francisco , CA , USA ( \n\t\t\t\t\t\t\t\t W William PhD) \n\t\t\t\t\t\t\t\t Department of Pediatrics \n\t\t\t\t\t\t\t\t Bakar Computational Health Sciences Institute \n\t\t\t\t\t\t\t\t University of California \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t San Francisco \n\t\t\t\t\t\t\t\t\t San Francisco \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "A  Prof A J Butte PhD) ; Center for Data-Driven Insights and Innovation , University of California Health , Oakland , CA , USA ( \n\t\t\t\t\t\t\t\t Prof A J Butte PhD) \n\t\t\t\t\t\t\t\t Center for Data-Driven Insights and Innovation \n\t\t\t\t\t\t\t\t University of California Health \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Oakland \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "J Butte) ; Stanford Health Care , Palo Alto , CA , USA ( \n\t\t\t\t\t\t\t\t J Butte) \n\t\t\t\t\t\t\t\t Stanford Health Care \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Palo Alto \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "Prof N H Shah PhD) ; Department of Medicine , and Clinical Excellence Research Center , School of Medicine , Stanford University , Stanford , CA , USA ( \n\t\t\t\t\t\t\t\t Department of Medicine \n\t\t\t\t\t\t\t\t and Clinical Excellence Research Center \n\t\t\t\t\t\t\t\t School of Medicine \n\t\t\t\t\t\t\t\t Prof N H Shah PhD) \n\t\t\t\t\t\t\t\t Stanford University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Stanford \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "Prof N H Shah) ; Singapore Health Services , Pharmacy and Therapeutics Council Office , Singapore (L S T Chew) ; Department of Pharmacy , National Cancer Centre Singapore , Singapore ( \n\t\t\t\t\t\t\t\t Pharmacy and Therapeutics Council Office \n\t\t\t\t\t\t\t\t Department of Pharmacy \n\t\t\t\t\t\t\t\t Prof N H Shah) \n\t\t\t\t\t\t\t\t Singapore Health Services \n\t\t\t\t\t\t\t\t Singapore (L S T Chew) \n\t\t\t\t\t\t\t\t National Cancer Centre Singapore \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Singapore"
    },
    {
      "affiliation": "L S T Chew) ; Harvard Paulson School of Engineering and Applied Sciences , Harvard University , Cambridge , MA , USA StatNLP Research Group , \n\t\t\t\t\t\t\t\t L S T Chew) \n\t\t\t\t\t\t\t\t Harvard Paulson School of Engineering and Applied Sciences \n\t\t\t\t\t\t\t\t StatNLP Research Group \n\t\t\t\t\t\t\t\t Harvard University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Cambridge \n\t\t\t\t\t\t\t\t\t MA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "Singapore University of Technology and Design , Singpore \n\t\t\t\t\t\t\t\t Singapore University of Technology and Design \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Singpore"
    },
    {
      "affiliation": "Viewpoint Children's Research Institute , Melbourne , VIC , Australia Centre for Biomedical Ethics , \n\t\t\t\t\t\t\t\t Centre for Biomedical Ethics \n\t\t\t\t\t\t\t\t Viewpoint Children's Research Institute \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Melbourne \n\t\t\t\t\t\t\t\t\t VIC \n\t\t\t\t\t\t\t\t\t Australia"
    },
    {
      "affiliation": "Yong Loo Lin School of Medicine , National University of Singapore , Singapore ( \n\t\t\t\t\t\t\t\t Yong Loo Lin School of Medicine \n\t\t\t\t\t\t\t\t National University of Singapore \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Singapore"
    },
    {
      "affiliation": "Prof J Savulescu) ; Oxford Uehiro Centre for Practical Ethics , Faculty of Philosophy , University of Oxford , Oxford , UK Artificial Intelligence and Digital Innovation , \n\t\t\t\t\t\t\t\t Prof J Savulescu) \n\t\t\t\t\t\t\t\t Oxford Uehiro Centre for Practical Ethics \n\t\t\t\t\t\t\t\t Faculty of Philosophy \n\t\t\t\t\t\t\t\t Artificial Intelligence and Digital Innovation \n\t\t\t\t\t\t\t\t University of Oxford \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Oxford \n\t\t\t\t\t\t\t\t\t UK"
    },
    {
      "affiliation": "Singapore Eye Research Institute , Singapore National Eye Center , Singapore Health Service , Singapore ( D S W Ting) ; \n\t\t\t\t\t\t\t\t D S W Ting) \n\t\t\t\t\t\t\t\t Singapore Eye Research Institute \n\t\t\t\t\t\t\t\t Singapore National Eye Center \n\t\t\t\t\t\t\t\t Singapore Health Service \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Singapore"
    },
    {
      "affiliation": "Byers Eye Institute , Stanford University , Palo Alto , CA , USA Artificial Intelligence and Digital Innovation , \n\t\t\t\t\t\t\t\t Artificial Intelligence and Digital Innovation \n\t\t\t\t\t\t\t\t Byers Eye Institute \n\t\t\t\t\t\t\t\t Stanford University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Palo Alto \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "Singapore Eye Research Institute , Singapore National Eye Center , Singapore Health Service , Singapore 168751 \n\t\t\t\t\t\t\t\t Singapore Eye Research Institute \n\t\t\t\t\t\t\t\t Singapore National Eye Center \n\t\t\t\t\t\t\t\t Singapore Health Service \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 168751 \n\t\t\t\t\t\t\t\t\t Singapore"
    }
  ],
  "doi": "",
  "sections": [
    {
      "title": "Introduction",
      "text": "In the wake of ChatGPT's public release, over a thousand prominent computer scientists and technology industry experts, including Elon Musk and Steve Wozniak, took the unprecedented step of signing a letter calling for an immediate 6-month pause on AI. They argued that the current trajectory of generative AI development had spiralled \"out-of-control\", posing \"profound risks to society\".  1  Despite objections, the medical fraternity continued the pursuit of scaling-up generative AI research and integration into medicine. Archetypal discussions on AI ethics in medicine revolves around poor model accuracy for users not represented in the training data, transparency of models and model-building, accountability for model output, potential model bias, and risk for privacy and confidentiality breaches.  2, 3 owever, these concerns fail to fully capture distinctive concerns posed by LLMs.\n\nIn this context, we find ourselves in a situation that mirrors the classic Collingridge dilemma: \"Attempting to control a technology is difficult\u2026because during its early stages, when it can be controlled, not enough can be known about its harmful social consequences to warrant controlling its development; but by the time these consequences are apparent, control has become costly and slow.\"  4  This dilemma can be viewed as a problem of pacing-although technology development advances rapidly, governance and regulation lags behind. To effectively regulate LLMs, grasping the fundamental ethical issues inherent in their design and use is crucial. 1 year after the release of ChatGPT, we now have a better understanding of the limitations and risks the technology poses. LLMs differ substantially from AI-based technologies that are already regulated, creating unique regulatory hurdles: (1) data privacy and rights of use associated with training on massive datasets sourced from the internet;  5, 6  (2) data provenance, intellectual property contamination, and the uncertainty about the data derivatives that could hamper the accuracy of output; and (3) the so-called plastic nature of LLMs that allows for dynamic learning and evolution of LLM applications based on user inputs and changing clinical contexts (table). The broad use of LLM-based models across different industries limit the utility of a single governing framework. Identification and audit of the societal risks posed by LLM-based models becomes challenging because the precise mechanisms of their tuning or modifications remain opaque.  7 In this Viewpoint, we discuss these important peculiarities to position LLMs in the large literature on the ethics of AI."
    },
    {
      "title": "Data privacy and data rights of use",
      "text": "The development and deployment of LLM models challenge the boundaries of data privacy regulations. When identifiable patient data are used during training, there is potential risk that these models inadvertently memorise and disclose sensitive information in the absence of proper security measures. The use of patient information for LLM pre-training without obtaining explicit informed consent contravenes rights-of-data policies.  8 In addition, data breach of sensitive patient information can occur after adversarial attacks,  7  and the re-identification of even anonymised medical data is now possible with few spatiotemporal datapoints.  9  greater effort is needed to enhance data privacy and security of LLM-based models. Clinical LLM models trained with patient information should undergo rigorous cross-examination before implementation 10 as a form of penetration test. Cybersecurity measures, such as the use of pseudonyms implementing differential privacy techniques, could be used to counteract the risks of malicious attacks and data poisoning through deliberate adversarial prompting. Preliminary studies have suggested that LLMs can be taught to shield or protect specific categories of personal information under simulated scenarios.  11 What is currently absent are benchmark approaches that effectively measure the balance between privacy and the utility of LLMs. This benchmarking would help in evaluating the models' ability to maintain confidentiality while still delivering valuable outputs in a controlled environment. In addition, such benchmarking and evaluation tools will need to take on a multimodal approach, given that a multimodal LLM capable of integrating different inputs of text, images, and audio is fast gaining traction in medical applications.  12 oving forward, data protection regulations and guidance will need to take on a more pragmatic approach to avoid placing a hard stop on LLM development and implementation. For example, the use of a tiered approach based on classification of training and input data: public, internal (eg, non-patient data, information, or intellectual property for which there are proprietary interest or contractual obligations), confidential (eg, deidentified or anonymised patient data), and restricted data (eg, identifiable patient data). Data security and information technology infrastructural requirements will differ between different data risk categories (eg, airgap environment for high-risk tiers). Patients should provide broad informed consent to share data, and should be proactively educated on rights of data, such as right to access, right to erasure, and right to limit data processing. However, in the event whereby the classification of training data is unclear because of an absence of transparency, regulators will need to weigh between the potential risks of data breach and the benefits that the LLM-based model can bring to the general population."
    },
    {
      "title": "Data provenance and intellectual property contamination",
      "text": "The provenance of data refers to the origins, custody, and ownership of the information used to train these models. When LLMs ingest massive amounts of data from various sources, some of this content might be used without proper licensing despite being protected by intellectual property laws. Additionally, users might inadvertently prompt these models with references to copyrighted or trademarked works, raising questions about the legality and ethics of generating outputs derived from copyrighted or patented inputs. Regulatory rules can be implemented to restrict LLM training on appropriately licensed datasets, but this poses risk to the development, refinement, and maturation of technical standards.\n\nWe encourage developers to maintain transparency when describing the training datasets used in developing LLM-based models when possible, including the source, quantity, and diversity of data.  13 One of the key factors contributing to the accuracy of LLMs is their large-scale pre-training on vast amounts of text data from diverse sources. However, the accuracy can also be influenced by biases present in the training data, the quality of the input data, and the inherent limitations of the model architecture. Unlike other AI models, common techniques used to mitigate AI model bias, such as data resampling, prejudice removal, or subgroup modelling, cannot be easily adopted for LLM-based models. There is an absence of robust quantification on the amplification effect of model bias when building fine-tuned models on general-purpose ones. Water marking techniques seek to address concerns over originality and ownership through embedding a mark into AI-generated content before its release,  14  albeit the robustness of such techniques has been challenged. Work is underway to evaluate feasibility of unlearning in LLMs  15  to enable models to be updated to comply with updated legislation and data protection standards. The techniques present as potential stop-gap measures. However, to comprehensively address this issue, we will most likely need a paradigm shift in current market structures, incentivisation, and reimbursement strategies for LLM-based models or LLM-incorporated medical devices. Segal and colleagues  16  proposed a decentralised or blockchainbased, token economy-based market for medical research and publishing. Purported benefits of this blockchain-based platform include data and workflow transparency, immutability of original work, the minimisation of fraud, and incentivisation of reviewers through token payments. Such endeavours need prospective research and review to evaluate the feasibility of scaling.\n\nDevelopers and LLM researchers Regulators and governance bodies Data privacy and data rights of use Have a greater focus on: the cross examination of LLM-based models for risk of data breach; penetration tests for adversarial attacks; the development of benchmarks to evaluate the privacy vs utility trade-off; and validation frameworks for multimodal LLM evaluation Use a pragmatic, tiered approach to regulation based on the sensitivity of data used in training and inputs into LLM; evaluate data security measures required in accordance with data risk category Data provenance and contamination of intellectual property Promote transparency in training datasets used, including source, quality, and quantity; conduct conceptualisation, testbed, and prospective reviews of new market structures A generative AI take on fair use doctrine Broad applications and plasticity of LLMs Develop benchmarking frameworks and risk-assessment methodologies (eg, quality improvement and failure modes and effects analysis): highlight high-risk areas of harm, including quantification of hallucinations, reproducibility of output, and bias; enforce prospective and continuous stewardship Create sandbox environments, taking on an iterative approach to the development of regulatory guidance on the basis of new knowledge\n\nUsers and consumers (ie, the clinicians and their patients) must be familiarised with the rights of data (ie, right of access, right to rectification, right to erasure, right to restrict processing, right to data portability, right to object, right to not be subject to decisions based solely on automated processing). LLM=large language model."
    },
    {
      "title": "Table: Ethical concerns relating to framework and mitigating strategies for responsible development and use of LLMs in medicine"
    },
    {
      "title": "Viewpoint",
      "text": "The fair use doctrine is a legal framework promoting freedom of expression through permitting unlicensed use of copyright-protected works under specific circumstances.  17 Regulators can apply principles of the fair use doctrine for generative AI-based models developed for medical uses (panel)."
    },
    {
      "title": "Broad applications and plasticity of LLMs",
      "text": "The potential applications of LLMs in medicine can be broad ranging and heterogeneous. LLMs facilitate research by summarising texts and extracting key points from published literature, enhance medical education through data synthesis and interactive learning, and improve clinical tasks by streamlining administrative efforts and supporting decision making.  2  The industry is exploring the performance of medical chatbots in assisting patient care, counselling, expressing empathy, and providing information about health recommendations. These broad and varied applications of LLMs in medicine mean that a single governing framework for their use is impractical. In addition, the plastic nature of LLMs allows for dynamic learning and continuous evolution based on user inputs and changing clinical contexts. Much like neuroplasticity of the human brain,  21  LLMs are capable of changing characteristics of its response to stimuli, such as different prompting strategies or different fine-tuning data inputs. Drawing parallels to human neuroplasticity, structural or functional changes to LLMs can be positive (eg, enhanced personalisation of response through active reinforcement learning) or negative (eg, through propagation of inherent bias and so-called AI hallucinations). The identification and audit of the societal risks posed by LLM-based models becomes challenging as the precise mechanisms of their tuning or modifications remain opaque-known as the black-box nature of LLMs.  22 here is an urgent need to develop robust frameworks for evaluating LLM-based models for medicine to mitigate the risks discussed in this Viewpoint. Such a framework can incorporate clear assessment methodologies before implementation, such as quality improvement  23  and failure modes and effects analysis,  24  to identify and mitigate potential risks and harms. The evaluation of LLM-based models for medicine in areas of high risk is of utmost importance: the propagation of bias or discrimination, the quantification and reduction of AI hallucinations, and the reproducibility of the model outputs. Tech niques such as retrieval-augmented generation can help in minimising harm and bolstering the self-consistency of responses by cross-referencing with reliable data sources.  25 Bias evaluation is another crucial aspect, whereby assessment checklists and benchmarking frameworks are applicable. In one study, published as a preprint, authors developed a generative AI assessment checklist specific to models developed for medical indications.  26 Continuous stewardship after deployment is essential to address any emergent biases or model drifts.  27 Regulatory bodies can take on a proactive role through the creation of sandbox environments to allow exploration, inte raction, and evaluation of LLM-based applications without compromising on security and can mitigate risks to patient safety.  28, 29"
    },
    {
      "title": "Conclusion",
      "text": "The rapid advancement of LLMs in the medical field has ushered in a new era of technological capabilities alongside complex ethical and regulatory considerations. Such challenges are unique to LLM-based models as opposed to conventional machine-learning or deep-learning-based models. Developers and regulatory bodies need to work in tandem to encompass the multifaceted nature of LLMs, ensuring data protection without stifling innovation. As we navigate these challenges, a balanced"
    },
    {
      "title": "Panel: Fair use doctrine principles",
      "text": "When evaluating fair use, the fair use doctrine calls for:\n\n(1) Purpose and character of use Describes the intended use of original material, whether for commercial or not-for-profit use. Highly transformative applications that repurpose the use of the material and cannot be substituted by the original work. Generalist LLMs, such as ChatGPT, are trained on highly diverse datasets,  18  much of which is probably for non-medical intents. LLMbased medical applications with clearly defined attributes might hence be considered as transformative solutions, work that is largely repurposed from its original material.\n\n(2) Nature of the original work The use of a creative or imaginative work, such as novels or movies, is less likely to support a claim of fair use than the use of factual work. Although creative industries such as art and music encourage imagination and originality, the practice of medicine thrives upon an evidence-based approach grounded on factual information that is more likely to be considered fair use."
    },
    {
      "title": "(3) Amount and substantiality of original material used",
      "text": "The black-box nature of LLMs (ie, the input and output is known to users, but the internal mechanisms remain unknown) render this evaluation highly challenging. Uncertainty over data derivatives and data provenance pose a barrier to accurate quantification of original material used. Foundation models pre-trained on clinical information  19  present with a clearer definition on data lineage."
    },
    {
      "title": "(4) Effect of use upon the potential market for, or value of, copyrighted work",
      "text": "The extent to which unlicensed use of the original work harms the existing or future market for copyrighted owners' original work. LLM-based applications that are developed for specific medical purposes might be regulated as medical devices.  20 iewpoint approach that fosters innovation while upholding ethical standards will be essential for the responsible integration of LLMs into medical practice."
    },
    {
      "title": "Contributors",
      "text": "DSWT formulated the direction of the article. JCLO and SY-HC led the literature search and manuscript writing. The manuscript was revised and finetuned by WW, AJB, NHS, LSTC, NL, FD-V, WL, JS, DSWT, and finalised by JCLO and SY-HC."
    },
    {
      "title": "Declaration of interests",
      "text": "DSWT holds patents on a deep-learning system for the detection of retinal diseases. AJB is a cofounder and consultant for Personalis and NuMedii; is a consultant to Mango Tree Corporation; has previously been a consultant for Samsung, 10x Genomics, Helix, Pathway Genomics, and Verinata (Illumina); has served on paid advisory panels or boards for Geisinger Health, Regenstrief Institute, Gerson Lehman Group, AlphaSights, Covance, Novartis, Genentech, and Merck, and Roche; is a shareholder in Personalis and NuMedii; is a minor shareholder in Apple, Meta (Facebook), Alphabet (Google), Microsoft, Amazon, Snap, 10x Genomics, Illumina, Regeneron, Sanofi, Pfizer, Royalty Pharma, Moderna, Sutro, Doximity, BioNtech, Invitae, Pacific Biosciences, Editas Medicine, Nuna Health, Assay Depot, and Vet24seven, and several other non-health related companies and mutual funds; has received honoraria and travel reimbursement for invited talks from Johnson and Johnson, Roche, Genentech, Pfizer, Merck, Lilly, Takeda, Varian, Mars, Siemens, Optum, Abbott, Celgene, AstraZeneca, AbbVie, Westat, and many academic institutions, medical-specific or disease-specific foundations and associations, and health systems; receives royalty payments through Stanford University for several patents and other disclosures licensed to NuMedii and Personalis; has done research funded by NIH, Peraton (as the prime on an NIH contract), Genentech, Johnson and Johnson, FDA, Robert Wood Johnson Foundation, Leon Lowenstein Foundation, Intervalien Foundation, Priscilla Chan and Mark Zuckerberg, and the Barbara and Gerson Bakar Foundation; and has previously done research funded by the March of Dimes, Juvenile Diabetes Research Foundation, California Governor's Office of Planning and Research, California Institute for Regenerative Medicine, L'Oreal, and Progenity. NL is a scientific advisor to TIIM SG. NHS is a cofounder of Prealize Health (a predictive analytics company) and Atropos Health (an on-demand evidence generation company);"
    },
    {
      "title": "Search strategy and selection criteria",
      "text": "We included original papers, reviews, narratives, correspondences, perspectives, and viewpoints identified through searches of PubMed and arXiv from Jan 1, 2020, to Aug 10, 2023, using the search terms (\"natural language processing\" OR \"generative adversarial network\" OR \"generative model\" OR \"generative artificial intelligence\" OR \"generative AI\" OR \"transformer model\" OR \"reinforcement learning\" OR \"large language model\" OR \"LLM\" OR \"foundation model\" OR \"recurrent neural network\" OR \"RNN\" OR \"bidirectional encoder representations from transformers\" OR \"generative pretrained transformer\" OR \"ChatGPT\" OR \"Chat Generative Pre-training Transformer\" OR \"LLaMA\" OR \"Large Language Model Meta AI\" OR \"Pathways Language Model\") AND (\"ethics\" OR \"bioethics\" OR \"medical ethics\" OR \"regulation\" OR \"regulatory\"). We excluded publications that did not mention or discuss ethical issues. 37 of 998 articles from the search on PubMed and 21 of 668 articles on arXiv were relevant to the topics and eligible for our study. Only papers published in English were reviewed.\n\nreceives funding from the Gordon and Betty Moore Foundation for developing virtual model deployments; and is a member of working groups of the Coalition for Health AI (CHAI), a consensus-building organisation providing guidelines for the responsible use of artificial intelligence in health care. JS, through his involvement with the Murdoch Children's Research Institute, receives funding from the Victorian State Government through the Operational Infrastructure Support (OIS) programme. JCLO is supported by grants from the National Medical Research Council Singapore (MOH-CIAINV21nov-001) and AI Singapore OTTIC (AISG2-TC-2022-006). All other authors declare no competing interests."
    }
  ],
  "references": [
    {
      "title": "Elon Musk and others call for pause on AI, citing profound risks to society",
      "authors": [
        "C Metz",
        "G Schmidt"
      ],
      "year": 2023
    },
    {
      "title": "Large language models in medicine",
      "authors": [
        "A Thirunavukarasu",
        "Dsj Ting",
        "K Elangovan",
        "L Gutierrez",
        "T Tan",
        "Dsw Ting"
      ],
      "year": 2023,
      "doi": "10.1038/s41591-023-02448-8"
    },
    {
      "title": "ChatGPT in society: emerging issues",
      "authors": [
        "M Farina",
        "A Lavazza"
      ],
      "year": 2023,
      "doi": "10.3389/frai.2023.1130913"
    },
    {
      "title": "Collingridge and the dilemma of control: towards responsible and accountable innovation",
      "authors": [
        "A Genus",
        "A Stirling"
      ],
      "year": 2018,
      "doi": "10.1016/j.respol.2017.09.012"
    },
    {
      "title": "The imperative for regulatory oversight of large language models (or generative AI) in healthcare",
      "authors": [
        "B Mesk\u00f3",
        "E Topol"
      ],
      "year": 2023,
      "doi": "10.1038/s41746-023-00873-0"
    },
    {
      "title": "The challenges for regulating medical use of ChatGPT and other large language models",
      "authors": [
        "T Minssen",
        "E Vayena",
        "I Cohen"
      ],
      "year": 2023,
      "doi": "10.1001/jama.2023.9651"
    },
    {
      "title": "Auditing large language models: a three-layered approach",
      "authors": [
        "J M\u00f6kander",
        "J Schuett",
        "H Kirk",
        "L Floridi"
      ],
      "year": 2023,
      "doi": "10.1007/s43681-023-00289-2"
    },
    {
      "title": "Rights of the individual",
      "year": 2023,
      "doi": "10.1201/9781138069848-14"
    },
    {
      "title": "A governance model for the application of AI in health care",
      "authors": [
        "S Reddy",
        "S Allan",
        "S Coghlan",
        "P Cooper"
      ],
      "year": 2020,
      "doi": "10.1093/jamia/ocz192"
    },
    {
      "title": "Artificial intelligence-based ethical hacking for health information systems: simulation study",
      "authors": [
        "Y He",
        "E Zamani",
        "I Yevseyeva",
        "C Luo"
      ],
      "year": 2023
    },
    {
      "title": "Can language models be instructed to protect personal information?",
      "authors": [
        "Y Chen",
        "E Mendes",
        "S Das",
        "W Xu",
        "A Ritter"
      ],
      "year": 2023,
      "doi": "10.48550/arXiv.2310.02224"
    },
    {
      "title": "The impact of multimodal large language models on health care's future",
      "authors": [
        "B Mesk\u00f3"
      ],
      "year": 2023,
      "doi": "10.2196/52865"
    },
    {
      "title": "Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist",
      "authors": [
        "B Norgeot",
        "G Quer",
        "B Beaulieu-Jones"
      ],
      "year": 2020,
      "doi": "10.1038/s41591-020-1041-y"
    },
    {
      "title": "A novel model watermarking for protecting generative adversarial network",
      "authors": [
        "T Qiao",
        "M Yuyan",
        "Z Ning"
      ],
      "year": 2023,
      "doi": "10.1016/j.cose.2023.103102"
    },
    {
      "title": "Machine unlearning for generative AI",
      "authors": [
        "Y Viswanath",
        "S Jamthe",
        "S Lokiah",
        "E Bianchini"
      ],
      "year": 2024,
      "doi": "10.69554/kzrs2422"
    },
    {
      "title": "A blockchain-based computerized network infrastructure for the transparent, immutable calculation and dissemination of quantitative, measurable parameters of academic and medical research publications",
      "authors": [
        "G Segal",
        "Y Martisiano",
        "A Markinzon",
        "A Mayer",
        "A Halperin",
        "E Zimlichman"
      ],
      "year": 2023,
      "doi": "10.1177/20552076231194851"
    },
    {
      "title": "Peer review of GPT-4 technical report and systems card",
      "authors": [
        "J Gallifant",
        "A Fiske",
        "Levites Strekalova"
      ],
      "year": 2024,
      "doi": "10.1371/journal.pdig.0000417"
    },
    {
      "title": "Foundation models for generalist medical artificial intelligence",
      "authors": [
        "M Moor",
        "O Banerjee",
        "Hossein Abad"
      ],
      "year": 2023,
      "doi": "10.1038/s41586-023-05881-4"
    },
    {
      "title": "Regulatory responses to medical machine learning",
      "authors": [
        "T Minssen",
        "S Gerke",
        "M Aboy",
        "N Price",
        "G Cohen"
      ],
      "year": 2020,
      "doi": "10.1093/jlb/lsaa002"
    },
    {
      "title": "Neuroplasticity",
      "authors": [
        "M Puderbaugh",
        "P Emmady"
      ],
      "year": 2023,
      "doi": "10.3998/mpub.11373292.cmp.2587"
    },
    {
      "title": "ChatGPT is a black box: how AI research can break it open",
      "authors": [
        "Nature"
      ],
      "year": 2023
    },
    {
      "title": "Clinical artificial intelligence quality improvement: towards continual monitoring and updating of AI algorithms in healthcare",
      "authors": [
        "J Feng",
        "R Phillips",
        "I Malenica"
      ],
      "year": 2022,
      "doi": "10.1038/s41746-022-00611-y"
    },
    {
      "title": "The medical algorithmic audit",
      "authors": [
        "X Liu",
        "B Glocker",
        "M Mccradden",
        "M Ghassemi",
        "A Denniston",
        "L Oakden-Rayner"
      ],
      "year": 2022,
      "doi": "10.1016/s2589-7500(22)00003-6"
    },
    {
      "title": "Development of a liver disease-specific large language model chat interface using retrieval augmented generation",
      "authors": [
        "J Ge",
        "S Sun",
        "J Owens"
      ],
      "year": 2024,
      "doi": "10.1097/hep.0000000000000834"
    },
    {
      "title": "Generative artificial intelligence in healthcare: ethical considerations and assessment checklist",
      "authors": [
        "Y Ning",
        "S Teixayavong",
        "Y Shang"
      ],
      "year": 2023,
      "doi": "10.48550/arXiv.2311.02107"
    },
    {
      "title": "The case for algorithmic stewardship for artificial intelligence and machine learning technologies",
      "authors": [
        "S Eaneff",
        "Z Obermeyer",
        "A Butte"
      ],
      "year": 2020,
      "doi": "10.1001/jama.2020.9371"
    },
    {
      "title": "Harvard designs AI sandbox that enables exploration, interaction without compromising security",
      "year": 2023,
      "doi": "10.1093/ww/9780199540884.013.u242658"
    },
    {
      "title": "First of its kind generative AI evaluation sandbox for trusted AI by AI Verify Foundation and IMDA",
      "year": 2023,
      "doi": "10.1007/978-1-4842-9367-6_6"
    }
  ],
  "num_references": 28,
  "original_doi": "https://doi.org/10.13039/501100001352"
}
