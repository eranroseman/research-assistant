{
  "paper_id": "JYDJGSXA",
  "title": "Research and Applications",
  "abstract": "Objectives: To evaluate the capability of using generative artificial intelligence (AI) in summarizing alert comments and to determine if the AIgenerated summary could be used to improve clinical decision support (CDS) alerts. \n Materials and Methods: We extracted user comments to alerts generated from September 1, 2022 to September 1, 2023 at Vanderbilt University Medical Center. For a subset of 8 alerts, comment summaries were generated independently by 2 physicians and then separately by GPT-4. We surveyed 5 CDS experts to rate the human-generated and AI-generated summaries on a scale from 1 (strongly disagree) to 5 (strongly agree) for the 4 metrics: clarity, completeness, accuracy, and usefulness. Results: Five CDS experts participated in the survey. A total of 16 human-generated summaries and 8 AI-generated summaries were assessed. Among the top 8 rated summaries, five were generated by GPT-4. AI-generated summaries demonstrated high levels of clarity, accuracy, and usefulness, similar to the human-generated summaries. Moreover, AI-generated summaries exhibited significantly higher completeness and usefulness compared to the human-generated summaries (AI: 3.4 \u00b1 1.2, human: 2.7 \u00b1 1.2, P \u00bc .001). \n Conclusion: End-user comments provide clinicians' immediate feedback to CDS alerts and can serve as a direct and valuable data resource for improving CDS delivery. Traditionally, these comments may not be considered in the CDS review process due to their unstructured nature, large volume, and the presence of redundant or irrelevant content. Our study demonstrates that GPT-4 is capable of distilling these comments into summaries characterized by high clarity, accuracy, and completeness. AI-generated summaries are equivalent and potentially better than human-generated summaries. These AI-generated summaries could provide CDS experts with a novel means of reviewing user comments to rapidly optimize CDS alerts both online and offline.",
  "year": 2019,
  "date": "2019",
  "journal": "ONC Data Br",
  "publication": "ONC Data Br",
  "authors": [
    {
      "forename": "Siru",
      "surname": "Liu",
      "name": "Siru Liu",
      "affiliation": "1  Department of Biomedical Informatics , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States",
      "email": "siru.liu@vumc.org"
    },
    {
      "forename": "Allison",
      "surname": "Mccoy",
      "name": "Allison Mccoy",
      "affiliation": "1  Department of Biomedical Informatics , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States",
      "orcid": "0000-0003-2292-9147"
    },
    {
      "forename": "Aileen",
      "surname": "Wright",
      "name": "Aileen Wright",
      "affiliation": "1  Department of Biomedical Informatics , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "Scott",
      "surname": "Nelson",
      "name": "Scott Nelson",
      "affiliation": "1  Department of Biomedical Informatics , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States",
      "orcid": "0000-0002-1941-1817"
    },
    {
      "forename": "Sean",
      "surname": "Huang",
      "name": "Sean Huang",
      "affiliation": "1  Department of Biomedical Informatics , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "Hasan",
      "surname": "Ahmad",
      "name": "Hasan Ahmad",
      "affiliation": "4  Department of Biomedical Informatics and Medical Education , University of Washington , Seattle , WA 98195 , United States , \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics and Medical Education \n\t\t\t\t\t\t\t\t University of Washington \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 98195 \n\t\t\t\t\t\t\t\t\t Seattle \n\t\t\t\t\t\t\t\t\t WA \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "Sabrina",
      "surname": "Carro",
      "name": "Sabrina Carro",
      "affiliation": "5  Department of Pediatrics , Vanderbilt University Medical Center , Nashville , TN 37212 , United States \n\t\t\t\t\t\t\t\t Department of Pediatrics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "Jacob",
      "surname": "Franklin",
      "name": "Jacob Franklin",
      "affiliation": "3  Department of Medicine , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Medicine \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "James",
      "surname": "Brogan",
      "name": "James Brogan",
      "affiliation": "3  Department of Medicine , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Medicine \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "Adam",
      "surname": "Wright",
      "name": "Adam Wright",
      "affiliation": "1  Department of Biomedical Informatics , Vanderbilt University Medical Center , Nashville , TN 37212 , United States , \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States",
      "orcid": "0000-0001-6844-145X"
    },
    {
      "affiliation": "Department of Biomedical Informatics , Vanderbilt University Medical Center , 2525 West End Ave #1475 , Nashville , TN 37212 , United States \n\t\t\t\t\t\t\t\t Department of Biomedical Informatics \n\t\t\t\t\t\t\t\t Vanderbilt University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 2525 West End Ave #1475 \n\t\t\t\t\t\t\t\t\t 37212 \n\t\t\t\t\t\t\t\t\t Nashville \n\t\t\t\t\t\t\t\t\t TN \n\t\t\t\t\t\t\t\t\t United States"
    }
  ],
  "doi": "",
  "keywords": [
    "clinical decision support",
    "alert fatigue",
    "health personnel",
    "large language model"
  ],
  "sections": [
    {
      "title": "Introduction",
      "text": "With the widespread adoption of the electronic health record (EHR), the use of clinical decision support (CDS) systems has also expanded.  1  CDS systems include a variety of tools such as alerts, reminders, order sets, and documentation templates that provide important insights and recommendations to healthcare providers as they care for patients.  2 CDS alerts can be categorized as actionable or nonactionable based on whether they provide clear, actionable next steps. For example, an alert about influenza vaccine eligibility that includes a button to order the vaccine is an actionable alert. On the other hand, an alert that simply displays an elevated pediatric early warning score is a nonactionable alert. Well-designed and successfully implemented CDS tools have been reported to play a key role in improving clinical practice and addressing racial and ethnic disparities in healthcare.  [3] [4] 4] [5]  The foundation of CDS tool design and implementation is the \"Five Rights\" principle, which includes \"the right information, to the right person, in the right intervention format, through the right channel, at the right time in workflow.\" 6 However, a prevalent challenge persists in the real world: approximately 90% of CDS alerts are not accepted, or acted upon, with some of them supported by valid reasons such as low relevance and inappropriate timing.  7, 8 These common problems occur when CDS is not working as intended and lead to \"alert fatigue,\" which can seriously jeopardize patient safety.  9 fforts to address this challenge include 2 main categories: (1) the use of manual review and (2) data science-related tools to optimize alerts. For example, Vanderbilt University Medical Center (VUMC) created the \"Clickbusters\" program, which reduced unnecessary alert triggers by 15% through a detailed 10-step review in collaboration with relevant healthcare providers and CDS experts.  10 While manual review ensures each insight to be clinically appropriate, it is labor-intensive and may fall short in offering immediate or comprehensive feedback. Conversely, automated tools supported by data science could be used as supplementary support in the manual review process to provide a more sustainable and scalable solution for maintaining the effectiveness of CDS.  11, 12 Notably, current methods to enhance CDS do not typically take into account the comments or other information provided by healthcare providers when alerts are overridden. This information is commonly unorganized free-text and may contain irrelevant information beyond the override reasons. An example of 1 alert and the corresponding user comments are shown in Figure  1 . This alert was designed to notify clinicians to order an influenza vaccine. The user chose to override this alert and left a comment \"defer to primary team.\" Other examples of user comments for this alert included \"patient declined\" and \"out of season.\" Earlier researchers have identified that some freetext comments could be used to determine the reason behind why certain alerts do not work as designed. For example, an alert that suggests an emergency room patient to take insulin is often overridden with a comment such as \"patient is already on insulin,\" which upon further investigation, researchers found to be a logical error that caused the alert to miss certain previous insulin administrations.  13 In another study, researchers developed a Na\u00efve Bayes model to classify important override comments and used them to identify malfunctions in 26% of rules in CDS alerts.  14 However, classification model development requires manual labeling followed by manual review of a sizeable number of comments which are classified into categories. The classification performance of the Na\u00efve Bayes model (Area under the ROC Curve [AUC] \u00bc 0.738) also might lead to important comments being incorrectly categorized. In addition to the 2 main categories above, other efforts to improve CDS include investigating the completeness of the data used in the alert logic, performing systematic maintenance of the alert logic, and clarifying the clinical significance of the alert to determine the threshold for interrupting the clinical workflow.  15  significant number of free-text comments generated by healthcare providers remain archived in databases, awaiting efficient summarization for timely integration into CDS optimization systems. To illustrate, 85 886 comments were generated at VUMC within a year, with an average of 239 comments per alert. An intuitive solution would be to automatically summarize comments for each alert, thus enabling CDS experts to use these summaries to improve alerts. Advanced generative artificial intelligence (AI) tools such as GPT-4, the instruction-tuned large language models, have shown excellent performance in summarization tasks.  16 A recent study demonstrated that GPT-4 strongly aligns with humans in the summarization tasks.  17 One study focused on news summarization and found that the quality of GPT-3 generated summarization was on par with human-written summarizations.  18 These generative AI tools have also presented great possibilities in clinical tasks, such as summarizing radiology reports and leading expert panel discussion in pediatric palliative care.  [19] [20] ] [21]  The objectives of this study were: (1) to evaluate the capability of using generative AI in summarizing alert comments and (2) to determine if the AI-generated summary could be used to improve CDS alerts."
    },
    {
      "title": "Methods"
    },
    {
      "title": "Setting and data collection",
      "text": "We conducted this study at VUMC. This research was reviewed by the Vanderbilt University Institutional Review Board and found to be exempt. We extracted user responses and comments for alerts (BestPractice Advisories [BPAs]) generated from September 1, 2022 to September 1, 2023 from clarity, an epic (Epic Systems, Verona, WI) data warehouse. We grouped alerts into actionable alerts and nonactionable alerts based on whether they provided clear actionable next steps (eg, providing a clickable button to order an influenza vaccine). Notably, if alerts only had links without specific next steps, we grouped them as nonactionable alerts. Within each group, we chose alerts with large numbers of comments, overrides, or a high override rate. We excluded any alerts that had been retired or had fewer than 20 comments. The comment preprocessing involved the following steps: (1) removal of comments with only a single character, (2) exclusion of empty comments, and (3) filtering out comments with the terms \"na,\" \"n/a,\" \"ok,\" or indications of passwords. For alerts with over 500 comments, we made a random selection of 500. If the number of comments was below this threshold, all were retained. In total, we selected 8 alerts (3919 comments) for analysis (Table  1 )."
    },
    {
      "title": "Human-generated summaries",
      "text": "Two physicians (1 internal medicine physician and 1 psychiatrist) reviewed a total of 3919 comments for 8 alerts independently to generate summaries. Both physicians also had a strong background in CDS tools. Prior to generating the summaries, we had a meeting with them to introduce the research project. They knew that their summaries would be used to compare with those generated by AI. For each selected alert, they reviewed the alert logic, the description of alerts, and the screenshots to understand the alert. Then, they reviewed relevant comments and generated their own summaries for these comments."
    },
    {
      "title": "AI-generated summaries",
      "text": "For the AI-generated summaries, we used Microsoft Azure's hosted version of OpenAI GPT-4 large language model. The model was deployed in a protected environment at VUMC, which is approved for usage with protected health information. We used the prompt: \"Your task is to analyze comments from clinicians who received a CDS alert but chose not to accept it. The comments are enclosed in double quotes and separated by a line break. Your objective is to categorize and comprehensively summarize the key reasons for not accepting the alert, based on the comments provided by the clinicians. For each summarized reason, list 3-5 representative comments from the document. Please number the summarized reasons and list the comments under each reason.\\n ====\u00bcComments: [COMMENT].\" Each comment was separated by a new line character and used double quotes. The prompt was designed through an iterative process. To improve the effectiveness of prompt, we described the task and the objective to provide contextual information and clarified the format of the inputs and outputs.  22 Additionally, we asked for several examples under each generated reasons in the summary to ensure that the prompt captured a wide range of comments, thus avoiding premature conclusions based on a limited dataset."
    },
    {
      "title": "Expert review of summarizations",
      "text": "We performed a questionnaire survey to rate the humangenerated summarizations and AI-generated summarizations. Five CDS experts (4 physicians and 1 pharmacist) participated in the survey. The experts evaluated the quality of  Order VTE prophylaxis panel; order permanent contraindication to VTE prophylaxis for rest of encounter Naloxone co-prescribing\n\nThis patient is at risk for unintentional opioid overdose and a naloxone prescription is required to be offered by TN law.\n\nOrder naloxone (Narcan) 4 mg/actuation nasal spray summaries written for override comments of CDS alerts following these steps:\n\n1) Carefully read the alert information and relevant comments. 2) Read the proposed summaries A-C (2 human-generated and 1 AI-generated; presented in random order without indication of the source [ie, blinded]). 3) Rate each summary on a scale from 1 (strongly disagree) to 5 (strongly agree) for the 4 metrics: a) Clarity: The summary is logical and easy to understand. b) Completeness: There are no important topics in the comments that the summary misses. c) Accuracy: There are no points in the summary that are not actually found in the comments. d) Usefulness: The summary would be helpful to me as I tried to improve the BPA.\n\n4) Add a comment regarding any hallucinations found in the summary."
    },
    {
      "title": "Evaluation",
      "text": "For each metric, we reported the mean and standard deviation.\n\nWe calculated the overall score by averaging the score across the 4 metrics. We conducted a Mann-Whitney-Wilcoxon test, which is nonparametric, to evaluate the difference between the ratings of AI-generated and human-generated summaries. We considered P < .01 as the threshold for statistical significance. Moreover, we measured the intraclass correlation coefficient (ICC) to assess the consistency among raters. An ICC below 0.5 indicates low reliability, between 0.5 and 0.74 indicates moderate reliability, from 0.75 to 0.9 suggests good reliability, and above 0.9 signifies excellent reliability.  23 We used Python 3.8 for statistical analysis. Descriptive statistics were also reported to characterize the survey participants, including their clinical specialties, roles, and years of CDS experience."
    },
    {
      "title": "Results",
      "text": "This survey involved 5 CDS experts with backgrounds in internal medicine, pharmacy, geriatric medicine, and pediatrics. On average, they had 6 years of clinical experience with the EHR (since graduation from medical school). They were currently in practice and received CDS alerts on a daily basis. Details about the survey participants can be found in Table  S1 . The survey demonstrated good reliability, as indicated by an ICC value of 0.75."
    },
    {
      "title": "Examples of AI-generated summaries and humangenerated summaries",
      "text": "Among the top 8 rated summaries (based on the overall score), five were generated by GPT-4 and three were generated by humans. Table  2  lists these generated summaries and their ratings for clarity, completeness, accuracy, and usefulness. The remaining generated summaries were listed in Table  S2 ."
    },
    {
      "title": "Results of expert review of AI-generated suggestions and human-generated suggestions",
      "text": "Participants evaluated 8 AI-generated and 16 humangenerated summaries corresponding to 8 alerts. Every AI-generated summary scored at least 3 in the overall metric, with the highest being 4.6 \u00b1 0.3. The AI-generated summaries typically received agreement for clarity and usefulness, while completeness received a neutral response, neither agree nor disagree. Accuracy, however, tended toward strong agreement. Figure  2A  displays a stacked bar chart of the AIgenerated summaries' scores across each metric, while Figure  2B  presents the same for human-generated summaries. Most of the AI-generated summaries, 87.5% (7 out of 8) and 75% (12 out of 16) of human-generated summaries scored 4 or above for clarity. For completeness, 75% of AI-generated and 37.5% of human-generated summaries scored 3 or higher. All summaries from both AI and humans achieved a score of 4 or above for accuracy. This study did not detect any instances of AI hallucination. Regarding usefulness, 87.5% of AI-generated and 93.75% of human-generated suggestions were rated 3 or higher. AI-generated summaries were rated significantly higher than human-generated summaries in terms of completeness (AI: 3.4 \u00b1 1.2, human: 2.7 \u00b1 1.2, P \u00bc .001). Other metrics were rated similarly. AI-generated summaries achieved a score of 4.2 \u00b1 1.1 for clarity, 4.5 \u00b1 0.7 for accuracy, 4 \u00b1 1.1 for usefulness, and an overall score of 4 \u00b1 0.8. Meanwhile, human-generated summaries were rated with a clarity score of 4.1 \u00b1 1.1, accuracy of 4.5 \u00b1 0.7, usefulness of 3.9 \u00b1 0.8, and an overall score of 3.8 \u00b1 0.6. No significant differences were found in the human-generated summaries (Table  3 )."
    },
    {
      "title": "Discussion",
      "text": "In this study, we explored the feasibility of using GPT-4 to summarize user override comments on CDS alerts, comparing its performance with human-generated summaries. AIgenerated summaries demonstrated high levels of clarity, accuracy, and usefulness, similar to the human-generated summaries. Moreover, AI-generated summaries exhibited significantly higher completeness and usefulness compared to the human-generated summaries.\n\nThe quality of human-generated summaries might be affected by several factors. One example is the anchoring effect, where only certain words are considered in generating summaries. For instance, in the case of the VTE prophylaxis alert, several comments like \"patient is ambulatory\" and \"patient up and walking\" were adeptly identified and classified by GPT-4 as \"ambulatory or physically active.\" On the other hand, 1 human-generated summary did not include this point, while another confused \"ambulatory\" as referring to a patient location \"patient location (ambulatory).\" Ambulatory in the comments was referring to patients being ambulatory (ie, walking around) and not needing VTE prophylaxis, not to their being in an outpatient setting (the alert only fired in the inpatient setting). Reviewing a long list of comments is tedious, and humans may be more likely to accidentally omit a category due to the sheer number of comments listed.\n\nCreating fewer, broad, vague categories might be logical/ correct but less helpful when trying to improve the BPA than having more, detailed categories in the summary. For example, a part of an AI-generated summary \"specific medical circumstances\" may be accurate yet too ambiguous to be actionable. In contrast, a human-generated summary like \"the time check of PTT recommended by the BPA was not accurate\" pinpoints a specific issue needing improvements. The prompt could be engineered to deliver more specific, actionable categories in the AI-generated summary. that \"it would be more helpful if the model gave the category and then some examples.\" AI-generated summaries could help CDS experts to improve alerts both offline and online (Figure  3 ). GPT-4 allows for the summarization of millions of user comments, offering the possibility to integrate user comments into a CDS alert review process, such as Clickbusters. In addition, summarizing user comments using GPT-4 provides a way to automatically monitor user comments and identify new themes to send to stakeholders. It should be emphasized that the integration of AI-generated summaries of user reviews is a valuable component of CDS alert optimization, not a replacement for the entire optimization process. While user feedback is an important source of immediate insight, it may not capture all of the factors that influence user acceptance of alerts.\n\nIn addition, the process of using user comments to refine CDS alerts could serve as an incentive for increased user engagement. The expectation that their comments will be promptly and effectively used to refine CDS alerts may encourage more substantive comments. This potential for increased engagement may extend the applicability of our findings to a broader range of institutions, fostering a more interactive and responsive environment between CDS experts and healthcare providers. Future studies include: (1) a comprehensive analysis of using AI-generated summaries of all user comments to improve CDS alerts, and (2) a comparative analysis of the potential impact on user behavior and patient outcomes.\n\nWhen discussing how GPT-4 can be employed to summarize CDS alerts, the logistics of its implementation need to be taken into consideration. At VUMC, between September Table  3 . Means and SD for survey questions rated on a 5-point Likert scale, with 1 indicating \"strongly disagree\" and 5 indicating \"strongly agree.\""
    },
    {
      "title": "AI-generated summaries mean (SD)"
    },
    {
      "title": "Human-generated summaries mean (SD) P",
      "text": "Clarity: The summary is logical and easy to understand. 4.2 (1.1) 4.1 (1.1) .176 Completeness: There are no important topics in the comments that the summary misses."
    },
    {
      "title": "(1.2) 2.7 (1.2) .001",
      "text": "Accuracy: There are no points in the summary that are not actually found in the comments.\n\n4.5 (0.7) 4.5 (0.7) .499\n\nUsefulness: The summary would be helpful to me as I tried to improve the alert.\n\n4.0 (1.1) 3.9 (0.8) .011 2022 and September 2023, we identified 195 alerts with a minimum of 20 comments, averaging around 434 comments. Given an average comment token size of 6.02 tokens and an average response length of 487 tokens in our evaluation dataset, using GPT-4 Turbo as of November 21, 2023, which has a token limit of 128k, would require generating 196 prompts with approximately 530 722 input tokens and around 95 452 output tokens to process all user comments. At the current rate of $0.01 per 1000 tokens for input and $0.03 per 1000 tokens for output, the total cost would amount to $8.17, which is quite inexpensive when compared with the possibility of asking expert clinicians to do the same review.\n\nUsing GPT-3.5 Turbo, the total cost would be even lower at $0.73."
    },
    {
      "title": "Limitations",
      "text": "There are several limitations to our study. First, we evaluated the generated summaries from CDS experts' perspectives.\n\nThe impact of generated summaries from user comments on the actual change of CDS alerts or relevant patient outcomes was not evaluated. Second, as a preliminary study, we randomly selected 500 comments for each alert. If we had used all comments, the AI-generated summaries and the humangenerated summaries may have differed. Third, in this study, 2 physicians generated summaries. The quality of humangenerated summaries might be improved with the involvement of a larger number of physicians in the summarization process. Fourth, this study was conducted at an institution with a large number of user comments for CDS alert. The generalizability of this study to other institutions remains unknown, especially in smaller medical centers."
    },
    {
      "title": "Conclusion",
      "text": "End-user comments provide clinicians' immediate feedback to CDS alerts and can serve as valuable data resource for optimizing CDS interventions. Traditionally, these comments may not be considered in the CDS review process due to their unstructured nature, large volume, and the presence of redundant or irrelevant content. Our study demonstrates that GPT-4 is capable of distilling these comments into summaries characterized by high clarity, accuracy, and completeness. AIgenerated summaries are equivalent and potentially better than human-generated summaries. These AI-generated summaries could provide CDS experts with a novel means of reviewing user comments to rapidly optimize CDS alerts both online and offline."
    },
    {
      "text": "Figure 1. An example of a CDS alert and relevant user comments. CDS \u00bc clinical decision support."
    },
    {
      "text": "Review home meds; take action on home medsVTE prophylaxisPatient may require VTE prophylaxis-open the panel below for VTE prophylaxis options or select an exclusion reason."
    },
    {
      "text": "Figure 3. Using AI-generated summary of user comments to improve CDS alerts offline and online. AI \u00bc artificial intelligence, CDS \u00bc clinical decision support."
    },
    {
      "text": "Selected alerts, descriptions and offered actions."
    },
    {
      "text": "Top 8 summaries and their ratings for clarity (C1), completeness (C2), accuracy (A), usefulness (U), and overall (O)."
    }
  ],
  "references": [
    {
      "title": "Hospitals' use of electronic health records data, 2015-2017",
      "authors": [
        "S Parasrampuria",
        "J Henry"
      ],
      "year": 2019,
      "doi": "10.1201/9781315153100-4"
    },
    {
      "title": "Development and evaluation of a comprehensive clinical decision support taxonomy: comparison of front-end tools in commercial and internally developed electronic health record systems",
      "authors": [
        "A Wright",
        "D Sittig",
        "J Ash"
      ],
      "year": 2011,
      "doi": "10.1136/amiajnl-2011-000113"
    },
    {
      "title": "Rapid review: identification of digital health interventions in atherosclerotic-related cardiovascular disease populations to address racial, ethnic, and socioeconomic health disparities",
      "authors": [
        "Thomas Craig",
        "K Fusco",
        "N Lindsley"
      ],
      "year": 2020,
      "doi": "10.1016/j.cvdhj.2020.11.001"
    },
    {
      "title": "Best practices in clinical decision support: the case of preventive care reminders",
      "authors": [
        "A Wright",
        "S Phansalkar",
        "M Bloomrosen"
      ],
      "year": 2010,
      "doi": "10.4338/ACI-2010-05-RA-0031"
    },
    {
      "title": "The impact of clinical decision support on health disparities and the digital divide",
      "authors": [
        "B Douthit",
        "A Mccoy",
        "S Nelson"
      ],
      "year": 2023,
      "doi": "10.1055/s-0043-1768722"
    },
    {
      "title": "Improving Outcomes with Clinical Decision Support: An Implementer's Guide",
      "authors": [
        "J Osheroff"
      ],
      "year": 2012,
      "doi": "10.4324/9781498757461"
    },
    {
      "title": "What, if all alerts were specific-estimating the potential impact on drug interaction alert burden",
      "authors": [
        "H Seidling",
        "U Klein",
        "M Schaier"
      ],
      "year": 2014,
      "doi": "10.1016/j.ijmedinf.2013.12.006"
    },
    {
      "title": "Overriding of drug safety alerts in computerized physician order entry",
      "authors": [
        "H Van Der Sijs",
        "J Aarts",
        "A Vulto"
      ],
      "year": 2006,
      "doi": "10.1197/jamia.m1809"
    },
    {
      "title": "Reduced effectiveness of interruptive drug-drug interaction alerts after conversion to a commercial electronic health record",
      "authors": [
        "A Wright",
        "S Aaron",
        "D Seger"
      ],
      "year": 2018,
      "doi": "10.1007/s11606-018-4415-9"
    },
    {
      "title": "Clinician collaboration to improve clinical decision support: the Clickbusters initiative",
      "authors": [
        "A Mccoy",
        "E Russo",
        "K Johnson"
      ],
      "year": 2022,
      "doi": "10.1093/jamia/ocac027"
    },
    {
      "title": "The potential for leveraging machine learning to filter medication alerts",
      "authors": [
        "S Liu",
        "K Kawamoto",
        "Del Fiol"
      ],
      "year": 2022,
      "doi": "10.1093/jamia/ocab292"
    },
    {
      "title": "Using AI-generated suggestions from ChatGPT to optimize clinical decision support",
      "authors": [
        "S Liu",
        "A Wright",
        "B Patterson"
      ],
      "year": 2023,
      "doi": "10.1093/jamia/ocad072"
    },
    {
      "title": "Clinical decision support alert malfunctions: analysis and empirically derived taxonomy",
      "authors": [
        "A Wright",
        "Ai Ash"
      ],
      "year": 2018,
      "doi": "10.1093/jamia/ocx106"
    },
    {
      "title": "Cranky comments: detecting clinical decision support malfunctions through free-text override reasons",
      "authors": [
        "S Aaron",
        "D Mcevoy",
        "S Ray",
        "T Hickman",
        "A Wright"
      ],
      "year": 2019,
      "doi": "10.1093/jamia/ocy139"
    },
    {
      "title": "Drug-Drug interactions that should be non-interruptive in order to reduce alert fatigue in electronic health records",
      "authors": [
        "S Phansalkar",
        "H Van Der Sijs",
        "A Tucker"
      ],
      "year": 2013,
      "doi": "10.1136/amiajnl-2012-001089"
    },
    {
      "title": "SummIt: iterative text summarization via ChatGPT",
      "authors": [
        "H Zhang",
        "X Liu",
        "J Zhang"
      ],
      "year": 2023,
      "doi": "10.18653/v1/2023.findings-emnlp.714"
    },
    {
      "title": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
      "authors": [
        "Y Liu",
        "D Iter",
        "Y Xu"
      ],
      "year": 2023,
      "doi": "10.18653/v1/2023.emnlp-main.153"
    },
    {
      "title": "Benchmarking large language models for news summarization",
      "authors": [
        "T Zhang",
        "F Ladhak",
        "E Durmus"
      ],
      "year": 2023,
      "doi": "10.1162/tacl_a_00632"
    },
    {
      "title": "ImpressionGPT: an iterative optimizing framework for radiology report summarization with ChatGPT",
      "authors": [
        "C Ma",
        "Z Wu",
        "J Wang"
      ],
      "year": 2023
    },
    {
      "title": "Enhancing expert panel discussions in pediatric palliative care: innovative scenario development and summarization with ChatGPT-4",
      "authors": [
        "M Almazyad",
        "F Aljofan",
        "N Abouammoh"
      ],
      "year": 2023,
      "doi": "10.7759/cureus.38249"
    },
    {
      "title": "Utility of ChatGPT in clinical practice",
      "authors": [
        "J Liu",
        "C Wang",
        "S Liu"
      ],
      "year": 2023,
      "doi": "10.2196/48568"
    },
    {
      "title": "Prompt Engineering as an Important Emerging Skill for Medical Professionals: Tutorial",
      "authors": [
        "B Mesk\ufffd"
      ],
      "year": 2023,
      "doi": "10.2196/50638"
    },
    {
      "title": "A guideline of selecting and reporting intraclass correlation coefficients for reliability research",
      "authors": [
        "T Koo",
        "M Li"
      ],
      "year": 2016,
      "doi": "10.1016/j.jcm.2016.02.012"
    }
  ],
  "num_references": 23,
  "original_doi": "https://doi.org/10.13039/100000002"
}
