{
  "paper_id": "UHBB5IRG",
  "title": "Current applications and challenges in large language models for patient care: a systematic review Check for updates",
  "abstract": "Background The introduction of large language models (LLMs) into clinical practice promises to improve patient education and empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the popularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, this systematic review aims to synthesize current applications and limitations of LLMs in patient care. Methods We systematically searched 5 databases for qualitative, quantitative, and mixed methods articles on LLMs in patient care published between 2022 and 2023. From 4349 initial records, 89 studies across 29 medical specialties were included. Quality assessment was performed using the Mixed Methods Appraisal Tool 2018. A data-driven convergent synthesis approach was applied for thematic syntheses of LLM applications and limitations using free line-by-line coding in Dedoose. Results We show that most studies investigate Generative Pre-trained Transformers (GPT)-3.5 (53.2%, n = 66 of 124 different LLMs examined) and GPT-4 (26.6%, n = 33/124) in answering medical questions, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations include 6 secondorder and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations include 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. Conclusions This review systematically maps LLM applications and limitations in patient care, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare settings.",
  "year": 2023,
  "date": "2023-02-02",
  "journal": "Communications Medicine",
  "publication": "Communications Medicine",
  "authors": [
    {
      "forename": "Felix",
      "surname": "Busch",
      "name": "Felix Busch",
      "affiliation": "1  School of Medicine and Health , Department of Diagnostic and Interventional Radiology , Klinikum rechts der Isar , TUM University Hospital , Technical University of Munich , Munich , Germany. \n\t\t\t\t\t\t\t\t School of Medicine and Health \n\t\t\t\t\t\t\t\t Department of Diagnostic and Interventional Radiology \n\t\t\t\t\t\t\t\t Klinikum rechts \n\t\t\t\t\t\t\t\t Isar \n\t\t\t\t\t\t\t\t TUM University Hospital \n\t\t\t\t\t\t\t\t Technical University of Munich \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Munich \n\t\t\t\t\t\t\t\t\t Germany",
      "email": "felix.busch@tum.de",
      "orcid": "0000-0001-9770-8555"
    },
    {
      "forename": "Lena",
      "surname": "Hoffmann",
      "name": "Lena Hoffmann",
      "affiliation": "2  Department of Neuroradiology , Charit\u00e9 -Universit\u00e4tsmedizin Berlin , Corporate Member of Freie Universit\u00e4t Berlin and Humboldt Universit\u00e4t zu Berlin , Berlin , Germany. \n\t\t\t\t\t\t\t\t Department of Neuroradiology \n\t\t\t\t\t\t\t\t Corporate Member of Freie \n\t\t\t\t\t\t\t\t Charit\u00e9 -Universit\u00e4tsmedizin Berlin \n\t\t\t\t\t\t\t\t Universit\u00e4t Berlin \n\t\t\t\t\t\t\t\t Humboldt Universit\u00e4t zu Berlin \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Berlin \n\t\t\t\t\t\t\t\t\t Germany"
    },
    {
      "forename": "Christopher",
      "surname": "Rueger",
      "name": "Christopher Rueger",
      "affiliation": "2  Department of Neuroradiology , Charit\u00e9 -Universit\u00e4tsmedizin Berlin , Corporate Member of Freie Universit\u00e4t Berlin and Humboldt Universit\u00e4t zu Berlin , Berlin , Germany. \n\t\t\t\t\t\t\t\t Department of Neuroradiology \n\t\t\t\t\t\t\t\t Corporate Member of Freie \n\t\t\t\t\t\t\t\t Charit\u00e9 -Universit\u00e4tsmedizin Berlin \n\t\t\t\t\t\t\t\t Universit\u00e4t Berlin \n\t\t\t\t\t\t\t\t Humboldt Universit\u00e4t zu Berlin \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Berlin \n\t\t\t\t\t\t\t\t\t Germany"
    },
    {
      "forename": "Elon",
      "surname": "Van Dijk",
      "name": "Elon Van Dijk",
      "affiliation": "3  Department of Ophthalmology , Leiden University Medical Center , Leiden , The Netherlands. \n\t\t\t\t\t\t\t\t Department of Ophthalmology \n\t\t\t\t\t\t\t\t Leiden University Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Leiden \n\t\t\t\t\t\t\t\t\t The Netherlands"
    },
    {
      "forename": "Rawen",
      "surname": "Kader",
      "name": "Rawen Kader",
      "affiliation": "5  Division of Surgery and Interventional Sciences , University College London , London , United Kingdom. \n\t\t\t\t\t\t\t\t Division of Surgery and Interventional Sciences \n\t\t\t\t\t\t\t\t University College London \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t United Kingdom",
      "orcid": "0000-0001-9133-0838"
    },
    {
      "forename": "Esteban",
      "surname": "Ortiz-Prado",
      "name": "Esteban Ortiz-Prado",
      "affiliation": "6  One Health Research Group , \n\t\t\t\t\t\t\t\t One Health Research Group"
    },
    {
      "forename": "Marcus",
      "surname": "Makowski",
      "name": "Marcus Makowski",
      "affiliation": "1  School of Medicine and Health , Department of Diagnostic and Interventional Radiology , Klinikum rechts der Isar , TUM University Hospital , Technical University of Munich , Munich , Germany. \n\t\t\t\t\t\t\t\t School of Medicine and Health \n\t\t\t\t\t\t\t\t Department of Diagnostic and Interventional Radiology \n\t\t\t\t\t\t\t\t Klinikum rechts \n\t\t\t\t\t\t\t\t Isar \n\t\t\t\t\t\t\t\t TUM University Hospital \n\t\t\t\t\t\t\t\t Technical University of Munich \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Munich \n\t\t\t\t\t\t\t\t\t Germany",
      "orcid": "0000-0001-8778-647X"
    },
    {
      "forename": "Luca",
      "surname": "Saba",
      "name": "Luca Saba"
    },
    {
      "forename": "Martin",
      "surname": "Hadamitzky",
      "name": "Martin Hadamitzky"
    },
    {
      "forename": "Jakob",
      "surname": "Nikolas Kather",
      "name": "Jakob Nikolas Kather",
      "orcid": "0000-0002-3730-5348"
    },
    {
      "forename": "Daniel",
      "surname": "Truhn",
      "name": "Daniel Truhn",
      "orcid": "0000-0002-9605-0728"
    },
    {
      "forename": "Renato",
      "surname": "Cuocolo",
      "name": "Renato Cuocolo"
    },
    {
      "forename": "Lisa",
      "surname": "Adams",
      "name": "Lisa Adams",
      "affiliation": "1  School of Medicine and Health , Department of Diagnostic and Interventional Radiology , Klinikum rechts der Isar , TUM University Hospital , Technical University of Munich , Munich , Germany. \n\t\t\t\t\t\t\t\t School of Medicine and Health \n\t\t\t\t\t\t\t\t Department of Diagnostic and Interventional Radiology \n\t\t\t\t\t\t\t\t Klinikum rechts \n\t\t\t\t\t\t\t\t Isar \n\t\t\t\t\t\t\t\t TUM University Hospital \n\t\t\t\t\t\t\t\t Technical University of Munich \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Munich \n\t\t\t\t\t\t\t\t\t Germany"
    },
    {
      "forename": "Keno",
      "surname": "Bressem",
      "name": "Keno Bressem",
      "affiliation": "1  School of Medicine and Health , Department of Diagnostic and Interventional Radiology , Klinikum rechts der Isar , TUM University Hospital , Technical University of Munich , Munich , Germany. \n\t\t\t\t\t\t\t\t School of Medicine and Health \n\t\t\t\t\t\t\t\t Department of Diagnostic and Interventional Radiology \n\t\t\t\t\t\t\t\t Klinikum rechts \n\t\t\t\t\t\t\t\t Isar \n\t\t\t\t\t\t\t\t TUM University Hospital \n\t\t\t\t\t\t\t\t Technical University of Munich \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Munich \n\t\t\t\t\t\t\t\t\t Germany"
    }
  ],
  "doi": "10.1038/s43856-024-00717-2",
  "arxiv": "arXiv:2303.08774",
  "sections": [
    {
      "text": "Public and academic interest in large language models (LLMs) and their potential applications has increased substantially, especially since the release of OpenAI's ChatGPT (Chat Generative Pre-trained Transformers) in November 2022  [1] [2] [3]  . One of the main reasons for their popularity is the remarkable ability to mimic human writing, a result of extensive training on massive amounts of text and reinforcement learning from human feedback  4  .\n\nSince most LLMs are designed as general-purpose chatbots, recent research has focused on developing specialized models for the medical domain, such as Meditron or BioMistral, by enriching the training data of LLMs with medical knowledge  5, 6  . However, this approach to fine-tuning LLMs requires significant computational resources that are not available to everyone and is also not applicable to closed-source LLMs, which are often the most powerful. Therefore, another approach to improve LLMs for database in the Supplementary Methods). After importing the bibliographic data into Rayyan and removing duplicates, LH and CR conducted an independent blind review of each article's title and abstract  16  . Any article flagged as potentially eligible by either reviewer proceeded to the full-text evaluation stage. For this stage, LH and CR used a custom data extraction form created in Google Forms (available online)  17  to collect all relevant data independently from the studies that met the inclusion criteria. Quality assessment was also performed independently for each article within this data extraction form, using the Mixed Methods Appraisal Tool (MMAT) 2018  18  . Disagreements at any stage of the review were resolved through discussion with the author FB. In cases of studies with incomplete data, we have tried to contact the corresponding authors for clarification or additional information."
    },
    {
      "title": "Data analysis",
      "text": "Due to the diversity of investigated outcomes and study designs we sought to include, a meta-analysis was not practical. Instead, a data-driven convergent synthesis approach was selected for thematic syntheses of LLM applications and limitations in patient care  19  . Following Thomas and Harden, FB coded each study's numerical and textual data in Dedoose using free line-by-line coding  20, 21  . Initial codes were then systematically categorized into descriptive and subsequently into analytic themes, incorporating new codes for emerging concepts within a hierarchical tree structure. Upon completion of the codebook, FB and LH reviewed each study to ensure consistent application of codes. Discrepancies were resolved through discussion with the author KKB, and the final codebook and analytical themes were discussed and refined in consultation with all contributing authors."
    },
    {
      "title": "Results"
    },
    {
      "title": "Screening results",
      "text": "Of the 4349 reports identified, 2991 underwent initial screening, and 126 were deemed suitable for potential inclusion and underwent full-text screening. Two articles could not be retrieved because the authors or the corresponding title and abstract could not be identified online. Following full-text screening, 35 articles were excluded, and 89 articles were included in the final review. Most studies were excluded because they targeted the wrong discipline (n = 10/35, 28.6%) or population (n = 7/35, 20%) or were not original research (n = 8/35, 22.9%) (see Supplementary Dataset file 2). For example, we evaluated a study that focused on classifying physician notes to identify patients without active bleeding who were appropriate candidates for thromboembolism prophylaxis  22  . Although the classification tasks may lead to patient treatment, the primary outcome was informing clinicians rather than directly forwarding this information to patients. We also reviewed a study assessing the accuracy and completeness of several LLMs when answering Methotrexate-related questions  23  . This study was excluded because it focused solely on the pharmacological treatment of rheumatic disease. For a detailed breakdown of the inclusion and exclusion process at each stage, please refer to the PRISMA flowchart in Fig.  1 ."
    },
    {
      "title": "Characteristics of included studies",
      "text": "Supplementary Dataset file 3 summarizes the characteristics of the analyzed studies, including their setting, results, and conclusions. One study (n = 1/ 89, 1.1%) was published in 2022  24  , 84 (n = 84/89, 94.4%) in 2023  13,  , and 4 (n = 4/89, 4.5%) in 2024  [108] [109] [110] [111]  (all of which were peer-reviewed publications of preprints published in 2023). Most s udies were quantitative nonrandomized (n = 84/89, 94.4%)  13, [25] [26] [27] 103, 104, 106, 107, [109] [110] [111]  , 4 (n = 4/89, 4.5%)  28, 102, 105, 108  had a qualitative study design, and one (n = 1/89, 1.1%)  24  was quantitative randomized according to the MMAT 2018 criteria. However, the LLM outputs were often first analyzed quantitatively but followed by a qualitative analysis of certain responses. Therefore, if the primary outcome was quantitative, we considered the study design to be quantitative rather than mixed methods, resulting in the inclusion of zero mixed methods studies. The quality of the included studies was mixed (see Supplementary Dataset file 4). The authors were primarily affiliated with institutions in the United States (n = 47 of 122 different countries identified per publication, 38.5%), followed by Germany (n = 11/122, 9%), Turkey (n = 7/122, 5.7%), the United Kingdom (n = 6/122, 4.9%), China/Australia/Italy (n = 5/122, 4.1%, respectively), and 24 (n = 36/122, 29.5%) other countries. Most studies examined one or more applications based on the GPT-3.5 architecture (n = 66 of 124 different LLMs examined per study, 53.2%)  13,26-29,31-34, 36-40,42-49,52-54,56-61,63,65-67,71,72,74,75,77,78,81-89,91,92,94,95,97-100,102-104,106-109,111  , followed by GPT-4 (n = 33/124, 26.6%)  13, 25, 27, 29, 30, [34] [35] [36] 41, 43, 50, 51, 54, 55, 58, 61, 64, [68] [69] [70] 74, 76, [79] [80] [81] 83, 87, 89, 90, 93, 96, 98, 99, 101, 105  , Bard (n = 10/124, 8.1%; now known as Gemini)  33, 48, 49, 55, 73, 74, 80, 87, 94, 99  , Bing Chat (n = 7/124, 5.7%; now Microsoft Copilot)  49, 51, 55, 73, 94, 99, 110  , and other applications based on Bidirectional Encoder Representations from Transformers (BERT; n = 4/124, 3.2%)  13, 83, 84  , Large Language Model Meta-AI (LLaMA; n = 3/124, 2.4%)  55  , or Claude by Anthropic (n = 1/124, 0.8%)  55  . The maj rity of applications were primarily targeted at patients (n = 64 of 89 included studies, 73%)  24, 25, 29, 32, [34] [35] [36] [37] [38] [39] [41] [42] [43] [45] [46] [47] [48] [52] [53] [54] [56] [57] [58] [59] [60] 62, 63, 65, 66, [68] [69] [70] [71] [73] [74] [75] [77] [78] [79] [80] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] 97, 99, 100, [102] [103] [104] [105] [106] [107] [108] [109] [110] [111]  or both patients and caregivers (n = 25/89, 27%)  13, [26] [27] [28] 30, 31, 33, 40, 44, [49] [50] [51] 55, 61, 64, 67, 72, 76, [81] [82] [83] [84] 96, 98, 101  . Information about conflicts of interest and funding was not explicitly stated in 23 (n = 23/89, 25.8%) studies, while 48 (n = 48/89, 53.9%) reported that there were no conflicts of interest or funding. A total of 18 (n = 18/89, 20.2%) studies reported the presence of conflicts of interest and funding  13, 24, 38, 40, 54, 58, 59, 67, [69] [70] [71] 74, 80, 84, 96, 103, 105, 111  . Most studies did not report information about the institutional review board (IRB) approval (n = 55/89, 61.8%) or deemed IRB approval unnecessary (n = 28/89, 31.5%). Six studies obtained IRB approval (n = 6/89, 6.7%)  52, 82, [84] [85] [86] 92  ."
    },
    {
      "title": "Applications of large language models",
      "text": "An overview of the presence of codes for each study is provided in the Supplementary Dataset file 3. The majority of articles investigated the use and feasibility of LLMs as medical chatbots (n = 84/89, 94.4%) 13,24-62,64-66,68,69,71-96,98-111 , while fewer reports additionally or exclusively focused on the generation of patient information (n = 18/89, 20.2%)  24, 31, 43, 48, 49, 57, 59, 62, 67, 79, [88] [89] [90] [91] 97, 102, 106, 107  , including clinical documentation such as informed consent forms (n = 5/89, 5.6%)  43, 67, 91, 97, 102  and discharge instructions (n = 1/89, 1.1%)  31  , or translation/summarization tasks of medical texts (n = 5/89, 5.6%)  24, 49, 57, 79, 89  , creation of patient education materials (n = 5/89, 5.6%)  48, 62, 90, 106, 107  , and simplification of radiology reports (n = 2/89, 2.3%)  59, 88  . Most reports evaluated LLMs in English (n = 88/89, 98.9%)  13, [105] [106] [107] [108] [109] [110] [111]  , followed by Arabic (n = 2/84, 2.3%)  32, 104  , Mandarin (n = 2/84, 2.3%)  36, 75  , and Korean or Spanish (n = 1/89, 1.1%, respectively)  75  . The top-five specialties studied were ophthalmology (n = 10/89, 11.2%)  37, 40, 48, 51, 65, 74, 97, 98, 100, 101  , gastroenterology (n = 9/89, 10.1%)  25, 32, 34, 36, 39, 61, 62, 72, 96  , head and neck surgery/otolaryngology (n = 8/89, 9%)  35, 42, 56, 64, 66, 76, 78, 79  , and radiology  59, 70, [88] [89] [90] 110  or plastic surgery  45, 47, 49, 102, 107, 108  (n = 6/89, 6.7%, respectively). A schematic ill stration of the identified concepts of LLM applications in patient care is shown in Fig.  2 ."
    },
    {
      "title": "Limitations of large language models",
      "text": "The thematic synthesis of limitations resulted in two main concepts: one related to design limitations and one related to output. Figure  3  illustrates the hierarchical tree structure and quantity of the codes derived from the thematic synthesis of limitations. Supplementary Dataset file 5 provides an overview of the taxonomy of all identified limitation concepts, including their description and examples.\n\nDesign limitations. In terms of design limitations, many authors noted the limitation that LLMs are not optimized for medical use (n = 46/89, 51.7%)  13, 26, 28, 34, 35, [37] [38] [39] 46, 49, 50, [54] [55] [56] [57] [58] [59] 61, 62, 65, 66, 68, 70, 71, [79] [80] [81] [83] [84] [85] 88, 91, [93] [94] [95] [96] [97] [98] [100] [101] [102] [103] [104] [105] [106] [107] 109  , including implicit knowledge/lack of clinical context (n = 13/89, 14.6%)  28, 39, 46, 66, 71, 79, 81, [83] [84] [85] 98, 103  , limitations in clinical reasoning (n = 7/89, Fig.  1  | Preferred reporting items for systematic reviews and meta-analyzes (PRISMA) flow diagram. A total of 4349 reports were identified from Web of Science, PubMed, Embase/Embase Classic, ACM Digital Library, and IEEE Xplore. After excluding 1358 duplicates, 2991 underwent initial screening and 126 were deemed suitable for potential inclusion and underwent full-text screening. Two articles could not be retrieved because the authors or the corresponding title and abstract could not be identified online. After full text screening, 35 articles were excluded and 89 articles were included in the final review.\n\n7.9%)  55, 84, 95, [102] [103] [104] [105]  , limitations in medical image processing/production (n = 5/89, 5.6%)  37, 55, 91, 106, 107  , and misunderstanding of medical information and terms by the model (n = 7/89, 7.9%)  28, 38, 39, 59, 62, 65, 97  . In addition, data-related limitations were identified, including limited access to data on the internet (n = 22/89, 24.7%)  38, 39, 41, 43, [54] [55] [56] [57] 59, 60, 64, 76, 79, [82] [83] [84] 88, 91, 94, 96, 104, 109  , the undisclosed origin of training data (n = 36/89, 40.5%)  25, 26, 29, 30, 32, 34, 36, 37, 40, 46, 47, 50, 51, [53] [54] [55] [56] [57] [58] [59] [60] 64, 65, 70, 71, 76, 82, 83, 91, [94] [95] [96] 101, 105, 109  , limitations in providing, evaluating, and validating references (n = 20/89, 22.5%)  45, 49, [54] [55] [56] [57] 65, 71, 73, 76, 80, 83, 85, 91, 94, 96, 98, 101, 103, 105  , and storage/processing of sensitive health information (n = 8/89, 9%)  13, 34, 46, 55, 62, 76, 83, 109  . Further second-order concepts include  black-box algorithms, i.e., nonexplainable AI (n = 12/89, 13.5%)  27, 36, 55, 57, 65, 73, 76, 83, 91, 94, 103, 105  , limited engagement and dialog capabilities (n = 10/89, 11.2%)  13, 27, 28, 37, 38, 51, 56, 66, 95, 103  , and the inability of self-validation and correction (n = 4/89, 4.5%)  61, 73, 74, 107  .\n\nOutput limitations. The evaluation of limitations in output data yielded 7 second-order codes concerning the non-reproducibility (n = 38/89, 42.7%)  28, 29, 34, 38, 39, 41, 43, 45, 46, 49, [54] [55] [56] [57] [58] [59] [60] [61] 64, 65, [71] [72] [73] 76, 80, 82, 83, 85, 90, 91, 94, 96, 98, 99, 101, [103] [104] [105]  , noncomprehensiveness (n = 78/89, 87.6%) 13,25,26,28-30,32-44,46,48-62,64,65,67-79, 81-98,100,102-107,109-111 , incorrectness (n = 78/89, 87.6%) 13,25-44,46,49-52, 54-62,64-66,69-79,81-85,87-107,109-111\n\n, (un-)safeness (n = 39/89, 43.8%)  28, 30, 35, 37, 39, 40, [42] [43] [44] 46, 50, 51, [57] [58] [59] [60] 62, 64, 65, 69, 70, 73, 74, 76, [78] [79] [80] 82, 84, 85, 91, 94, 95, [98] [99] [100] 105, 106, 109  , bias (n = 6/89, 6.7%)  26, 32, 34, 36, 66, 103  , and the dependence of the quality of output on the prompt-/input provided (n = 27/89, 30.3%)  [26] [27] [28] 34, 38, 41, 44, 46, 51, 52, 56, [68] [69] [70] [71] [72] 74, 76, 78, 79, [81] [82] [83] 90, 94, 95, 100, 101  or the environment (n = 16/89, 18%)  13, 34, 46, [49] [50] [51] 54, 58, 60, 72, 73, 88, 90, 93, 97, 109  . Non-reproducibility. For non-reproducibility, key concepts included the non For non-reproducibility, key concepts included the non-deterministic nature of the output, e.g., due to inconsistent results across multiple iterations (n = 34/89, 38.2%)  28, 29, 34, 38, 39, 41, 43, 46, [58] [59] [60] [61] 72, 76, 82, 90, 94, 98, 99, 101, 103, 104  and the inability to provide reliable references (n = 20/89, 22.5%)  45, 49, [54] [55] [56] [57] 65, 71, 73, 76, 80, 83, 85, 91, 94, 96, 98, 101, 103, 105  . 38.2%)  13, 28, 30, 34, 37, 38, 41, 43, 49, 51, 56, 57, 59, 61, 65, 70, 77, 79, 81, [84] [85] [86] 90, 94, 95, 100, [102] [103] [104] [105] [106] [107] 110  , incompleteness of output (n = 68/89, 76.4%)  13, 25, 26, [28] [29] [30] 32, [34] [35] [36] [37] [38] [39] [41] [42] [43] [44] 46 ,49-52,55-62,64,65,67-69,72-77, 79,81-86,89-98,100,102-107,109-111"
    },
    {
      "title": "Non",
      "text": ", provision of information that is not standard of care (n = 24/89, 27%)  28, 40, 43, 46, 49, 50, 54, 57, 58, 65, 69, 72, 73, 77, 78, 81, 85, 91, 94, 98, 100, 103, 107, 111  and/or outdated (n = 12/89, 13.5%)  13, 25, 32, 34, 38, 41, 43, 44, 49, 54, 83, 84  , and production of oversimplified (n = 10/89, 11.2%)  38, 46, 49, 54, 59, 79, 84, 85, 103  , superfluous (n = 16/89, 18%)  13, 28, 34, 38, 46, 62, 72, 79, 86, 90, 94, 97, 100, 106, 107  , o v e r c a u t i o u s ( n = 7/89, 7.9%)  13, 28, 37, 51, 70, 103, 110  , overempathic (n = 1/89, 1.1%)  13  , or output with inappropriate complexity/reading level for patients (n = 22/89, 24.7%)  13, 34, 42, 48, 50, 51, 53, 55, 56, 67, 71, 78, 79, 85, 87, 88, 90, 93, 106, 107, 109, 110  .\n\nIncorrectness. For incorrectness, we identified 6 key concepts. Some of the incorrect information could be attributed to what is commonly known as hallucination (n = 38/89, 42.7%)  25, 28, 32, 33, [35] [36] [37] [38] [40] [41] [42] [43] [44] [49] [50] [51] [57] [58] [59] [60] 65, 73, 74, 76, 77, 81, 83, 85, 91, 94, [96] [97] [98] 100, 103, 106, 107, 109  , i.e., the creation of entirely fictitious or false information that has no basis in the input provided or in reality (e.g., \"You may be asked to avoid eating or drinking for a few hours before the scan\" for a bone scan). Other instances of misinformation were more appropriately classified unde  alternative concepts of the original psychiatric analogy, as described in detail by Currie et al.  43, 112, 113  . These include illusion (n = 12/89, 13.5%) 28,36,38,43,57,59,77,78,85,88,94,105 , which is characterized by the generation of deceptive perceptions or the distortion of information by conflating similar but separate concepts (e.g., suggesting that MRI-type sounds might be experienced during standard nuclear medicine imaging), delirium (n = 34/89, 38.2%)  13, 26, 28, 30, 37, 43, 50, 58, 59, 61, 65, 70, [72] [73] [74] [75] 77, 79, [81] [82] [83] [84] [85] [90] [91] [92] 94, 95, 98, 102, 103, 107, 109, 110  , which indicates significant gaps in vital information, resulting in a fragmented or confused understanding of a subject (e.g., omission of crucial information about caffeine cessation for stress myocardial perfusion scans), extrapolation (n = 11/89, 12.4%)  43, 59, 65, 78, 81, 91, 94, 106, 107, 110  , which involves applying general knowledge or patterns to specific situations where they are inapplicable (e.g., advice about injection-site discomfort that is more typical of CT contrast administration), delusion (n = 14/89, 15.7%)  28, 30, 43, 50, 59, 65, 69, 73, 74, 78, 81, 94, 103, 111  , a fixed, false belief despite contradictory evidence (e.g., inaccurate waiting times for the thyroid scan), and confabulation (n = 18/89, 20.2%)  25, 28, [36] [37] [38] 40, 46, 59, 62, 65, 71, [77] [78] [79] 94, 103, 107  , i.e., filling in memory or knowledge gaps with plausible but invented information (e.g., \"You should drink plenty of fluids to help flush the radioactive material from your body\" for a biliary system-excreted radiopharmaceutical). Safety and bias. Many studies rated the generated output as unsafe, including misleading (n = 34/89, 38.2%) 28,30,35,43,44,46,50,51,57-60,62,64,65,69,73,74,76, LLM limitations (n = 89) Fig.  3  | Illustration of the hierarchical tree structure for the thematic synthesis of large language model (LLM) limitations in patient care, including the presence of codes for each concept. The font size of each concept is shown in proportion to its frequency in the studies analyzed. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations included 6 second-order and 12 thirdorder codes, while output limitations included 9 second-order and 32 third-order codes. 78-80,82,84,85,94,95,98-100,105,106,109   or even harmful content (n = 26/89, 29.2%)  28, 30, 37, 39, 40, 42, 43, 50, 51, [58] [59] [60] 70, 73, 74, 76, 79, 84, 85, 91, 94, 95, [98] [99] [100] 109  .\n\nA minority of reports identified biases in the output, which were related to language (n = 2/89, 2.3%)  32, 36  , insurance status 103 , underserved racial groups  26  , or underrepresented procedures  34  (n = 1/89, 1.1%, each).\n\nDependence on input and environment. Many authors suggested that performance was related to the prompting/input provided or the environment, i.e., depending on the evidence (n = 7/89, 7.9%)  52, 68, 69, 71, 81, 82, 95  , complexity (n = 11/89, 12.4%)  28, 34, 44, 46, 70, 74, 76, 79, 94, 102  , specificity (n = 13/89, 14.6%)  27, 38, 41, 56, 70, 72, 74, 76, 78, 81, 95, 100, 101  , quantity (n = 3/89, 3.4%)  26, 52, 74  of the input, type of conversation (n = 3/89, 3.4%)  27, 51, 90  , or the appropriateness of the output related to the target group (n = 9/89, 10.1%)  46, 49, 51, 54, 72, 90, 93, 97, 109  , provider/organization (n = 4/89, 4.5%)  13, 50, 60, 88  , and local/national medical resources (n = 5/89, 5.6%)  34, 50, 58, 60, 73  ."
    },
    {
      "title": "Discussion",
      "text": "In this systematic review, we synthesized the current applications and limitations of LLMs in patient care, incorporating a broad analysis across 29 medical specialties and highlighting key limitations in LLM design and output, providing a comprehensive framework and taxonomy for describing and categorizing limitations that may arise when using LLMs in healthcare settings.\n\nMost articles examined the use of LLMs based on the GPT-3.5 or GPT-4 architecture for answering medical questions, followed by the generation of patient information, including medical text summarization or translation and clinical documentation. The conceptual synthesis of LLM limitations revealed two key concepts: the first related to design, including 6 secondorder and 12 third-order codes, and the second related to output, including 9 second-order and 32 third-order codes. By systematically categorizing the limitations of LLMs in clinical settings, our taxonomy aims to provide healthcare professionals and developers with a framework for assessing potential risks associated with the use of LLMs in patient care. In addition, our work highlights key areas for improvement in the development of LLMs and aims to enable clinicians to make more informed decisions by understanding the limitations inherent in the design and output, thereby supporting the establishment of best practices for LLM use in clinical settings.\n\nAlthough many LLMs have been developed specifically for the biomedical domain in recent years, we found that ChatGPT has been a disruptor in the medical literature on LLMs, with GPT-3.5 and GPT-4 accounting for almost 80% of the LLMs examined in this systematic review. While it was not possible to conduct a meta-analysis of the performance on medical tasks, many authors provided a positive outlook towards the integration of LLMs into clinical practice. However, we have conceptualized several key limitations in the design and output of LLMs, some of the most prevalent in our systematic review are briefly discussed in the following paragraphs.\n\nThe majority of studies (n = 55/89) reported limitations that were conceptualized as related to the underlying data of the LLMs studied. Especially the use of proprietary models such as ChatGPT in the biomedical field was a concern in many of the studies analyzed, mainly because of the lack of training data transparency (third-order code: undisclosed origin of training data). In practice, it is widely recognized that limited access to the underlying algorithms, training data, and data processing and storage mechanisms of LLMs is a significant barrier to their application in healthcare  114  . This opacity makes it difficult for healthcare professionals to fully understand how these models function, assess their reliability, or ensure compliance with local medical standards and regulations. Consequently, the use of such models in healthcare settings can be problematic, and the need to recognize and correct potential limitations in the outputs of such models is paramount.\n\nMoreover, integrating proprietary models into clinical practice introduces a vulnerability to performance changes that occur with model updates  115  . As these models are updated by their developers, functionalities that healthcare providers rely on may be altered or broken, potentially leading to harmful outcomes for patients, which was also conceptualized in our study under output limitations (second-order code: unsafe; third-order codes: misleading/harmful). This unpredictability is a serious concern in the biomedical field, where consistency and reliability are crucial. Notably, the unpredictability of LLMs was another concept of output limitations in our systematic review (second-order code: non-reproducible; third-order codes: non-deterministic/non-referenceable).\n\nAs a result, open-source models such as BioMistral may offer a viable solution  6  . Such open source models not only offer more transparency, as their algorithms and training data are accessible but can also be adapted locally. However, given the limited number of articles on open-source LLMs in our review, we strongly encourage future studies investigating the applicability of open-source LLMs in patient care.\n\nAbout half of the studies analyzed reported limitations related to LLMs not being optimized for the medical domain. One possible solution to this limitation may be to provide medical knowledge during inference using RAG  116  . However, even when trained for general purposes, ChatGPT has previously been shown to pass the United States Medical Licensing Examination (USMLE), the German State Examination in Medicine, or even a radiology board-style examination without images  [117] [118] [119] [120]  . Although outperformed on specific tasks by specialized medical LLMs, such as Google's MedPaLM-2, this suggests that general-purpose LLMs can comprehend complex medical literature and case scenarios to a degree that meets professional standards 121 . Furthermore, given the large amounts of data on which proprietary models such as ChatGPT are trained, it is not unlikely that they have been exposed to more medical data overall than smaller specialized models despite being generalist models. Notably, a recent study even suggested that fine-tuning LLMs on biomedical data does not improve performance compared to their general-purpose counterparts  122  .\n\nIt should also be noted that passing these exams does not equate to the practical competence required of a healthcare provider, which was also a limitation identified in our review (third-order codes: implicit knowledge/ lack of clinical context; limited clinical reasoning; misunderstanding of medical information/terms; limited in processing/producing medical images)  123  . In addition, reliance on exam-based assessments carries a significant risk of bias. For example, if the exam questions or similar variants are publicly available and, thus, may be present in the training data, the LLM does not demonstrate any knowledge outside of training data memorization  124  . In fact, these types of tests can be misleading in estimating the model's true abilities in terms of comprehension or analytical skills.\n\nThe non-reproducibility of LLM output, as conceptualized in 38 studies, highlights key challenges in ensuring consistency and determinism in LLMgenerated results. One major issue is the inherent stochasticity in the models' architecture, particularly in transformer-based models, which utilize probabilistic techniques during inference (e.g., beam search or temperature sampling)  125  . This non-determinism can lead to different outputs for the same input, making it difficult to replicate results exactly across different instances or even across models with identical training data. Further external factors contributing to non-reproducibility, such as variations in hardware, software versions, or context windows, complicate the assurance of reproducibility 126 . As the reproducibility of results is a central principle in medical practice, our concepts highlight the need for more standardized protocols, improved documentation of model configurations, the examination of nondeterminism for evaluation purposes, and further research on how robust results can be achieved before implementing LLMs in real-world clinical practice. Interestingly, Ouyang et al. reported that only a minority of studies take non-determinism into account in their experimental evaluation when using ChatGPT for code generation, suggesting that this limitation is also prevalent and overlooked in other domains of LLM use  125  .\n\nThe concept of non-comprehensiveness was prevalent in almost 90% of the studies analyzed (n = 78/89). For this concept, the majority of thirdorder codes were related to LLM outputs that were incomplete. This issue is particularly significant when considering the application of LLMs in medical tasks such as clinical decision support or diagnosis, where incomplete or partial results can have serious consequences. In clinical practice, missing key information could lead to suboptimal patient outcomes, incorrect diagnoses, or improper treatment recommendations. For instance, an incomplete therapy suggestion could render the entire treatment plan insufficient, potentially resulting in harm to the patient. Given the potential of using LLMs in medical decision-making, these limitations underscore the necessity for expert supervision and validation of LLM outputs depending on their application. While LLMs used as chatbots for general patient inquiries may not require consistent human oversight, using LLMs for treatment advice would require consistent validation to ensure that incomplete information does not lead to adverse outcomes. Depending on their application, the same problem arises when the LLM generates generic or non-personalized information, which was another third-order code identified. The generation of content with high complexity and an inappropriate reading level, which was above the American Medical Association (AMA) recommended 6th-grade reading level in almost all of the 22 studies that analyzed the complexity level of the output, may further limit its usefulness for patient information 127 . Again, the best solution to the lack of comprehensiveness in clinical practice so far seems to be human oversight.\n\nIncorrectness, alongside non-comprehensiveness (as above), was the most common second-order code, identified in about 90% of studies (n = 78/89). In our conceptual synthesis of incorrect results, we followed the taxonomy of Currie et al. to classify incorrect outputs more precisely into illusions, delusions, delirium, confabulation, and extrapolation, thus proposing a framework for a more precise and structured error classification to improve the characterization of incorrect outputs and enabling more detailed performance comparisons with other research  43, 112, 113  .\n\nMany studies currently refer to all non-factual LLM results as \"hallucinations.\" However, this generalization fails to capture the complexity of errors when considering the original psychiatric analogy. Simply classifying errors as hallucinations restricts their description to invented information, overlooking errors that, for example, omit critical information and leading to fragmented or confused understanding (third-order code: delirium). Notably, the third-order code \"delirium\" was observed in nearly as many studies as the third-order code \"hallucination.\" However, a non-detailed classification of incorrect results can affect not only the comparability of research findings but also has implications for clinical practice. While hallucinations (for example, fabricating instructions like \"You may need to fast before a bone scan.\") may not always have serious consequences, errors classified as delirium-such as omitting crucial details like caffeine cessation before a stress myocardial perfusion scan-would always result in undesired outcomes (in the here presented example, most likely in repeating postponing the examination). As a result, our review advocates for a more detailed classification of incorrect results in order to increase the qualitative comparability of incorrect LLM outputs and, ultimately, the relevance and implications of these results for clinical practice.\n\nThe conceptualization of unsafeness in 39 of 89 studies presents a significant concern when considering the integration of LLMs into medical practice. In the field of medicine, any tool or intervention that could lead to misleading or harmful outcomes must be critically assessed, as the potential for patient harm is high  128, 129  . Such tools are generally only accepted when the benefits clearly outweigh the risks, and even then, informed consent from the affected individual is essential 130 . While informed consent might ensure that patients understand the risks involved and are able to make an educated decision about their care, which could be obtained, for example, in the form of a disclaimer before using the LLM, studies suggest that even when obtaining informed consent the patient understanding increases not significantly  131  . In the case of disclaimers, there might also be the risk that these are accepted without proper reading or understanding  132  . The practicality of informed consent once LLMs are deeply integrated into clinical workflows also remains an issue, as it is when patients no longer have the ability to opt out, such as in the case of serious illness. In any case, the finding that nearly half of the studies reported limitations related to LLM unsafeness suggests that LLMs are not yet reliable enough for autonomous medical use, and there is a critical need for safety measures and regulatory and human oversight to prevent adverse consequences in medical contexts  133  .\n\nFurther second-order concepts suggested that the output is influenced by the input or environment in which it is expressed. In fact, LLMs can be highly dependent on the quality and specificity of input, making their output prone to errors when faced with vague or incomplete information  [134] [135] [136]  . Again, this poses significant risks in patient care, where incorrect outputs can lead to adverse outcomes, such as inappropriate triage or treatment. For example, in our review, eleven studies reported a decrease in performance with increasing complexity of the input, which can have implications in clinical practice, such as failing to consider multifactorial medical issues like comorbidities, thus compromising the quality of care for those patients.\n\nWe found that the environment also influences the appropriateness of LLM outputs in medical settings. Models may recommend treatments that are inappropriate for certain patient populations, such as offering adult care protocols for pediatric patients or suggesting therapies that are not available in certain regions. This also raises ethical concerns, particularly in resourceconstrained settings, where making inappropriate or inaccessible recommendations may reinforce existing inequalities and lead to uncomfortable situations for both the healthcare provider and the patient  137  . One solution may be to provide adequate training to LLM users, or in our scenario, to patients, on how to present input to the model to achieve the best results. Another solution is to train or fine-tune the model to the environment in which it will be used. For example, if the LLM is trained on the standard operating procedure for handling patients with major adverse cardiovascular events in a particular hospital, it is more likely to recommend the adequate procedure in this setting than when it is trained on worldwide data from an unknown time frame, where there is a chance that it will suggest non-standard care that may only be relevant in other countries where most of the training data is coming from, or even provides outdated information (which is another third-order code that was conceptualized under noncomprehensiveness) if it is trained on data that is not current.\n\nUltimately, only six studies have identified biases in their results, for example, reflecting the unequal representation of certain content or the biases inherent in human-generated text in the training data  138  . Here, we conceptualized the results of studies that identified bias in their analysis and not only mentioned bias as a theoretical limitation. Thus, these results may indicate that the implemented safeguards are effective. On the other hand, identifying bias was not the primary outcome of most studies, and not much is known about the technology and developer policies of proprietary LLMs. Moreover, previous work has shown that automated jailbreak generation is possible across various commercial LLM chatbots 139 . In the end, LLMs are trained on large datasets that inevitably contain biases-such as gender, racial, or cultural biases-embedded in the text 140 . These biases can be amplified or reflected by the models, leading to unfair or harmful outputs. Despite the use of various mitigation techniques, such as debiasing algorithms, curating balanced datasets, or incorporating fairness-focused training objectives, eliminating bias entirely is a persistent challenge  [141] [142] [143]  . This is because LLMs learn patterns from their training data, and human biases are inherently present in much of the data they consume. Moreover, the biases introduced or reinforced by LLM are not always obvious, making them more difficult to detect and correct, which may have contributed to the comparatively low number of studies that reported any bias in their results. Notably, subtle biases, such as those related to linguistic connotations, regional dialects, or implicit associations, can be especially insidious and difficult to eliminate through technical safeguards  144  . Therefore, the results of our review may encourage future studies to more explicitly examine the biases inherent in LLMs when used for medical tasks and how such biases could be mitigated.\n\nOur findings raise a key question when applying LLMs to the medical domain: how can we entrust our patients to LLMs if they are neither reliable nor transparent? Given that models like ChatGPT are already publicly accessible and widely used, patients may already refer to them for medical questions in much the same way they use Google Search, making concerns about their early adoption somewhat academic  145  .\n\nIn addition to the advances in the development of LLMs and the focus on open source, adopting appropriate security measures to prevent the identified LLM limitations in clinical practice out-of-the-box will become increasingly important. For example, strategies to ensure LLM security and privacy can include continuous monitoring for new vulnerabilities, implementing input validation, conducting regular audits of training data, and using secure data pipelines 146 . Additionally, data anonymization, encryption, access controls, and regular security updates are essential to prevent data leakage, model theft, and privacy breaches.\n\nMoreover, expert oversight of the final LLM output could mitigate any remaining risks in the last instance, ensuring that erroneous or inappropriate suggestions are identified and corrected before they can impact patient care. Recently, efforts have been made in this direction by adopting the widely recognized Physician Documentation Quality Instrument (PDQI-9) for the assessment of AI transcripts and clinical summaries 147 . However, whether ongoing human oversight and validation of LLM-generated content is feasible and can reduce the likelihood of adverse outcomes remains the subject of further research at this early stage of LLM deployment in healthcare.\n\nAnother important factor for the successful clinical implementation of LLMs in patient care could be patient acceptance, which was not assessed in any of the studies analyzed. The growing use of LLMs in healthcare might be perceived as a reduction in the interpersonal relationship between healthcare professionals and patients, potentially leading to a sense of dehumanization in medicine 148 . Therefore, to promote a positive reception of AI tools among patients, incorporating their perspectives already during the AI development and implementation process could be key 149 . Eventually, patient perspectives are already considered in AI regulatory frameworks, such as in the European Union AI Act, which came into force in August 2024  150  . The associated challenges faced by generative AI and LLM, for example, in terms of training data transparency and validation of nondeterministic output, will show which approaches the companies will take to bring these models into compliance with the law 151 . How the notified bodies interpret and enforce the law in practice will likely be decisive for the further development of LLMs in the biomedical sector.\n\nOur study has limitations. First, our review focused on LLM applications and limitations in patient care, thus excluding research directed at clinicians only. Future studies may extend our synthesis approach to LLM applications that explicitly focus on healthcare professionals. Second, while it was not possible to conduct a meta-analysis of LLM performance to the different study designs and evaluation methods used, this will be an important area for future work as the field of LLM research in clinical settings continues to evolve. Third, there is a risk that potentially eligible studies were not included in our analysis if they were not present in the 5 databases reviewed or were not available in English. However, we screened nearly 3,000 articles in total and systematically analyzed 89 articles, providing a comprehensive overview of the current state of LLMs in patient care, even if some articles could have been missed. With our chosen cut-off date of January 2022, there is also a risk of missing relevant publications on predecessor LLM models, such as GPT-3, which was introduced in 2020. However, as our review focused on current LLM applications and limitations, it seemed most beneficial to include only recent publications from the last two years on the most advanced models, especially when considering that ChatGPT was first made available in November 2022. Finally, the rapid development and advancement of LLMs make it difficult to keep this systematic review up to date. For example, Gemini 1.5 Pro was published in February 2024, and corresponding articles are not included in this review, which synthesized articles from 2022 to 2023. This also has implications for our introduced taxonomy of LLM limitations, as new limitations may emerge as models evolve, and previous limitations may become less relevant or even obsolete. For example, our taxonomy identifies \"limited access to internet data\" as a limitation; however, with the introduction of web browsing capabilities for GPT-4 in May 2023, this particular limitation no longer applies to that model. Given these ongoing developments, we strongly encourage future studies to test, update, and extend our taxonomy to ensure that it remains a relevant tool for categorizing LLM limitations in clinical and other high-stakes applications."
    },
    {
      "text": "Fig. 2 | Schematic illustration of the identified disciplines, languages, and clinical concepts of large language models (LLMs) applications in patient care. A Column plot showing the distribution of medical specialties in which LLMs have been tested for patient care. B Pie chart illustrating the distribution of languages in which LLMs have been tested. C Schematic representation of the concepts identified for the application of LLMs in patient care."
    },
    {
      "text": "Underrepresented procedures (n = 1) Underserved racial groups (n = 1) Insurance status (n = 1) Language (n = 2) Local/national medical resources (n = 5) Provider/organization (n = 4) Target-group (n = 9) Conversation type (n = 3) Quantity (n = 3) Specificity (n = 13) Complexity (n = 11) Evidence (n = 7) Harmful (n = 26) Misleading (n = 34) Confabulation (n = 18) Illusion (n = 12) Hallucination (n = 38) Delusion (n = 14) Delirium (n = 34) Extrapolation (n = 10) High complexity/reading level (n = 22) Overempathic (n = 1) Overcautious (n = 7) Superfluous (n = 16) Oversimplification (n = 10) Outdated (n = 12) Non-standard of care (n = 24) Incomplete (n = 68) Generic/non-personalized (n = 34) Non-referenceable (n = 20) Non-deterministic (n = 24) Not open source (n = 10) Not freely accessible (n = 9) Limited number of prompts (n = 3) Stores/processes sensitive health information (n = 8) Limited in reference provision/evaluation/validation (n = 20) Undisclosed origin of training data (n = 36) Restricted access to internet data (n = 22) Misunderstanding of medical information/terms (n = 7) Limited in processing/producing medical images (n = 5) Limited clinical reasoning (n = 7) Implicit knowledge/lack of clinical context (n = 13) Bias (n = 6) Environment-dependent (n = 16) Prompt-/input dependent (n = 27) Unsafe (n = 39) Incorrect (n = 78) Non-comprehensive (n = 78) Non-reproducible (n = 38) Incapable of self-validation/correction (n = 4) Limited engagement/dialogue capabilities (n = 10) Black box (n = 12) Accessibility (n = 18) Data (n = 55) Not optimized for the medical domain (n = 46) Output (n = 86) Design (n = 67)"
    }
  ],
  "references": [
    {
      "title": "ChatGPT reaches 100 million users two months after launch",
      "authors": [
        "D Milmo"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Openai"
      ],
      "year": 2023
    },
    {
      "title": "A survey of large language models",
      "authors": [
        "W Zhao"
      ],
      "year": 2023
    },
    {
      "title": "The future landscape of large language models in medicine",
      "authors": [
        "J Clusmann"
      ],
      "year": 2023
    },
    {
      "title": "Meditron-70b: Scaling medical pretraining for large language models",
      "authors": [
        "Z Chen"
      ],
      "year": 2023
    },
    {
      "title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains",
      "authors": [
        "Y Labrak"
      ],
      "year": 2024,
      "doi": "10.18653/v1/2024.findings-acl.348"
    },
    {
      "title": "Benchmarking Retrieval-Augmented Generation for Medicine",
      "authors": [
        "G Xiong",
        "Q Jin",
        "Z Lu",
        "A Zhang"
      ],
      "year": 2024,
      "doi": "10.18653/v1/2024.findings-acl.372"
    },
    {
      "title": "A large language model for electronic health records",
      "authors": [
        "X Yang"
      ],
      "year": 2022
    },
    {
      "title": "Opportunities and challenges for ChatGPT and large language models in biomedicine and health",
      "authors": [
        "S Tian"
      ],
      "doi": "10.1093/bib/bbad493"
    },
    {
      "title": "Leveraging GPT-4 for post hoc transformation of free-text radiology reports into structured reporting: a multilingual feasibility study",
      "authors": [
        "L Adams"
      ],
      "year": 2023,
      "doi": "10.1148/radiol.230725"
    },
    {
      "title": "Towards accurate differential diagnosis with large language models",
      "authors": [
        "D Mcduff"
      ],
      "year": 2023
    },
    {
      "title": "Health system-scale language models are allpurpose prediction engines",
      "authors": [
        "L Jiang"
      ],
      "year": 2023,
      "doi": "10.1038/s41586-023-06160-y"
    },
    {
      "title": "Leveraging Large Language Models for Generating Responses to Patient Messages",
      "authors": [
        "S Liu"
      ],
      "year": 2023,
      "doi": "10.1101/2023.07.14.23292669"
    },
    {
      "title": "A systematic review of current large language model applications and biases in patient care",
      "authors": [
        "F Busch",
        "L Hoffmann",
        "L Adams",
        "K Bressem"
      ],
      "year": 2024,
      "doi": "10.1101/2024.03.04.24303733"
    },
    {
      "title": "The PRISMA 2020 statement: an updated guideline for reporting systematic reviews",
      "authors": [
        "M Page"
      ],
      "year": 2021
    },
    {
      "title": "Rayyan -a web and mobile app for systematic reviews",
      "authors": [
        "M Ouzzani",
        "H Hammady",
        "Z Fedorowicz",
        "A Elmagarmid"
      ],
      "year": 2016,
      "doi": "10.1186/s13643-016-0384-4"
    },
    {
      "title": "Data extraction form",
      "year": 2024,
      "doi": "10.18173/2354-1075.2024-0020"
    },
    {
      "title": "The Mixed Methods Appraisal Tool (MMAT) version 2018 for information professionals and researchers",
      "authors": [
        "Q Hong"
      ],
      "year": 2018,
      "doi": "10.3233/efi-180221"
    },
    {
      "title": "Convergent and sequential synthesis designs: implications for conducting and reporting systematic reviews of qualitative and quantitative evidence",
      "authors": [
        "Q Hong",
        "P Pluye",
        "M Bujold",
        "M Wassef"
      ],
      "year": 2017
    },
    {
      "title": "Methods for the thematic synthesis of qualitative research in systematic reviews",
      "authors": [
        "J Thomas",
        "A Harden"
      ],
      "year": 2008,
      "doi": "10.1186/1471-2288-8-45"
    },
    {
      "title": "Dedoose Version 9.2.4, cloud application for managing, analyzing, and presenting qualitative and mixed method research data",
      "year": 2024,
      "doi": "10.1038/s43856-024-00717-2"
    },
    {
      "title": "A Large Language Model Screening Tool to Target Patients for Best Practice Alerts: Development and Validation",
      "authors": [
        "T Savage",
        "J Wang",
        "L Shieh"
      ],
      "year": 2023
    },
    {
      "title": "Assessing the accuracy and completeness of artificial intelligence language models in providing information on methotrexate use",
      "authors": [
        "B Coskun",
        "B Yagiz",
        "G Ocakoglu",
        "E Dalkilic",
        "Y Pehlivan"
      ],
      "year": 2023,
      "doi": "10.1007/s00296-023-05473-5"
    },
    {
      "title": "Increasing Women's Knowledge about HPV Using BERT Text Summarization: An Online Randomized Study",
      "authors": [
        "H Bitar",
        "A Babour",
        "F Nafa",
        "O Alzamzami",
        "S Alismail"
      ],
      "year": 2022,
      "doi": "10.3390/ijerph19138100"
    },
    {
      "title": "Artificial Intelligence and Patient Education: Examining the Accuracy and Reproducibility of Responses to Nutrition Questions Related to Inflammatory Bowel Disease by GPT-4",
      "authors": [
        "J Samaan"
      ],
      "year": 2023,
      "doi": "10.1101/2023.10.28.23297723"
    },
    {
      "title": "Racial Disparities in Knowledge of Cardiovascular Disease by a Chat-Based Artificial Intelligence Model",
      "authors": [
        "O Eromosele",
        "T Sobodu",
        "O Olayinka",
        "D Ouyang"
      ],
      "year": 2023,
      "doi": "10.1101/2023.09.20.23295874"
    },
    {
      "title": "Guidelines For Rigorous Evaluation of Clinical LLMs For Conversational Reasoning",
      "authors": [
        "S Johri"
      ],
      "year": 2024,
      "doi": "10.1101/2023.09.12.23295399"
    },
    {
      "title": "Use of ChatGPT in Pediatric Urology and its Relevance in Clinical Practice: Is it useful? medRxiv",
      "authors": [
        "A Braga"
      ],
      "year": 2023,
      "doi": "10.1101/2023.09.11.23295266"
    },
    {
      "title": "Appropriateness of ChatGPT in answering heart failure related questions",
      "authors": [
        "R King"
      ],
      "year": 2023,
      "doi": "10.1101/2023.07.07.23292385"
    },
    {
      "title": "Fact Check: Assessing the Response of ChatGPT to Alzheimer's Disease Statements with Varying Degrees of Misinformation",
      "authors": [
        "S Huang"
      ],
      "year": 2023,
      "doi": "10.1101/2023.09.04.23294917"
    },
    {
      "title": "Assessing Racial and Ethnic Bias in Text Generation for Healthcare-Related Tasks by ChatGPT1",
      "authors": [
        "J Hanna",
        "A Wakene",
        "C Lehmann",
        "R Medford"
      ],
      "year": 2023,
      "doi": "10.1101/2023.08.28.23294730"
    },
    {
      "title": "ChatGPT's ability to comprehend and answer cirrhosis related questions in Arabic",
      "authors": [
        "J Samaan"
      ],
      "year": 2023
    },
    {
      "title": "Quantitative evaluation of ChatGPT versus Bard responses to anaesthesia-related queries",
      "authors": [
        "S Patnaik",
        "U Hoffmann"
      ],
      "year": 2024
    },
    {
      "title": "Evaluating the performance of ChatGPT in responding to questions about endoscopic procedures for patients",
      "authors": [
        "H Ali"
      ],
      "year": 2023
    },
    {
      "title": "Utility of GPT-4 as an Informational Patient Resource in Otolaryngology",
      "authors": [
        "K Suresh"
      ],
      "year": 2023,
      "doi": "10.1101/2023.05.14.23289944"
    },
    {
      "title": "GPT-4 outperforms ChatGPT in answering non-English questions related to cirrhosis",
      "authors": [
        "Y Yeo"
      ],
      "year": 2023,
      "doi": "10.1101/2023.05.04.23289482"
    },
    {
      "title": "Assessment of ChatGPT in the Prehospital Management of Ophthalmological Emergencies -An Analysis of 10 Fictional Case Vignettes",
      "authors": [
        "D Knebel"
      ],
      "year": 2023,
      "doi": "10.1055/a-2149-0447"
    },
    {
      "title": "Can the ChatGPT and other large language models with internet-connected database solve the questions and concerns of patient with prostate cancer and help democratize medical knowledge",
      "authors": [
        "L Zhu",
        "W Mou",
        "R Chen"
      ],
      "year": 2023,
      "doi": "10.1186/s12967-023-04123-5"
    },
    {
      "title": "Evaluating the Utility of a Large Language Model in Answering Common Patients' Gastrointestinal Health-Related Questions: Are We There Yet?",
      "authors": [
        "A Lahat",
        "E Shachar",
        "B Avidan",
        "B Glicksberg",
        "E Klang"
      ],
      "year": 2023,
      "doi": "10.3390/diagnostics13111950"
    },
    {
      "title": "Comparison of ophthalmologist and large language model chatbot responses to online patient eye care questions",
      "authors": [
        "I Bernstein"
      ],
      "year": 2023
    },
    {
      "title": "Can You Prepare My Patients for [18F]FDG PET/CT and Explain My Reports",
      "authors": [
        "J Rogasch"
      ],
      "year": 2023,
      "doi": "10.2967/jnumed.123.266114"
    },
    {
      "title": "Evaluating ChatGPT Responses on Thyroid Nodules for Patient Education",
      "authors": [
        "D Campbell"
      ],
      "year": 2023,
      "doi": "10.1089/thy.2023.0491"
    },
    {
      "title": "ChatGPT and patient information in nuclear medicine: GPT-3.5 versus GPT-4",
      "authors": [
        "G Currie",
        "S Robbie",
        "P Tually"
      ],
      "year": 2023,
      "doi": "10.2967/jnmt.123.266151"
    },
    {
      "title": "Are ChatGPT's Free-Text Responses on Periprosthetic Joint Infections of the Hip and Knee Reliable and Useful?",
      "authors": [
        "A Draschl"
      ],
      "year": 2023,
      "doi": "10.3390/jcm12206655"
    },
    {
      "title": "Online patient education in body contouring: a comparison between Google and ChatGPT",
      "authors": [
        "M Alessandri-Bonetti",
        "H Liu",
        "M Palmesano",
        "V Nguyen",
        "F Egro"
      ],
      "year": 2023,
      "doi": "10.1016/j.bjps.2023.10.091"
    },
    {
      "title": "Can ChatGPT, an artificial intelligence language model, provide accurate and highquality patient information on prostate cancer?",
      "authors": [
        "B Coskun",
        "G Ocakoglu",
        "M Yetemen",
        "O Kaygisiz"
      ],
      "year": 2023,
      "doi": "10.1016/j.urology.2023.05.040"
    },
    {
      "title": "Artificial Intelligence Versus Expert Plastic Surgeon: Comparative Study Shows ChatGPT \"Wins\" Rhinoplasty Consultations: Should We Be Worried? Facial Plastic Surgery & Aesthetic Medicine",
      "authors": [
        "K Durairaj"
      ],
      "year": 2023,
      "doi": "10.1089/fpsam.2023.0224"
    },
    {
      "title": "The Use of Large Language Models to Generate Education Materials about Uveitis",
      "authors": [
        "R Kianian",
        "D Sun",
        "E Crowell",
        "E Tsui"
      ],
      "year": 2023,
      "doi": "10.1016/j.oret.2023.09.008"
    },
    {
      "title": "Exploring the Role of a Large Language Model on Carpal Tunnel Syndrome Management: An Observation Study of ChatGPT",
      "authors": [
        "I Seth"
      ],
      "year": 2023
    },
    {
      "title": "Can ChatGPT explain it? Use of artificial intelligence in multiple sclerosis communication",
      "authors": [
        "H Inojosa"
      ],
      "year": 2023,
      "doi": "10.1186/s42466-023-00270-8"
    },
    {
      "title": "Artificial intelligence chatbot performance in triage of ophthalmic conditions",
      "authors": [
        "R Lyons",
        "S Arepalli",
        "O Fromal",
        "J Choi",
        "N Jain"
      ],
      "year": 2023,
      "doi": "10.1016/j.jcjo.2023.07.016"
    },
    {
      "title": "Potential use of ChatGPT for patient information in periodontology: a descriptive pilot study",
      "authors": [
        "O Babayi\u011fit",
        "Z Tastan Eroglu",
        "D Ozkan Sen",
        "F Ucan Yarkac"
      ],
      "year": 2023,
      "doi": "10.7759/cureus.48518"
    },
    {
      "title": "ChatGPT in answering queries related to lifestyle-related diseases and disorders",
      "authors": [
        "H Mondal",
        "I Dash",
        "S Mondal",
        "J Behera"
      ],
      "year": 2023
    },
    {
      "title": "Assessing the performance of ChatGPT's responses to questions related to epilepsy: a cross-sectional study on natural language processing and medical information retrieval",
      "authors": [
        "H Kim",
        "D Shin",
        "J Kim",
        "G Lee",
        "J Cho"
      ],
      "year": 2023,
      "doi": "10.1016/j.seizure.2023.11.013"
    },
    {
      "title": "Evaluating the performance of different large language models on health consultation and patient education in urolithiasis",
      "authors": [
        "H Song"
      ],
      "year": 2023,
      "doi": "10.1007/s10916-023-02021-3"
    },
    {
      "title": "Can ChatGPT help patients answer their otolaryngology questions?",
      "authors": [
        "H Zalzal",
        "A Abraham",
        "J Cheng",
        "R Shah"
      ],
      "year": 2023,
      "doi": "10.1002/lio2.1193"
    },
    {
      "title": "ChatGPT performs strongly as a fertility counseling tool with limitations",
      "authors": [
        "J Chervenak",
        "H Lieman",
        "M Blanco-Breindel",
        "S Jindal"
      ],
      "year": 2023,
      "doi": "10.1038/s43856-024-00717-2information"
    },
    {
      "title": "ChatGPT, can you help me save my child's life?\"diagnostic accuracy and supportive capabilities to lay rescuers by ChatGPT in prehospital basic life support and paediatric advanced life support cases -an in-silico analysis",
      "authors": [
        "S Bushuven"
      ],
      "year": 2023,
      "doi": "10.1007/s10916-023-02019-x"
    },
    {
      "title": "ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports",
      "authors": [
        "K Jeblick"
      ],
      "year": 2023,
      "doi": "10.1007/s00330-023-10213-1"
    },
    {
      "title": "Assessing the accuracy of responses by the language model ChatGPT to questions regarding bariatric surgery",
      "authors": [
        "J Samaan"
      ],
      "year": 2023,
      "doi": "10.1007/s11695-023-06603-5"
    },
    {
      "title": "Exploring ChatGPT's potential for consultation, recommendations and report diagnosis: gastric cancer and gastroscopy reports' case",
      "authors": [
        "J Zhou",
        "T Li",
        "S Fong",
        "N Dey",
        "R Crespo"
      ],
      "year": 2023
    },
    {
      "title": "Toward improving health literacy in patient education materials with neural machine translation models",
      "authors": [
        "D Oniani"
      ],
      "year": 2023
    },
    {
      "title": "The future of patient education: ai-driven guide for type 2 diabetes",
      "authors": [
        "C Hernandez"
      ],
      "year": 2023
    },
    {
      "title": "Is ChatGPT accurate and reliable in answering questions regarding head and neck cancer?",
      "authors": [
        "O Ku\u015fcu",
        "A Pamuk",
        "N S\u00fctay S\u00fcsl\u00fc",
        "S Hosal"
      ],
      "year": 2023,
      "doi": "10.3389/fonc.2023.1256459"
    },
    {
      "title": "Assessing the utility of ChatGPT as an artificial intelligencebased large language model for information to answer questions on myopia",
      "authors": [
        "S Biswas",
        "N Logan",
        "L Davies",
        "A Sheppard",
        "J Wolffsohn"
      ],
      "year": 2023
    },
    {
      "title": "Exploring the potential of Chat-GPT as a supportive tool for sialendoscopy clinical decision making and patient information support",
      "authors": [
        "C Chiesa-Estomba"
      ],
      "year": 2023,
      "doi": "10.1007/s00405-023-08104-8"
    },
    {
      "title": "Large language model-based chatbot vs surgeongenerated informed consent documentation for common procedures",
      "authors": [
        "H Decker"
      ],
      "year": 2023
    },
    {
      "title": "Exploring the potential of ChatGPT as a supplementary tool for providing orthopaedic information",
      "authors": [
        "J Kaarre"
      ],
      "year": 2023
    },
    {
      "title": "Evaluation of ChatGPT dermatology responses to common patient queries",
      "authors": [
        "A Ferreira",
        "B Chu",
        "J Grant-Kels",
        "T Ogunleye",
        "J Lipoff"
      ],
      "year": 2023
    },
    {
      "title": "A pilot study on the efficacy of GPT-4 in providing orthopedic treatment recommendations from MRI reports",
      "authors": [
        "D Truhn"
      ],
      "year": 2023
    },
    {
      "title": "Evaluation High-Quality of Information from ChatGPT (Artificial Intelligence-Large Language Model) Artificial Intelligence on Shoulder Stabilization Surgery",
      "authors": [
        "E Hurley"
      ],
      "year": 2023,
      "doi": "10.1016/j.arthro.2023.07.048"
    },
    {
      "title": "Reliability and usefulness of ChatGPT for inflammatory bowel diseases: an analysis for patients and healthcare professionals",
      "authors": [
        "R Cankurtaran",
        "Y Polat",
        "N Aydemir",
        "E Umay",
        "O Yurekli"
      ],
      "year": 2023
    },
    {
      "title": "Large language model (LLM)-powered chatbots fail to generate guideline-consistent content on resuscitation and may provide potentially harmful advice",
      "authors": [
        "A Birkun",
        "A Gautam"
      ],
      "year": 2023
    },
    {
      "title": "Popular large language model chatbots' accuracy, comprehensiveness, and self-awareness in answering ocular symptom queries",
      "authors": [
        "K Pushpanathan"
      ],
      "year": 2023,
      "doi": "10.1016/j.isci.2023.108163"
    },
    {
      "title": "Appropriateness and comprehensiveness of using ChatGPT for perioperative patient education in thoracic surgery in different language contexts: survey study",
      "authors": [
        "C Shao"
      ],
      "year": 2023
    },
    {
      "title": "Accuracy of ChatGPT-Generated Information on Head and Neck and Oromaxillofacial Surgery: A Multicenter Collaborative Analysis",
      "authors": [
        "L Vaira"
      ],
      "year": 2023,
      "doi": "10.1002/ohn.489"
    },
    {
      "title": "Use of artificial intelligence Chatbots for cancer treatment information",
      "authors": [
        "S Chen"
      ],
      "year": 2023
    },
    {
      "title": "BPPV Information on Google Versus AI (ChatGPT)",
      "authors": [
        "J Bellinger"
      ],
      "year": 2023,
      "doi": "10.1002/ohn.506"
    },
    {
      "title": "Validity of the large language model ChatGPT (GPT4) as a patient information source in otolaryngology by a variety of doctors in a tertiary otorhinolaryngology department",
      "authors": [
        "J Nielsen",
        "C Von Buchwald",
        "C Gr\u00f8nh\u00f8j"
      ],
      "year": 2023
    },
    {
      "title": "Clinical accuracy of large language models and google search responses to postpartum depression questions: cross-sectional study",
      "authors": [
        "E Sezgin",
        "F Chekeni",
        "J Lee",
        "S Keim"
      ],
      "year": 2023
    },
    {
      "title": "Current Strengths and Weaknesses of ChatGPT as a Resource for Radiation Oncology Patients and Providers",
      "authors": [
        "W Floyd"
      ],
      "year": 2023,
      "doi": "10.1016/j.ijrobp.2023.10.020"
    },
    {
      "title": "Dr ChatGPT\": is it a reliable and useful source for common rheumatic diseases?",
      "authors": [
        "C Uz",
        "E Umay"
      ],
      "year": 2023
    },
    {
      "title": "The potential of chatbots in chronic venous disease patient management",
      "authors": [
        "A Athavale",
        "J Baier",
        "E Ross",
        "E Fukaya"
      ],
      "year": 2023,
      "doi": "10.1016/j.jvsvi.2023.100019"
    },
    {
      "title": "ChatDoctor: a medical chat model fine-tuned on a large language model meta-AI (LLaMA) using medical domain knowledge",
      "authors": [
        "Y Li"
      ],
      "year": 2023
    },
    {
      "title": "Comparing the efficacy of large language models ChatGPT, BARD, and Bing AI in providing information on rhinoplasty: an observational study",
      "authors": [
        "I Seth"
      ],
      "year": 2023
    },
    {
      "title": "Evaluation of a chat GPT generated patient information leaflet about laparoscopic cholecystectomy",
      "authors": [
        "E Lockie",
        "J Choi"
      ],
      "year": 2023,
      "doi": "10.1111/ans.18834"
    },
    {
      "title": "Use of ChatGPT, GPT-4, and Bard to improve readability of ChatGPT's answers to common questions about lung cancer and lung cancer screening",
      "authors": [
        "H Haver",
        "C Lin",
        "A Sirajuddin",
        "P Yi",
        "J Jeudy"
      ],
      "year": 2023
    },
    {
      "title": "Decoding radiology reports: potential application of OpenAI ChatGPT to enhance patient understanding of diagnostic reports",
      "authors": [
        "H Li"
      ],
      "year": 2023
    },
    {
      "title": "Feasibility of GPT-3 and GPT-4 for in-depth patient education prior to interventional radiological procedures: a comparative analysis",
      "authors": [
        "M Scheschenja"
      ],
      "year": 2024
    },
    {
      "title": "Enhancing patient communication With Chat-GPT in radiology: evaluating the efficacy and readability of answers to common imaging-related questions",
      "authors": [
        "E Gordon"
      ],
      "year": 2024
    },
    {
      "title": "Large language models: Are artificial intelligence-based chatbots a reliable source of patient information for spinal surgery?",
      "authors": [
        "A Stroop"
      ],
      "year": 2023,
      "doi": "10.1007/s00586-023-07975-z"
    },
    {
      "title": "ChatGPT in the development of medical questionnaires. The example of the low back pain",
      "authors": [
        "D Coraci"
      ],
      "year": 2023,
      "doi": "10.4081/ejtm.2023.12114"
    },
    {
      "title": "Doctor Versus Artificial Intelligence: Patient and Physician Evaluation of Large Language Model Responses to Rheumatology Patient Questions in a Cross-Sectional Study",
      "authors": [
        "C Ye",
        "E Zweck",
        "Z Ma",
        "J Smith",
        "S Katz"
      ],
      "doi": "10.1002/art.42737"
    },
    {
      "title": "Validity and reliability of artificial intelligence chatbots as public sources of information on endodontics",
      "authors": [
        "H Mohammad-Rahimi"
      ],
      "year": 2024,
      "doi": "10.1038/s43856-024-00717-2"
    },
    {
      "title": "Let's chat about cervical cancer: Assessing the accuracy of ChatGPT responses to cervical cancer questions",
      "authors": [
        "C Hermann"
      ],
      "year": 2023
    },
    {
      "title": "Accuracy of ChatGPT in Common Gastrointestinal Diseases: Impact for Patients and Providers",
      "authors": [
        "A Kerbage"
      ],
      "year": 2023,
      "doi": "10.1016/j.cgh.2023.11.008"
    },
    {
      "title": "Generating Informed Consent Documents Related to Blepharoplasty Using ChatGPT",
      "authors": [
        "M Shiraishi"
      ],
      "year": 2023,
      "doi": "10.1097/iop.0000000000002574"
    },
    {
      "title": "Quality and Agreement With Scientific Consensus of ChatGPT Information Regarding Corneal Transplantation and Fuchs Dystrophy",
      "authors": [
        "K Barclay"
      ],
      "year": 2023,
      "doi": "10.1097/ico.0000000000003439"
    },
    {
      "title": "Can Large Language Models Safely Address Patient Questions Following Cataract Surgery?",
      "authors": [
        "A Qarajeh"
      ],
      "year": 2023
    },
    {
      "title": "Development and evaluation of aeyeconsult: a novel ophthalmology chatbot leveraging verified textbook knowledge and GPT-4",
      "authors": [
        "M Singer",
        "J Fu",
        "J Chow",
        "C Teng"
      ],
      "year": 2024
    },
    {
      "title": "Aesthetic surgery advice and counseling from artificial intelligence: a rhinoplasty consultation with ChatGPT",
      "authors": [
        "Y Xie"
      ],
      "year": 2023,
      "doi": "10.1007/s00266-023-03338-7"
    },
    {
      "title": "A vignette-based evaluation of ChatGPT's ability to provide appropriate and equitable medical advice across care contexts",
      "authors": [
        "A Nastasi",
        "K Courtright",
        "S Halpern",
        "G Weissman"
      ],
      "year": 2023
    },
    {
      "title": "Can ChatGPT be Your Personal Medical Assistant?",
      "authors": [
        "M Biswas",
        "A Islam",
        "Z Shah",
        "W Zaghouani",
        "S Brahim Belhaouari"
      ],
      "year": 2023
    },
    {
      "title": "Evaluating the Potential of LLMs and ChatGPT on Medical Diagnosis and Treatment",
      "authors": [
        "D Panagoulias",
        "F Palamidas",
        "M Virvou",
        "G Tsihrintzis"
      ],
      "year": 2023
    },
    {
      "title": "Utility of allergen-specific patient-directed handouts generated by chat generative pretrained transformer",
      "authors": [
        "A Chandra",
        "M Davis",
        "D Hamann",
        "C Hamann"
      ],
      "year": 2023
    },
    {
      "title": "Comparison of patient education materials generated by chat generative pretrained transformer versus experts: an innovative way to increase readability of patient education materials",
      "authors": [
        "Y.-C Hung",
        "S Chaker",
        "M Sigel",
        "M Saad",
        "E Slater"
      ],
      "year": 2023,
      "doi": "10.1055/a-2219-4901"
    },
    {
      "title": "Testing ChatGPT ability to answer laypeople questions about cardiac arrest and cardiopulmonary resuscitation",
      "authors": [
        "T Scquizzato"
      ],
      "year": 2024,
      "doi": "10.1016/j.resuscitation.2023.110077"
    },
    {
      "title": "A large language model artificial intelligence for patient queries in atopic dermatitis",
      "authors": [
        "P Sulejmani"
      ],
      "doi": "10.1111/jdv.19737"
    },
    {
      "title": "ChatGPT in nuclear medicine education",
      "authors": [
        "G Currie",
        "K Barry"
      ],
      "year": 2023
    },
    {
      "title": "Academic integrity and artificial intelligence: is ChatGPT hype, hero or heresy?",
      "authors": [
        "G Currie"
      ],
      "year": 2023
    },
    {
      "title": "ChatGPT in healthcare: a taxonomy and systematic review",
      "authors": [
        "J Li",
        "A Dada",
        "B Puladi",
        "J Kleesiek",
        "J Egger"
      ],
      "year": 2024
    },
    {
      "title": "Exploring Vulnerabilities and Protections in Large Language Models: A Survey",
      "authors": [
        "F Liu",
        "C Hu"
      ],
      "year": 2024
    },
    {
      "title": "Capabilities of gpt-4 on medical challenge problems",
      "authors": [
        "H Nori",
        "N King",
        "S Mckinney",
        "D Carignan",
        "E Horvitz"
      ],
      "year": 2023
    },
    {
      "title": "Comparing ChatGPT and GPT-4 performance in USMLE soft skill assessments",
      "authors": [
        "D Brin"
      ],
      "year": 2023,
      "doi": "10.1038/s41598-023-43436-9"
    },
    {
      "title": "ChatGPT passes German State examination in medicine with picture questions omitted",
      "authors": [
        "L Jung"
      ],
      "year": 2023,
      "doi": "10.3238/arztebl.m2023.0113"
    },
    {
      "title": "Performance of ChatGPT on a radiology board-style examination: insights into current strengths and limitations",
      "authors": [
        "R Bhayana",
        "S Krishna",
        "R Bleakney"
      ],
      "year": 2023,
      "doi": "10.1148/radiol.230582"
    },
    {
      "title": "Towards expert-level medical question answering with large language models",
      "authors": [
        "K Singhal"
      ],
      "year": 2023
    },
    {
      "title": "Biomedical Large Languages Models Seem not to be Superior to Generalist Models on Unseen Medical Data",
      "authors": [
        "F Dorfner"
      ],
      "year": 2024
    },
    {
      "title": "Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models",
      "authors": [
        "T Kung"
      ],
      "year": 2023
    },
    {
      "title": "Promises and pitfalls of artificial intelligence for legal applications",
      "authors": [
        "S Kapoor",
        "P Henderson",
        "A Narayanan"
      ],
      "year": 2024,
      "doi": "10.2139/ssrn.4695412"
    },
    {
      "title": "LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation",
      "authors": [
        "S Ouyang",
        "J Zhang",
        "M Harman",
        "M Wang"
      ],
      "year": 2023
    },
    {
      "title": "LLM Stability: A detailed analysis with some surprises",
      "authors": [
        "B Atil"
      ],
      "year": 2024
    },
    {
      "title": "Health Literacy: A Manual for Clinicians",
      "authors": [
        "B Weis"
      ],
      "year": 2003
    },
    {
      "title": "Explainability for artificial intelligence in healthcare: a multidisciplinary perspective",
      "authors": [
        "J Amann"
      ],
      "year": 2020
    },
    {
      "title": "Ethical and legal challenges of artificial intelligence-driven healthcare",
      "authors": [
        "S Gerke",
        "T Minssen",
        "G Cohen"
      ],
      "doi": "10.1016/b978-0-12-818438-7.00012-5"
    },
    {
      "title": "Fully informed consent can be needlessly cruel",
      "authors": [
        "J Tobias",
        "R Souhami"
      ],
      "year": 1993
    },
    {
      "title": "The reality of informed consent: empirical studies on patient comprehension-systematic review",
      "authors": [
        "T Pietrzykowski",
        "K Smilowska"
      ],
      "year": 2021
    },
    {
      "title": "Mandatory disclaimers on dietary supplements do not reliably communicate the intended issues",
      "authors": [
        "A Kesselheim",
        "J Connolly",
        "J Rogers",
        "J Avorn"
      ],
      "year": 2015
    },
    {
      "title": "High-performance medicine: the convergence of human and artificial intelligence",
      "authors": [
        "E Topol"
      ],
      "year": 2019
    },
    {
      "title": "Semantic consistency for assuring reliability of large language models",
      "authors": [
        "H Raj",
        "V Gupta",
        "D Rosati",
        "S Majumdar"
      ],
      "year": 2023
    },
    {
      "title": "Large Language Models Are Human-Level Prompt Engineers",
      "authors": [
        "Y Zhou"
      ],
      "year": 2022
    },
    {
      "title": "Certified Robustness for Large Language Models with Self-Denoising",
      "authors": [
        "Z Zhang"
      ],
      "year": 2023
    },
    {
      "title": "Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology -a recent scoping review",
      "authors": [
        "E Ullah",
        "A Parwani",
        "M Baig",
        "R Singh"
      ],
      "year": 2024,
      "doi": "10.1038/s43856-024-00717-2138"
    },
    {
      "title": "Biases in large language models: origins, inventory, and discussion",
      "authors": [
        "R Navigli",
        "S Conia",
        "B Ross"
      ],
      "year": 2023
    },
    {
      "title": "Jailbreaker: Automated jailbreak across multiple large language model chatbots",
      "authors": [
        "G Deng"
      ],
      "year": 2023
    },
    {
      "title": "Large language models show humanlike content biases in transmission chain experiments",
      "authors": [
        "A Acerbi",
        "J Stubbersfield"
      ],
      "year": 2023
    },
    {
      "title": "Adaptive data debiasing through bounded exploration",
      "authors": [
        "Y Yang",
        "Y Liu",
        "P Naghizadeh"
      ],
      "year": 2022
    },
    {
      "title": "Bias and Fairness in Large Language Models: A Survey",
      "authors": [
        "I Gallegos"
      ],
      "doi": "10.1162/coli_a_00524"
    },
    {
      "title": "On the Fairness ROAD: Robust Optimization for Adversarial Debiasing",
      "authors": [
        "V Grari",
        "T Laugel",
        "T Hashimoto",
        "S Lamprier",
        "M Detyniecki"
      ],
      "year": 2023,
      "doi": "10.1007/s10994-022-06206-8"
    },
    {
      "title": "AI generates covertly racist decisions about people based on their dialect",
      "authors": [
        "V Hofmann",
        "P Kalluri",
        "D Jurafsky",
        "S King"
      ],
      "year": 2024
    },
    {
      "title": "Head-to-Head Comparison of ChatGPT Versus Google Search for Medical Knowledge Acquisition",
      "authors": [
        "N Ayoub",
        "Y Lee",
        "D Grimm",
        "V Divi"
      ],
      "doi": "10.1002/ohn.465"
    },
    {
      "title": "A survey on large language model (LLM) security and privacy: the good, the bad, and the ugly",
      "authors": [
        "Y Yao"
      ],
      "year": 2024
    },
    {
      "title": "Medical AI and human dignity: contrasting perceptions of human and artificially intelligent (AI) decision making in diagnostic and medical resource allocation contexts",
      "authors": [
        "A Tierney"
      ],
      "year": 2022
    },
    {
      "title": "Patient perspectives on the use of artificial intelligence in health care: a scoping review",
      "authors": [
        "S Moy"
      ],
      "year": 2024
    },
    {
      "title": "Navigating the european union artificial intelligence act for healthcare",
      "authors": [
        "F Busch"
      ],
      "year": 2024
    },
    {
      "authors": [
        "Else Kroener"
      ]
    },
    {
      "title": "busch@tum",
      "authors": [
        "Lisa Adams",
        "Keno Bressem"
      ]
    }
  ],
  "num_references": 147
}
