{
  "paper_id": "P65CBEJG",
  "title": "Candidates undertaking (invigilated) assessment online show no differences in performance compared to those undertaking assessment offline",
  "abstract": "Background: Medical education has historically relied on high stakes knowledge tests sat in examination centres with invigilators monitoring academic malpractice. The COVID-19 pandemic has made such examination formats impossible, and medical educators have explored the use of online assessments as a potential replacement. This shift has in turn led to fears that the change in format or academic malpractice might lead to considerably higher attainment scores on online assessment with no underlying improvement in student competence. Method: Here, we present an analysis of 8092 sittings of the Prescribing Safety Assessment (PSA), an assessment designed to test the prescribing skills of final year medical students in the UK. Inperson assessments for the PSA were cancelled partway through the academic year 2020, with 6048 sittings delivered in an offline, traditionally invigilated format, and then 2044 sittings delivered in an online, webcam invigilated format. Results: A comparison (able to detect very small effects) showed no attainment gap between online (M \u00bc 0.762, SD \u00bc 0.34) and offline (M \u00bc 0.761, SD \u00bc 0.34) performance. Conclusions: The finding suggests that the transition to online assessment does not affect student performance. The findings should increase confidence in the use of online testing in highstakes assessment.",
  "year": 2007,
  "date": "2007",
  "journal": "Encycl Measur Stat",
  "publication": "Encycl Measur Stat",
  "authors": [
    {
      "forename": "David",
      "surname": "Hope",
      "name": "David Hope",
      "affiliation": "a  Medical Education Unit , College of Medicine and Veterinary Medicine , The University of Edinburgh , Edinburgh , United Kingdom; \n\t\t\t\t\t\t\t\t Medical Education Unit \n\t\t\t\t\t\t\t\t College of Medicine and Veterinary Medicine \n\t\t\t\t\t\t\t\t The University of Edinburgh \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Edinburgh \n\t\t\t\t\t\t\t\t\t United Kingdom;",
      "email": "david.hope@ed.ac.uk"
    },
    {
      "forename": "Veronica",
      "surname": "Davids",
      "name": "Veronica Davids",
      "affiliation": "b  Medical Schools Council , London , United Kingdom; \n\t\t\t\t\t\t\t\t Medical Schools Council \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t United Kingdom;"
    },
    {
      "forename": "Lynne",
      "surname": "Bollington",
      "name": "Lynne Bollington",
      "affiliation": "c  Prescribing Safety Assessment , British Pharmacological Society , London , United Kingdom; \n\t\t\t\t\t\t\t\t Prescribing Safety Assessment \n\t\t\t\t\t\t\t\t British Pharmacological Society \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t London \n\t\t\t\t\t\t\t\t\t United Kingdom;"
    },
    {
      "forename": "Simon",
      "surname": "Maxwell",
      "name": "Simon Maxwell",
      "affiliation": "d  Internal Medicine Office , Medical Education Centre , Western General Hospital , Edinburgh , United Kingdom \n\t\t\t\t\t\t\t\t Internal Medicine Office \n\t\t\t\t\t\t\t\t Medical Education Centre \n\t\t\t\t\t\t\t\t Western General Hospital \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Edinburgh \n\t\t\t\t\t\t\t\t\t United Kingdom"
    },
    {
      "affiliation": "Medical Education Unit , College of Medicine and Veterinary Medicine , The University of Edinburgh , The Chancellor's Building , 49 Little France Crescent , Edinburgh , EH16 4SB Scotland , United Kingdom \u00df 2021 \n\t\t\t\t\t\t\t\t Medical Education Unit \n\t\t\t\t\t\t\t\t College of Medicine and Veterinary Medicine \n\t\t\t\t\t\t\t\t The University of Edinburgh \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t The Chancellor's Building 49 Little France Crescent \n\t\t\t\t\t\t\t\t\t EH16 4SB \u00df 2021 \n\t\t\t\t\t\t\t\t\t Edinburgh Scotland \n\t\t\t\t\t\t\t\t\t United Kingdom"
    },
    {
      "affiliation": "Informa UK Limited , \n\t\t\t\t\t\t\t\t Informa UK Limited"
    }
  ],
  "doi": "10.1080/0142159x.2021.1887467",
  "keywords": [
    "Assessment",
    "psychometrics",
    "invigilation",
    "academic misconduct",
    "COVID-19"
  ],
  "sections": [
    {
      "title": "Introduction",
      "text": "The COVID-19 global pandemic has heavily disrupted every aspect of medical education. New social distancing requirements designed to promote safety have necessitated halting campus-based work, reduced clinical teaching, and delayed or even cancelled assessment  (Cleland et al. 2020; Lee et al. 2020) . It is crucial that new doctors continue to enter the workforce and undergo adequate training: a key current objective for medical educators is therefore to reconcile ongoing educational needs with an entirely novel set of practical challenges.\n\nMedical education research has always been constrained by practical issues  (Todres et al. 2007; Cook 2012) . Many innovations go unexplored because novel techniques are too expensive or the risks of a failed intervention too severe. However, the pandemic has thrust us into a 'there is no alternative' environment where entirely new approaches are being tested dailyoften with great uncertainty as to their utility  (Fuller et al. 2020) . Medical educators have a unique opportunity to test new ideas and create long-lasting improvementsassuming the new innovations can be properly investigated and disseminated  (Cleland et al. 2020; Gibbs 2020) . Rapid evaluation is especially urgent as many authors have noted significant concerns among educators, students and regulators about the rapid shift to under-evaluated methods  (Eachempati and Ramnarayan 2020; Jervis and Brown 2020) . Documentation will be essential to reassure stakeholders of the utility of novel approaches and to ensure successful changes are retained in the coming years. The evaluation of assessment changes near graduation will be especially sensitive. Regulators have an ongoing obligation to ensure new graduates are safe and competent  (General Medical Council 2015)  and, as assessment acts as a gateway to clinical practice and higher stages of training, new approaches must be carefully evidenced."
    },
    {
      "title": "Practice points",
      "text": "A comparison shows no performance gap between online and offline assessment despite different forms of invigilation. The large multi-site sample increases confidence that the findings are generalisable throughout undergraduate medical education. Pooling assessment data across institutions allows for rapid and reliable evaluation of innovations. Online assessments may be defensible even in high-stakes settings. Investigating the student experience of online assessmentespecially invigilationis an important priority.\n\nHistorically, written assessment in medical education has relied heavily on multiple-choice questions  (Collins 2006; Palmer and Devitt 2007; Brunnquell et al. 2011)  sat in an examination centre with invigilators present to prevent academic misconduct. These methods are no longer viable due to social distancing  (Fuller et al. 2020) . The emerging alternativethe delivery of assessment in an online format without invigilationremains relatively untested, and concern has particularly focused on the risk of academic malpractice.\n\n'Academic malpractice' is a broad term describing a range of inappropriate behaviours including sharing answers among candidates, one candidate producing work on behalf of another candidate, candidates claiming to have completed work that has never been undertaken, and plagiarismall of which medical students admit to doing at varying rates  (Rennie and Crosby 2001) . The evidence base contrasting offline against online assessment is not large  (Bengtsson 2019) . Some researchers within higher education have suggested that student cohorts sitting non-invigilated assessment experience higher variability compared to invigilated assessment, but no difference in average performance  (Hollister and Berenson 2009) . Other researchers have found that performance increases considerably (on the order of 10-20%) in the absence of in-person invigilation  (Daffin and Jones 2018) . These changes may be reconcilable by other studies showing gaps between online and offline assessment may take time to emerge, so that initially undetectable differences become significant over multiple diets  (Chen et al. 2020) .\n\nWithin medical education, research using a simulated OSCE exam security breach suggests student performance changes significantly with prior exposure to material  (Gotzmann et al. 2017 ) and this is similarly true for written assessment  (Joncas et al. 2018) . By contrast, very recent evidence in medical education written assessment has suggested no difference between online and offline formats  (Sam et al. 2020) .\n\nThe available evidence, then, is highly contradictory, and there is no relevant literature comparing invigilated and un-invigilated high-stakes remote examinations. The effect sizes between online and offline assessment typically range from non-existent to moderate, with some emerging immediately and others emerging only after a prolonged period of time. It is not clear whether medical education should expect to see similar trends to the rest of higher education, or if the nature of the content or study techniques makes medical education somehow unique. Some researchers argue malpractice can be heavily mitigated against by randomisation, focusing on higher-order cognitive skills, or by inculcating a culture of professionalism into students  (Hollister and Berenson 2009; Sam et al. 2020) . Others have focused on the need to introduce remote invigilation to online assessment to reduce such concernsan expensive and possibly invasive action that may or may not be equivalent to traditional in-person invigilation  (Hylton et al. 2016; Lee et al. 2020) . Medical educators must quickly develop an evidence base exploring where and how written assessment exhibit online vs. offline attainment gaps and whether such gaps can be explained by academic malpractice or other factors.\n\nInevitably, comparisons across formats are challenging when there are multiple sources of possible error. One option to reduce error is to make use of identical, shared content that repeats across multiple assessments. These 'anchor items' are selected for high item quality  (Pibal and Cesnik 2011)  and can be used to compare the equivalence of assessments even when the majority of the content is not shared. Importantly, a comparison of anchor items allows for the elimination of errors due to differences in content.\n\nHere, we present a large (n \u00bc 8092) study in which finalyear medical students sat a multi-site prescribing assessment. We present a comparison of student performance on identical content (anchor items) in an offline, traditionally invigilated format against an online, webcam invigilated format and report on mean differences and variability. This analysis allows us to test whether performance varies depending on whether candidates are assessed in an online or offline format."
    },
    {
      "title": "Materials and methods"
    },
    {
      "title": "Participants",
      "text": "Final year medical students from 34 UK medical schools sat the Prescribing Safety Assessment (PSA) on either 3d February or 13th March 2020 for a total of 6048 sittings. Two further assessment dates were cancelled due to the impact of social distancing as it was not possible to organise traditional in-person, invigilated assessments at that time. Instead, medical schools who planned to sit in later diets were given the opportunity to take an online paper on either 11th May or 5th June 2020, for a total of 2044 sittings. Medical schools were individually responsible for invigilation, which involved using any applicable video conferencing or online proctoring system to maintain continuous visual contact with students throughout the examination. The offline and online forms represented 8092 sittings in total. Of these sittings, 7606 (94%) were first attempts and 486 (6%) were resits. All individual attempts were included in the analyses described here. While international students and foundation doctors also sit the PSA, they are not included in this dataset."
    },
    {
      "title": "Procedure",
      "text": "The PSA tests prescribing skills in a sixty item, 200-mark assessment designed to take two hours to complete. Items are designed to test prescribing, prescribing reviews, planning and management, providing information about medicine, calculation skills, an understanding of adverse drug reactions, drug monitoring, and data interpretation. Eight general prescribing items are scored out of ten and eight prescribing review items are scored out of four. Each of these items allows for a partially correct response. The remaining 44 items are each scored out of two and are dichotomous: the candidate is either fully correct or incorrect. Items are presented sequentially, and not randomised for each candidate.\n\nItems are standard set by the PSA using the Modified Angoff method  (Ricker 2006 ) whereby experts judge the proportion of borderline passing students capable of answering each item correctly from zero to one and then total the results to create an overall pass mark. Items are periodically reviewed, and a detailed psychometric report using item response theory helps identify problem items and evaluate overall reliability.\n\nPSA items are allocated to papers based on a blueprint to ensure a representative array of items. Rather than a single paper, the PSA consists of four separate papers (hereafter papers A, B, C and D), each with mostly unique content. To ensure the comparability of standards, a series of anchor items are embedded in each paper. Anchor items are selected from each item type and chosen because they have been previously found to discriminate effectively and have acceptable difficulty  (Pibal and Cesnik 2011) . Anchor items are paired across papers, so some items are shared between paper A and paper B, while others are shared between paper A and paper C and so on. Through this network of shared items, it is possible to evaluate the equivalence of papers even for unique content, using the anchor items as a starting point for cross-paper comparisons."
    },
    {
      "title": "Context",
      "text": "Schools (rather than individual candidates) are allocated to papers with several constraints. Schools typically have a preferred date for sitting the assessment. Some large schools need multiple papers (generally one for a morning and one for an afternoon) as their facilities are not sufficiently large to deliver the PSA to the entire cohort in one sitting. After factoring in these requirements, the PSA allocates schools to papers based on size and past performance. For the 2020 cohort, each of the four papers were sat by around the same number of candidates, and by an equal number of historically high-and low-scoring schools. Papers A-D were sat in testing centres with invigilation using an electronic delivery system.\n\nThe two online papers (hereafter papers E and F) were constructed after the cancellation of the later PSA sittings. The papers are made of previously used items that have demonstrated acceptable psychometric characteristics  (Mellenbergh 2011) . Papers E and F were blueprinted in the usual way, use the same standards as the offline papers, and share a significant number of anchor items with papers A-D. Students were free to sit the assessment in any location but were observed continuously via webcam as a form of invigilation.\n\nThe PSA board evaluated the reliability and validity of all papers at the end of the academic year and ratified the results. A fuller description of PSA content and processes has already been published  (Maxwell et al. 2017) . Ethical approval for this work was granted the University of Edinburgh Medical Education Ethics Committee. No identifiable student data was included at any point during the project."
    },
    {
      "title": "Statistical analyses",
      "text": "Table  1  provides an overview of the key statistics for each paper. We estimated reliability for each paper using Cronbach's alpha (a measure of internal consistency) and Standard Error of Measurement (SEM), an estimate of how accurately the assessment can identify the candidate's true score  (Cronbach 1951; Bland and Altman 1997; McManus 2012) . All values were considered acceptable for highstakes assessment and the alpha values did not differ significantly between papers. The pass rates per paper were similar to each other and to pass rates in 2019. The one exception to this was paper C: closer investigation suggested a slightly larger number of historically lower-scoring schools had been allocated to this paper. We then explored the anchor items in more detail to test whether the papers were equivalent. As items were scored out of 2, 4 or 10, we standardised all items to a facility score with 0 indicating a completely wrong answer and one indicating a completely correct answer. A Levene's test for heterogeneity of variance  (Levene 1960; Hines and Hines 2000)  showed a significant difference in exam variability across papers (F \u00bc 23.24, df \u00bc 5, p \u00bc 0.01), but as subsequent analyses only investigated anchor item performance this did not affect the results."
    },
    {
      "title": "Results",
      "text": "A summary of the anchor items is provided in Table  2 . For each pair, we identified the shared anchor items (ranging from eight to 32). For each paper, we then calculated the mean and standard deviation of the anchor item facility scores. After confirming the suitability of the data for parametric testing, we compared each pair via t-tests, corrected for multiple comparisons  (Abdi 2007 ) and reported effect sizes for significant associations  (Cohen 1992) .\n\nFive significant associations were detected, four of which involved paper C. Of the nine associations comparing online and offline delivery, two were significant, and in each case the offline paper performance was slightly higher. Across all anchor items, candidates sitting online anchor items (M \u00bc 0.762, SD \u00bc 0.34) very slightly outperformed candidates sitting offline items (M \u00bc 0.761, SD \u00bc 0.34), but the difference was non-significant and would equate to a difference of less than one mark on the assessment. Similarly, we identified no significant differences in variability. All effect sizes recorded were small (ds of around .1, with the threshold for small effects being .2). There was no evidence of performance change as a result of moving to online assessment."
    },
    {
      "title": "Discussion",
      "text": "In a large study involving thirty-four medical schools, with sufficient power to detect small effects, we identified no differences between online, traditionally invigilated assessment and offline, webcam invigilated assessment. No differences were observed in either mean performance or variability. This finding increases confidence that online assessment can replace traditional assessment formats.\n\nOur work adds to an already somewhat contradictory field. The findings confirm research showing that there are limited differences between online and offline formats  (Sam et al. 2020 ) and may support evidence that complex, cognitively challenging assessment reduces the practical risks of academic misconduct. Or, it may be that medical students have higher level of probity than that found in some other fields  (Hollister and Berenson 2009) . The present study uses a different methodology to some past research which suggests attainment gaps between online and offline assessment  (Gotzmann et al. 2017 ) and our study cannot test whether such gaps might emerge after repeated non-invigilated sittings  (Joncas et al. 2018; Chen et al. 2020) .\n\nThis study has several strengths. It is large (with the ability to detect very small effects), compared a large number of identical anchor items across many papers, and across many medical schools. Importantly, the multi-site nature of the design gives reassurance that a cohort across an entire country shows broadly similar results across multiple papers: it does not appear to be the case that some groups are exhibiting significantly different responses in the transition to online assessment. Furthermore, the content of the assessment tests a practical, important skillset relevant to work as a doctorprescribing. If an assessment testing skills so relevant to everyday practice exhibit no differences in online vs. offline attainment scores, it increases our confidence that similar tests will show similar results. Medical educators can then be more confident of high-stakes assessment results derived from online assessment.\n\nHowever, the study has some limitations. It is a snapshot of a single cohort, and so cannot evaluate whether differences might emerge after repeated deliveries. In particular, the recording and circulation of confidential assessment material would only alter the results for later examinations. It only explores candidates at UK medical schools, and so it is not known how generalisable such results are to other contexts. The analysis only directly evaluates the anchor items and does not attempt to compare the non-shared assessment. Finally, the PSA pass rate has historically been quite high: it is not clear what might happen on an assessment where a large minority of students expect to fail.\n\nThese issues logically suggest further avenues for research. Monitoring online assessment year-on-year is feasible and will rapidly demonstrate whether changes in attainment are taking place. Replicating such work in assessments with lower pass rates will be very valuable (as the incentives for malpractice may be higher), as will contrasting online assessments with and without invigilation. Such research will allow medical educators to very quickly identify whether or not online assessment is a suitable method for high-stakes assessment for the future and the relative importance of invigilation.\n\nAny migration to online assessment must be carefully considered in light of the evidence. Assessors must review how different groups experience this migration, and how to routinely evaluate the change. Attainment gaps between groups are complex and often poorly understood  (Woolf et al. 2016; Woolf 2020)  and assessors should consider the potential impact of changes before implementation. Finally, online invigilation must be evaluated for efficacy while being considerate of candidate privacy  (Hylton et al. 2016) . In summary, the present study represents an important contribution to our understanding of what happens when we rapidly transition offline, traditionally invigilated assessments to online, webcam-based invigilation. The absence of observable attainment gaps is encouraging for the defensibility of such assessment and our confidence in using online assessment formats for the future."
    },
    {
      "title": "Glossary",
      "text": "Anchor Items: A set of identical items deployed across multiple assessments. The use of anchor items enables comparisons between cohorts and can be used to directly compare performance (or progress) even when a majority of the items on each assessment are not shared. Pibal F, Cesnik HS. 2011 . Evaluating the quantity-quality tradeoff in the selection of anchor items: a vertical scaling approach. Pract Assess Res Eval. 16(1):6."
    },
    {
      "text": "An overview of key statistics for the six papers. SD: standard deviation. Papers A-D were offline and invigilated in examination halls, papers E and F were online and invigilated via webcam."
    },
    {
      "text": "An overview of anchor items.Note: 'Pair' indicates which papers are being compared. SD: Standard Deviation. \u00c3 indicates statistical significance at p \u00bc 0.05 (adjusted for multiple comparisons). Only model details of significant associations are reported. All effect sizes are small."
    }
  ],
  "references": [
    {
      "title": "The Bonferonni and Sid ak corrections for multiple comparisons",
      "authors": [
        "H Abdi"
      ],
      "year": 2007
    },
    {
      "title": "Take-home exams in higher education: a systematic review",
      "authors": [
        "L Bengtsson"
      ],
      "year": 2019,
      "doi": "10.3390/educsci9040267"
    },
    {
      "title": "Statistics notes: Cronbach's alpha",
      "authors": [
        "J Bland",
        "D Altman"
      ],
      "year": 1997,
      "doi": "10.1136/bmj.314.7080.572"
    },
    {
      "title": "Webbased application to eliminate five contraindicated multiple-choice question practices",
      "authors": [
        "A Brunnquell",
        "\u00c3 Degirmenci",
        "S Kreil",
        "J Kornhuber",
        "M Weih"
      ],
      "year": 2011,
      "doi": "10.1177/0163278710370459"
    },
    {
      "title": "Learning to cheat: Quantifying changes in score advantage of unproctored assessments over time",
      "authors": [
        "B Chen",
        "S Azad",
        "M Fowler",
        "M West",
        "C Zilles"
      ],
      "year": 2020
    },
    {
      "title": "Adapting to the impact of COVID-19: sharing stories, sharing practice",
      "authors": [
        "J Cleland",
        "J Mckimm",
        "R Fuller",
        "D Taylor",
        "J Janczukowicz",
        "T Gibbs"
      ],
      "year": 2020,
      "doi": "10.1080/0142159x.2020.1757635"
    },
    {
      "title": "A power primer",
      "authors": [
        "J Cohen"
      ],
      "year": 1992
    },
    {
      "title": "Education techniques for lifelong learning: writing multiple-choice questions for continuing medical education activities and self-assessment modules",
      "authors": [
        "J Collins"
      ],
      "year": 2006,
      "doi": "10.1148/rg.262055145"
    },
    {
      "title": "Randomized controlled trials and meta-analysis in medical education: what role do they play?",
      "authors": [
        "D Cook"
      ],
      "year": 2012,
      "doi": "10.3109/0142159x.2012.671978"
    },
    {
      "title": "Coefficient alpha and the internal structure of tests",
      "authors": [
        "L Cronbach"
      ],
      "year": 1951,
      "doi": "10.1007/bf02310555"
    },
    {
      "title": "Comparing student performance on proctored and non-proctored exams in online psychology courses",
      "authors": [
        "L Daffin",
        "A Jones"
      ],
      "year": 2018
    },
    {
      "title": "Covido-pedago-phobia",
      "authors": [
        "P Eachempati",
        "K Ramnarayan"
      ],
      "year": 2020,
      "doi": "10.1111/medu.14257"
    },
    {
      "title": "Could COVID-19 be our 'There is no alternative' (TINA) opportunity to enhance assessment?",
      "authors": [
        "R Fuller",
        "V Joynes",
        "J Cooper",
        "K Boursicot",
        "T Roberts"
      ],
      "year": 2020,
      "doi": "10.1080/0142159x.2020.1779206"
    },
    {
      "title": "Outcomes for graduates",
      "year": 2015,
      "doi": "10.1177/0025817215579169"
    },
    {
      "title": "The Covid-19 pandemic: provoking thought and encouraging change",
      "authors": [
        "T Gibbs"
      ],
      "year": 2020,
      "doi": "10.1080/0142159x.2020.1775967"
    },
    {
      "title": "Cheating in OSCEs: the impact of simulated security breaches on OSCE performance",
      "authors": [
        "A Gotzmann",
        "A Champlain",
        "F Homayra",
        "A Fotheringham",
        "I Vries",
        "M Forgie",
        "D Pugh"
      ],
      "year": 2017,
      "doi": "10.1080/10401334.2016.1202832"
    },
    {
      "title": "Increased power with modified forms of the Levene (Med) test for heterogeneity of variance",
      "authors": [
        "Wgs Hines",
        "R Hines"
      ],
      "year": 2000,
      "doi": "10.1111/j.0006-341x.2000.00451.x"
    },
    {
      "title": "Proctored versus unproctored online exams: studying the impact of exam environment on student performance",
      "authors": [
        "K Hollister",
        "M Berenson"
      ],
      "year": 2009,
      "doi": "10.1111/j.1540-4609.2008.00220.x"
    },
    {
      "title": "Utilizing webcam-based proctoring to deter misconduct in online exams",
      "authors": [
        "K Hylton",
        "Y Levy",
        "L Dringus"
      ],
      "year": 2016,
      "doi": "10.1016/j.compedu.2015.10.002"
    },
    {
      "title": "The prospects of sitting 'end of year' open book exams in the light of COVID-19: a medical student's perspective",
      "authors": [
        "C Jervis",
        "L Brown"
      ],
      "year": 2020,
      "doi": "10.1080/0142159x.2020.1766668"
    },
    {
      "title": "Re-using questions in classroom-based assessment: an exploratory study at the undergraduate medical education level",
      "authors": [
        "S Joncas",
        "C St-Onge",
        "S Bourque"
      ],
      "year": 2018,
      "doi": "10.1007/s40037-018-0482-1"
    },
    {
      "title": "Using technologies to prevent cheating in remote assessments during the COVID-19 pandemic",
      "authors": [
        "J Lee",
        "R Kim",
        "S-Y Park",
        "M Henning"
      ],
      "year": 2020,
      "doi": "10.1002/jdd.12350"
    },
    {
      "title": "Robust tests for equality of variances",
      "authors": [
        "H Levene"
      ],
      "year": 1960
    },
    {
      "title": "Prescribing Safety Assessment 2016: delivery of a national prescribing assessment to 7343 UK final-year medical students",
      "authors": [
        "Srj Maxwell",
        "J Coleman",
        "L Bollington",
        "C Taylor",
        "D Webb"
      ],
      "year": 2017,
      "doi": "10.1111/bcp.13319"
    },
    {
      "title": "The misinterpretation of the standard error of measurement in medical education: a primer on the problems, pitfalls and peculiarities of the three different standard errors of measurement",
      "authors": [
        "I Mcmanus"
      ],
      "year": 2012,
      "doi": "10.3109/0142159x.2012.670318"
    },
    {
      "title": "A conceptual introduction to psychometrics: development, analysis and application of psychological and educational tests",
      "authors": [
        "G Mellenbergh"
      ],
      "year": 2011
    },
    {
      "title": "Assessment of higher order cognitive skills in undergraduate education: modified essay or multiple choice questions? Research paper",
      "authors": [
        "E Palmer",
        "P Devitt"
      ],
      "year": 2007,
      "doi": "10.1186/1472-6920-7-49"
    },
    {
      "title": "Evaluating the quantity-quality trade-off in the selection of anchor items: a vertical scaling approach",
      "authors": [
        "F Pibal",
        "H Cesnik"
      ],
      "year": 2011
    },
    {
      "title": "Are \"tomorrow's doctors\" honest? Questionnaire study exploring medical students' attitudes and reported behaviour on academic misconduct",
      "authors": [
        "S Rennie",
        "J Crosby"
      ],
      "year": 2001
    },
    {
      "title": "Setting cut-scores: a critical review of the Angoff and modified Angoff methods",
      "authors": [
        "K Ricker"
      ],
      "year": 2006
    },
    {
      "title": "High-stakes, remote-access, openbook examinations",
      "authors": [
        "A Sam",
        "M Reid",
        "A Amin"
      ],
      "year": 2020,
      "doi": "10.1111/medu.14247"
    },
    {
      "title": "Medical education research remains the poor relation",
      "authors": [
        "M Todres",
        "A Stephenson",
        "R Jones"
      ],
      "year": 2007,
      "doi": "10.1136/bmj.39253.544688.94"
    },
    {
      "title": "Differential attainment in medical education and training",
      "authors": [
        "K Woolf"
      ],
      "year": 2020,
      "doi": "10.1136/bmj.m339"
    },
    {
      "title": "Perceived causes of differential attainment in UK postgraduate medical training: a national qualitative study",
      "authors": [
        "K Woolf",
        "A Rich",
        "R Viney",
        "S Needleman"
      ],
      "year": 2016,
      "doi": "10.1136/bmjopen-2016-013429"
    }
  ],
  "num_references": 34
}
