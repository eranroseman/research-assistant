{
  "paper_id": "M3KRI573",
  "title": "The impact of artificial intelligence on the reading times of radiologists for chest radiographs",
  "abstract": "Whether the utilization of artificial intelligence (AI) during the interpretation of chest radiographs (CXRs) would affect the radiologists' workload is of particular interest. Therefore, this prospective observational study aimed to observe how AI affected the reading times of radiologists in the daily interpretation of CXRs. Radiologists who agreed to have the reading times of their CXR interpretations collected from September to December 2021 were recruited. Reading time was defined as the duration in seconds from opening CXRs to transcribing the image by the same radiologist. As commercial AI software was integrated for all CXRs, the radiologists could refer to AI results for 2 months (AI-aided period). During the other 2 months, the radiologists were automatically blinded to the AI results (AI-unaided period). A total of 11 radiologists participated, and 18,680 CXRs were included. Total reading times were significantly shortened with AI use, compared to no use (13.3 s vs. 14.8 s, p < 0.001). When there was no abnormality detected by AI, reading times were shorter with AI use (mean 10.8 s vs. 13.1 s, p < 0.001). However, if any abnormality was detected by AI, reading times did not differ according to AI use (mean 18.6 s vs. 18.4 s, p = 0.452). Reading times increased as abnormality scores increased, and a more significant increase was observed with AI use (coefficient 0.09 vs. 0.06, p < 0.001). Therefore, the reading times of CXRs among radiologists were influenced by the availability of AI. Overall reading times shortened when radiologists referred to AI; however, abnormalities detected by AI could lengthen reading times.",
  "year": 2021,
  "date": "2021",
  "journal": "Korean J. Radiol",
  "publication": "Korean J. Radiol",
  "authors": [
    {
      "forename": "Hyun",
      "surname": "Shin",
      "name": "Hyun Shin",
      "affiliation": "1  Department of Radiology , Research Institute of Radiological Science and Center for Clinical Imaging Data Science , Yongin Severance Hospital , Yonsei University College of Medicine , 363 , Dongbaekjukjeon-daero , Giheung-gu , Yongin-si , Gyeonggi-do 16995 , South Korea. \n\t\t\t\t\t\t\t\t Department of Radiology \n\t\t\t\t\t\t\t\t Research Institute of Radiological Science \n\t\t\t\t\t\t\t\t Center for Clinical Imaging Data Science \n\t\t\t\t\t\t\t\t Yongin Severance Hospital \n\t\t\t\t\t\t\t\t Yonsei University College of Medicine \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 363 Dongbaekjukjeon-daero Giheung-gu Yongin-si \n\t\t\t\t\t\t\t\t\t 16995 \n\t\t\t\t\t\t\t\t\t Gyeonggi-do \n\t\t\t\t\t\t\t\t\t South Korea",
      "orcid": "0000-0002-7462-2609"
    },
    {
      "forename": "Kyunghwa",
      "surname": "Han",
      "name": "Kyunghwa Han",
      "affiliation": "3  Department of Radiology , Severance Hospital , Research Institute of Radiological Science and Center for Clinical Imaging Data Science , Yonsei University College of Medicine , 50-1 Yonsei-Ro , Seodaemun-Gu , Seoul 03722 , South Korea. \n\t\t\t\t\t\t\t\t Department of Radiology \n\t\t\t\t\t\t\t\t Research Institute of Radiological Science and Center for Clinical Imaging Data Science \n\t\t\t\t\t\t\t\t Severance Hospital \n\t\t\t\t\t\t\t\t Yonsei University College of Medicine \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 50-1 Yonsei-Ro Seodaemun-Gu \n\t\t\t\t\t\t\t\t\t 03722 \n\t\t\t\t\t\t\t\t\t Seoul \n\t\t\t\t\t\t\t\t\t South Korea"
    },
    {
      "forename": "Leeha",
      "surname": "Ryu",
      "name": "Leeha Ryu",
      "affiliation": "4  Department of Biostatistics and Computing , Yonsei University Graduate School , 50-1 Yonsei-Ro , Seodaemun-Gu , Seoul 03722 , South Korea. \n\t\t\t\t\t\t\t\t Department of Biostatistics and Computing \n\t\t\t\t\t\t\t\t Yonsei University Graduate School \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 50-1 Yonsei-Ro Seodaemun-Gu \n\t\t\t\t\t\t\t\t\t 03722 \n\t\t\t\t\t\t\t\t\t Seoul \n\t\t\t\t\t\t\t\t\t South Korea"
    },
    {
      "forename": "Eun-Kyung",
      "surname": "Kim",
      "name": "Eun-Kyung Kim",
      "affiliation": "1  Department of Radiology , Research Institute of Radiological Science and Center for Clinical Imaging Data Science , Yongin Severance Hospital , Yonsei University College of Medicine , 363 , Dongbaekjukjeon-daero , Giheung-gu , Yongin-si , Gyeonggi-do 16995 , South Korea. \n\t\t\t\t\t\t\t\t Department of Radiology \n\t\t\t\t\t\t\t\t Research Institute of Radiological Science \n\t\t\t\t\t\t\t\t Center for Clinical Imaging Data Science \n\t\t\t\t\t\t\t\t Yongin Severance Hospital \n\t\t\t\t\t\t\t\t Yonsei University College of Medicine \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 363 Dongbaekjukjeon-daero Giheung-gu Yongin-si \n\t\t\t\t\t\t\t\t\t 16995 \n\t\t\t\t\t\t\t\t\t Gyeonggi-do \n\t\t\t\t\t\t\t\t\t South Korea",
      "email": "ekkim@yuhs.acwww",
      "orcid": "0000-0002-3368-5013"
    },
    {
      "affiliation": "Seoul National University Bundang Hospital \n\t\t\t\t\t\t\t\t Seoul National University Bundang Hospital"
    }
  ],
  "doi": "",
  "sections": [
    {
      "title": "INTRODUCTION",
      "text": "Artificial intelligence (AI) has been widely utilized for research in radiology, and with the emergence of commercial AI software, more efforts have been made to demonstrate the efficacy of AI software in actual practice because of clinical necessity  [1] [2] [3]  . Research has focused on the impact of AI on patient management and the decisionmaking process of doctors, in addition to the achievement of reasonable diagnostic performance using AI  2  . For radiologists, questions of interest are whether AI assistance can help prioritize images for reading, reduce missing cases, or affect reading times  2, 4, 5  .\n\nRecent studies have demonstrated better diagnostic performance with AI when reprioritizing brain computed tomography (CT) for the detection of hemorrhage  6, 7  . Integration of AI into mammography has been found to enhance the diagnostic performance of radiologists without increasing reading time  8  . A similar tendency was observed in the detection of bone fractures using radiographs  9, 10  . Several studies have also tried to demonstrate how AI affects the reading times for chest radiographs (CXRs) or CT among radiologists  [11] [12] [13]  . However, most of these past studies were retrospective studies performed by simulating the clinical process or only with selected cases and radiologists in a prospective manner.\n\nCXRs are the most commonly performed imaging studies; however, timely interpretation of CXRs by radiologists, especially for those containing critical lesions, is difficult in hospitals. Most clinicians in outpatient clinics or the emergency room (ER) frequently interpret CXRs on their own before receiving official reading reports. Due to this situation, the application of AI for CXR has attracted more attention from researchers, and the development of commercially available AI software has widely been for CXRs  1, 14  . For radiologists, whether the utilization of AI during the interpretation would affect their workload is of particular interest. Concerning the reading time of radiologists, there could be a concern as to whether referring to AI results would increase workload by adding working steps or reduce decision-making time as an effective computer-assisted diagnosis system  4  . To our knowledge, few studies have demonstrated how AI actually affects reading time in real clinical situations.\n\nTherefore, this prospective observational study aims to observe how AI affects the actual reading times of radiologists in the daily interpretation of CXRs in real-world clinical practice. In this study involving 11 radiologists and 18,680 CXRs, total reading times significantly shorten with AI use, particularly when no abnormality is detected by AI. However, if any abnormality is detected by AI, reading times do not differ between AI use and no AI use. Our findings inform that the availability of AI influences the reading times of CXRs among radiologists and that AI integration can overall shorten reading times. However, it is important to note that abnormalities detected by AI may lengthen reading times."
    },
    {
      "title": "RESULTS"
    },
    {
      "title": "Subjects and CXRs",
      "text": "During the study period, a total of 11 radiologists participated in this prospective study, and they accounted for approximately 79% of the radiologists in our institution. All radiologists who participated in the study were board-certified specialists in radiology. The participating radiologists had a minimum of 10 years and a maximum of 23 years of experience in the field of radiology. The flow diagram of the study process is summarized in Fig.  1 . The data are provided in Supplementary information. The subspecialties of the participating radiologists were as follows: thoracic radiology = 1, abdominal radiology = 4, neuroradiology = 2, musculoskeletal radiology = 2, breast and thyroid radiology = 1, and health check-up = 1.\n\nDuring the study period, a total of 21,152 consecutive CXRs were read by the radiologists. Among them, 2472 CXRs were excluded due to reading time outliers of 51 s according to the interquartile range (IQR) methods. Therefore, a total of 18,680 CXRs were finally analyzed. A comparison of the total number of included CXRs and the age of patients in AI-unaided and AI-aided periods is summarized in Table  1 . Among the included CXRs, 9109 CXRs (49%) were read in the AI-aided period. Patient age was significantly lower in the AI-aided period (mean 57.9 years vs. 59.2 years, p < 0.001), and the proportion of outpatient clinic patients was higher in the AI-aided period (51.6% vs. 45.1%, p < 0.001). The number of CXRs containing abnormalities was significantly lower in the AI-aided period (37.4% vs. 44.5 %, p < 0.001)."
    },
    {
      "title": "Comparison of reading times according to patient characteristics",
      "text": "A comparison of reading times between AI-unaided and AI-aided conditions according to patient characteristics is summarized in Table  2 . Total reading times were significantly shortened with the use of AI compared to no use (estimated mean 13.3 s vs. 14.8 s, p < 0.001) (Fig.  2a ). The sex and age of patients did not affect reading times significantly (p = 0.108 and 0.774, respectively). Among the inpatient and outpatient clinics, reading times for outpatients significantly decreased more than those for inpatients with the use of AI (decrement -1.8 s in outpatient clinics vs. -0.5 s in inpatient locations, p < 0.001) (Table  2 ). Reading times were significantly different according to patient location (p < 0.001). Reading times were significantly lower with AI use when patients were in outpatient and general ward locations (p < 0.001 and 0.002, respectively). The presence of a previous comparable CXR did not affect reading times (p = 0.524) (Table  2 )."
    },
    {
      "title": "Comparison of reading times according to the presence of lesions",
      "text": "Using an operating point of 15% as a cutoff value, the presence of a lesion could be determined by AI. Reading times according to the presence of lesions are summarized in Table  3 . When there was no abnormality detected by AI on CXR, reading times were significantly shorter in the AI-aided period (estimated mean 10.8 s vs. 13.1 s, p < 0.001). However, when there was any abnormality detected by AI, reading times were not significantly different between the AI-aided and AI-unaided periods (estimated mean 18.6 s vs. 18.4 s, p = 0.452). The time difference between AI-aided and AI-unaided periods was significantly different according to the presence of lesions (difference of 0.2 s in the presence of any lesion vs. -2.2 s without any abnormality, p < 0.001) (Table  3 ). These tendencies were also similar for specific lesion types, except for pneumoperitoneum and pneumothorax, in terms of time differences."
    },
    {
      "title": "Comparison of reading times according to abnormality scores",
      "text": "When the abnormality score analyzed by AI was considered as a continuous variable, reading times significantly increased as scores increased, and a more significant increase was observed with the use of AI, compared to no use (regression coefficient 0.09 vs. 0.06 for 1 s increases, p < 0.001) (Table  4 , Fig.  2b ). These tendencies were also similar for specific lesion types, except for pneumoperitoneum and pneumothorax."
    },
    {
      "title": "DISCUSSION",
      "text": "Here we report reading times in the daily CXR interpretations of 11 radiologists and include all consecutive CXRs read by radiologists during 4 months to determine whether reading times are affected by the use of AI. With increases in the work burden of radiologists, whether AI could be a potential solution for reducing fatigue and enhancing the accuracy of radiologists is an interesting topic  4  . Because CXRs are read by all radiologists in our institution under preset requirements for each month, this study design mirrored what would occur in actual practice. This is an observational study performed by simply adjusting the automatic display of AI results in the PACS by month and extracted time data using PACS log records. Radiologists could read CXRs in their daily practice with or without utilizing AI results. We found that overall reading times were affected by the use of AI and, interestingly, shortened for normal CXRs. However, reading times did not significantly differ according to AI use for CXR with abnormalities. When the abnormality score on CXR increased, reading times also increased. This could be due to radiologists reporting normal CXRs with more confidence after referring to AI results, allowing them to make faster decisions. Conversely, when there was any lesion depicted by AI, radiologists might take more time to judge the validity of the AI assessment and to report more details about the findings seen on images regardless of the accuracy of displayed AI results.\n\nSeveral studies have focused on reading times according to AI use. Reading times for detecting bone fractures in radiographs tended to decrease with AI  9, 15  . For mammography, studies have shown conflicting results, with reading times not being significantly affected by the use of AI  16  or decreasing up to 22.3% when AI results are available  17  . In a study by Lee et al., reading times were affected by the experience levels of radiologists even with AI, as general radiologists showed longer reading times; breast radiologists did not show any change in reading times with AI use  8  . Interestingly, a study by Pacile et al. reported results for mammography that were similar to the findings seen in this study  18  . According to the AI score in mammography, reading times decreased with lower scores and increased with higher scores representing the probability of malignancy. Authors suggested that AI results could help radiologists save time with normal mammograms by reassuring them that they had made the right judgment call and instead enabling them to focus more on images with suspicious findings  18  .\n\nFor CXR, Sung et al. performed a retrospective study with a randomized crossover design including 228 CXRs interpreted by 6 radiologists  11  . They demonstrated that the mean reading time was reduced from 24 \u00b1 21 s to 12 \u00b1 8 s with AI. They suggested that the relatively lower false-positive results of commercially available AI software could reduce reading times and that this impact was bigger than the risk of increasing reading times by unnecessary false-positive findings  11  . A recent multicenter study by Kim et al. used the same software as we did and demonstrated the actual influence of AI on reading times for a health screening cohort  12  . They reviewed the readings of the radiologists for all CXRs taken during 2 months with or without integration of AI on PACS. They reported a concordance rate of 86.8% between the reports made by AI and radiologists and found the median reading time to increase from 14 to 19 s with AI  12  . In a subgroup analysis, reading times increased for normal CXRs but decreased for abnormal CXRs. This result contradicts our own, which may be due to differences in the study cohort and the proportion of normal CXRs between the health screening center and our general hospital. In addition, our study utilized the most recent version of AI software, which could detect a total of eight lesions and displayed a contour map, abbreviations, and abnormality scores for each lesion on the analyzed images  1, 19, 20  . The software used in the study by Lee et al. could detect three kinds of lesions, including nodules, consolidation, and pneumothorax, without displaying separate abbreviations or scores for the detected lesions. This could have resulted in the different tendencies for reading times as our study additionally analyzed the influence of each lesion type and abnormality scores.\n\nThere are several limitations to this study. First, this study only utilized one source of commercially available software and the generalizability of its results could be limited. However, because our hospital integrated the AI-based lesion detection software for all CXRs and the processes for referring AI results are well organized, this could be an advantage when proving the actual influence of AI on workflow efficiency. Second, the number of CXRs containing lesions was different in the AI-unaided and aided periods unexpectedly because we did not control CXR types for participants in this observational study. One possible explanation is that the participating radiologists may have been able to read a greater number of easy and normal CXRs in the AI-aided period than in the AI-unaided period using total abnormality scores visualized on the worklist. The involved radiologists might preferentially read CXRs with low AI scores during the AI-aided period. Another possibility is that the radiologists not participating in this study could read normal CXRs more and fast in the AIunaided period than participating radiologists using the sorting function of scores on the worklist. However, it was impossible to control CXR images containing similar proportions of each lesion during the 4-month study period, and whether radiologists prefer to read normal CXRs using the AI scoring system was not assessed in this study. Third, we could not check whether the participating radiologists indeed referred to AI results in all CXRs or prioritized worklists according to the scores during the AI-aided period. To encourage participation and compliance in this prospective study over 4 months, we allowed radiologists to read images just as they normally did and did not force them to refer to AI results for all CXRs in the AI-aided period. However, in a recent study, radiologists of our hospital answered that they refer to the AI results in about 83% of CXRs that they read in a day  21  . Therefore, we could suggest that our study reflected the actual influence of AI on the daily interpretation of radiologists. In addition, as there was only one chest radiology specialist at our institution, it was not possible to compare the reading times between specialists and non-specialists in chest radiology. We believe that investigating whether there are differences in reading times based on the experience and expertise of radiologists will be an important area for future research following this study. At last, we did not evaluate whether the presence of lesions or the abnormality score was accurate according to the radiologists' reports or CT images. We only utilized the AI results concerning lesion type and scores when evaluating the impact of AI software on reading times. Since this study focused on the impact of AI on reading time, we could not address the separate topic of the accuracy of the AI program's image findings. This software is already known for its excellent diagnostic performance  12, 19, 22  . For example, the diagnostic accuracy for lung nodule detection was excellent by showing an area under the receiver operating characteristic curve greater than 0.9  23, 24  . In addition, similar accuracy has been reported for pneumothorax or consolidation  19, 25  . Additionally, in recent studies at our institution, we demonstrated the actual clinical utility of AI for CXRs and also the importance of early detection of lung cancer  20, 21, 26  . We agreed that whether AI had accurate results and also affected the diagnosis of actual radiologists is an important  point, we expect to broaden our research encompass whether AI influences the diagnostic performance, false recall rate, or prioritization of urgent findings and to further evaluate the actual accuracy of AI in subsequent studies.\n\nIn conclusion, this prospective observational study of real-world clinical practice demonstrated that the reading times of CXRs among radiologists were influenced by the availability of AI results. Overall reading times shortened when radiologists referred  December 2021 (AI-aided period) automatically (Fig.  1 ). During the AI-unaided period, AI results, including secondary capture images attached to the original CXR and the abnormality score column on the worklist, were not shown on the PACS automatically, and the participating radiologists were blinded to them. However, during the AI-aided period, the results were made available and could be freely utilized by radiologists. The CXRs of patients more than 18 years old were included for analysis because the software has been approved for adult CXRs. We excluded reading time outliers with a duration of more than 51 s based on the outlier detection method. These outliers in reading time could be from various conditions, such as from delayed interpretation of corresponding CXRs after opening by unexpected interruption from other work  12  .\n\nFor the included CXRs, patient age, sex, and information on whether CXRs were taken at an inpatient or outpatient clinics were reviewed using electronic medical records. The location of patients at the time of the CXR, including the ER, general ward, and intensive care unit, was also reviewed. The presence of previous comparable CXRs was analyzed as a possible factor affecting reading times. For the AI results, the abnormality score was analyzed as both a continuous variable using the number itself and a categorical variable by applying a cutoff value of 15%. This cutoff value was chosen because our hospital has employed an operating point of 15% when determining the presence of lesions according to the vendor's guidelines  12  When the operating point was above 15%, the AI software marked the lesion location with a contour map, score, and abbreviation for each lesion on images  20  . Therefore, the presence of lesions, including atelectasis, cardiomegaly, consolidation, fibrosis, nodule, pleural effusion, pneumoperitoneum, and pneumothorax, were evaluated by using each abnormality score itself as a continuous variable and by applying the operating point. In addition, the highest score was used as a total abnormality score of each CXR and used to determine whether the CXRs included any abnormalities."
    },
    {
      "title": "Statistical analysis",
      "text": "For statistical analysis, the R program (4.1.3, Foundation for Statistical Computing, Vienna, Austria, package lme4, lmerTest) was used. We used the 1.5 IQR method to exclude CXRs with reading time outliers. This method is a conventional method to define outliers by using the first quartile (6 s in our study) and the third quartile (24 s). The formula to determine a cutoff value for the outlier was as follows; 24 + (24-6) \u00d7 1.5 = 51 s. The chisquare test and two-sample t-test were used for comparison of the total number of included CXRs and the ages of the patients in the AI-unaided and AI-aided periods. A linear mixed model was used to compare reading times considering the random effects of radiologists and patients. Reading times in seconds were compared between AI-unaided and AI-aided periods according to patient characteristics (sex, age, location, and presence of previous comparable CXR). Reading times were compared according to the presence of lesions detected by AI (any one of the following eight abnormalities: atelectasis, cardiomegaly, consolidation, fibrosis, nodule, pleural effusion, pneumoperitoneum, pneumothorax) using an operating point of 15%. When the abnormality score was considered as a continuous variable, reading times were compared between AI-unaided and AI-aided conditions. The variables, AI availability, and their interactions were considered as fixed effects for the linear mixed model. p-values less than 0.05 were considered statistically significant."
    },
    {
      "text": "Fig. 1 Flow diagram of the study process. Flow diagram of the study process is summarized."
    },
    {
      "text": "Fig.2Reading time in AI-unaided and aided conditions. a According to the presence or absence of a lesion on chest radiographs (total abnormality scores-high: \u226515% representing the presence of any lesion, low: <15% representing the absence of any lesion by AI) and b according to the total abnormality scores (0-100%)."
    },
    {
      "text": "Fig. 3 Integration of AI for CXRs on PACS. a The AI result attached to the second image of the original CXR contains a contour map, abbreviations, and the abnormality score of detected lesions. Doctors can simply refer to the AI results by scrolling down the original image on the PACS. b The highest abnormality score is used as the total abnormality score of each CXR, and this was listed as a separate column (red square) on the PACS."
    },
    {
      "text": "Total number of chest radiographs in AI-unaided and AIaided conditions."
    },
    {
      "text": "Comparison of reading times according to patient characteristics."
    },
    {
      "text": "Comparison of reading times according to the presence of lesions. Absent 14.585 (12.124, 17.045) 12.867 (10.406, 15.327) -1.718 (-2.027, -1.409)"
    },
    {
      "text": "Comparison of reading times according to abnormality scores."
    }
  ],
  "references": [
    {
      "title": "Use of artificial intelligence-based software as medical devices for chest radiography: a position paper from the Korean Society of Thoracic Radiology",
      "authors": [
        "E Hwang"
      ],
      "year": 2021,
      "doi": "10.3348/kjr.2021.0544"
    },
    {
      "title": "How does artificial intelligence in radiology improve efficiency and health outcomes?",
      "authors": [
        "K Van Leeuwen"
      ],
      "year": 2021,
      "doi": "10.1007/s00247-021-05114-8"
    },
    {
      "title": "The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database",
      "authors": [
        "S Benjamens",
        "P Dhunnoo",
        "B Mesk\u00f3"
      ],
      "year": 2020,
      "doi": "10.1038/s41746-020-00324-0"
    },
    {
      "title": "Mandating limits on workload, duty, and speed in radiology",
      "authors": [
        "R Alexander"
      ],
      "year": 2022,
      "doi": "10.1148/radiol.212631"
    },
    {
      "title": "Current applications and future impact of machine learning in radiology",
      "authors": [
        "G Choy"
      ],
      "year": 2018,
      "doi": "10.1148/radiol.2018171820"
    },
    {
      "title": "Active reprioritization of the reading worklist using artificial intelligence has a beneficial effect on the turnaround time for interpretation of head CT with intracranial hemorrhage",
      "authors": [
        "T O'neill"
      ],
      "year": 2021,
      "doi": "10.1148/ryai.2020200024"
    },
    {
      "title": "Improvement of the diagnostic accuracy for intracranial haemorrhage using deep learning-based computer-assisted detection",
      "authors": [
        "Y Watanabe"
      ],
      "year": 2021,
      "doi": "10.1007/s00234-020-02566-x"
    },
    {
      "title": "Improving the performance of radiologists using artificial intelligence-based detection support software for mammography: a multi-reader study",
      "authors": [
        "J Lee"
      ],
      "year": 2022,
      "doi": "10.3348/kjr.2021.0476"
    },
    {
      "title": "Improving radiographic fracture recognition performance and efficiency using artificial intelligence",
      "authors": [
        "A Guermazi"
      ],
      "year": 2022,
      "doi": "10.1148/radiol.210937"
    },
    {
      "title": "Improving rib fracture detection accuracy and reading efficiency with deep learning-based detection software: a clinical evaluation",
      "authors": [
        "B Zhang"
      ],
      "year": 2021,
      "doi": "10.1259/bjr.20200870"
    },
    {
      "title": "Added value of deep learning-based detection system for multiple major findings on chest radiographs: a randomized crossover study",
      "authors": [
        "J Sung"
      ],
      "year": 2021,
      "doi": "10.1148/radiol.2021202818"
    },
    {
      "title": "Concordance rate of radiologists and a commercialized deeplearning solution for chest X-ray: Real-world experience with a multicenter health screening cohort",
      "authors": [
        "E Kim"
      ],
      "year": 2022,
      "doi": "10.1371/journal.pone.0264383"
    },
    {
      "title": "Impact of concurrent use of artificial intelligence tools on radiologists reading time: a prospective feasibility study",
      "authors": [
        "F M\u00fcller"
      ],
      "year": 2022,
      "doi": "10.1016/j.acra.2021.10.008"
    },
    {
      "title": "Clinical implementation of deep learning in thoracic radiology: potential applications and challenges",
      "authors": [
        "E Hwang",
        "C Park"
      ],
      "year": 2020,
      "doi": "10.3348/kjr.2019.0821"
    },
    {
      "title": "Added value of an artificial intelligence solution for fracture detection in the radiologist's daily trauma emergencies workflow",
      "authors": [
        "L Canoni-Meynet"
      ],
      "year": 2022,
      "doi": "10.1016/j.diii.2022.06.004"
    },
    {
      "title": "Impact of artificial intelligence in breast cancer screening with mammography",
      "authors": [
        "L Dang"
      ],
      "year": 2022,
      "doi": "10.1007/s12282-022-01375-9"
    },
    {
      "title": "Deep learning model improves radiologists' performance in detection and classification of breast lesions",
      "authors": [
        "Y Sun"
      ],
      "year": 2021,
      "doi": "10.21203/rs.3.rs-746374/v1"
    },
    {
      "title": "Improving breast cancer detection accuracy of mammography with the concurrent use of an artificial intelligence tool",
      "authors": [
        "S Pacil\u00e8"
      ],
      "year": 2020,
      "doi": "10.1148/ryai.2020190208"
    },
    {
      "title": "Diagnostic performance of artificial intelligence approved for adults for the interpretation of pediatric chest radiographs",
      "authors": [
        "H Shin",
        "N Son",
        "M Kim",
        "E Kim"
      ],
      "year": 2022,
      "doi": "10.1038/s41598-022-14519-w"
    },
    {
      "title": "Successful implementation of an artificial intelligence-based computer-aided detection system for chest radiography in daily clinical practice",
      "authors": [
        "S Lee",
        "H Shin",
        "S Kim",
        "E Kim"
      ],
      "year": 2022,
      "doi": "10.3348/kjr.2022.0193"
    },
    {
      "title": "Hospital-wide survey of clinical experience with artificial intelligence applied to daily chest radiographs",
      "authors": [
        "H Shin"
      ],
      "year": 2023,
      "doi": "10.1371/journal.pone.0282123"
    },
    {
      "title": "Development and validation of a deep learning algorithm detecting 10 common abnormalities on chest radiographs",
      "authors": [
        "J Nam"
      ],
      "year": 2021,
      "doi": "10.1183/13993003.03061-2020"
    },
    {
      "title": "Development and validation of deep learning-based automatic detection algorithm for malignant pulmonary nodules on chest radiographs",
      "authors": [
        "J Nam"
      ],
      "year": 2019,
      "doi": "10.1148/radiol.2018180237"
    },
    {
      "title": "Performance of a deep learning algorithm compared with radiologic interpretation for lung cancer detection on chest radiographs in a health screening population",
      "authors": [
        "J Lee"
      ],
      "year": 2020,
      "doi": "10.1148/radiol.2020201240"
    },
    {
      "title": "Diagnostic effect of artificial intelligence solution for referable thoracic abnormalities on chest radiography: a multicenter respiratory outpatient diagnostic cohort study",
      "authors": [
        "K Jin"
      ],
      "year": 2022,
      "doi": "10.1007/s00330-021-08397-5"
    },
    {
      "title": "Incidentally found resectable lung cancer with the usage of artificial intelligence on chest radiographs",
      "authors": [
        "S Kwak"
      ],
      "year": 2023,
      "doi": "10.1371/journal.pone.0281690"
    },
    {
      "title": "Current state and strategy for establishing a digitally innovative hospital: memorial review article for opening of Yongin Severance Hospital",
      "authors": [
        "S Kim"
      ],
      "year": 2020,
      "doi": "10.3349/ymj.2020.61.8.647"
    }
  ],
  "num_references": 27,
  "original_doi": "https://doi.org/10.13039/501100003710"
}
