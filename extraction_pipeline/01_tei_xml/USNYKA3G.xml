<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_k74d8KW">Diverse patients&apos; attitudes towards Artificial Intelligence (AI) in diagnosis</title>
				<funder ref="#_dRD63SM">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000002</idno>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">Â© 2023 Robertson et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
				</availability>
				<date type="published" when="2023-05-19">May 19, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Robertson</surname></persName>
							<idno type="ORCID">0000-0001-6188-8005</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Arizona , Tucson , Arizona , United States of America ,</note>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<settlement>Tucson</settlement>
									<region>Arizona</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Boston University , Boston , Massachusetts , United States of America ,</note>
								<orgName type="institution">Boston University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>Massachusetts</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Woodsi</surname></persName>
							<idno type="ORCID">0000-0003-2381-0842</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Arizona , Tucson , Arizona , United States of America ,</note>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<settlement>Tucson</settlement>
									<region>Arizona</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kelly</forename><surname>Bergstrand</surname></persName>
							<idno type="ORCID">0000-0002-4620-4434</idno>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> University of Texas at Arlington , Arlington Texas , United States of America</note>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Arlington Texas</settlement>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jess</forename><surname>Findley</surname></persName>
							<idno type="ORCID">0000-0002-4452-6114</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Arizona , Tucson , Arizona , United States of America ,</note>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<settlement>Tucson</settlement>
									<region>Arizona</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cayley</forename><surname>Balser</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Arizona , Tucson , Arizona , United States of America ,</note>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<settlement>Tucson</settlement>
									<region>Arizona</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Marvin</forename><forename type="middle">J</forename><surname>Slepian</surname></persName>
							<email>slepian@arizona.edu</email>
							<idno type="ORCID">0000-0002-7864-6691</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of Arizona , Tucson , Arizona , United States of America ,</note>
								<orgName type="institution">University of Arizona</orgName>
								<address>
									<settlement>Tucson</settlement>
									<region>Arizona</region>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">UCSF : University of California San Francisco , UNITED STATES</note>
								<orgName type="institution" key="instit1">UCSF</orgName>
								<orgName type="institution" key="instit2">University of California San</orgName>
								<address>
									<settlement>Francisco</settlement>
									<country key="US">UNITED STATES</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_8xVTqmp">Diverse patients&apos; attitudes towards Artificial Intelligence (AI) in diagnosis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-19">May 19, 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">91EE039E185EE7829E359AFEEDE7DBE7</idno>
					<idno type="DOI">10.1371/journal.pdig.0000237</idno>
					<note type="submission">Received: February 27, 2022 Accepted: March 20, 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T12:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_jMEMqZm"><p xml:id="_UkYhagQ"><s xml:id="_xUazCza">Artificial intelligence (AI) has the potential to improve diagnostic accuracy.</s><s xml:id="_vBJmbhC">Yet people are often reluctant to trust automated systems, and some patient populations may be particularly distrusting.</s><s xml:id="_EzpQzgy">We sought to determine how diverse patient populations feel about the use of AI diagnostic tools, and whether framing and informing the choice affects uptake.</s><s xml:id="_neKUYQp">To construct and pretest our materials, we conducted structured interviews with a diverse set of actual patients.</s><s xml:id="_uctfTUd">We then conducted a pre-registered (osf.io/9y26x),</s><s xml:id="_eNynReE">randomized, blinded survey experiment in factorial design.</s><s xml:id="_J76FDGe">A survey firm provided n = 2675 responses, oversampling minoritized populations.</s><s xml:id="_CQ5k4F6">Clinical vignettes were randomly manipulated in eight variables with two levels each: disease severity (leukemia versus sleep apnea), whether AI is proven more accurate than human specialists, whether the AI clinic is personalized to the patient through listening and/or tailoring, whether the AI clinic avoids racial and/or financial biases, whether the Primary Care Physician (PCP) promises to explain and incorporate the advice, and whether the PCP nudges the patient towards AI as the established, recommended, and easy choice.</s><s xml:id="_KXSjk2T">Our main outcome measure was selection of AI clinic or human physician specialist clinic (binary, "AI uptake").</s><s xml:id="_bEZtcJ9">We found that with weighting representative to the U.S. population, respondents were almost evenly split (52.9% chose human doctor and 47.1% chose AI clinic).</s><s xml:id="_PV2PtH9">In unweighted experimental contrasts of respondents who met pre-registered criteria for engagement, a PCP's explanation that AI has proven superior accuracy increased uptake (OR = 1.48,</s><s xml:id="_8Fa7xJj">CI 1.24-1.77,</s><s xml:id="_gN2ynPr">p &lt; .001),</s><s xml:id="_gQHw3jf">as did a PCP's nudge towards AI as the established choice (OR = 1.25, CI: 1.05-1.50,</s><s xml:id="_sMbYTxg">p = .013),</s><s xml:id="_WhRtkbs">as did reassurance that the AI clinic had trained counselors to listen to the patient's unique perspectives (OR = 1.27,</s><s xml:id="_HpvQ8w6">CI: 1.07-1.52,</s><s xml:id="_6Up8AcU">p = .008).</s><s xml:id="_RERwhPB">Disease severity (leukemia versus sleep apnea) and other manipulations did not affect AI uptake significantly.</s><s xml:id="_7UefNge">Compared to White respondents, Black respondents selected AI less often (OR = .73,</s><s xml:id="_Gc6etyn">CI: .55-.96, p = .023)</s><s xml:id="_pRd5NrY">and Native Americans selected it more often (OR: 1.37, CI: 1.01-1.87,</s><s xml:id="_7jXdkHX">p = .041).</s><s xml:id="_DNzebgz">Older respondents were less likely to choose AI (OR: .99,</s><s xml:id="_pxEf2E4">CI: .987-.999, p = .03),</s><s xml:id="_xaX86ct">as were those who identified as politically conservative (OR: .65,</s><s xml:id="_WHrZMm7">CI: .52-.81, p &lt; .001) or viewed religion as important (OR: .64,</s><s xml:id="_Z9aRYRe">CI: .52-.77, p &lt; .001).</s><s xml:id="_FjTXjn5">For each unit increase in education, the odds are 1.10 greater for selecting an AI provider (OR: 1.10, CI: 1.03-1.18,</s><s xml:id="_GaBUdHz">p = .004).</s><s xml:id="_bktf8p9">While many patients appear resistant to the use of AI, accuracy information, nudges and a listening patient experience</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_J3EKu8U">Introduction</head><p xml:id="_9grJQEM"><s xml:id="_GGmAUzD">Artificial intelligence (AI) is poised to transform healthcare.</s><s xml:id="_Gkk2y3Z">Today, AI is used to analyze tumors in chest images <ref type="bibr" target="#b0">[1]</ref>, regulate implanted devices <ref type="bibr" target="#b1">[2]</ref>, and select personalized courses of care <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_4TcZeKK">Despite the promise of AI, there is broad public skepticism about AI in a range of domains from transportation to criminal justice to healthcare <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</s><s xml:id="_MZwDvwW">Doctors and patients tend to primarily rely on doctors' clinical judgment, even when it is at odds with statistical judgment <ref type="bibr" target="#b5">[6]</ref>.</s></p><p xml:id="_EcGV6PF"><s xml:id="_8Jx4MZ4">Research shows that patients prefer human doctors to AI-powered machines in diagnosis, screening, and treatment <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_DeuyRsW">In an early study, patients were more likely to follow medical advice from a physician than a computer and were less trustful of computers as providers of medical advice <ref type="bibr" target="#b6">[7]</ref>.</s><s xml:id="_ur66tsM">Other work shows that patients are less trusting of doctors that rely on non-human decision aids <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</s><s xml:id="_ZcAG8Qx">More recently, in a series of studies, patients were less willing to schedule an appointment to be diagnosed by a robot, and they were willing to pay significantly more money for a human provider, with a reported perception that AI providers are less able to account for patients' unique characteristics <ref type="bibr" target="#b9">[10]</ref>.</s></p><p xml:id="_xPGqsyv"><s xml:id="_p7fsF9f">Yet acceptance of AI may depend on specific features of the system and how the choice is framed, and there may be differences among groups of patients <ref type="bibr" target="#b10">[11]</ref>.</s><s xml:id="_4jHycE4">Outside of healthcare, consumers have been shown to more often trust AI systems for objective tasks, while subjective tasks are viewed as more appropriate for humans <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_FcNJKDN">Some qualitative research suggests that lower levels of patient education are associated with lower trust in computerization <ref type="bibr" target="#b12">[13]</ref>.</s><s xml:id="_fqM2Hwe">Other small studies suggest that AI may be acceptable if human physicians ultimately remain in control <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_FHrDKaE">And, although patients may prefer human physicians, some work suggests that they may better adhere to advice coming from algorithms <ref type="bibr" target="#b14">[15]</ref>.</s></p><p xml:id="_SmYutFj"><s xml:id="_Wc2XMQx">More generally, research suggests that patients' trust in their physicians is an essential component of effective healing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</s><s xml:id="_xR7Sw33">Black, Hispanic, and Native Americans reportedly have lower levels of trust in their physicians <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>.</s><s xml:id="_J6W8YCB">These communities have experienced harm historically from components of the medical system (e.g., the Tuskegee Syphilis Study).</s><s xml:id="_b37BV4z">Yet trust can be enhanced when patients and their providers have similar background, geography, or ethnic groups <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</s><s xml:id="_kgZZ88y">Patients may be concerned about human physicians being biased by their financial relationships with pharmaceutical companies <ref type="bibr" target="#b23">[24]</ref> or biased by implicit racial stereotypes <ref type="bibr" target="#b24">[25]</ref>.</s><s xml:id="_pm8YWPT">Although initial forays into algorithmic decisions gave rise to similar concerns <ref type="bibr" target="#b25">[26]</ref>, AI systems may be rigorously designed, tested, and continuously monitored to minimize racial or financial biases in healthcare <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</s></p><p xml:id="_DvK5PRS"><s xml:id="_6tm7af3">There are many drivers to the development and uptake of AI in healthcare, including commercial incentives and physician attitudes <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</s><s xml:id="_DWDVgq5">Trustworthiness may depend on the relationship established between users, infrastructures, technologies, and practitioners, rather than the certainty and accuracy of the technology <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</s></p><p xml:id="_xjYVjGh"><s xml:id="_s3Cf4Au">To the extent that patients retain the right of informed consent with regard to their own healthcare, much will depend on patient attitudes towards AI.</s><s xml:id="_6dmqSHT">To that end, we used qualitative and quantitative methods to study diverse patient populations' views about AI in medicine.</s></p><p xml:id="_ZPZwMtu"><s xml:id="_3RjAhwF">To our knowledge, this is the first large-scale population-based survey experiment, with random assignment to realistic clinical vignettes systematically manipulated to analyze a range of factors that could influence AI uptake with patients.</s><s xml:id="_Er5c56M">Moreover, our study is enriched to allow sufficient sample size to compare AI uptake across five different racial/ethnic groups, including those who have historically shown lower levels of trust in the healthcare system.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JHbfGTF">Methods</head><p xml:id="_UTCjgPu"><s xml:id="_Uk8n2x6">We conducted two study phases, one qualitative and one quantitative.</s><s xml:id="_Mwu4KwS">In the qualitative phase (February to December 2020), we conducted structured interviews with 24 patients recruited for racial and ethnic diversity to understand their reactions to current and future AI technologies.</s><s xml:id="_5h8R97y">In the quantitative phase (January and February 2021), we used an internet-based survey experiment oversampling Black, Hispanic, Asian, and Native American populations.</s><s xml:id="_8pCwXGh">Both phases placed respondents as mock patients into clinical vignettes to explore whether they would prefer to have an AI system versus a doctor for diagnosis and treatment and under what circumstances.</s></p><p xml:id="_uAuBVWd"><s xml:id="_YABC4sS">We chose this mixed-methods design for a few reasons.</s><s xml:id="_HevhN4y">First, because this is the first study of its kind, we wanted to ensure that the vignettes driving our quantitative survey were realistic and intuitive; the qualitative pre-study helped us to gauge participant reaction.</s><s xml:id="_CPGVWxN">Second, largescale quantitative surveys often raise a number of questions about why people respond the way they do.</s><s xml:id="_84xvSy3">The mixed-method design allows us to accomplish something that neither approach -purely quantitative nor purely qualitative-would achieve on its own.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_tjaXjuu">Development and oversight of the survey</head><p xml:id="_CdXuXSD"><s xml:id="_SSu3JR5">To develop clinical vignettes, we consulted physicians specializing in cardiology, pulmonology, hematology, and sleep medicine to develop vignettes, which were reviewed by physician coauthor (MJS), for authenticity.</s><s xml:id="_eHkTNnx">This study was determined to be exempt by the Human Subjects Protection Program (Institutional Review Board) at the University of Arizona, and all subjects consented.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KN8pdXK">Qualitative pre-study</head><p xml:id="_rtSWppd"><s xml:id="_hbCQwhn">Our qualitative study pre-tested the vignettes and generated hypotheses.</s><s xml:id="_h8JFCd6">For 30-60 minute qualitative interviews in Spanish or English, we recruited 24 individuals from clinics in Tucson, Arizona, including 10 White, 8 Hispanic, 3 Black, 2 Native American, and 1 Asian patients.</s><s xml:id="_t6GC9RG">In total, 16 were females and 8 were males.</s><s xml:id="_ZxKZYtu">Ages ranged from 19 to 92 years, with most over 50, as would be expected given our recruitment from a cardiac clinic.</s><s xml:id="_7uR5NgE">Educational achievement was relatively high, with 7 of the subjects having a graduate degree, and 5 having a bachelor's degree.</s></p><p xml:id="_tNbkKJK"><s xml:id="_mU33uN2">The nature of these interviews was semi-structured.</s><s xml:id="_UZ5d9GU">All participants were given the same script describing the topic, the research design, and the sample vignettes.</s><s xml:id="_gpjQksz">The prompts were deliberately open-ended to allow participants to share reactions and feedback to make sure the vignettes were understood.</s><s xml:id="_pqe6TPu">We recorded all of our interviews and had them transcribed to better inform the design of the quantitative survey.</s></p><p xml:id="_xFAP84a"><s xml:id="_yvGKx9T">After getting informed consent, each interview began with an open-ended question asking the participant to think back to a difficult medical decision, and in particular "who or what influenced your decision?"</s><s xml:id="_c3AWCpZ">This prompted a wide array of responses.</s><s xml:id="_7hMKEFZ">The most common influence on participants' decision-making was their primary care physician, though several also noted family and friends influencing their decision.</s><s xml:id="_WtjSTyx">We asked another open-ended question: "generally how do you feel about doctors relying on computer systems to make treatment decisions for you?"</s><s xml:id="_tdZKyKy">This prompted a broad array of responses, with some participants expressing fear or anxiety (and occasionally humor) about the increasing use of machines in everyday life.</s><s xml:id="_2Z3jNFK">We often probed to distinguish routine use of electronic health records (EHRs) systems, which were quite familiar to respondents, versus computerized diagnostic tools, which were less familiar.</s></p><p xml:id="_YyfAbHv"><s xml:id="_u2cZWDn">The core of our interviews were the vignettes which asked participants to imagine themselves in particular medical scenarios.</s><s xml:id="_yJHSAkT">We started by asking participants to imagine their primary care physician recommending a change to their diet and exercise, based on a family history of leukemia and advice from "the MedX computer system with data from millions of other patients."</s><s xml:id="_UvmjV2m">This provoked a mild reaction, with most participants noting they were already being told to mind their diet and exercise habits.</s><s xml:id="_cu3aPWm">Then we raised the stakes.</s><s xml:id="_5TfjyGB">Participants were told to suppose they start to feel tired and achy, and their primary care physician wants a second opinion from either an oncologist or a new AI-driven lab that "is more accurate than the oncologist."</s><s xml:id="_xvZPen8">The participants were asked whether they'd choose one or the other, or pay $1000 to visit both.</s><s xml:id="_nDqWW98">Tellingly, the majority of patients said they'd prefer to see only the oncologist, despite being told the oncologist was less accurate than the AI lab.</s></p><p xml:id="_4Pfajte"><s xml:id="_7EY9XjP">We presented another vignette involving sleep apnea, where participants were asked whether they'd rather visit a traditional sleep clinic requiring an overnight stay away from home and interpretation by a human physician versus an at-home device that relies on selfplaced sensors and AI diagnostic interpretation.</s><s xml:id="_UqdRSbP">We saw a broad range of views in response to this vignette, with several participants having strong and perhaps idiosyncratic reactions based on their personal experiences dealing with sleep apnea and visiting sleep clinics.</s></p><p xml:id="_FKyxh5T"><s xml:id="_A5fyxDH">Overall, while some patients expressed confidence that AI systems could achieve greater accuracy in diagnosis and treatment compared to a physician, several patients called on their own experiences with technology to suggest that an AI system could be fallible.</s><s xml:id="_aDdkqgR">Other patients, especially those who were non-White, expressed lack of trust with the healthcare system more generally and recounted anecdotes where they felt unheard or mistreated.</s><s xml:id="_8Ce8RTW">Patients nearly uniformly said they would rely heavily on their physicians to guide their choice of whether an AI system would be used for their diagnosis or treatment, but most nonetheless emphasized that they would generally want to know of such use, suggesting that it is material for their informed consent.</s><s xml:id="_3Gv7g8n">Several patients indicated that they had greater confidence in their human physicians than an AI system to personalize treatment decisions to the patient's own situation.</s><s xml:id="_bJW4Tw7">The technology was more attractive for younger and more educated patients.</s><s xml:id="_CMNpja4">Several patients invoked their belief in God as being important to their healthcare decisions, and some suggested confidence that God would work through human physicians.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_P8ahkb5">Quantitative survey experimental design and materials</head><p xml:id="_wbHR2uc"><s xml:id="_99b64zA">We designed our quantitative phase as a blinded, randomized survey experiment in factorial design, manipulating eight variables by substituting text in or out of the base vignette.</s><s xml:id="_du8SdDJ">Respondents were block randomized by race/ethnicity to experimental conditions.</s><s xml:id="_XXfBtbJ">We counterbalanced whether respondents answered certain covariate questions before or after our vignettes and primary outcomes.</s></p><p xml:id="_5hMGyNB"><s xml:id="_eJyMhb8">The full text of the vignettes and manipulations are shown at osf.io/9y26x.</s><s xml:id="_hpQAbmf">These materials were based on the vignettes that we tested in the qualitative phase, with refinements and clarifications based on feedback from those participants.</s><s xml:id="_H5DGeZu">The base clinical vignette was split into two parts, with an initial segment laying out the patient's history and primary care physician's (PCP's) initial impressions.</s><s xml:id="_Bys2PQK">As one of the experimental manipulations, all respondents saw either a leukemia or a sleep apnea version of the base case, with the PCP explaining that leukemia could be fatal if not properly diagnosed and treated, while sleep apnea was described as interfering with the patient's comfort and lifestyle.</s><s xml:id="_PqsB5AB">Respondents were asked to explain their "reactions and feelings at this point in the story."</s></p><p xml:id="_b7rBfHa"><s xml:id="_SF6ESek">A final segment of the vignette explained that, "your doctor would like to get a second opinion on whether you have leukemia, and if so get the best treatment plan," or "to determine whether you actually have an apnea, determine its type, and determine the best course of treatment, your physician suggests a sleep study."</s><s xml:id="_45uvR6P">The PCP then presented the choice of AI versus physician specialist, followed by the experimental manipulations.</s><s xml:id="_PaaAxNA">Table <ref type="table">1</ref> displays the description of the two providers.</s><s xml:id="_ryyCaCj">Table <ref type="table">2</ref> shows a summary of the manipulations in either Level 1 or Level 2, which followed this presentation.</s><s xml:id="_EQjGHTm">In several of the manipulations, when Level 1 was randomly selected, the vignettes were simply silent about the issue.</s></p><p xml:id="_5GxDrdD"><s xml:id="_qCUHV6W">Table 1.</s><s xml:id="_fvQdmMk">Presentation of Provider Choice.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zatqTNS">Text Presented to Respondents</head><p xml:id="_7fJc6cc"><s xml:id="_wYCEW7q">"Your doctor offers either of two options: â¢ You could go to the offices of Dr. Williams, [a hematologist-oncologist / NA], a specialist doctor who is trained to diagnose [leukemias / sleep disorders], or â¢ You could go to the office of Med-X, which is built around a proprietary computer system designed to diagnose [leukemias / sleep disorders].</s><s xml:id="_aKAUkR7">Your blood and genetic information would be drawn by a nurse, and the medical analysis would be done entirely by a machine using artificial intelligence (AI).</s><s xml:id="_dHeNhAM">With every case it sees from tens of thousands of patients worldwide, the AI system gets more accurate in its diagnoses.</s><s xml:id="_KqU4Bns">If you visit the AI clinic, your data will be de-identified and then become part of the system.</s><s xml:id="_Bz6TxjG">[accuracy manipulation] Your insurance will only cover one of the services so you must choose which one to use to diagnose your possible [sleep apnea / leukemia].</s><s xml:id="_PQMjctC">Your out-of-pocket cost is the same either way.</s><s xml:id="_f24Cda2">[other manipulations]" <ref type="url" target="https://doi.org/10.1371/journal.pdig.0000237.t001">https://doi.org/10.1371/journal.pdig.0000237.t001</ref></s></p><p xml:id="_v238sUn"><s xml:id="_pG7mYc9">Table 2. Experimental Conditions.</s><s xml:id="_j9xETEu">Variable Level 1 Level 2 Illness Severity Leukemia: ". .</s><s xml:id="_kj5MaXQ">.There are several different types of leukemia.</s><s xml:id="_kmUfrCH">Some . . .</s><s xml:id="_tuqyRTq">can spread to lymph nodes or the central nervous system if not treated.</s><s xml:id="_9U8K9gp">Leukemia can be fatal. .</s><s xml:id="_MaGzqWd">.")</s><s xml:id="_Ds3XMSr">Sleep apnea: ". .</s><s xml:id="_K9XTHJp">.you are getting poor sleep at night, with very loud snoring, and sometimes it seems like you stop breathing or gasp for air during sleep. .</s><s xml:id="_YhnpdXP">..</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_W3A3GsJ">AI Accuracy</head><p xml:id="_5RwhR6y"><s xml:id="_DMw9BJQ">Silent: No description of this issue provided in the vignette.</s><s xml:id="_uDCS25x">"Your doctor tells you that, based on scientific studies in leading journals, the AI system is proven more accurate at diagnoses compared to even specialist human physicians."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_w9Vfs3B">AI Listens (Personal)</head><p xml:id="_WYkfZsY"><s xml:id="_Y3R264H">Silent: No description of this issue provided in the vignette.</s><s xml:id="_eqjszDD">"The Med-X clinic staff will carefully listen to understand your lifestyle, preferences, values, and goals. . . .</s><s xml:id="_QGUSBkQ">you will have an extensive 45-minutes interview with a trained counselor, who will ask a range of questions to get your perspective on your healthcare."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sWjGVvk">AI Tailored (Personal)</head><p xml:id="_asu7ygq"><s xml:id="_UCuMGjn">Silent: No description of this issue provided in the vignette.</s><s xml:id="_qDaaSS6">"The AI system's advice will be tailored to you.</s></p><p xml:id="_b8KAb2Q"><s xml:id="_s68aDsJ">[It] will incorporate 36 different measurements and attributes specific to you to generate a unique and personalized treatment plan, just for you."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_yzfRjA4">AI Racial Unbiased</head><p xml:id="_revdKED"><s xml:id="_JrvFNuq">Silent: No description of this issue provided in the vignette.</s><s xml:id="_ePDbvvM">"Although research suggests that human physicians can be biased by racial and ethnic stereotypes, the AI system has been carefully designed and tested to ensure that treatment recommendations are unbiased."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_p8ZQYMx">AI Financial Unbiased</head><p xml:id="_DyENHkF"><s xml:id="_6nzVDkA">Silent: No description of this issue provided in the vignette.</s><s xml:id="_4bk2Cc8">"Although research suggests that human physicians can be biased by their financial relationships with drugmakers and insurance companies, the AI system has been carefully designed and tested to ensure that treatment recommendations are unbiased."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AneAu7K">PCP Incorporates AI Advice</head><p xml:id="_2DBftxZ"><s xml:id="_S393rAH">Defer-Portal: "Your doctor explains, 'From either clinic, you will receive the results as an electronic message in your patient portal, the next day.</s></p><p xml:id="_QJmhXME"><s xml:id="_ys6hT6x">It will tell you the diagnosis and what to do next.'"</s><s xml:id="_K7cc7sB">Incorporate-Explain: "Your doctor explains, "When we get the results back from either clinic, I will explain them and talk them through with you.</s><s xml:id="_uXKQ92s">I will incorporate the results into my ultimate opinion on what we should do next.'"</s><s xml:id="_Wbr5T7w">PCP Nudges toward AI None: "Your doctor says, 'We can get you into either Dr. Williams or the Med-X AI clinic; it is your choice.'"</s><s xml:id="_GUuNaeX">Default-Easy: "Your doctor says, 'For some time, I have been recommending the Med-X AI clinic for all my patients, and the nurse has already confirmed available appointments for you.</s><s xml:id="_MCDjCsk">But if you prefer to see Dr. Williams, I can give you a referral instead.</s><s xml:id="_EpJuRx4">It is your choice.'"</s><s xml:id="_dn6DBMf">Note: For the unbiased and personalization variables, when both types (race and financial, or listens and tailored) were presented, the text was concisely integrated into a single statement, as shown in the survey text at osf.io/9y26x.</s><s xml:id="_9rAQQ9j"><ref type="url" target="https://doi.org/10.1371/journal.pdig.0000237.t002">https://doi.org/10.1371/journal.pdig.0000237.t002</ref></s></p><p xml:id="_P4WW28u"><s xml:id="_a8GAp7G">Our primary outcome variable ("AI uptake") was binary, "Which provider would you choose to diagnose your health problem?</s><s xml:id="_fsgrRXN">Dr. Williams, the specialist physician [or] The Med-X clinic, the AI computer system," with presentation order randomized.</s><s xml:id="_UhVqHtm">Finally, we presented several debriefing questions.</s><s xml:id="_3YqfMY6">These included an attention-check question to test whether the respondent could identify the disease featured in the vignette read a few minutes prior and a self-assessment of the respondent's understanding of the vignette, on a ten-point scale.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_tqhxgSh">Survey administration and screening</head><p xml:id="_MJTZVMY"><s xml:id="_T3yWPdk">YouGov interviewed 2875 U.S. adults who identified as White, Black, Latino, Asian or Native American, who were then matched down to a sample of 2675 to produce the final dataset.</s><s xml:id="_WMbdsnm">To allow well-powered estimates for particular racial and ethnic groups, the sample was designed to over-represent non-White Americans as follows: 650 Whites (24%), 575 Blacks (21%), 550 Latinos (21%), 550 Asians (21%), and 350 Native Americans (13%).</s><s xml:id="_PktCkrn">YouGov's matching and weighting approach is described in S1 Text.</s></p><p xml:id="_KJvmpUR"><s xml:id="_66UKaUR">We report both weighted and unweighted analyses.</s><s xml:id="_znTCW9B">This allows us to both represent the United States population as well as investigate differences by racial groups and those who fully engaged with the vignettes.</s><s xml:id="_fFFbZHw">For the unweighted analyses we exclude respondents based on some of the previously established criteria in the pre-registration plan: those that failed an attention check about whether respondent could correctly recall the disease presented in the vignette, and those reporting that they did not understand the vignette (bottom two levels on 10-point scale).</s><s xml:id="_WuHrBxn">After these removals, the sample for our primary analyses was N = 2,472.</s><s xml:id="_z4z3Ug2">We present models without exclusions in S1</s></p><p xml:id="_SNzE9m9"><s xml:id="_tMqKCEA">Table and S2 Table.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gpGNHYG">Characteristics of the respondents</head><p xml:id="_2XwyHvR"><s xml:id="_cXT27a2">Our sample was diverse and representative after weights are applied.</s><s xml:id="_hfkxkye">As shown in Tables <ref type="table">3</ref> and <ref type="table">4</ref>, in addition to race/ethnicity coverage, respondents span multiple educational levels, with roughly equal amounts of respondents in the categories of high school degree or less, some college or a two-year degree, and a four-year college degree or higher.</s><s xml:id="_fxJCxK5">We have slightly more females in the sample.</s><s xml:id="_XXgJmQj">The average age was 48 years, and about half made less than $50,000 a year (N = 1194; 54%).</s><s xml:id="_fWeKMT4">Nearly one-quarter of respondents (23.27%) identified as political conservatives, as the top-two levels on a standard six-point scale ("very liberal", "liberal", "moderate", "conservative", or "very conservative", with respondents also allowed to say "not sure" or skip the question).</s><s xml:id="_h9UxkRV">Nearly two-thirds of respondents (62.14%) viewed religion as important, as measured by the top two levels on a four-point Likert scale ("very important", "somewhat important", "not too important", or "not at all important").</s><s xml:id="_462Cvat">Table <ref type="table">2B</ref> disaggregates these descriptive statistics by race and ethnic groups.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Ta6HseS">Statistical analysis</head><p xml:id="_yhrzZB9"><s xml:id="_TEJahdJ">Our primary analyses rely on multivariable logistic regressions in STATA, and the computation of predictive margins and 95% confidence intervals (CI), exploiting the randomized design.</s><s xml:id="_6gSer8C">We use Î± = .05</s><s xml:id="_3qdrP4U">as the threshold for significance of p-values.</s></p><p xml:id="_D4n8eQ4"><s xml:id="_JKn7Ej2">To generate estimates representative of the U.S. population, we conducted analyses with weighted data, without exclusions (N = 2672).</s><s xml:id="_EGTchtu">Here we use the Stata svy suite of commands to estimate the proportions shown in Fig <ref type="figure" target="#fig_1">1</ref> and the text.</s><s xml:id="_YHQwAQ4">Combined, these analyses can both speak to our specific interest in diverse populations as well as reflect larger trends in the U.S. population.</s></p><p xml:id="_PunV2f6"><s xml:id="_qssger3">In Table <ref type="table">5</ref>, Model 1 merely shows effects of our experimental conditions controlling for whether covariates were collected before or after the vignettes (order) and familiarity or experience with the illness.</s><s xml:id="_5Az9rYu">Model 2 adds demographic controls shown in the Table, although the sample size is smaller primarily due to missing income data.</s><s xml:id="_fwZq5tc">Model 3 also controls for respondents' attitudes regarding trust in providers, hospitals and AI companies, and these estimates and p-values are displayed in  <ref type="table">6</ref>.</s><s xml:id="_apcetVa">The logistic regression models use unweighted data, exclude respondents who failed the manipulation checks, and use listwise deletion for missing data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_QsN649p">Results</head><p xml:id="_MkNneta"><s xml:id="_4uBWBpj">We found a substantial resistance to artificial intelligence.</s><s xml:id="_fVEcPh6">With weighting representative to the U.S. population, most respondents (52.9%) chose the human doctor and 47.1% chose AI clinic, with some variation along race and ethnicity, as shown in Fig <ref type="figure" target="#fig_1">1</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_C4j6ETb">Effects of experimental manipulations</head><p xml:id="_j4BBRmq"><s xml:id="_pX8mt3b">As shown in Table <ref type="table">5</ref> Model 3, when AI was proven to be more accurate, respondents were substantially more likely to select it; this was one of the most pronounced effects (OR = 1.48,</s><s xml:id="_Rkf3Bap">CI</s></p><p xml:id="_fM24Abx"><s xml:id="_QDHDXZa">Table 3. Descriptive Traits of Participants (N = 2472).</s><s xml:id="_Dy6edqF">Factor Mean (SD) or N (%) Age 48.07 (17.18)</s><s xml:id="_EJKPvVj">Female 1380 (55.83%)</s><s xml:id="_W9gGs24">Married 1183 (47.86%)</s><s xml:id="_K3wqewg">Employed Full-Time 853 (34.51%)</s><s xml:id="_3HPp42Y">Less than 50K Income 1194 (54.27%) High School or Less 864 (34.95%)</s><s xml:id="_KyHeptF">Some College or Associate Degree 777 (31.43%)</s><s xml:id="_9TqCJQd">Bachelor's Degree or More 831 (33.61%)</s><s xml:id="_At6vYPN">Conservative* 575 (23.27%)</s><s xml:id="_MYNrN9z">Religion Important* 1536 (62.14%) White 617 (24.96%)</s><s xml:id="_Gxs4jZz">Black 528 (21.36%)</s><s xml:id="_Eys9Edr">Hispanic 489 (19.78%)</s><s xml:id="_KeEmRh2">Asian 511 (20.67%)</s><s xml:id="_WDKarbC">Native American 327 (13.23%)</s><s xml:id="_nnhRpCU">Good Health* 1897 (76.74%)</s><s xml:id="_7wPsvC5">Have Provider* 2001 (80.95%) Trust Provider* 1956 (79.13%) Trust Hospital* 1735 (70.21%) Trust AI Companies* 1346 (54.45%)</s><s xml:id="_UjzrQdk">Note: Factors shown with asterisk (*) are based on groupings in Likert-scales.</s><s xml:id="_htpeNWH">For example, good health</s></p><p xml:id="_m3MmKxz"><s xml:id="_eryc3T4">(1 = excellent/very good/good), religion viewed as important (1 = very/somewhat important), and conservative (very conservative/conservative).</s><s xml:id="_Ebkpc2G">The trust measures (provider/hospital/AI companies) were grouped by selections of 1-5 on a -5 (complete distrust) to 5 (complete trust) scale.</s><s xml:id="_PXB3fgV">Income was measured on 16-point scale (from less than $10,000 to more than $500,000), education was measured on a 6-point scale (from less than high school to graduate school).</s></p><p xml:id="_jspgBaK"><s xml:id="_V7TaDAT"><ref type="url" target="https://doi.org/10.1371/journal.pdig.0000237.t003">https://doi.org/10.1371/journal.pdig.0000237.t003</ref></s></p><p xml:id="_hBfXdnW"><s xml:id="_bfxwxPz">1.24-1.77,</s><s xml:id="_kRhGEbD">p &lt; .001).</s><s xml:id="_rguDzBX">This effect arises from the manipulation that "Your doctor tells you that, based on scientific studies in leading journals, the AI system is proven more accurate at diagnoses compared to even specialist human physicians."</s></p><p xml:id="_gwHbu8b"><s xml:id="_U3dZ7vG">Table 4. Descriptive Traits of Participants (Mean (SD) or N (%)), Split by Race / Ethnicity (N = 2472).</s><s xml:id="_DZVkq6Q">Factor White Black Hispanic Asian Native American Age 51.43 (17.86) 49.42 (16.63) 44.69 (16.36) 43.63 (16.70) 51.54 (16.35)</s><s xml:id="_HYCUKPp">Female 314 (50.89%) 302 (57.20%) 290 (59.30%) 281 (54.99%) 193 (59.02%)</s><s xml:id="_QZGgrmk">Married 315(51.05%)</s><s xml:id="_NRDdHUV">191 (36.17%) 253 (51.74%) 248 (48.53%) 176 (53.82%)</s><s xml:id="_4Zzbhc4">Employed Full-Time 218 (35.33%) 165 (31.25%) 172 (35.17%) 210 (41.10%) 88(26.91%)</s><s xml:id="_eejCU66">Less than 50K Income 271 (50.00%) 332 (69.17%) 253 (57.24%) 157 (35.93%) 181(60.54%)</s><s xml:id="_PZPnSjd">High School or Less 207 (33.55%) 228(43.18%)</s><s xml:id="_3WHRkJG">247(50.51%)</s><s xml:id="_wQ2CfgE">90(17.61%)</s><s xml:id="_m2483BJ">92(28.13%)</s><s xml:id="_SWjtjTn">Some College or Associate Degree 189 (30.63%) 178 (33.71%) 143 (29.24%) 118 (23.09%) 149 (45.57%)</s><s xml:id="_7y8DCa5">Bachelor's Degree or More 221 (35.82%) 122 (23.10%) 99 (20.25%)</s><s xml:id="_4PZYMaF">303 (59.30%) 86 (26.30%)</s><s xml:id="_kapwFAu">Conservative* 183 (29.66%) 67 (12.69%) 110 (22.49%) 91 (17.81%) 124 (38.04%)</s><s xml:id="_PVA494Y">Religion Important* 331(53.65%)</s><s xml:id="_adz3Pvq">399(75.57%)</s><s xml:id="_t8NwB48">337 (68.92%) 253 (49.51%) 216 (66.06%)</s><s xml:id="_zNtPZz9">Good Health* 468 (75.85%) 398 (75.38%) 379 (77.51%) 415 (81.21%) 237 (72.48%)</s><s xml:id="_GPJJv7v">Have Provider 509 (82.50%) 416 (78.79%) 372 (76.07%) 414 (81.02%) 290 (88.69%) Trust Provider* 492 (79.74%) 409 (77.46%) 372 (76.07%) 414 (81.02%) 269 (82.26%) Trust Hospital* 438 (70.99%) 351 (66.60%) 346 (70.76%) 373 (72.99%) 227 (69.42%) Trust AI Companies* 305 (49.42%) 301 (57.01%) 282 (57.67%) 312 (61.06%) 143 (43.73%)</s><s xml:id="_8ggj3hQ">N 617 528 489 511 327 Note: Factors shown with asterisk (*) are based on groupings in Likert-scales.</s><s xml:id="_X5GGh25">For example, good health (1 = excellent/very good/good), religion viewed as important</s></p><p xml:id="_GCKjj4j"><s xml:id="_mNgQ9pu">(1 = very/somewhat important), and conservative (very conservative/conservative).</s><s xml:id="_TdhCAjy">The trust measures (provider/hospital/AI companies) were grouped by selections of 1-5 on a -5 (complete distrust) to 5 (complete trust) scale.</s><s xml:id="_wKzHVqX">Income was measured on 16-point scale (from less than $10,000 to more than $500,000), education was measured on a 6-point scale (from less than high school to graduate school).</s></p><p xml:id="_eWGAWSd"><s xml:id="_XBPNbPP"><ref type="url" target="https://doi.org/10.1371/journal.pdig.0000237.t004">https://doi.org/10.1371/journal.pdig.0000237.t004</ref></s><s xml:id="_9QRBnXg">When the PCP nudged the patient toward AI as the established option, patients were more likely to choose it (OR = 1.25, CI: 1.05-1.50,</s><s xml:id="_RT63dbv">p = .013).</s><s xml:id="_2BSebWR">This effect arises from the PCP saying that "For some time, I have been recommending the Med-X AI clinic for all my patients, and the nurse has already confirmed available appointments for you."</s></p><p xml:id="_E3Uzr2Y"><s xml:id="_zHUAnRW">The results also show greater AI uptake when the AI system is personalized to listen to the patient (OR = 1.27,</s><s xml:id="_AUXzJ4S">CI: 1.07-1.52,</s><s xml:id="_3X32wqd">p = .008).</s><s xml:id="_UC3EDut">This effect is caused by the text manipulation: "The Med-X clinic staff will carefully listen to understand your lifestyle, preferences, values,</s></p><p xml:id="_SZcTEuF"><s xml:id="_5Xk2bNx">Table 5. Logistic Regression Predicting AI Provider Choice Versus Human Physician.</s><s xml:id="_WSDsHTV">Model 1: Experimental Conditions Model 2: + Demographics Model 3: + Trust Illness Severity 0.96 (0.08) 1.00 (0.09) 0.98 (0.09) Accuracy 1.51*** (0.12) 1.54*** (0.14) 1.48*** (0.13) Incorporation 0.92 (0.07) 0.91 (0.08) 0.88 (0.08) Established 1.28** (0.10) 1.28** (0.11) 1.25* (0.11) Tailored 1.06 (0.09) 1.07 (0.09) 1.11 (0.10) Listens 1.20* (0.10) 1.23* (0.11) 1.27** (0.12) Race Unbiased 1.08 (0.09) 1.07 (0.09) 1.10 (0.10) Financial Unbiased 1.13 (0.09) 1.12 (0.10) 1.10 (0.10) Order 1.20* (0.10) 1.20* (0.11) 1.13 (0.10) Illness Experience 1.12 (0.10) 1.18 + (0.11) 1.17 (0.12) Black 0.84 (0.11) 0.73* (0.10) Hispanic 1.00 (0.14) 0.88 (0.12) Asian 0.94 (0.13) 0.85 (0.12) Native 1.26 (0.19) 1.37* (0.21) Income 1.02 (0.02) 1.02 (0.02) Age 0.99* (0.00) 0.99* (0.00) Conservative 0.61*** (0.07) 0.65*** (0.08) Religion Important 0.67*** (0.06) 0.64*** (0.06) Sex 1.00 (0.09) 0.98 (0.09) Education 1.07* (0.04) 1.10** (0.04) Married 0.96 (0.09) 0.97 (0.10) Employed Full-Time 1.03 (0.10) 1.05 (0.11) Good Health 1.09 (0.12) 0.99 (0.11) Have Provider 1.12 (0.13) 1.08 (0.14) Trust Provider 0.96 (0.03) Trust Hospital 0.95+ (0.03) Trust AI Companies 1.29*** (0.03) N 2471 2199 2198 Notes: Odds Ratios; Standard errors in parentheses + p &lt; .10</s><s xml:id="_uan8dZC">* p &lt; .05</s><s xml:id="_Y9Yg3A2">** p &lt; .01</s><s xml:id="_2RX77xj">*** p &lt; .001.</s><s xml:id="_2qssvhv">For race, Whites are the reference group.</s><s xml:id="_hGtuhTv">Income (16-point scale from less than $10,000 to more than $500,000), age (years), education (6-point scale from less than high school to graduate school), sex (1 = male, 0 = female).</s><s xml:id="_zGzXAVd">Likert-scales are coded as good health (1 = excellent/very good/good), religion viewed as important (1 = very/somewhat important), and conservative (very conservative/conservative).</s><s xml:id="_KKHHHSq">Analysis excludes respondents that failed an attention check about whether respondent could correctly recall the disease presented in the vignette, and those reporting that they did not understand the vignette (bottom two levels on 10-point scale).</s><s xml:id="_8fSucm6"><ref type="url" target="https://doi.org/10.1371/journal.pdig.0000237.t005">https://doi.org/10.1371/journal.pdig.0000237.t005</ref></s></p><p xml:id="_6snABtc"><s xml:id="_DyKQfzN">and goals.</s><s xml:id="_HY5utzd">At the Med-X clinic, you will have an extensive 45-minutes interview with a trained counselor, who will ask a range of questions to get your perspective on your healthcare."</s><s xml:id="_CANKux9">Other experimental conditions did not have significant effects, such as illness severity, whether the doctor would merely defer to the results or incorporate them into his or her opinion, whether the AI system would be racially or financially unbiased, and whether the AI used multiple measurements to tailor the treatment plan (all p &gt; .05).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_neuaKZt">Demographic and attitudinal associations</head><p xml:id="_zkD5vsr"><s xml:id="_kehzUVG">Also shown in Table <ref type="table">5</ref> Model 3, several demographic and attitudinal variables were also associated with AI uptake.</s><s xml:id="_ruauzrB">Older respondents had significantly lower odds of choosing AI (OR: .99,</s><s xml:id="_8e9feAR">CI: .987-.999, p = .03),</s><s xml:id="_DeHHcuV">which is modelled as a year-by-year effect.</s><s xml:id="_bnHGYFu">Conservatives have 35% less than equal odds than non-conservatives of choosing AI (OR: .65,</s><s xml:id="_gcSVNhw">CI: .52-.81, p &lt; .001)</s><s xml:id="_PQQ9vYX">and for each unit increase in education, the odds are 1.10 greater for selecting an AI provider (OR: 1.10, CI: 1.03-1.18,</s><s xml:id="_XDzKZPP">p = .004).</s><s xml:id="_JdvK2zx">Respondents who viewed religion as important had significantly lower odds of choosing AI (OR: .64,</s><s xml:id="_kMuQuwa">CI: .52-.77, p &lt; .001).</s><s xml:id="_7ZGqeYz">In terms of the attitudinal variables, trust in AI companies was associated with significantly increased odds of choosing an AI provider (OR: 1.29, CI: 1.23-1.36,</s><s xml:id="_hc2vHTp">p &lt; .001).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cmguZhs">Associations with race/ethnicity</head><p xml:id="_6nZQYsd"><s xml:id="_nPgFs7Y">As shown in Table <ref type="table">5</ref> Model 3 and Fig 2 , we found significant associations between AI uptake and respondent's race or ethnicity; specifically Black respondents had lower odds of selecting AI (OR = .73,</s><s xml:id="_3BcwqVG">CI: .55-.96, p = .023)</s><s xml:id="_UfCGxst">and Native Americans had higher odds of selecting AI (OR: 1.37, CI: 1.01-1.87,</s><s xml:id="_YgRjS8T">p = .041)</s><s xml:id="_KgPZrGV">than White respondents.</s></p><p xml:id="_JCefk35"><s xml:id="_3pZn5Xa">Table <ref type="table">64</ref> investigates the effects of other variables within race/ethnicity subsets.</s><s xml:id="_CVrC6Br">AI accuracy continued to be significant and positive regarding AI provider selection for Whites, Hispanics, and Asians.</s><s xml:id="_F75bTdq">The PCP nudge towards AI as an established and easy option resulted in significantly greater odds of choosing AI for Hispanics and Asians, whereas learning that AI was unbiased by financial relationships with drugmakers and insurance companies increased AI selection for Native Americans.</s><s xml:id="_DyBuyW3">Interestingly, the racial subsets analyses revealed other demographic associations.</s><s xml:id="_pzkgEFz">Being conservative had the greatest effects on lowering AI selection for Whites and Asians, while the importance of religion significantly lowered odds for Whites and Blacks.</s><s xml:id="_KyJrJm4">For example, Black respondents who viewed religion as important have 57% less than equal odds of choosing the AI clinic, compared to those placing less importance on religion (OR: .43,</s><s xml:id="_bJwWfvv">CI: .27-.69, p = .001).</s></p><p xml:id="_RF4b9DX"><s xml:id="_a5ndqa6">Table 6.</s><s xml:id="_32HKTTg">Logistic Regression in Respondent Race/Ethnicity Subsets Predicting AI Provider Choice Versus Human Physician.</s><s xml:id="_fUT346t">White Black Hispanic Asian Native Illness Severity 0.97 (0.20) 1.05 (0.23) 1.01 (0.22) 0.78 (0.17) 0.97 (0.27) Accuracy 1.76** (0.33) 1.42 + (0.28) 1.68* (0.36) 1.69* (0.37) 0.82 (0.22) Incorporation 0.97 (0.18) 0.86 (0.17) 0.81 (0.17) 0.85 (0.18) 0.70 (0.18) Established 1.16 (0.22) 0.91 (0.18) 1.57* (0.33) 1.63* (0.35) 1.15 (0.30) Tailored 1.11 (0.21) 1.33 (0.27) 1.09 (0.23) 1.14 (0.25) 0.92 (0.24) Listens 1.25 (0.24) 1.45 + (0.29) 1.22 (0.25) 1.10 (0.23) 1.48 (0.40) Race Unbiased 1.10 (0.21) 1.17 (0.23) 1.49 + (0.32) 0.85 (0.18) 0.95 (0.25) Financial Unbiased 0.90 (0.17) 1.11 (0.23) 1.15 (0.24) 1.13 (0.24) 1.86* (0.50) Income 1.00 (0.03) 1.02 (0.04) 1.07 + (0.04) 1.03 (0.03) 0.96 (0.05) Age 1.00 (0.01) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01) 0.99 (0.01) Conservative 0.58* (0.14) 0.82 (0.25) 0.69 (0.18) 0.55* (0.16) 0.60 + (0.18) Religion Important 0.59** (0.12) 0.43*** (0.10) 0.79 (0.19) 0.66 + (0.15) 0.80 (0.24) Sex 0.81 (0.16) 1.43 + (0.30) 1.44 + (0.32) 0.68 + (0.15) 0.75 (0.21) Education 1.10 (0.08) 1.13 + (0.08) 1.15 + (0.09) 1.03 (0.09) 1.31* (0.15) Married 1.00 (0.21) 0.74 (0.17) 1.14 (0.25) 0.75 (0.18) 1.48 (0.42) Employed Fulltime 1.02 (0.22) 1.13 (0.26) 1.07 (0.25) 1.41 (0.34) 0.62 (0.21) Good Health 1.27 (0.30) 1.07 (0.26) 0.55* (0.15) 1.04 (0.30) 1.13 (0.33) Have Provider 1.01 (0.30) 1.65 + (0.46) 0.76 (0.21) 1.01 (0.30) 1.54 (0.64) Trust Provider 0.94 (0.06) 0.94 (0.06) 0.94 (0.06) 1.02 (0.07) 0.97 (0.07) Trust Hospital 0.92 (0.05) 0.99 (0.07) 0.98 (0.06) 0.83* (0.06) 0.99 (0.07) Trust AI Companies 1.36*** (0.08) 1.20** (0.07) 1.36 *** (0.08) 1.36 *** (0.09) 1.34 *** (0.09) Order 1.05 (0.20) 1.17 (0.23) 1.01 (0.21) 1.50 + (0.32) 0.99 (0.27) Illness Experience 1.12 (0.23) 1.41 (0.32) 0.74 (0.17) 1.29 (0.30) 1.28 (0.38) N 542 479 442 437 298 Notes: Odds Ratios; Standard errors in parentheses + p &lt; .10</s><s xml:id="_rZgtTZA">* p &lt; .05</s><s xml:id="_WFc7PP8">** p &lt; .01</s><s xml:id="_pKnBAuB">*** p &lt; .001.</s><s xml:id="_cbvruNG">Income (16-point scale from less than $10,000 to more than $500,000), age (years), education (6-point scale from less than high school to graduate school), sex (1 = male, 0 = female).</s><s xml:id="_NTe5ppm">Likert-scales are coded as good health (1 = excellent/very good/good), religion viewed as important (1 = very/somewhat important), and conservative (very conservative/conservative).</s><s xml:id="_zK7PyAF"><ref type="url" target="https://doi.org/10.1371/journal.pdig.0000237.t006">https://doi.org/10.1371/journal.pdig.0000237.t006</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MbThuRP">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fmkp9Fr">Strengths and limitations</head><p xml:id="_MVWXNQB"><s xml:id="_QrP3CJR">Our qualitative interview study with actual patients relies on a small, convenience sample in one city.</s><s xml:id="_pNKTshj">As such it does not allow strong conclusions when standing alone.</s><s xml:id="_RjTbKJf">It does provide a groundwork for our rigorous quantitative approach, helping us to ensure that the vignettes will be clear and understandable, and also generating hypotheses subject to quantitative testing subsequently, through systematic manipulation of the vignettes.</s><s xml:id="_F56CQdY">Those hypotheses are tested in our randomized, blinded experiment, which allows causal inference about the impact of the manipulations, and the factorial design allows strong statistical power, because every respondent provides an observation for every variable.</s><s xml:id="_NqbBmB5">Our diverse population from a high-quality survey sample allows extrapolation to the U.S. population, when weighted.</s></p><p xml:id="_K2FT6ay"><s xml:id="_3PWBbvr">Prior work has shown that vignette experiments can have external validity <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> but it may be harder for respondents to predict their own decisions in peculiar situations they have not experienced and where emotional salience is high.</s><s xml:id="_z88Jvsq">We speculate that in a real-world context, patients may be even more susceptible to their PCP's influence.</s></p><p xml:id="_VsxftGS"><s xml:id="_cswxKYr">When it comes to race, we did find variations in preferences as to the AI system, but we do not suggest that these differences are likely to be biological or genetic in particular.</s><s xml:id="_w5gJybw">Specifically, Black respondents were less likely to choose the AI system.</s><s xml:id="_6bQCnDq">Here, longstanding effects from structural racism could manifest in continued distrust.</s><s xml:id="_bbS7Vpx">In contrast, Native Americans were more likely to choose the AI system, presenting a puzzle.</s><s xml:id="_P5tyCmm">Our Native American sample is older and more conservative than the national average for the group, and also had the smallest sample size of the racial groups, so we cannot rule out that this could be due to unmeasured variables affecting the outcome.</s><s xml:id="_sHyRNHu">Future studies with a larger sample could better shed light on this effect.</s></p><p xml:id="_SKDDGJb"><s xml:id="_uWBerNP">We purposefully included hypothetical features in our descriptions of the AI system, when testing a range of manipulations, such as its lack of racial bias or its proven accuracy beyond that of human specialists.</s><s xml:id="_nKxnVer">However, these traits may or may not be true about any particular AI system.</s></p><p xml:id="_Kg3JrXK"><s xml:id="_qtEHpwH">While we tested two distinct clinical vignettes (leukemia and sleep apnea) with very different levels of severity and found similar results, other clinical situations may be different.</s><s xml:id="_Q78KQat">Neither of these scenarios were presented as being acute or emergent, where patients may have greater deference to their PCP such situations <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>.</s><s xml:id="_BptZ3kW">Other manipulations or other implementations thereof may also have different effects.</s></p><p xml:id="_gNfxPXD"><s xml:id="_S9kAYSQ">Finally, in addition to the exclusions noted above, we had pre-registered a plan to also exclude respondents who were familiar with the condition (leukemia or sleep apnea, depending on experimental assignment) because of their work in a healthcare setting, or because they, or a close family or friend, has had it.</s><s xml:id="_vyVwTEp">However, a larger than expected portion of the sample turned out to be familiar with the conditions (N = 998) and these respondents were more familiar with sleep apnea (N = 665) than leukemia (N = 333).</s><s xml:id="_R8aZrBb">Excluding those respondents would unbalance the experimental conditions, and dramatically reduce statistical power, especially for racial/ethnic subgroups.</s><s xml:id="_AsfkbTe">Instead, we control for illness familiarity in the main models and include models that exclude illness familiarity in S1 Table and S2 Table .</s><s xml:id="_dUxM6cs">Results for experimental manipulations were similar, with AI accuracy, PCP nudging as established care, and AI listening to patients all significant predictors of uptake.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AejrjTf">Findings and implications</head><p xml:id="_Z2BVWmz"><s xml:id="_9XDUv6f">Consistent with other studies, we found substantial resistance to the use of AI, what some have called "algorithm aversion" <ref type="bibr" target="#b4">[5]</ref> or "robophobia" <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_eBYhPSb">Our study contributes to this literature by showing the robustness of this resistance to AI in a diverse population of patients and across a range of other manipulated features of and applications of an AI system.</s></p><p xml:id="_xgf9Ga6"><s xml:id="_SWSSwB9">We find this aversion to AI diagnosis across two levels of disease severity, within specific racial and ethnic groups, and even where the AI was proven to be more accurate and the PCP nudged the patient towards AI, though these efforts were significantly helpful.</s><s xml:id="_ZGUjXay">Some of our hypothesized attempts to mitigate this resistance were ineffectual.</s></p><p xml:id="_AgCS5pU"><s xml:id="_4ZeXQBm">Most surprising, having the PCP emphasize that he or she would explain and incorporate the AI system's advice into the ultimate treatment decision did not increase uptake of AI.</s><s xml:id="_AmtC6sX">We expected greater resistance to AI where the ultimately treatment decision is simply outsourced to this machine.</s><s xml:id="_WKNuhXv">In contrast, we expected that patients would be more accepting of medical AI in the half of the vignettes where AI was presented as merely an input to their trusted human physician's ultimate decision.</s></p><p xml:id="_EdAS7Rc"><s xml:id="_PYuFJur">In a world where AI is (or will have the potential to be) actually more accurate than human specialists, our findings suggest that patients may suffer additional mortality and morbidity, and the healthcare system may suffer inefficiencies, due to patient resistance.</s><s xml:id="_wdbMcMg">The passage of time and expanded use of AI in a range of settings familiar to laypersons (such as self-driving cars) may help patients become more familiar with and supportive of AI in healthcare.</s><s xml:id="_9wvqYhV">Indeed, we found that generalized trust in AI companies is nearly as important for AI uptake as the disclosure that this particular AI is proven more accurate that human specialists.</s><s xml:id="_CUtvwUm">On the other hand, some sort of future scandal or crisis or politicization associated with AI may actually harm overall trust in such systems.</s></p><p xml:id="_5Xxsezj"><s xml:id="_ekfhfbm">For simplicity, respondents were asked to assume insurance coverage and out-of-pocket costs would be the same for either clinic.</s><s xml:id="_aGDjsF2">Insurers or healthcare systems may someday use these economic dimensions to direct or undermine patient choice of provider, as a form of value-based reimbursement.</s><s xml:id="_rNw85Mq">Additionally, physicians or healthcare systems may deploy AI without giving the patient a choice or asking permission, especially where AI has become the standard of care.</s><s xml:id="_VQbqwu4">While such economic, professional, and managerial gatekeeping are common in a range of settings, they raise distinct ethical and legal concerns <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>, recognizing that patient autonomy and patient welfare may sometimes come into tension.</s><s xml:id="_MsbUqAT">In addition to our focus on patient perspectives, additional research is required on drivers of physician uptake of AI.</s></p><p xml:id="_XyypPZJ"><s xml:id="_53D24gG">Our findings will be useful for the development of theory-based and evidence-driven recommendations for how physicians and patients might integrate AI into the informed consent process and into real world use and delivery of care.</s><s xml:id="_g6PBbte">Patient resistance to AI diagnosis may impinge uptake in ways that undermine treatment goals, but human physicians can support adoption, where the technology is designed with the patient experience in mind and supported by evidence of accuracy.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_wAGbB3F"><s xml:id="_JqBTZ72">Fig 2 and discussed in the text, except where otherwise noted.</s><s xml:id="_c3fFb7M">Wald tests of model fit find significant improvement moving from Model 1 to Model 2 [Ï 2 (16, N = 2199) = 87.15;</s><s xml:id="_bagzAD2">p &lt; .001]</s><s xml:id="_DDRJFNq">as well as moving from Model 2 to Model 3 [Ï 2 (3, N = 2198) = 106.75;</s><s xml:id="_gWGUC5w">p &lt; .001].</s><s xml:id="_2tyykz7">We then analyze the final model in racial subsets in Table</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 1 .</head><label>1</label><figDesc><div><p xml:id="_TyEfZAU"><s xml:id="_dYytKq6">Fig 1. Proportion of Respondents Selecting an AI Provider with 95% Confidence Intervals by Respondent Race / Ethnicity (N = 2672, Weighted to Represent U.S. Population).</s><s xml:id="_Gxu9ZMD">Notes: Using the 2018 American Community Survey as the reference, the sample was weighted age, gender, race/ethnicity, years of education, and region to produce estimates for the U.S. population.</s><s xml:id="_BSCxCX8">https://doi.org/10.1371/journal.pdig.0000237.g001</s></p></div></figDesc><graphic coords="8,200.01,446.17,360.00,216.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 2 .</head><label>2</label><figDesc><div><p xml:id="_ruz3YEv"><s xml:id="_UM8f2C5">Fig 2. Odds of Selecting an AI Provider by Experimental Condition, Demographics, and Attitudinal Variables, With 95% Confidence Intervals (N = 2198).</s><s xml:id="_TSrzdQE">Notes: Analysis excludes respondents that failed an attention check requiring recall of the disease presented in the vignette, and those reporting that they did not understand the vignette (bottom two levels on 10-point scale).</s><s xml:id="_2CvfzMK">See Table 3, Model 3 for details.</s><s xml:id="_EWrAZYJ">https://doi.org/10.1371/journal.pdig.0000237.g002</s></p></div></figDesc><graphic coords="10,200.01,78.01,360.00,261.81" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_5pxUtRs"><s xml:id="_mhjeAEN">PLOS Digital Health | https://doi.org/10.1371/journal.pdig.0000237May</s><s xml:id="_2DJRWRU">19, 2023</s></p></note>
		</body>
		<back>

			<div type="funding">
<div xml:id="_aNrMwzm"><p xml:id="_ymvxEMt"><s xml:id="_GgC8X57">This study was funded by the <rs type="funder">National Institutes of Health</rs> (<rs type="grantNumber">3R25HL126140-05S1</rs> to CR may help increase acceptance.</s><s xml:id="_tUSvPh8">To ensure that the benefits of AI are secured in clinical practice, future research on best methods of physician incorporation and patient decision making is required.</s><s xml:id="_MqxgUHj">and AW).</s><s xml:id="_tJqvUC3">The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dRD63SM">
					<idno type="grant-number">3R25HL126140-05S1</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_BjHGqJh"><p xml:id="_Z5DPTjw"><s xml:id="_sBChc6s">Analysis was done in Stata 17.</s><s xml:id="_d7yVm6x">Data and code are publicly available on the Open Science Framework, osf.io/9y26x.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_c2AxzQT" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_MaCKngK">How AI is improving cancer diagnostics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Savage</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/d41586-020-00847-2" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_J4Q7Bvv">Nature</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">94</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019-03">2019 Mar</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Savage N. How AI is improving cancer diagnostics. Nature [Internet]. 2019 Mar [cited 2021 May 10]; 2 (94):1-10. Available from: https://www.nature.com/articles/d41586-020-00847-2</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_uSkWJ7Z">Medical device surveillance with electronic health records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Â´c</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Huddleston</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Giori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Delp</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-019-0168-z</idno>
		<ptr target="https://doi.org/10.1038/s41746-019-0168-zPMID:31583282" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_ybw595H">NP J Digit Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">94</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019 Sept</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Callahan A, Fries JA, Re Â´C, Huddleston JI III, Giori NJ, Delp S, et al. Medical device surveillance with electronic health records. NP J Digit Med [Internet]. 2019 Sept. [cited 2021 May 10]; 2(94):1-10. Avail- able from: https://www.nature.com/articles/s41746-019-0168-z.pdf https://doi.org/10.1038/s41746- 019-0168-z PMID: 31583282</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_jyYm7nE">Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges, and future perspectives</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanchez-Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00439-019-01970-5</idno>
		<ptr target="https://doi.org/10.1007/s00439-019-01970-5PMID:30671672" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_BRG9Ty7">Hum Genet</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="124" />
			<date type="published" when="2019-02">2019 Feb</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu J, Yang P, Xue S, Sharma B, Sanchez-Martin M, Wang F, et al. Translating cancer genomics into precision medicine with artificial intelligence: applications, challenges, and future perspectives. Hum Genet. 2019 Feb; 138(2);109-24. https://doi.org/10.1007/s00439-019-01970-5 PMID: 30671672</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><surname>Robophobia</surname></persName>
		</author>
		<idno type="DOI">10.2172/10103207</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_X4jkMW6">Univ Colo Law Rev</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Woods AK. Robophobia. Univ Colo Law Rev. 2021; 93:1.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_t9Y72x9">Algorithm aversion: people erroneously avoid algorithms after seeing them err</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Dietvorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1037/xge0000033</idno>
		<ptr target="https://doi.org/10.1037/xge0000033PMID:25401381" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_ptjQ4CQ">J Exp Psychol</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dietvorst BJ, Simmons JP, Massey C. Algorithm aversion: people erroneously avoid algorithms after seeing them err. J Exp Psychol. 2015; 144(1):114-26. https://doi.org/10.1037/xge0000033 PMID: 25401381</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main" xml:id="_TUWNsU2">Clinical vs. statistical prediction: a theoretical analysis and a review of the evidence</title>
		<author>
			<persName><forename type="first">P</forename><surname>Meehl</surname></persName>
		</author>
		<idno type="DOI">10.1037/11281-000</idno>
		<imprint>
			<date type="published" when="1954">1954</date>
			<publisher>Echo Point Books and Media</publisher>
			<pubPlace>Brattleboro, VT</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Meehl P. Clinical vs. statistical prediction: a theoretical analysis and a review of the evidence. Brattle- boro, VT: Echo Point Books and Media; 1954.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_tFZxXbm">Do patients trust computers?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Promberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<idno type="DOI">10.1002/bdm.542</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RgDuvHp">J Behav Decis Making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="455" to="468" />
			<date type="published" when="2006-11">2006 Nov</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Promberger M, Baron J. Do patients trust computers?. J Behav Decis Making. 2006 Nov; 19:455-68.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_KD2StYv">Patients derogate physicians who use a computer-assisted diagnostic aid</title>
		<author>
			<persName><forename type="first">H</forename><surname>Arkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Medow</surname></persName>
		</author>
		<idno type="DOI">10.1177/0272989x06297391</idno>
		<ptr target="https://doi.org/10.1177/0272989X06297391PMID:17409368" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_pZ4nAsH">Med Decis Making</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="189" to="202" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arkes H, Shaffer V, Medow M. Patients derogate physicians who use a computer-assisted diagnostic aid. Med Decis Making. 2007; 27:189-202. https://doi.org/10.1177/0272989X06297391 PMID: 17409368</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_sAtntMX">Why do patients derogate physicians who use a computerbased diagnostic support system?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Merkle</surname></persName>
		</author>
		<idno type="DOI">10.1177/0272989x12453501</idno>
		<ptr target="https://doi.org/10.1177/0272989X12453501PMID:22820049" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_JczNNrT">Med Decis Making</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="118" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shaffer V, Probst CA, Merkle EC. 2012. Why do patients derogate physicians who use a computer- based diagnostic support system?. Med Decis Making. 2012; 33(1):108-18. https://doi.org/10.1177/ 0272989X12453501 PMID: 22820049</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_NSkqPAG">Resistance to medical artificial intelligence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Longoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morewedge</surname></persName>
		</author>
		<idno type="DOI">10.1093/jcr/ucz013</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kyjYQm7">J Consum Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="629" to="650" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Longoni C, Bonezzi A, Morewedge C. Resistance to medical artificial intelligence. J Consum Res. 2019; 46:629-50.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_XGCVaXT">Resistance to medical artificial intelligence is an attribute in a compensatory decision process: Response to Pezzo and Beckstead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Longoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonezzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morewedge</surname></persName>
		</author>
		<idno type="DOI">10.1017/s1930297500007233</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_q8cBrgM">Judgm Decis Mak</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="446" to="448" />
			<date type="published" when="2020-01">2020 Jan</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Longoni C, Bonezzi A, Morewedge C. Resistance to medical artificial intelligence is an attribute in a compensatory decision process: Response to Pezzo and Beckstead. Judgm Decis Mak. 2020 Jan; 15:446-8.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_88cEpCG">Task-dependent algorithm aversion</title>
		<author>
			<persName><forename type="first">N</forename><surname>Castelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.1177/0022243719851788</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eeBbYdf">J Marketing Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="809" to="825" />
			<date type="published" when="2019-10">2019 Oct</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Castelo N, Bos MW, Lehmann DR. Task-dependent algorithm aversion. J Marketing Res. 2019 Oct; 56 (5):809-25.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_V7MJwEv">The future of breast cancer screening: what do participants in a breast cancer screening program think about automation using artificial intelligence?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Jonmarker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Brandberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lindholm</surname></persName>
		</author>
		<idno type="DOI">10.1177/2058460119880315</idno>
		<ptr target="https://doi.org/10.1177/2058460119880315PMID:31839989" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_V6aVbpM">Acta Radiol Open</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2058460119880315</biblScope>
			<date type="published" when="2019-12">2019 Dec</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jonmarker O, Strand F, Brandberg Y, Lindholm P. The future of breast cancer screening: what do par- ticipants in a breast cancer screening program think about automation using artificial intelligence?. Acta Radiol Open. 2019 Dec; 8(12):2058460119880315. https://doi.org/10.1177/2058460119880315 PMID: 31839989</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_FD3ujeJ">Attitudes of patients and their relatives toward artificial intelligence in neurosurgery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Palmisciano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jamjoom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Marcus</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wneu.2020.03.029</idno>
		<idno type="PMID">32179185</idno>
		<ptr target="https://doi.org/10.1016/j.wneu.2020.03.029" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_sY8mRgV">World Neurosurg</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="2020-06-01">2020 Jun 1</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Palmisciano P, Jamjoom AA, Taylor D, Stoyanov D, Marcus HJ. Attitudes of patients and their relatives toward artificial intelligence in neurosurgery. World Neurosurg. 2020 Jun 1; 138:e627-33. https://doi. org/10.1016/j.wneu.2020.03.029 PMID: 32179185</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_vnUXCzB">Algorithm appreciation: People prefer algorithmic to human judgment</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Logg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Minson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.obhdp.2018.12.005</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_a9uJJqZ">Organ Behav Decis Process</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="90" to="103" />
			<date type="published" when="2019-03-01">2019 Mar 1</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Logg JM, Minson JA, Moore DA. Algorithm appreciation: People prefer algorithmic to human judgment. Organ Behav Decis Process. 2019 Mar 1; 151:90-103.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_UVuyVWU">The impact of managed care on patients&apos; trust in medical care and their physicians</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mechanic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlesinger</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.1996.03530450083048</idno>
		<idno type="PMID">8637148</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PvGDqyR">JAMA</title>
		<imprint>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page" from="1693" to="1697" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mechanic D, Schlesinger M. The impact of managed care on patients&apos; trust in medical care and their physicians. JAMA. 1996; 275:1693-7. PMID: 8637148</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_NGkjBEa">Linking primary care performance to outcomes of care</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tarlov</surname></persName>
		</author>
		<idno type="DOI">10.1097/00005650-199805000-00012</idno>
		<idno type="PMID">9752374</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_He4vEH2">J Fam Pract</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="213" to="220" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Safran DG, Taira DA, Rogers WH, Kosinski M, Ware JE, Tarlov AR. Linking primary care performance to outcomes of care. J Fam Pract. 1998; 47:213-20. PMID: 9752374</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_a92dRzY">A legacy of distrust: African Americans and medical research</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Gamble</surname></persName>
		</author>
		<idno type="PMID">8123285</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VVtkXxG">Am J Prev Med</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="35" to="38" />
			<date type="published" when="1993-11">1993 Nov-Dec</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
	<note type="raw_reference">Gamble VN. A legacy of distrust: African Americans and medical research. Am J Prev Med. 1993 Nov- Dec; 9(6 Suppl):35-38. PMID: 8123285</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_HfDKfeG">Uses and abuses of Tuskegee</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Fairchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bayer</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.284.5416.919</idno>
		<idno type="PMID">10357678</idno>
		<ptr target="https://doi.org/10.1126/science.284.5416.919" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_epfBcvB">Science</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="issue">5416</biblScope>
			<biblScope unit="page" from="919" to="921" />
			<date type="published" when="1999-05-07">1999 May 7</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fairchild AL, Bayer R. Uses and abuses of Tuskegee. Science. 1999 May 7; 284(5416):919-921. https://doi.org/10.1126/science.284.5416.919 PMID: 10357678</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_dMXUTRb">Racial differences in factors that influence the willingness to participate in medical research studies</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Shavers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Burmeister</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1047-2797%2801%2900265-4</idno>
		<ptr target="https://doi.org/10.1016/s1047-2797(01)00265-4PMID:11988413" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_WbcScgp">Ann Epidemiol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="248" to="256" />
			<date type="published" when="2002-05">2002 May</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shavers VL, Lynch CF, Burmeister LF. Racial differences in factors that influence the willingness to par- ticipate in medical research studies. Ann Epidemiol. 2002 May; 12(4):248-256. https://doi.org/10.1016/ s1047-2797(01)00265-4 PMID: 11988413</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_prs7Xjb">Understanding concordance in patient-physician relationships: personal and ethnic dimensions of shared identity</title>
		<author>
			<persName><forename type="first">Rl O'</forename><surname>Street</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haidet</surname></persName>
		</author>
		<idno type="DOI">10.1370/afm.821</idno>
		<ptr target="https://doi.org/10.1370/afm.821PMID:18474881" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_KDjk59C">Ann Fam Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="198" to="205" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Street RL O&apos;Malley KJ, Cooper LA Haidet P. Understanding concordance in patient-physician relation- ships: personal and ethnic dimensions of shared identity. Ann Fam Med [Internet]. 2008 [cited 2021 May 10]; 6(3):198-205. Available from: https://doi.org/10.1370/afm.821 PMID: 18474881</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_2zGwdD5">Patient-physician racial/ethnic concordance and blood pressure control: the role of trust and medication adherence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schoenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Montague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baier</forename><surname>Manwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Linzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.1080/13557858.2013.857764</idno>
		<idno type="PMID">24266617</idno>
		<ptr target="https://doi.org/10.1080/13557858.2013.857764" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_TUAAsUR">Ethn Health</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="565" to="578" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Schoenthaler A, Montague E, Baier Manwell L, Brown R, Schwartz MD, Linzer M. Patient-physician racial/ethnic concordance and blood pressure control: the role of trust and medication adherence. Ethn Health [Internet]. 2014 [cited 2021 May 10];19(5),:565-78. Available from: https://doi.org/10.1080/ 13557858.2013.857764 PMID: 24266617</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_Wt6XrXn">Patient trust in physicians and shared decision-making among African-Americans with diabetes</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gorawara-Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odoms-Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Chin</surname></persName>
		</author>
		<idno type="DOI">10.1080/10410236.2012.710873</idno>
		<idno type="PMID">23050731</idno>
		<ptr target="https://doi.org/10.1080/10410236.2012.710873" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_ub7QkyD">Health Commun</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="616" to="623" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Peek ME, Gorawara-Bhat R, Quinn MT, Odoms-Young A, Wilson SC, Chin MH. (2013). Patient trust in physicians and shared decision-making among African-Americans with diabetes. Health Commun [Internet]. 2013 [cited 2021 May 10]; 28(6):616-23. Available from: https://doi.org/10.1080/10410236. 2012.710873 PMID: 23050731</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_4vavUFb">Association between physicians&apos; interaction with pharmaceutical companies and their clinical practices: A systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Brax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fadlallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Al-Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Kahale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>El-Jardali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Akl</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0175493</idno>
		<idno type="PMID">28406971</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0175493" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_yvgRDJz">PloS One</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">175493</biblScope>
			<date type="published" when="2017-04-13">2017 Apr 13</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brax H, Fadlallah R, Al-Khaled L, Kahale LA, Nas H, El-Jardali F, Akl EA. Association between physi- cians&apos; interaction with pharmaceutical companies and their clinical practices: A systematic review and meta-analysis. PloS One. 2017 Apr 13; 12(4):e0175493. https://doi.org/10.1371/journal.pone.0175493 PMID: 28406971</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_YgYhhac">A systematic review of the impact of physician implicit racial bias on clinical decision making</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dehon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Faulconer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sterling</surname></persName>
		</author>
		<idno type="DOI">10.1111/acem.13214</idno>
		<ptr target="https://doi.org/10.1111/acem.13214PMID:28472533" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_f8unaUy">Acad Emerg Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="895" to="904" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dehon E, Weiss N, Jones J, Faulconer W, Hinton E, Sterling S. A systematic review of the impact of physician implicit racial bias on clinical decision making. Acad Emerg Med. 2017; 24(8):895-904. https://doi.org/10.1111/acem.13214 PMID: 28472533</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_2gSvwAu">Bias in data-driven artificial intelligence systems-an introductory survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ntoutsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fafalios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gadiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_a3GRCeh">Wiley Interdiscip Rev Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e1356</biblScope>
			<date type="published" when="2020-02-03">2020 Feb 3</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ntoutsi E, Fafalios P, Gadiraju U, Iosifidis V, Nejdl W, Vidal ME, et al. Bias in data-driven artificial intelli- gence systems-an introductory survey. Wiley Interdiscip Rev Data Min Knowl Discov. 2020 Feb 3; 10 (3):e1356.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_wmtK2GZ">An algorithmic approach to reducing unexplained pain disparities in underserved populations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-020-01192-7</idno>
		<ptr target="https://doi.org/10.1038/s41591-020-01192-7PMID:33442014" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_qgKYEH5">Nat Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="136" to="140" />
			<date type="published" when="2021-01">2021 Jan</date>
		</imprint>
	</monogr>
	<note>internet cited 2021 May 10</note>
	<note type="raw_reference">Pierson E, Cutler DM, Leskovec J. et al. An algorithmic approach to reducing unexplained pain dispari- ties in underserved populations. Nat Med [internet]. 2021 Jan [cited 2021 May 10]; 27(1):136-40. Avail- able from: https://doi.org/10.1038/s41591-020-01192-7 PMID: 33442014</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_MFmEyPR">Assessing and mitigating bias in medical artificial intelligence: the effects of race and ethnicity on a deep learning model for ECG analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">I</forename><surname>Attia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lopez-Jimenez</surname></persName>
		</author>
		<idno type="DOI">10.1161/circep.119.007988</idno>
		<idno type="PMID">32064914</idno>
		<ptr target="https://doi.org/10.1161/CIRCEP.119.007988" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_rNa2yAd">Circ Arrhythm Electrophysiol</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7988</biblScope>
			<date type="published" when="2020-03">2020 Mar</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Noseworthy PA, Attia ZI, Brewer LC, Hayes SN, Yao X, Kapa S, Friedman PA, Lopez-Jimenez F. Assessing and mitigating bias in medical artificial intelligence: the effects of race and ethnicity on a deep learning model for ECG analysis. Circ Arrhythm Electrophysiol [Internet]. 2020 Mar [cited 2021 May 10];13(3)e007988. Available from: https://doi.org/10.1161/CIRCEP.119.007988 PMID: 32064914</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_f7EVjv9">Physician perspectives on integration of artificial intelligence into diagnostic pathology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Faust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Richer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Ommeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diamandis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-019-0106-0</idno>
		<ptr target="https://doi.org/10.1038/s41746-019-0106-0PMID:31304375" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_GXHVJv9">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019-04-26">2019 Apr 26</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sarwar S, Dent A, Faust K, Richer M, Djuric U, Van Ommeren R, Diamandis P. Physician perspectives on integration of artificial intelligence into diagnostic pathology. NPJ Digit Med. 2019 Apr 26; 2(1):1-7. https://doi.org/10.1038/s41746-019-0106-0 PMID: 31304375</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_d6q9Zbd">Augmenting medical diagnosis decisions? An investigation into physicians&apos; decision making process with artificial intelligence</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jussupow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Spohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Heinzl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gawlitza</surname></persName>
		</author>
		<idno type="DOI">10.1287/isre.2020.0980</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gWUr7kZ">Inf Syst Res</title>
		<imprint>
			<biblScope unit="page">tba</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jussupow E, Spohrer K, Heinzl A, Gawlitza J. Augmenting medical diagnosis decisions? An investiga- tion into physicians&apos; decision making process with artificial intelligence. Inf Syst Res. 2020:tba.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_5Qwqvrj">Epistemic trust in science</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wilholt</surname></persName>
		</author>
		<idno type="DOI">10.1093/bjps/axs007</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_A7vfkeQ">British J Phil Science</title>
		<imprint>
			<date type="published" when="2020-12-23">2020 Dec 23</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wilholt T. Epistemic trust in science. British J Phil Science. 2020 Dec 23.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_4bEbNXh">Public epistemic trustworthiness and the integration of patients in psychiatric classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bueter</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11229-018-01913-z</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JSvycr4">Synthese</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="4711" to="4729" />
			<date type="published" when="2021-08">2021 Aug</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bueter A. Public epistemic trustworthiness and the integration of patients in psychiatric classification. Synthese. 2021 Aug; 198(19):4711-29.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_4a7u7rH">Validating vignette and conjoint survey experiments against real-world behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hainmueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hangartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamamoto</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1416587112</idno>
		<ptr target="https://doi.org/10.1073/pnas.1416587112PMID:25646415" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_BgXV7k7">Proc Natl Acad Sci U S A</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2395" to="2400" />
			<date type="published" when="2015-02-24">2015 Feb 24</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hainmueller J, Hangartner D, Yamamoto T. Validating vignette and conjoint survey experiments against real-world behavior. Proc Natl Acad Sci U S A. 2015 Feb 24; 112(8):2395-400. https://doi.org/10.1073/ pnas.1416587112 PMID: 25646415</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_EJrRfzG">Hypothetical vignettes in empirical bioethics research</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Ratcliffe</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1479-3709(07)11008-6</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2beFMXs">Empirical methods for bioethics: a primer</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Jacoby</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Siminoff</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ulrich CM, Ratcliffe SJ. Hypothetical vignettes in empirical bioethics research. In: Jacoby L, Siminoff LA, editors. Empirical methods for bioethics: a primer. Oxford, UK: Elsevier; 2008.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_jNwg6km">Leadership in times of crisis: a framework for assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Overdijk</surname></persName>
		</author>
		<ptr target="https://doi.org/10.108012294659.2013.1080521" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_5x5dYx2">International Rev Public Admin</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="91" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Boin A, Kuipers S, Overdijk W. Leadership in times of crisis: a framework for assessment. International Rev Public Admin [Internet]. 2013 [cited 2021 May 10]; 18(1):79-91. Available from: https://doi.org/10. 108012294659.2013.1080521</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_Yzq8mCy">Leadership competencies and the essential role of human resource development in times of crisis: a response to Covid-19 pandemic</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Dirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barhate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gunasekara</surname></persName>
		</author>
		<idno type="DOI">10.1080/13678868.2020.1780078</idno>
		<ptr target="https://doi.org/10.1080/13678868.2020.1780078" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_PCeCgSS">Human Resource Devel International</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020-05-28">2020 May 28</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Dirani KM, Abadi M, Alizadeh A, Barhate B, Garza RC, Gunasekara N, et al. Leadership competencies and the essential role of human resource development in times of crisis: a response to Covid-19 pan- demic. Human Resource Devel International [Internet]. 2020 May 28 [cited 2021 May 10]; 23(4):1-15. Available from: https://doi.org/10.1080/13678868.2020.1780078</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_6GsvQ27">Human factors aspects of ICT for crisis management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Schraagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Ven</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10111-011-0175-6</idno>
		<ptr target="https://doi.org/10.1007/s10111-011-0175-6" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_vg2MK5m">Cogn Technol Work</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="187" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Internet cited 2021 May 10</note>
	<note type="raw_reference">Schraagen JM, van de Ven J. Human factors aspects of ICT for crisis management. Cogn Technol Work [Internet]. 2011 [cited 2021 May 10]; 13(3):175-87. Available at: https://doi.org/10.1007/s10111- 011-0175-6</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_7sZE2Hn">Keeping the patient at the center of machine learning in healthcare</title>
		<author>
			<persName><forename type="first">J</forename><surname>Findley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Slepian</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2020.1820100</idno>
		<idno type="PMID">33103979</idno>
		<ptr target="https://doi.org/10.1080/15265161.2020.1820100" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_AWA72Av">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="54" to="56" />
			<date type="published" when="2020-11-01">2020 Nov 1</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Findley J, Woods AK, Robertson C, Slepian M. Keeping the patient at the center of machine learning in healthcare. Am J Bioeth. 2020 Nov 1; 20(11):54-6. https://doi.org/10.1080/15265161.2020.1820100 PMID: 33103979</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_DkMjzra">Can rationing through inconvenience be ethical?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Eyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1002/hast.806</idno>
		<ptr target="https://doi.org/10.1002/hast.806PMID:29457241" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_cQV22x4">Hastings Cent Rep</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="22" />
			<date type="published" when="2018-01">2018 Jan</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eyal N, Romain PL, Robertson C. Can rationing through inconvenience be ethical?. Hastings Cent Rep. 2018 Jan; 48(1):10-22. https://doi.org/10.1002/hast.806 PMID: 29457241</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
