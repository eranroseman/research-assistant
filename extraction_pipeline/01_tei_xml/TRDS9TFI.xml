<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_qGPq8qH">Off-Policy Estimation of Long-Term Average Outcomes with Applications to Mobile Health</title>
				<funder>
					<orgName type="full">National Heart, Lung, and Blood Institute</orgName>
					<orgName type="abbreviated">NHLBI</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000050</idno>
				</funder>
				<funder>
					<orgName type="full">National Institute on Alcohol Abuse and Alcoholism</orgName>
					<orgName type="abbreviated">NIAAA</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000027</idno>
				</funder>
				<funder>
					<orgName type="full">National Institute on Drug Abuse</orgName>
					<orgName type="abbreviated">NIDA</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000026</idno>
				</funder>
				<funder>
					<orgName type="full">National Cancer Institute</orgName>
					<orgName type="abbreviated">NCI</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000054</idno>
				</funder>
				<funder ref="#_cFbNp9b #_tqpKbWJ #_PEHG94S #_jCfgZGJ #_u95xmgt #_m2323A7">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000002</idno>
				</funder>
				<funder>
					<orgName type="full">National Institute of Biomedical Imaging and Bioengineering</orgName>
					<orgName type="abbreviated">NIBIB</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000070</idno>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-07-22">22 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peng</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">Department of Statistics , University of Michigan Predrag Klasnja School of Information , University of Michigan Department of Statistics , Harvard University</note>
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">Klasnja School of Information</orgName>
								<orgName type="department" key="dep3">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">University of Michigan Predrag</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><surname>Murphy</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">Department of Statistics , University of Michigan Predrag Klasnja School of Information , University of Michigan Department of Statistics , Harvard University</note>
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">Klasnja School of Information</orgName>
								<orgName type="department" key="dep3">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">University of Michigan Predrag</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_P6N6Xnd">Off-Policy Estimation of Long-Term Average Outcomes with Applications to Mobile Health</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-22">22 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">CF6F340D2BE21F7AA4D53AC169D70880</idno>
					<idno type="arXiv">arXiv:1912.13088v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T12:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_JPjvcaA">sequential decision making</term>
					<term xml:id="_cnRcYGW">policy evaluation</term>
					<term xml:id="_mH9kTMk">markov decision process</term>
					<term xml:id="_rSHBj99">reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_mPJwB9N"><p xml:id="_FM66Vzz"><s xml:id="_2FbnmZG">Due to the recent advancements in wearables and sensing technology, health scientists are increasingly developing mobile health (mHealth) interventions.</s><s xml:id="_mxQPSZ6">In mHealth interventions, mobile devices are used to deliver treatment to individuals as they go about their daily lives.</s><s xml:id="_fhfFH4x">These treatments are generally designed to impact a near time, proximal outcome such as stress or physical activity.</s><s xml:id="_dGcRdFR">The mHealth intervention policies, often called just-in-time adaptive interventions, are decision rules that map a individual's current state (e.g., individual's past behaviors as well as current observations of time, location, social activity, stress and urges to smoke) to a particular treatment at each of many time points.</s><s xml:id="_JBzUY36">The vast majority of current mHealth interventions deploy expert-derived policies.</s><s xml:id="_7xV8rXK">In this paper, we provide an approach for conducting inference about the performance of one or more such policies using historical data collected under a possibly different policy.</s><s xml:id="_acEngVK">Our measure of performance is the average of proximal outcomes over a long time period should the particular mHealth policy be followed.</s><s xml:id="_KDaG52X">We provide an estimator as well as confidence intervals.</s><s xml:id="_TcsRE4T">This work is motivated by HeartSteps, an mHealth physical activity intervention.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_sVxVV4v">Introduction</head><p xml:id="_4AFXhQh"><s xml:id="_DTQWeVc">Due to the recent advancement in mobile device and sensing technology, health scientists are more and more interested in developing mobile health (mHealth) interventions.</s><s xml:id="_8aGxkVH">In mHealth, mobile devices (e.g., wearables and smartphones) are used to deliver interventions to individuals as they go about their daily lives.</s><s xml:id="_79gyQeh">In general, there are two types of mHealth treatments.</s></p><p xml:id="_ehkuW2q"><s xml:id="_vyRv4s9">Most are pull treatments that reside on the individual's mobile device and allow the individual to access treatment content as needed.</s><s xml:id="_7w5uYRY">This work focuses on the second type, the "push" treatment, typically in the form of a notification or a text message that appears on a mobile device.</s><s xml:id="_R3STsgP">There is a wide variety of possible treatment messages (e.g., behavioral, cognitive, and motivational message and reminders).</s><s xml:id="_Hdc53yh">These treatments are generally intended to impact a near time, proximal outcome, such as stress or behaviors such as physical activity over some subsequent minutes/hours.</s><s xml:id="_5p3JsaZ">The mHealth intervention policies, often called just-in-time adaptive interventions in the mHealth literature <ref type="bibr" target="#b24">(Nahum-Shani et al. 2018)</ref>, are decision rules that map the individuals current state (e.g., past behaviors as well as current observations of location, social activity, stress and urges to smoke) to a particular treatment at each of many time points.</s><s xml:id="_nT5vcbh">Many mHealth interventions are designed for long-term use in chronic disease management <ref type="bibr" target="#b16">(Lee et al. 2018)</ref>.</s><s xml:id="_KzQBJet">The vast majority of current mHealth interventions deploy expert-derived policies with limited use of data evidence (for an example see <ref type="bibr" target="#b13">Kizakevich et al. (2014)</ref>), however the long-term efficacy of these policies on the health behavior is not well understood.</s><s xml:id="_t7WCv4T">An important first step toward developing data-based, effective mHealth interventions is to properly measure the long-term performance of these policies.</s><s xml:id="_R8H8ZXe">In this work, we provide an approach for conducting inference about the optimality of one or more mHealth policies of interest.</s><s xml:id="_pkKBeaE">Our optimality criterion is the long-term average of the proximal outcomes should a particular mHealth policy be followed.</s><s xml:id="_XtrqBYX">We develop a flexible method to estimate the performance of an mHealth policy using a historical dataset in which the treatments are decided by a possibly different policy.</s><s xml:id="_s9rej4A">This work is motivated by HeartSteps <ref type="bibr" target="#b14">(Klasnja et al. 2015)</ref>, an mHealth physical activity intervention.</s><s xml:id="_g4mZy5e">To design this intervention, we are conducting a series of studies.</s><s xml:id="_pzCnZen">The first, already completed, study was for 42 days.</s><s xml:id="_bnsuPPR">The last study will be for one year.</s><s xml:id="_eVuzguk">Here we focus on the intervention component involving activity suggestions.</s><s xml:id="_D5bDryk">These suggestions may be delivered at each of the five individual-specified times per day.</s><s xml:id="_Qt4Zu96">While in the first study there were 42 ×5 = 210 time points per individual, in the year-long study there will be about 2,000 time points per individual.</s><s xml:id="_2x2eHWF">The proximal outcome is the step count in the 30 minutes following each of the five times per day.</s><s xml:id="_J4jceKa">Our goal is to use the data collected from the first 42day study to predict and estimate the long-term average of proximal outcomes for a variety of policies that could be used to decide whether or not to send the activity suggestion at each time point in the year-long study.</s><s xml:id="_eds8Atb">The 42-day study was a Micro-Randomized Trial (MRT) <ref type="bibr" target="#b14">(Klasnja et al. 2015</ref><ref type="bibr" target="#b18">, Liao et al. 2016)</ref>.</s><s xml:id="_sNK9ewN">In an MRT, a known stochastic policy, also called a behavior policy, is used to decide when and which type of treatment to provide at each time point.</s><s xml:id="_MvHPfYh">A partial list of MRTs in the field or completed can be found at the website<ref type="foot" target="#foot_0">foot_0</ref> .</s><s xml:id="_rGPPvKm">From an experimental point of view, the stochastic behavior policy is used to conduct sequential randomizations within each individual.</s><s xml:id="_jTHgQkk">Here the adjective, "stochastic", means that at each time point each individual is randomized between the possible treatments.</s><s xml:id="_KZgwuC3">In this work we focus on settings in which the randomization probabilities are known functions of the individual's past data; this is the case with MRTs by design.</s></p><p xml:id="_ugB4yFS"><s xml:id="_9nQzVkW">The rest of the article is organized as follows.</s><s xml:id="_Nm5XWEJ">Section 2 provides a review of Markov Decision Processes.</s><s xml:id="_gB9tCaB">In Section 3, we review related work.</s><s xml:id="_753T86Y">Section 4 develops an estimator for the long-term average proximal outcome; then Section 5 provides the asymptotic distribution of this estimator.</s><s xml:id="_MSdJrXN">As the estimation requires tuning parameters, in Section 6 we provide a procedure to select the tuning parameters.</s><s xml:id="_ehcz9uF">Simulations are used to assess the coverage probability of the proposed confidence intervals in various settings.</s><s xml:id="_R6Edzbv">A case study using data from the 42-day MRT of HeartSteps is presented in Section 7. We end with a discussion of future work in Section 8.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_QPxgK2N">Distributional Assumptions and Goal</head><p xml:id="_7ywFjqp"><s xml:id="_gvVhaVr">The data for each individual is of the form</s></p><formula xml:id="formula_0">D = {S 1 , A 1 , S 2 , A 2 , S 3 , . . . , S t , A t , S t+1 , . . . , S T +1 },</formula><p xml:id="_fx3s6gs"><s xml:id="_K27PGwm">where t indexes time points, S t ∈ S is the individual's state and A t ∈ A is the treatment (usually called the action) assigned at time, t.</s><s xml:id="_fXVNQFb">The action space, A, is discrete and finite.</s></p><p xml:id="_Q4RDGfc"><s xml:id="_JHrQa9h">In mHealth, the state, S t , contains the time-varying information (e.g., current location) as well as summaries of historical data up to and including time, t (e.g., summaries of previous physical activity).</s><s xml:id="_BPuuaTS">The actions are different types of treatments that are delivered to the individual via a smartdevice; these treatments can be reminders, motivational messages, messages prompting self-reflection and so on.</s><s xml:id="_XSrPXJ7">For simplicity, we assume that the duration over which data is collected, T , is non-random and same for all individuals.</s><s xml:id="_yqtRuJr">The proximal outcome (also called the reward), denoted by R t+1 ∈ R, is assumed to be a known function of (S t , A t , S t+1 ).</s><s xml:id="_A8tPJQp">In mHealth, the reward is often chosen to measure the near-term impact of the current action (e.g., the number of steps in a pre-specified time window after each time point).</s><s xml:id="_AxPpgWx">In this work, we focus on the case of continuous rewards (see Section 8 for a discussion about other types of rewards).</s><s xml:id="_7s9qQrq">In HeartSteps, the binary action is whether an activity suggestion is delivered and the reward is the 30-min step count following each decision time.</s></p><p xml:id="_zRcBX3b"><s xml:id="_un6UcJk">We assume that the distribution of the states satisfies the Markovian property, that is,</s></p><formula xml:id="formula_1">for t ≥ 1, S t+1 ⊥ {S 1 , A 1 , . . . , S t-1 , A t-1 } | {S t , A t }. Furthermore, we assume that the condi- tional distribution (also called the transition kernel) of S t+1 | {S t , A t } is time-homogeneous.</formula><p xml:id="_hBQ9fCw"><s xml:id="_sXdHzGk">Denote the transition kernel by P ; thus given a measurable set, B, in the state space,</s></p><formula xml:id="formula_2">S, P (B | s, a) = Pr(S t+1 ∈ B | S t = s, A t = a)</formula><p xml:id="_46JV5mg"><s xml:id="_gVMPYZS">. Note that P does not depend on t due to the above time-homogeneity assumption.</s><s xml:id="_3YkSWEB">Denote by p(s ′ | s, a) the transition density with respect to some reference measure on S (e.g., counting measure when S is discrete).</s><s xml:id="_q7mtNau">Let r(s, a) denote the conditional expectation of the reward given state and action, i.e., r(s, a) = E(R t+1 | S t = s, A t = a).</s><s xml:id="_HNrEgXb">The tuple, (S, A, P ), is called a Markov Decision Process (MDP) <ref type="bibr" target="#b10">(Howard 1960</ref><ref type="bibr" target="#b26">, Puterman 1994</ref><ref type="bibr" target="#b29">, Sutton &amp; Barto 2018)</ref>.</s><s xml:id="_59tywvs">In mHealth, non-stationarity in P is likely to occur if there are unobserved aspects of the current state (e.g., individual's engagement and/or burden).</s><s xml:id="_yKsndUp">Therefore, practically, it is critical to strive to collect sufficient information (via self-report or wearable sensors) to represent individual's state.</s></p><p xml:id="_AXwWBaA"><s xml:id="_uHZ9UQv">Note that the MDP does not specify the distribution of the actions.</s><s xml:id="_Y4MRudN">And indeed the distribution of the actions may not satisfy the Markovian property.</s><s xml:id="_bwMnDSG">In an MRT, the actions, {A t } T t=1 , are randomized with probabilities that can depend on the entire history prior to time point t,</s></p><formula xml:id="formula_3">H t = {S 1 , A 1 , . . . , S t }. Denote the distribution of A t | H t by π b t (• | H t ). We call π b = {π b 1 , . . . , π b</formula><p xml:id="_FwKZaSU"><s xml:id="_q4jFyQh">T }, a stochastic behavior policy.</s><s xml:id="_DXPSCdz">Throughout we assume that π b is known (as is the case in an MRT) and that the probabilities are strictly positive, i.e., π b t (a | H t ) ≥ p min &gt; 0 for all a ∈ A, H t and t ≤ T .</s></p><p xml:id="_wHeEedE"><s xml:id="_hrpnmfP">Suppose that a pre-specified time-invariant, Markovian policy, π, is being considered for use in future.</s><s xml:id="_jqPrasU">Our goal is to conduct inference for the resulting average of the rewards over a large number of time points.</s><s xml:id="_6gX4f9R">In mHealth, the policy might be an expert-constructed policy.</s><s xml:id="_a8FcvX6">Considering a long time period makes most sense for individuals who are struggling with chronic problems or disorders for which, at this time, there is no general cure.</s><s xml:id="_cAsKDGt">Many health-behavior problems fall into this area including obesity, hypertension, adherence to medications for AIDs, mental illness and addictions.</s><s xml:id="_tHPyTjc">Let π(a | s) be the probability of choosing the action, a, at the state, s.</s><s xml:id="_5VSFZvP">Given a dataset that consists of n independent, identically distributed (i.i.d.) observations of D, we aim to estimate the average reward of the policy, defined as</s></p><formula xml:id="formula_4">η π (s) = limsup t * →∞ E π 1 t * t * t=1 R t+1 S 1 = s ,<label>(1)</label></formula><p xml:id="_fAKVmaF"><s xml:id="_6yCVYAN">where the expectation, E π , is taken over the trajectory {S 1 , A 1 , S 2 , . . .</s><s xml:id="_TuKjRUC">, S t * , A t * , S t * +1 } in which the actions are selected according to the policy, π, that is, the likelihood in the expectation is given by</s></p><formula xml:id="formula_5">1 {S 1 =s} t * t=1 π(A t | S t )p(S t+1 | S t , A t ).</formula><p xml:id="_qShVB5n"><s xml:id="_CVxAanE">The policy, π, induces a Markov chain on the state with the transition kernel,</s></p><formula xml:id="formula_6">P π (• | s) = a∈A π(a | s)P (• | s, a).</formula><p xml:id="_wDUFZDG"><s xml:id="_xMhvNxg">Suppose for now the state space, S, is finite.</s><s xml:id="_pAmjSCw">It is known that the limit in (1) exists, i.e., <ref type="bibr" target="#b26">Puterman (1994)</ref>).</s></p><formula xml:id="formula_7">η π (s) = lim t * →∞ E π ( 1 t * t * t=1 R t+1 | S 1 = s) (Theorem A.6 on p. 595 in</formula><p xml:id="_PUgJQDn"><s xml:id="_kNDeJHG">Furthermore, when the induced Markov chain, P π , is irreducible, the average reward is independent of initial state and is given by</s></p><formula xml:id="formula_8">η π (s) = η π = s,a π(a | s)d π (s)r(s, a),<label>(2)</label></formula><p xml:id="_56RU6ma"><s xml:id="_TKzEArY">where d π (•) is the stationary distribution.</s><s xml:id="_6pGU2Xr">The existence of stationary distribution is guaranteed by the irreducibility assumption on P π (Puterman (1994), p. 592).</s><s xml:id="_JrxTxKD">The above results can be extended to general state spaces (e.g., S ⊂ R d ) under more involved conditions on the transition kernel, P π , analogous to the finite state case (see, for example, Hernández-Lerma &amp; Lasserre (1999), chap.</s><s xml:id="_vmTA98a">7).</s><s xml:id="_tAszdKm">Practically the irreducibility assumption implies that time-invariant information cannot be included in the state.</s><s xml:id="_dByGUPt">Motivated by mHealth applications, in Supplement A we present a generalization that allows the average reward to depend on time-invariant variables.</s><s xml:id="_cEzcUBg">In the case of mHealth, time-invariant variables might be gender, baseline severity, genetics and so on.</s></p><p xml:id="_5MjsJ3w"><s xml:id="_PW8HSUP">We propose to conduct inference about the long-term performance of each policy, π, via its average reward, η π .</s><s xml:id="_BKQ7erU">This is because the average reward, η π , is an asymptotic surrogate of the average of finite rewards over a long period of time.</s><s xml:id="_CUxHpd9">In fact, it can be shown that</s></p><formula xml:id="formula_9">sup s∈S E π 1 t * t * t=1 R t+1 S 1 = s -η π = O(1/t * ),</formula><p xml:id="_yHxx57Q"><s xml:id="_TEFrcKT">where the leading constant depends on the mixing time of P π (see Theorem 7.5.10 in <ref type="bibr" target="#b9">Hernández-Lerma &amp; Lasserre (1999)</ref>).</s><s xml:id="_KKBHdug">In the case of HeartSteps, the goal is to use the data from the 42-day MRT study to estimate the average reward, η π , for a variety of policies π.</s></p><p xml:id="_TQDsrBE"><s xml:id="_hcufkRK">The average reward, η π , provides a proxy for the average of the 30-min step counts when the policy, π, is used to determine whether to send the activity suggestions over a long time period (e.g., a year: 5 × 365 time points).</s></p><p xml:id="_UN9rZQG"><s xml:id="_b8CQ2wF">Note that the data, D, on each individual includes observations over T time points and the actions are selected according to a behavior policy.</s><s xml:id="_EYj56se">However, as will be seen, the above assumptions including the Markovian and irreducibility assumptions will allow us to estimate the average reward over a long time period and under a different policy.</s></p><p xml:id="_2fdraw6"><s xml:id="_C3gkXmr">Lastly as mentioned above the focus here is to conduct inference for the long run average of equally weighted rewards under the target policy using data collected under a possibly different policy.</s><s xml:id="_UujPuzq">An alternate, and more common, inference target is based on an expected discounted sum of rewards, E π ( ∞ t=1 γ t-1 R t+1 | S 1 = s), with the discount rate, γ ∈ [0, 1).</s><s xml:id="_czpcMRH">When the discount rate, γ, is small (e.g., γ = 0.5), the discounted sum of rewards focuses only on finitely many near-term rewards.</s><s xml:id="_k2y92vv">Note that even with a large discount rate of γ = 0.99, the reward at t = 100 has a weight of 0.37 and the reward t = 200 has a weight of 0.13.</s></p><p xml:id="_srY8rg8"><s xml:id="_jGRW94w">Recall our motivating mHealth intervention is being designed to optimize the overall physical activity over one year.</s><s xml:id="_vDcHWff">From a scientific point of view, the rewards in the distant future are as important as the near-term ones, especially when considering the effect of habituation and burden.</s><s xml:id="_HQxYpBs">With this in mind, we opt for the long-term average reward, which can be viewed as a proxy for the (undiscounted) average of rewards over a long period of time.</s><s xml:id="_y6aumTq">In fact, the conditional expectation of the sum of discounted rewards is related to the average reward; as γ → 1, the above conditional expectation of the sum of the discounted rewards normalized by the constant, 1/(1γ), converges to the average reward, η π <ref type="bibr" target="#b21">(Mahadevan 1996)</ref>.</s><s xml:id="_ag6Ewjr">In the online setting many researchers focus on a discounted sum of rewards.</s><s xml:id="_zDCGcDA">This is because the Bellman operator, for the expected discounted sum of rewards, is a contraction <ref type="bibr" target="#b29">(Sutton &amp; Barto 2018)</ref>; the contraction provides greater computational stability and simpler convergence arguments.</s><s xml:id="_gsSWq32">However, as we shall see below, consideration of the average reward is not problematic in the batch (i.e., off-line) setting.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_VqdW9X7">Related Work</head><p xml:id="_QR8e9Ys"><s xml:id="_pHZ5NdN">The evaluation of a given target policy using data collected from a different policy (i.e., the behavior policy) is called off-policy evaluation.</s><s xml:id="_NpArKRp">This has been widely studied in both the statistical and reinforcement learning (RL) literature.</s><s xml:id="_QfgnCxF">Many authors have evaluated and contrasted policies in terms of the expected sum of rewards over a finite number of time points <ref type="bibr" target="#b23">(Murphy et al. 2001</ref><ref type="bibr" target="#b2">, Chakraborty &amp; Moodie 2013</ref><ref type="bibr" target="#b11">, Jiang &amp; Li 2015)</ref>.</s><s xml:id="_TSAspaH">However, because these methods often use products of weights with probabilities from the behavior policy in the denominator, the extension to problems with a large number of time points often suffers from a large variance <ref type="bibr" target="#b30">(Thomas &amp; Brunskill 2016</ref><ref type="bibr" target="#b11">, Jiang &amp; Li 2015)</ref>.</s></p><p xml:id="_WQnAkk9"><s xml:id="_PusGZmt">The most common off-policy evaluation methods for infinite-horizon problems (i.e., a large number of time points) focus on a discounted sum of rewards and are thus based in some way on the value function (in the discounted reward setting E π ( ∞ t=1 γ t-1 R t+1 | S 1 = s) considered as a function of s is the value function).</s><s xml:id="_5v8yncz"><ref type="bibr" target="#b5">Farahmand et al. (2016)</ref> proposed a regularized version of Least Square Temporal Difference <ref type="bibr" target="#b1">(Bradtke &amp; Barto 1996)</ref> and statistical properties were studied.</s><s xml:id="_npjBjsZ">They used a non-parametric model to estimate the value function and derived the convergence rate when training data consists of i.i.d.</s><s xml:id="_SYdsNDJ">transition samples in the form of state, action, reward and next state.</s><s xml:id="_RRFWE6B">From a technical point of view, our estimation method is similar to <ref type="bibr" target="#b5">Farahmand et al. (2016)</ref>, albeit focused on the average reward; most importantly our method relaxes the assumption that Bellman operator can be modeled correctly for each candidate relative value function and only assumes the data consists of i.i.d.</s><s xml:id="_kVC8zXJ">samples of trajectories.</s><s xml:id="_df422ua"><ref type="bibr" target="#b20">Luckett et al. (2020)</ref> also focused on the discounted reward setting.</s><s xml:id="_d2M3mbv">They evaluated policy, π, based on an average of E π ( ∞ t=1 γ t-1 R t+1 | S 1 = s) with respect to a pre-selected reference distribution of the state.</s><s xml:id="_D57xDpv">While the reference distribution can be naturally chosen as the distribution of the initial state <ref type="bibr" target="#b7">(Farajtabar et al. 2018</ref><ref type="bibr" target="#b19">, Liu et al. 2018</ref><ref type="bibr" target="#b20">, Luckett et al. 2020</ref><ref type="bibr" target="#b30">, Thomas &amp; Brunskill 2016)</ref>, choosing a "right" discount rate, γ, can be non-trivial, at least in mHealth.</s><s xml:id="_XVePGFA">They assumed a parametric model for the value function and developed a regularized estimating equation.</s><s xml:id="_HCBxSNc">In computer science literature, there also exists many off-policy evaluation methods for the discounted reward setting.</s><s xml:id="_HvK5qGh">We refer the interested reader to the recent works by <ref type="bibr" target="#b7">Farajtabar et al. (2018)</ref> and <ref type="bibr" target="#b12">Kallus &amp; Uehara (2019)</ref> and references therein.</s></p><p xml:id="_qVbj8gY"><s xml:id="_TH3UyFz">Closest to the setting of this work is the recent work by <ref type="bibr" target="#b22">Murphy et al. (2016)</ref> and <ref type="bibr" target="#b19">Liu et al. (2018)</ref>.</s><s xml:id="_G5w25KP"><ref type="bibr" target="#b22">Murphy et al. (2016)</ref> considered the average reward setting.</s><s xml:id="_uu3gUZZ">They assumed a linear model for the value function and constructed the estimating equations to estimate the average reward.</s><s xml:id="_hBPCGKz">However the linearity assumption of the value function is unlikely to hold in practice and difficult to validate (e.g., the value function involves the infinite sum of the rewards).</s><s xml:id="_UfYGRdR">Our method allows the use of a non-parametric model for the value function to increase robustness.</s><s xml:id="_PhCSPtJ"><ref type="bibr" target="#b19">Liu et al. (2018)</ref> also considered the average reward and proposed an estimator for the average reward based on estimating the ratio of the stationary distribution under the target policy divided by the stationary distribution under the behavior policy.</s><s xml:id="_8QHwXUp">However they did not provide confidence intervals or other inferential methods besides an estimator for the average reward.</s><s xml:id="_ZCN2m8Y">In addition, they restricted the behavior policy to be Markovian and time-stationary.</s><s xml:id="_W7xr5uf">In mHealth, the behavior policy can be determined by an algorithm based on the accruing data and thus violates this assumption <ref type="bibr" target="#b17">(Liao et al. 2018</ref><ref type="bibr" target="#b3">, Dempsey et al. 2020)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_PNdKDF4">Estimator for Off-Policy Evaluation</head><p xml:id="_gCPvBCj"><s xml:id="_uQy48hu">We assume that the dataset, D n , consists of n trajectories:</s></p><formula xml:id="formula_10">D n = D i n i=1 = S i 1 , A i 1 , S i 2 , . . . , S i T , A i T , S i T +1 n i=1 .</formula><p xml:id="_eWJ36w6"><s xml:id="_QSBkJSQ">Each trajectory, D i , is an i.i.d.</s><s xml:id="_DNTKXWK">copy of D described in Section 2. Recall that {A t } T t=1 , the actions in D, are selected by the behavior policy, π b .</s><s xml:id="_jJQCEKZ">In the following, the expectation, E, without the subscript is with respect to the distribution of the trajectory, D, under the behavior policy.</s></p><p xml:id="_F9WUsAn"><s xml:id="_RbwMCsX">Below we introduce the estimator for η π .</s><s xml:id="_bBBVZv4">We follow the so-called "model-free" approach (i.e., does not require modeling the transition kernel, P ) to estimate the average reward.</s><s xml:id="_3qUHQWq">Our estimator is based on the Bellman equation, also known as the Poisson equation <ref type="bibr" target="#b26">(Puterman 1994</ref>); as will be discussed below this equation characterizes the average reward.</s></p><p xml:id="_eK52S4x"><s xml:id="_kpGrKRy">First consider the setting where the state space, S, is finite and the induced Markov chain, P π , is irreducible.</s><s xml:id="_j8WpDBm">Recall that in this setting the average reward, η π , is a constant given in (2).</s><s xml:id="_624bfSy">Define the relative value function by</s></p><formula xml:id="formula_11">Q π (s, a) = lim t * →∞ 1 t * t * t=1 E π t k=1 {R k+1 -η π } S 1 = s, A 1 = a ;</formula><p xml:id="_JCpB8fQ"><s xml:id="_2zU46Dy">(3) this limit is well-defined <ref type="bibr" target="#b26">(Puterman (1994)</ref>, p. 338).</s><s xml:id="_sDHnHV6">If the induced Markov chain is aperiodic, then the relative value function (3) can be expressed as <ref type="bibr">a)</ref>, measures the difference between the expected cumulative rewards under policy π and the average reward when the initial state is s and the first action is a.</s><s xml:id="_5exnudF">It is easy to verify from the definition that (η π , Q π ) is a solution of the Bellman equation:</s></p><formula xml:id="formula_12">Q π (s, a) = E π { ∞ t=1 (R t+1 -η π ) | S 1 = s, A 1 = a}. The relative value function, Q π (s,</formula><formula xml:id="formula_13">E π {R t+1 + Q(S t+1 , A t+1 ) | S t = s, A t = a} = η + Q(s, a), ∀(s, a) ∈ S × A.</formula><p xml:id="_V9ZxkUK"><s xml:id="_94m3FYb">(4)</s></p><p xml:id="_pcYrEw5"><s xml:id="_uD5wNWu">Furthermore, when the induced Markov chain is irreducible, the Bellman equation ( <ref type="formula">4</ref>) uniquely identifies the average reward, η π , and identifies the relative value function, Q π , up to a constant (see <ref type="bibr" target="#b26">Puterman (1994)</ref>, p. 343 for details).</s><s xml:id="_PnNgFRy">That is, the set of the solutions of the Bellman equation ( <ref type="formula">4</ref>) is given by</s></p><formula xml:id="formula_14">{(η π , Q) : Q = Q π + c1, c ∈ R, 1(s, a) = 1}. These re-</formula><p xml:id="_9Pkbg2n"><s xml:id="_4btReeE">sults can be generalized to general state spaces (see chap. 7 in Hernández-Lerma &amp; Lasserre (1999)).</s><s xml:id="_RK2mdde">The key assumption for the method proposed here is as follows.</s></p><p xml:id="_3z6tvjV"><s xml:id="_vZ9H5S5">Assumption 1.</s><s xml:id="_rmeywQu">The average reward of the target policy, π, is independent of state and satisfies (2).</s><s xml:id="_FNVYvHT">(η π , Q π ) is the unique solution of the Bellman equation ( <ref type="formula">4</ref>) up to a constant for Q π .</s><s xml:id="_55tEyNT">The stationary distribution of the induced transition kernel, P π , exists.</s></p><p xml:id="_66H33Cu"><s xml:id="_ygy28xZ">As the focus of this work is to estimate the average reward, it will be sufficient to estimate a specific version of Q π .</s><s xml:id="_Kfr2pjd">Define the shifted relative value function by Qπ (s, a) = Q π (s, a) -Q π (s * , a * ) for a specific state-action pair, (s * , a * ).</s><s xml:id="_VTuJpck">Obviously Qπ (s * , a * ) = 0 and Qπ (s 1 , a 1 ) -</s></p><formula xml:id="formula_15">Qπ (s 2 , a 2 ) = Q π (s 1 , a 1 ) -Q π (s 2 , a 2 )</formula><p xml:id="_zhNQ7uV"><s xml:id="_eWnXHHN">, that is, the difference in the relative value remains the same.</s><s xml:id="_qA6VM6F">By restricting the relative value function to satisfy Q(s * , a * ) = 0, the solution of Bellman equation ( <ref type="formula">4</ref>) is unique and given by (η π , Qπ ).</s></p><p xml:id="_WyKmJU2"><s xml:id="_TBMjfZP">In the following, we assume that Qπ ∈ Q, where Q denotes a vector space of functions on the state-action space S × A such that Q(s * , a * ) = 0 for all Q ∈ Q.</s><s xml:id="_UqVJEfZ">The Bellman operator, T π , with respect to the target policy π is given by</s></p><formula xml:id="formula_16">T π (s, a; Q) = E R t+1 + a ′ π(a ′ | S t+1 )Q(S t+1 , a ′ ) S t = s, A t = a .</formula><p xml:id="_7unYepH"><s xml:id="_2S5PUDE">(5) Note that the above conditional expectation does not depend on the behavior policy due to the conditioning on current state and action.</s><s xml:id="_mHRs2p7">The Bellman error at (s, a) with respect to (η, Q) and π is defined as T π (s, a; Q)η -Q(s, a).</s><s xml:id="_ZEGmbRk">From the Bellman equation, this error is zero for all (s, a) when η = η π and Q = Qπ .</s></p><p xml:id="_HxBqsmH"><s xml:id="_tHRAzHV">Note that the Bellman operator (5) involves the (unknown) transition kernel, P .</s><s xml:id="_ZXCfSQe">Suppose for now that P is known and thus the Bellman operator is known.</s><s xml:id="_YAe2ZmS">Since the Bellman error is zero at η = η π and Q = Qπ , a natural way to estimate (η π , Qπ ) is to minimize the empirical squared Bellman error, i.e., min</s></p><formula xml:id="formula_17">(η,Q)∈R×Q P n 1 T T t=1 {T π (S t , A t ; Q) -η -Q(S t , A t )} 2 , (<label>6</label></formula><formula xml:id="formula_18">)</formula><p xml:id="_NapZ6HR"><s xml:id="_yJd8MfK">where</s></p><formula xml:id="formula_19">P n f (D) = (1/n) n i=1 f (D i</formula><p xml:id="_DWqh6vt"><s xml:id="_Nkcjxum">) is the empirical mean over the training data, D n , for a function of the trajectory, f (D).</s><s xml:id="_5zQBXpG">Obviously, this is not a feasible estimator as we don't know the transition kernel and thus T π (S t , A t ; Q) is unknown.</s><s xml:id="_DRDK6Jx">A natural idea is to replace the Bellman operator by its sample counterpart, i.e., replace</s></p><formula xml:id="formula_20">T π (S t , A t ; Q) by R t+1 + a ′ π(a ′ | S t+1 )Q(S t+1 , a ′ )</formula><p xml:id="_eHtakka"><s xml:id="_CjysQ85">in the objective function of (6).</s><s xml:id="_Au4T3Uf">Unlike the regression problem in which the dependent variable is fully observed, the dependent variable here is R t+1 + a ′ π(a ′ | S t+1 )Q(S t+1 , a ′ ), which involves the unknown relative value function, Q.</s><s xml:id="_4tVvXxE">As a result, this natural plug-in estimator is biased (see <ref type="bibr" target="#b0">Antos et al. (2008)</ref> for a similar discussion in the discounted reward setting).</s></p><p xml:id="_dfh6T7H"><s xml:id="_Jh7r3U6">The above argument motivates a coupled estimator in which we use the estimated Bellman error to form an objective function.</s><s xml:id="_3dwuhQn">In particular, for each (η, Q), we replace the Bellman error, T π (S t , A t ; Q)η -Q(S t , A t ), in ( <ref type="formula" target="#formula_17">6</ref>) by an estimate of the "projection" of the Bellman error into a second function class, G:</s></p><formula xml:id="formula_21">g * π (•, •; η, Q) = argmin g∈G E 1 T T t=1 {T π (S t , A t ; Q) -η -Q(S t , A t ) -g(S t , A t )} 2 . (<label>7</label></formula><formula xml:id="formula_22">)</formula><p xml:id="_zbgmwwr"><s xml:id="_csumhXt">Throughout we assume the solution of the above optimization exists and is in G and we call g * π (•, •; η, Q) a projection for simplicity.</s><s xml:id="_eDDxNgU">Recall that members of Q satisfy Q(s * , a * ) = 0.</s><s xml:id="_XFnYczJ">A similar constraint needs not be placed on the members of G.</s><s xml:id="_qmn8bZt">It is worth noting that we do not require the assumption that the Bellman error is modeled correctly by G, that is,</s></p><formula xml:id="formula_23">T π (•, •; Q) -η -Q(•, •) may not be in G. A natural choice of G is R ⊕ Q = {c + Q : c ∈ R, Q ∈</formula><p xml:id="_z3vr5sa"><s xml:id="_6cMuS9Y">Q}, however this is not mandatory.</s><s xml:id="_MuYzuqy"><ref type="bibr" target="#b5">Farahmand et al. (2016)</ref> assumed that the Bellman error (in discounted setting) is in fact in Q in order to develop a non-parametric estimator for the value function.</s><s xml:id="_CNNNcp9">As we will see, the assumption that the Bellman error belongs to G is in fact not necessary and can be relaxed (our proof will use the weaker assumption 4 in Section 5).</s></p><p xml:id="_8kKZpft"><s xml:id="_NQK4ryC">The key reason why the projected Bellman error (7) allows us to identify (η π , Qπ ) is because</s></p><formula xml:id="formula_24">g * π (•, •; η π , Qπ ) = 0 (see also (iii) in Assumption 4 ).</formula><p xml:id="_naSjdGs"><s xml:id="_fRJ5yU7">We now formally introduce the estimator for (η π , Qπ ).</s><s xml:id="_8htpvYE">This estimator is designed to minimize the projected Bellman error (7).</s><s xml:id="_vv6BqUE">Specifically, the estimator, (η π n , Qπ n ), of (η π , Qπ ), is found by solving a coupled (or nested) optimization problem:</s></p><formula xml:id="formula_25">min (η,Q)∈R×Q P n 1 T T t=1 ĝ2 n,π (S t , A t ; η, Q) + λ n J 2 1 (Q),<label>(8)</label></formula><p xml:id="_J5ahfNw"><s xml:id="_ZU2K4JQ">where for each (η, Q), ĝn,π (•, •; η, Q) is an estimator for the projection of the Bellman error given by ĝn,π (•,</s></p><formula xml:id="formula_26">•; η, Q) = argmin g∈G P n 1 T T t=1 R t+1 + a ′ π(a ′ | S t+1 )Q(S t+1 , a ′ ) -η -Q(S t , A t ) -g(S t , A t ) 2 + µ n J 2 2 (g). (<label>9</label></formula><formula xml:id="formula_27">)</formula><p xml:id="_vz6wQ5G"><s xml:id="_7JZXfsS">where J 1 : Q → R + and J 2 : G → R + are two regularizers and λ n and µ n are tuning parameters.</s></p><p xml:id="_p3AKGe3"><s xml:id="_52Kj8Sy">We can see that for every (η, Q), ĝn,π (•, •; η, Q) is a penalized estimator for the projected Bellman error g * π (•, •; η, Q) in ( <ref type="formula" target="#formula_21">7</ref>).</s><s xml:id="_J2fdvn9">On the other hand, the objective function in ( <ref type="formula" target="#formula_25">8</ref>) is a plug-in version of the objective function in (6) where we replace the Bellman error by ĝn,π (•, •; η, Q).</s></p><p xml:id="_FuUARU7"><s xml:id="_cXQ825Y">Compared to the classic empirical risk minimization, (η π n , Qπ n ) solves a nested optimization problem in the sense that the objective function (8) depends on ĝn,π (•, •; η, Q) which itself is the solution of another, lower-level optimization (9).</s></p><p xml:id="_phbmWNM"><s xml:id="_6KK3U8P">The penalty term, λ n J 2 1 (Q), is used to balance between the model fitting (i.e., the squared estimated Bellman error) and the complexity of the relative value function measured by J 1 (Q).</s><s xml:id="_6NAZnZD">Similarly, µ n J 2 2 (g) is used to control the overfitting in estimating the projected Bellman error when the function class, G, is complex.</s><s xml:id="_s7ZTS98">In the case where the function space is k-th order Sobolev space, the regularizer is typically defined by the k-th order derivative to capture the smoothness of function.</s><s xml:id="_TvySH5n">In the case where the function space is Reproducing Kernel Hilbert Space (RKHS), the regularizer is the endowed norm.</s><s xml:id="_kUSgwSX">In Supplement D, we provide a closed-form solution of the estimator when both Q and G are RKHSs.</s></p><p xml:id="_sXzJZYz"><s xml:id="_WHhk9qd">So far we have focused on evaluating a single target policy.</s><s xml:id="_gXsgwFm">In practice, one might want to compare the target policy to some reference policy or contrast multiple target policies of interest.</s><s xml:id="_VE98gwA">Suppose we are interested in K different target policies, {π j } K j=1 .</s><s xml:id="_H6WmcmP">The above procedure (8) can be applied to estimate {η π j } K j=1 .</s><s xml:id="_NxVdBXr">In the next section, we will provide the result of the joint asymptotic distribution of ηπ j n K j=1 (see Corollary 1).</s><s xml:id="_S3vqGhN">This can be used, for example, to construct the confidence interval of the difference of the average rewards between two policies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_kBtmqbg">Theoretical Results</head><p xml:id="_KTX8fpt"><s xml:id="_8jpk6dS">In this section, we first derive the global rate of convergence for (η π n , Qπ n ) in ( <ref type="formula" target="#formula_25">8</ref>) and derive the asymptotic distribution of ηπ n for a single policy.</s><s xml:id="_Bcm4Bdx">We then extend the results to the case of multiple policies.</s><s xml:id="_AyRPZMm">For any state-action function, f (s, a, s ′ ), and distribution, ν, on S × A, denote the L 2 (ν) norm by f 2 ν = f 2 (s, a)dν(s, a).</s><s xml:id="_mBxu5Gc">If the norm does not have a subscript, then the expectation is with respect to the average state-action distribution in the trajectory,</s></p><formula xml:id="formula_28">D, that is, f 2 = E (1/T ) T t=1 f 2 (S t , A t ) .</formula><p xml:id="_MXvhPYP"><s xml:id="_QMgtVmw">We first state two standard assumptions used in the non-parametric regression literature <ref type="bibr" target="#b8">(Györfi et al. 2006)</ref>.</s><s xml:id="_9pk6j5j">Recall that the shifted relative value function is defined as Qπ =</s></p><formula xml:id="formula_29">Q π -Q π (s * , a * ).</formula><p xml:id="_fFE3Szj"><s xml:id="_kfNFbJJ">Assumption 2. The reward is uniformly bounded:</s></p><formula xml:id="formula_30">|R t+1 | ≤ R max &lt; ∞ for all t ≥ 1. The shifted relative value function is bounded: | Qπ (s, a)| ≤ Q max for all s ∈ S and a ∈ A. Assumption 3. The function class, Q, satisfies (i) Q(s * , a * ) = 0 and Q ∞ ≤ Q max for all Q ∈ Q and (ii) Qπ ∈ Q.</formula><p xml:id="_DjN25uf"><s xml:id="_b4sCUPn">The assumption of a bounded reward is mainly to simplify the proof and can be relaxed to the sub-Gaussian case, that is, the error R t+1 -r(S t , A t ) is sub-Gaussian for all t ≤ T .</s><s xml:id="_mSNPey8">The boundedness assumption on the shifted relative value function can be ensured by assuming certain smoothness assumptions on the transition distribution <ref type="bibr" target="#b25">(Ortner &amp; Ryabko 2012)</ref> or assuming geometric convergence to the stationary distribution (Hernández-Lerma &amp; Lasserre 1999).</s><s xml:id="_fETU6zp">The boundedness assumption, (i), for members of the function class, Q, is used to simplify the proof; a truncation argument can be used to avoid this assumption.</s></p><p xml:id="_AAftQdF"><s xml:id="_eSGKjKV">Recall that g * π (•, •; η, Q) is a projected Bellman error in (7) into a function class, G.</s><s xml:id="_DGDmeZc">We make the following assumptions about G.</s></p><formula xml:id="formula_31">Assumption 4. The function class, G, satisfies (i) 0 ∈ G, (ii) g ∞ ≤ G max for all g ∈ G, and (iii) κ = inf g * π (•, •; η, Q) : T π (•, •; Q) -η -Q(•, •) = 1, η ∈ R, Q ∈ Q &gt; 0.</formula><p xml:id="_qARZgCJ"><s xml:id="_TYUEtMB">Given R max and Q max , G max can be chosen as 2(R max +Q max ).</s><s xml:id="_yeQpPpx">Similar to Q, the boundedness assumption of G is used to simply the proof and can be relaxed  <ref type="formula">2016</ref>), here we do not assume</s></p><formula xml:id="formula_32">T π (•, •; Q) -η -Q(•, •) ∈ G for every (η, Q).</formula><p xml:id="_2gDkY5D"><s xml:id="_NvSR3cj">If this were the case, then we would have</s></p><formula xml:id="formula_33">g * π (•, •; η, Q) = T π (•, •; Q) -η -Q(•, •) and thus κ = 1.</formula><p xml:id="_BV4deB8"><s xml:id="_26mmWr4">Below we make assumptions on the complexity of the function classes, Q and G.</s><s xml:id="_NANGw5M">These assumptions are satisfied for common function classes, for example RKHS and Sobolev spaces (Van de Geer 2000, <ref type="bibr" target="#b33">Zhao et al. 2016</ref><ref type="bibr" target="#b28">, Steinwart &amp; Christmann 2008</ref><ref type="bibr" target="#b8">, Györfi et al. 2006</ref>).</s><s xml:id="_FPYKN4D">We denote by N (ǫ, F , • ) the ǫ-covering number of a set of functions, F , with respect to the norm, • .</s></p><p xml:id="_yva55Tv"><s xml:id="_ZMv8d5s">Assumption 5. (i) The regularization functional J 1 and J 2 are pseudo norms and induced by the inner products J 1 (•, •) and J 2 (•, •), respectively.</s><s xml:id="_FtEnpBp">There exist constants C 1 , C 2 such that</s></p><formula xml:id="formula_34">J 2 (g * π (•, •; η, Q)) ≤ C 1 + C 2 J 1 (Q) holds for all (η, Q) ∈ R × Q. (ii) Let Q M = {c + Q : |c| ≤ R max , Q ∈ Q, J 1 (Q) ≤ M} and G M = {g : g ∈ G, J 2 (g) ≤ M}.</formula><p xml:id="_WYn3G6k"><s xml:id="_rnCZ5Gk">There exist constants C 3 and α ∈ (0, 1), such that for any ǫ, M &gt; 0,</s></p><formula xml:id="formula_35">max log N(ǫ, G M , • ∞ ), log N (ǫ, Q M , • ∞ ) ≤ C 3 M ǫ 2α .</formula><p xml:id="_xJkr83h"><s xml:id="_v6wkbHc">The upper bound on J 2 (g * π (•, •; η, Q)) in (i) is realistic when the transition kernel is sufficiently smooth (see <ref type="bibr" target="#b5">Farahmand et al. (2016)</ref> for an example of MDP satisfying this condition).</s><s xml:id="_StFZzr4">We use a common α ∈ (0, 1) for both Q and G in (ii) to simply the proof.</s><s xml:id="_tjgh9Yw">Now we are ready to state the theorem about the convergence rate for (η π n , Qπ n ) in terms of the Bellman error.</s></p><p xml:id="_UaWzxm7"><s xml:id="_dNdmKSF">Theorem 1 (Global Convergence Rate).</s><s xml:id="_BRZc7Tk">Let (η π n , Qπ n ) be the estimator defined in ( <ref type="formula" target="#formula_25">8</ref>).</s><s xml:id="_CUKFPX6">Suppose Assumptions 1-5 hold and the tuning parameters,</s></p><formula xml:id="formula_36">(λ n , µ n ), satisfy τ -1 n -1 1+α ≤ µ n ≤ τ λ n</formula><p xml:id="_btxYW49"><s xml:id="_MX8FC2j">for some constant, τ &gt; 0. Then the following bounds hold with probability at least 1δ,</s></p><formula xml:id="formula_37">T π (•, •; Qπ n ) -ηπ n -Qπ n (•, •) 2 κ -2 λ n (1 + J 2 1 ( Qπ ))(1 + log(1/δ)), J 1 ( Qπ n ) 1 + log(1/δ) + J 2 1 ( Qπ ),</formula><p xml:id="_4Yw5g6h"><s xml:id="_4w2eVqz">where the leading constants depend only on</s></p><formula xml:id="formula_38">(τ, R max , Q max , G max , C 1 , C 2 , C 3 , α).</formula><p xml:id="_jnBTxYA"><s xml:id="_RaH3KFH">In Lemma ?? in Supplement B, we show that up to a constant,</s></p><formula xml:id="formula_39">|η π n -η π | T π (•, •; Qπ n ) - ηπ n -Qπ n (•, •) 2</formula><p xml:id="_7FBz224"><s xml:id="_FYQwq7R">and thus ηπ n is a consistent estimator for η π when λ n = o P (1).</s><s xml:id="_Z7fNNVC">When the tuning parameters are chosen such that λ n ≍ µ n and λ n ≍ n -1/(1+α) , the Bellman error at (η π n , Qπ n ) has the optimal rate of convergence, i.e., T π (•,</s></p><formula xml:id="formula_40">•; ηπ n , Qπ n ) -ηπ n -Qπ n (•, •) 2 = O P (n -1/(1+α)</formula><p xml:id="_QmuHn4a"><s xml:id="_EaZ4dGH">).</s><s xml:id="_9xSx5Wh">The proof of Theorem 1 is provided in Supplement B.</s></p><p xml:id="_9S6McYN"><s xml:id="_YK2by5k">In the following, we provide the asymptotic distribution of the estimated average reward.</s><s xml:id="_WgnFjAV">The direction, e π , is used to control the bias (η π nη π ) caused by the penalization on the non-parametric component (i.e., relative value function) in the estimator (8).</s><s xml:id="_MEU9AGN">This is akin to <ref type="bibr" target="#b4">Donald et al. (1994)</ref>, <ref type="bibr" target="#b31">Van de Geer (2000)</ref> for the analysis in the regression problem).</s><s xml:id="_Z8n7gQq">In our setting, the direction, e π (s, a), satisfies the following orthogonality: for any state-action function, Q,</s></p><formula xml:id="formula_41">partially linear regression problem, Y = f (Z) + X ⊤ β + ǫ, in which the analog of e π (s, a) is the residual x -E(X | Z = z) (see</formula><formula xml:id="formula_42">E 1 T T t=1 e π (S t , A t ) Q(S t , A t ) - a ′ π(a ′ | S t+1 )Q(S t+1 , a ′ ) = 0. (<label>11</label></formula><formula xml:id="formula_43">)</formula><p xml:id="_j2yJD7z"><s xml:id="_PhKyw3R">To see this, note that <ref type="formula">10</ref>) is a ratio between the stationary distribution of state-action pair under target policy, π, and the average distribution of state-action pair in the trajectory, D, under the behavior policy.</s><s xml:id="_UF4XANq">The denominator is the expectation of the ratio under the stationary distribution.</s><s xml:id="_CWVNkAb">As a result of the denominator, we have e π (s, a)d π (s, a)dsda = 1.</s></p><formula xml:id="formula_44">Q(s, a)d π (s, a)dsda = a ′ π(a ′ | s ′ )Q(s ′ , a ′ )P (s ′ | s, a)d π (s, a)dsdads ′ . The numerator in (</formula><formula xml:id="formula_45">Next define q π (s, a) = lim t * →∞ 1 t * t * t=1 E π t k=1 {1 -e π (S k , A k )} | S 1 = s, A 1 = a . Note q π has</formula><p xml:id="_9B6xGB2"><s xml:id="_EpV8ZDA">a similar structure to that of the relative value function (3) in which the "reward" at time, t, is {1e π (S t , A t )} and the "average reward" is zero (i.e., {1-e π (s, a)}d π (s, a)dsda = 0).</s><s xml:id="_B9b4DVj">Similar to the relative value function (3), q π (•, •) satisfies a Bellman-like equation:</s></p><formula xml:id="formula_46">q(s, a) = 1 -e π (s, a) + E a ′ π(S t+1 , a ′ )q(S t+1 , a ′ ) S t = s, A t = a . (<label>12</label></formula><formula xml:id="formula_47">)</formula><p xml:id="_7pdrtWJ"><s xml:id="_SA8ZrUa">We make the following smoothness assumption about e π and q π , akin to the assumptions used in partially linear regression literature (Van de Geer 2000, <ref type="bibr" target="#b33">Zhao et al. 2016</ref>).</s></p><p xml:id="_AtD54gG"><s xml:id="_aC2hgcc">Assumption 6.</s><s xml:id="_2pnqPhj">The shifted function, qπ = q πq π (s * , a * ) ∈ Q and e π ∈ G.</s></p><p xml:id="_zukQWPg"><s xml:id="_VfTMufA">Recall that in Assumption 3 we restrict Q(s * , a * ) = 0 for all Q ∈ Q.</s><s xml:id="_BsbW7Be">Thus we consider the shifted function qπ in the assumption above.</s><s xml:id="_ZzwCJTt">The analog of qπ ∈ Q in partially linear</s></p><formula xml:id="formula_48">regression problem, Y = f (Z) + X ⊤ β + ǫ, is the standard assumption that E[X|Z = •] ∈ F ,</formula><p xml:id="_wSgXmcv"><s xml:id="_knE4ex2">where F is the function class to model the nonparametric component, f (z) <ref type="bibr" target="#b4">(Donald et al. 1994</ref><ref type="bibr" target="#b31">, Van de Geer 2000)</ref>.</s><s xml:id="_EPkpDBd">The condition, qπ ∈ Q, will be used to prove the √ n rate of convergence and asymptotic normality of ηπ n .</s><s xml:id="_pcDAxUv">On the other hand, unlike in the regression setting, we assume that the direction function, e π , is sufficiently smooth (i.e., e π ∈ G).</s></p><p xml:id="_K8N3CMk"><s xml:id="_UFnE5wF">This assumption will be used to show that the bias of the coupled estimator, ηπ n , decreases sufficiently fast to zero.</s></p><p xml:id="_cQgeM7D"><s xml:id="_9UTDJ3w">The last assumption is a contraction-type property.</s><s xml:id="_CCRJbK7">This assumption will be used to control the variance of a remainder term caused by the estimation of Q π .</s></p><formula xml:id="formula_49">Assumption 7. Let (P π f )(•, •) = E π {f (S t+1 , A t+1 ) | S t = •, A t = •}</formula><p xml:id="_HU46JYU"><s xml:id="_fakCNSv">be the function of the conditional expectation and µ π (f ) = f (s, a)d π (s, a)dsda be the expectation under stationary distribution induced by π for a state-action function, f .</s><s xml:id="_Thz2s45">There exist constants, C 4 &gt; 0 and</s></p><formula xml:id="formula_50">0 ≤ β &lt; 1, such that for f ∈ L 2 and t ≥ 1, (P π ) t (f ) -µ π (f ) ≤ C 4 f β t .<label>(13)</label></formula><p xml:id="_UrwNxS4"><s xml:id="_ACxqCAy">The parameter, β, in Assumption 7 is akin to the discount factor, γ, in the discounted reward setting.</s><s xml:id="_NkpCMaC">Intuitively, this is related to the "mixing rate" of the Markov chain induced by the target policy π.</s><s xml:id="_F8JgPQs">A similar assumption was imposed in Van Roy (1998) (Assumption 7.2 on p. 99).</s><s xml:id="_P3emcF5">Now we are ready to present our main result, the asymptotic normality of the estimated average reward, ηπ n .</s></p><p xml:id="_W7Q9T9P"><s xml:id="_ruf8E3Y">Theorem 2 (Asymptotic Distribution).</s><s xml:id="_z9YAEbH">Suppose the conditions in Theorem 1 hold.</s><s xml:id="_HBtUdqs">In addition, suppose Assumption 6 and 7 hold and λ n = a n n -1/2 with a n → 0. The estimator, ηπ n , in ( <ref type="formula" target="#formula_26">9</ref>) is √ n-consistent and asymptotically normal:</s></p><formula xml:id="formula_51">√ n(η π n -η π ) ⇒ N(0, σ 2 ),</formula><p xml:id="_sn4DBRj"><s xml:id="_Yk6Npxh">where</s></p><formula xml:id="formula_52">σ 2 = Var 1 T T t=1 d π (S t , A t ) dT (S t , A t ) R t+1 + a ′ π(a ′ | S t+1 )Q π (S t+1 , a ′ ) -η π -Q π (S t , A t ) .</formula><p xml:id="_ZkctEXj"><s xml:id="_XJ4Vpun">From Theorem 2, the variance in estimating the average reward parameter, η π , depends on the length of trajectory and the ratio between the stationary distribution of the stateaction pair induced by the target policy (i.e., d π ) and the average state-action distribution in the training data (i.e., dT ).</s><s xml:id="_aqe9Qmy">To gain intuition of how these impact the asymptotic variance of ηπ n , consider a simplified setting where the conditional variance of</s></p><formula xml:id="formula_53">R t+1 + a ′ Q π (S t+1 , a ′ ) - η π -Q π (S t , A t ) given (S t , A t ) is a constant, denoted by σ 2 0 .</formula><p xml:id="_cTYUswQ"><s xml:id="_r9jG9gv">It can be shown that the asymptotic variance becomes σ 2 = σ 2 0 T (1 + (d π / dT ) -1 2 ).</s><s xml:id="_3e7Bmmc">Thus the smaller (d π / dT ) -1 2 (i.e., the ratio, d π / dT , close to one), the smaller the asymptotic variance of the estimated average reward.</s><s xml:id="_qhmyCnT">Although here we focus only on the asymptotic properties of ηπ n for large n (recall n is the number of i.i.d.</s><s xml:id="_NXjGdj9">trajectories), one can see that increasing length of the trajectory, T , reduces the asymptotic variance.</s><s xml:id="_yDEPM8d">Now we present the result for evaluating a class of policies, Π = {π 1 , . . .</s><s xml:id="_a4XAPZa">, π K }.</s><s xml:id="_Hz8N9vK">Denote by ηπ j n the estimated average reward of the policy, π j , using (8).</s></p><p xml:id="_bJZ7DrQ"><s xml:id="_T9JkDPD">Corollary 1 (Multiple Policies).</s><s xml:id="_4ucgmMu">Suppose the conditions in Theorem 1 and 2 hold for each</s></p><formula xml:id="formula_54">π ∈ Π. Let ǫ π t = d π (St,At) dT (St,At) [R t+1 + a ′ π(a ′ | S t+1 )Q π (S t+1 , a ′ ) -η π -Q π (S t , A t )]</formula><p xml:id="_jQNBhvY"><s xml:id="_wBkvPkd">for each π ∈ Π.</s><s xml:id="_HdMV2Tn">Then the estimated average rewards, {η π 1 n , . . .</s><s xml:id="_mmYrZ4V">, ηπ K n }, jointly converge in distribution to a multivariate Gaussian distribution:</s></p><formula xml:id="formula_55">     √ n(η π 1 n -η π 1 ) . . . √ n(η π K n -η π K )      ⇒ MVN(0, Σ),</formula><p xml:id="_5pb46tN"><s xml:id="_3CKhwdH">where the (i, j) element of Σ is given by</s></p><formula xml:id="formula_56">E (1/T ) T t=1 ǫ π i t (1/T ) T t=1 ǫ π j t .</formula><p xml:id="_AWAN7Pn"><s xml:id="_xTTrgQd">To conduct inference, we need to estimate the asymptotic variance, Σ.</s><s xml:id="_U7WdrGk">For each π ∈ Π, we denote the plug-in estimation of ǫ π t (defined in Corollary 1) by ǫπ t in which we plug in (η π n , Qπ n ) and an estimator for the ratio, d π (s, a)/ dT (s, a).</s><s xml:id="_BD4c6BJ">We then estimate the asymptotic variance, Σ, by Σn = P n 1 T T t=1</s></p><formula xml:id="formula_57">ǫπ i t 1 T T t=1 ǫπ j t K i,j=1</formula><p xml:id="_fWkYb5g"><s xml:id="_uDvTAZ4">.</s></p><p xml:id="_qTrCMXC"><s xml:id="_WqxyXXa">We can estimate the ratio, d π / dT , as follows.</s><s xml:id="_WaGzY3n">First, we note that by taking the expectation on both sides of (10), the ratio can be written in terms of e π : d π (s, a)/ dT (s, a) = e π (s, a)/E{(1/T )</s></p><p xml:id="_HwWr6Ky"><s xml:id="_bYx7fZB">T t=1 e π (S t , A t )}.</s><s xml:id="_DyUfgVk">It is enough to construct an estimator for e π , which we denote by êπ n , and then estimate the ratio by êπ n (s, a)/P n {(1/T ) T t=1 êπ n (S t , A t )}.</s><s xml:id="_g7eGXc8">Motivated by the orthogonality (11) and the expression (12), we construct the estimator for e π (•, •) by êπ n (•, •) = gn,π (•, •; qπ n ), where qπ n (•, •) = argmin q∈Q P n {(1/T ) T t=1 g2</s></p><p xml:id="_Md9rrqk"><s xml:id="_yw9Qc2Z">n,π (S t , A t ; q)} + λn J 2 1 (q) and gn,π (•, •; q) = argmin g∈G P n [(1/T ) T t=1 {1-q(S t , A t )+ a ′ π(a ′ | S t+1 )q(S t+1 , a ′ )g(S t , A t )} 2 ] + μn J 2 2 (g) for each q ∈ Q.</s><s xml:id="_yMpnW6c">Here ( λn , μn ) are some tuning parameters.</s><s xml:id="_8ddE6DH">Following a similar argument as in the proof of Theorem 1, qπ n can be shown to be a consistent estimator for qπ .</s><s xml:id="_6byGwU2">Under the assumption that e π ∈ G, e π can be consistently estimated by <ref type="formula" target="#formula_46">12</ref>).</s><s xml:id="_Zkww3QZ">See Supplement C for additional details about the estimator êπ n .</s><s xml:id="_EuUJjjY">In Supplement D, we provide a closed-form solution for the estimator for the asymptotic variance when Q and G are RKHSs.</s></p><formula xml:id="formula_58">êπ n (•, •) = gn,π (•, •; qπ n ) based on (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_7gZtfxz">Simulation</head><p xml:id="_8bb9F4p"><s xml:id="_52c2FUb">In this section, we conduct a simulation study to evaluate the performance of the proposed method.</s><s xml:id="_VNya4Bj">The generative model is given as follows.</s><s xml:id="_fYGswuk">We follow the state generative model in <ref type="bibr" target="#b20">Luckett et al. (2020)</ref>.</s><s xml:id="_2zzdqZx">Specifically, the state, S t = (S t,1 , S t,2 ), is a two-dimensional vector and the action, A t ∈ {0, 1}, is binary.</s><s xml:id="_UW7xwc6">Given the current state, S t , and action, A t , the next state, S t+1 = (S t+1,1 , S t+1,2 ), is generated by S t+1,1 = (3/4)(2A t -1)S t,1 + (1/4)S t,1 S t,2 + N (0, 0.5 2 ) and S t+1,2 = (3/4)(1 -2A t )S t,2 + (1/4)S t,1 S t,2 + N (0, 0.5 2 ).</s><s xml:id="_PjTaAgt">Note that receiving a treatment (A t = 1) increases the value of S t,1 while decreases S t,2 .</s><s xml:id="_8ffUxNK">The reward is generated by R t+1 = S t+1,1 + (1/2)S t+1,2 + (1/4)(2A t -1).</s><s xml:id="_Jte2Mur">For each trajectory in the training data, the state variables are generated as independent standard normal random variables and the behavior policy is to choose A t = 1 with a fixed probability 0.5.</s><s xml:id="_TrehfCk">We evaluate and compare two natural policies: the "always treat" policy, π 1 (a | s) = 1, and "no treatment" policy, π 2 (a | s) = 0.</s></p><p xml:id="_PEWvnkX"><s xml:id="_Ft9sep7">In the implementation, we use RKHS with the radial basis function (RBF) kernel to construct the function classes, Q and G.</s><s xml:id="_NtRZJgK">The details of how to modify an arbitrary RKHS such that the value at (s * , a * ) is zero can be found in Supplement D. The bandwidth parameter in the RBF kernel is chosen by the median heuristic.</s><s xml:id="_d9RN7Dk">Recall that the estimator (8) involves two tuning parameters, (λ n , µ n ).</s><s xml:id="_4rSBYk9">Following the idea in <ref type="bibr" target="#b6">Farahmand &amp; Szepesvári (2011)</ref>, we select these tuning parameters as follows.</s><s xml:id="_NXhX6pv">We first split the dataset into a training set, D trn , and a validation set, D val .</s><s xml:id="_X72SuvJ">For each candidate value of the tuning parameters, (λ, µ), the training set, D trn , is used to form the estimator by ( <ref type="formula" target="#formula_25">8</ref>) and ( <ref type="formula" target="#formula_26">9</ref>).</s><s xml:id="_BpRh7Nf">Denote the corresponding estimator by ηπ (λ, µ), Qπ (•, •; λ, µ) .</s><s xml:id="_4TAKFBk">Then the temporal difference (TD) error, R + a ′ Qπ (S ′ , a ′ ; λ, µ)ηπ (λ, µ) -Qπ (S, A; λ, µ), is calculated for each transition sample, (S, A, S ′ , R), in the validation set, D val .</s><s xml:id="_wSuaWEm">Recall that the Bellman error is zero at (η π , Qπ ).</s><s xml:id="_yurNxzz">We use the validation set, D val , to fit a model for the Bellman error with respect to (η π (λ, µ), Qπ (•, •; λ, µ)) and denote the estimated Bellman error by f (•, •; λ, µ).</s><s xml:id="_dRnWvhn">Note that this step is essentially a regression problem (i.e., the dependent variable is the TD error and independent variables are the current state and action).</s><s xml:id="_7EM6cYv">Finally, we choose (λ, µ) that minimizes the squared estimated Bellman error over the validation set, i.e., (S,A)∈D val f 2 (S, A; λ, µ).</s></p><p xml:id="_E3Qwtmv"><s xml:id="_BWenK3S">The final estimator for η π is then calculated with the optimal tuning parameters using the entire dataset.</s><s xml:id="_CrTEdzN">In the simulation, we use (1/2) of the trajectories for the training set and</s></p><p xml:id="_vaws78J"><s xml:id="_tQrSzP8">(1/2) for the validation set and we use Gaussian Process regression to estimate the Bellman error in the validation step.</s></p><p xml:id="_gzTRuSS"><s xml:id="_JUU8vqg">We consider different scenarios of the number of the trajectories, n ∈ {25, 40}, and the length of each trajectory, T ∈ {25, 50, 75}.</s><s xml:id="_ADgMk5h">In each scenario, we generate 500 simulated dataset and for each dataset we construct the 95% confidence intervals of η π 1 , η π 2 and η π 1η π 2 .</s><s xml:id="_gtxeCRP">The coverage probability of each confidence interval is calculated over 500 repetitions.</s></p><p xml:id="_sWnwqq4"><s xml:id="_ax2pRK3">The simulation result is reported in Table <ref type="table" target="#tab_1">1</ref>.</s><s xml:id="_bFMgN3D">When the number of trajectories is small (i.e., n = 25), the simulated coverage probability is slightly smaller than the claimed value, 0.95, especially when the length of the trajectory, T , is small.</s><s xml:id="_fUTnt7s">It can be seen that the coverage probability slightly improves when T increases.</s><s xml:id="_mDrq6eH">When n = 40, the coverage probability becomes closer to 0.95 as desired.</s><s xml:id="_2gJ49K3">Overall, the simulation result demonstrates the validity of the inference and the selection procedure for the tuning parameters.</s><s xml:id="_qejBPy4">It suggests that it is necessary to perform a small-sample correction when both n and T are small.</s><s xml:id="_2yQDkXW">This is left for future work.</s><s xml:id="_zxbH89H">We apply the method to the data collected in the first study in HeartSteps <ref type="bibr" target="#b14">(Klasnja et al. 2015</ref><ref type="bibr" target="#b18">, Liao et al. 2016</ref><ref type="bibr" target="#b15">, Klasnja et al. 2019</ref>).</s><s xml:id="_FhC775f">Below we refer to this study by HS1 for simplicity.</s><s xml:id="_YTKtARA">HS1 was a 42-day MRT with 44 healthy sedentary adults.</s><s xml:id="_jUrQazk">We focus on the activity suggestion intervention component.</s><s xml:id="_Tn7dAmQ">There were five individual-specified times in a day which were roughly separated by 2.5 hours and corresponded to the individuals morning commute, mid-day, mid-afternoon, evening commute, and post-dinner times.</s><s xml:id="_mAnw57U">At each decision time, an activity suggestion was sent with a fixed probability 0.6 only if the participants were considered to be available for treatment.</s><s xml:id="_ajpsuRQ">For example, the participants were considered unavailable when they were currently physically active (e.g., walking or running) or driving a vehicle.</s></p><p xml:id="_x4AQPTK"><s xml:id="_VKmCw7r">The activity suggestions were intended to motivate near-time walking.</s><s xml:id="_rRfnfeA">Each participant wore a Jawbone wrist tracker and the minute-level step count data was recorded.</s></p><p xml:id="_ZNxtjVn"><s xml:id="_WHJadnP">We construct the state based on the participant's step count data (e.g., the 30-min step count prior to the decision time and the total step count from yesterday), location, temperature and number of the notifications received over the last seven days.</s><s xml:id="_JCWqP8j">We also include in the state the time slot index in the day (1 to 5) and the indicator measuring how the step count varies at the current time slot over the last seven days.</s><s xml:id="_VfTHTCH">The reward is formed by the log transformation of the total step count collected in 30-min window after the decision time.</s><s xml:id="_qhsjv9f">The log transformation is performed as the step count data is positively skewed <ref type="bibr" target="#b15">(Klasnja et al. 2019</ref>).</s><s xml:id="_xCBYRBt">The step count data might be missing because the Jawbone tracker recorded data only when there were steps occurred.</s><s xml:id="_JzRvD2s">We use the same imputation procedure as in <ref type="bibr" target="#b15">Klasnja et al. (2019)</ref>.</s><s xml:id="_jbEDWwF">The state related to the step count are constructed based on the imputed step counts.</s><s xml:id="_f4p6HzQ">The variables in the state are chosen to be predictive of the reward.</s><s xml:id="_RdSQY5T">In particular, each variable is selected, at the significance level of 0.05, based on a marginal Generalized Estimating Equation (GEE) analysis.</s><s xml:id="_9qMR4D5">In the analysis, we exclude seven participants' data as in the primary analysis in <ref type="bibr" target="#b15">Klasnja et al. (2019)</ref> (three due to technical issues and four due to early dropout).</s><s xml:id="_6vdMU6d">In addition, from the 37 participants' data we exclude the decision times when participants were traveling abroad or experiencing technical issues or when the reward (i.e., post 30-min step count) is considered as missing (see <ref type="bibr" target="#b15">Klasnja et al. (2019)</ref> for details).</s></p><p xml:id="_NbPHpEY"><s xml:id="_QEBrFGh">We consider three target policies.</s><s xml:id="_MU9Qkxx">The first policy, π nothing , is "do nothing".</s><s xml:id="_mBgKc4j">The second policy, π always , is the "always treat" policy.</s><s xml:id="_s6HU2fa">Recall that in HeartSteps the activity suggestion can be sent only when the participant is available.</s><s xml:id="_J8MP8fx">So here the "always treat" policy refers to always send the suggestion whenever the participant is available.</s><s xml:id="_NGjbQ2d">The third policy, π location , is based on the location.</s><s xml:id="_pRY8ZcM">Specifically, we consider the policy that sends the activity suggestion when the participant is at either home or work location and available.</s><s xml:id="_Pd89yZV">This policy is of interest because people at home or work are in a more structured environment and thus might be able to better respond to an activity suggestion as compared with at other locations.</s><s xml:id="_Bca9eau">In HS1, about 44% of the available decision times were at times that the participants were at their home or work location.</s><s xml:id="_WBHWfEW">Thus the policy, π location , is different from the "always treat" policy, π always .</s></p><p xml:id="_ntCXSB6"><s xml:id="_8acNN5U">In the implementation, we use the RKHS with the radial basis function kernel to form the function classes, Q and G.</s><s xml:id="_zPYpFt3">The tuning parameters are selected based on the procedure described in Section 6.</s><s xml:id="_fhSwDTk">The estimated average reward of the location-based policy, π location , is 3.155 with the 95% confidence interval , [2.893, 3.417], which is slightly better than the "do nothing" policy.</s><s xml:id="_47e8WCv">Specifically, the estimated average reward of π nothing is 2.962 and the 95% confidence interval of the difference, η π locationη π nothing , is <ref type="bibr">[-0.016, 0.402]</ref>.</s><s xml:id="_Amv2QXa">Translating back to the raw step count as in <ref type="bibr" target="#b15">Klasnja et al. (2019)</ref>, the location-based policy is able to increase the average 30-min step count roughly by 22% (i.e., exp(3.16</s><s xml:id="_Z8t9Z4M">-2.96) -1 = 1.22), corresponding to 55 steps (the mean post-decision time step count is 248 across all decision times in the dataset).</s><s xml:id="_kVaVdUa">However if we compare the "always treat" policy (η π always = 3.127, 95% confidence interval is [2.840, 3.413]) with the location-based policy, π location , we see no indication that providing treatment only at home or work is better than always providing treatment (the 95% confidence interval of η π locationη π always is [-0.161,</s><s xml:id="_FBNtZhq">0.217]).</s><s xml:id="_ASJHx4n">Recall that the sample size for this study is n = 37 thus this non-significant finding may be due to the small sample.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" xml:id="_zQMbuBw">Discussion</head><p xml:id="_WcDSk7a"><s xml:id="_sERu4Eq">In this work we developed a flexible method to conduct inference about the the long-term average outcomes for given target policies using data collected from a possibly different behavior policy.</s><s xml:id="_cP6AUYk">We believe that this is an important first step towards developing data-based just-in-time adaptive interventions.</s><s xml:id="_yNU3YvN">Below we discuss some directions for future research.</s></p><p xml:id="_NF2fdaa"><s xml:id="_K6JucRy">In many MRT studies, a natural choice of the proximal outcome to assess the effectiveness of the intervention is binary.</s><s xml:id="_TjuwxPg">For example, in the Substance Abuse Research Assistance study <ref type="bibr" target="#b27">(Rabbi et al. 2018)</ref> Non-stationarity occurs mainly because of the unobserved aspects of the current state (e.g., the engagement and/or burden) in many mHealth applications.</s><s xml:id="_sfcWPpR">It will be interesting to generalize the average reward framework to incorporate the non-stationarity detected in the observed trajectory.</s><s xml:id="_Zhu2SHX">Alternatively, one can consider evaluating the treatment policy in the indefinite horizon setting where there is an absorbing state (akin to the individual disengaging from the mobile app) and thus we aim to conduct inference about the expected total rewards until the absorbing state is reached.</s></p><p xml:id="_8nbcn7f"><s xml:id="_ypEJpDf">We focused on evaluating and contrasting multiple pre-specified treatment policies.</s><s xml:id="_kbtaPt2">An important next step is to extend the method to learn the optimal policy that would lead to the largest long-term average reward and to develop the inferential methods to assess the usefulness of certain variables in the policy.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_UjMM6wh"><s xml:id="_MbrHqwF">. The value of κ measures how well the function class, G, approximates the Bellman error for all (η, Q) in which η ∈ R and Q ∈ Q.</s><s xml:id="_87v24T3">The condition of a strictly positive κ ensures the estimator (8) based on minimizing the projected Bellman error onto the space, G, is able to identify the true values, (η π , Qπ ).</s><s xml:id="_XZCuPYx">This is similar to the eigenvalue condition (Assumption 5) in<ref type="bibr" target="#b20">Luckett et al. (2020)</ref>, but they are essentially using the same function class for Q and G. Recall that, unlike inFarahmand et al. (</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc><div><p xml:id="_Pt8JZh8"><s xml:id="_pJ68skx">This requires additional notation as follows.</s><s xml:id="_9T2p9bf">Define d π (s, a) = π(a | s)d π (s); d π is the density of the stationary distribution of the state-action under the target policy, π.</s><s xml:id="_QeZrEp7">For each t ≥ 1, denote by d t (s, a) the density of the state-action pair in the trajectory, D, under the behavior policy.</s><s xml:id="_ytPhr4N">Let dT (s, a) be the average density over T decision times.</s><s xml:id="_yuWp6Hp">Motivated by the least favorable direction in partial linear regression problems (Van de Geer 2000, Zhao et al. 2016), we define the direction function, e π (s, a), by e π (s, a) = d π (s, a)/ dT (s, a) (d π (s, ã)/ dT (s, ã))d π (s, ã)dsdã .</s><s xml:id="_XfuDk9X">(10)</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc><div><p xml:id="_337ZZ4C"><s xml:id="_CyzVxK5">, the proximal outcome was whether the individual completed a daily survey.</s><s xml:id="_4Be7KgJ">An interesting open question is how to extend the method to the binary reward setting, which would require carefully choosing the model to represent the relative value function and/or the loss functions used in estimating the Bellman error and solving the Bellman equation.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc><div><p xml:id="_5G5kCrf"><s xml:id="_famcMVB">Coverage probability of the 95% confidence interval and MAD (mean absolute deviation) over 500 repetitions.</s><s xml:id="_eED4J93">Case 1: policy evaluation of π 1 .</s><s xml:id="_uAFyb36">Case 2: policy evaluation of π 2 .</s><s xml:id="_uFCgyWn">Case 3: policy comparison between π 1 and π 2 .</s></p></div></figDesc><table><row><cell></cell><cell cols="3">n T Coverage Prob. MAD</cell><cell cols="3">n T Coverage Prob. MAD</cell></row><row><cell></cell><cell>25 25</cell><cell>0.926</cell><cell cols="2">0.0702 40 25</cell><cell>0.944</cell><cell>0.0546</cell></row><row><cell>Case 1</cell><cell>25 50</cell><cell>0.930</cell><cell cols="2">0.0535 40 50</cell><cell>0.944</cell><cell>0.0427</cell></row><row><cell></cell><cell>25 75</cell><cell>0.938</cell><cell cols="2">0.0438 40 75</cell><cell>0.948</cell><cell>0.0346</cell></row><row><cell></cell><cell>25 25</cell><cell>0.934</cell><cell cols="2">0.0368 40 25</cell><cell>0.928</cell><cell>0.0313</cell></row><row><cell>Case 2</cell><cell>25 50</cell><cell>0.946</cell><cell cols="2">0.0261 40 50</cell><cell>0.940</cell><cell>0.0224</cell></row><row><cell></cell><cell>25 75</cell><cell>0.922</cell><cell cols="2">0.0222 40 75</cell><cell>0.942</cell><cell>0.0185</cell></row><row><cell></cell><cell>25 25</cell><cell>0.932</cell><cell cols="2">0.0761 40 25</cell><cell>0.946</cell><cell>0.0612</cell></row><row><cell>Case 3</cell><cell>25 50</cell><cell>0.928</cell><cell cols="2">0.0598 40 50</cell><cell>0.948</cell><cell>0.0461</cell></row><row><cell></cell><cell>25 75</cell><cell>0.932</cell><cell cols="2">0.0480 40 75</cell><cell>0.948</cell><cell>0.0388</cell></row><row><cell cols="4">7 Case Study: HeartSteps</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p xml:id="_B5mGM6v"><s xml:id="_hrkEnqH">http://people.seas.harvard.edu/ ~samurphy/JITAI_MRT/mrts4.html</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_S3zKzN9">Acknowledgment</head><p xml:id="_Gk3vaa5"><s xml:id="_beC6FFF">This work was supported by <rs type="funder">National Institute on Alcohol Abuse and Alcoholism (NIAAA)</rs> of the <rs type="funder">National Institutes of Health</rs> under award number <rs type="grantNumber">R01AA23187</rs>, <rs type="funder">National Institute on Drug Abuse (NIDA)</rs> of the <rs type="funder">National Institutes of Health</rs> under award numbers <rs type="grantNumber">P50DA039838</rs> and <rs type="grantNumber">R01DA039901</rs>, <rs type="funder">National Institute of Biomedical Imaging and Bioengineering (NIBIB)</rs> of the <rs type="funder">National Institutes of Health</rs> under award number <rs type="grantNumber">U54EB020404</rs>, <rs type="funder">National Cancer Institute (NCI)</rs> of the <rs type="funder">National Institutes of Health</rs> under award number <rs type="grantNumber">U01CA229437</rs>, and <rs type="funder">National Heart, Lung, and Blood Institute (NHLBI)</rs> of the <rs type="funder">National Institutes of Health</rs> under award number <rs type="grantNumber">R01HL125440</rs>.</s><s xml:id="_pYC2MY9">The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cFbNp9b">
					<idno type="grant-number">R01AA23187</idno>
				</org>
				<org type="funding" xml:id="_tqpKbWJ">
					<idno type="grant-number">P50DA039838</idno>
				</org>
				<org type="funding" xml:id="_PEHG94S">
					<idno type="grant-number">R01DA039901</idno>
				</org>
				<org type="funding" xml:id="_jCfgZGJ">
					<idno type="grant-number">U54EB020404</idno>
				</org>
				<org type="funding" xml:id="_u95xmgt">
					<idno type="grant-number">U01CA229437</idno>
				</org>
				<org type="funding" xml:id="_m2323A7">
					<idno type="grant-number">R01HL125440</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_Nh7WEcj">Learning near-optimal policies with bellmanresidual minimization based fitted policy iteration and a single sample path</title>
		<author>
			<persName><forename type="first">A</forename><surname>Antos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-007-5038-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UrGYRcT">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="129" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Antos, A., Szepesvári, C. &amp; Munos, R. (2008), &apos;Learning near-optimal policies with bellman- residual minimization based fitted policy iteration and a single sample path&apos;, Machine Learning 71(1), 89-129.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_4qraVq3">Linear least-squares algorithms for temporal difference learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bradtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1018056104778</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7MZACm7">Machine learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="33" to="57" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bradtke, S. J. &amp; Barto, A. G. (1996), &apos;Linear least-squares algorithms for temporal difference learning&apos;, Machine learning 22(1-3), 33-57.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moodie</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4614-7428-9</idno>
		<title level="m" xml:id="_7UB2CrH">Statistical methods for dynamic treatment regimes</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chakraborty, B. &amp; Moodie, E. (2013), Statistical methods for dynamic treatment regimes, Springer.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_Am8DbED">The stratified microrandomized trial design: sample size considerations for testing nested causal effects of time-varying treatments</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1214/19-aoas1293</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kSZXTgB">Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="661" to="684" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dempsey, W., Liao, P., Kumar, S., Murphy, S. A. et al. (2020), &apos;The stratified micro- randomized trial design: sample size considerations for testing nested causal effects of time-varying treatments&apos;, Annals of Applied Statistics 14(2), 661-684.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_2rYB6me">Series estimation of semilinear models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Donald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Newey</surname></persName>
		</author>
		<idno type="DOI">10.1006/jmva.1994.1032</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VNGDz95">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="40" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Donald, S. G., Newey, W. K. et al. (1994), &apos;Series estimation of semilinear models&apos;, Journal of Multivariate Analysis 50(1), 30-40.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_Nxczhkn">Regularized policy iteration with nonparametric function spaces</title>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-89722-4_5</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kXRAgvQ">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4809" to="4874" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Farahmand, A.-m., Ghavamzadeh, M., Szepesvári, C. &amp; Mannor, S. (2016), &apos;Regularized policy iteration with nonparametric function spaces&apos;, The Journal of Machine Learning Research 17(1), 4809-4874.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_aPwGYvm">Model selection in reinforcement learning</title>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Farahmand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bvASDJ8">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="332" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Farahmand, A.-m. &amp; Szepesvári, C. (2011), &apos;Model selection in reinforcement learning&apos;, Machine learning 85(3), 299-332.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_bU5Uvmu">More robust doubly robust off-policy evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_V8ug2k6">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1447" to="1456" />
		</imprint>
	</monogr>
	<note type="raw_reference">Farajtabar, M., Chow, Y. &amp; Ghavamzadeh, M. (2018), More robust doubly robust off-policy evaluation, in &apos;International Conference on Machine Learning&apos;, pp. 1447-1456.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_wgq3MnC">A distribution-free theory of nonparametric regression</title>
		<author>
			<persName><forename type="first">L</forename><surname>Györfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Walk</surname></persName>
		</author>
		<idno type="DOI">10.1007/b97848</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Györfi, L., Kohler, M., Krzyzak, A. &amp; Walk, H. (2006), A distribution-free theory of non- parametric regression, Springer Science &amp; Business Media.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main" xml:id="_MtVS7tx">Further topics on discrete-time Markov control processes</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hernández-Lerma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Lasserre</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4612-0561-6</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Hernández-Lerma, O. &amp; Lasserre, J. B. (1999), Further topics on discrete-time Markov con- trol processes, Vol. 42, Springer.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_aNtGYAJ">Dynamic programming and markov processes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Howard, R. A. (1960), &apos;Dynamic programming and markov processes.&apos;.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_82TDb4R">Doubly robust off-policy value evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03722</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jiang, N. &amp; Li, L. (2015), &apos;Doubly robust off-policy value evaluation for reinforcement learn- ing&apos;, arXiv preprint arXiv:1511.03722 .</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_un7UZYQ">Intrinsically efficient, stable, and bounded off-policy evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uehara</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/asad059</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tdSrHxu">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3320" to="3329" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kallus, N. &amp; Uehara, M. (2019), Intrinsically efficient, stable, and bounded off-policy evalu- ation for reinforcement learning, in &apos;Advances in Neural Information Processing Systems&apos;, pp. 3320-3329.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_kfhjbEu">A personal health information toolkit for health intervention research</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Kizakevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eckhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weeks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bakalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lyden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Spira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ug6pC2C">Stud Health Technol Inform</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="35" to="39" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kizakevich, P. N., Eckhoff, R., Weger, S., Weeks, A., Brown, J., Bryant, S., Bakalov, V., Zhang, Y., Lyden, J. &amp; Spira, J. (2014), &apos;A personal health information toolkit for health intervention research&apos;, Stud Health Technol Inform 199, 35-39.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_6e2U36c">Micro-randomized trials: An experimental design for developing just-in-time adaptive interventions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boruvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1037/hea0000305</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rtmFzw4">Health Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">S</biblScope>
			<date type="published" when="1220">2015. 1220</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Klasnja, P., Hekler, E., Shiffman, S., Boruvka, A., Almirall, D., Tewari, A. &amp; Murphy, S. (2015), &apos;Micro-randomized trials: An experimental design for developing just-in-time adaptive interventions.&apos;, Health Psychology 34(S), 1220.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_VGJzjqa">Efficacy of contextually tailored suggestions for physical activity: A microrandomized optimization trial of heartsteps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Seewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Hekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kQhXEYW">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="573" to="582" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Klasnja, P., Smith, S., Seewald, N. J., Lee, A., Hall, K., Luers, B., Hekler, E. B. &amp; Murphy, S. A. (2019), &apos;Efficacy of contextually tailored suggestions for physical activity: A micro- randomized optimization trial of heartsteps&apos;, Annals of Behavioral Medicine 53(6), 573- 582.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_zB4JqR3">Effective behavioral intervention strategies using mobile health applications for chronic disease management: a systematic review</title>
		<author>
			<persName><forename type="first">J.-A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12911-018-0591-0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6XSxE7s">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee, J.-A., Choi, M., Lee, S. A. &amp; Jiang, N. (2018), &apos;Effective behavioral intervention strate- gies using mobile health applications for chronic disease management: a systematic review&apos;, BMC medical informatics and decision making 18(1), 12.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_MeCdEXZ">Just-in-time but not too much: Determining treatment timing in mobile health</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Absi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3287057</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Xj3KcHt">Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">179</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liao, P., Dempsey, W., Sarker, H., Hossain, S. M., al&apos;Absi, M., Klasnja, P. &amp; Murphy, S. (2018), &apos;Just-in-time but not too much: Determining treatment timing in mobile health&apos;, Proceedings of the ACM on interactive, mobile, wearable and ubiquitous tech- nologies 2(4), 179.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_UURMSVH">Micro-randomized trials in mhealth</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasjna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1002/sim.6847</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Um9pQyw">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1944" to="1971" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liao, P., Klasjna, P., Tewari, A. &amp; Murphy, S. (2016), &apos;Micro-randomized trials in mhealth&apos;, Statistics in Medicine 35(12), 1944-71.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_PttS2jJ">Breaking the curse of horizon: Infinite-horizon off-policy estimation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RfgE5WT">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="5356" to="5366" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, Q., Li, L., Tang, Z. &amp; Zhou, D. (2018), Breaking the curse of horizon: Infinite-horizon off-policy estimation, in &apos;Advances in Neural Information Processing Systems&apos;, pp. 5356- 5366.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_KwYDQ3t">Estimating dynamic treatment regimes in mobile health using v-learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Luckett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kahkoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Maahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mayer-Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2018.1537919</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bZEB6gf">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">530</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luckett, D. J., Laber, E. B., Kahkoska, A. R., Maahs, D. M., Mayer-Davis, E. &amp; Kosorok, M. R. (2020), &apos;Estimating dynamic treatment regimes in mobile health using v-learning&apos;, Journal of the American Statistical Association 115(530), 692-706.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_BSRNCdD">Average reward reinforcement learning: Foundations, algorithms, and empirical results</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1018064306595</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gRyMUn3">Machine learning</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="159" to="195" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mahadevan, S. (1996), &apos;Average reward reinforcement learning: Foundations, algorithms, and empirical results&apos;, Machine learning 22(1-3), 159-195.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main" xml:id="_HEUVHaz">A batch, off-policy, actor-critic algorithm for optimizing the average reward</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05047</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Murphy, S. A., Deng, Y., Laber, E. B., Maei, H. R., Sutton, R. S. &amp; Witkiewitz, K. (2016), &apos;A batch, off-policy, actor-critic algorithm for optimizing the average reward&apos;, arXiv preprint arXiv:1607.05047 .</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_KaZFTDq">Marginal mean models for dynamic regimes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P P R</forename><surname>Group</surname></persName>
		</author>
		<idno type="DOI">10.1198/016214501753382327</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9ttUrgz">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">456</biblScope>
			<biblScope unit="page" from="1410" to="1423" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Murphy, S. A., van der Laan, M. J., Robins, J. M. &amp; Group, C. P. P. R. (2001), &apos;Marginal mean models for dynamic regimes&apos;, Journal of the American Statistical Association 96(456), 1410-1423.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_BHxaQSC">Just-in-time adaptive interventions (jitais) in mobile health: key components and design principles for ongoing health behavior support</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12160-016-9830-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6GwwHAS">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="446" to="462" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nahum-Shani, I., Smith, S. N., Spring, B. J., Collins, L. M., Witkiewitz, K., Tewari, A. &amp; Murphy, S. A. (2018), &apos;Just-in-time adaptive interventions (jitais) in mobile health: key components and design principles for ongoing health behavior support&apos;, Annals of Behavioral Medicine 52(6), 446-462.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_PtXpafR">Online regret bounds for undiscounted continuous reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ryabko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NFq5Jea">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page" from="1763" to="1771" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ortner, R. &amp; Ryabko, D. (2012), Online regret bounds for undiscounted continuous reinforce- ment learning, in &apos;Advances in Neural Information Processing Systems&apos;, pp. 1763-1771.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_MkRD3SE">Markov decision processes: Discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470316887</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Puterman, M. L. (1994), &apos;Markov decision processes: Discrete stochastic dynamic program- ming&apos;.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_vYTjaEs">Toward increasing engagement in substance use data collection: development of the substance abuse research assistant app and protocol for a microrandomized trial using adolescents and emerging adults</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Bonar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.2196/resprot.9850</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5Euyv75">JMIR research protocols</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">166</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi, M., Kotov, M. P., Cunningham, R., Bonar, E. E., Nahum-Shani, I., Klasnja, P., Walton, M. &amp; Murphy, S. (2018), &apos;Toward increasing engagement in substance use data collection: development of the substance abuse research assistant app and protocol for a microrandomized trial using adolescents and emerging adults&apos;, JMIR research protocols 7(7), e166.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main" xml:id="_RJj9zv5">Support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Steinwart, I. &amp; Christmann, A. (2008), Support vector machines, Springer Science &amp; Business Media.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main" xml:id="_bT7AW6y">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton, R. S. &amp; Barto, A. G. (2018), Reinforcement learning: An introduction, MIT press.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_QuaU6tY">Data-efficient off-policy policy evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_DJCBKYv">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2139" to="2148" />
		</imprint>
	</monogr>
	<note type="raw_reference">Thomas, P. &amp; Brunskill, E. (2016), Data-efficient off-policy policy evaluation for reinforce- ment learning, in &apos;International Conference on Machine Learning&apos;, pp. 2139-2148.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Van De Geer</surname></persName>
		</author>
		<title level="m" xml:id="_Qj3B92q">Empirical Processes in M-estimation</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Van de Geer, S. (2000), Empirical Processes in M-estimation, Vol. 6, Cambridge university press.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main" xml:id="_5gygjyj">Learning and value function approximation in complex decision processes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
		<idno type="DOI">10.5176/2301-394x_ace17.117</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">Van Roy, B. (1998), Learning and value function approximation in complex decision pro- cesses, PhD thesis, Massachusetts Institute of Technology.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_6FAEBgB">A partially linear framework for massive heterogeneous data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1214/15-aos1410</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7Vg2zRS">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1400</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhao, T., Cheng, G. &amp; Liu, H. (2016), &apos;A partially linear framework for massive heteroge- neous data&apos;, Annals of statistics 44(4), 1400.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
