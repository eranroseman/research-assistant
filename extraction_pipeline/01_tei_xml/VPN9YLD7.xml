<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_eGUP2HE">Medical foundation large language models for comprehensive text analysis and beyond Check for updates</title>
				<funder ref="#_WVpXXm6">
					<orgName type="full">NIH/NCATS</orgName>
				</funder>
				<funder ref="#_TyAyS6B #_edTnEfQ">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000002</idno>
				</funder>
				<funder ref="#_AeVryej">
					<orgName type="full">Patient-Centered Outcomes Research Institute</orgName>
					<orgName type="abbreviated">PCORI</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100006093</idno>
				</funder>
				<funder ref="#_C6zsMkf #_GgspkSM">
					<orgName type="full">CDC</orgName>
				</funder>
				<funder ref="#_uUThj5D #_RQWVbvZ #_nbS53cu #_qUfeSya #_YShcPgn #_M6Kvt5E #_Fb7mgB7 #_u3acXkJ #_PUXwPQZ #_NamxWea #_8Cw2GgE">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qianqian</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aokun</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Health Outcomes and Biomedical Informatics , College of Medicine , University of Florida , Gainesville , FL , USA.</note>
								<orgName type="department" key="dep1">Department of Health Outcomes and Biomedical Informatics</orgName>
								<orgName type="department" key="dep2">College of Medicine</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Health Outcomes and Biomedical Informatics , College of Medicine , University of Florida , Gainesville , FL , USA.</note>
								<orgName type="department" key="dep1">Department of Health Outcomes and Biomedical Informatics</orgName>
								<orgName type="department" key="dep2">College of Medicine</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> School of Biomedical Informatics , University of Texas Health Science , Center at Houston , Houston , TX , USA.</note>
								<orgName type="department">School of Biomedical Informatics</orgName>
								<orgName type="institution" key="instit1">University of Texas Health Science</orgName>
								<orgName type="institution" key="instit2">Center at Houston</orgName>
								<address>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fongci</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xueqing</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jimin</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vipina</forename><surname>Keloth</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lingfei</forename><surname>Qian</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dennis</forename><surname>Shung</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> Department of Medicine (Digestive Diseases) , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Medicine (Digestive Diseases)</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lucila</forename><surname>Ohno-Machado</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Health Outcomes and Biomedical Informatics , College of Medicine , University of Florida , Gainesville , FL , USA.</note>
								<orgName type="department" key="dep1">Department of Health Outcomes and Biomedical Informatics</orgName>
								<orgName type="department" key="dep2">College of Medicine</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Xu</surname></persName>
							<email>hua.xu@yale.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Biomedical Informatics and Data Science , Yale School of Medicine , Yale University , New Haven , CT , USA.</note>
								<orgName type="department" key="dep1">Department of Biomedical Informatics and Data Science</orgName>
								<orgName type="department" key="dep2">Yale School of Medicine</orgName>
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
							<email>bianji@iu.edu</email>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Health Outcomes and Biomedical Informatics , College of Medicine , University of Florida , Gainesville , FL , USA.</note>
								<orgName type="department" key="dep1">Department of Health Outcomes and Biomedical Informatics</orgName>
								<orgName type="department" key="dep2">College of Medicine</orgName>
								<orgName type="institution">University of Florida</orgName>
								<address>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_cD5Hjvz">Medical foundation large language models for comprehensive text analysis and beyond Check for updates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FF3AECDCCA4F3830527008804673A969</idno>
					<idno type="DOI">10.1038/s41746-025-01533-1</idno>
					<note type="submission">Received: 14 November 2024; Accepted: 20 February 2025;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_S7F28a9"><p xml:id="_vBSVn8H"><s xml:id="_R7BaQcN">Recent advancements in large language models (LLMs) show significant potential in medical applications but are hindered by limited specialized medical knowledge.</s><s xml:id="_zBxNFgz">We present Me-LLaMA, a family of open-source medical LLMs integrating extensive domain-specific knowledge with robust instruction-following capabilities.</s><s xml:id="_gkRHHhW">Me-LLaMA is developed through continual pretraining and instruction tuning of LLaMA2 models using diverse biomedical and clinical data sources (e.g., biomedical literature and clinical notes).</s><s xml:id="_uzppXPN">We evaluated Me-LLaMA on six text analysis tasks using 12 benchmarks (e.g., PubMedQA and MIMIC-CXR) and assessed its clinical utility in complex case diagnosis through automatic and human evaluations.</s><s xml:id="_MBkn4Wv">Me-LLaMA outperforms existing open medical LLMs in zero-shot and supervised settings and surpasses ChatGPT and GPT-4 after task-specific instruction tuning for most text analysis tasks.</s><s xml:id="_Aqe9eRR">Its performance is also comparable to ChatGPT and GPT-4 for diagnosing complex clinical cases.</s><s xml:id="_AyQbpWn">Our findings highlight the importance of combining domain-specific continual pretraining with instruction tuning to enhance performance in medical LLMs.</s></p><p xml:id="_CtNTeuH"><s xml:id="_acEu3M9">Large language models (LLMs) have shown great potential in improving medical applications such as clinical documentation, diagnostic accuracy, and patient care management 1-3 .</s><s xml:id="_5RnpVBD">However, general-domain LLMs often lack specialized medical knowledge because they are primarily trained on nonmedical datasets 4 , limiting their effectiveness in healthcare settings.</s><s xml:id="_6MZKsVz">Although commercial LLMs, such as ChatGPT and GPT-4 4 , offer advanced capabilities, their closed-source nature restricts the flexible customization and accessibility required for medical use.</s><s xml:id="_vHqJTpH">This limitation has spurred the research towards developing open-source LLMs such as LLaMA <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> ; Yet these models still fall short due to their general-domain training <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> .</s></p><p xml:id="_BD6CRCw"><s xml:id="_8NABpAD">To address these challenges, researchers have explored strategies to develop domain-specific LLMs for the medical domain.</s><s xml:id="_WqQd4Sy">Instruction finetuning of general-domain models, as seen in MedAlpaca 3 , ChatDoctor 10 , and AlpaCare 11 , attempts to enhance medical capabilities but is limited by the base models' lack of specialized knowledge; instruction fine-tuning alone cannot compensate for this deficiency.</s><s xml:id="_X5veGYw">Training models from scratch using medical corpora, exemplified by GatorTronGPT 8 , overcomes this limitation but demands substantial computational resources and time.</s><s xml:id="_dZQmECR">A more costeffective alternative is continual pretraining, enabling models to acquire specialized medical knowledge while leveraging existing model architectures; notable examples include PMC-LLaMA 2 , Meditron 9 , and Clinical LLaMA 12 .</s></p><p xml:id="_bDT5X9F"><s xml:id="_aT2mP8b">Despite these advances, existing LLMs of continual pretraining in the medical domain exhibit notable limitations: (1) Although both domain knowledge and instruction-following capabilities are crucial, only PMC-LLaMA 2 has combined continual pretraining with instruction fine-tuning, revealing a gap in leveraging the synergy between these two aspects.</s><s xml:id="_GhtcR8X">(2) Only one model (Clinical LLaMA) used clinical notes from electronic health records, which is crucial for real-world clinical applications as it provides context-specific information from direct patient care.</s><s xml:id="_MqurSeG">None of the existing models used both biomedical literature and clinical notes, which is one of the goals of this project.</s><s xml:id="_tR44pqE">(3) Due to the limited medical datasets utilized for model development, these models still lack essential domain knowledge, which hampers their effectiveness.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_pPdthtq"><p xml:id="_pVqpmJp"><s xml:id="_DxsPmEe">clinical notes, we generated the largest biomedical pre-training dataset (129B tokens), compared to the previous efforts (i.e., 79B tokens in PMC-LLaMA as the highest).</s><s xml:id="_M7z6Mk2">( <ref type="formula">4</ref>) Evaluations have predominantly centered on medical question-answering (QA) tasks, lacking comprehensive assessments on the generalizability of those foundation models across diverse medical tasks.</s></p><p xml:id="_dTJ99f4"><s xml:id="_NFdc9Wc">To overcome these limitations, we present Me-LLaMA, a novel family of open-source medical large language models that uniquely integrate extensive domain-specific knowledge with robust instruction-following capabilities.</s><s xml:id="_8SD8TAX">Me-LLaMA comprises foundation models (Me-LLaMA 13B and 70B) and their chat-enhanced versions, developed through comprehensive continual pretraining and instruction tuning of LLaMA2 6 models.</s><s xml:id="_wwqKnh4">Leveraging an extensive medical dataset-combining 129 billion pretraining tokens and 214,000 instruction samples from scientific literature, clinical guidelines, and electronic health record clinical notes-Me-LLaMA excels across a wide spectrum of medical text analysis and real-world clinical tasks.</s><s xml:id="_z9vfpfp">Prior studies <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> have primarily focused on evaluating the QA task.</s><s xml:id="_mYS9STT">For example, PMC-LLaMA <ref type="bibr" target="#b1">2</ref> and Meditron 9 evaluated their model performance on medical QA tasks derived from domain-specific literature, while MedAlpaca <ref type="bibr" target="#b2">3</ref> and ChatDoctor <ref type="bibr" target="#b9">10</ref> focused on conversational QA.</s><s xml:id="_hCr58Kg">In contrast, we conduct a comprehensive evaluation covering six critical tasksquestion answering, relation extraction, named entity recognition, text classification, text summarization, and natural language inference-across twelve datasets from both biomedical and clinical domains.</s><s xml:id="_Fsk4vPJ">Our results demonstrate that Me-LLaMA not only surpasses existing open-source medical LLMs in both zero-shot and supervised settings but also, with taskspecific instruction tuning, outperforms leading commercial LLMs such as ChatGPT on seven out of eight datasets and GPT-4 on five out of eight datasets.</s><s xml:id="_fk99Use">Furthermore, to evaluate Me-LLaMA's potential clinical utility, we assessed the models on complex clinical case diagnosis tasks, comparing their performance with other commercial LLMs using both automatic and human evaluations.</s><s xml:id="_KAdHScD">Our findings indicate that Me-LLaMA's performance is comparable to that of ChatGPT and GPT-4, despite their substantially larger model sizes.</s></p><p xml:id="_KhtjHVZ"><s xml:id="_Zjur9hV">Our findings underscore the importance of combining domainspecific continual pretraining with instruction tuning to develop effective large language models for the medical domain.</s><s xml:id="_yvEdnzU">Recognizing the significant resources required, we have publicly released our Me-LLaMA models on PhysioNet under appropriate Data Use Agreements (DUAs) to lower barriers and foster innovation within the medical AI community.</s><s xml:id="_bRUpC26">Alongside the models, we provide benchmarks and evaluation scripts on GitHub to facilitate further development.</s><s xml:id="_VqUJNyg">We anticipate that these contributions will benefit researchers and practitioners alike, advancing this critical field toward more effective and accessible medical AI applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KbqAeKD">Results</head><p xml:id="_6bTA37T"><s xml:id="_uj9W7aW">Overall performance of medical text analysis Table <ref type="table" target="#tab_0">1</ref> compares the performance of our Me-LLaMA 13/70B foundation models against other open LLMs in the supervised setting.</s><s xml:id="_VkZpwEF">The performance of Meditron 70B on the PubMedQA, MedQA, and MedMCQA datasets is cited from the meditron paper to have a fair comparison.</s><s xml:id="_mU8NYwV">We can observe that the Me-LLaMA 13B model surpassed the similar-sized medical foundation model PMC-LLaMA 13B on 11 out of 12 datasets and outperformed the general foundation model LLaMA2 13B on 10 out of 12 datasets.</s><s xml:id="_ZsReZkr">Moreover, it is noticed that the Me-LLaMA 13B model was competitive with LLaMA2 70B and Meditron 70B, which have significantly larger parameter sizes, on 8 out of 12 datasets.</s><s xml:id="_xxdkXyh">As for 70B models, Me-LLaMA 70B achieved the best performance on 9 out of 12 datasets, when benchmarked against LLaMA2 70B and Meditron 70B.</s></p><p xml:id="_Nnvgsg3"><s xml:id="_dvKNmK2">Table <ref type="table" target="#tab_1">2</ref> shows the zero-shot performance of Me-LLaMA chat models and other instruction-tuned open LLMs with chat ability on various tasks.</s><s xml:id="_7sMrSDQ">Among 13B models, Me-LLaMA 13B-chat outperformed LLaMA2 13B-chat, PMC-LLaMA-chat, Medalpaca 13B in almost all 12 datasets.</s><s xml:id="_j8XU28s">Me-LLaMA outperformed AlpaCare-13B in 9 out of 12 datasets.</s><s xml:id="_Kmdjpe5">Among models with 70B parameters, Me-LLaMA 70B-chat consistently outperformed LLaMA2-70B-chat on 11 out of 12 datasets.</s><s xml:id="_jKkeYbU">It is worth noting that Me-LLaMA13B-chat showed better performance than LLaMA2-70B-chat-a model with a significantly larger parameter size-on 6 out of 12 datasets and was competitive with the LLaMA2-70B-chat in 3 out of 6 remaining datasets.</s></p><p xml:id="_RUEWn4y"><s xml:id="_Knk6cPs">Figure <ref type="figure" target="#fig_0">1</ref> further compares the performance of Me-LLaMA models in the zero-shot and supervised learning setting, against ChatGPT and GPT-4.</s><s xml:id="_bSfkX3j">Due to privacy concerns, which preclude the transmission of clinical datasets with patient information to ChatGPT and GPT-4, we conducted our comparison across 8 datasets that are not subject to these limitations.</s><s xml:id="_379H9gX">The results of ChatGPT and GPT-4 on three QA datasets are referenced from the OpenAI's   <ref type="url" target="https://doi.org/10.1038/s41746-025-01533-1">https://doi.org/10.1038/s41746-025-01533-1</ref></s></p><p xml:id="_5EfH6mF"><s xml:id="_kvVH8ux">and zero-shot learning across a broad spectrum of medical tasks, underscoring its efficiency and potential in the field.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_r5Dd4TN">Performance of complex clinical case diagnosis</head><p xml:id="_ED9QgZS"><s xml:id="_DQMfNjf">Figure <ref type="figure" target="#fig_1">2</ref> shows the top-K (1 ≤ K ≤ 5) accuracy of Me-LLaMA-70B-chat, ChatGPT, GPT-4, and LLaMA2-70B-chat, in the complex clinical case diagnosis task.</s><s xml:id="_pmshacT">We can see Me-LLaMA-70B-chat model achieved comparable performance with GPT-4 and ChatGPT and significantly outperforms LLaMA2-70B-chat.</s><s xml:id="_5UcuhW2">The human evaluation result in Fig. <ref type="figure" target="#fig_2">3</ref> again shows that Me-LLaMA-70B-chat outperformed GPT-4 in both top-1 and top-5 accuracy.</s><s xml:id="_sj96QHH">These results demonstrated the potential of Me-LLaMA models for challenging clinical applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_dmdE8SB">Impact of continual pretraining and instruction tuning</head><p xml:id="_Avaa4EX"><s xml:id="_s5dJ7gX">Table 3 demonstrates the impact of continual pre-training and instruction tuning on zero-shot performance across medical NLP tasks.</s><s xml:id="_MRqEFRw">It clearly demonstrates that both continual pre-training and instruction tuning significantly enhanced the zero-shot capabilities of models.</s><s xml:id="_4tCH9ZD">Instruction tuning alone provides significant performance improvements over the base LLaMA2 models, as seen in LLaMA2 13B, where accuracy on PubMedQA increases from 0.216 to 0.436.</s><s xml:id="_buSVM3c">This suggests that instruction tuning is highly effective in enhancing the model's ability to follow task-specific prompts.</s><s xml:id="_rdDsrjE">In contrast, continual pre-training on medical data yields relatively modest improvements, particularly for smaller models.</s><s xml:id="_7nqb2SY">Me-LLaMA 13B shows only slight gains over LLaMA2 13B, likely due to the smaller scale of domain-specific pre-training data compared to LLaMA2's original training corpus, which exceeds 2 T tokens.</s><s xml:id="_dY6dfB2">Additionally, continual pre-training may not provide as strong of a task-specific signal as instruction tuning, limiting its impact in zero-shot settings.</s><s xml:id="_vBuJSxY">However, for larger models like Me-LLaMA 70B, continual pre-training results in more notable improvements, with performance gains ranging from 2.1% to 55% across various datasets, demonstrating its value in capturing specialized domain knowledge.</s><s xml:id="_T4uTauz">The best results are consistently achieved when both continual pre-training and instruction tuning are applied together, as seen in Me-LLaMA-70B-chat, which outperforms all other configurations.</s><s xml:id="_UKwpHtn">This indicates that while instruction tuning is the most efficient approach for improving task performance, continual pre-training provides a complementary boost, particularly for larger models where additional domain adaptation enhances overall effectiveness.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_srxKDXH">Discussion</head><p xml:id="_7wgdbMq"><s xml:id="_eqUVukf">We introduced a novel medical LLM family including, Me-LLaMA 13B and Me-LLaMA 70B, which encode comprehensive medical knowledge, along with their chat-optimized variants: Me-LLaMA-13/70B-chat, with strong zero-shot learning ability, for medical applications.</s><s xml:id="_rnFseVn">These models were developed through the continual pre-training and instruction tuning of LLaMA2 models, using the largest and most comprehensive biomedical and clinical data.</s><s xml:id="_gbV2HDf">Compared to existing studies, we perform the most comprehensive evaluation, covering six critical text analysis tasks.</s><s xml:id="_wvsDfUG">Our evaluations reveal that Me-LLaMA models outperform existing open-source medical LLMs in various learning scenarios, showing less susceptibility to catastrophic forgetting and achieving competitive results against major commercial models including ChatGPT and GPT-4.</s><s xml:id="_nZFZTtT">Our work paves the way for more accurate, reliable, and comprehensive medical LLMs, and underscores the potential of LLMs on medical applications.</s><s xml:id="_9ft64ZG">Despite these strengths, we observed certain challenges in specific tasks, such as NER and RE <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15</ref> , where even advanced models like GPT-4 exhibited low performance.</s><s xml:id="_e55nPqC">When compared with other NLP tasks with higher performance, we noticed that one of the main reasons for low performance is that LLMs' responses often lacked the conciseness and precision expected, with instances of missing outputs noted.</s><s xml:id="_CMVx2fr">The unexpected outputs also cause significant challenges to automatic evaluation metrics.</s><s xml:id="_uPg2xrU">Therefore, more investigation is needed to further improve medical LLMs' performance across tasks in the zero-shot setting and enhance the automatic assessment of these medical LLMs' zero-shot capabilities.</s><s xml:id="_peFXbZc">For the complex clinical case diagnosis, the Me-LLaMA-chat model had competitive performance and even outperformed GPT-4 in human evaluation.</s><s xml:id="_tyvnqrC">Existing studies have demonstrated GPT-4 is arguably one of the strongest LLMs in this task <ref type="bibr" target="#b15">16</ref> .</s><s xml:id="_tqDY6Mb">The robust performance of Me-LLaMA showed potential in assisting challenging clinical applications.</s><s xml:id="_f4DShcW">It is noticed that variations in test sizes and evaluation methods across different studies contribute to the observed differences in performance between GPT-4 in our paper and other studies.</s><s xml:id="_DT67wQA">We also noted that both the Me-LLaMA-chat model and GPT-4 faced difficulties identifying the correct diagnosis within the top ranks, underscoring the difficulty of this task.</s><s xml:id="_SM5u8uc">Additionally, while the NEJM CPCs offer a rigorous test for these models, they do not encompass the full range of a physician's duties or broader clinical competence.</s><s xml:id="_XNjcxym">Therefore, complex clinical diagnosis remains a challenging area that demands more effective models and improved evaluation benchmarks to better capture the complexities of real-world clinical scenarios.</s></p><p xml:id="_4fubXVs"><s xml:id="_mRQZ4Gq">Our results also emphasize the importance of data diversity during model development.</s><s xml:id="_VuZkpJq">Our empirical results revealed that the PMC-LLaMA 13B model, which employed a data mix ratio of 19:1 between medical and general domain data, exhibited around 2.7% performance drop across both general and biomedical tasks.</s><s xml:id="_KfNPHNN">On the other hand, the Meditron models, 7B,  <ref type="url" target="https://doi.org/10.1038/s41746-025-01533-1">https://doi.org/10.1038/s41746-025-01533-1</ref></s></p><p xml:id="_7pPkMqq"><s xml:id="_x2T48kc">and 70B, with a 99:1 mix ratio, demonstrated improvements in biomedical tasks, yet they still saw around 1% declines in the performance of general tasks.</s><s xml:id="_aCtNaqW">In contrast, our models, which adopt a 4:1 ratio, have shown enhancements in their performance for both general and medical tasks.</s><s xml:id="_7dVbvuJ">This suggests that the integration of general domain data plays a vital role in mitigating the knowledge-forgetting issue during pre-training <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9</ref> .</s><s xml:id="_r38JwaU">However, determining the optimal balance between general domain data and specialized medical data is nontrivial, requiring careful empirical analysis.</s><s xml:id="_N36Nunx">Future studies should examine methods to better determine the optimal ratio.</s></p><p xml:id="_QB2RVg3"><s xml:id="_w3eUUVe">The cost-effectiveness of instruction tuning is another important consideration.</s><s xml:id="_vmYU6M4">Pre-training, exemplified by the LLaMA2 70B model, is notably resource-heavy, requiring about 160*700 GPU hours per epoch.</s><s xml:id="_U8CqhTe">Conversely, instruction tuning is far less resource-demanding, needing roughly 8*70 GPU hours per epoch, making it much more affordable than pre-training.</s><s xml:id="_z45jQfP">While continual pre-training aims to incorporate specialized medical knowledge into the model, the observed performance improvements, particularly for smaller models like Me-LLaMA 13B, are relatively modest.</s><s xml:id="_tftmMCD">This limited improvement can be attributed to several factors.</s><s xml:id="_ZhswrFh">First, the amount of domain-specific pre-training data used is significantly smaller compared to the original pre-training data of LLaMA2, which exceeds 2 T tokens.</s><s xml:id="_m6MEWSg">This discrepancy suggests that larger amounts of domain-specific data may be required to fully activate the model's potential.</s><s xml:id="_6ufTQEH">Second, the continual pre-training strategy itself could be optimized further.</s><s xml:id="_r6jP9cX">As noted earlier, the process faces challenges such as catastrophic forgetting, where the model loses general-domain knowledge during adaptation to specialized data.</s><s xml:id="_EBf7JWZ">Despite this, models trained only with the instruction tuning demonstrate that instruction tuning alone can significantly enhance performance at a fraction of the computational cost.</s><s xml:id="_6t9GRge">This highlights instruction tuning as a practical and cost-effective alternative, particularly in scenarios where computational resources are limited.</s></p><p xml:id="_j73qjPy"><s xml:id="_JmFbamN">The Me-LLaMA models, available in both 13B and 70B sizes, as well as in base and chat-optimized versions, enable a wide array of medical applications, guided by the crucial balance between model size and resource availability.</s><s xml:id="_QHYMVG7">The base models provide a strong foundation for supervised fine-tuning on specialized tasks, while the chat-optimized versions excel in instruction-following and zero-shot scenarios.</s><s xml:id="_jc56FUX">Larger models, like the 70B, deliver superior reasoning capabilities but require significant computational resources, making the 13B models a practical alternative for broader accessibility.</s><s xml:id="_rs3C2Bh">Notably, the Me-LLaMA 13B model achieves performance comparable to its 70B counterpart across most datasets, demonstrating its utility for diverse medical tasks in resource-limited settings.</s><s xml:id="_3xZavMS">These features suggest that Me-LLaMA models could be explored for various medical applications.</s><s xml:id="_x8bttQk">Potential areas of use include: (1) clinical decision support, where these models might assist in analyzing patient records, generating differential diagnoses, and synthesizing medical literature to support evidence-based decision-making; (2) medical education, where chatoptimized versions could serve as interactive tools for teaching medical students and trainees by providing explanations for complex medical topics and simulating diagnostic reasoning; and (3) administrative tasks, where these models may help streamline workflows by summarizing clinical notes and generating discharge summaries, potentially reducing the documentation burden on clinicians.</s><s xml:id="_D4zES6b">Further research and evaluation are warranted to assess Me-LLaMA's real-world effectiveness and limitations in these clinical application settings.</s></p><p xml:id="_qAYfhqk"><s xml:id="_wkCrGAZ">Despite these advancements, it is crucial to acknowledge the current Me-LLaMA models still have certain limitations that require further attention.</s><s xml:id="_3RqBKxx">Like all existing LLMs, they are susceptible to generating information with factual errors or biased information.</s><s xml:id="_RKuRwsd">To mitigate this, future studies could incorporate methodologies like reinforcement learning from human feedback (RLHF) <ref type="bibr" target="#b16">17</ref> .</s><s xml:id="_FnsccJb">This approach could align the models' responses more closely with human values and ensure they are grounded in factual medical knowledge.</s><s xml:id="_AdrxYDU">Another limitation is the current token handling capacity, capped at 4096 tokens, which is a constraint inherited from the backbone LLaMA2 model.</s><s xml:id="_DPrhbQ9">Addressing this limitation could involve extending the models' capability to handle longer contexts.</s><s xml:id="_RWXsb6T">This could be achieved by integrating advanced attention techniques, such as sparse local attention <ref type="bibr" target="#b17">18</ref> , that are able to handle extensive contexts.</s></p><p xml:id="_xvWNrvc"><s xml:id="_hGfHybq">Additionally, while MIMIC is the largest publicly available EHR dataset, its size is still relatively small compared to other data sources, which Haven Health and the University of Florida Health.</s><s xml:id="_XpeypbG">However, the terms of distribution and dissemination for models trained on such proprietary data will need to be carefully negotiated with our institutions' data governance committees to ensure the safety and confidentiality of clinical data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RKB9Jqh">Methods</head><p xml:id="_d8mZ6eY"><s xml:id="_hMNQQc9">We utilized LLaMA2 6 as the backbone model and developed Me-LLaMA through the process of continual pre-training and instruction tuning of LLaMA2, using 129B tokens and 214 K instruction tuning samples from general, biomedical, and clinical domains.</s><s xml:id="_yrWgmdx">Figure <ref type="figure">4</ref> shows an overview of our study.</s></p><p xml:id="_dhBftsU"><s xml:id="_sudMztg">Table 4 presents the comparison of Me-LLaMA models and existing open source medical LLMs.</s><s xml:id="_UgECKj6">Continual pre-training data To effectively adapt backbone LLaMA2 models for the medical domain through continual pre-training, we developed a mixed continual pretraining dataset, comprised of biomedical literature, clinical notes, and general domain data.</s><s xml:id="_XsXay3W">Our dataset integrates a vast collection of biomedical literature from PubMed Central and PubMed Abstracts, sourced from the Pile dataset 19 .</s><s xml:id="_hDkY6Rd">The PubMed Central subset includes 3,098,931 biomedical articles, and the PubMed Abstracts section encompasses abstracts from 15,518,009 documents.</s><s xml:id="_Zjvzvtr">This comprehensive biomedical dataset provides a rich source of medical knowledge and research findings.</s><s xml:id="_YQVjtfa">To incorporate Fig. 4 | Overview of the study.</s><s xml:id="_HrKWKVX">Our study has three main components including pre-training, instruction fine-tuning and evaluation.</s><s xml:id="_3NZ5BXR">Pre-training: we first developed the Me-LLaMA base models by continual pre-training LLaMA2 with 129 billion tokens from mixed pre-training text data.</s><s xml:id="_ZPaKtcG">Instruction fine-tuning: Me-LLaMA-chat models were further developed by instruction-tuning Me-LLaMA base models with 214 K instructions.</s><s xml:id="_yBxbvfa">Evaluation: Finally, we evaluated the Me-LLaMA base models in a supervised learning setting across six text analysis tasks, and the Me-LLaMA-chat models in a zero-shot setting on both text analysis tasks and a clinical diagnosis task.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_aK45sa6">Medical instruction tuning data</head><p xml:id="_NN4MVMt"><s xml:id="_tFVCVBH">To enhance our model's ability to follow instructions and generalize across diverse medical tasks, we further developed a novel medical instruction tuning dataset with 214,595 high-quality samples from a wide array of data sources.</s><s xml:id="_ZyV9rYT">This dataset stands out from those used in existing medical LLMs due to its comprehensive coverage of both biomedical and clinical domains.</s></p><p xml:id="_p6t2uj9"><s xml:id="_B55yhm9">Our data sources included biomedical literature, clinical notes, clinical guidelines, wikidoc, knowledge graphs, and general domain data, as shown in Table <ref type="table" target="#tab_6">5</ref>.</s><s xml:id="_cyb3A5v">The diverse tasks aim to refine the model's ability to process and respond to medical information accurately and contextually.</s><s xml:id="_Fyw38mg">Detailed prompts for each data and the data example are shown in the Supplementary Information, Supplementary Table <ref type="table" target="#tab_0">1</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2bpTWm3">Training details</head><p xml:id="_8uYBKfc"><s xml:id="_6QaFnkF">As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, we developed the Me-LLaMA 13B and 70B base models by continually pre-training the LLaMA2 13B and 70B models.</s><s xml:id="_U5C52Y9">These base models were then instruction-tuned to create the Me-LLaMA-13B-chat and Me-LLaMA-70B-chat models.</s></p><p xml:id="_BusPaPe"><s xml:id="_rBRN72S">The first phase aims to develop Me-LLaMA base models, and adapt LLaMA2 models to better understand and generate text relevant to the medical context using the pre-training datasets we constructed.</s><s xml:id="_MsTzR5M">The objective is to enhance the model's ability to understand and generate domain-specific text by optimizing it to predict the next word in a sequence based on the preceding context.</s><s xml:id="_ptSVkqk">This training was executed on the University of Florida's HiPerGator AI supercomputer with 160 A100 80GB GPUs.</s><s xml:id="_HKCMT9P">We employed the AdamW optimizer with hyperparameters set to β 1 to 0.9 and β 2 to 0.95, alongside a weight decay of 0.00001 and a learning rate of 8e-6.</s><s xml:id="_2pufXc7">We used a cosine learning rate scheduler with a 0.05 warmup ratio for gradual adaptation to training complexity and bf16 precision for computational efficiency.</s><s xml:id="_YEpea37">Gradient accumulation was set to 16 steps, and training was limited to one epoch.</s><s xml:id="_uk3cGbh">We utilized DeepSpeed 24 for model parallelism.</s></p><p xml:id="_rgwfYqz"><s xml:id="_gD23EE3">We further fine-tuned Me-LLaMA base models to develop Me-LLaMA chat models, using the developed 214k instruction samples.</s><s xml:id="_pQVybWS">In this phase, the models are trained to produce accurate and contextually appropriate responses to specific input instructions.</s><s xml:id="_dBz2Tz3">Executed using 8 A100 GPUs, the fine-tuning process was set to run for 3 epochs with a learning rate of 1e-5.</s><s xml:id="_CxnzpTZ">We used a weight decay of 0.00001 and a warmup ratio of 0.01 for regularization and gradual learning rate increase.</s><s xml:id="_PMBjnC3">We utilized LoRAbased <ref type="bibr" target="#b24">25</ref> parameter-efficient fine-tuning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AwurG39">Evaluation benchmark</head><p xml:id="_d3B7ubg"><s xml:id="_vJukWxX">Existing studies <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9</ref> in the medical domain have primarily focused on evaluating the QA task.</s><s xml:id="_CBDrAnU">In this study, we build an extensive medical evaluation benchmark (MIBE), encompassing six critical text analysis tasks: QA, NER, RE, Text Classification, Text Summarization and NLI.</s><s xml:id="_kD9NCX9">These tasks collectively involve 12 datasets meticulously sourced from biomedical, and clinical domains as shown in Table <ref type="table" target="#tab_7">6</ref>.</s></p><p xml:id="_kkVMXEz"><s xml:id="_UxY3Add">We further assessed the effectiveness of Me-LLaMA in diagnosing complex clinical cases, a critical task given the increasing burden of diseases and the need for timely and accurate diagnosis to support clinicians.</s><s xml:id="_YkzJSbW">Recent studies demonstrate that LLMs have the potential to address this challenge <ref type="bibr" target="#b25">26</ref> .</s><s xml:id="_8sZ7Whc">Specifically, we evaluated the diagnostic accuracy of Me-LLaMA on 70 challenging medical cases from the New England Journal of Medicine clinicopathologic conferences (NEJM CPCs) published between January 2021 and December 2022, as collected from an existing study <ref type="bibr" target="#b26">27</ref> .</s><s xml:id="_SkJFxgg">The NEJM CPCs are well-known for their unique and intricate clinical cases, which have long been used as benchmarks for evaluating challenging medical scenarios.</s><s xml:id="_uJgK8xX">In line with previous research <ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27</ref> , we employed automatic evaluations based on top-K (where k = 1,2,3,4,5) accuracy, defined as the percentage of cases where the correct diagnosis appeared within the top-K positions of the differential diagnosis list predicted by the assessed models.</s><s xml:id="_rNYDqQv">We utilized GPT-4o, a state-of-the-art (SOTA) LLM, to automatically assess</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 |</head><label>1</label><figDesc><div><p xml:id="_k7SyKGH"><s xml:id="_jsZarEd">Fig. 1 | Performance comparison of Me-LLaMA models with ChatGPT and GPT-4.</s><s xml:id="_uBTya6p">The figure presents the zero-shot performance of Me-LLaMA (Me-LLaMA zero-shot) alongside its supervised learning performance (Me-LLaMA task-specific), compared against the zero-shot performance of ChatGPT and GPT-4 across 8 datasets.</s></p></div></figDesc><graphic coords="3,90.48,360.28,419.98,281.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 |</head><label>2</label><figDesc><div><p xml:id="_sYTfTEw"><s xml:id="_JdPc5MC">Fig. 2 | Model performance in the complex clinical case diagnosis task under automatic evaluation.</s><s xml:id="_843VXXn">The figure presents the top-K accuracy (where 1 ≤ K ≤ 5) of Me-LLaMA-70B-chat, ChatGPT, GPT-4, and LLaMA2-70B-chat on a complex clinical case diagnosis task, evaluated automatically.</s></p></div></figDesc><graphic coords="4,47.23,49.72,239.98,176.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 |</head><label>3</label><figDesc><div><p xml:id="_ezJVxeK"><s xml:id="_Ge46xzR">Fig. 3 | Model performance in the complex clinical case diagnosis task under human evaluation.</s><s xml:id="_G5kF8vE">The figure shows the top-1 and top-5 accuracy of Me-LLaMA/-70B-chat and GPT-4 in a complex clinical case diagnosis task, evaluated through human assessment.</s></p></div></figDesc><graphic coords="4,319.69,49.72,227.96,167.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="6,237.26,47.79,324.00,349.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 |</head><label>1</label><figDesc><div><p xml:id="_4QdXtpx"><s xml:id="_99rekVW">The supervised fine-tuning performance of various open source LLMs on six tasks</s></p></div></figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Metric</cell><cell cols="6">LLaMA2 13B PMC-LLaMA 13B Me-LLaMA 13B LLaMA2 70B Meditron 70B Me-LLaMA 70B</cell></row><row><cell>Question answering</cell><cell>PubMedQA</cell><cell>Acc</cell><cell>0.800</cell><cell>0.778</cell><cell>0.802</cell><cell>0.800</cell><cell>0.800</cell><cell>0.814</cell></row><row><cell></cell><cell></cell><cell cols="2">Macro-F1 0.560</cell><cell>0.544</cell><cell>0.562</cell><cell>0.560</cell><cell>-</cell><cell>0.572</cell></row><row><cell></cell><cell>MedQA</cell><cell>Acc</cell><cell>0.467</cell><cell>0.456</cell><cell>0.493</cell><cell>0.598</cell><cell>0.607</cell><cell>0.623</cell></row><row><cell></cell><cell></cell><cell cols="2">Macro-F1 0.465</cell><cell>0.454</cell><cell>0.487</cell><cell>0.595</cell><cell>-</cell><cell>0.621</cell></row><row><cell></cell><cell>MedMCQA</cell><cell>Acc</cell><cell>0.527</cell><cell>0.548</cell><cell>0.557</cell><cell>0.626</cell><cell>0.651</cell><cell>0.643</cell></row><row><cell></cell><cell></cell><cell cols="2">Macro-F1 0.524</cell><cell>0.545</cell><cell>0.551</cell><cell>0.625</cell><cell>-</cell><cell>0.640</cell></row><row><cell></cell><cell>EmrQA</cell><cell>Acc</cell><cell>0.789</cell><cell>0.810</cell><cell>0.857</cell><cell>0.847</cell><cell>0.850</cell><cell>0.854</cell></row><row><cell></cell><cell></cell><cell>F1</cell><cell>0.730</cell><cell>0.738</cell><cell>0.751</cell><cell>0.751</cell><cell>0.751</cell><cell>0.751</cell></row><row><cell>Named entity recognition</cell><cell>i2b2</cell><cell cols="2">Macro-F1 0.904</cell><cell>0.901</cell><cell>0.906</cell><cell>0.913</cell><cell>0.908</cell><cell>0.910</cell></row><row><cell>Relation extraction</cell><cell>DDI</cell><cell cols="2">Macro-F1 0.622</cell><cell>0.622</cell><cell>0.559</cell><cell>0.746</cell><cell>0.737</cell><cell>0.779</cell></row><row><cell>Classification</cell><cell>HoC</cell><cell cols="2">Macro-F1 0.696</cell><cell>0.422</cell><cell>0.684</cell><cell>0.818</cell><cell>0.702</cell><cell>0.841</cell></row><row><cell></cell><cell>MTsample</cell><cell cols="2">Macro-F1 0.430</cell><cell>0.345</cell><cell>0.451</cell><cell>0.458</cell><cell>0.284</cell><cell>0.544</cell></row><row><cell>Summarization</cell><cell>PubMed</cell><cell>R-L</cell><cell>0.191</cell><cell>0.091</cell><cell>0.197</cell><cell>0.211</cell><cell>0.197</cell><cell>0.209</cell></row><row><cell></cell><cell></cell><cell>BERTS</cell><cell>0.663</cell><cell>0.516</cell><cell>0.679</cell><cell>0.689</cell><cell>0.677</cell><cell>0.700</cell></row><row><cell></cell><cell cols="2">MIMIC-CXR R-L</cell><cell>0.437</cell><cell>0.139</cell><cell>0.453</cell><cell>0.440</cell><cell>0.458</cell><cell>0.476</cell></row><row><cell></cell><cell></cell><cell>BERTS</cell><cell>0.816</cell><cell>0.694</cell><cell>0.821</cell><cell>0.813</cell><cell>0.824</cell><cell>0.828</cell></row><row><cell cols="2">Natural language inference BioNLI</cell><cell cols="2">Macro-F1 0.409</cell><cell>0.332</cell><cell>0.447</cell><cell>0.447</cell><cell>0.444</cell><cell>0.566</cell></row><row><cell></cell><cell>MedNLI</cell><cell cols="2">Macro-F1 0.881</cell><cell>0.868</cell><cell>0.903</cell><cell>0.884</cell><cell>0.897</cell><cell>0.916</cell></row></table><note xml:id="_cSVTryQ"><p>paper<ref type="bibr" target="#b0">1</ref> </p><p>. We compared the Rouge-1<ref type="bibr" target="#b12">13</ref> </p><p><s xml:id="_kZ6qRG2">score for the summarization dataset PubMed, the accuracy score for three QA datasets, and the Macro-F1 score for the remaining datasets.</s><s xml:id="_9JhXP3U">With task-specific supervised fine-tuning, Me-LLaMA models surpassed ChatGPT on 7 out of 8 datasets and excelled GPT-4 on 5 out of 8 datasets. In</s><s xml:id="_CXDpqS7">the zero-shot setting, Me-LLaMA models outperformed ChatGPT on 5 datasets; but it fell short on 7 datasets, when compared with GPT-4.</s><s xml:id="_Dn6cxRR">It's crucial to highlight that Me-LLaMA's model size is significantly smaller-13/70B parameters versus at least 175B for ChatGPT and GPT-4.</s><s xml:id="_Wd9qYub">Despite this size discrepancy, Me-LLaMA models have showcased an impressive performance and a strong ability for supervised learning</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 |</head><label>2</label><figDesc><div><p xml:id="_HXphMsB"><s xml:id="_gdD3cQe">The zero-shot performance of various open source LLMs with chat capability</s></p></div></figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Metric</cell><cell>LLaMA2-</cell><cell>PMC-</cell><cell cols="3">Medalpaca-13B AlpaCare-13B Me-LLaMA</cell><cell>LLaMA2-</cell><cell>Me-LLaMA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>13B-chat</cell><cell>LLaMA-</cell><cell></cell><cell></cell><cell>13B-chat</cell><cell>70B-chat</cell><cell>70B-chat</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>chat</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Question answering PubMedQA</cell><cell>Accuracy</cell><cell>0.546</cell><cell>0.504</cell><cell>0.238</cell><cell>0.538</cell><cell>0.700</cell><cell>0.668</cell><cell>0.768</cell></row><row><cell></cell><cell></cell><cell cols="2">Macro-F1 0.457</cell><cell>0.305</cell><cell>0.192</cell><cell>0.373</cell><cell>0.504</cell><cell>0.477</cell><cell>0.557</cell></row><row><cell></cell><cell>MedQA</cell><cell>Accuracy</cell><cell>0.097</cell><cell>0.207</cell><cell>0.143</cell><cell>0.304</cell><cell>0.427</cell><cell>0.376</cell><cell>0.523</cell></row><row><cell></cell><cell></cell><cell cols="2">Macro-F1 0.148</cell><cell>0.158</cell><cell>0.102</cell><cell>0.281</cell><cell>0.422</cell><cell>0.367</cell><cell>0.521</cell></row><row><cell></cell><cell>MedMCQA</cell><cell>Accuracy</cell><cell>0.321</cell><cell>0.212</cell><cell>0.205</cell><cell>0.385</cell><cell>0.449</cell><cell>0.339</cell><cell>0.539</cell></row><row><cell></cell><cell></cell><cell cols="2">Macro-F1 0.243</cell><cell>0.216</cell><cell>0.164</cell><cell>0.358</cell><cell>0.440</cell><cell>0.273</cell><cell>0.538</cell></row><row><cell></cell><cell>EmrQA</cell><cell>Accuracy</cell><cell>0.001</cell><cell>0.053</cell><cell>0.000</cell><cell>0.001</cell><cell>0.048</cell><cell>0.050</cell><cell>0.119</cell></row><row><cell></cell><cell></cell><cell>F1</cell><cell>0.098</cell><cell>0.304</cell><cell>0.040</cell><cell>0.198</cell><cell>0.307</cell><cell>0.251</cell><cell>0.346</cell></row><row><cell>Named entity</cell><cell>i2b2</cell><cell cols="2">Macro-F1 0.143</cell><cell>0.091</cell><cell>0.000</cell><cell>0.173</cell><cell>0.166</cell><cell>0.321</cell><cell>0.329</cell></row><row><cell>recognition</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relation extraction</cell><cell>DDI</cell><cell cols="2">Macro-F1 0.090</cell><cell>0.147</cell><cell>0.058</cell><cell>0.110</cell><cell>0.214</cell><cell>0.087</cell><cell>0.283</cell></row><row><cell>Classification</cell><cell>HoC</cell><cell cols="2">Macro-F1 0.228</cell><cell>0.184</cell><cell>0.246</cell><cell>0.267</cell><cell>0.335</cell><cell>0.309</cell><cell>0.544</cell></row><row><cell></cell><cell>MTsample</cell><cell cols="2">Macro-F1 0.133</cell><cell>0.083</cell><cell>0.003</cell><cell>0.273</cell><cell>0.229</cell><cell>0.254</cell><cell>0.384</cell></row><row><cell>Summarization</cell><cell>PubMed</cell><cell>Rouge-L</cell><cell>0.161</cell><cell>0.028</cell><cell>0.014</cell><cell>0.167</cell><cell>0.116</cell><cell>0.192</cell><cell>0.169</cell></row><row><cell></cell><cell></cell><cell>BERTS</cell><cell>0.671</cell><cell>0.128</cell><cell>0.117</cell><cell>0.671</cell><cell>0.445</cell><cell>0.684</cell><cell>0.678</cell></row><row><cell></cell><cell cols="2">MIMIC-CXR Rouge-L</cell><cell>0.144</cell><cell>0.139</cell><cell>0.010</cell><cell>0.134</cell><cell>0.400</cell><cell>0.131</cell><cell>0.418</cell></row><row><cell></cell><cell></cell><cell>BERTS</cell><cell>0.704</cell><cell>0.694</cell><cell>0.502</cell><cell>0.702</cell><cell>0.797</cell><cell>0.696</cell><cell>0.787</cell></row><row><cell>Natural language</cell><cell>BioNLI</cell><cell cols="2">Macro-F1 0.173</cell><cell>0.159</cell><cell>0.164</cell><cell>0.170</cell><cell>0.195</cell><cell>0.297</cell><cell>0.436</cell></row><row><cell>inference</cell><cell>MedNLI</cell><cell cols="2">Macro-F1 0.412</cell><cell>0.175</cell><cell>0.175</cell><cell>0.275</cell><cell>0.472</cell><cell>0.515</cell><cell>0.675</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 |</head><label>3</label><figDesc><div><p xml:id="_QMqbAPh"><s xml:id="_5sb9dKx">The comparison of zero-shot performances among Me-LLaMA models and their backbone models LLaMA2</s></p></div></figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>LLaMA2 13B</cell><cell>Me-LLaMA</cell><cell>LLaMA2 13B-</cell><cell>Me-LLaMA-</cell><cell>LLaMA2 70B</cell><cell>Me-LLaMA</cell><cell>LLaMA2 70B-</cell><cell>Me-LLaMA-</cell></row><row><cell></cell><cell></cell><cell>(backbone)</cell><cell>13B</cell><cell>instruct</cell><cell>13B-chat</cell><cell>(backbone)</cell><cell>70B</cell><cell>instruct</cell><cell>70B-chat</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(backbone +</cell><cell>(backbone +</cell><cell>(backbone +</cell><cell></cell><cell>(backbone +</cell><cell>(backbone +</cell><cell>(backbone +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>pre-</cell><cell>instruction</cell><cell>pre-train +</cell><cell></cell><cell>pre-</cell><cell>instruction</cell><cell>pre-train +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>train only)</cell><cell>tuning only)</cell><cell>instruction</cell><cell></cell><cell>train only)</cell><cell>tuning only)</cell><cell>instruction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tuning)</cell><cell></cell><cell></cell><cell></cell><cell>tuning)</cell></row><row><cell>PubMedQA</cell><cell>Acc</cell><cell>0.216</cell><cell>0.266</cell><cell>0.436</cell><cell>0.700</cell><cell>0.132</cell><cell>0.682</cell><cell>0.764</cell><cell>0.768</cell></row><row><cell></cell><cell cols="2">Macro-F1 0.177</cell><cell>0.250</cell><cell>0.416</cell><cell>0.504</cell><cell>0.152</cell><cell>0.520</cell><cell>0.531</cell><cell>0.557</cell></row><row><cell>MedQA</cell><cell>Acc</cell><cell>0.000</cell><cell>0.000</cell><cell>0.013</cell><cell>0.427</cell><cell>0.005</cell><cell>0.281</cell><cell>0.499</cell><cell>0.523</cell></row><row><cell></cell><cell cols="2">Macro-F1 0.000</cell><cell>0.000</cell><cell>0.024</cell><cell>0.422</cell><cell>0.009</cell><cell>0.350</cell><cell>0.493</cell><cell>0.521</cell></row><row><cell>MedMCQA</cell><cell>Acc</cell><cell>0.003</cell><cell>0.003</cell><cell>0.014</cell><cell>0.449</cell><cell>0.012</cell><cell>0.447</cell><cell>0.501</cell><cell>0.539</cell></row><row><cell></cell><cell cols="2">Macro-F1 0.006</cell><cell>0.005</cell><cell>0.029</cell><cell>0.440</cell><cell>0.024</cell><cell>0.396</cell><cell>0.493</cell><cell>0.538</cell></row><row><cell>EmrQA</cell><cell>Acc</cell><cell>0.000</cell><cell>0.005</cell><cell>0.050</cell><cell>0.048</cell><cell>0.000</cell><cell>0.021</cell><cell>0.181</cell><cell>0.119</cell></row><row><cell></cell><cell>F1</cell><cell>0.038</cell><cell>0.122</cell><cell>0.286</cell><cell>0.307</cell><cell>0.000</cell><cell>0.172</cell><cell>0.399</cell><cell>0.346</cell></row><row><cell>i2b2</cell><cell cols="2">Macro-F1 0.008</cell><cell>0.030</cell><cell>0.232</cell><cell>0.263</cell><cell>0.181</cell><cell>0.224</cell><cell>0.245</cell><cell>0.329</cell></row><row><cell>DDI</cell><cell cols="2">Macro-F1 0.035</cell><cell>0.036</cell><cell>0.164</cell><cell>0.214</cell><cell>0.034</cell><cell>0.118</cell><cell>0.121</cell><cell>0.283</cell></row><row><cell>HoC</cell><cell cols="2">Macro-F1 0.253</cell><cell>0.210</cell><cell>0.194</cell><cell>0.335</cell><cell>0.255</cell><cell>0.252</cell><cell>0.563</cell><cell>0.544</cell></row><row><cell>MTsample</cell><cell cols="2">Macro-F1 0.042</cell><cell>0.072</cell><cell>0.176</cell><cell>0.229</cell><cell>0.066</cell><cell>0.226</cell><cell>0.364</cell><cell>0.384</cell></row><row><cell>PubMed</cell><cell>R-L</cell><cell>0.170</cell><cell>0.168</cell><cell>0.183</cell><cell>0.116</cell><cell>0.167</cell><cell>0.119</cell><cell>0.112</cell><cell>0.169</cell></row><row><cell></cell><cell>BERTS</cell><cell>0.654</cell><cell>0.654</cell><cell>0.667</cell><cell>0.445</cell><cell>0.654</cell><cell>0.654</cell><cell>0.601</cell><cell>0.678</cell></row><row><cell cols="2">MIMIC-CXR R-L</cell><cell>0.051</cell><cell>0.172</cell><cell>0.360</cell><cell>0.400</cell><cell>0.059</cell><cell>0.137</cell><cell>0.367</cell><cell>0.418</cell></row><row><cell></cell><cell>BERTS</cell><cell>0.566</cell><cell>0.697</cell><cell>0.791</cell><cell>0.797</cell><cell>0.577</cell><cell>0.649</cell><cell>0.784</cell><cell>0.787</cell></row><row><cell>BioNLI</cell><cell cols="2">Macro-F1 0.109</cell><cell>0.060</cell><cell>0.185</cell><cell>0.195</cell><cell>0.285</cell><cell>0.499</cell><cell>0.345</cell><cell>0.436</cell></row><row><cell>MedNLI</cell><cell cols="2">Macro-F1 0.172</cell><cell>0.206</cell><cell>0.457</cell><cell>0.472</cell><cell>0.265</cell><cell>0.256</cell><cell>0.657</cell><cell>0.675</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 |</head><label>4</label><figDesc><div><p xml:id="_TWHF8ak"><s xml:id="_tKAJzFz">The comparison of Me-LLaMA models and existing open source medical LLMs</s></p></div></figDesc><table><row><cell>Model</cell><cell cols="3">Backbone Model size Biomedical</cell><cell>Clinical</cell><cell>Continual pre-</cell><cell>Instruction tuning</cell><cell>Evaluation tasks</cell><cell>Release</cell></row><row><cell></cell><cell></cell><cell></cell><cell>literature</cell><cell>notes</cell><cell>training (# of tokens)</cell><cell>(# of instructions)</cell><cell></cell><cell>date</cell></row><row><cell>MedAlpaca 3</cell><cell>LLaMA</cell><cell>7/13B</cell><cell>✓</cell><cell>✗</cell><cell>-</cell><cell>160 K</cell><cell>QA</cell><cell>04/14/2023</cell></row><row><cell>ChatDoctor 12</cell><cell>LLaMA2</cell><cell>7B</cell><cell>✓</cell><cell>✗</cell><cell>-</cell><cell>100 K</cell><cell>QA</cell><cell>05/24/2023</cell></row><row><cell>AlpaCare 28</cell><cell>LLaMA</cell><cell>7/13B</cell><cell>✓</cell><cell>✗</cell><cell>-</cell><cell>52 K</cell><cell>QA, Summarization</cell><cell>10/23/2023</cell></row><row><cell cols="2">Clinical LLaMA 11 LLaMA</cell><cell>7B</cell><cell>✗</cell><cell>✓</cell><cell>-</cell><cell>-</cell><cell>Classification</cell><cell>07/06/2023</cell></row><row><cell>Meditron 10</cell><cell>LLaMA2</cell><cell>7/70B</cell><cell>✓</cell><cell>✗</cell><cell>48B</cell><cell>-</cell><cell>QA</cell><cell>11/27/2023</cell></row><row><cell>PMC-LLaMA 2</cell><cell>LLaMA</cell><cell>7/13B</cell><cell>✓</cell><cell>✗</cell><cell>79B</cell><cell>514 K</cell><cell>QA</cell><cell>04/27/2023</cell></row><row><cell>Me-LLaMA</cell><cell>LLaMA2</cell><cell>13/70B</cell><cell>✓</cell><cell>✓</cell><cell>129B</cell><cell>214 K</cell><cell>QA, NER, RE, Classification,</cell><cell>06/05/2024</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Summarization, NLI,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Medical Diagnosis</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 |</head><label>5</label><figDesc><div><p xml:id="_jNyFs2r"><s xml:id="_z8Zyswe">The overall instruction tuning dataset</s></p></div></figDesc><table><row><cell>Task</cell><cell>Type</cell><cell>Source</cell><cell>Size</cell><cell>Copy right</cell></row><row><cell>General</cell><cell>Conversation</cell><cell>Alpaca 29</cell><cell>20,000</cell><cell>CC-BY-NC 4.0</cell></row><row><cell></cell><cell></cell><cell>Dolly 30</cell><cell></cell><cell>CC-BY-SA-3.0</cell></row><row><cell></cell><cell></cell><cell>ShareGPT 31</cell><cell></cell><cell>Apache-2.0</cell></row><row><cell>Biomedical</cell><cell>Conversation</cell><cell>HealthCareMagic 10</cell><cell>20,000</cell><cell>Reserved by HealthCareMagic and Icliniq</cell></row><row><cell></cell><cell></cell><cell>Icliniq 10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Instructions</cell><cell>MedInstruct 11</cell><cell>52,000</cell><cell>CC BY-NC 4.0</cell></row><row><cell></cell><cell>Question Answering</cell><cell>Medical Flash Cards 3</cell><cell>34,000</cell><cell>No commercialized use</cell></row><row><cell></cell><cell></cell><cell>MEDIQA 32</cell><cell>2,220</cell><cell>CC BY 4.0</cell></row><row><cell></cell><cell></cell><cell>MedicationQA 33</cell><cell>690</cell><cell>CC BY 4.0</cell></row><row><cell></cell><cell></cell><cell>LiveQA 34</cell><cell>634</cell><cell>CC BY 4.0</cell></row><row><cell></cell><cell></cell><cell>WikiDocPatient 3</cell><cell>5490</cell><cell>CC BY-SA 4.0</cell></row><row><cell></cell><cell></cell><cell>GuidelineQA</cell><cell>2000</cell><cell>Common Crawl (other)</cell></row><row><cell></cell><cell>Summarization</cell><cell>PubMed Central</cell><cell>10,000</cell><cell>CC BY</cell></row><row><cell></cell><cell>Next Sentence Generation</cell><cell>PubMed Central</cell><cell>20,000</cell><cell>CC BY</cell></row><row><cell></cell><cell>Key words prediction</cell><cell>PubMed Central</cell><cell>10,000</cell><cell>CC BY</cell></row><row><cell></cell><cell>Causal Relation Detection</cell><cell>PubMed 35</cell><cell>2450</cell><cell>CC BY</cell></row><row><cell></cell><cell>Relation Extraction</cell><cell>UMLS knowledge graph 2</cell><cell>10,000</cell><cell>Openrail</cell></row><row><cell>Clinical</cell><cell>QA, summarization, classification, mortality prediction</cell><cell>MIMIC-III 20 , MIMIC-IV 21</cell><cell>30,000</cell><cell>PhysioNet credentialed health data use agreement 1.5.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 |</head><label>6</label><figDesc><div><p xml:id="_uedUMtD"><s xml:id="_yeS8GGW">Details of data splits and evaluation metrics of each dataset in the evaluation benchmark</s></p></div></figDesc><table><row><cell>Data</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_mVEdjdt"><s xml:id="_uqu2YDR">BERTS means BERTScore<ref type="bibr" target="#b27">28</ref> .</s><s xml:id="_Fjw67gy">https://doi.org/10.1038/s41746-025-01533-1</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_2R3aFZP"><s xml:id="_pSSmSMz">npj Digital Medicine | (2025) 8:141</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_KNZqfrB"><s xml:id="_FxHqBUB">© The Author(s) 2025 https://doi.org/10.1038/s41746-025-01533-1</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_Uj3reag">Acknowledgements</head><p xml:id="_t3jW8ts"><s xml:id="_yMyUfP7">This work received support from the <rs type="funder">National Institutes of Health (NIH)</rs> under grant numbers: <rs type="grantNumber">1RF1AG072799</rs>, <rs type="grantNumber">1R01AG078154</rs>, <rs type="grantNumber">R01AG073435</rs>, <rs type="grantNumber">R01LM013519</rs>, <rs type="grantNumber">RF1AG084178</rs>, <rs type="grantNumber">R01AG083039</rs>, <rs type="grantNumber">R01CA284646</rs>, <rs type="grantNumber">R01AI172875</rs>, <rs type="grantNumber">R01AG080991</rs>, <rs type="grantNumber">R01AG080624</rs>, <rs type="grantNumber">R01AG080429</rs>, <rs type="grantNumber">1K99LM01402</rs>, <rs type="grantNumber">1K99LM014614-01</rs>, <rs type="funder">NIH/NCATS</rs> <rs type="grantNumber">UL1 TR001427</rs>, <rs type="funder">CDC</rs> <rs type="grantNumber">U18 DP006512</rs>, and <rs type="funder">Patient-Centered Outcomes Research Institute (PCORI)</rs> under grant numbers: <rs type="grantNumber">PCORI RI-FLORIDA-01-PS1</rs>, <rs type="grantNumber">PCORI ME-2018C3-14754</rs>.</s><s xml:id="_GhP39kn">We express our sincere appreciation to the creators of datasets such as the MIMIC, the Pile, and <rs type="institution">RedPajama</rs> for making these valuable resources available to the research community.</s><s xml:id="_yDTWNVM">We extend our gratitude to the <rs type="institution">UF Research Computing team</rs>, under the leadership of <rs type="person">Dr. Erik Deumens</rs>, for their generous provision of computational resources through the <rs type="institution" subtype="infrastructure">UF HiperGator-AI cluster</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TyAyS6B">
					<idno type="grant-number">1RF1AG072799</idno>
				</org>
				<org type="funding" xml:id="_edTnEfQ">
					<idno type="grant-number">1R01AG078154</idno>
				</org>
				<org type="funding" xml:id="_uUThj5D">
					<idno type="grant-number">R01AG073435</idno>
				</org>
				<org type="funding" xml:id="_RQWVbvZ">
					<idno type="grant-number">R01LM013519</idno>
				</org>
				<org type="funding" xml:id="_nbS53cu">
					<idno type="grant-number">RF1AG084178</idno>
				</org>
				<org type="funding" xml:id="_qUfeSya">
					<idno type="grant-number">R01AG083039</idno>
				</org>
				<org type="funding" xml:id="_YShcPgn">
					<idno type="grant-number">R01CA284646</idno>
				</org>
				<org type="funding" xml:id="_M6Kvt5E">
					<idno type="grant-number">R01AI172875</idno>
				</org>
				<org type="funding" xml:id="_Fb7mgB7">
					<idno type="grant-number">R01AG080991</idno>
				</org>
				<org type="funding" xml:id="_u3acXkJ">
					<idno type="grant-number">R01AG080624</idno>
				</org>
				<org type="funding" xml:id="_PUXwPQZ">
					<idno type="grant-number">R01AG080429</idno>
				</org>
				<org type="funding" xml:id="_NamxWea">
					<idno type="grant-number">1K99LM01402</idno>
				</org>
				<org type="funding" xml:id="_WVpXXm6">
					<idno type="grant-number">1K99LM014614-01</idno>
				</org>
				<org type="funding" xml:id="_C6zsMkf">
					<idno type="grant-number">UL1 TR001427</idno>
				</org>
				<org type="funding" xml:id="_GgspkSM">
					<idno type="grant-number">U18 DP006512</idno>
				</org>
				<org type="funding" xml:id="_AeVryej">
					<idno type="grant-number">PCORI RI-FLORIDA-01-PS1</idno>
				</org>
				<org type="funding" xml:id="_8Cw2GgE">
					<idno type="grant-number">PCORI ME-2018C3-14754</idno>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">UF HiperGator-AI cluster</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_C7rNHCd">Data availability</head><p xml:id="_25ynFeS"><s xml:id="_KNM5qxE">All datasets employed in the continual pre-training process and evaluation are accessible from their original published venues.</s><s xml:id="_8tcgcZW">The PubMed Central and PubMed Abstracts subset from The Pile are available at <ref type="url" target="https://huggingface.co/datasets/EleutherAI/pile">https://  huggingface.co/datasets/EleutherAI/pile</ref>.</s><s xml:id="_enT7Das">MIMIC-IV and MIMIC-CXR datasets can be accessed under the PhysioNet Credentialed Health Data Use Agreement 1.5.0 at <ref type="url" target="https://physionet.org/content/mimic-iv-note/2.2/">https://physionet.org/content/mimic-iv-note/2.2/</ref> and <ref type="url" target="https://physionet.org/content/mimic-cxr/2.0.0/">https://physionet.org/content/mimic-cxr/2.0.0/</ref> respectively.</s><s xml:id="_XCvBZCh">The Red-Pajama data is open-released at <ref type="url" target="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1">https://huggingface.co/datasets/  togethercomputer/RedPajama-Data-1</ref>.</s><s xml:id="_YrjuYUk">Alpaca data is openly released at: <ref type="url" target="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</ref>.</s><s xml:id="_mkmgwPN">Dolly data is openly released at: <ref type="url" target="https://huggingface.co/datasets/databricks/databricks-dolly-15k">https://huggingface.co/datasets/databricks/databricks-dolly-15k</ref>.</s><s xml:id="_j49BJAg">Share GPT data can be accessed at: <ref type="url" target="https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered">https://huggingface.co/datasets/  anon8231489123/ShareGPT_Vicuna_unfiltered</ref>.</s><s xml:id="_G2hMqw2">The clinical instruction tuning data based on MIMIC-IV and MIMIC-CXR can be accessed under the PhysioNet Credentialed Health Data Use Agreement 1.5.0 through: <ref type="url" target="https://huggingface.co/clinicalnlplab">https://huggingface.co/clinicalnlplab</ref>.</s><s xml:id="_FxRrTSG">The Medical Flash Cards and wikidoc QA datasets can be accessed at <ref type="url" target="https://huggingface.co/medalpaca">https://huggingface.co/medalpaca</ref>.</s><s xml:id="_t8yKpWu">Other remaining instruction tuning data can be openly accessed at: <ref type="url" target="https://huggingface.co/clinicalnlplab">https://  huggingface.co/clinicalnlplab</ref>.</s><s xml:id="_uzheTWh">Me-LLaMA 13B and Me-LLaMA 70B models can be accessed at: <ref type="url" target="https://physionet.org/content/me-llama/1.0.0/">https://physionet.org/content/me-llama/1.0.0/</ref>, subject to the completion of a credentialed health data use agreement.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_b2brms2">Code availability</head><p xml:id="_ZQ2p98U"><s xml:id="_mupGbRw">The code used for evaluation is available at: <ref type="url" target="https://github.com/BIDS-Xu-Lab/">https://github.com/BIDS-Xu- Lab/</ref> Me-LLaMA.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_smd79jj"><p xml:id="_edm4rvc"><s xml:id="_vYGWWYh">whether each diagnosis from the model's differential diagnosis list matched the gold standard final diagnosis, consistent with these prior studies.</s><s xml:id="_wmwz7yP">Existing studies <ref type="bibr" target="#b26">27</ref> have shown that LLM-based automatic calculation of top-K accuracy is comparable to human evaluation.</s><s xml:id="_vK4RzQg">Besides automatic evaluation, we had a clinician specializing in internal medicine perform a manual evaluation of top-k accuracy (k = 1, 5).</s><s xml:id="_xCedrEY">For more details on data processing, automatic evaluation, and human evaluation, see the Supplementary Information.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wWzPMXk">Evaluation settings</head><p xml:id="_TEQYHgS"><s xml:id="_9VaRtkU">We evaluated Me-LLaMA at two evaluation settings including zero-shot and supervised learning to evaluate their performance and generalization ability across various tasks compared to baseline models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AhXUFqf">Supervised learning</head><p xml:id="_eJEjC2W"><s xml:id="_gg8yqTj">In the supervised learning setting, we evaluated Me-LLaMA 13/70B base models' performances adapted to downstream tasks.</s><s xml:id="_eKuTBs5">We conducted the task-specific finetuning on Me-LLaMA base models (Me-LLaMA taskspecific) with each training set of assessed datasets in Table <ref type="table">6</ref>, and then assessed the performance of Me-LLaMA task-specific models on test datasets.</s><s xml:id="_NeefSPT">We employed the AdamW optimizer.</s><s xml:id="_8rpwHJ8">For datasets with fewer than 10,000 training samples, we fine-tuned the models for 5 epochs, while for larger datasets, the fine-tuning was conducted for 3 epochs.</s><s xml:id="_gnVUDZf">A uniform learning rate of 1e-5 was used across all datasets.</s><s xml:id="_3RazzsC">Our baseline models including LLaMA2 Models (7B/13B/70B) <ref type="bibr" target="#b5">6</ref> : they are open-sourced LLMs released by Meta AI.</s><s xml:id="_G2GBbtc">PMC-LLaMA 13B 2 is a biomedical LLM continually pre-trained on biomedical papers and medical books.</s><s xml:id="_4fknDxs">Meditron7B/70B 9 : these are medical LLMs based on LLaMA2-7B/70B, continually pre-trained with a mix of clinical guidelines, medical papers and abstracts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CEkHEWc">Zero-shot Learning</head><p xml:id="_NKssu7a"><s xml:id="_P9EREFk">We assessed our Me-LLaMA 13/70B-chat models' zero-shot learning capabilities, which are key for new task understanding and response without specific prior training.</s><s xml:id="_FHCUYZQ">We compared our models and baseline models' zeroshot, using standardized prompts (detailed in the Supplementary Information, Supplementary Table <ref type="table">2</ref>) for each test dataset from Table <ref type="table">2</ref>.</s><s xml:id="_ht6KSXS">We compared Me-LLaMA 13/70B-chat models with the following baseline models: ChatGPT/GPT-4 4 : SOTA commercialized LLMs.</s><s xml:id="_hdSaR8T">We used the version of "gpt-3.5-turbo-0301"</s><s xml:id="_HSQAnsD">for ChatGPT, and the version of "gpt-4-0314" for GPT-4.</s><s xml:id="_snNnjSS">LLaMA2-7B/13B/70B-chat 6 models were adaptations of the LLaMA2 series, optimized for dialogue and conversational scenarios.</s><s xml:id="_k4vP3tS">Medalpaca-7B/13B 3 models were based on LLaMA-7B/13B, specifically fine-tuned for tasks in the medical domain.</s><s xml:id="_GktRtNk">The PMC-LLaMA-13B-chat 2 model is an instruction-tuned medical LLM based on PMC-LLaMA-13B.</s><s xml:id="_CaNmJkT">The AlpaCare-13B <ref type="bibr" target="#b10">11</ref> model is specifically tailored for clinical tasks based on LLaMA-2 13B by instruction tuning.</s><s xml:id="_dHxm8Sm">Meditron 70B 9 is a medical LLM, continually pre-trained with a mix of clinical guidelines, biomedical papers, and abstracts based on LLaMA2 70B.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AuMSErT">Author contributions</head><p xml:id="_j23y56e"><s xml:id="_5q4VfTQ">Q.X.</s><s xml:id="_vTYytgh">contributed to the conceptualization of the study, conducted the literature search, developed the methodology, contributed to the software development, carried out validation processes, and was primarily responsible for writing the original draft of the manuscript.</s><s xml:id="_6Xye45G">Q.C. contributed to the conceptualization of the study, developed the methodology, and contributed to reviewing and editing the manuscript.</s><s xml:id="_6jp2C47">A.C. played a key role in data curation and project administration, overseeing the planning and execution of research activities, and contributing to reviewing and editing the manuscript.</s><s xml:id="_m9yAQxr">C.P., Y.H., F.L., X.P., J.H., J.Z., V.K., X.Z., and L.Q. were instrumental in software development and validation and reviewing the manuscript.</s><s xml:id="_YcYa6nZ">H.H. took charge of visualization, specifically in the preparation of figures to support the study's findings, involved in the discussion and reviewing the manuscript.</s><s xml:id="_9eEyKNV">D.S. was involved in the discussion, human evaluation, and reviewing the manuscript.</s><s xml:id="_WbsyTEE">L.O.M. and Y.W. were involved in the discussion, review, and editing of the paper.</s><s xml:id="_BCh5WgH">H.X and J.B. provided overall supervision for the project, including study design, execution, and evaluation, coordination of study team and resources, and thorough review and revision of the manuscript.</s><s xml:id="_4CW6RhM">All authors reviewed the manuscript critically for scientific content, and all authors gave final approval of the manuscript for publication.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gyBmnWC">Competing interests</head><p xml:id="_BVgvWuZ"><s xml:id="_gYD7UwZ">The authors declare no competing interests.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_P37pAT6">Additional information</head><p xml:id="_bnnw6Jd"><s xml:id="_DRej59u">Supplementary information The online version contains supplementary material available at <ref type="url" target="https://doi.org/10.1038/s41746-025-01533-1">https://doi.org/10.1038/s41746-025-01533-1</ref>.</s></p><p xml:id="_acKXkUZ"><s xml:id="_pqH5JZg">Correspondence and requests for materials should be addressed to Hua Xu or Jiang Bian.</s></p><p xml:id="_eJKMzhn"><s xml:id="_PKN7JVp">Reprints and permissions information is available at <ref type="url" target="http://www.nature.com/reprints">http://www.nature.com/reprints</ref></s><s xml:id="_vrsbDuZ">Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p><p xml:id="_P9Kwkh4"><s xml:id="_5muZuvV">Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material.</s><s xml:id="_tynWSJp">You do not have permission under this licence to share adapted material derived from this article or parts of it.</s><s xml:id="_3kWUm3G">The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.</s><s xml:id="_jmSSh79">If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</s><s xml:id="_v9WD8kY">To view a copy of this licence, visit <ref type="url" target="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by- nc-nd/4.0/</ref>.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_3UQg5BD">Capabilities of GPT-4 on Medical Challenge Problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.13375</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.13375" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nori, H., King, N., McKinney, S. M., Carignan, D. &amp; Horvitz, E. Capabilities of GPT-4 on Medical Challenge Problems. Preprint at https://doi.org/10.48550/arXiv.2303.13375 (2023).</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_6k6UJRa">PMC-LLaMA: toward building open-source language models for medicine</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocae045</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jHN38AV">J. Am. Med Inf. Assoc</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1833" to="1843" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, C. et al. PMC-LLaMA: toward building open-source language models for medicine. J. Am. Med Inf. Assoc. 31, 1833-1843 (2024).</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_meSb7Mc">MedAlpaca -An Open-Source Collection of Medical Conversational AI Models and Training Data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2304.08247</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2304.08247" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Han, T. et al. MedAlpaca -An Open-Source Collection of Medical Conversational AI Models and Training Data. Preprint at https://doi. org/10.48550/arXiv.2304.08247 (2023).</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Achiam</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.08774</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.08774" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report</note>
	<note type="raw_reference">Achiam, O. J. et al. GPT-4 Technical Report. Preprint at https://doi. org/10.48550/arXiv.2303.08774 (2023).</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main" xml:id="_h7jArrQ">LLaMA: Open and Efficient Foundation Language Models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2302.13971</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2302.13971" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Touvron, H. et al. LLaMA: Open and Efficient Foundation Language Models. Preprint at https://doi.org/10.48550/arXiv.2302.13971 (2023).</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main" xml:id="_KAH8cn2">Llama 2: Open Foundation and Fine-Tuned Chat Models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2307.09288</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2307.09288" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint at https://doi.org/10.48550/arXiv.2307.09288 (2023).</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_m8aMWE5">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06291-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HeBRjEx">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="page" from="172" to="180" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172-180 (2022).</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_wGhSnt4">A study of generative large language model for medical research and healthcare</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A I</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Dds4TsC">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peng, C. A. I. et al. A study of generative large language model for medical research and healthcare. NPJ Digital Medicine. 6 (2023).</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_PvQrSnB">MEDITRON-70B: Scaling Medical Pretraining for Large Language Models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2311.16079</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2311.16079" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen, Z. et al. MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. Preprint at https://doi.org/10.48550/arXiv.2311. 16079 (2023).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_kn9C74q">ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DhE6Hs5">Cureus</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, Y. et al. ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. Cureus. 15 (2023).</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_WNe4VVW">AlpaCare: Instruction-tuned Large Language Models for Medical Application</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2310.14558</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2310.14558" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, X. et al. AlpaCare: Instruction-tuned Large Language Models for Medical Application. Preprint at https://doi.org/10.48550/arXiv. 2310.14558 (2023).</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_DRgbWKP">Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Alex</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.clinicalnlp-1.9</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rgvHV7k">Proceedings of the 6th Clinical Natural Language Processing Workshop</title>
		<meeting>the 6th Clinical Natural Language Processing Workshop<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="91" to="104" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gema, A., Minervini, P., Daines, L., Hope, T. &amp; Alex, B. Parameter- Efficient Fine-Tuning of LLaMA for the Clinical Domain. In Proceedings of the 6th Clinical Natural Language Processing Workshop. 91-104. Mexico City, Mexico (Association for Computational Linguistics, 2024).</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_ypduCGW">A Package for Automatic Evaluation of Summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Rouge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CPMPDM5">Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
	<note>Text Summarization Branches Out</note>
	<note type="raw_reference">Lin, C.-Y. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. 74-81. Barcelona, Spain (Annual Meeting of the Association for Computational Linguistics, 2004).</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_PvH5tCN">A systematic evaluation of large language models for biomedical natural language processing: benchmarks, baselines, and recommendations</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2305.16326</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2305.16326" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen, Q. et al. A systematic evaluation of large language models for biomedical natural language processing: benchmarks, baselines, and recommendations. Preprint at https://doi.org/10.48550/arXiv.2305. 16326 (2023).</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_b5Tjg65">Improving large language models for clinical named entity recognition via prompt engineering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VEmxYnV">J. Am. Med. Inform. Assoc. JAMIA</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1812" to="1820" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hu, Y. et al. Improving large language models for clinical named entity recognition via prompt engineering. J. Am. Med. Inform. Assoc. JAMIA 31, 1812-1820 (2023).</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_kYnV6gU">Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-024-01010-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TRxuV4t">NPJ Digit. Med</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Savage, T., Nayak, A., Gallo, R., Rangan, E. S. &amp; Chen, J. H. Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine. NPJ Digit. Med. 7 (2023).</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_9eGH5Rk">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VAPRXx7">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Stiennon, N. et al. Learning to summarize with human feedback. Advances in neural information processing systems. (2020).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_T9hD6VB">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2309.12307</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2309.12307" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
	<note type="raw_reference">Chen, Y. et al. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. Preprint at https://doi.org/10.48550/arXiv.2309. 12307 (2023).</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main" xml:id="_MFXGEqq">The Pile: An 800GB Dataset of Diverse Text for Language Modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2101.00027</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2101.00027" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gao, L. et al. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. Preprint at https://doi.org/10.48550/arXiv.2101.00027 (2020).</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_ukwvp4D">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wdWk36v">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database. Sci. Data. 3 (2016).</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_VDT5B6x">MIMIC-IV, a freely accessible electronic health record dataset</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-022-01899-x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZAY8H3b">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johnson, A. E. W. et al. MIMIC-IV, a freely accessible electronic health record dataset. Sci. Data. 10 (2023).</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_sXwpZfN">MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZEEke5T">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johnson, A. E. W. et al. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports. Sci. Data. 6 (2019).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_rVjUYfQ">RedPajama: an Open Dataset for Training Large Language Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eV8JCBa">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weber, M. et al. RedPajama: an Open Dataset for Training Large Language Models. Advances in neural information processing systems. (2025).</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_2ewh6VY">System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Deepspeed</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3406703</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_yNSNKZY">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rasley, J., Rajbhandari, S., Ruwase, O. &amp; He, Y. DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. (2020).</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_wj6FMxy">LoRA: Low-Rank Adaptation of Large Language Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FSsHFU4">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hu, J. E. et al. LoRA: Low-Rank Adaptation of Large Language Models. In The Twelfth International Conference on Learning Representations. (2022).</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_szbwt8a">Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kanjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodman</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2023.8288</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8Ww9FdH">JAMA</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kanjee, Z., Crowe, B. &amp; Rodman, A. Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic Challenge. JAMA. (2023).</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_JqTwxyG">Towards Accurate Differential Diagnosis with Large Language Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2312.00164</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2312.00164" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McDuff, D. et al. Towards Accurate Differential Diagnosis with Large Language Models. Preprint at https://doi.org/10.48550/arXiv.2312. 00164 (2023).</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_JhEEQNx">Evaluating Text Generation with BERT</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><surname>Bertscore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wfMTRxC">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q. &amp; Artzi, Y. BERTScore: Evaluating Text Generation with BERT. In International Conference on Learning Representations. (2019).</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Taori</surname></persName>
		</author>
		<ptr target="https://github.com/tatsu-lab/stanford_alpaca" />
		<title level="m" xml:id="_xMX8UhX">Stanford Alpaca: An Instruction-following LLaMA model</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Taori, R. et al. Stanford Alpaca: An Instruction-following LLaMA model https://github.com/tatsu-lab/stanford_alpaca (2023).</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Conover</surname></persName>
		</author>
		<ptr target="https://huggingface.co/datasets/databricks/databricks-dolly-15k" />
		<title level="m" xml:id="_fMUPJvU">Free dolly: Introducing the world&apos;s first truly open instruction-tuned llm</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Conover, M. et al. Free dolly: Introducing the world&apos;s first truly open instruction-tuned llm https://huggingface.co/datasets/databricks/ databricks-dolly-15k (2023).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_CNSGvtC">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_k2xXgXz">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zheng, L. et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. Advances in Neural Information Processing Systems. (2023).</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_uNCtcsV">Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QjeQf42">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
	<note type="raw_reference">Abacha, A. B., Shivade, C. P. &amp; Demner-Fushman, D. Overview of the MEDIQA 2019 Shared Task on Textual Inference, Question Entailment and Question Answering. In Proceedings of the 18th BioNLP Workshop and Shared Task. 370-379. Florence, Italy (Association for Computational Linguistics, 2019).</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_VKRtbyW">Bridging the gap between consumers&apos; medication questions and trusted answers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4rj34Wz">Stud. health Technol. Inform</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abacha, A. B. et al. Bridging the gap between consumers&apos; medication questions and trusted answers. Stud. health Technol. Inform. 264, 25-29 (2019).</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_QSJyyS7">Overview of the Medical Question Answering Task at TREC 2017 LiveQA</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eSJJqKm">Text Retrieval Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abacha, A. B., Agichtein, E., Pinter, Y. &amp; Demner-Fushman, D. Overview of the Medical Question Answering Task at TREC 2017 LiveQA. In (Text Retrieval Conference, 2017).</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_4736uYN">Detecting Causal Language Use in Science Findings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9XfagTM">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 4664-4674</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 4664-4674<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu, B., Li, Y. &amp; Wang, J. Detecting Causal Language Use in Science Findings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 4664-4674. Hong Kong, China (Association for Computational Linguistics, 2019).</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_RP2DMPf">A Dataset for Biomedical Research Question Answering</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><surname>Pubmedqa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1259</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YHEnCTj">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2567-2577</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2567-2577<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. &amp; Lu, X. PubMedQA: A Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2567-2577. Hong Kong, China (Association for Computational Linguistics, 2019).</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_uHxE2Kh">Medical Exam Question Answering with Large-scale Reading Comprehension</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sphW6rA">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, X., Wu, J., He, Z., Liu, X. &amp; Su, Y. Medical Exam Question Answering with Large-scale Reading Comprehension. In Proceedings of the AAAI conference on artificial intelligence. (2018).</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_Nn7ZCMK">A Largescale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sankarasubbu</surname></persName>
		</author>
		<author>
			<persName><surname>Medmcqa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.conll-1.21</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wCnPc26">Proceedings of the Conference on Health, Inference, and Learning</title>
		<meeting>the Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="248" to="260" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
	<note type="raw_reference">Pal, A., Umapathi, L. K. &amp; Sankarasubbu, M. MedMCQA : A Large- scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering. In Proceedings of the Conference on Health, Inference, and Learning. 248-260. (Proceedings of Machine Learning Research, 2022).</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_SUR5muv">emrQA: A Large Corpus for Question Answering on Electronic Medical Records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pampari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1258</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bn36ccQ">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2357" to="2368" />
		</imprint>
	</monogr>
	<note type="raw_reference">Pampari, A., Raghavan, P., Liang, J. J. &amp; Peng, J. emrQA: A Large Corpus for Question Answering on Electronic Medical Records. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 2357-2368. Brussels, Belgium (2018).</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_AHsVzq2">i2b2/VA challenge on concepts, assertions, and relations in clinical text</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Duvall</surname></persName>
		</author>
		<idno type="DOI">10.1136/amiajnl-2011-000203</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ArkWqTc">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="552" to="556" />
			<date type="published" when="2010">2010. 2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Uzuner, Ö., South, B. R., Shen, S. &amp; Duvall, S. L. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. J. Am. Med. Inform. Assoc. 18 5, 552-556 (2011).</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_zqXTet5">Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013)</title>
		<author>
			<persName><forename type="first">I</forename><surname>Segura-Bedmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herrero-Zazo</surname></persName>
		</author>
		<author>
			<persName><surname>Semeval</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2014.05.007</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2Z9U4Pt">Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013)</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation (SemEval 2013)<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="341" to="350" />
		</imprint>
	</monogr>
	<note>Second Joint Conference on Lexical and Computational Semantics (*SEM)</note>
	<note type="raw_reference">Segura-Bedmar, I., Martínez, P. &amp; Herrero-Zazo, M. SemEval-2013 Task 9 : Extraction of Drug-Drug Interactions from Biomedical Texts (DDIExtraction 2013). In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013). 341-350. Atlanta, Georgia, USA (Association for Computational Linguistics, 2013).</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_KkMyhSx">Automatic semantic classification of scientific literature according to the hallmarks of cancer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btv585</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kta34PS">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="432" to="440" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Baker, S. et al. Automatic semantic classification of scientific literature according to the hallmarks of cancer. Bioinformatics 32, 432-440 (2016).</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_PEJZ3rz">A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2097</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EzUaRww">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="615" to="621" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
	<note type="raw_reference">Cohan, A. et al. A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 615-621. New Orleans, Louisiana (Association for Computational Linguistics, 2018).</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Bastan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Bionli</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-025-01533-1AdversarialExamples</idno>
		<ptr target="https://doi.org/10.1038/s41746-025-01533-1AdversarialExamples" />
		<title level="m" xml:id="_NG3h9CH">Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for</title>
		<meeting><address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics: EMNLP 2022. 5093-5104</note>
	<note type="raw_reference">Bastan, M., Surdeanu, M. &amp; Balasubramanian, N. BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for https://doi.org/10.1038/s41746-025-01533-1 Adversarial Examples. In Findings of the Association for Computational Linguistics: EMNLP 2022. 5093-5104. Abu Dhabi, United Arab Emirates (Association for Computational Linguistics, 2022).</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_qpVQwHh">Lessons from Natural Language Inference in the Clinical Domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Shivade</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1187</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_x3ySrgp">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1586" to="1596" />
		</imprint>
	</monogr>
	<note type="raw_reference">Romanov, A. &amp; Shivade, C. P. Lessons from Natural Language Inference in the Clinical Domain. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 1586-1596. Brussels, Belgium (Association for Computational Linguistics, 2018).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
