<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_TwDtjCH">Investigating Preferred Food Description Practices in Digital Food Journaling</title>
				<funder ref="#_MDyc8FR">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lucas</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
							<email>silvald@uci.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of California , Irvine</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
							<email>epstein@ics.uci.edu</email>
							<affiliation key="aff1">
								<note type="raw_affiliation">University of California , Irvine</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_EmgqAXz">Investigating Preferred Food Description Practices in Digital Food Journaling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">837BE1369F2C09C2CA4FBA2A3455B5EB</idno>
					<idno type="DOI">10.1145/3461778.3462145</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T12:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_2EMXv54">Human-centered computing</term>
					<term xml:id="_p96wEQ8">• Human computer interaction (HCI)</term>
					<term xml:id="_9E4gpmT">• HCI design and evaluation Food journaling, prototype, personal informatics, multimodality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_9SrwgFc"><p xml:id="_WE3AWN9"><s xml:id="_dzArqNF">Journaling of consumed foods through digital devices is a popular self-tracking strategy for weight loss and eating mindfulness.</s><s xml:id="_EJcU9JX">Research has explored modalities, like photos and open-ended text and voice descriptions, to make journaling less burdensome and more descriptive than traditional barcode and database searches.</s><s xml:id="_7TPrDVV">However, less is known about how people prefer to journal foods when less constrained by limitations of databases, natural language processing, and image recognition.</s><s xml:id="_mQwdrW5">We deployed a food journal prototype supporting varied devices and input modalities, which 15 participants used to journal 1008 food logs over two weeks.</s><s xml:id="_FrJfSeh">Participants had diverse strategies for indicating what and how much they ate, varying from ambiguous foods to specifying varieties and using different measurements for clarifying amount.</s><s xml:id="_cRmgVuh">Some strategies were interpretable by natural language food identification and image classification services, while others point to open research questions.</s><s xml:id="_wvtDFaR">We finally discuss opportunities for accounting for variance in food journaling.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_gfFkh8v">INTRODUCTION</head><p xml:id="_NFuekHj"><s xml:id="_jfJ2Ree">Food tracking, or digital food journaling, has become one of the most popular self-tracking strategies to help with self-awareness and eating mindfulness, with 42% of U.S. adults having tried an app for diet or nutrition tracking as of 2017 <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b32">35,</ref><ref type="bibr" target="#b34">37,</ref><ref type="bibr" target="#b77">81]</ref>.</s><s xml:id="_QeGQhKn">Tracking of food intake has been shown to help people achieve health goals such as losing weight <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b63">67,</ref><ref type="bibr" target="#b77">81]</ref> and managing chronic diseases (e.g., diabetes) <ref type="bibr">[27,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b44">48,</ref><ref type="bibr" target="#b59">63]</ref>, identifying intolerances <ref type="bibr" target="#b49">[53,</ref><ref type="bibr" target="#b71">75]</ref>, and making healthier food choices <ref type="bibr" target="#b54">[58]</ref>.</s><s xml:id="_JzqpDS5">Commercial applications typically support people in journaling the foods they eat through lookups to food databases and scanning barcodes on packaged foods, enabling them to aggregate a record of the foods they eat and monitor how their daily intake aligns with calorie or nutrient goals <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b73">77,</ref><ref type="bibr" target="#b77">81]</ref>.</s></p><p xml:id="_PgpRwrM"><s xml:id="_cWGwzTy">Although food tracking can promote health benefits, it is widely regarded as burdensome, requiring a person to reliably journal to produce useful logs and contend with challenges around journaling accurately <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b48">52]</ref>.</s><s xml:id="_KZpGWnf">For example, some foods may not appear in food databases (e.g., foods from cultures the database was not designed to support), and some social contexts may make journaling uncomfortable or awkward <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_4hHd5Xh">In light of food tracking challenges, various research efforts have sought to minimize tracking burden by examining input modalities (i.e., methods of input) beyond barcode and database searches, such as photos <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b63">67]</ref> and voice memos <ref type="bibr" target="#b58">[62,</ref><ref type="bibr" target="#b73">77]</ref>.</s><s xml:id="_vbHs6ph">Several efforts have also invested in automating or complementing food tracking through eating moment detection <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b85">89]</ref>, food image analysis <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b41">45,</ref><ref type="bibr" target="#b61">65,</ref><ref type="bibr" target="#b79">83]</ref>, and identifying food consumption in natural language descriptions (e.g., social media posts) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">25,</ref><ref type="bibr" target="#b75">79]</ref>.</s><s xml:id="_JYhxN68">Other devices pose further opportunity for people to track their foods as they navigate different contexts throughout their life, such as through conversational interactions with increasingly-available voice assistants (VA) <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b64">68,</ref><ref type="bibr" target="#b72">76]</ref>.</s></p><p xml:id="_GebxFC6"><s xml:id="_pBK78zB">Food journaling can be used to support open-ended awareness and mindfulness goals <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b34">37]</ref> as well as calorie and nutrientconsumption goals <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b73">77,</ref><ref type="bibr" target="#b77">81]</ref>.</s><s xml:id="_BvjaaNy">To support awareness and mindfulness, technology has often leveraged flexible food journaling through text descriptions or photos to allow people to self-describe their food or eating moments however they desire <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b27">30]</ref>.</s><s xml:id="_gk7XgUK">Towards nutrient consumption goals, substantial work has examined how to recognize the foods and amount eaten to convert these input modalities into logs which contain consumed nutrients.</s><s xml:id="_dcNzyGk">For example, research in computer vision <ref type="bibr" target="#b13">[16,</ref><ref type="bibr" target="#b41">45,</ref><ref type="bibr" target="#b78">82]</ref> and crowdsourcing <ref type="bibr" target="#b63">[67]</ref> has examined labeling foods in images, while work in natural language processing <ref type="bibr" target="#b51">[55,</ref><ref type="bibr" target="#b52">56]</ref> has sought to identify what and how much a person ate from a text description.</s><s xml:id="_fQS3t4y">Research has also looked at improving the coverage of food databases <ref type="bibr" target="#b48">[52]</ref>.</s></p><p xml:id="_S85Vgzn"><s xml:id="_576Z6Dg">As food journaling becomes incorporated into more devices and systems, people will have access to increasingly varied methods of tracking their foods in their daily routines.</s><s xml:id="_yT3hfuP">However, less is known about how people wish to record their foods when under fewer technology constraints around recognition, accurate entry, and desire for recall.</s><s xml:id="_yN9evNE">For example, input modalities can include varying levels of detail about the foods people eat, from listing ingredients to describing a high-level category food fall under.</s><s xml:id="_k4GrsE8">Additionally, people often incorporate contextual information of where they ate and who they ate with into flexible logs <ref type="bibr" target="#b27">[30]</ref>.</s><s xml:id="_cdypfxJ">Understanding how people choose to describe what they eat in real-world settings can offer suggestions for how technology can better support people's preferred ways of journaling, such as opportunities for technology to assist in clarifying descriptions, adding context when desired, suggesting areas where recognition can improve, or accounting for variance in how people describe their foods.</s></p><p xml:id="_WC5ecGY"><s xml:id="_hZv3PVq">To understand how people prefer to journal their foods when under fewer constraints, we developed and deployed ModEat, a lightweight prototype for journaling on phones, computers, and voice assistants, that supported different input modalities, like barcode scanning, free text entry, voice logging, photo-taking, and simulating a database search.</s><s xml:id="_5pEj24Z">15 participants journaled 1008 food logs with the prototype over two weeks.</s><s xml:id="_wrsgQUd">In analyzing food logs and post-deployment interviews, we identify preferences and strategies for describing foods.</s><s xml:id="_PqgQ6Pj">To understand gaps between description and recognition, we further investigate how participant's logs were and were not interpretable by commercial natural language processing (NLP) and food image classification services.</s><s xml:id="_nJEh24J">Through these analyses, we note high variance in how people prefer to describe what they eat, both between individuals based on goals and among individuals based on their foods and circumstances.</s><s xml:id="_6XzrPYc">We contribute:</s></p><p xml:id="_DFsWJPf"><s xml:id="_qbeVN5R">• An understanding of how people choose to describe what and how much they ate when less constrained by recognition limitations, through the deployment of ModEat, a flexible technology prototype.</s><s xml:id="_XuSnphD">Participant's journal entries varied in how they described the granularity, specificity, amount, and context of the foods they ate.</s><s xml:id="_6bp7fJF">Participant's descriptions were still typically interpretable by NLP and Computer Vision services, but were less effective at evaluating more ambiguous descriptions or unclear food packages.</s><s xml:id="_w8VQs3Q">• An understanding of how people's food journaling goals and modality use influence their preferred journaling strategies.</s><s xml:id="_C9XpqgS">Input modality tended to influence the level of granularity and specificity participants used to describe foods, aggregating multiple foods into a single input more often and describing foods less specifically in more flexible input modalities, like plain text and voice descriptions.</s><s xml:id="_38pUsJy">Participants interested in quantifying their nutrition typically included formal measurements or counts of distinct items they ate, while people with less quantitative focus tended to not indicate how much they ate.</s><s xml:id="_BNF8xVa">• Design recommendations for addressing and accounting for variability in how people prefer to describe foods.</s><s xml:id="_EFQKgEp">We suggest that designs could help mitigate variance in journal entries through conversational approaches, but can also acknowledge or leverage ambiguity to promote reminiscence.</s><s xml:id="_DH7mCvR">By combining with general-purpose classifiers, recognition services could also detect more contextual information in people's text descriptions and images.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_TxZaXhZ">BACKGROUND</head><p xml:id="_xRJ2xBk"><s xml:id="_sVpdgK2">Our work builds on previous self-tracking research examining approaches for manual and automated food journaling, leveraging prototype deployment for eliciting people's perspective on use of technology in real-world settings.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_rScMmPu">Food Journaling</head><p xml:id="_PwwsNBt"><s xml:id="_xx7Mukw">Self-tracking technology, or personal informatics technology, aims to help people monitor and understand their habits <ref type="bibr" target="#b53">[57]</ref>.</s><s xml:id="_rK4JEAb">Food journaling is one of the most popular self-tracking domains, helping people monitor and understand their food related practices <ref type="bibr" target="#b36">[39]</ref> and change their behaviors towards healthier eating habits <ref type="bibr" target="#b46">[50]</ref>.</s></p><p xml:id="_dqcmJ2K"><s xml:id="_eaZY2jw">While food journaling can be done on paper, it has been supported in technology through barcode scans of packaged foods <ref type="bibr" target="#b73">[77]</ref> and database lookups <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b73">77,</ref><ref type="bibr" target="#b77">81]</ref>.</s><s xml:id="_g8r8xkD">These strategies aim to accurately identify nutrient information from food databases and barcode libraries.</s><s xml:id="_R99aUZD">These techniques are pervasive in commercial apps such as MyFitnessPal <ref type="bibr" target="#b62">[66]</ref>, WW (formerly Weight Watchers) <ref type="bibr" target="#b82">[86]</ref>, and Lose It! <ref type="bibr" target="#b55">[59]</ref>, as well as various research that employ food journaling systems <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b30">33,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b77">81]</ref>.</s><s xml:id="_wtwR5EK">However, people often find needing to search for and correctly identify every food eaten in food databases tedious <ref type="bibr" target="#b48">[52]</ref>, with unreliable information or difficult to find specific foods and amounts <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_MbnxKVR">Barcode scanning can lower this burden, but can potentially nudge people from eating fresh and healthier foods in favor of packaged ones <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_XcwZtmN">Recent work has also tried to lower the burden of database searches.</s><s xml:id="_8jsZmxM">In the design of EaT <ref type="bibr" target="#b48">[52]</ref>, Jung et al. leverage a search-accelerator for narrowing search results in a large food database.</s><s xml:id="_w7vTZxn">Participants found the search-accelerator easy to use and effective for reducing typing, but logged accuracy was impaired when users did not provide details for some composite foods.</s></p><p xml:id="_EFh8876"><s xml:id="_qXrW6JA">In general, manual self-tracking requires substantial effort <ref type="bibr" target="#b20">[23]</ref>, with food tracking in particular introducing additional challenges that make the practice burdensome.</s><s xml:id="_P5X5BJg">Cordeiro et al. identified several barriers to typical database and barcode food journals that negatively impact food tracking practices <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_6bmCVDM">People often do not want to journal in social situations because of a perceived stigma, forget to journal and fall out of the habit, find homemade or ethnic foods more difficult to journal, struggle to contend with unreliable food databases, and feel shame or judgment when not reaching a foodrelated goal due to prominent calorie and nutrient goal features <ref type="bibr" target="#b28">[31]</ref>.</s></p><p xml:id="_27gQKUq"><s xml:id="_xKfznCU">Research has examined approaches to make manual food journaling more flexible to promote eating mindfulness and awareness rather than collecting calorie or nutrient metrics.</s><s xml:id="_7cmQrxF">Photos, free text, and voice inputs have been found to be feasible ways of recording and describing foods <ref type="bibr" target="#b12">[15,</ref><ref type="bibr">27,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b34">37]</ref>.</s><s xml:id="_UVTQqSq">Some commercial apps, such as YouFood <ref type="bibr" target="#b84">[88]</ref> and Ate <ref type="bibr" target="#b6">[9]</ref>, have also leveraged voice and photo-based journaling.</s><s xml:id="_ck75A9d">Free text input has typically been incorporated to complement photo entries [27, <ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b34">37]</ref>, often to add more details to assist social contacts <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b56">60]</ref> or clinicians <ref type="bibr">[27,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b57">61]</ref> in interpretation.</s></p><p xml:id="_HgngrA5"><s xml:id="_XmeNfbW">Various research efforts have further examined automation for lowering food journaling burden.</s><s xml:id="_696mZn8">These efforts have been examined detecting eating moments through sound (e.g., chewing noises) <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b8">11,</ref><ref type="bibr" target="#b67">71]</ref>, automatic photo-taking with wearable cameras <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b76">80]</ref>, movement or proximity with necklace-like wearables <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b85">89]</ref>, and combining multiple sensor modalities <ref type="bibr" target="#b66">[70]</ref>.</s><s xml:id="_73dqQ8j">Research has also sought to estimate food volume and label identified foods through crowdsourcing nutritional estimates <ref type="bibr" target="#b63">[67]</ref>, computer vision <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b61">65,</ref><ref type="bibr" target="#b78">82,</ref><ref type="bibr" target="#b79">83,</ref><ref type="bibr" target="#b81">85]</ref>, and natural language processing (NLP) <ref type="bibr" target="#b51">[55,</ref><ref type="bibr" target="#b52">56]</ref>.</s></p><p xml:id="_CvSUXgy"><s xml:id="_UegZ3Zg">With increased power of computer vision and machine learning, food recognition from images have been introduced into systems for ingredient identification and nutritional estimation.</s><s xml:id="_4Ws2XdY">Im2Calories uses CNN-based deep learning to identify foods from restaurants, interpreting their food volume and their nutritional values <ref type="bibr" target="#b61">[65]</ref>.</s><s xml:id="_9pvz7y9">Similarly, Menu-Match logs calorie intake estimated from image classification and food databases, but also has a focus on restaurant foods <ref type="bibr" target="#b10">[13]</ref>.</s><s xml:id="_Q6hKK9X">Ingredients or nutrients recognized can further be used to create more abstract representations, such as digital postcards summarizing what was eaten for later reflection <ref type="bibr" target="#b75">[79]</ref> or reverseengineering recipes from photos <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b70">74]</ref>.</s><s xml:id="_Mw94bqy">Different from using deep-learning techniques, Yang et al. proposed representing foods as pairwise statistics over image pixels, reasoning that this can indicate spatial combinations of ingredients (e.g., a "bread" pixel next to a "cheese" pixel) <ref type="bibr" target="#b83">[87]</ref>.</s></p><p xml:id="_RA9kgrz"><s xml:id="_2yECj7x">Other systems have proposed the use of NLP for automatically interpreting foods from spoken or open-ended written descriptions.</s><s xml:id="_gPg2DEu"><ref type="bibr">Korpusik et al.</ref> proposed the use of deep learning for semantic mapping foods in text based meal descriptions with searches in USDA's [40] food database <ref type="bibr" target="#b51">[55]</ref>.</s><s xml:id="_nqRZ42u">In a different contribution, Korpusik et al. incorporated this technique onto a food journaling app, using both text-based descriptions and voice input with speech recognition for then NLP and database mapping <ref type="bibr" target="#b52">[56]</ref>.</s><s xml:id="_ywQg4j7">Studies have also aimed to identify and classify foods in free text descriptions of foods posted to social media to understand community-level eating practices <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">25]</ref>.</s></p><p xml:id="_SCBbrG8"><s xml:id="_MnkGdAu">Although these systems posit that automation can lower food journaling burden, complete automation can be inaccurate and is still far from being practical <ref type="bibr" target="#b38">[42,</ref><ref type="bibr" target="#b54">58,</ref><ref type="bibr" target="#b59">63]</ref>.</s><s xml:id="_G3RrMDe">Choe et al. further highlight that fully automating data capture can reduce awareness and engagement, suggesting that self-monitoring technology balance manual and automatic tracking <ref type="bibr" target="#b20">[23]</ref>.</s><s xml:id="_dsqCYFc">We expand opportunities for supporting manual and semi-automated food journaling by understanding people's journaling preferences when less constrained by current recognition limitations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_7SmE4YT">Elicitation of In-Situ Everyday Technology Use</head><p xml:id="_rsjN8nJ"><s xml:id="_uBgYyVe">Elicitation studies are often used to understand people's preferences for interacting with technology, such as input methods.</s><s xml:id="_n2MvMfD">For example, they have been used to discover preferred gestures for interacting with mobile devices <ref type="bibr" target="#b69">[73]</ref> or interactive surfaces <ref type="bibr" target="#b80">[84]</ref>.</s></p><p xml:id="_mmHcA8f"><s xml:id="_xVVehsd">Although these studies uncover ways people wish to interact with technology, they can fall short of considering real-world contexts.</s><s xml:id="_7kG9fCJ">For example, people's preferences might be influenced by their social and environmental contexts, or additional factors unaccounted for in studies that are not "in-the-wild" <ref type="bibr" target="#b45">[49]</ref>.</s><s xml:id="_pC7SKez">In self-tracking, studies have used different approaches to circumvent constraints of in-lab elicitation studies.</s><s xml:id="_ZMYnA3y">For example, Gorm et al. suggest that participant-driven photo taking can elicit in-situ technology use, such as for understanding activity tracking practices <ref type="bibr" target="#b39">[43]</ref>.</s><s xml:id="_2NAffBx">Gouveia et al. similarly leveraged video recordings from wearable cameras to understand people's use of activity trackers in daily life <ref type="bibr" target="#b40">[44]</ref>.</s><s xml:id="_VJbjmd8">Other self-tracking studies have instrumented the deployed apps to understand their use, for example to identify how often participants engaged with a particular feature <ref type="bibr" target="#b40">[44,</ref><ref type="bibr" target="#b60">64]</ref> or evaluate novel interactions and approaches <ref type="bibr" target="#b42">[46,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b56">60]</ref>.</s><s xml:id="_DSHZ5Cd">Similarly, we deploy a flexible prototype, ModEat, to understand people's preferred interactions with technology in real-world settings.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_nPjVqkD">METHODS</head><p xml:id="_wJvbTcz"><s xml:id="_CvN5vCw">We created and deployed a prototype to understand how participants would like to journal their food when less constrained by recognition or a desire for recall.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_hUXwkwM">The ModEat Prototype</head><p xml:id="_3A9j3r9"><s xml:id="_gKEdw45">We developed ModEat (Figure <ref type="figure">1</ref>), a multiplatform system available for Android and iOS phones, computers, and Amazon Alexa and Google Assistant voice assistants.</s><s xml:id="_Xn3jp3P">We aimed to include common input techniques for food journaling in ModEat that prior research has suggested to support calorie and nutrient-consumption goals or open-ended awareness and mindfulness goals (e.g., <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b62">66,</ref><ref type="bibr" target="#b73">77,</ref><ref type="bibr" target="#b77">81]</ref>, see section 2.1), while supporting many of the devices people frequently interact with.</s><s xml:id="_UHRTewA">ModEat for phone supported six different input modalities: text, voice log, picture from device camera, simulated database search, website URL, and barcode scanning.</s><s xml:id="_rXYWCbu">ModEat for computer ran on web browsers supporting the same input modalities as the phone, with images supported as uploads and barcodes are typable.</s><s xml:id="_FxDCyA2">ModEat for VAs supported conversational interaction commands, allowing people to create a new food description (e.g., "journal green eggs and ham") or request and hear their previous journal entry ("read last entry").</s><s xml:id="_jCvTg5B">ModEat for computer and mobile supported reviewing previous entries by displaying the result (e.g., the text description, the numeric barcode, a simulated database search), with voice logs being displayed as text.</s><s xml:id="_MjtaaaD">We also implemented ModEat for Apple Watch via voice logging, but none of the participants regularly wore an Apple Watch, so we do not report further on watch entry.</s><s xml:id="_zF9XMuY">We sought to avoid incorporating suggestions for what or how to journal foods in ModEat.</s><s xml:id="_EwuzhAr">We intentionally did not add food recognition features to ModEat (e.g., image recognition, database searches, look up UPC barcodes), instead asking participants to suspend belief about feedback and journal as if receiving idealized responses.</s><s xml:id="_bztpWK7">Doing so spared participants from limitations of current technology (e.g., incorrect or missing foods in databases, images which could not be recognized, barcodes which could not be identified), while prompting consideration of preferred entry methods.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_PE6RBK6">Participants</head><p xml:id="_PFak8hq"><s xml:id="_S5y2DnQ">Our study was approved by our university's IRB prior to recruitment.</s><s xml:id="_8NejhKd">We advertised our study through a screener survey sent to local mailing lists and subreddits related to food tracking or cities close to our university.</s><s xml:id="_M42RkTS">We targeted the recruitment of people with prior journaling experience or interested in starting to journal.</s><s xml:id="_8Khm9Q6">We primarily recruited participants with prior experience to ensure participants were highly-motivated to pursue a journaling goal, and would therefore carefully consider how their descriptions of foods would support their goals.</s><s xml:id="_2jHvwbD">Participant's past experiences also made them aware of the capabilities and recognition constraints of traditional food journaling approaches, enabling them to enact different uses of ModEat if they wished and allow them to journal  In addition, we required participants speak English and be 18 years or older.</s><s xml:id="_6dmNEjG">We also required that participants have access to a phone, computer and Amazon Alexa or Google Assistant.</s><s xml:id="_ER8xzza">In case a participant did not own a VA, we offered to lend one if they were located near our university.</s><s xml:id="_Y2uSD97">Participants used ModEat for two weeks between February and April 2020.</s><s xml:id="_uHYJkza">Participants were offered $30 as compensation for full participation.</s><s xml:id="_qm6gCx7">55 people responded to our screener survey, out of which 33 satisfied our requirements and 18 responded to our contact.</s><s xml:id="_VBqn5h6">One participant dropped out of the study due to family health issues and two were dropped because they became unresponsive during deployment.</s><s xml:id="_TrgayBh">The remaining 15 participants had a median age of 33 (range: 25 -50), 8 identified as female, 6 as male, and 1 as a woman.</s></p><p xml:id="_kBnAnhh"><s xml:id="_jANqHN6">While P9 had no prior food journaling experience, the other participants had a median journaling experience of 10 months (range: 1 month -4 years).</s><s xml:id="_2YapA3h">Participants had various personal motivations for journaling that could fall under one of two categories: quantitative goals, that focused on various numerical information (e.g., calories, micro or macro nutrients); or awareness goals, that focused on broad food consumption information (e.g., eating more greens, frequency of snacking, general healthy eating).</s><s xml:id="_HGSCW3M">Table <ref type="table" target="#tab_0">1</ref> summarizes participant related information.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_EsqxDdy">Study Procedures</head><p xml:id="_X883hKk"><s xml:id="_Fjuu4nJ">Each participant went through an initial 30-minute onboarding interview to understand the study's goal, learn about the ModEat, configure it on their devices, and tell us about their journaling experience and goals.</s><s xml:id="_BmPpFjc">For a few participants (P5, P6, P7, P19) this interview was in-person in public locations, but an increase in the spread of COVID-19 led every other participant-researcher interaction to be conducted through remote video calls.</s><s xml:id="_vhqD4vg">We encouraged participants to journal their foods according to their personal food goals and with whatever input modality they preferred at a given moment, thinking past the recognition constraints of similar current technology they had used or seen before.</s><s xml:id="_97j5Q4D">Participants were also provided with a manual describing ModEat's configuration and features for later reference if needed (included in the supplemental materials).</s></p><p xml:id="_3zSJ6SN"><s xml:id="_9REJYTY">Participants used ModEat for their daily food tracking for at least two weeks (mean 14.7 days, min 14, max 16).</s><s xml:id="_t92XXzw">Other food journaling studies have similarly deployed systems between 2 to 4 weeks to understand participant's regular use <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b47">51,</ref><ref type="bibr" target="#b77">81,</ref><ref type="bibr" target="#b85">89,</ref><ref type="bibr" target="#b86">90]</ref>.</s><s xml:id="_Zz9MPv6">To help the research team understand the circumstances surrounding each journal entry, participants also answered a daily survey questionnaire.</s><s xml:id="_mBSagcn">The questionnaire showed participants the entries they made that day, asking them to describe where they ate (e.g., home, restaurant), whether they ate with others, the type of meal (e.g., full meal, snack), and when they journaled relative to when they ate (e.g., long before, long after, while eating).</s></p><p xml:id="_eTAWJHJ"><s xml:id="_j3fGS49">We conducted a one-hour post-deployment interview with each participant to discuss their experiences with ModEat.</s><s xml:id="_EjAdK6k">During the interview, we showed participants their food logs and asked questions to clarify how and why they chose their logging techniques.</s><s xml:id="_VasCs7K">We also asked participants about the limitations they experienced with current journaling strategies and what they would ideally wish to be able to do with food journaling technologies.</s><s xml:id="_aK4hgt8">Participants were compensated at the end of the interview and returned any lent device in a socially distant manner (e.g., leaving device in the porch).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" xml:id="_Van2m5A">Data Analysis</head><p xml:id="_tgggWVY"><s xml:id="_dDfEkCE">All journal entries were separated by input (e.g., text description, barcode, photo), resulting in 1008 individual inputs.</s><s xml:id="_pVZYzGB">The first author followed thematic analysis <ref type="bibr">[17]</ref> to analyze the food logs, first open coding and then discussing themes with the research team.</s><s xml:id="_5uHMw8C">After refining definitions and coding criteria, the final codebook contained 39 codes in 12 categories, such as how many food items were present in a log, how specifically foods were described, and if and how logs described food amounts.</s><s xml:id="_XvBD8BF">For example, the code category amount had the subcodes numeric scale, numeric only, broad, comparative/reference, non-standard, and non-quantified).</s><s xml:id="_QTEWASs">Two authors independently coded the same 10% of inputs, reaching near-perfect agreement on 34 codes (Cohen's κ ≥ 0.8) and substantial agreement on the remaining 5 codes (Cohen's κ ≥ 0.6).</s><s xml:id="_Rqnh7eW">They discussed and resolved differences in code application, then the first author coded the remaining data.</s><s xml:id="_CGPd6te">All final interviews were audio-recorded and transcribed through a university-approved vendor.</s><s xml:id="_tPvWzs4">Authors reviewed interview transcripts to understand participant's reasons for how they described their foods to support the analysis of the food logs.</s></p><p xml:id="_gUXynMJ"><s xml:id="_mCnDwHw">Once all logs were coded, we used logistic regression to quantitatively analyze the influence of contextual factors surrounding the creation of journal entries based on the frequency of specific codes (e.g., whether journal entries created with certain modalities were more likely to be more granular or describe amounts).</s><s xml:id="_S8YE9Hp">We examined four contextual factors from the daily surveys as fixed effects: modality used, how many others they ate with, meal type, whether they journaled before or after eating.</s><s xml:id="_Pn4wRWc">We treated participant id as a random effect to account for individual differences in how participants described foods (e.g., participants who were more likely to include amounts).</s><s xml:id="_3DkGWvg">We corrected for multiple comparisons in post-hoc tests between levels of fixed effects with Tukey corrections.</s></p><p xml:id="_DmM7ns7"><s xml:id="_dpTj2uA">To understand how participant's preferred methods of food journaling aligned with traditional approaches to food recognition, we ran participant's logs through commercially-available recognition services.</s><s xml:id="_JusEzHY">It is not our intention to evaluate or compare the overall accuracy or quality of these methods for food identification, rather to explore how they might need to adjust to people's strategies for describing their food.</s></p><p xml:id="_dDZXEWc"><s xml:id="_jQk7vsy">We submitted database search, text, and voice input descriptions to commercially-available NLP services.</s><s xml:id="_SzkBqBR">We are not aware of prior research comparing and ranking the quality of NLP services for foods, so we ran three services (Nutritionix <ref type="bibr" target="#b65">[69]</ref>, Edamam <ref type="bibr" target="#b33">[36]</ref>, and Spoonacular <ref type="bibr" target="#b74">[78]</ref>) regarded as highly used and frequently mentioned in top lists (e.g., <ref type="bibr" target="#b68">[72]</ref>) on a random 10% of journal inputs, comparing the results against our manual inspection of the descriptions.</s><s xml:id="_f4mVCmT">These services accept requests with food descriptions in text and aim to return a list of described foods and their amounts, including an amount unit (e.g., grams, cups), calories, and nutrients (e.g., fats, sugar).</s><s xml:id="_KK7Pwbv">For example, a text input of "100g rice and 2 eggs" should identify "rice" and "eggs", and "100 grams" and "2" as their respective amounts, returning calorie and nutritional information.</s><s xml:id="_x4zycmF">This differs from non-NLP services that only do direct database searches for provided food items, such as the USDA FoodData Central <ref type="bibr">[40]</ref>.</s><s xml:id="_FMGxN3a">The NLP services proved to have different levels of success for a set of 10% random journal inputs.</s><s xml:id="_GQehRYn">All three services identified amounts fairly similarly (37.6%, 33.6%, 35.6% of inputs), but Nutritionix was more successful in identifying all food items in an input (78.2%), versus Edamam (37.6%) and Spoonacular (57.43%).</s><s xml:id="_9sPqSs4">Furthermore, Nutritionix failed to identify any food item or amount in just 1.9% of inputs, while Edamam in 21.7% and Spoonacular in 19.8%.</s><s xml:id="_zb8KHCX">Therefore, we chose to analyze the remaining inputs with results from Nutritionix.</s></p><p xml:id="_sEqTMyt"><s xml:id="_HQSbs58">We also used commercially-available image classification services to understand opportunities for improving recognition of the pictures people used to represent their foods.</s><s xml:id="_Vg77zuy">Similar to NLP services, we chose three popular services regarded in top service lists (e.g., [1]): Clarifai <ref type="bibr" target="#b26">[29]</ref>, Google CloudVision <ref type="bibr" target="#b37">[41]</ref>, and Amazon Rekognition <ref type="bibr">[4]</ref>.</s><s xml:id="_dENRQnV">Clarifai provides a service module specific for classification of food images, while the other two are publicized as image classification more generally and offer food detection as an example.</s><s xml:id="_w2v36N4">These services return a list of identified elements paired with a confidence level (e.g., a percentage from 0-100, probability from 0-1).</s><s xml:id="_7FWhdfW">For example, a query for an image of a chocolate cake could result in "[chocolate(1.00)</s><s xml:id="_hFkgESm">cake(1.00)</s><s xml:id="_gyGsJ6U">brownie(0.94)</s><s xml:id="_qtj8qFa">... flour(0.38)</s><s xml:id="_NR2nByA">pumpernickel(0.34)</s><s xml:id="_Mf36t3J">cookie(0.32)]".</s><s xml:id="_fX2tuJH">We ran the three services over 54 of our full set of 60 images, again comparing against our manual inspection of the foods present in the images and discarding 6 images where we could not manually identify any of the foods.</s><s xml:id="_QFGr85p">We took a conservative approach to recognition, considering all foods identified by the service with confidences above 0.8 as recognized.</s><s xml:id="_8qYSQ2F">For instance, we coded an image as successfully identified if the classifier service identified any element present in an image of a ham and cheese sandwich (e.g., "bread", "cheese") with at least 0.8 probability, regardless of whether foods which were not present were identified with high confidence (e.g., "cookie" at 0.92 confidence).</s></p><p xml:id="_pgRcGns"><s xml:id="_ZC88mve">We also searched barcode inputs in a barcode search website [8], searching for unidentified items using the barcode search in MyFit-nessPal <ref type="bibr" target="#b62">[66]</ref>, which also leverages user-created database items.</s></p><p xml:id="_6gxHkWE"><s xml:id="_NScGa2z">We quote participants with PXX when presenting quotes from interviews or journal descriptions they created.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5" xml:id="_f7Hezaq">Limitations</head><p xml:id="_fftMgwJ"><s xml:id="_q6ngGTH">We acknowledge our limited sample size, and although relatively diverse in gender and occupation, findings might not be generalizable to the journaling practices of different age or cultural groups.</s><s xml:id="_7Dg98cs">For instance, low-income communities have particular needs and expectations around food journaling technologies <ref type="bibr" target="#b42">[46]</ref>, and care needs to be taken in applying our findings to these cultural settings.</s><s xml:id="_KxeU9fu">Furthermore, while our participants' high degree of interest and prior experience using food journaling technologies led them to carefully consider how their approaches to journaling in ModEat would support their goals, their prior experience may have also influenced a few participants towards approaches they were familiar with.</s><s xml:id="_VBbhteY">Though most participants used and appreciated a range of input techniques, a few participants described intending to leverage database searches in ModEat in similar ways to their journaling prior experiences.</s><s xml:id="_5pjbxyK">None of our participants had a disease diagnosis or management goal, although it is a commonly-studied motivation for food journaling <ref type="bibr" target="#b44">[48,</ref><ref type="bibr" target="#b49">53,</ref><ref type="bibr" target="#b71">75,</ref><ref type="bibr" target="#b86">90]</ref>.</s><s xml:id="_j3nx3kp">We suspect that people with such a motivation might be inclined to emphasize specificity in their food descriptions and may have other different journaling preferences from our participants.</s></p><p xml:id="_8r8dv5U"><s xml:id="_EuzrdY2">Study deployment coincided with the COVID-19 pandemic, with 10 participants (P1-5, P7, P9, P11, P12, P14) mentioning feeling significant impact on the eating habits and stress levels.</s><s xml:id="_UX2NkHw">For instance, participants that would frequently eat at work or at restaurants almost exclusively ate at home during the study.</s><s xml:id="_xYMEruF">The pandemic also influenced available foods, such as P13 that mentioned "I would love to eat more fresh foods . . .</s><s xml:id="_Zkz2tvg">[but] I can't go to the grocery store multiple times".</s><s xml:id="_WTEKXNv">Other than the emotional and social consequences, only P9 and P13 felt their food description practices were impacted by the pandemic, with P9 reporting being "a little more observant" to detailing food compositions and P13 doing the opposite by relaxing her diet restrictions.</s><s xml:id="_U2mcEVn">Overall, participants still used regularly Mod-Eat to explore varied journaling strategies, despite the pandemic influencing what they ate and changing contexts surrounding how they ate.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_BWJyDzE">FINDINGS</head><p xml:id="_hDD4FUc"><s xml:id="_JxtSTMr">Participants used the ModEat prototype to make 659 food journal entries with 1008 modality inputs (average 1.53 modality inputs per entry, max 13).</s><s xml:id="_Kpj6gDZ">Participants journaled fairly frequently, averaging 2.98 entries per day (min 1.07, max 4.26).</s><s xml:id="_tqW3YH9">Out of the three descriptive inputs (database search, text description, and voice description), database search was the most used (37.8%),</s><s xml:id="_fzpGjTJ">followed by text description (27.4%) and voice description (23.1%).</s><s xml:id="_qBVQAvj">Images and barcodes were used 60 (5.9%) and 51 (5.1%) times, while URLs were recorded 7 times (0.7%).</s></p><p xml:id="_rHceaEp"><s xml:id="_JG4DsgT">Overall, participants tended to use database search, text, and voice descriptions in similar ways, often describing their food choices and amounts.</s><s xml:id="_Uyty6pE">Participants varied in how they used those modalities to describe and measure food.</s><s xml:id="_mnDZ64d">Descriptions varied in granularity and specificity, occasionally captured contextual information, and indicated amounts using measurement scales or numeric values alongside subjective measures, but entries were occasionally ambiguous or unclear.</s><s xml:id="_mPhkbeQ">Similarly, participants varied in how they used images to depict their food, such as arranging foods for aesthetics and clear amount compositions, use of stock images, and packages.</s><s xml:id="_QJq7Wg9">This input variability had consequences for the recognition and performance of commercially-available NLP and image classification ML models, with some styles of entry more accurately interpreted than others.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_JxD9JxD">Granularity</head><p xml:id="_q29TJvv"><s xml:id="_ubGcrkP">We define the granularity of a food log as the quantity of food items present in a single input log.</s><s xml:id="_C8XtY5F">We observed that food descriptions (text input, database search, voice input) were either single food item, a single item decomposed into its requisite ingredients, or aggregated foods.</s><s xml:id="_V9TeYHD">As described in Table <ref type="table" target="#tab_1">2</ref>, most food entries were composed of a single item (62.9% e.g., "1 cup blueberry", "fajitas").</s><s xml:id="_qRzswEP">However, participants occasionally described single foods with detailed ingredient compositions, such as the ingredients in a sandwich or a salad (8.2% of inputs).</s><s xml:id="_QnGpajV">Some of these inputs had a food's common name followed by its composition (e.g., "breakfast burrito with a whole wheat tortilla, two eggs, bacon. .</s><s xml:id="_KenzkXU">.",</s><s xml:id="_VXJvsUK">P14), while others described the ingredients without indicating a common name (e.g., "2 tortilla with butter and honey", P6).</s></p><p xml:id="_VnV42Pw"><s xml:id="_c4Kxu8f">Participants also regularly aggregated distinct food items into a single input (28.9% of descriptive modalities), averaging 3.09 foods per input when they aggregated (min 2, max 9).</s><s xml:id="_KZCd7hP">These differ from decomposed single food's in that they typically combined foods eaten together in a single event (e.g., meal, a snack), but represented distinct foods.</s><s xml:id="_phV8BEU">We classified 101 aggregated food inputs (39%) as main course dishes journaled with one or more side dishes.</s><s xml:id="_nhVb9mz">For example, P13 logged "egg omelet and side salad", P9 logged "wonton soup, Chinese book choy, rice, breaded shrimp.</s><s xml:id="_uSkCKq5">Orange", P4 logged "Veggie enchiladas, half a cookie, grapes, pistacios [sic]".</s><s xml:id="_SXUESqy">60 aggregated food inputs (23.3%) were foods alongside drinks, such as "coffee and banana" (P5) and "tea and chips" (P9).</s><s xml:id="_HmU6fyB">Participants tended to aggregate entries more often when eating with others versus alone (Z=2.04,</s><s xml:id="_wgJ27N9">p&lt;0.05, 95% CI 2%-90% more likely to aggregate), perhaps suggesting that participants tended to aggregate when in social situations where they wanted to journal multiple items quickly.</s></p><p xml:id="_qBgYUpt"><s xml:id="_Q4uysUW">Input modality tended to influence the granularity with which participants entered food (χ 2 (2, N=890)=89.56,</s><s xml:id="_k49f6vb">p&lt;0.001), but we did not observe a statistically significant impact of food journaling goals on granularity (p=0.15).</s><s xml:id="_Me7XrQA">Figure <ref type="figure" target="#fig_2">2a</ref> shows that nearly all database searches were of single food items (94.5%), versus about half of voice input entries (52%), and a quarter of text inputs (28.6%).</s><s xml:id="_v8th3qH">P6 described  <ref type="figure" target="#fig_2">2b</ref>.</s></p><p xml:id="_RjVzk7m"><s xml:id="_jk6C6MD">Five participants incorporated symbols to help describe or explain the components of their foods in text entry fields.</s><s xml:id="_JCJFKYX">For example, P6 used a "/" character as a delimiter when decomposing a food, such as "shrimp burger / 2 shrimp patty, 1 wheat bun, 2 tsp.</s><s xml:id="_RsxjG9w">sriracha&amp;mayo, 1/4c.</s><s xml:id="_cJWTEYu">spring mix".</s><s xml:id="_tHekzku">Similarly, P7 used ":" with the same objective.</s><s xml:id="_7QZG6hh">P7 would also use this approach to translate and describe varieties of ethnic foods inside parenthesis: "[Name of restaurant]: Lobster hand roll (x2), cooked scallop (hotate), albacore, salmon (sake), squid (ika), tuna (maguro), escolar (ono), yellow tail (hamachi), bluefin tuna (akami), [. .</s><s xml:id="_wqB7Y8x">.])".</s><s xml:id="_smQ8Fdg">P1 used "+" to denote combinations, such as "cheese toast with cream cheese + coffee".</s></p><p xml:id="_cTytHCD"><s xml:id="_xrucv57">After using the ModEat prototype, some participants imagined that future journaling systems could encourage people to aggregate foods they ate in a single meal or setting through conversational approaches.</s><s xml:id="_Ec5hEzM">Participant P15 described wanting to interact with a VA similarly to a drive-through window.</s><s xml:id="_ZEGMHfq">He said, "Like you drive up the window [. .</s><s xml:id="_D92dz2M">.]</s><s xml:id="_eb5hma4">Then I'd say, 'Alexa, journal food, ' and then she just says, 'Ready.</s><s xml:id="_zqx79UJ">' Then I start, 'All right, I'm having, a double-double and fries and a shake.</s><s xml:id="_TYBHYNV">' If there's a pause, she can ask, 'Is there more?' Then I can just reply, 'Oh, add some chicken nuggets,' or whatever".</s><s xml:id="_AaauURG">P14 had a similar idea, but felt "it could be a burden" to say so many details to the VA.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_PbAy43p">Specificity</head><p xml:id="_AybCQ69"><s xml:id="_6k4dAZz">We define specificity as the level of detail of a food description belonging to a particular food item, observing three levels: generic, specific, and varietal.</s><s xml:id="_ahNrG7V">Inputs with multiple food items could also have multiple levels of specificity (e.g., one item is specific with another being generic).</s><s xml:id="_5p5NKMQ">A minority of food descriptions consisted of foods with generic ingredients or contents (e.g., "dumplings. .</s><s xml:id="_cyvMXb9">."</s><s xml:id="_nWce23R">P1, "veggie taco salad. .</s><s xml:id="_ZxGzAP8">."</s><s xml:id="_QefHVvQ">P4), comprising 8.1% of all food items.</s><s xml:id="_WFZpSMQ">Participants instead tended to describe foods in ways which were specific enough to distinguish between foods or ingredients of prepared foods (e.g., "peanut butter 14 gram" P5, "broccoli, chicken, rice. .</s><s xml:id="_YqW5wgg">.." P9; 53.4% of all food items) or further describing varietals of same food (e.g., "red beans. .</s><s xml:id="_HEQzpNH">."</s><s xml:id="_r8TTK9n">P12, "roast chicken breast. .</s><s xml:id="_DDTZCcT">.",</s><s xml:id="_nBTb5WG">P4; 38.5% of all food items).</s><s xml:id="_prbhGm5">Table <ref type="table" target="#tab_1">2</ref> shows additional examples of inputs at different levels of specificity.</s><s xml:id="_wPS4GnP">Similar to granularity, we did not observe a statistically significant correlation between participant goal and specificity levels (p=0.83).</s><s xml:id="_YJabf6P">Instead, all participants greatly varied individually in how specifically they journaled their foods, as visualized in Figure <ref type="figure" target="#fig_3">3a</ref>.</s></p><p xml:id="_a8zakRp"><s xml:id="_wGuhaWf">Descriptions of aggregated foods were not always clear about how they were composed, leading to potential uncertainty or ambiguity around what was eaten.</s><s xml:id="_p2P9EyA">39% of descriptive inputs with multiple food items were joined with conjunctions or prepositions like "in", "and", or "with" (e.g., "chicken broth with rice and chicken meat" P11), while 25% of text input and database searches with multiple food items used commas as a separation symbol (e.g., "orange juice, egg roll biscuits" P9), and 9.4% used both (e.g., "coffee, 1/2 bagel with cream cheese", P13).</s><s xml:id="_WPGNBY6">However, some aggregated or decomposed food inputs (92, or 27.8%) had unclear food descriptions, especially when lacking conjunctions or item separators.</s><s xml:id="_ZFssXyB">For example, descriptions like "coffee cinnamon rolls" (P2), "cup of soy milk small pastries" (P8), and "half apple chicken link" (P15), could  (a) Distribution of specificity of food items per participant (b) Distribution of amount description per participant be interpreted as flavors or varieties, or separate items that were combined into a single entry.</s><s xml:id="_WZCyJG5">For example, P5's description of "warrior chia bar cinnamon and apple" could be interpreted as a bar with cinnamon and apple flavor, versus bar with cinnamon flavor and a side apple.</s><s xml:id="_vsYa5um">Database entries were more likely to include more specific items than either text descriptions or voice descriptions (Z=4.68,</s><s xml:id="_VPxXhRu">p&lt;0.001, 95% CI 28%-87% more likely).</s><s xml:id="_CxCjJV5">Participants explained that they expected that the input to database searches would need to be specific.</s><s xml:id="_7DcqXeX">For example, P14 said, "if it's lasagna, there's going to be 10,000 homemade lasagna's in the database", expressing that the more generic description could have varied nutrient information or ingredients.</s><s xml:id="_umHKYgY">P5 sought to circumvent this by decomposing foods into their elements, she said that "you start by [searching] ingredients [. .</s><s xml:id="_D3bmpQ7">.]</s><s xml:id="_ZQdwbBD">So, when I use the search function for those, I would get the accurate calorie counts".</s><s xml:id="_6uAPaD5">However, "it's annoying to find all the ingredients" (P14) and can be a "chore to log your food, and to be accurate with it" (P12), revealing tension between accuracy, effort and specificity for this modality.</s></p><p xml:id="_A4KcnXF"><s xml:id="_akBpGcf">Participants described simply not being able to add more specificity in some circumstances.</s><s xml:id="_2gfK46K">P12 reflected that sometimes when eating at a restaurant, "you can't be as finely detailed, unfortunately.</s><s xml:id="_6nkQDzH">generic food logs such as "random salad greens" and "random greens in tortilla".</s><s xml:id="_YjfDFmX">Burdens of entry occasionally led participants to be less specific than they wanted to be.</s><s xml:id="_5BjCDJW">For example, participants reported that issues with VAs not hearing or understanding them led to repetition and frustration, and also felt it led them to speak shorter descriptions in hopes of decreasing errors.</s><s xml:id="_U35mVhR">For instance, P10 said, "as you increase the amount of time using voice input, the chances for the number of errors that you would have using the voice entry increases.</s><s xml:id="_fX2c4NM">So, I felt compelled to use only short phrases that I could really enunciate and that I believe would be easily recognizable by the voice assistant.</s><s xml:id="_jprCxJV">" Similarly, P2 said that, "I got the impression with it that the command had to be really short, that if there was too long of a pause it would just [finish] recording what it was".</s><s xml:id="_VhPSmCA">This aligns with previous work identifying that people often shorten and simplify their sentences in the hopes of being better understood by voice interfaces <ref type="bibr" target="#b11">[14]</ref>.</s></p><p xml:id="_yfGBbDX"><s xml:id="_hnPkhWQ">When journaling with images, participants used different strategies to depict the specificity of their foods.</s><s xml:id="_jXkZnK2">Most images were of foods just about to be eaten, such as on plates and in wrappers (43/60).</s><s xml:id="_HnfQ2Vk">Some images were pictures of food packages (7/60), others were stock images retrieved from the web (10/60).</s><s xml:id="_ku753yE">P10 purposefully took a picture of his food alongside the receipt while eating at a restaurant to better describe what he ate (Figure <ref type="figure" target="#fig_4">4a</ref>).</s><s xml:id="_9jnQRKs">In another situation, he took a picture of an empty plate (Figure <ref type="figure" target="#fig_4">4b</ref>) alongside a text input describing its consumed content "cut fruit and 1/2 a panera berry danish".</s><s xml:id="_WddErhQ">P8 would often journal with stock or menu images found online, especially for restaurant meals and when journaling long after eating (e.g., Figure <ref type="figure" target="#fig_4">4c</ref>).</s></p><p xml:id="_TUXVdZz"><s xml:id="_XG9Hsej">Most images had clear identifiable composition (54/60).</s><s xml:id="_TKg4CxF">For example, packaged food images (e.g., Figure <ref type="figure" target="#fig_4">4d</ref>, <ref type="figure" target="#fig_4">4e</ref>) had clear food details in text.</s><s xml:id="_7FA2TNh">Unclear food images typically had indistinct ingredient mixes (e.g., liquid substances in Figure <ref type="figure" target="#fig_4">4f</ref>, <ref type="figure" target="#fig_4">4g</ref>), an empty plate (Figure <ref type="figure" target="#fig_4">4b</ref>), or food inside unlabeled wraps.</s><s xml:id="_jmFMPkX">Participants typically focused and arranged foods to be fully captured in their photos (45/50 non-stock images).</s><s xml:id="_ceYqcHE">In a few cases, photos modeled the food arrangement to better capture the exact amount or variety of foods eaten (4/50 non-stock images), such as Figures <ref type="figure" target="#fig_4">4h</ref> and <ref type="figure" target="#fig_4">4i</ref>.</s><s xml:id="_UjcH2d6">P7 explained that Figure <ref type="figure" target="#fig_4">4h</ref>'s arrangement was because she "didn't want to list all the foods I was consuming.</s><s xml:id="_4DFar3K">Plus, I thought the presentation Single Food "soy milk 18 oz" "2 cuties" "small plate of seasoned almonds" "spaghetti carbonara" "2 tbsp chia seeds" "spaghetti squash .25"</s><s xml:id="_tdvtvkB">"handful walnuts" "granola" "Cheese 1 oz" "4 eggs" "a small glass of wine" "cereal" Decomposed Single Food "PB&amp;J / 1 low carb Mission tortilla; 1 tbsp jelly; 1 tbsp sunflower seed spread" "2 tortilla with butter and honey" "protein shake with almond milk 1 scoop protein powder" "collard wraps with turkey" "Trader Joe's, Tahini, Pepita, &amp; Apricot slaw kit (75g)" "[a] warrior chia bar cinnamon and apple" "cauliflower rice 2 servings" "sesame beef tacos"</s></p><p xml:id="_vuP9hjc"><s xml:id="_SvzjqCD">"banh mi 4 oz pork 2 eggs 6 in baguette"</s></p><p xml:id="_U9MPxAW"><s xml:id="_W7X63nj">"toast with jelly" "a bowl of rice and 3 meatballs" "chicken noodle soup"</s></p><p xml:id="_qpX3MnT"><s xml:id="_JZJ4JSt">Aggregated Foods "5oz 85% ground beef 2 tbsp sriracha 2 tbsp teriyaki; [. .</s><s xml:id="_bkbfvWS">.]" "1/2 Calzone with salad" "chips &amp; salsa, Marbella market samples, 1 spoonful each" "coffee and oatmeal" "baked potato with 1 tbsp butter" "[a] baked potato with butter" "medium cup vanilla flavor yogurt land with oreo" "locro with rice" "4 oz chicken thigh 1 tbsp Italian dressing salad mix" "half a sandwich in 2 oranges" "one avocado one bowl of mixed vegetable and a handful beef jerky" "cheese and wine" # of Food Items 226 331 106 907 looked nice."</s><s xml:id="_WqyA8WA">P6 used manual labeling on paper to discriminate the different varieties of salsa present in their meal, explaining that she arranged Figure 4i according to her lighting and that "I tried to be pretty thorough with my recording here.</s><s xml:id="_BvyAQuu">" P8 explained that for her aiming for aesthetics "has to do a lot with what I'm used to seeing too on Instagram or social media, nice food [. .</s><s xml:id="_eYwMEjb">.]</s><s xml:id="_zB57Mes">I would want to take a picture that would look nicer.</s><s xml:id="_X2YewUa">"</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_QGT8EPB">Amount</head><p xml:id="_Htypude"><s xml:id="_cE6j8JW">For descriptive inputs, participants used different methods to articulate how much they ate, such as using formal scales (14.3% of food items; e.g., cup, grams), numbers (21.0% of food items; e.g., "1 roma tomato", P15), and non-standard measures (6.7% of food items; e.g., serving, bowl, handful, slice).</s><s xml:id="_QFpAdcK">10.1% of aggregated or decomposed food logs used more than one strategy for describing amounts.</s><s xml:id="_nMxne2G">For instance, 33% of these mixed-amount inputs combined some food items measured using a formal scale with counted items (e.g., "[a] baked potato with 1 tbsp butter" P15).</s><s xml:id="_hD9pkst">More than half (64.4%) of mixed-amount inputs had a quantified food item alongside food items with no amount at all, such as ". . .</s><s xml:id="_7fduXBB">plain burger, fries, 1 glass dry white wine" (P5), "raisin bran [cereal] and an egg" (P4), and "3 catfish tacos, corn tortillas, salsa" (P3).</s><s xml:id="_KnWVqRR">One explanation is that some foods are more difficult to count or quantify than others, especially foods that are small, numerous, or liquids.</s><s xml:id="_BjvEEUH">Participants structured their amount descriptions in various ways, reflecting however they preferred to describe their foods.</s><s xml:id="_gSDHgnA">287 inputs had amount description in front of each food item (e.g., "18 oz silk vanilla soymilk 1 oz chia seeds", P10), while 88 indicated amount after the item (e.g., "curried chicken sandwich .75",</s><s xml:id="_d8q5dMv">P14). 2 inputs used one amount to reference every item (e.g., "2 spoonful each of roja, fiesta, roasted", P6).</s><s xml:id="_zS3JXMa">Participants occasionally used quantity descriptions in ways that they could interpret, even if not exact measures.</s><s xml:id="_XtfbTev">For example, P6 acknowledged estimation by adding "∼" to scale amounts (e.g., ". .</s><s xml:id="_KDhwwFw">.∼2 oz.</s><s xml:id="_PhCqP6X">salmon; ∼.5c weed greens; ∼.25c avocado").</s><s xml:id="_s3sJCgX">Similarly, non-standard amounts could be imprecise, referencing food containers with variable sizes such as plate, spoonful, bowl, scoop, or glass, as exemplified in Table <ref type="table" target="#tab_2">3</ref> Although rare (7 inputs), participants occasionally referenced other known sizes in amounts.</s><s xml:id="_QTCJxPb">For example, P6 fractioned a package ".25 pkg Trader Joe's Asian noodle salad [. .</s><s xml:id="_eAAGhkw">.]" and a bottle "Health-Ade Kombucha, pink lady apple, half bottle".</s><s xml:id="_wXSAGrz">Similarly, P5 referred to a personally-known package size: "lays baked chips subway size".</s><s xml:id="_PCV6CcN">In other inputs, food amount used subjective qualifiers such as "large", "big", and "small" (16 inputs), or related to portion sizes with "slice", "cut", "entire", and "serving" (34 inputs).</s><s xml:id="_nnqtnZQ">For example, "one small vegetarian pizza" (P11), "2 slices of honey turkey breast" (P15), "entire pizza" (P5), and "bacon .4</s><s xml:id="_HzeJHGS">serving" (P14).</s><s xml:id="_k5YQMYu">22 inputs used "handful" for snacks or ingredients.</s><s xml:id="_DbXJdJn">Variations included fractions (e.g., "half handful almond", P11), or other qualifiers ("small handful craisins", P15).</s></p><p xml:id="_sV9Xb8T"><s xml:id="_FRcY2sr">More than half (57.5%) of described food items had no amount clarification, and we observed no significant difference in the rate at which amounts were clarified between voice, text, or database search input modalities (p=0.36).</s><s xml:id="_9FcgB8K">Participant's goals typically influenced their decision for choosing whether to describe their food amounts.</s><s xml:id="_FW8sD74">Typically, participants that had weight management, nutrient, or calorie-focused goals mentioned a desire for measuring their foods.</s><s xml:id="_wQAdupw">For instance, P12 said, "If you're trying to be really, really anal and accurate, you've got to remember these grams".</s><s xml:id="_nQpYGvZ">Likewise, P5 preferred measuring her foods, explaining that she "wanted to make sure I get correct amount of fats logged" when journaling "28 gram mozzarella" with a scale amount.</s><s xml:id="_XjGbbJx">In contrast, P3 was primarily interested in becoming more aware of his eating habits and explained that "the way I would log would be more just what I ate rather than a quantity [. .</s><s xml:id="_ExqvfsH">.].</s><s xml:id="_KTF2qbB">I would just put what I did with some qualitative things, 'I had a small plate of this', or 'I had a couple of this'.</s><s xml:id="_5dQZEUw">I wanted to make it easy to log.</s><s xml:id="_KW6FwCV">[...].</s><s xml:id="_JVQBRXt">Just a description of the meal, rather than getting into, 'I had three eggs and 200 grams of ham and blah blah blah'."</s><s xml:id="_BthfcjV">P13 similarly aimed to be "cognizant of what they're eating" and felt less of a need to clarify the amounts they ate.</s><s xml:id="_ztJhBvn">Overall, participants with quantitative goals were more likely to clarify the amount of food they ate in an entry than participants with awareness goals (Z=1.71,</s><s xml:id="_ku9VCDg">p&lt;0.05, 95% CI 14%-328% more likely).</s><s xml:id="_cEys4CE">Participants variated substantially in how often their food item descriptions included amounts, with 7 indicating amount in less than 25%, 3 indicating amount in more than 75%, and the remaining 5 in between.</s><s xml:id="_Q2CgR3D">This variance is illustrated in Figure <ref type="figure" target="#fig_3">3b</ref>.</s></p><p xml:id="_npEPCfq"><s xml:id="_uFaMWU9">Participants also used images to convey food quantity, usually through an angle which enabled determining volume and container dimensions.</s><s xml:id="_hU3HWzN">P9 felt that "the picture would be a way to document that in terms of how many servings I had".</s><s xml:id="_swSzYNV">A sense of quantity was usually present in images that had full plates (e.g., Figure <ref type="figure" target="#fig_4">4g</ref>), bowls (e.g., Figure <ref type="figure" target="#fig_4">4f</ref>, <ref type="figure" target="#fig_4">4g</ref>), cups (e.g., Figure <ref type="figure" target="#fig_4">4j</ref>), or food units (e.g., food wrap in Figure <ref type="figure" target="#fig_4">4a</ref>, chocolate square in Figure <ref type="figure" target="#fig_4">4k</ref>, a banana in Figure <ref type="figure" target="#fig_4">4j</ref>).</s><s xml:id="_RSesmbH">Most images conveyed some form of food amount (57/60).</s><s xml:id="_jeGcqYQ">However, even in cases where the foods in pictures could represent a single serving, they typically left no indication that the full amount of food shown in the image was eaten.</s><s xml:id="_uDtzJdH">For instance, P2 had a plate with pasta, but later revealed in a survey answer, "I took a picture of the meal and then I forgot to eat it".</s></p><p xml:id="_yRHphGc"><s xml:id="_ynB8Tr7">Pictures or stock images of packages usually had visible weight descriptions on the packages, but they were sometimes still ambiguous around how much was consumed.</s><s xml:id="_bEm7thU">For instance, although it could be reasonably assumed that small snack packages were fully consumed when a person journaled after they ate (e.g., the protein bar wrapper in Figure <ref type="figure" target="#fig_4">4d</ref>), most other packages were larger (9/12 of picture and stock images).</s><s xml:id="_gPwxbxx">For example, P11 uploaded the same stock photo of a 2 lb.</s><s xml:id="_mxadyV5">bag of oatmeal (Figure <ref type="figure" target="#fig_4">4e</ref>) for multiple entries.</s><s xml:id="_VccjEst">Barcodes similarly varied in whether amounts could be reasonably inferred.</s><s xml:id="_Y4NphbZ">For instance, the barcode input of a 330 ml.</s><s xml:id="_bQDrsSr">Vita Coco coconut water bottle (P7) could be assumed as fully consumed, whereas a barcode of a 24 oz.</s><s xml:id="_6cmEfbJ">Kellogg's Raisin Bran cereal box (P4) is unlikely to be.</s><s xml:id="_5pyKDpy">In a few journal entries <ref type="bibr" target="#b3">(5)</ref>, participants clarified this ambiguity by combining inputs to detail actual eaten amounts of barcode foods.</s><s xml:id="_FJgk3Uu">For instance, P6 made a barcode input of an 8count tortilla package combined with a text input of "Salmon wrap (2) ∼1c.</s><s xml:id="_Ftpe4wf">marinated salmon ∼.5c microgreens [. .</s><s xml:id="_V5KxqYV">.]", likely indicating that two tortillas were eaten.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" xml:id="_dbtvHYh">Context</head><p xml:id="_pkhSeyh"><s xml:id="_KAp4wC3">Participants included contextual information related to the food and eating event in a few entries (4.9%).</s><s xml:id="_U7UyEnJ">21 of these inputs had implicit or explicit indications of where the person ate the food.</s><s xml:id="_ZVuVqrP">For example, P5 mentioned in a text input making a homecooked meal, "quick homemade stir fry sauce; 15 calories", versus another meal in a different place, "dinner at friends house, bbq chicken with mac and cheese".</s><s xml:id="_rUYpHea">Implicit locations were present in inputs with foods from restaurant, such as "Chinese takeout chowmein beef broccoli" (P2), and "WABA grill: salad, brown rice, chicken, beef, . .</s><s xml:id="_swfgxWW">." (P3).</s></p><p xml:id="_6Bgudwp"><s xml:id="_CukuZf7">Like descriptive inputs, some images (14/60) also had implicit location indications.</s><s xml:id="_jt3cnnQ">For example, participants journaled images with background elements such as desktop computers at the office or at home (e.g., Figure <ref type="figure" target="#fig_4">4f</ref>, <ref type="figure" target="#fig_4">4l</ref>), kitchen appliances (e.g., Figure <ref type="figure" target="#fig_4">4m</ref>), restaurant names and time of meals on receipts (Figure <ref type="figure" target="#fig_4">4a</ref>), and other household objects like living room tables and TV controllers.</s><s xml:id="_F3rNAaF">Similar to Cordeiro et al.'s findings <ref type="bibr" target="#b27">[30]</ref>, participants used images in this way to improve interpretability for reflection.</s><s xml:id="_MdKMZ6h">However, participants seemed to focus more on capturing the nuances of their food than on capturing context.</s><s xml:id="_65vpzz9">For instance, P10 said that he thought images "sufficient for me to effectively reconstruct or eyeball how much protein I have that day", similar to P5 that said images were useful to "remember what you ate".</s><s xml:id="_aaZQCQe">Unlike Cordeiro et al. 's <ref type="bibr" target="#b27">[30]</ref> participants, none of ours took a picture of others present during a meal.</s></p><p xml:id="_PgumRyd"><s xml:id="_EZ33HRk">Participants also gave contextual information that gave more detail about when they were eating.</s><s xml:id="_VFxscYd">For example, 12 inputs specified the type of meal, such as snack (e.g., "jubes snack", P3), dinner, dessert (e.g., "Persian dessert, many", P6), lunch (e.g., "lunch: slice of pizza with side salad", P13), and supplement (e.g., "supplement set / 3 multi, 2 fish oil, 2 D3 [. .</s><s xml:id="_6TpxbQF">.]",</s><s xml:id="_H8J38BY">P6).</s><s xml:id="_RMd7M96">Other inputs <ref type="bibr" target="#b32">(35)</ref> gave glimpses at participant's routines.</s><s xml:id="_V4rp5X2">P3 would often tag mealtimes when journaling long after he ate, such as "(meal eaten at 10 PM Saturday March 21st) Pasta with broccoli", and recurring food, such as "same pasta, chicken, veggies, and now hot sauce" and "[. .</s><s xml:id="_DwUZj4A">.] leftover chashu and bok choy".</s><s xml:id="_N8g6dx8">Similarly, P6 had a particular recurring meal that she would label as a daily mixture, detailing its varying contents each time: "daily mix / 3 multi, 1 elderberry, 4 fiber".</s></p><p xml:id="_Z2ejAyB"><s xml:id="_BnDmrcf">Some participants (P2, P6, P8, P12, P14) indicated that knowing meal contexts would help them reflect on their eating behaviors.</s><s xml:id="_6UDHS8m">For instance, P6 mentioned a desire to "explore my emotions around my food emotions [. .</s><s xml:id="_CpQwskU">.] because I'm really interested in how food would impact emotions or how my emotions impact what I eat", and suggested that this could be through "writing and answering a questionnaire or photos ".</s><s xml:id="_DUyFHHT">Similarly, P14 said that capturing context was lacking in her past journaling experience and could have given more insight for her food choices.</s><s xml:id="_8h2RnZZ">She said: "I have in the past thought about when I look back on my journal, on MyFitnessPal, [that] I can identify things that I felt good about eating and things that I sort of felt like, 'well that was a bit of a waste'.</s><s xml:id="_ZFG8PBJ">[I would like] Having a bit of context, if there was a way to easily visualize that somehow to sort of know, because what I would think I might find is I eat a bunch of crap, I don't need to eat late at night or during a stressful day or something like that."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5" xml:id="_zANCCS5">Automatic food interpretation</head><p xml:id="_X8e8CYT"><s xml:id="_69mfuUR">Participants expressed interest in leveraging automatic interpretation of food descriptions logged.</s><s xml:id="_S2uR2VZ">Nine participants (P2, P5, P6, P10, P11, P12, P13, P14, P15) wished that VAs could execute a background database search on described foods during the conversation or for later reflection.</s><s xml:id="_hpYCY2V">For instance, P11 said, "I wish the [Alexa] VA can figure out the total calories of the food after I tell her what kind of the food I had and the quantity of the food.".</s><s xml:id="_fPYj7eg">There were similar requests for interpreting text descriptions and images.</s><s xml:id="_cnXHpff">P9 wished that text inputs would retrieve nutritional information, combining with database search, saying "a blend of those two [db search and text input] would be great [. .</s><s xml:id="_ynxWmjN">.] it would give me the option of providing me the additional nutritional facts about each of the items that was in my [text] description".</s><s xml:id="_sTqs8Da">P9 suggested a similar feature for capturing food composition from images, comparing to Shazam, a popular music classification app: "it would be like the Shazam of food [images].</s><s xml:id="_GzzzYte">[. .</s><s xml:id="_NpFxBX5">.]</s><s xml:id="_Zz2YQhp">I think that would certainly add additional value".</s></p><p xml:id="_SGAfRbd"><s xml:id="_7pdHvV5">Current automated approaches were overall successful given how participants desired journaling their foods, but had some limitations depending on how participants wished to structure their entries.</s></p><p xml:id="_kjZ33nK"><s xml:id="_R5RgkeA">4.5.1 Interpretation of natural language food descriptions.</s><s xml:id="_CQkWRnc">Overall, food items were generally identified correctly by the NLP systems we tested, with 80.7% of descriptive entries correctly being interpreted and returning relevant nutritional information for every food item or component (e.g., calories, micro and macro-nutrients).</s><s xml:id="_7pY8fnE">For example, the entry "eggs in cheese sauce over English muffin with coffee" (P13) was interpreted as four separate ingredients: "eggs", "cheese sauce", "English muffin", and "coffee".</s><s xml:id="_pPtNvP8">The remaining 19.3% inputs were not fully interpreted correctly, but 77.9% of these had at least one food item that was correctly identified.</s><s xml:id="_AhBaDSM">For instance, "4 oz chicken breast 3 oz spinach 125g tamaki haiga 2 tsp soy sauce" (P10) had all items identified except for "tamaki haiga".</s><s xml:id="_TYtkPST">Overall, only 4.3% of inputs completely failed, either not matching any items (14 inputs) or wrongly identifying foods (24 inputs), such as "2 tablespoons salad topper" (P15) being classified as "salad".</s><s xml:id="_GcGUpmG">Six of the non-matched foods were direct references to brands, such as "2 square 70% lindt" (P5), while four others were ethnic foods such as, "chapaguri" (P7).</s></p><p xml:id="_NwBqYga"><s xml:id="_QgDFSbt">Modality impacted the rate at which inputs were accurately interpreted (χ 2 (2, N=890)=36.91,</s><s xml:id="_jZgamKq">p&lt;0.001).</s><s xml:id="_MJa5nCF">Text inputs were less likely to be interpreted correctly than voice inputs or database searches (Z=-5.70,</s><s xml:id="_Yp4tYXX">p&lt;0.001, 95% CI 49%-121% less likely).</s><s xml:id="_6PqnrVQ">Text inputs had greater opportunity for at least one item not being understood due to most entries being aggregated foods, versus database searches and voice inputs that had a majority of single food inputs (Figure <ref type="figure" target="#fig_3">3a</ref>).</s><s xml:id="_Zk5vr3Q">79.4% of text inputs had at least one item correctly understood.</s></p><p xml:id="_47UaKjK"><s xml:id="_4mk2amv">Specificity also impacted in interpretability of food descriptions (χ 2 (2, N=890)=36.39,</s><s xml:id="_UthKK3j">p&lt;0.001), with specific foods more likely to be interpreted than either generic or varietal foods (Z=4.74,</s><s xml:id="_PmwRkQ2">p&lt;0.001, 95% CI 32%-98% more likely).</s><s xml:id="_8FmJjtf">Many varietal descriptions used adjectives to describe food names, which could lead to misinterpretations and ambiguity.</s><s xml:id="_38HBZq7">For instance, the voice input "chicken eggs and avocado" (P10) was interpreted as "chicken eggs" and "avocado", but could alternatively be chicken meat (e.g., "chicken and rice 4 oz" P10) and not a description of egg type.</s><s xml:id="_aM4c73Y">Other examples include "salmon cakes . .</s><s xml:id="_AP266TS">." (P14), "peanut butter muffin" (P12), and "banana tea" (P9).</s><s xml:id="_CA7HDVD">Similarly, decomposed foods that had a food name followed by individual ingredients could be counted twice.</s><s xml:id="_gTHCnyH">For instance, "pasta / 3 oz.</s><s xml:id="_TnYdjvN">edamame spaghetti, 4 variety tomato, .5 tbsp.</s><s xml:id="_rbteFxh">olive oil, [. .</s><s xml:id="_2tk4wwH">.]" (P6) was interpreted as general pasta as well as edamame spaghetti, tomato, and so forth.</s></p><p xml:id="_ePRFC9t"><s xml:id="_geBKhPH">Most descriptive inputs had food items where amount was specified, but amount interpretability depended on how it was described.</s><s xml:id="_ZcMXxFA">Scale and numeric descriptions had 78.7% and 80.0% of inputs completely and correctly interpreted, while non-standard measures were correctly interpreted in about half of inputs (53.8%).</s><s xml:id="_BcKpPB9">Some of the non-standard measures could be occasionally understood and return estimated nutritional metrics, such as bowl, spoonful, bottle, plate and scoop.</s><s xml:id="_ea9bh4F">However, "handful" was not captured as a measure in any of its 23 occurrences.</s><s xml:id="_Pgrq8Gv">Inputs where the amount was not clarified tended to return a default scale measure estimated by serving size, such as "cereal with milk" (P13) being assumed as 1 cup each.</s><s xml:id="_aVGZYVn">Similarly, "serving" was also mapped to default measures, such as "bacon .4</s><s xml:id="_DnhBsRy">serving" being record as 0.4 of unit "slice".</s><s xml:id="_ju4PcJx">4.5.2</s><s xml:id="_EZde7GY">Classification of food images.</s><s xml:id="_V26haug">Most images had at least one food composition identified by the Clarifai service (42/54) with probabilities above 0.8, versus CloudVision and Rekognition that correctly identified components in 22/60 and 21/60, respectively.</s><s xml:id="_AddFmrU">However, the latter two services accurately identified background elements in 14 images (e.g., table, keyboard) and food containers in 36 images (e.g., plate, bowl), whereas Clarifai did not identify these elements at all.</s><s xml:id="_JkmCZFA">We based our analysis on food identification on Clarifai's results and background elements on results from CloudVision and Rekognition.</s></p><p xml:id="_SFvvDZU"><s xml:id="_mm8cgUW">Participants frequently took photos of packages, wrapped foods, or uploaded stock photos of food items, representing a third of the images participants uploaded <ref type="bibr">(19/60)</ref>.</s><s xml:id="_Uy7aeTH">Non-stock images of packages were mostly classified correctly (5/7), such as Figure <ref type="figure" target="#fig_4">4d</ref> being classified as "chocolate" (1.00), "candy" (0.98), "sweet" (0.91).</s><s xml:id="_334tzFd">However, shape and color occasionally influenced the recognition, with the cookie snack in Figure <ref type="figure" target="#fig_4">4l</ref> classified as "beer" (0.96), "bacon" (0.95), "cake" (0.92), or "chips" (0.83).</s><s xml:id="_7PPxuab">7/10 stock images had key components identified, such as Figure <ref type="figure" target="#fig_4">4e</ref> that had "oatmeal" and "cereal" among high-confidence food items.</s><s xml:id="_MGRy25u">However, the three other stock image inputs, two inputs of "Dried mango slices" and one of "Apple Smoked Bacon", had food names written on the package but failed to be correctly classified.</s><s xml:id="_NNpKSx6">This may be because pictures of the food items were not prominently displayed on the packaging, although other similar packages were classified correctly (e.g., Figure <ref type="figure" target="#fig_4">4e</ref>).</s><s xml:id="_Z9XsJdk">Neither of the 2 photos of wrapped foods were correctly identified, with food composition suggestions being influenced by the package and components of the background.</s><s xml:id="_aSRDDPx">For example, Clarifai incorrectly classified the wrapped sandwich in Figure <ref type="figure" target="#fig_4">4a</ref> as "chocolate" or "cake" with high confidence (0.92 and 0.88), whereas CloudVision and Rekognition identified the "aluminum foil" (0.98) rather than the sandwich.</s></p><p xml:id="_ujY2PKG"><s xml:id="_n76Z8sR">As expected, images with unclear food composition (6) were not well-classified by models.</s><s xml:id="_TmcSRzc">However, images of foods that had clear identifiable composition and that were not stock or packaged foods were mostly classified correctly (29/33).</s><s xml:id="_rq4fzWN">For example, Figure <ref type="figure" target="#fig_4">4i</ref> was classified as "salsa" (0.97), "corn" (0.96), "vegetable" (0.95), "tortilla chips" (0.87), "pepper" (0.86), "tomato" (0.84), and "chili" (0.81).</s><s xml:id="_Nq28vsY">In contrast, Figure <ref type="figure" target="#fig_4">4n</ref> also had tortilla chips and salsa but was incorrectly classified as "bread" (0.84) "peanut" (0.82), or "peanut butter" (0.81).</s><s xml:id="_HUjEa5w">This demonstrates that although generally images were correctly classified, inconsistencies around recognition make the method appear unpredictable and unexplainable.</s></p><p xml:id="_QScAjKb"><s xml:id="_Bpkeqx8">Several background elements in 14 images were correctly identified, such as computer keyboards (e.g., Figure <ref type="figure" target="#fig_3">3l</ref>, 0.99 probability), screens (e.g., Figure <ref type="figure" target="#fig_4">4f</ref>, 0.92 probability), tables (e.g., Figure <ref type="figure" target="#fig_4">4f</ref>, 0.64 probability), and even a kitchen oven in Figure <ref type="figure" target="#fig_4">4m</ref> (0.57).</s><s xml:id="_dK3xfuT">Food containers were also mostly identified <ref type="bibr">(36/38)</ref>, such as bowls (e.g., Figure <ref type="figure" target="#fig_4">4g</ref>, 0.95 probability), plates (e.g., Figure <ref type="figure" target="#fig_4">4b</ref> and 4i, 0.62 and 0.60 probabilities), cups (e.g., Figure <ref type="figure" target="#fig_4">4j</ref>, 0.98 probability), and the blender in Figure <ref type="figure" target="#fig_4">4m</ref> (0.98).</s><s xml:id="_CmrBp4d">However, the bowl in Figure <ref type="figure" target="#fig_4">4f</ref> and the cake pan in Figure <ref type="figure" target="#fig_4">4o</ref> were not detected, either because other background elements were identified with higher confidence (display 0.92, screen 0.92, monitor 0.92 . .</s><s xml:id="_wZyzT2g">.) or the food itself was recognized (chocolate 0.97, fudge 0.80, . .</s><s xml:id="_VXXuwtF">.).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_Q4yeFVR">DISCUSSION</head><p xml:id="_uQ5psew"><s xml:id="_mucm8ky">Through deploying ModEat, we observed that when provided with a range of input modalities less constrained by recognition methods than current commercial tools and research prototypes, participants had high variance in how they preferred to describe their foods.</s><s xml:id="_egJEHJa">Participants varied in how they described their foods collectively and individually, ranging from single to aggregated inputs with varying levels of specificity and detail, often describing food composition and varieties but sometimes created less specific entries.</s><s xml:id="_Y5TE725">Participants also varied in how and whether they described amounts, either with numbers or formal scales but also with subjective references that made personal sense.</s><s xml:id="_sw6qaH2">Our study also suggests that automatic NLP for food description can identify food elements people describe fairly well, and classification services for image recognition can identify some food elements in most images.</s><s xml:id="_Dpt8eq2">However, our findings also suggest that these automatic recognition services are optimized for identifying well-formed descriptions over free text, and plated food rather than the packages or stock photos participants often showed when journaling.</s></p><p xml:id="_993nXUU"><s xml:id="_3Gz2x2T">Reflecting on our results, we consider implications for designing both to mitigate and leverage the high levels of variance in how people prefer to describe foods.</s><s xml:id="_z8N7wjm">We also reflect on implications of automatic food interpretation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_tuTBCV9">Designing to Mitigate Variance in Food Descriptions</head><p xml:id="_QmRAxdR"><s xml:id="_zm99ndW">Prior work in personal informatics have suggested that technology can better support people's goals through customization or flexibility about what is recorded, allowing people to align self-tracking to their needs <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b50">54]</ref>.</s><s xml:id="_2QuCQfc">However, we have also observed that supporting flexibility during data collection led to great variance in food description styles, some of which are not precise or introduce ambiguity.</s><s xml:id="_APyNCa7">Ambiguity, in turn, can lead to challenges when data might want to be reviewed or reflected on later.</s><s xml:id="_y4mMMb7">This can be a particular issue for those with goals related to quantifying nutritional aspects of food consumption.</s><s xml:id="_p8qdu8z">Ambiguous food descriptions can lead to inaccuracy in food metrics, uncertainty about nutritional information, and ultimately not allowing for more precise reflection on progress toward a quantitative goal (e.g., Did I surpass my calorie budget?</s><s xml:id="_twwzv8x">Have I consumed my protein quota for the day?).</s><s xml:id="_M5s8F2n">Likewise, completely unstructured inputs run the risk of introducing enough uncertainty that days or weeks later, people with awareness goals (e.g., learning about eating habits) might not be able to interpret their logs.</s></p><p xml:id="_KFQGmYm"><s xml:id="_G7KeM62">To mitigate ambiguity and uncertainty in logs, food journaling systems could encourage inclusion of food granularity, specificity, and amount by surfacing what was recognized and enabling correction or incrementation.</s><s xml:id="_btPpDKf">Although recognition libraries often attempt to estimate nutrient values for ambiguous foods and those where amounts are unspecified, these values could be off from the reality of what a person consumed.</s><s xml:id="_vSFHmsc">Compared to gold-standard clinician-assisted 24-and 48-hour recalls, commercial food journals typically underestimate foods consumed <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b19">22]</ref>.</s><s xml:id="_nCsPwea">Current food databases show portion, calorie, and nutrient estimates based on what they search for, allowing them to edit or confirm prior to entry.</s><s xml:id="_NuHaerC">Implementations of image recognition libraries or voice journaling could operate similarly, asking a person to confirm whether a food was correctly identified and how much was eaten.</s></p><p xml:id="_NEWg3tH"><s xml:id="_UdCHPKk">For people with quantitative food goals, conversational journaling could enable prompting for greater specificity or clearer amounts to produce journals which more accurately represent what a person ate.</s><s xml:id="_WyKjuG3">We observed that generic food descriptions were often foods which could vary widely in calorie and nutrient information, such as "pizza" varying by toppings, slices eaten, and slice size.</s><s xml:id="_gxuRqCb">When someone with a goal that requires accurate metrics creates an entry that contains ambiguous characteristics, conversational journals could detect and interact to highlight the issue (e.g., report that an amount is missing) and offer suggestions to clarify or increase details of the entry (e.g., ask if the estimated or standard serving quantity for that food is accurate).</s><s xml:id="_9Wc8k3f">This type of feedback could trigger more mindful consideration of foods consumed <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b34">37]</ref>, but requires careful consideration for balancing improving journal entry detail with burden <ref type="bibr" target="#b20">[23]</ref> and feelings of judgment <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_3mjNKBP">Conversational journaling could further adapt to people's use of non-standard amount measures.</s><s xml:id="_Y9Ag5KA">For instance, if someone uses a personally-meaningful reference point, the journal could work with the person journaling to jointly estimate portion size, such as comparing with an object with relatively standard proportions (e.g., a tennis ball) and remembering that estimate for future logs <ref type="bibr" target="#b17">[20]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_8JgbNBz">Designing to Allow for and Leverage Variance in Food Descriptions</head><p xml:id="_tupS9nA"><s xml:id="_5YrKS7f">Although uncertainty and inaccuracy is often seen as a negative in journaling and self-tracking <ref type="bibr" target="#b21">[24]</ref>, methods aimed to address variance could introduce burdens to the already-demanding food journaling domain.</s><s xml:id="_JZwHBbw">Allowing for flexibility or even treating variance in how people prefer to describe their food as a design opportunity could allow systems to support people's journaling goals without introducing unnecessary demands.</s><s xml:id="_taKEcar">Supporting variance may be particularly beneficial for people with awareness goals, who may have less of a need for a detailed or accurate record of what they ate.</s></p><p xml:id="_gKtTXDm"><s xml:id="_rzDeqKx">While designing to effectively support the creation of accurate calorie or nutrient logs will ensure completeness and decrease variance, it has the downside of imposing structure on the data a person must enter.</s><s xml:id="_f9Fed7E">Beyond using conversational approaches or clarification questions to add detail about the food a person is eating, journals could aim to flexibly support adding different kinds of further detail.</s><s xml:id="_gkp4B9U">For example, a journal could provide open-ended fields for a person to include contextual information (e.g., whether eating alone or with others), information which could promote later reminiscence (e.g., how they are feeling, a memory associated with that food), or anything else they wish the journal to know about their food.</s><s xml:id="_6HHWmsD">This could potentially balance desires for flexibility and detailed entries, leaving people to journal however they prefer and perhaps support awareness alongside quantitative journaling goals.</s></p><p xml:id="_5ZnXvhE"><s xml:id="_KCuMJxT">Past work has suggested that interpreting ambiguous journal entries can also provide an opportunity for deeper reflection around the circumstances under which the data was collected <ref type="bibr" target="#b2">[3]</ref>, potentially highlighting the social and cultural celebratory nature of food <ref type="bibr" target="#b43">[47]</ref>.</s><s xml:id="_RJFdUpb">For instance, we observed that the presence of others during food journaling impacted how foods were described (e.g., aggregating foods in a single description).</s><s xml:id="_S9JPzYe">From this perspective, food description ambiguity can positively serve as a means of highlighting people's positive interactions with foods and encourage reflecting on around the circumstances which surrounded such an entry.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_mnbu4hW">Implications of Automatic Food Interpretation</head><p xml:id="_4mT9Ebr"><s xml:id="_HEVVxXu">We found that commercial NLP and image classification services were reasonably successful in interpreting and identifying the foods that participants logged using their preferred strategies.</s><s xml:id="_MWCTeEj">Our results indicate that input structure influences NLP performance, with inputs with more specificity and standard amount descriptions more likely to be correctly recognized.</s><s xml:id="_eQCYDTd">We were surprised that most food descriptions were correctly interpreted and returned nutritional information for identified foods.</s><s xml:id="_vhxHah7">While this might be sufficient for quantitative-focused goals, commercial NLP systems were unable to interpret the contextual cues participants occasionally put in in food descriptions describing their location or social circumstances.</s><s xml:id="_uJjtbAA">Likewise, non-standard food descriptions that reference routines (e.g., "same as lunch. .</s><s xml:id="_naARHHj">.") or subjective amount descriptions, were also a challenge for automatic inference of logs, but are valuable information that people wanted to record.</s><s xml:id="_evjgUbE">Similarly, image recognition libraries traded off accuracy for identifying the foods in an image with accuracy for identifying contextual information, such as what room a person might be eating in or whether they are eating from a plate or another container.</s></p><p xml:id="_kHe22u6"><s xml:id="_FuVPpTh">Our results, as well as others, suggest that people intend to collect contextual information for later reflection <ref type="bibr" target="#b27">[30]</ref>, perhaps pointing towards an opportunity for recognition models that comprehend not only food items, but other data.</s><s xml:id="_NAymuf6">Incorporating models specifically trained for recognizing food in text and images together with other classification models (e.g., optical character recognition, more general object recognition models, barcode recognizers, amount classifiers) could enable adding such context as well as supporting recognition.</s><s xml:id="_KYD6vfx">Even if not identified with high detail or accuracy, these models could help point to fun or personally meaningful experiences <ref type="bibr" target="#b43">[47]</ref>, such as surfacing restaurant names or household objects visible in communal dining.</s><s xml:id="_qjQXXms">Further mining of information embedded in photo metadata or passively recorded (e.g., location, time) could further provide context to complement reminiscence and reflection for both quantitative and awareness goal groups.</s></p><p xml:id="_2jJ5m2k"><s xml:id="_mcJ3YFS">While automation is typically leveraged for food tracking towards calorie or nutrient goals, surfacing contextual elements from food photos and descriptions might also be beneficial for people with mindfulness and behavior awareness goals.</s><s xml:id="_cypNhNx">As food journals become easier to collect passively through automated sensing <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b66">70,</ref><ref type="bibr" target="#b85">89]</ref> or with lower journaling burden <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b27">30]</ref>, there are increased opportunities for reflecting on long-term logs.</s><s xml:id="_BjEPKtn">Photos and text descriptions can be difficult to aggregate, but automation can promote longer-term reflection or reminiscence by mining abstract concepts from these logs.</s><s xml:id="_RdrgbmT">For instance, people could be provided with a cloud of words with the names of frequent foods or food categories they journaled, or display a color gradient representing how the color of these foods has varied over time.</s></p><p xml:id="_ZJckbFV"><s xml:id="_fTxrHEZ">Food journaling can also benefit from image classification for increasing detail of food consumption.</s><s xml:id="_N8trH8p">People with calorie and nutrient goals could leverage this by confirming identified foods in the image, adding further specificity about the ingredient makeup, and possibly clarifying amounts.</s><s xml:id="_H7kreBw">Identified and confirmed foods could then be automatically searched for nutritional data in a database.</s><s xml:id="_jwy85Zz">Amounts could also be suggested based on contextual information present in the image, such as text on packages or food inside containers (e.g., plate, cup).</s><s xml:id="_feTbtu6">This semi-automated approach might potentially lower journaling time and effort <ref type="bibr" target="#b54">[58]</ref>, while still promoting engagement <ref type="bibr" target="#b20">[23]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_w4uQqUB">CONCLUSION</head><p xml:id="_crJj5cr"><s xml:id="_zEKG6dA">In deploying ModEat, a lightweight food journaling technology prototype, we have identified that participant's strategies for describing foods had high variance, ranging from granular to aggregated inputs, and different levels of specificity and ways of describing amounts.</s><s xml:id="_H55neDH">We also observed that food descriptions or images could also be ambiguous and often not clear as to actual consumed amounts.</s><s xml:id="_q3KtNqJ">The strategies which people use to create food logs were typically interpretable by recognition libraries, but were less successful for aggregated or less specific food inputs.</s><s xml:id="_pJTcdZC">Our findings point to opportunities for conversational food journaling to help mitigate variance by supporting adding further detail, but also for technology to leverage journaling variance to promote reminiscence.</s><s xml:id="_AMuAF3y">Leveraging automatic food interpretation can additionally lower journaling burden or add context, supporting increased value from long-term food logs.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_eG43tZF"><s xml:id="_fkzqRTc">Figure 1: Examples of modality inputs on ModEat phone and VA.</s></p></div></figDesc><graphic coords="4,187.94,194.75,97.64,55.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc><div><p xml:id="_ZYCa8Am"><s xml:id="_FdxSdqH">(a) Distribution of granularity per modality (b) Distribution of granularity per participant</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc><div><p xml:id="_6yaH6x5"><s xml:id="_zqTeuhu">Figure 2: Participants varied in the granularity they used to describe their foods in each modality, typically journaling a single item with database searches and often aggregating multiple items in a single input or detailing a food's ingredient composition with text and voice inputs.</s><s xml:id="_uhrxQuZ">Individually, most participants entries and journaled foods varied in granularity and specificity, though a few participants consistently created single-food entries.</s></p></div></figDesc><graphic coords="8,310.98,306.22,211.21,140.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc><div><p xml:id="_T82FxWe"><s xml:id="_9QPEPRc">Figure 3: Specificity of food detail varied per modality while amount tended to align with personal food journaling goal.</s></p></div></figDesc><graphic coords="8,58.93,306.88,211.20,139.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc><div><p xml:id="_k7Ex6NQ"><s xml:id="_e8A57yn">Figure 4: Participants had various styles of using photos to capture foods and eating events.</s><s xml:id="_gFt9WGR">Participants used photos indicate amounts of foods eaten by referencing containers, leveraged stock images to represent foods similar to what was eaten, intentionally laid for better recognition of foods eaten, and took photos of foods at different stages of being eaten.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc><div><p xml:id="_6cmAnEE"><s xml:id="_A89p5nN">Nearly all participants had prior experience using digital food journals, but had a mix of awareness and quantitative goals.</s></p></div></figDesc><table><row><cell>ID</cell><cell>Gender</cell><cell>Occupation</cell><cell>Age</cell><cell>Journaling Experience</cell><cell>How Journaled</cell><cell>Journaling Goal</cell></row><row><cell>P1</cell><cell>Female</cell><cell>Designer</cell><cell>36</cell><cell>4 years</cell><cell>Calendar</cell><cell>Awareness</cell></row><row><cell>P2</cell><cell>Female</cell><cell>Massage Therapist</cell><cell>35</cell><cell>2.5 years</cell><cell>Paper, LoseIt, MyFitnessPal</cell><cell>Quantitative</cell></row><row><cell>P3</cell><cell>Male</cell><cell>Civil Engineer</cell><cell>33</cell><cell>2.5 months</cell><cell>Spreadsheet</cell><cell>Awareness</cell></row><row><cell>P4</cell><cell>Male</cell><cell>Engineering Manager</cell><cell>38</cell><cell>1 month</cell><cell>Paper</cell><cell>Awareness</cell></row><row><cell>P5</cell><cell>Female</cell><cell>Student</cell><cell>28</cell><cell>3 years</cell><cell>MyFitnessPal, Self-made app</cell><cell>Quantitative</cell></row><row><cell>P6</cell><cell>Female</cell><cell>Student</cell><cell>25</cell><cell>∼10 months</cell><cell>Cronometer</cell><cell>Quantitative</cell></row><row><cell>P7</cell><cell>Female</cell><cell>Retail</cell><cell>30</cell><cell>3 months</cell><cell>Paper</cell><cell>Awareness</cell></row><row><cell>P8</cell><cell>Female</cell><cell>Accounting Clerk</cell><cell>27</cell><cell>1 month</cell><cell>Spreadsheet</cell><cell>Awareness</cell></row><row><cell>P9</cell><cell>Male</cell><cell>Engineer</cell><cell>31</cell><cell>-</cell><cell>-</cell><cell>Awareness</cell></row><row><cell>P10</cell><cell>Male</cell><cell>Student</cell><cell>28</cell><cell>2 years</cell><cell>MyFitnessPal</cell><cell>Quantitative</cell></row><row><cell>P11</cell><cell>Female</cell><cell>Researcher</cell><cell>50</cell><cell>∼2 months</cell><cell>FitDay</cell><cell>Awareness</cell></row><row><cell>P12</cell><cell>Male</cell><cell>Engineer</cell><cell>43</cell><cell>"On and off"</cell><cell>MyFitnessPal</cell><cell>Quantitative</cell></row><row><cell>P13</cell><cell>Female</cell><cell>Academic Librarian</cell><cell>44</cell><cell>2 years</cell><cell>MyFitnessPal</cell><cell>Awareness</cell></row><row><cell>P14</cell><cell>Woman</cell><cell>Student</cell><cell>33</cell><cell>3 years</cell><cell>MyFitnessPal</cell><cell>Quantitative</cell></row><row><cell>P15</cell><cell>Male</cell><cell>Drafting Design</cell><cell>31</cell><cell>2 months</cell><cell>MyFitnessPal</cell><cell>Quantitative</cell></row><row><cell cols="5">with more freedom to go beyond current recognition constraints.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc><div><p xml:id="_6Hy59CJ"><s xml:id="_bNGmzDg">Examples of granularity and specificity from participant food journal entries.</s></p></div></figDesc><table><row><cell></cell><cell>Generic</cell><cell>Specific</cell><cell>Varietal</cell><cell># of Inputs</cell></row><row><cell>Single Food</cell><cell>"Pizza"</cell><cell>"Chicken"</cell><cell>"4 oz chicken thigh"</cell><cell>560 (62.9%)</cell></row><row><cell></cell><cell>"tea"</cell><cell>"Milk"</cell><cell>"240 ml whole milk"</cell><cell></cell></row><row><cell></cell><cell>"mixed vegetables"</cell><cell>"Orange"</cell><cell>"Hawaiian beef "</cell><cell></cell></row><row><cell>Decomposed Single</cell><cell>"Random greens in</cell><cell>"Hummus and cheese sandwich"</cell><cell>"protein shake with almond milk 1</cell><cell></cell></row><row><cell>Food</cell><cell>tortilla"</cell><cell></cell><cell>scoop protein powder"</cell><cell></cell></row><row><cell></cell><cell>"vegetable soup"</cell><cell>"cauliflower rice 2 servings"</cell><cell>"Trader Joe's, tahini, pepita, &amp;</cell><cell>73 (8.2%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>apricot slaw kit"</cell><cell></cell></row><row><cell></cell><cell>"chips soup"</cell><cell>"a bowl of rice and 3 meatballs"</cell><cell>"drunken chicken noodles"</cell><cell></cell></row><row><cell>Aggregated Foods</cell><cell>"slice of pizza with</cell><cell>"coffee and oatmeal"</cell><cell>"pizza and chicken wings"</cell><cell>257(28.9%)</cell></row><row><cell></cell><cell>side salad"</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>"taco and burrito"</cell><cell>"spaghetti and half an orange"</cell><cell>"orange juice, egg roll biscuits"</cell><cell></cell></row><row><cell></cell><cell>"sandwich and</cell><cell>"peanut butter bagel and coffee"</cell><cell>"bean burrito and corn salsa salad"</cell><cell></cell></row><row><cell></cell><cell>steamed vegetables"</cell><cell></cell><cell></cell><cell></cell></row><row><cell># of Food Items</cell><cell>128</cell><cell>842</cell><cell>607</cell><cell></cell></row></table><note xml:id="_rEMW8Yn"><p><s xml:id="_YsgGhTK">using database search for single food items because she considered it as keyword input, while other modalities were more appropriate for inputting multiple items.</s><s xml:id="_j5CFdAf">She said, "Database was most useful generally because it's keywords.</s><s xml:id="_9P7Ujgg">So, a lot of the time I put 'cutie tangerine' because we have tons of those and quick keywords.</s><s xml:id="_9dzdkWT">'Banana', same thing.</s><s xml:id="_bymrGRm">[. .</s><s xml:id="_NXMMwh2">.]. [Text] Description was because I would eat several different foods at one time and didn't want to have to put a bunch of different database searches down one long entry."</s><s xml:id="_esAHJMq">Decomposed single food descriptions were provided most often in voice input (8.5%) and text (14.5%), but rarely in database searches (3.1%).</s><s xml:id="_Rf2xK4C">Aggregating foods was prominent in text (56.9%) and voice (39.5%) inputs, but infrequent in database searches (2.4%).</s><s xml:id="_DQjZN5g">P9 explained that he found it easier to journal in this way "when I want to record a handful of items, because I could just rattle off a bunch of things I ate [to voice assistant] [. .</s><s xml:id="_c8Kbqe5">.] [or] I easily put down multiple items at the same time [in text description]".</s><s xml:id="_JdNXuMh">Participant's logs varied in granularity both as a group and individually, other than P14 and P15 that mostly journaled single foods and in database searches, as shown in Figure</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc><div><p xml:id="_hqj92Eh"><s xml:id="_sJujxJj">Examples of amount strategies grouped by granularity from participant food entries</s></p></div></figDesc><table><row><cell>Scale</cell><cell>Numeric</cell><cell>Non-standard</cell><cell>Non-quantified</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_5MFQbM5">ACKNOWLEDGMENTS</head><p xml:id="_grNdPZx"><s xml:id="_NGNVpJ2">We thank <rs type="person">Kimberly Flores</rs> and <rs type="person">Yuqi Huai</rs> for helping to develop ModEat.</s><s xml:id="_BJb4Nyg">We also thank <rs type="person">Elizabeth Ankrah</rs>, <rs type="person">Isil Oygur</rs>, and <rs type="person">Zhaoyuan Su</rs> for feedback on deployment piloting, <rs type="person">Jong Ho Lee</rs> and <rs type="person">David V. Nguyen</rs> for helping with analysis of some of the commercial services, and our participants.</s><s xml:id="_QPx42HG">This research was supported in part by the <rs type="funder">National Science Foundation</rs> under award <rs type="grantNumber">IIS-1850389</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MDyc8FR">
					<idno type="grant-number">IIS-1850389</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_SrXsfYt">Best Image Recognition APIs</title>
		<ptr target="https://nordicapis.com/7-best-image-recognition-apis/" />
		<imprint>
			<date type="published" when="2021-02-10">10 February, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Best Image Recognition APIs. Retrieved 10 February, 2021 from https:// nordicapis.com/7-best-image-recognition-apis/</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_dv9Fffg">You Tweet What You Eat: Studying Food Consumption Through Twitter</title>
		<author>
			<persName><forename type="first">Sofiane</forename><surname>Abbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelena</forename><surname>Mejova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702153</idno>
		<ptr target="http://doi.org/10.1145/2702123.2702153" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_fgtACfq">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3197" to="3206" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sofiane Abbar, Yelena Mejova, and Ingmar Weber. (2015). You Tweet What You Eat: Studying Food Consumption Through Twitter. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015), 3197-3206. http: //doi.org/10.1145/2702123.2702153</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_2js6VyF">The Role of Uncertainty as a Facilitator to Reflection in Self-Tracking</title>
		<author>
			<persName><forename type="first">Deemah</forename><surname>Alqahtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markel</forename><surname>Vigo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357236.3395448</idno>
		<ptr target="http://doi.org/10.1145/3357236.3395448" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_vDvcMBx">Proceedings of the Conference on Designing Interactive Systems (DIS 2020)</title>
		<meeting>the Conference on Designing Interactive Systems (DIS 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1807" to="1818" />
		</imprint>
	</monogr>
	<note type="raw_reference">Deemah Alqahtani, Caroline Jay, and Markel Vigo. (2020). The Role of Uncer- tainty as a Facilitator to Reflection in Self-Tracking. Proceedings of the Conference on Designing Interactive Systems (DIS 2020), 1807-1818. http://doi.org/10.1145/ 3357236.3395448</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_DepRWPn">Analysis of Chewing Sounds for dietary monitoring</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Amft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Stäger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Lukowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Tröster</surname></persName>
		</author>
		<idno type="DOI">10.1007/11551201_4</idno>
		<ptr target="http://doi.org/10.1007/11551201_4" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_7HedaUp">Proceedings of the International Conference on Ubiquitous Computing (Ubicomp 2005)</title>
		<meeting>the International Conference on Ubiquitous Computing (Ubicomp 2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
	<note type="raw_reference">Oliver Amft, Mathias Stäger, Paul Lukowicz, and Gerhard Tröster. (2005). Analy- sis of Chewing Sounds for dietary monitoring. Proceedings of the International Conference on Ubiquitous Computing (Ubicomp 2005), 56-72. http://doi.org/10. 1007/11551201_4</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_mEbAJW6">Simplifying Mobile Phone Diaries: Design and Evaluation of a Food Index-Based Nutrition Diary</title>
		<author>
			<persName><forename type="first">Adrienne</forename><forename type="middle">H</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaetano</forename><surname>Borriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<idno type="DOI">10.4108/icst.pervasivehealth.2013.252101</idno>
		<ptr target="http://doi.org/bbkk" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_3HKuFRB">Proceedings of the International Conference on Pervasive Computing Technologies for Healthcare</title>
		<meeting>the International Conference on Pervasive Computing Technologies for Healthcare<address><addrLine>PervasiveHealth</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="260" to="263" />
		</imprint>
	</monogr>
	<note type="raw_reference">Adrienne H. Andrew, Gaetano Borriello, and James Fogarty. (2013). Simplifying Mobile Phone Diaries: Design and Evaluation of a Food Index-Based Nutrition Diary. Proceedings of the International Conference on Pervasive Computing Tech- nologies for Healthcare (PervasiveHealth 2013), 260-263. http://doi.org/bbkk</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_7UxRU9f">Trackly: A Customisable and Pictorial Self-Tracking App to Support Agency in Multiple Sclerosis Self-Care</title>
		<author>
			<persName><forename type="first">Amid</forename><surname>Ayobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376809</idno>
		<ptr target="http://doi.org/10.1145/3313831.3376809" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_z5MNZDh">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2020)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note type="raw_reference">Amid Ayobi, Paul Marshall, and Anna L. Cox. (2020). Trackly: A Customisable and Pictorial Self-Tracking App to Support Agency in Multiple Sclerosis Self-Care. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2020), 1-15. http://doi.org/10.1145/3313831.3376809</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_M5aEwr9">Nutrition epidemiology: How do we know what they ate</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barrett-Connor</surname></persName>
		</author>
		<idno type="DOI">10.1093/ajcn/54.1.182s</idno>
		<ptr target="http://doi.org/10.1093/ajcn/54.1.182s" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_KkWEwMu">American Journal of Clinical Nutrition</title>
		<imprint>
			<biblScope unit="page" from="182" to="189" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Elizabeth Barrett-Connor. (1991). Nutrition epidemiology: How do we know what they ate? American Journal of Clinical Nutrition, 182-189. http://doi.org/10.1093/ ajcn/54.1.182s</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_FRpTj4C">Prescriptive Persuasion and Open-Ended Social Awareness: Expanding the Design Space of Mobile Health</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherri</forename><forename type="middle">Jean</forename><surname>Baumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jill</forename><forename type="middle">E</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">L</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Gonzales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Pollak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Retelny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">M</forename><surname>Niederdeppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geri</forename><forename type="middle">K</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><surname>Gay</surname></persName>
		</author>
		<idno type="DOI">10.1145/2145204.2145279</idno>
		<ptr target="http://doi.org/10.1145/2145204.2145279" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_djQBTFQ">Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2012)</title>
		<meeting>the ACM Conference on Computer Supported Cooperative Work (CSCW 2012)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="475" to="484" />
		</imprint>
	</monogr>
	<note type="raw_reference">Eric P.S. Baumer, Sherri Jean Katz, Jill E. Freeman, Phil Adams, Amy L. Gonza- les, John Pollak, Daniela Retelny, Jeff Niederdeppe, Christine M. Olson, and Geri K. Gay. (2012). Prescriptive Persuasion and Open-Ended Social Aware- ness: Expanding the Design Space of Mobile Health. Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2012), 475-484. http://doi.org/10.1145/2145204.2145279</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_PukShHH">EarBit: Using Wearable Sensors to Detect Eating Episodes in Unconstrained Environments</title>
		<author>
			<persName><forename type="first">Abdelkareem</forename><surname>Bedri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Temiloluwa</forename><surname>Prioleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Yan</forename><surname>Beh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thad</forename><surname>Starner</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130902</idno>
		<ptr target="http://doi.org/10.1145/3130902" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZUaJe3D">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
	<note type="raw_reference">Abdelkareem Bedri, Gregory Abowd, Richard Li, Malcolm Haynes, Raj Prateek Kosaraju, Ishaan Grover, Temiloluwa Prioleau, Min Yan Beh, Mayank Goel, and Thad Starner. (2017). EarBit: Using Wearable Sensors to Detect Eating Episodes in Unconstrained Environments. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 1(3), 1-20. http://doi.org/10.1145/ 3130902</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_EdCjRmG">FitByte: Automatic Diet Monitoring in Unconstrained Situations Using Multimodal Sensing on Eyeglasses</title>
		<author>
			<persName><forename type="first">Abdelkareem</forename><surname>Bedri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rushil</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Bhuwalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376869</idno>
		<ptr target="http://doi.org/10.1145/3313831.3376869" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_KedFrc7">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2020)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Abdelkareem Bedri, Diana Li, Rushil Khurana, Kunal Bhuwalka, and Mayank Goel. (2020). FitByte: Automatic Diet Monitoring in Unconstrained Situations Using Multimodal Sensing on Eyeglasses. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2020), 1-12. http://doi.org/10.1145/ 3313831.3376869</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_MxGaAWy">Menu-match: Restaurant-Specific Food Logging From Images</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Saponas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Khullar</surname></persName>
		</author>
		<idno type="DOI">10.1109/wacv.2015.117</idno>
		<ptr target="http://doi.org/10.1109/WACV.2015.117" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_5QjPC6c">Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV 2015)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="844" to="851" />
		</imprint>
	</monogr>
	<note type="raw_reference">Oscar Beijbom, Neel Joshi, Dan Morris, Scott Saponas, and Siddharth Khullar. (2015). Menu-match: Restaurant-Specific Food Logging From Images. Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV 2015), 844- 851. http://doi.org/10.1109/WACV.2015.117</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_rj7RU7x">Communication Breakdowns Between Families and Alexa</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Beneteau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><forename type="middle">K</forename><surname>Richards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Kientz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Hiniker</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300473</idno>
		<ptr target="http://doi.org/10.1145/3290605.3300473" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Zh7zP4z">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2019)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Erin Beneteau, Olivia K. Richards, Mingrui Zhang, Julie A. Kientz, Jason Yip, and Alexis Hiniker. (2019). Communication Breakdowns Between Families and Alexa. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2019), 1-13. http://doi.org/10.1145/3290605.3300473</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_vRjjHJF">OneNote Meal: A Photo-Based Diary Study for Reflective Meal Tracking</title>
		<author>
			<persName><forename type="first">Johnna</forename><surname>Blair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sooyeon</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun</forename><forename type="middle">Kyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Choe</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371351/" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_hSbXqhW">AMIA Annual Symposium proceedings (AMIA 2018)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="252" to="261" />
		</imprint>
	</monogr>
	<note type="raw_reference">Johnna Blair, Yuhan Luo, Ning F. Ma, Sooyeon Lee, and Eun Kyoung Choe. (2018). OneNote Meal: A Photo-Based Diary Study for Reflective Meal Tracking. AMIA Annual Symposium proceedings (AMIA 2018), 252-261. https://www.ncbi.nlm.nih. gov/pmc/articles/PMC6371351/</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_4B7VRaF">Simultaneous Food Localization and Recognition</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Bolanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petia</forename><surname>Radeva</surname></persName>
		</author>
		<idno type="DOI">10.1109/icpr.2016.7900117</idno>
		<ptr target="http://doi.org/10.1109/ICPR.2016.7900117" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_nqsX3Uw">Proceedings of International Conference on Pattern Recognition (ICPR 2016)</title>
		<meeting>International Conference on Pattern Recognition (ICPR 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="3140" to="3145" />
		</imprint>
	</monogr>
	<note type="raw_reference">Marc Bolanos and Petia Radeva. (2016). Simultaneous Food Localization and Recognition. Proceedings of International Conference on Pattern Recognition (ICPR 2016), 0, 3140-3145. http://doi.org/10.1109/ICPR.2016.7900117</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_NyPncY5">Using Thematic Analysis in Psychology</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1191/1478088706qp063oa</idno>
		<ptr target="http://doi.org/10.1191/1478088706qp063oa" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_DZP56j5">Qualitative Research in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="101" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Virginia Braun and Victoria Clarke. (2006). Using Thematic Analysis in Psy- chology. Qualitative Research in Psychology, 3(2), 77-101. http://doi.org/10.1191/ 1478088706qp063oa</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_nFZWueF">The Effect of Electronic Self-Monitoring on Weight Loss and Dietary Intake: A Randomized Behavioral Weight Loss Trial</title>
		<author>
			<persName><forename type="first">Lora</forename><forename type="middle">E</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Molly</forename><forename type="middle">B</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">M</forename><surname>Sereika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Okan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mindi</forename><forename type="middle">A</forename><surname>Elci</surname></persName>
		</author>
		<author>
			<persName><surname>Styn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sushama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">J</forename><surname>Sevick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Ewing</surname></persName>
		</author>
		<author>
			<persName><surname>Glanz</surname></persName>
		</author>
		<idno type="DOI">10.1038/oby.2010.208</idno>
		<ptr target="http://doi.org/10.1038/oby.2010.208" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_q2zJWbr">Obesity</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="338" to="344" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lora E. Burke, Molly B. Conroy, Susan M. Sereika, Okan U. Elci, Mindi A. Styn, Sushama D. Acharya, Mary A. Sevick, Linda J. Ewing, and Karen Glanz. (2011). The Effect of Electronic Self-Monitoring on Weight Loss and Dietary Intake: A Randomized Behavioral Weight Loss Trial. Obesity, 19(2), 338-344. http://doi.org/ 10.1038/oby.2010.208</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_Dxv3fRY">Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings. ning Semantic Text-Image Embeddings</title>
		<author>
			<persName><forename type="first">Micael</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210036</idno>
		<ptr target="http://doi.org/10.1145/3209978.3210036" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_pQXYVvN">Proceedings of The Conference on Research &amp; Development in Information Retrieval (SIGIR 2018)</title>
		<meeting>The Conference on Research &amp; Development in Information Retrieval (SIGIR 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
	<note type="raw_reference">Micael Carvalho, Rémi Cadène, David Picard, Laure Soulier, Nicolas Thome, and Matthieu Cord. (2018). Cross-Modal Retrieval in the Cooking Context: Learning Semantic Text-Image Embeddings. ning Semantic Text-Image Embeddings. Pro- ceedings of The Conference on Research &amp; Development in Information Retrieval (SIGIR 2018), 35-44. http://doi.org/10.1145/3209978.3210036</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_CVmuNHw">Evaluation of a Food Portion Size Estimation Interface for a Varying Literacy Population</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beenish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Schaefbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><forename type="middle">A</forename><surname>Jelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kay</forename><surname>Siek</surname></persName>
		</author>
		<author>
			<persName><surname>Connelly</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858554</idno>
		<ptr target="http://doi.org/10.1145/2858036.2858554" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Kjx6JVK">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2016)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5645" to="5657" />
		</imprint>
	</monogr>
	<note type="raw_reference">Beenish M. Chaudhry, Christopher Schaefbauer, Ben Jelen, Katie A. Siek, and Kay Connelly. (2016). Evaluation of a Food Portion Size Estimation Interface for a Varying Literacy Population. Proceedings of the SIGCHI Conference on Human Fac- tors in Computing Systems (CHI 2016), 5645-5657. http://doi.org/10.1145/2858036. 2858554</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_qXFKFQp">The Use of a Food Logging App in the Naturalistic Setting Fails to Provide Accurate Measurements of Nutrients and Poses Usability Challenges</title>
		<author>
			<persName><forename type="first">Juliana</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Berkman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manal</forename><surname>Bardouh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching</forename><surname>Yan Kammy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Allman-Farinelli</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.nut.2018.05.003</idno>
		<ptr target="http://doi.org/10.1016/j.nut.2018.05.003" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_rsQWsYa">Nutrition</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="208" to="216" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Juliana Chen, William Berkman, Manal Bardouh, Ching Yan Kammy Ng, and Mar- garet Allman-Farinelli. (2019). The Use of a Food Logging App in the Naturalistic Setting Fails to Provide Accurate Measurements of Nutrients and Poses Usability Challenges. Nutrition, 57, 208-216. http://doi.org/10.1016/j.nut.2018.05.003</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_jTwPjzy">The Most Popular Smartphone Apps for Weight Loss: A Quality Assessment</title>
		<author>
			<persName><forename type="first">Juliana</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">E</forename><surname>Cade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Allman-Farinelli</surname></persName>
		</author>
		<idno type="DOI">10.2196/mhealth.4334</idno>
		<ptr target="http://doi.org/10.2196/mhealth.4334" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_vsMdNCQ">Journal of Medical Internet Research (JMIR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Juliana Chen, Janet E. Cade, and Margaret Allman-Farinelli. (2015). The Most Popular Smartphone Apps for Weight Loss: A Quality Assessment. Journal of Medical Internet Research (JMIR 2015), 3(4), e104. http://doi.org/10.2196/mhealth. 4334</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_hxy9UEX">Semi-Automated Tracking: A Balanced Approach for Self-Monitoring Applications</title>
		<author>
			<persName><forename type="first">Eun</forename><surname>Kyoung Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mashfiqui</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edison</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felicia</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanzeem</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bongshin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Kientz</surname></persName>
		</author>
		<idno type="DOI">10.1109/MPRV.2017.18</idno>
		<ptr target="http://doi.org/10.1109/MPRV.2017.18" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_NmEv3s6">IEEE Pervasive Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="84" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eun Kyoung Choe, Saeed Abdullah, Mashfiqui Rabbi, Edison Thomaz, Daniel A. Epstein, Felicia Cordeiro, Matthew Kay, Gregory D. Abowd, Tanzeem Choudhury, James Fogarty, Bongshin Lee, Mark Matthews, and Julie A. Kientz. (2017). Semi- Automated Tracking: A Balanced Approach for Self-Monitoring Applications. IEEE Pervasive Computing, 16(1), 74-84. http://doi.org/10.1109/MPRV.2017.18</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_tbZPA9H">Understanding Quantified-Selfers&apos; Practices in Collecting and Exploring Personal Data</title>
		<author>
			<persName><forename type="first">Eun</forename><surname>Kyoung Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bongshin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanda</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Kientz</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557372</idno>
		<ptr target="http://doi.org/10.1145/2556288.2557372" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_UGmJK48">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2014)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1143" to="1152" />
		</imprint>
	</monogr>
	<note type="raw_reference">Eun Kyoung Choe, Nicole B. Lee, Bongshin Lee, Wanda Pratt, and Julie A. Kientz. (2014). Understanding Quantified-Selfers&apos; Practices in Collecting and Explor- ing Personal Data. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2014), 1143-1152. http://doi.org/10.1145/2556288.2557372</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_spkkTBy">Characterizing Dietary Choices, Nutrition, and Language in Food Deserts Via Social Media</title>
		<author>
			<persName><forename type="first">Munmun</forename><surname>De Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
		<idno type="DOI">10.1145/2818048.2819956</idno>
		<ptr target="http://doi.org/10.1145/2818048.2819956" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_ButppTy">Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2016)</title>
		<meeting>the ACM Conference on Computer Supported Cooperative Work (CSCW 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1157" to="1170" />
		</imprint>
	</monogr>
	<note type="raw_reference">Munmun De Choudhury, Sanket Sharma, and Emre Kiciman. (2016). Charac- terizing Dietary Choices, Nutrition, and Language in Food Deserts Via Social Media. Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2016), 1157-1170. http://doi.org/10.1145/2818048.2819956</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_Tyn5tym">Detecting Eating Episodes by Tracking Jawbone Movements with a Non-Contact Wearable Sensor</title>
		<author>
			<persName><forename type="first">Sarnab</forename><surname>Keum San Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edison</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><surname>Thomaz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3191736</idno>
		<ptr target="http://doi.org/10.1145/3191736" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_MTBMVdM">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note type="raw_reference">Keum San Chun, Sarnab Bhattacharya, and Edison Thomaz. (2018). Detecting Eating Episodes by Tracking Jawbone Movements with a Non-Contact Wearable Sensor. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 2(1), 1-21. http://doi.org/10.1145/3191736</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_Uc4fQDB">Identifying and Planning for Individualized Change</title>
		<author>
			<persName><forename type="first">Chia-Fang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaosi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3314394</idno>
		<ptr target="http://doi.org/10.1145/3314394" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_TqvmJ7G">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chia-Fang Chung, Qiaosi Wang, Jessica Schroeder, Allison Cole, Jasmine Zia, James Fogarty, and Sean A. Munson. (2019). Identifying and Planning for Individualized Change. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 3(1), 1-27. http://doi.org/10.1145/3314394</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_gBC7hsW">When Personal Tracking Becomes Social: Examining the Use of Instagram for Healthy Eating</title>
		<author>
			<persName><forename type="first">Chia</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Agapie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonali</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025747</idno>
		<ptr target="http://doi.org/10.1145/3025453.3025747" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_cmzEtcr">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1674" to="1687" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chia Fang Chung, Elena Agapie, Jessica Schroeder, Sonali Mishra, James Fogarty, and Sean A. Munson. (2017). When Personal Tracking Becomes Social: Examining the Use of Instagram for Healthy Eating. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017), 1674-1687. http://doi.org/ 10.1145/3025453.3025747</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_uWg8Gcz">Clarifai&apos;s Food Model | AI Prediction of Specific Food in Meals</title>
		<idno type="DOI">10.4135/9781529719499</idno>
		<ptr target="https://www.clarifai.com/models/food" />
		<imprint>
			<date type="published" when="2021-02-10">10 February, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Clarifai&apos;s Food Model | AI Prediction of Specific Food in Meals. Retrieved 10 February, 2021 from https://www.clarifai.com/models/food</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_FEGPAPR">Rethinking the Mobile Food Journal: Exploring Opportunities for Lightweight Photo-Based Capture</title>
		<author>
			<persName><forename type="first">Felicia</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Bales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702154</idno>
		<ptr target="http://doi.org/10.1145/2702123.2702154" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_U63dBFR">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3207" to="3216" />
		</imprint>
	</monogr>
	<note type="raw_reference">Felicia Cordeiro, Elizabeth Bales, Erin Cherry, and James Fogarty. (2015). Re- thinking the Mobile Food Journal: Exploring Opportunities for Lightweight Photo-Based Capture. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015), 3207-3216. http://doi.org/10.1145/2702123.2702154</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_VBv8hhf">Barriers and Negative Nudges: Exploring Challenges in Food Journaling</title>
		<author>
			<persName><forename type="first">Felicia</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edison</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Bales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><forename type="middle">K</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702155</idno>
		<ptr target="http://doi.org/10.1145/2702123.2702155" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_mPvKzd8">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1159" to="1162" />
		</imprint>
	</monogr>
	<note type="raw_reference">Felicia Cordeiro, Daniel A. Epstein, Edison Thomaz, Elizabeth Bales, Arvind K. Jagannathan, Gregory D. Abowd, and James Fogarty. (2015). Barriers and Negative Nudges: Exploring Challenges in Food Journaling. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2015), 1159-1162. http://doi.org/10.1145/2702123.2702155</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_9upurbt">A Review of Nutritional Tracking Mobile Applications for Diabetes Patient Use</title>
		<author>
			<persName><forename type="first">Alaina</forename><surname>Darby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Strum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gatwood</surname></persName>
		</author>
		<idno type="DOI">10.1089/dia.2015.0299</idno>
		<ptr target="http://doi.org/10.1089/dia.2015.0299" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_h5tzYYw">Diabetes Technology &amp; Therapeutics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alaina Darby, Matthew W. Strum, Erin Holmes, and Justin Gatwood. (2016). A Review of Nutritional Tracking Mobile Applications for Diabetes Patient Use. Diabetes Technology &amp; Therapeutics, 18(3), 200-212. http://doi.org/10.1089/dia. 2015.0299</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_F88pfBK">BALANCE: Towards a Usable Pervasive Wellness Application With Accurate Activity Inference</title>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Denning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrienne</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Chaudhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Hartung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaetano</forename><surname>Borriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glen</forename><surname>Duncan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1514411.1514416</idno>
		<ptr target="http://doi.org/10.1145/1514411.1514416" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Pay4P6p">Proceedings of the 10th Workshop on Mobile Computing Systems and Applications (HotMobile&apos;09)</title>
		<meeting>the 10th Workshop on Mobile Computing Systems and Applications (HotMobile&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tamara Denning, Adrienne Andrew, Rohit Chaudhri, Carl Hartung, Jonathan Lester, Gaetano Borriello, and Glen Duncan. (2009). BALANCE: Towards a Usable Pervasive Wellness Application With Accurate Activity Inference. Proceedings of the 10th Workshop on Mobile Computing Systems and Applications (HotMobile&apos;09), 5. http://doi.org/10.1145/1514411.1514416</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_xh6ZHCg">Personal Health Oracle: Explorations of Personalized Predictions in Diabetes Self-Management</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pooja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><forename type="middle">G</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">L</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Albers</surname></persName>
		</author>
		<author>
			<persName><surname>Mamykina</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300600</idno>
		<ptr target="http://doi.org/10.1145/3290605.3300600" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_jV5D6RK">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2019)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Pooja M Desai, Elliot G Mitchell, Maria L Hwang, Matthew E Levine, David J Albers, and Lena Mamykina. (2019). Personal Health Oracle: Explorations of Personalized Predictions in Diabetes Self-Management. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2019), 1-13. http://doi.org/10.1145/3290605.3300600</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<idno type="DOI">10.7717/peerj-cs.2828/fig-10</idno>
		<ptr target="https://www.statista.com/statistics/378850/top-mobile-health-application-categories-used-by-us-consumers/" />
		<title level="m" xml:id="_tGQNzsB">E-health application categories used by U.S. adults</title>
		<imprint>
			<publisher>Statista</publisher>
			<date type="published" when="2017-02-10">2017. 10 February, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E-health application categories used by U.S. adults 2017 | Statista. Retrieved 10 February, 2021 from https://www.statista.com/statistics/378850/top-mobile- health-application-categories-used-by-us-consumers/</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main" xml:id="_8VdgPGn">Edamam Nutrition Analysis API</title>
		<idno type="DOI">10.1007/978-1-4842-1305-6_11</idno>
		<ptr target="https://developer.edamam.com/edamam-nutrition-api" />
		<imprint>
			<date type="published" when="2021-02-10">10 February, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Edamam Nutrition Analysis API. Retrieved 10 February, 2021 https://developer. edamam.com/edamam-nutrition-api</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_bCCVdyM">Crumbs: Lightweight Daily Food Challenges to Promote Engagement and Mindfulness</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felicia</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cordeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Munson</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858044</idno>
		<ptr target="http://doi.org/10.1145/2858036.2858044" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_VJEajJp">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2016)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2016)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5632" to="5644" />
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel A. Epstein, Felicia Cordeiro, James Fogarty, Gary Hsieh, and Sean A. Munson. (2016). Crumbs: Lightweight Daily Food Challenges to Promote Engage- ment and Mindfulness. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2016), 5632-5644. http://doi.org/10.1145/2858036.2858044</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_PXQekNZ">Amazon says 100 million Alexa devices have been sold -The Verge</title>
		<ptr target="https://www.theverge.com/2019/1/4/18168565/amazon-alexa-devices-how-many-sold-number-100-million-dave-limp" />
		<imprint>
			<date type="published" when="2021-02-10">10 February, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Amazon says 100 million Alexa devices have been sold -The Verge. Retrieved 10 February, 2021 https://www.theverge.com/2019/1/4/18168565/amazon-alexa- devices-how-many-sold-number-100-million-dave-limp</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_P9PHDXB">A Focused Review of Smartphone Diet-Tracking Apps: Usability, Functionality, Coherence With Behavior Change Theory, and Comparative Validity of Nutrient Intake and Energy Estimates</title>
		<author>
			<persName><forename type="first">Giannina</forename><surname>Ferrara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><surname>Seto</surname></persName>
		</author>
		<idno type="DOI">10.2196/mhealth.9232</idno>
		<ptr target="http://doi.org/10.2196/mhealth.9232" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_s4s5gAU">JMIR mHealth and uHealth</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2019">2019. 9232</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Giannina Ferrara, Jenna Kim, Shuhao Lin, Jenna Hua, and Edmund Seto. (2019). A Focused Review of Smartphone Diet-Tracking Apps: Usability, Functionality, Coherence With Behavior Change Theory, and Comparative Validity of Nutrient Intake and Energy Estimates. JMIR mHealth and uHealth, 7(5), e9232. http://doi. org/10.2196/mhealth.9232</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main" xml:id="_UrgqWYD">Google&apos;s Cloud Vision API</title>
		<idno type="DOI">10.59287/as-proceedings.607</idno>
		<ptr target="https://cloud.google.com/vision/docs/" />
		<imprint>
			<date type="published" when="2021-02-10">10 February, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Google&apos;s Cloud Vision API. Retrieved 10 February, 2021 from https://cloud.google. com/vision/docs/</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_G5Jfeqe">Google&apos;s &quot;smart&quot; Food Diary is Actually Kind of Dumb</title>
		<idno type="DOI">10.7717/peerj-cs.862/table-1</idno>
		<ptr target="https://www.theverge.com/2015/6/2/8707851/google-calories-food-photos-im2calories" />
		<imprint>
			<date type="published" when="2021-02-10">10 February, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Google&apos;s &quot;smart&quot; Food Diary is Actually Kind of Dumb. Retrieved 10 February, 2021 from https://www.theverge.com/2015/6/2/8707851/google-calories-food- photos-im2calories</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_J6RAPnK">Participant Driven Photo Elicitation for Understanding Activity Tracking: Benefits and Limitations</title>
		<author>
			<persName><forename type="first">Nanna</forename><surname>Gorm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Shklovski</surname></persName>
		</author>
		<idno type="DOI">10.1145/2998181.2998214</idno>
		<ptr target="http://doi.org/10.1145/2998181.2998214" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_vd92P3f">Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2017)</title>
		<meeting>the ACM Conference on Computer Supported Cooperative Work (CSCW 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1350" to="1361" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nanna Gorm and Irina Shklovski. (2017). Participant Driven Photo Elicitation for Understanding Activity Tracking: Benefits and Limitations. Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2017), 1350-1361. http://doi.org/10.1145/2998181.2998214</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_8BaQYWq">Activity Tracking in Vivo</title>
		<author>
			<persName><forename type="first">Rúben</forename><surname>Gouveia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Karapanos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hassenzahl</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173936</idno>
		<ptr target="http://doi.org/10.1145/3173574.3173936" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_J2GvAgb">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2018)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rúben Gouveia, Evangelos Karapanos, and Marc Hassenzahl. (2018). Activity Tracking in Vivo. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2018), 1-13. http://doi.org/10.1145/3173574.3173936</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_f8b3zGH">Single Image-Based Food Volume Estimation Using Monocular Depth-Prediction Networks</title>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Graikos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Charisis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Iakovakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stelios</forename><surname>Hadjidimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leontios</forename><surname>Hadjileontiadis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-49108-6_38</idno>
		<ptr target="http://doi.org/10.1007/978-3-030-49108-6_38" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_WRJVAje">Universal Access in Human-Computer Interaction. Applications and Practice</title>
		<meeting><address><addrLine>HCII</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="532" to="543" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alexandros Graikos, Vasileios Charisis, Dimitrios Iakovakis, Stelios Hadjidim- itriou, and Leontios Hadjileontiadis. (2020). Single Image-Based Food Volume Estimation Using Monocular Depth-Prediction Networks. Universal Access in Human-Computer Interaction. Applications and Practice (HCII 2020), 532-543. http://doi.org/10.1007/978-3-030-49108-6_38</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_DeUMrW2">EatWell: Sharing Nutrition-Related Memories in a Low-Income Community</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Bednar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">David</forename><surname>Bolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">E</forename><surname>Grinter</surname></persName>
		</author>
		<idno type="DOI">10.1145/1460563.1460579</idno>
		<ptr target="http://doi.org/10.1145/1460563.1460579" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_WpkSNct">Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2008)</title>
		<meeting>the ACM Conference on Computer Supported Cooperative Work (CSCW 2008)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
	<note type="raw_reference">Andrea Grimes, Martin Bednar, Jay David Bolter, and Rebecca E Grinter. (2008). EatWell: Sharing Nutrition-Related Memories in a Low-Income Community. Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2008), 87-96. http://doi.org/10.1145/1460563.1460579</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_bFNP3dw">Celebratory Technology: New Directions for Food Research in HCI</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Harper</surname></persName>
		</author>
		<idno type="DOI">10.1145/1357054.1357130</idno>
		<ptr target="http://doi.org/10.1145/1357054.1357130" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Y63xHee">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>CHI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
	<note type="raw_reference">Andrea Grimes and Richard Harper. (2008). Celebratory Technology: New Direc- tions for Food Research in HCI. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2008), 467-476. http://doi.org/10.1145/1357054. 1357130</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_WsjVFsR">The Role of Diet in Symptoms of Irritable Bowel Syndrome in Adults: A Narrative Review</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">D</forename><surname>Heizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Southern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Mcgovern</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jada.2009.04.012</idno>
		<ptr target="http://doi.org/10.1016/j.jada.2009.04.012" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_pBK52MK">Journal of the American Dietetic Association</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1204" to="1214" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">William D. Heizer, Susannah Southern, and Susan McGovern. (2009). The Role of Diet in Symptoms of Irritable Bowel Syndrome in Adults: A Narrative Review. Journal of the American Dietetic Association, 109(7), 1204-1214. http://doi.org/10. 1016/j.jada.2009.04.012</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_pfhCXwZ">Gestures in The Wild: Studying Multi-Touch Gesture Sequences on Interactive Tabletop Exhibits</title>
		<author>
			<persName><forename type="first">Uta</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheelagh</forename><surname>Carpendale</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979391</idno>
		<ptr target="http://doi.org/10.1145/1978942.1979391" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_tXmXhut">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2011)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3023" to="3032" />
		</imprint>
	</monogr>
	<note type="raw_reference">Uta Hinrichs and Sheelagh Carpendale. (2011). Gestures in The Wild: Studying Multi-Touch Gesture Sequences on Interactive Tabletop Exhibits. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2011), 3023-3032. http://doi.org/10.1145/1978942.1979391</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_dQ9kagj">Weight Loss During the Intensive Intervention Phase of the Weight-Loss Maintenance Trial</title>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">F</forename><surname>Hollis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">M</forename><surname>Gullion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">J</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">J</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamy</forename><forename type="middle">D</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">M</forename><surname>Ard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arlene</forename><surname>Champagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Dalcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristine</forename><surname>Erlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><surname>Laferriere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwa</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><surname>Loria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">M</forename><surname>Samuel-Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">P</forename><surname>Vollmer</surname></persName>
		</author>
		<author>
			<persName><surname>Svetkey</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.amepre.2008.04.013</idno>
		<ptr target="http://doi.org/10.1016/j.amepre.2008.04.013" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_fcUuKdE">American Journal of Preventive Medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="118" to="126" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jack F. Hollis, Christina M. Gullion, Victor J. Stevens, Phillip J. Brantley, Lawrence J. Appel, Jamy D. Ard, Catherine M. Champagne, Arlene Dalcin, Thomas P. Erlinger, Kristine Funk, Daniel Laferriere, Pao Hwa Lin, Catherine M. Loria, Carmen Samuel-Hodge, William M. Vollmer, and Laura P. Svetkey. (2008). Weight Loss During the Intensive Intervention Phase of the Weight-Loss Maintenance Trial. American Journal of Preventive Medicine, 35(2), 118-126. http://doi.org/10. 1016/j.amepre.2008.04.013</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_zCFGkDY">MAMAS: Supporting Parent -Child Mealtime Interactions Using Automated Tracking and Speech Recognition</title>
		<author>
			<persName><forename type="first">Eunkyung</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeonseok</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myeonghan</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun</forename><forename type="middle">Jee</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungmook</forename><surname>Leem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwajung</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3392876</idno>
		<ptr target="http://doi.org/10.1145/3392876" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Sa5Rzrc">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Eunkyung Jo, Hyeonseok Bang, Myeonghan Ryu, Eun Jee Sung, Sungmook Leem, and Hwajung Hong. (2020). MAMAS: Supporting Parent -Child Mealtime Interactions Using Automated Tracking and Speech Recognition. Proceedings of the ACM on Human-Computer Interaction, 4(CSCW1). http://doi.org/10.1145/ 3392876</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_tVgmrJ8">Foundations for Systematic Evaluation and Benchmarking of a Mobile Food Logger in a Largescale Nutrition Study</title>
		<author>
			<persName><forename type="first">Jisu</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalina</forename><surname>Yacef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Allman-Farinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyndal</forename><surname>Wellard-Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Koprinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Allman-Farinelli</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397327</idno>
		<ptr target="http://doi.org/10.1145/3397327" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_jreg8DQ">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jisu Jung, Kalina Yacef, Margaret Allman-farinelli, Judy Kay, Lyndal Wellard-Cole, Colin Cai, Irena Koprinska, and Margaret Allman-Farinelli. (2020). Foundations for Systematic Evaluation and Benchmarking of a Mobile Food Logger in a Large- scale Nutrition Study. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 4(2), 47. http://doi.org/10.1145/3397327</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_XNtZ6Na">TummyTrials: A Feasibility Study of Using Self-Experimentation to Detect Individualized Food Triggers</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Karkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">R</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Scofield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Kientz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Vilardaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Zia</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025480</idno>
		<ptr target="http://doi.org/10.1145/3025453.3025480" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_TYrKaDZ">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017)</meeting>
		<imprint>
			<date type="published" when="2017-05">2017. 2017-May</date>
			<biblScope unit="page" from="6850" to="6863" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ravi Karkar, Jessica Schroeder, Daniel A. Epstein, Laura R. Pina, Jeffrey Scofield, James Fogarty, Julie A. Kientz, Sean A. Munson, Roger Vilardaga, and Jasmine Zia. (2017). TummyTrials: A Feasibility Study of Using Self-Experimentation to Detect Individualized Food Triggers. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017), 2017-May, 6850-6863. http: //doi.org/10.1145/3025453.3025480</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_EfyQSGS">OmniTrack: A Flexible Self-Tracking Approach Leveraging Semi-Automated Tracking</title>
		<author>
			<persName><forename type="first">Young-Ho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Ho Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bongshin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun</forename><forename type="middle">Kyoung</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwook</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3130930</idno>
		<ptr target="http://doi.org/10.1145/3130930" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_eYUK2Cy">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
	<note type="raw_reference">Young-Ho Kim, Jae Ho Jeon, Bongshin Lee, Eun Kyoung Choe, and Jinwook Seo. (2017). OmniTrack: A Flexible Self-Tracking Approach Leveraging Semi- Automated Tracking. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 1(3), 1-28. http://doi.org/10.1145/3130930</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_yTFj8sw">Semantic Mapping of Natural Language Input to Database Entries Via Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Korpusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2017.7953245</idno>
		<ptr target="http://doi.org/10.1109/ICASSP.2017.7953245" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_5QuR9m2">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="5685" to="5689" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mandy Korpusik, Zachary Collins, and James Glass. (2017). Semantic Mapping of Natural Language Input to Database Entries Via Convolutional Neural Networks. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017), 5685-5689. http://doi.org/10.1109/ICASSP.2017.7953245</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_DXkaaJs">Spoken Language Understanding for a Nutrition Dialogue System</title>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Korpusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2017.2694699</idno>
		<ptr target="http://doi.org/10.1109/TASLP.2017.2694699" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_3d2cy49">IEEE/ACM Transactions on Audio Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1450" to="1461" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mandy Korpusik and James Glass. (2017). Spoken Language Understanding for a Nutrition Dialogue System. IEEE/ACM Transactions on Audio Speech and Language Processing, 25(7), 1450-1461. http://doi.org/10.1109/TASLP.2017.2694699</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_tFZMwTU">A Stage-Based Model of Personal Informatics Systems</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753409</idno>
		<ptr target="http://doi.org/10.1145/1753326.1753409" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_U9br798">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>CHI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ian Li, Anind Dey, and Jodi Forlizzi. (2010). A Stage-Based Model of Personal Informatics Systems. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2010), 1, 557-566. http://doi.org/10.1145/1753326.1753409</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_tVsVHtE">Trade-off between Automation and Accuracy in Mobile Photo Recognition Food Logging</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinni</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengdong</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3080631.3080640</idno>
		<ptr target="http://doi.org/10.1145/3080631.3080640" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_dm2DxSr">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
	<note type="raw_reference">Brian Y. Lim, Xinni Chng, and Shengdong Zhao. (2017). Trade-off between Au- tomation and Accuracy in Mobile Photo Recognition Food Logging. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2017), 53-59. http://doi.org/10.1145/3080631.3080640</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<idno type="DOI">10.1145/3041021.3055131</idno>
		<ptr target="https://www.loseit.com/" />
		<title level="m" xml:id="_DvAsQaR">Lose It! -Weight Loss That Fits</title>
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lose It! -Weight Loss That Fits. Retrieved February 10, 2021 from https://www. loseit.com/</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_Abrzpp7">TableChat: Mobile Food Journaling to Facilitate Family Support for Healthy Eating</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Lukoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taoxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1145/3274383</idno>
		<ptr target="http://doi.org/10.1145/3274383" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_qndEAG2">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kai Lukoff, Taoxi Li, Yuan Zhuang, and Brian Y. Lim. (2018). TableChat: Mobile Food Journaling to Facilitate Family Support for Healthy Eating. Proceedings of the ACM on Human-Computer Interaction, 2(CSCW), 1-28. http://doi.org/10.1145/ 3274383</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_wn2PjJx">Data-driven health management: reasoning about personally generated data in diabetes with information technologies</title>
		<author>
			<persName><forename type="first">Lena</forename><surname>Mamykina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">G</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arlene</forename><forename type="middle">M</forename><surname>Smaldone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noemie</forename><surname>Elhadad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Albers</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocv187</idno>
		<ptr target="http://doi.org/10.1093/jamia/ocv187" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_V2v5Xkr">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="526" to="531" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lena Mamykina, Matthew E. Levine, Patricia G. Davidson, Arlene M Smaldone, Noemie Elhadad, and David J Albers. (2016). Data-driven health management: reasoning about personally generated data in diabetes with information tech- nologies. Journal of the American Medical Informatics Association, 23(3), 526-531. http://doi.org/10.1093/jamia/ocv187</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_dn4ywGs">MAHI: Investigation of Social Scaffolding for Reflective Thinking in Diabetes Management</title>
		<author>
			<persName><forename type="first">Lena</forename><surname>Mamykina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Mynatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Greenblatt</surname></persName>
		</author>
		<idno type="DOI">10.1145/1357054.1357131</idno>
		<ptr target="http://doi.org/10.1145/1357054.1357131" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Xyrf8Mt">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>CHI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page">477</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Lena Mamykina, Elizabeth Mynatt, Patricia Davidson, and Daniel Greenblatt. (2008). MAHI: Investigation of Social Scaffolding for Reflective Thinking in Diabetes Management. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2008), 477. http://doi.org/10.1145/1357054.1357131</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_uP72SvV">A Survey on Food Computing</title>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1145/3329168</idno>
		<ptr target="http://doi.org/10.1145/3329168" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_gSugJKB">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weiqing Min, Shuqiang Jiang, Linhu Liu, Yong Rui, and Ramesh Jain. (2019). A Survey on Food Computing. ACM Computing Surveys, 52(5), 1-36. http://doi.org/ 10.1145/3329168</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_YREmBCK">Managing In-home Environments through Sensing, Annotating, and Visualizing Air Quality Data</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Goffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miriah</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Lundrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Patwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Sward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wiese</surname></persName>
		</author>
		<idno type="DOI">10.1145/3264938</idno>
		<ptr target="http://doi.org/10.1145/3264938" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_V5QnPwx">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jimmy Moore, Pascal Goffin, Miriah Meyer, Philip Lundrigan, Neal Patwari, Katherine Sward, and Jason Wiese. (2018). Managing In-home Environments through Sensing, Annotating, and Visualizing Air Quality Data. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 2(3), 1-28. http://doi.org/10.1145/3264938</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_6F5SaNG">Im2Calories: Towards an automated mobile vision food diary</title>
		<author>
			<persName><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.146</idno>
		<ptr target="http://doi.org/10.1109/ICCV.2015.146" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_9jufsKg">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1233" to="1241" />
		</imprint>
	</monogr>
	<note type="raw_reference">Austin Myers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, and Kevin Murphy. (2015). Im2Calories: Towards an automated mobile vision food diary. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 1233-1241. http://doi.org/10.1109/ICCV.2015.146</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main" xml:id="_h7hpRwM">Calorie Counter, Diet &amp; Exercise Journal</title>
		<author>
			<persName><surname>Myfitnesspal</surname></persName>
		</author>
		<ptr target="https://www.myfitnesspal.com/" />
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">MyFitnessPal: Calorie Counter, Diet &amp; Exercise Journal. Retrieved February 10, 2021 from https://www.myfitnesspal.com/</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_39k2WTp">PlateMate: Crowdsourcing Nutrition Analysis from Food Photographs</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Noronha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><forename type="middle">Z</forename><surname>Gajos</surname></persName>
		</author>
		<idno type="DOI">10.1145/2047196.2047198</idno>
		<ptr target="http://doi.org/10.1145/2047196.2047198" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_k5e8ewz">Proceedings of the Annual Symposium on User Interface Software and Technology (UIST 2011)</title>
		<meeting>the Annual Symposium on User Interface Software and Technology (UIST 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jon Noronha, Eric Hysen, Haoqi Zhang, and Krzysztof Z Gajos. (2011). PlateMate: Crowdsourcing Nutrition Analysis from Food Photographs. Proceedings of the Annual Symposium on User Interface Software and Technology (UIST 2011), 1-12. http://doi.org/10.1145/2047196.2047198</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<ptr target="https://voicebot.ai/2019/01/07/npr-study-says-118-million-smart-speakers-owned-by-u-s-adults/" />
		<title level="m" xml:id="_nwXRkSd">NPR Study Says 118 Million Smart Speakers Owned by U.S. Adults -Voicebot</title>
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">NPR Study Says 118 Million Smart Speakers Owned by U.S. Adults -Voicebot.ai. Retrieved February 10, 2021 from https://voicebot.ai/2019/01/07/npr-study-says- 118-million-smart-speakers-owned-by-u-s-adults/</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main" xml:id="_EePn4kv">Nutritionix Nutrition API</title>
		<ptr target="https://www.nutritionix.com/business/api" />
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nutritionix Nutrition API. Retrieved February 10, 2021 from https://www. nutritionix.com/business/api</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_YA4dG6X">Multimodal Food Journaling</title>
		<author>
			<persName><forename type="first">Hyungik</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soundarya</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1145/3264996.3265000</idno>
		<ptr target="http://doi.org/10.1145/3264996.3265000" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_CBPEZwC">Proceedings of the International Workshop on Multimedia for Personal Health and Health Care</title>
		<meeting>the International Workshop on Multimedia for Personal Health and Health Care<address><addrLine>HealthMedia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hyungik Oh, Jonathan Nguyen, Soundarya Soundararajan, and Ramesh Jain. (2018). Multimodal Food Journaling. Proceedings of the International Workshop on Multimedia for Personal Health and Health Care (HealthMedia 2018), 39-47. http://doi.org/10.1145/3264996.3265000</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_qeUzyGj">BodyBeat: Amobile system for sensing non-speech body sounds</title>
		<author>
			<persName><forename type="first">Tauhidur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobby</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaishu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanzeem</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.1145/2594368.2594386</idno>
		<ptr target="http://doi.org/10.1145/2594368.2594386" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_3gVhvGc">Proceedings of the International Conference on Mobile Systems, Applications, and Services (MobiSys 2014)</title>
		<meeting>the International Conference on Mobile Systems, Applications, and Services (MobiSys 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tauhidur Rahman, Alexander T. Adams, Mi Zhang, Erin Cherry, Bobby Zhou, Huaishu Peng, and Tanzeem Choudhury. (2014). BodyBeat: Amobile system for sensing non-speech body sounds. Proceedings of the International Conference on Mobile Systems, Applications, and Services (MobiSys 2014), 2-13. http://doi.org/10. 1145/2594368.2594386</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<ptr target="https://rapidapi.com/collection/nutrition" />
		<title level="m" xml:id="_4PaPyBr">RapidAPI -Top Nutrition APIs</title>
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">RapidAPI -Top Nutrition APIs. Retrieved February 10, 2021 from https://rapidapi. com/collection/nutrition</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_g5eWXxW">User-Defined Motion Gestures for Mobile Interaction</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Lank</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1978971</idno>
		<ptr target="http://doi.org/10.1145/1978942.1978971" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_HW9MfZy">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2011)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jaime Ruiz, Yang Li, and Edward Lank. (2011). User-Defined Motion Gestures for Mobile Interaction. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2011), 197-206. http://doi.org/10.1145/1978942.1978971</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_jYphDdj">Inverse Cooking: Recipe Generation from Food Images</title>
		<author>
			<persName><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Giro-I-Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01070</idno>
		<ptr target="http://doi.org/10.1109/CVPR.2019.01070" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_K2Y4xqp">Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR 2019)</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition (CVPR 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10445" to="10454" />
		</imprint>
	</monogr>
	<note type="raw_reference">Amaia Salvador, Michal Drozdzal, Xavier Giro-I-Nieto, and Adriana Romero. (2019). Inverse Cooking: Recipe Generation from Food Images. Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR 2019), 10445-10454. http://doi.org/10.1109/CVPR.2019.01070</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main" xml:id="_eCJFybM">Supporting Patient-Provider Collaboration to Identify Individual Triggers Using Food and Symptom Journals</title>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hoffswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia</forename><forename type="middle">Fang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Zia</surname></persName>
		</author>
		<idno type="DOI">10.1145/2998181.2998276</idno>
		<ptr target="http://doi.org/10.1145/2998181.2998276" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_W8xkTte">Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2017)</title>
		<meeting>the ACM Conference on Computer Supported Cooperative Work (CSCW 2017)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1726" to="1739" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jessica Schroeder, Jane Hoffswell, Chia Fang Chung, James Fogarty, Sean Munson, and Jasmine Zia. (2017). Supporting Patient-Provider Collaboration to Identify Individual Triggers Using Food and Symptom Journals. Proceedings of the ACM Conference on Computer Supported Cooperative Work (CSCW 2017), 1726-1739. http://doi.org/10.1145/2998181.2998276</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_FQcFSjg">Studies of In-Home Conversational Agent Usage</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnita</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">I</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3196709.3196772</idno>
		<ptr target="http://doi.org/10.1145/3196709.3196772" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_hErxrkt">Proceedings of the Conference on Designing Interactive Systems (DIS 2018)</title>
		<meeting>the Conference on Designing Interactive Systems (DIS 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="857" to="868" />
		</imprint>
	</monogr>
	<note>Hey Alexa, what&apos;s up?</note>
	<note type="raw_reference">Alex Sciuto, Arnita Saini, Jodi Forlizzi, and Jason I. Hong. (2018). &quot;Hey Alexa, what&apos;s up?&quot;: Studies of In-Home Conversational Agent Usage. Proceedings of the Conference on Designing Interactive Systems (DIS 2018), 857-868. http://doi.org/10. 1145/3196709.3196772</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_Q6YpX6W">When Do We Eat? An Evaluation of Food Items Input into an Electronic Food Monitoring Application</title>
		<author>
			<persName><forename type="first">Katie</forename><forename type="middle">A</forename><surname>Siek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kay</forename><forename type="middle">H</forename><surname>Connelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rohwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desiree</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">L</forename><surname>Welch</surname></persName>
		</author>
		<idno type="DOI">10.1109/PCTHEALTH.2006.361684</idno>
		<ptr target="http://doi.org/10.1109/PCTHEALTH.2006.361684" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_gnWVEha">Pervasive Health Conference and Workshops</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note type="raw_reference">Katie A. Siek, Kay H. Connelly, Yvonne Rogers, Paul Rohwer, Desiree Lambert, and Janet L. Welch. (2006). When Do We Eat? An Evaluation of Food Items Input into an Electronic Food Monitoring Application. 2006 Pervasive Health Conference and Workshops, 1-10. http://doi.org/10.1109/PCTHEALTH.2006.361684</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<idno type="DOI">10.70248/jcsit.v2i3.2663</idno>
		<ptr target="https://spoonacular.com/food-api" />
		<title level="m" xml:id="_gAS2uhW">Spoonacular food API</title>
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Spoonacular food API. Retrieved February 10, 2021 from https://spoonacular. com/food-api</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_fPaWj8E">A Postcard from Your Food Journey in the Past&quot;: Promoting Self-Refection on Social Food Posting</title>
		<author>
			<persName><forename type="first">Zhida</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Yürüten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357236.3395475</idno>
		<ptr target="http://doi.org/10.1145/3357236.3395475" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_TVCxkdj">Proceedings of the Conference on Designing Interactive Systems (DIS 2020)</title>
		<meeting>the Conference on Designing Interactive Systems (DIS 2020)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1819" to="1832" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhida Sun, Sitong Wang, Wenjie Yang, Onur Yürüten, Chuhan Shi, and Xiaojuan Ma. (2020). &quot;A Postcard from Your Food Journey in the Past&quot;: Promoting Self- Refection on Social Food Posting. Proceedings of the Conference on Designing Interactive Systems (DIS 2020), 1819-1832. http://doi.org/10.1145/3357236.3395475</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main" xml:id="_6KxyxMG">Feasibility of Identifying Eating Moments from First-Person Images Leveraging Human Computation</title>
		<author>
			<persName><forename type="first">Edison</forename><surname>Thomaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Parnami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<idno type="DOI">10.1145/2526667.2526672</idno>
		<ptr target="http://doi.org/10.1145/2526667.2526672" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_uqA4HCZ">Proceedings of the International SenseCam &amp; Pervasive Imaging Conference</title>
		<meeting>the International SenseCam &amp; Pervasive Imaging Conference<address><addrLine>SenseCam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="26" to="33" />
		</imprint>
	</monogr>
	<note type="raw_reference">Edison Thomaz, Aman Parnami, Irfan Essa, and Gregory D. Abowd. (2013). Fea- sibility of Identifying Eating Moments from First-Person Images Leveraging Hu- man Computation. Proceedings of the International SenseCam &amp; Pervasive Imaging Conference (SenseCam 2013), 26-33. http://doi.org/10.1145/2526667.2526672</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_Q9eU9eH">Usability and Feasibility of PmEB: A Mobile Phone Application for Monitoring Real Time Caloric Balance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunny</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">J</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">G</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Griswold</surname></persName>
		</author>
		<author>
			<persName><surname>Patrick</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11036-007-0014-4</idno>
		<ptr target="http://doi.org/10.1007/s11036-007-0014-4" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_BhQ9HD6">Mobile Networks and Applications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="173" to="184" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Christopher C. Tsai, Gunny Lee, Fred Raab, Gregory J. Norman, Timothy Sohn, William G. Griswold, and Kevin Patrick. (2007). Usability and Feasibility of PmEB: A Mobile Phone Application for Monitoring Real Time Caloric Balance. Mobile Networks and Applications, 12(2-3), 173-184. http://doi.org/10.1007/s11036-007- 0014-4</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_W9xpHgc">Mixed dish recognition through multi-label learning</title>
		<author>
			<persName><forename type="first">Yunan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jing Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><forename type="middle">Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat</forename><surname>Seng Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanli</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyan</forename><surname>Ming</surname></persName>
		</author>
		<idno type="DOI">10.1145/3326458.3326929</idno>
		<ptr target="http://doi.org/10.1145/3326458.3326929" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_GbkfbUe">Proceedings of the 11th Workshop on Multimedia for Cooking and Eating Activities (CEA 2019)</title>
		<meeting>the 11th Workshop on Multimedia for Cooking and Eating Activities (CEA 2019)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yunan Wang, Jing Jing Chen, Chong Wah Ngo, Tat Seng Chua, Wanli Zuo, and Zhaoyan Ming. (2019). Mixed dish recognition through multi-label learning. Proceedings of the 11th Workshop on Multimedia for Cooking and Eating Activities (CEA 2019), 1-8. http://doi.org/10.1145/3326458.3326929</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_WkZPHg7">Comparison of Digital Photography to Weighed and Visual Estimation of Portion Sizes</title>
		<author>
			<persName><forename type="first">; H Raymond</forename><surname>Donald A Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Pamela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Alfonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Gerald</surname></persName>
		</author>
		<author>
			<persName><surname>Hunt</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0002-8223(03)00974-X</idno>
		<ptr target="https://doi.org/10.1016/S0002-8223(03)00974-X" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Gcq5aBk">Journal of American Dietetic Associattion</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="1139" to="1145" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Donald A Williamson, ; H Raymond Allen, ; Pamela, Davis Martin, Anthony J Al- fonso, Bonnie Gerald, and Alice Hunt. (2003). Comparison of Digital Photography to Weighed and Visual Estimation of Portion Sizes. Journal of American Dietetic Associattion, 103, 1139-1145. https://doi.org/10.1016/S0002-8223(03)00974-X</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main" xml:id="_VedE2WM">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Jacob O Wobbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1145/1518701.1518866</idno>
		<ptr target="http://doi.org/10.1145/1518701.1518866" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_P9sdvQJ">CHI 2009)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob O Wobbrock, Meredith Ringel Morris, and Andrew D Wilson. (2009). Pro- ceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI 2009), 1083-1092. http://doi.org/10.1145/1518701.1518866</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main" xml:id="_78fkepJ">Learning to Make Better Mistakes: Semantics-Aware Visual Food Recognition</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Merler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosario</forename><surname>Uceda-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1145/2964284.2967205</idno>
		<ptr target="http://doi.org/10.1145/2964284.2967205" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_qQevxHS">Proceedings of the International Conference on Multimedia</title>
		<meeting>the International Conference on Multimedia<address><addrLine>MM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="172" to="176" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hui Wu, Michele Merler, Rosario Uceda-Sosa, and John R. Smith. (2016). Learning to Make Better Mistakes: Semantics-Aware Visual Food Recognition. Proceedings of the International Conference on Multimedia (MM 2016), 172-176. http://doi.org/ 10.1145/2964284.2967205</note>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main" xml:id="_AWPYcAu">Weight Loss &amp; Wellness Help</title>
		<author>
			<persName><surname>Ww (weight</surname></persName>
		</author>
		<author>
			<persName><surname>Watchers</surname></persName>
		</author>
		<ptr target="https://www.weightwatchers.com" />
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">WW (Weight Watchers): Weight Loss &amp; Wellness Help. Retrieved February 10, 2021 from https://www.weightwatchers.com</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_KJu3NkZ">Food Recognition Using Statistics of Pairwise Local Features</title>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5539907</idno>
		<ptr target="http://doi.org/10.1109/CVPR.2010.5539907" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_66bzPNv">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2010)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2249" to="2256" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shulin Yang, Mei Chen, Dean Pomerleau, and Rahul Sukthankar. (2010). Food Recognition Using Statistics of Pairwise Local Features. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2010), 2249-2256. http://doi.org/10.1109/CVPR.2010.5539907</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main" xml:id="_DfGDP4c">YouFood Photo Food Journal on the App Store</title>
		<idno type="DOI">10.1007/s00393-021-01099-9</idno>
		<ptr target="https://apps.apple.com/us/app/youfood-photo-food-journal/id719841416" />
		<imprint>
			<date type="published" when="2021-02-10">February 10, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">YouFood Photo Food Journal on the App Store. Retrieved February 10, 2021 from https://apps.apple.com/us/app/youfood-photo-food-journal/id719841416</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main" xml:id="_GMCtXXj">NeckSense: A Multi-Sensor Necklace for Detecting Eating Activities in Free-Living Conditions</title>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzung</forename><surname>Tri Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sougata</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josiah</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nabil</forename><surname>Alshurafa</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397313</idno>
		<ptr target="http://doi.org/10.1145/3397313" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_tr73jhM">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shibo Zhang, Yuqi Zhao, Dzung Tri Nguyen, Runsheng Xu, Sougata Sen, Josiah Hester, and Nabil Alshurafa. (2020). NeckSense: A Multi-Sensor Necklace for Detecting Eating Activities in Free-Living Conditions. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT), 4(2), 1-26. http://doi.org/10.1145/3397313</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main" xml:id="_hWweg9e">The Feasibility, Usability, and Clinical Utility of Traditional Paper Food and Symptom Journals for Patients with Irritable Bowel Syndrome</title>
		<author>
			<persName><forename type="first">Jasmine</forename><forename type="middle">K</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Fang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><forename type="middle">A</forename><surname>Kientz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Bales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanette</forename><forename type="middle">M</forename><surname>Schenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">M</forename><surname>Heitkemper</surname></persName>
		</author>
		<idno type="DOI">10.1111/nmo.12935</idno>
		<ptr target="http://doi.org/10.1111/nmo.12935" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_QsCJyuA">Neurogastroenterology &amp; Motility</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12935</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jasmine K. Zia, Chia-Fang Chung, Jessica Schroeder, Sean A. Munson, Julie A. Kientz, James Fogarty, Elizabeth Bales, Jeanette M. Schenk, and Margaret M. Heitkemper. (2017). The Feasibility, Usability, and Clinical Utility of Traditional Paper Food and Symptom Journals for Patients with Irritable Bowel Syndrome. Neurogastroenterology &amp; Motility, 29(2), e12935. http://doi.org/10.1111/nmo. 12935</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
