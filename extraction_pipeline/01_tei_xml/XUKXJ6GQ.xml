<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_UkEC2WV">Federated Learning with Pareto Optimality for Resource Efficiency and Fast Model Convergence in Mobile Environments †</title>
				<funder>
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/501100003725</idno>
				</funder>
				<funder ref="#_hannBeb #_Dwnj995">
					<orgName type="full">Ministry of Science and ICT (MSIT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">© 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/). sensors</p>
				</availability>
				<date type="published" when="2024-04-12">12 April 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Garzia</forename><surname>Fabio</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Soodamani</forename><surname>Ramalingam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gianluigi</forename><surname>Ferrari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">June-Pyo</forename><surname>Jung</surname></persName>
							<idno type="ORCID">0000-0003-2113-4676</idno>
						</author>
						<author>
							<persName><forename type="first">Young-Bae</forename><surname>Ko</surname></persName>
							<idno type="ORCID">0000-0002-8799-1761</idno>
						</author>
						<author>
							<persName><forename type="first">Sung-Hwa</forename><surname>Lim</surname></persName>
							<idno type="ORCID">0000-0002-9783-9050</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>2</label> Department of AI Convergence Network , Ajou Univeristy , 206 , World Cup-ro , Suwon-si 16499 , Republic of Korea;</note>
								<orgName type="department">Department of AI Convergence Network</orgName>
								<orgName type="institution">Ajou Univeristy</orgName>
								<address>
									<addrLine>206 World Cup-ro</addrLine>
									<postCode>16499</postCode>
									<settlement>Suwon-si</settlement>
									<country>Republic of Korea;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">Department of Multimedia , Namseoul University , 91 , Daehak-ro , Cheonan -si 31020 , Republic of Korea;</note>
								<orgName type="department">Department of Multimedia</orgName>
								<orgName type="institution">Namseoul University</orgName>
								<address>
									<addrLine>91 Daehak-ro -si</addrLine>
									<postCode>31020</postCode>
									<settlement>Cheonan</settlement>
									<country>Republic of Korea;</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_yR9hnvT">Federated Learning with Pareto Optimality for Resource Efficiency and Fast Model Convergence in Mobile Environments †</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-04-12">12 April 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">26924F2647EE9703D9F15B53A7529A77</idno>
					<idno type="DOI">10.3390/s24082476</idno>
					<note type="submission">Received: 4 March 2024 Revised: 28 March 2024 Accepted: 10 April 2024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_B8qwGcV">federated learning</term>
					<term xml:id="_zSg8NuE">Pareto optimality</term>
					<term xml:id="_7zrVFh7">mobile communication</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_V3JXHD2"><p xml:id="_wXdVE9H"><s xml:id="_9GN35sV">Federated learning (FL) is an emerging distributed learning technique through which models can be trained using the data collected by user devices in resource-constrained situations while protecting user privacy.</s><s xml:id="_SMvKyy9">However, FL has three main limitations: First, the parameter server (PS), which aggregates the local models that are trained using local user data, is typically far from users.</s><s xml:id="_g2RMPeE">The large distance may burden the path links between the PS and local nodes, thereby increasing the consumption of the network and computing resources.</s><s xml:id="_u36nFs9">Second, user device resources are limited, but this aspect is not considered in the training of the local model and transmission of the model parameters.</s><s xml:id="_nx5f5gv">Third, the PS-side links tend to become highly loaded as the number of participating clients increases.</s><s xml:id="_7GvYGyM">The links become congested owing to the large size of model parameters.</s><s xml:id="_fvnfA3C">In this study, we propose a resource-efficient FL scheme.</s><s xml:id="_useh58f">We follow the Pareto optimality concept with the biased client selection to limit client participation, thereby ensuring efficient resource consumption and rapid model convergence.</s><s xml:id="_dHRc8gr">In addition, we propose a hierarchical structure with location-based clustering for device-to-device communication using k-means clustering.</s><s xml:id="_fUJFMg8">Simulation results show that with prate at 0.75, the proposed scheme effectively reduced transmitted and received network traffic by 75.89% and 78.77%, respectively, compared to the FedAvg method.</s><s xml:id="_Q57DkKD">It also achieves faster model convergence compared to other FL mechanisms, such as FedAvg and D2D-FedAvg.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_t3K9Yzd">Introduction</head><p xml:id="_Qv8vkc2"><s xml:id="_GeGymxu">The accelerated development of big data has resulted in the increasing application of artificial intelligence (AI) technologies.</s><s xml:id="_8h7E2v5">The International Data Corporation has predicted that the amount of data generated through Internet of things (IoT) devices will reach 79.4 ZB in 2025 <ref type="bibr" target="#b0">[1]</ref>, exceeding the capacities of IoT and mobile devices worldwide <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_T3KeDjZ">Most of the data generated by a device are processed locally or in a remote cloud server.</s><s xml:id="_xvaWgB3">However, this process involves three main problems pertaining to the constraints associated with real environments and remote cloud servers <ref type="bibr" target="#b2">[3]</ref>:</s></p><p xml:id="_tjrb6nZ"><s xml:id="_7qV3tSx">1.</s></p><p xml:id="_FxG9kBc"><s xml:id="_H7b6pTB">Network Congestion: network congestion on a remote cloud server occurs when numerous user devices simultaneously send data to the server.</s><s xml:id="_5VT4ntx">2.</s></p><p xml:id="_mgNMzNu"><s xml:id="_NntFffA">Privacy Leak: private user experience data may be leaked due to malicious network attacks during transmission.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_99bdFut">3.</head><p xml:id="_85nCrCJ"><s xml:id="_TRvuSRW">Resource Constraints: the capacity of network resources (e.g., wireless channel subcarriers and bandwidth) and user devices (e.g., computing performances and battery life) is limited.</s></p><p xml:id="_xk8eGbt"><s xml:id="_fuVKkDS">can effectively reduce the wireless communication traffic generated when the FL model is updated for each client.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6cXR2w3">•</head><p xml:id="_CUmshX2"><s xml:id="_Htf277t">We propose a biased client-selection method for a clustered structure by using Pareto optimality.</s><s xml:id="_bBcrAqD">This client-selection method employs high training loss values to accelerate model convergence and reduce resource consumption.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_TyWR48P">Related Work</head><p xml:id="_HJuGrAm"><s xml:id="_GQ5aQQf">FL is an alternative distributed ML method.</s><s xml:id="_r3dkQyV">FL differs from conventional distributed ML in that it involves an extremely large number of clients with heterogeneous and unbalanced local data distributions.</s><s xml:id="_Qdhkvnp">A key task of FL is the generation of learning models using the data collected from clients.</s><s xml:id="_U5WncWU">These data are stored in local devices, which can help prevent privacy leaks and avoid model divergence due to insufficient data and failure to participate owing to a lack of resources (e.g., wireless channel subcarriers, bandwidth, computing performance, and battery life).</s><s xml:id="_9jTCvxP">The convergence of a learning model and resource consumption in FL exhibit a trade-off relationship.</s><s xml:id="_q5frRDH">Thus, many researchers have attempted to improve the efficiency of FL by simultaneously optimizing its performance aspects.</s></p><p xml:id="_2Y8wtep"><s xml:id="_t5JBvVg">Federated averaging (FedAvg) <ref type="bibr" target="#b10">[11]</ref>, which is the most conventional FL mechanism, adjusts the batch size and epochs of federated stochastic gradient descent to average the gradient descent generated from a learning process, thereby significantly reducing the overall number of communication rounds by iterating more local updates on a client device.</s><s xml:id="_4gCZ2m5">FedAsync <ref type="bibr" target="#b11">[12]</ref> is an asynchronous FL mechanism for updating global models, in which the mixing weight is adaptively set as a function of staleness.</s><s xml:id="_w58tqaS">Notably, in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, experiments were conducted on non-iid data, i.e., data that are not independently and identically distributed.</s><s xml:id="_H4cw6Zm">However, a theoretical guarantee could not be realized in a convex optimization setting.</s><s xml:id="_SKS7Jjk">In <ref type="bibr" target="#b12">[13]</ref>, convergence guarantee for FedAvg was ensured without the impractical assumptions that the data are iid and all clients are available.</s></p><p xml:id="_T34xN8d"><s xml:id="_37E5p44">A large number of participating clients in FL may lead to server-side congestion and bottlenecks in aggregating client model parameters.</s><s xml:id="_DqQUF8D">Additionally, a large number of participating clients can affect the model convergence for non-iid data <ref type="bibr" target="#b12">[13]</ref>.</s><s xml:id="_8nnnaqR">By appropriately selecting the participating clients, the above-mentioned problems can be solved, and model convergence can be improved.</s><s xml:id="_5Va3wvY">Unlike FedAvg, in which clients are randomly selected, certain researchers <ref type="bibr" target="#b13">[14]</ref> considered clients with high loss values and proved that biased client selection is directly related to model convergence.</s><s xml:id="_hxPwbw6">The FedCS FL protocol was developed <ref type="bibr" target="#b14">[15]</ref> for selecting clients within a deadline to manage the resources of heterogeneous clients.</s><s xml:id="_FYRUSD9">This method employed biased client selection; however, it did not ensure the convergence of models for non-iid and heterogeneous data.</s><s xml:id="_8PgngRn">Furthermore, stragglers may be present in a mobile communication or IoT environment, which cannot participate in FL because the network connection is not persistent or a client device has shut down.</s><s xml:id="_H8KTysf">The presence of stragglers may hinder model convergence.</s><s xml:id="_xZDUZ77">Therefore, the FLANP FL framework was proposed <ref type="bibr" target="#b15">[16]</ref> to alleviate the effect of stragglers by adaptively selecting clients in different communication rounds according to their computation speeds.</s></p><p xml:id="_h2mDW7W"><s xml:id="_8PUNHAj">FL mechanisms with various structures have been proposed.</s><s xml:id="_fU8Qgpe">In the hierarchical FL (HFL) mechanism proposed in <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>, a client and server communicated through an intermediate medium rather than a direct communication structure.</s><s xml:id="_9WrFjzP">In <ref type="bibr" target="#b18">[19]</ref>, the hierarchical edge federated learning (HED-FL) model enhances traditional FL with a multi-layered edge node architecture for energy-efficient learning.</s><s xml:id="_hYDQExK">Two heuristic methods were also introduced to assess the effects of static and dynamic round execution across these layers.</s><s xml:id="_tHXztXd">Moreover, a hierarchical cluster-based structure was developed <ref type="bibr" target="#b16">[17]</ref>, which divided clients into several clusters based on resource constraints.</s><s xml:id="_GScWGbb">A leader node (LN) was elected, which was similar to an intermediate server.</s><s xml:id="_NxhUNPG">Only the LN directly communicated with a PS.</s><s xml:id="_xZvtSKn">Thus, the bottleneck that may have occurred in the PS was eliminated, and the consumption of communication resources was reduced.</s><s xml:id="_qETkWZy">Similarly, an edge server was deployed between a PS and the clients <ref type="bibr" target="#b17">[18]</ref>.</s><s xml:id="_wvxE2nW">The edge association problem was solved using an evolutionary game between the clients and the edge server.</s><s xml:id="_B6TFghF">The communication resource allocation problem between the edge server and PS was solved using a Stackelberg differential game.</s></p><p xml:id="_4qu5kdX"><s xml:id="_HFCPFFp">D2D and peer-to-peer (P2P) communication has been introduced to reduce the communication overhead for the efficient transmission of model parameters.</s><s xml:id="_z6JEbz3">Certain researchers <ref type="bibr" target="#b19">[20]</ref> examined a social attribute that was used for k-means clustering in D2D communication.</s><s xml:id="_gnYHeAD">A software-defined networking controller was used for clustering by calculating the social attributes between devices, rather than clustering with an unspecified majority.</s><s xml:id="_Z3sgHVD">Other researchers proposed algorithms for D2D communication with resource allocation <ref type="bibr" target="#b20">[21]</ref>, which could efficiently manage resources and interference.</s><s xml:id="_XCtNFHY">Zhang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a D2D-assisted hierarchical FL scheme to reduce the communication overhead in D2D environments.</s><s xml:id="_FB3xjaG">Semi-decentralized federated edge learning (SD-FEEL) <ref type="bibr" target="#b22">[23]</ref> proposes a structure that aggregates clients' model parameters and exchanges model parameters with neighboring edge servers, followed by broadcasting the updated models.</s><s xml:id="_h2Mwt4d">Two timescale hybrid FL (TT-HF) <ref type="bibr" target="#b23">[24]</ref> extends the FL architecture through aperiodic local and global model consensus procedures based on D2D communications, proposing a new model of gradient diversity and an adaptive control algorithm.</s><s xml:id="_343PUt8">In another framework <ref type="bibr" target="#b24">[25]</ref>, clients communicated with one another without a server for aggregating model parameters in FL.</s><s xml:id="_RChqZAf">Moreover, topology construction was conducted through deep reinforcement learning for P2P FL <ref type="bibr" target="#b25">[26]</ref>.</s></p><p xml:id="_Y9UB4VY"><s xml:id="_BrhxC83">The novelty of our framework lies in the following aspects: In <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, they propose FL utilizing D2D communication, and <ref type="bibr" target="#b22">[23]</ref> forms clusters for aggregating clients' model parameters, similar to our work.</s><s xml:id="_X4E5pEn">However, we have introduced the k-means clustering technique for the formation of D2D communication networks.</s><s xml:id="_gjfKyVS">This approach enables the selection of leader clients located in optimal positions without exceeding the communication distance threshold.</s><s xml:id="_eSXEKMP">By collecting and transmitting model parameters within clusters, it offers an ideal solution to alleviate server-side bottleneck issues.</s><s xml:id="_FWy4wuH">In <ref type="bibr" target="#b26">[27]</ref>, Min-Max Pareto optimization was used to manage the trade-off relationship between the algorithmic fairness and performance inconsistency for each client.</s><s xml:id="_pUyNqTA">FedMGDA+ <ref type="bibr" target="#b27">[28]</ref>, which is similar to the framework proposed in <ref type="bibr" target="#b26">[27]</ref>, realizes the multi-objective optimization of robustness, fairness, and accuracy through the Pareto stationary solution.</s><s xml:id="_KnGtF2d">In contrast, we consider that the model performance is proportional to the client's resource consumption.</s><s xml:id="_MGreERN">Therefore, we solve the target problem by using the Pareto optimality and considering the trade-off relationship between the model convergence and resource consumption.</s><s xml:id="_UxXTFRu">In this manner, the proposed method is different from those described in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>: A comparative analysis of FL methods, including FedPO, is encapsulated in Table <ref type="table">1</ref>, which delineates the distinct communication method, hierarchical architecture, and client selection strategies employed by each technique.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Zyqzu7P">Table 1. Summary of FL technique characteristics.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zzb7kqR">Method Communication Method Hierarchical Architecture Client Selection</head><p xml:id="_ZHXvn49"><s xml:id="_hTZ5paN">FedAvg <ref type="bibr" target="#b10">[11]</ref> central FedAsync <ref type="bibr" target="#b11">[12]</ref> central POWER-OF-CHOICE Strategy <ref type="bibr" target="#b13">[14]</ref> central ✓ FedCS <ref type="bibr" target="#b14">[15]</ref> central ✓ HFL <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> ✓ D2D-assisted hierarchical FL <ref type="bibr" target="#b21">[22]</ref> D2D ✓ SD-FEEL <ref type="bibr" target="#b22">[23]</ref> Edge Server TT-HF <ref type="bibr" target="#b23">[24]</ref> D2D P2P FL <ref type="bibr" target="#b25">[26]</ref> P2P FedPO D2D ✓ ✓</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_xDYcBpV">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." xml:id="_Ex8wJmc">A Brief Overview on Federated Learning</head><p xml:id="_UAzap42"><s xml:id="_GMV8hKe">FL performs ML from a federation of clients.</s><s xml:id="_CREyjxW">The clients train a model using their data and update the training model using gradient descent.</s><s xml:id="_d8QKcmv">After training, the updated model parameters are sent to a PS.</s><s xml:id="_fxkpBjX">The PS updates a global model by aggregating the model parameters trained by each client and calculating the weighted averages according to the number of data samples of each client.</s><s xml:id="_CAWcVYW">The model is defined as the loss function generated by model parameter vector w as f i (x i , y i , w), where i denotes the input data, x i is a feature, and y i is a label.</s><s xml:id="_fbJ4r3B">Considering the FL framework with K clients, we define D k as the local data sample for client k.</s><s xml:id="_zm6hc39">Then, the loss function for each client on the local dataset can be expressed as in Equation ( <ref type="formula" target="#formula_0">1</ref>), where</s></p><formula xml:id="formula_0">N k is the size of D k (N k = |D k |). F(w) = 1 ∑ K k=1 N k K ∑ k=1 ∑ x i ,y i ∈D k f i (x i , y i , w)<label>(1)</label></formula><p xml:id="_wXsV4e5"><s xml:id="_p5ST4zU">Client k updates its gradient descent for each local learning during round T, where the learning rate is η &gt; 0. The local model parameter in round t is w t k , defined in Equation <ref type="bibr" target="#b1">(2)</ref>.</s></p><formula xml:id="formula_1">g(w t k ) = 1 N k ∑ x i ,y i ∈D k ∇ f (x i , y i , w t k ), w t k = w t-1 k -η • g(w t-1 k )<label>(2)</label></formula><p xml:id="_tAH6AUZ"><s xml:id="_4x8Bzbd">PS aggregates w t k for certain clients, and the weighted averages used to update the global model are</s></p><formula xml:id="formula_2">w t+1 ps = w t ps - 1 N K K ∑ k=1 N k • w t k (<label>3</label></formula><formula xml:id="formula_3">)</formula><p xml:id="_KPC3z3V"><s xml:id="_arRUK2A">The goal is to find a model parameter that can converge for all clients and minimize the loss function:</s></p><formula xml:id="formula_4">w * = argmin w (F(w))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2." xml:id="_72yDggh">Pareto Principle and Pareto Optimality</head><p xml:id="_wy5V3cG"><s xml:id="_UjThBvC">The Pareto principle, named after Italian economist Vilfredo Pareto, posits that 80% of all outcomes result from 20% of the causes.</s><s xml:id="_Gyj8VBj">Originally observed in the early 20th century to describe the unequal distribution of wealth in Italy-where 20% of the population owned 80% of the land.</s><s xml:id="_BGegGGd">In business, it is often used to focus on the most profitable products or the most engaged customers;</s></p><p xml:id="_csaekXw"><s xml:id="_bSgDGNj">In the context of FL, we leverage this principle to allow a smaller but more crucial subset of clients to make significant contributions to the model's convergence while also alleviating some communication issues.</s><s xml:id="_xs4UqS5">This principle has shown us that not all clients contribute equally, but it does not offer guidance on how to strike a balance between various factors such as computational power, data quality, and contribution to model accuracy.</s></p><p xml:id="_dbnedV5"><s xml:id="_Z8jERMm">That is where the concept of Pareto optimality, which asserts that a society achieves maximum satisfaction when no individual can be made better off without making another individual worse off <ref type="bibr" target="#b28">[29]</ref>, comes into play.</s><s xml:id="_uYs67Rm">In contrast to the Pareto principle, which highlights inequality in contributions, Pareto optimality provides a framework for making trade-offs among competing objectives.</s></p><p xml:id="_KjvAeEa"><s xml:id="_AAn2xtn">For instance, focusing solely on clients with high computational power could expedite the model's convergence but at the cost of underrepresenting clients with lower resources, thereby creating a biased model.</s><s xml:id="_4YJP8B3">Pareto optimality seeks to mitigate this by identifying a set of clients that provides the most balanced trade-off between model accuracy and representational fairness.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1." xml:id="_XCXpJfJ">Basic Definition</head><p xml:id="_t5hwg64"><s xml:id="_XD2ydBb">The concept of Pareto optimality in a multi-objective optimization context refers to a state being considered Pareto optimal if no other feasible states exist that improve at least one objective without worsening any of the other objectives.</s><s xml:id="_6hCKJhp">Mathematically, let A be a set of n-dimensional vectors representing possible states.</s><s xml:id="_bXyCPTY">A vector a ∈ A is considered Pareto optimal if there does not exist any vector b ∈ A that dominates a. Formally, the definition is as follows: ∄b ∈ A s.t.</s><s xml:id="_KzdSu5M">b i ≥ a i ∀i, and b i &gt; a i for at least one i</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2." xml:id="_Jhvhvsp">Pareto Front</head><p xml:id="_G7D5UX7"><s xml:id="_ERjZSJZ">The Pareto front comprises all Pareto optimal points in the decision space, serving as an essential reference for decision-makers in multi-objective optimization scenarios.</s><s xml:id="_DyEFy7X">Mathematically, for a set A and objective functions f 1 , f 2 , . . .</s><s xml:id="_3fy6xC2">, f k , a point a belongs to the Pareto front if:</s></p><formula xml:id="formula_5">∀b ∈ A, ∄c ∈ A s.t. f i (c) ≥ f i (b) ∀i, and f j (c) &gt; f j (b) for at least one j</formula><p xml:id="_Ujh3xcu"><s xml:id="_J8Dem3T">Here, f i and f j are specific objectives among the k different objectives under consideration.</s><s xml:id="_QDZBX2F">This ensures that each point on the Pareto front is not dominated by any other point across all objectives <ref type="bibr" target="#b29">[30]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_4jCgFKM">FedPO: Federated Learning with Pareto Optimality 4.1. Problem Formulation</head><p xml:id="_qmWuhEq"><s xml:id="_DDdZBf5">FedPO is an HFL method that uses biased client selection to identify the participating clients by solving for the Pareto optimality using the training loss and resource state of clients.</s></p><p xml:id="_33KuyN8"><s xml:id="_rneehAM">In FL, a trade-off relationship exists between the model convergence and resource consumption.</s><s xml:id="_2DqXsec">More local training or communication rounds for aggregation are required to increase the convergence speed of the model, resulting in the consumption of large amounts of network and computational resources.</s><s xml:id="_b7sGXrM">We use a method <ref type="bibr" target="#b13">[14]</ref> that selects clients with a high loss value to accelerate model convergence.</s><s xml:id="_pCqCEXX">Specifically, we select the client with the optimal state of model convergence (high loss value) and resources, following Pareto optimality.</s><s xml:id="_c7dphRq">We assume a two-dimensional Euclidean space R 2 , ordered by a Pareto cone R 2 + , to solve a Pareto optimality point for resources and loss.</s><s xml:id="_Sr6zt5s">In addition, we assume that E is a locally convex space, and C E is a convex pointed cone that defines a partial order (≥C E ) in E <ref type="bibr" target="#b28">[29]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DDpBsSr">Definition 1.</head><p xml:id="_JDdXhgQ"><s xml:id="_AC3Bx49">We define A = {(α, β) ⊆ R 2 : α ≥ 0, β ≥ 0 , where α and β are the resource state and loss value for a client, respectively}.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wJqUGaA">1.</head><p xml:id="_KBTFdmY"><s xml:id="_KeG25Bm">We assume an elliptic function x 2 a 2 + y 2 b 2 = 1 to illustrate Pareto optimality.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HKtpbV8">2.</head><p xml:id="_nyTQ458"><s xml:id="_B8xrxdg">Point c ∈ A is an ideal maximum point of A if (α, β) ≥ C E for every (α, β) ∈ A and closest to the elliptic function.</s></p><p xml:id="_S3PMHR3"><s xml:id="_QA4J6Js">In Definition 1, a is the highest resource state of the client in set R = {r 1 , ..., r k }, and the resource states are modeled to have a Gaussian distribution according to <ref type="bibr" target="#b30">[31]</ref>.</s><s xml:id="_aQVf8JM">b is the server's training loss value for the model parameters w t ps , which are aggregated at time t. a and b can be defined as follows:</s></p><formula xml:id="formula_6">a = max(r t k ) (5) b = f (x i , y i , w t ps )<label>(6)</label></formula><p xml:id="_pbaVGsT"><s xml:id="_dNxpwaJ">We consider the client at point c that satisfies Definition 1 as the optimal client for participating in FL. generated clusters and</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." xml:id="_xaGb5bP">FedPO Framework</head><formula xml:id="formula_7">M i = {x| x ⊆ K}. M i is mutually disjoint (∀M i ∈ M, if i ̸ = j then M i ∩ M j = ∅).</formula><p xml:id="_kUyHa9e"><s xml:id="_ZtBFCDe">A centroid is a point located at the center of a cluster when j clusters are generated, and each centroid in M can be represented by C = {c 1 , . . .</s><s xml:id="_7HDwG3Z">, c j }.</s><s xml:id="_ta3gxjV">The groups of LCs are expressed as LeaderClients = {lc 1 , . . .</s><s xml:id="_Qd6MBxk">, lc j }.</s><s xml:id="_EWjjkVh">In a cluster, the client whose location is the closest to the centroid is selected as an LC, which acts as an intermediate server for HFL as certain clients are selected.</s><s xml:id="_wrsTytP">Thus, LCs aggregate the updated model parameters of clients and transmit them to the server.</s><s xml:id="_XCwZJ6T">The PS updates the global model using the weighted average of the aggregated model parameters, w LC , received from all LCs.</s><s xml:id="_ZHBU5vB">The PS calculates Pareto optimality to select the clients that will participate in the next round of training for the LCs.</s><s xml:id="_6z9z5PK">In addition, in accordance with the Pareto principle, the proposed method involves fewer clients compared with those in conventional FL, thereby ensuring model convergence for heterogeneous data and the efficient transmission of model parameters.</s><s xml:id="_ugrCPPN">The details of the proposed method are presented in the following text. . .</s><s xml:id="_ZwYTccV">, distortion j }, where j does not exceed K/2 as the pairing for D2D communication.</s><s xml:id="_sM5Emh9">Additionally, we consider that at least two clients exist in each cluster.</s><s xml:id="_NHKTXYy">Therefore, when the number of clusters is j, the average Euclidean communication distance from lc j to the location of each client k belonging to cluster M j is expressed as follows:</s></p><formula xml:id="formula_8">distortion j = 1 j j ∑ i=1 ∑ k∈M j 1 |M i | ∥l k -c i ∥ 2<label>(7)</label></formula><p xml:id="_wDf956A"><s xml:id="_RGY9TM3">A threshold value is set for the communication distance.</s><s xml:id="_byphMnU">The average communication distance from each client in the clusters does not exceed the threshold, and the optimal j is the maximum value.</s></p><formula xml:id="formula_9">j = argmax j (|M|), s.t. distortion j ≤ threshold<label>(8) 2.</label></formula><p xml:id="_DjkNh3B"><s xml:id="_XxVPQKS">HFL: Similar to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>, we regard LCs as intermediate servers.</s><s xml:id="_C4EPr4V">In a cluster, the client located closest to the centroid is selected as the LC to minimize the distance between the LC and other clients in the cluster.</s><s xml:id="_veg6N3e">The model parameters of the clients belonging to each cluster are transmitted to the LCs.</s><s xml:id="_hGzRyMe">The LCs aggregate the model parameters in round t and perform weighted averaging, as follows:</s></p><formula xml:id="formula_10">w t M j = 1 N M j ∑ k∈M j w t k N k , N M j = ∑ k∈M j N k<label>(9)</label></formula><p xml:id="_y9cbMvH"><s xml:id="_zvqHwdC">Thereafter, each LC sends the averaged model parameters to the PS, which aggregates these model parameters.</s><s xml:id="_GaUUcfD">At time t, the global model parameters in the PS are</s></p><formula xml:id="formula_11">w t ps = 1 N M ∑ j i=1 w t M i N M i . 3.</formula><p xml:id="_pugaqCP"><s xml:id="_KRWdyUu">Biased client selection for Pareto principle and optimality: We use the Pareto principle and optimality to ensure model convergence and to optimize the resource consumption.</s><s xml:id="_68sudDc">Figure <ref type="figure" target="#fig_2">2</ref> shows the accuracy of biased and unbiased client-selection methods in FL.</s><s xml:id="_43PfBsZ">On the MNIST and FashionMNIST datasets, biased client selection leads to faster model convergence in the initial stage, and its accuracy is higher than that of unbiased client selection.</s><s xml:id="_mJZKkDG">This result can be interpreted considering the Pareto principle: a small number of clients selected through biased client selection can produce sufficient outcomes.</s><s xml:id="_kqUws4C">Furthermore, we select clients in accordance with the Pareto optimality function based on two criteria: loss value and resource state.</s><s xml:id="_BWH8RFW">Therefore, according to the convergence analysis in <ref type="bibr" target="#b13">[14]</ref>, the loss value is adopted as the criterion for using Pareto optimality for client selection.</s><s xml:id="_c9cc2ZR">The other criterion is the state of client resources because all clients have finite network and computational resources in actual environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." xml:id="_prHUSSq">Algorithm</head><p xml:id="_rXBaE5t"><s xml:id="_yrF9kbv">The process flow of FedPO is presented as Algorithm 1.</s><s xml:id="_qqEHuXr">In the case of FedAvg, the model parameters must be directly sent to all clients.</s><s xml:id="_HW6xtT8">In contrast, in the proposed method, the model parameters are sent only to the LCs by adopting HFL.</s><s xml:id="_szDAFqH">In Algorithm 1, j, M, and w t M j are the number of clusters, set of clusters, and group model parameters of M j in the t-th round, respectively.</s><s xml:id="_eH2Xqmz">The LocationBasedClusterting() procedure in line 3 of Algorithm 1 is presented as Algorithm 2. We build clusters for D2D communication according to client locations using k-means clustering.</s><s xml:id="_ATZX6nb">Algorithm 2 is used to identify the optimal number of clusters and adjust the limit on the communication distance to prevent resource wastage in D2D communication.</s><s xml:id="_CvFbqXe">As the proposed structure is HFL (lines 4 and 5 in Algorithm 1), the model parameters of the PS are transmitted to clients through lc j .</s></p><p xml:id="_tahY7PV"><s xml:id="_nRXcFCr">In addition, lc j transmits the model parameters received by the PS to clients at a low cost using D2D communication <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_je3HQC8">The SelectClient() procedure in line 8 in Algorithm 1 selects participating clients (i.e., SelectedClients) for FL, which consist of subset K presented in Algorithm 3. The PS selects the clients that are closest to the elliptic function through Pareto optimality or whose loss value is larger than that of the PS.</s><s xml:id="_Nu4FD4j">The number of participating clients is adjusted by applying prate to each cluster.</s><s xml:id="_DmWZvE8">However, in Algorithm 3, client k must be selected if its loss is larger than that of the global model.</s><s xml:id="_TKtaEYr">At this point, w t ps = w t M 7:</s></p><p xml:id="_gZV2fWK"><s xml:id="_a6kMKC8">Each client belonging to cluster M j trains w t M j and updates it using its local data 8:</s></p><p xml:id="_BDX82V7"><s xml:id="_7p86rJ2">PS performs SelectClient(r t k , f (x i , y i , w t k ), prate)</s></p><formula xml:id="formula_12">9:</formula><p xml:id="_MKMm23Z"><s xml:id="_f2f5Mjv">SelectedClients send w t k to lc j 10:</s></p><formula xml:id="formula_13">lc j aggregates SelectedClients' w t M j = 1 N M j ∑ k∈M j w t k N k via D2D communication 11:</formula><p xml:id="_TJQJYZd"><s xml:id="_4798MVD">PS aggregates and updates model</s></p><formula xml:id="formula_14">w t+1 ps = 1 N M ∑ j i=1 w t M i N M i 12: end for Algorithm 2: LocationBasedClustering Input: Client set K 1:</formula><p xml:id="_V5h9rEW"><s xml:id="_TSpqdGv">Clients are clustered based on their location using k-means clustering 2: Set of cluster M is generated 3:</s></p><formula xml:id="formula_15">distortion j = 1 j ∑ j i=1 ∑ k∈M j 1 |M i | ∥l k -c i ∥ 2 4: if distortion j ≤ threshold then 5: j = argmax j (|M|) 6: end if 7:</formula><p xml:id="_d7pdk62"><s xml:id="_PzPJW6x">Client closest to the centroid of cluster M j is selected as lc j Algorithm 3: SelectClient Input: Resource state r t k , f (x i , y i , w t k ), participation fraction prate 1: Clients belonging to M j send loss value f (x i , y i , w t k ) and resource state r t k to lc j 2: PS selects clients following Pareto optimality with Definition 1</s></p><formula xml:id="formula_16">3: if f (x i , y i , w t k ) ≥ f (x i , y i , w t ps ) then 4:</formula><p xml:id="_VNeQUdf"><s xml:id="_6Y3B7Nf">lc j receives model parameters from a fraction of clients within |K| * prate based on the high loss value 5: end if Output: SelectedClients</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_bFDkNjF">Performance Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1." xml:id="_7jpcKFQ">Simulation Settings</head><p xml:id="_fNA2sbd"><s xml:id="_vuvrzVU">Environment.</s><s xml:id="_3bh7XkG">To test the model convergence, we simulate FL using PyTorch and measure the networking overhead using an off-the-shelf network simulation tool named OPNET.</s><s xml:id="_cnMNwJ5">We assume a D2D communication network in an area of 2000 × 2000 m.</s><s xml:id="_v9yZMvp">Additionally, we assume that the base station (i.e., PS) is located at the centroid of the network.</s><s xml:id="_hrGq648">According to Algorithm 2, all clients in the cluster are located within a one-hop communication distance that does not exceed the threshold.</s><s xml:id="_u3Cwft4">The feasible communication distance (i.e., threshold) between clients is referred to from <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref>.</s><s xml:id="_MeQ5Tsf">We select FedAvg <ref type="bibr" target="#b10">[11]</ref>, the most conventional method in FL, which involves aggregating the average of model parameters after multiple local trainings, and D2D-FedAvg <ref type="bibr" target="#b21">[22]</ref>, the system most similar to ours with features including D2D communication and a hierarchical structure, as our comparison targets.</s><s xml:id="_G54kRAj">Moreover, we use the simulation parameters specified in <ref type="bibr" target="#b21">[22]</ref> to compare the network overhead with those of FedAvg, D2D-FedAvg, and FedPO as detailed in Table <ref type="table">2</ref>.</s></p><p xml:id="_Tv69rqX"><s xml:id="_tgwh3QP">For the experiments, resources are defined to consist of the computational and communication resources of a client.</s><s xml:id="_ksDgQgW">The distribution of client resources is modeled as a Gaussian distribution, as mentioned previously.</s><s xml:id="_XkyxeqV">We assume that one resource unit is consumed considering the distance over which a client communicates with the PS and the basic resources consumed during the communication and computational processes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2." xml:id="_U2c9VqN">Simulation Results</head><p xml:id="_3nwqRd4"><s xml:id="_Q9TFqzG">Resource Efficiency.</s><s xml:id="_ceK53sJ">The proportion of traffic transmitted and received by the serverside of the trained model is shown in Figure <ref type="figure" target="#fig_8">4a</ref>,b, respectively.</s><s xml:id="_fZHGE2T">The number of participating clients is |K| = 100, iterations are performed for T = 1000 rounds, and the traffic values of FedAvg, D2D-FedAvg, and FedPO are compared.</s><s xml:id="_F5NNBGm">Figure <ref type="figure" target="#fig_8">4a</ref> shows that for FedAvg, the transmission traffic is similar for all prate values, because the model parameters are transmitted to all clients in every round.</s><s xml:id="_3WjrmNW">Furthermore, in the case of D2D-FedAvg, as the number of participating clients in FL increases according to prate, residual clients that cannot participate in the grouping process for D2D communication and thus communicate directly with the PS emerge, leading to slightly higher amounts of transmission traffic compared with those of FedPO.</s><s xml:id="_ph982Ua">This phenomenon occurs because FedPO has a fixed amount of server-side transmission due to the use of location-based k-means clustering for the clients.</s><s xml:id="_zAcJkNx">The same result can be observed in Figure <ref type="figure" target="#fig_8">4b</ref>, which represents the amount of traffic received.</s><s xml:id="_4htTETY">In the case of FedAvg, the number of clients sending data to the PS increases as prate increases.</s><s xml:id="_bA34MUW">In contrast, both D2D-FedAvg and FedPO have an intermediate transmitter (e.g., MUE and LC) in the communication process, resulting in less traffic received by the server.</s><s xml:id="_f3q8wwz">This configuration can partially alleviate the potential bottleneck and delay problems, depending on the amount of data received by the PS.</s><s xml:id="_sDHQhDT">The exact ratios corresponding to Figure <ref type="figure" target="#fig_8">4</ref> are listed in Table <ref type="table" target="#tab_0">3</ref>.</s><s xml:id="_tX2CuRu">In terms of the transmitted traffic from the server, the results in Table <ref type="table" target="#tab_0">3</ref> show that D2D-FedAvg and FedPO have lower overhead than FedAvg, which transmits the parameters to all clients.</s><s xml:id="_uUGdqkS">D2D-FedAvg shows a slight increase in overhead with increasing participation rate, with values of 23.57%, 28.05%, and 37.99%.</s><s xml:id="_3hkCjgy">In contrast, FedPO maintains similar levels of overhead for all participation rates, with values of 23.73%, 23.67%, and 24.11%.</s><s xml:id="_7ADKzsv">In terms of the received traffic from the server, both D2D-FedAvg and FedPO exhibit similar overhead as that of FedAvg with a participation rate of 0.75.</s><s xml:id="_NmQDDgm">These results indicate the superiority of D2D-FedAvg and FedPO.</s></p><p xml:id="_gwcDaZk"><s xml:id="_wVWANS4">(a) (b)  In FL, the clients play a crucial role in the learning process as they consume both computational and communication resources.</s><s xml:id="_thRb2AD">Model Performance.</s><s xml:id="_uZrFqP5">Figures <ref type="figure">6</ref> and <ref type="figure">7</ref> show the model accuracies of FedAvg, D2D-FedAvg, and FedPO, where the LR and LSTM learning models are trained using the MNIST dataset with prate = 0.25, 0.50, and 0.75.</s><s xml:id="_TStM4rM">We set a small number of clients (|K| = 20) to clearly demonstrate that biased client selection is superior to unbiased client selection.</s><s xml:id="_dnM77Sk">Figure <ref type="figure">6</ref> shows the results of the first 100 rounds out of 1000 rounds to demonstrate the initial convergence with the LR training model.</s><s xml:id="_VzzuPKs">When prate = 0.25, the accuracy of the initial training differs by that T = 100 by 14%.</s><s xml:id="_rkSWFz7">For the remaining values of prate, similar model accuracy is obtained as the number of participating clients increases.</s><s xml:id="_TRahrSZ">Figure <ref type="figure">7</ref> shows the accuracy of the LSTM training model for T = 1000 rounds on the MNIST dataset.</s><s xml:id="_6VhYX4x">Unlike that for the LR, the effectiveness of FedPO is considerably higher than those of FedAvg and D2D-FedAvg when prate = 0.25 and 0.50.</s><s xml:id="_5bd2svV">The results for FashionMNIST are shown in Figure <ref type="figure">8</ref>.</s><s xml:id="_WqfZWFU">Additionally, the results show that FedPO achieves higher accuracy with fewer participating clients, particularly excelling in the case of LSTM models, similar to the results with MNIST.</s><s xml:id="_MkpYQzf">(a) (b) (c) Figure 7. Model accuracy on MNIST for different values of prate with LSTM training model.</s><s xml:id="_XkDD5QE">(a) prate = 0.25.</s><s xml:id="_ExJG9nm">(b) prate = 0.50.</s><s xml:id="_5KTFwAG">(c) prate = 0.75.</s><s xml:id="_qynVRab">(a) prate = 0.25 (b) prate = 0.50</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_QKQPD79">Conclusions and Future Work</head><p xml:id="_YXgEKPR"><s xml:id="_SMrWeyE">This paper proposes a new FL scheme named FedPO that uses k-means clustering for D2D communication and utilizes Pareto optimality to select participating clients based on their resource state and loss.</s><s xml:id="_dNRjKPu">The effectiveness of the proposed scheme is experimentally evaluated through experiments in comparison with two methods: FedAvg, the conventional centralized method, and D2D-FedAvg, a modified version for D2D communications.</s></p><p xml:id="_JzQyUhd"><s xml:id="_y7AeANU">Thus, FedPO is a promising approach for addressing bottlenecks, reducing serverside traffic, and saving client resources.</s><s xml:id="_4SQ6mhV">Additionally, this method achieves faster model convergence in the initial rounds compared with the other methods.</s></p><p xml:id="_EDUxCfy"><s xml:id="_ZYX5ARY">In future work, additional experiments should be performed to evaluate the effect of environmental factors, such as communication instability and disconnection, on the FL performance.</s><s xml:id="_FSFvZ3E">Furthermore, although we use Pareto optimality to select clients based on their loss and resource state, a wider range of considerations, such as battery life, connectivity, and computational capabilities of devices in real-world settings, may be considered for client selection.</s><s xml:id="_63WcegW">In addition, when selecting the threshold in k-means clustering, factors that may affect model convergence may be considered in addition to communication aspects.</s></p><p xml:id="_6QeqcnT"><s xml:id="_8pP22qE">Future research directions for FedPO implementation can be summarized as follows:</s></p><p xml:id="_UDG5e2m"><s xml:id="_c4HBH5k">• Considering the effect of environmental factors on the FL performance: future work can be aimed at examining the effects of factors such as communication instability, network disconnection, and device heterogeneity on the FL performance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_n7sZQ8c">•</head><p xml:id="_UqDxkwh"><s xml:id="_pD5QcTF">Optimizing the clustering approach: when selecting the threshold for k-means clustering, other factors affecting the model convergence, such as the data distribution and number of clusters, can be considered.</s></p><p xml:id="_DFUAjC9"><s xml:id="_9abfkhw">• Evaluating the performance of the proposed approach in real-world scenarios: The experiments in this study are conducted in simulated environments.</s><s xml:id="_8jWujZq">In future work, the performance of the proposed approach can be evaluated in real-world settings to assess its practicality and effectiveness.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc><div><p xml:id="_aUYGg9C"><s xml:id="_Hmbcad4">Figure 1 shows the proposed system model, which consists of clients, LCs, and the PS.</s><s xml:id="_zP2Dash">The clients are represented by set K = {1, . . .</s><s xml:id="_hHHrbTg">, k}, where k is the number of clients.</s><s xml:id="_VYwG4ns">A client updates model parameter w k using the local data generated through user experience.</s><s xml:id="_y5Uy8mJ">The set of locations for client K is represented by Location = {l 1 , . . .</s><s xml:id="_aAfgZ4r">, l k }.</s><s xml:id="_5pbnJgt">The clusters generated via k-means clustering are expressed as M = {M 1 , . . .</s><s xml:id="_am522mx">, M j }, where j is the number of</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc><div><p xml:id="_bWHYNrp"><s xml:id="_fDBbtCP">Figure 1.</s><s xml:id="_X6PnmvK">System model. 1. K-means clustering for D2D communication: Compared with short-distance wireless communication [9], D2D communication effectively reduces resource consumption and network delay.</s><s xml:id="_CvFjNSt">We use D2D communication for transmitting the model parameters, training loss, and resource state of clients to the LCs for HFL.</s><s xml:id="_FbZFgKj">The PS builds an intranetwork for D2D communication using k-means clustering, based on the locations of clients.</s><s xml:id="_p28kNWV">distortion j is the average communication distance between intraclients according to the number of clusters j: Distortion = {distortion 1 , .. .</s><s xml:id="_SYzY7yg">, distortion j }, where j does not exceed K/2 as the pairing for D2D communication.</s><s xml:id="_XaCfWb8">Additionally, we consider that at least two clients exist in each cluster.</s><s xml:id="_mMmnEWH">Therefore, when the number of clusters is j, the average Euclidean communication distance from lc j to the location of each client k belonging to cluster M j is expressed as follows:</s></p></div></figDesc><graphic coords="7,166.39,263.25,324.02,272.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc><div><p xml:id="_u78cgT5"><s xml:id="_AGhwcfZ">Figure 2. Example of client selection with model accuracy when different methods are applied in LR learning on (a) MNIST and (b) FashionMNIST in a distributed setting with 20 clients and a participation rate (prate) of 0.25.</s></p></div></figDesc><graphic coords="8,166.39,486.23,195.84,149.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 : 5 :</head><label>15</label><figDesc><div><p xml:id="_A2KewED"><s xml:id="_Wg5TTF2">Federated Learning with Pareto Optimality Input: participation rate prate, random location set of clients location, threshold for the distance between nodes threshold, client set K 1: Initialize model w 0 ps 2: for Communication round t = 1, 2, . . .</s><s xml:id="_wdHhkXc">, T do 3: PS performs LocationBasedClustering(K) 4: PS sends global model w t ps to LC lc j sends w t ps to each client belonging to cluster M j via D2D communication 6:</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Table 2 .</head><label>2</label><figDesc><div><p xml:id="_q9MvwzH"><s xml:id="_QWfXHvU">Parameters for D2D network simulation.</s><s xml:id="_266jHxx">Parameter Value Number of clients 100 Max.</s><s xml:id="_jYFsY3j">transmit power of the client, P max |dB 23 dBm Noise power level -174 dBm/Hz Transmit power of the parameter server 43 dBm Maximum distance between LC and clients 200 m threshold Model and Datasets.</s><s xml:id="_SBPMQ44">The learning model is tested using logistic regression (LR) and long short-term memory (LSTM), with the training data derived from MNIST and FashionMNIST datasets.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc><div><p xml:id="_HX7DAe7"><s xml:id="_u4rsJ8q">Figure 3 shows the configuration using MNIST data, with different classes (typically, four classes) of clients with local data introduced to reflect the non-iid situation in the experiments.</s><s xml:id="_ptZkYBa">Disproportionate amounts of data are assumed to be held by clients, characteristic of non-iid situations.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc><div><p xml:id="_fmAPP8q"><s xml:id="_amJCBv5">Figure 3. Class per 20 clients and unbalances in MNIST.</s></p></div></figDesc><graphic coords="10,166.39,541.95,360.01,201.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc><div><p xml:id="_QyzqgX3"><s xml:id="_fzzvHZE">Figure 4. Server-side traffic for different values of prates.</s><s xml:id="_vsJyk7F">(a) Server-side transmitted traffic for different values of prates.</s><s xml:id="_xj8CsCF">(b) Server-side received traffic for different values of prates.</s></p></div></figDesc><graphic coords="11,166.39,583.82,241.21,144.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc><div><p xml:id="_MuVYbDY"><s xml:id="_24heqBE">Computational resources are consumed during model training using local data, whereas communication resources are used for model parameter communication.</s><s xml:id="_n3Ttrhh">The amounts of resources consumed and remaining for each client after each round represent important criteria for client participation and selection in the next round.</s><s xml:id="_zVaDABg">To analyze the effect of different FL methods on the resource states of the clients, Figure 5 shows the sum of the remaining resources of clients in each round.</s><s xml:id="_a4muFjW">We compare FedPO with FedAvg as both D2D-FedAvg and FedAvg have similar algorithms and resource consumption levels.</s><s xml:id="_3cerWmG">FedPO outperforms FedAvg in terms of the resource conservation of clients.</s><s xml:id="_v9pwczk">This phenomenon occurs owing to the Pareto optimality-based client selection method used in FedPO, which takes into account the client's available resources and loss during training.</s><s xml:id="_32WC8Dv">Consequently, FedPO selects those clients who will participate in FL without compromising their resources and model convergence and increases the remaining resources of the clients.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 .</head><label>5</label><figDesc><div><p xml:id="_WatrrMQ"><s xml:id="_eRME5mQ">Figure 5. Sum of remaining resources of clients at each round.</s></p></div></figDesc><graphic coords="12,166.39,439.83,324.01,191.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .Figure 7 .Figure 8 .Figure 8 .</head><label>6788</label><figDesc><div><p xml:id="_3eSzEUQ"><s xml:id="_CjDw84P">Figure 6.</s><s xml:id="_zkF2hxb">Model accuracy on MNIST for different values of prate with LR training model.</s><s xml:id="_vBXgW4t">(a) prate = 0.25.</s><s xml:id="_Ffthjjh">(b) prate = 0.50.</s><s xml:id="_QC47P64">(c) prate = 0.75.</s></p></div></figDesc><graphic coords="13,166.39,501.37,237.60,142.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 .</head><label>3</label><figDesc><div><p xml:id="_8aAJeuC"><s xml:id="_Fm5KQDm">Server-side traffic ratios of D2D-FedAvg, FedPO, and FedAvg.</s></p></div></figDesc><table><row><cell>Schemes</cell><cell>Prate</cell><cell>Transmitted (%)</cell><cell>Received (%)</cell></row><row><cell>FedAvg</cell><cell>0.25</cell><cell>100</cell><cell>28.12</cell></row><row><cell></cell><cell>0.50</cell><cell>-</cell><cell>70.65</cell></row><row><cell></cell><cell>0.75</cell><cell>-</cell><cell>100</cell></row><row><cell>D2D-FedAvg</cell><cell>0.25</cell><cell>23.57</cell><cell>20.94</cell></row><row><cell></cell><cell>0.50</cell><cell>28.05</cell><cell>21.63</cell></row><row><cell></cell><cell>0.75</cell><cell>37.99</cell><cell>22.37</cell></row><row><cell>FedPO</cell><cell>0.25</cell><cell>23.73</cell><cell>21.09</cell></row><row><cell></cell><cell>0.50</cell><cell>23.67</cell><cell>21.23</cell></row><row><cell></cell><cell>0.75</cell><cell>24.11</cell><cell>21.27</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xml:id="_gFRwZUz"><p xml:id="_sDSEgK6"><s xml:id="_rvUrVD9">Funding: This research was funded by the <rs type="funder">National Research Foundation of Korea (NRF)</rs> grant funded by the <rs type="funder">Ministry of Science and ICT (MSIT)</rs> (<rs type="grantNumber">NRF-2020R1A2C1102284</rs> &amp; <rs type="grantNumber">NRF-2021R1A2C1012776</rs>).</s><s xml:id="_JUnqXze">Institutional Review Board Statement: Not applicable.</s><s xml:id="_9tEEaYS">Informed Consent Statement: Not applicable.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hannBeb">
					<idno type="grant-number">NRF-2020R1A2C1102284</idno>
				</org>
				<org type="funding" xml:id="_Dwnj995">
					<idno type="grant-number">NRF-2021R1A2C1012776</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_32zUeUF">Data Availability Statement:</head><p xml:id="_4fQaY8p"><s xml:id="_6q6qXjh">The data presented in this study are available on request from the corresponding author.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6D6qzgZ">Conflicts of Interest:</head><p xml:id="_ugxtdWQ"><s xml:id="_4XBh4PK">The authors declare no conflicts of interest.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_huTr4xD">The world in 2025-predictions for the next ten years</title>
		<author>
			<persName><forename type="first">R</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="DOI">10.1109/impact.2015.7365193</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pnKw8b9">Proceedings of the 2015 10th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT)</title>
		<meeting>the 2015 10th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT)<address><addrLine>Taipei, Taiwan; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-10-23">21-23 October 2015. 2015</date>
			<biblScope unit="page" from="192" to="195" />
		</imprint>
	</monogr>
	<note type="raw_reference">Taylor, R.; Baron, D.; Schmidt, D. The world in 2025-predictions for the next ten years. In Proceedings of the 2015 10th International Microsystems, Packaging, Assembly and Circuits Technology Conference (IMPACT), Taipei, Taiwan, 21-23 October 2015; IEEE: Piscateville, NJ, USA, 2015; pp. 192-195.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_da4f8PD">Fog and IoT: An overview of research opportunities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2016.2584538</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VKQdkZn">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="854" to="864" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chiang, M.; Zhang, T. Fog and IoT: An overview of research opportunities. IEEE Internet Things J. 2016, 3, 854-864. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_9bdrGE5">Privacy and big data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Gaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geetter</surname></persName>
		</author>
		<idno type="DOI">10.1109/mc.2014.161</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MhwVQ9E">Computer</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="7" to="9" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gaff, B.M.; Sussman, H.E.; Geetter, J. Privacy and big data. Computer 2014, 47, 7-9. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_Xzbc4Zw">When edge meets learning: Adaptive control for resource-constrained distributed machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salonidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Makaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1109/infocom.2018.8486403</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8JSr4bP">Proceedings of the IEEE INFOCOM 2018-IEEE Conference on Computer Communications</title>
		<meeting>the IEEE INFOCOM 2018-IEEE Conference on Computer Communications<address><addrLine>Honolulu, HI, USA; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-04-19">16-19 April 2018. 2018</date>
			<biblScope unit="page" from="63" to="71" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S.; Tuor, T.; Salonidis, T.; Leung, K.K.; Makaya, C.; He, T.; Chan, K. When edge meets learning: Adaptive control for resource-constrained distributed machine learning. In Proceedings of the IEEE INFOCOM 2018-IEEE Conference on Computer Communications, Honolulu, HI, USA, 16-19 April 2018; IEEE: Piscateville, NJ, USA, 2018; pp. 63-71.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main" xml:id="_b57yQQF">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konečn Ỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacon</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.029931</idno>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Konečn ỳ, J.; McMahan, H.B.; Yu, F.X.; Richtárik, P.; Suresh, A.T.; Bacon, D. Federated learning: Strategies for improving communication efficiency. arXiv 2016, arXiv:1610.05492.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_CQK2Ger">Combining Federated Learning and Edge Computing Toward Ubiquitous Intelligence in 6G Network: Challenges, Recent Advances, and Future Directions</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/comst.2023.3316615</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Un6yRDW">IEEE Commun. Surv. Tutor</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2892" to="2950" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Duan, Q.; Huang, J.; Hu, S.; Deng, R.; Lu, Z.; Yu, S. Combining Federated Learning and Edge Computing Toward Ubiquitous Intelligence in 6G Network: Challenges, Recent Advances, and Future Directions. IEEE Commun. Surv. Tutor. 2023, 25, 2892-2950. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_6yNYTp2">Towards federated learning at scale: System design</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grieskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ingerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konečn Ỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ieeeconf44664.2019.9049066</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YH6YCBW">Proc. Mach. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="374" to="388" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bonawitz, K.; Eichner, H.; Grieskamp, W.; Huba, D.; Ingerman, A.; Ivanov, V.; Kiddon, C.; Konečn ỳ, J.; Mazzocchi, S.; McMahan, B.; et al. Towards federated learning at scale: System design. Proc. Mach. Learn. Syst. 2019, 1, 374-388.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_MEBk9B4">Project adam: Building an efficient and scalable deep learning training system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalyanaraman</surname></persName>
		</author>
		<idno type="DOI">10.1109/msp.2010.134</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KQkM8kJ">Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)</title>
		<meeting>the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)<address><addrLine>Broomfield, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-08">6-8 October 2014</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chilimbi, T.; Suzue, Y.; Apacible, J.; Kalyanaraman, K. Project adam: Building an efficient and scalable deep learning training system. In Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14), Broomfield, CO, USA, 6-8 October 2014; pp. 571-582.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_AGSd38h">A survey of device-to-device communications: Research issues and challenges</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jameel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jabeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zeadally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Javed</surname></persName>
		</author>
		<idno type="DOI">10.1109/comst.2018.2828120</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uJBpU3k">IEEE Commun. Surv. Tutor</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2133" to="2168" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jameel, F.; Hamid, Z.; Jabeen, F.; Zeadally, S.; Javed, M.A. A survey of device-to-device communications: Research issues and challenges. IEEE Commun. Surv. Tutor. 2018, 20, 2133-2168. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_5sgWAmC">Resource Efficient Cluster-Based Federated Learning for D2D Communications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<idno type="DOI">10.1109/vtc2022-spring54318.2022.9860657</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_N8MBemv">Proceedings of the 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)</title>
		<meeting>the 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring)<address><addrLine>Helsinki, Finland; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06-22">19-22 June 2022. 2022</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jung, J.P.; Ko, Y.B.; Lim, S.H. Resource Efficient Cluster-Based Federated Learning for D2D Communications. In Proceedings of the 2022 IEEE 95th Vehicular Technology Conference: (VTC2022-Spring), Helsinki, Finland, 19-22 June 2022; IEEE: Piscateville, NJ, USA 2022; pp. 1-5.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_ZtshVmA">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A Y</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NEXhz8r">Proceedings of the Artificial Intelligence and Statistics</title>
		<meeting>the Artificial Intelligence and Statistics<address><addrLine>PMLR, Ft. Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
	<note type="raw_reference">McMahan, B.; Moore, E.; Ramage, D.; Hampson, S.; Arcas, B.A.Y. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the Artificial Intelligence and Statistics, PMLR, Ft. Lauderdale, FL, USA, 20-22 April 2017; pp. 1273-1282.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_5a3u5CG">Asynchronous Federated Optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03934</idno>
		<ptr target="http://arxiv.org/abs/1903.03934" />
		<imprint/>
	</monogr>
	<note type="raw_reference">Xie, C.; Koyejo, S.; Gupta, I. Asynchronous Federated Optimization. arXiv 2019, arXiv:1903.03934. http://arxiv.org/abs/1903.03934.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1090/mbk/121/79</idno>
		<idno type="arXiv">arXiv:1907.02189</idno>
		<title level="m" xml:id="_7vsAAtJ">On the convergence of fedavg on non-iid data</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">Li, X.; Huang, K.; Yang, W.; Wang, S.; Zhang, Z. On the convergence of fedavg on non-iid data. arXiv 2019, arXiv:1907.02189.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_CMnCzeQ">Client selection in federated learning: Convergence analysis and power-of-choice selection strategies</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.01243</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Cho, Y.J.; Wang, J.; Joshi, G. Client selection in federated learning: Convergence analysis and power-of-choice selection strategies. arXiv 2020, arXiv:2010.01243.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_hYHajJw">Client selection for federated learning with heterogeneous resources in mobile edge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nishio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yonetani</surname></persName>
		</author>
		<idno type="DOI">10.1109/icc.2019.8761315</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4GyT8N5">Proceedings of the ICC 2019-2019 IEEE International Conference on Communications (ICC)</title>
		<meeting>the ICC 2019-2019 IEEE International Conference on Communications (ICC)<address><addrLine>Shanghai, China; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05-24">20-24 May 2019. 2019</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nishio, T.; Yonetani, R. Client selection for federated learning with heterogeneous resources in mobile edge. In Proceedings of the ICC 2019-2019 IEEE International Conference on Communications (ICC), Shanghai, China, 20-24 May 2019; IEEE: Piscateville, NJ, USA, 2019; pp. 1-7.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_yzVtxg6">Straggler-resilient federated learning: Leveraging the interplay between statistical accuracy and system heterogeneity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reisizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pedarsani</surname></persName>
		</author>
		<idno type="DOI">10.1109/jsait.2022.3205475</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_255AeSS">IEEE J. Sel. Areas Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="197" to="205" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Reisizadeh, A.; Tziotis, I.; Hassani, H.; Mokhtari, A.; Pedarsani, R. Straggler-resilient federated learning: Leveraging the interplay between statistical accuracy and system heterogeneity. IEEE J. Sel. Areas Inf. Theory 2022, 3, 197-205. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_hQYJFCx">Resource-efficient federated learning with hierarchical aggregation in edge computing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1109/infocom42981.2021.9488756</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GDrk6qU">Proceedings of the IEEE INFOCOM 2021-IEEE Conference on Computer Communications</title>
		<meeting>the IEEE INFOCOM 2021-IEEE Conference on Computer Communications<address><addrLine>Vancouver, BC, Canada; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-05-13">10-13 May 2021. 2021</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, Z.; Xu, H.; Liu, J.; Huang, H.; Qiao, C.; Zhao, Y. Resource-efficient federated learning with hierarchical aggregation in edge computing. In Proceedings of the IEEE INFOCOM 2021-IEEE Conference on Computer Communications, Vancouver, BC, Canada, 10-13 May 2021; IEEE: Piscateville, NJ, USA, 2021; pp. 1-10.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_3AznfKU">Dynamic edge association and resource allocation in self-organizing hierarchical federated learning networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Niyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/jsac.2021.3118401</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cRDfAfy">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3640" to="3653" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lim, W.Y.B.; Ng, J.S.; Xiong, Z.; Niyato, D.; Miao, C.; Kim, D.I. Dynamic edge association and resource allocation in self-organizing hierarchical federated learning networks. IEEE J. Sel. Areas Commun. 2021, 39, 3640-3653. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_CWEbc2N">A hierarchical, energy efficient, and dynamic approach for edge Federated Learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>De Rango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guerrieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raimondo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Spezzano</surname></persName>
		</author>
		<author>
			<persName><surname>Hed-Fl</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.pmcj.2023.101804</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3puVRBV">Pervasive Mob. Comput</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page">101804</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">De Rango, F.; Guerrieri, A.; Raimondo, P.; Spezzano, G. HED-FL: A hierarchical, energy efficient, and dynamic approach for edge Federated Learning. Pervasive Mob. Comput. 2023, 92, 101804. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_PWGkSnY">A social-aware K means clustering algorithm for D2D multicast communication under SDN architecture</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aeue.2021.153610</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kUzUQ4c">AEU-Int. J. Electron. Commun</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page">153610</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>CrossRef</note>
	<note type="raw_reference">Gong, W.; Pang, L.; Wang, J.; Xia, M.; Zhang, Y. A social-aware K means clustering algorithm for D2D multicast communication under SDN architecture. AEU-Int. J. Electron. Commun. 2021, 132, 153610. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_YjmeENk">Resource allocation based on clustering for D2D communications in underlaying cellular networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_atmYpGr">Proceedings of the 2014 International Conference on Information and Communication Technology Convergence (ICTC)</title>
		<meeting>the 2014 International Conference on Information and Communication Technology Convergence (ICTC)<address><addrLine>Busan, Republic of Korea; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-10-24">22-24 October 2014. 2014</date>
			<biblScope unit="page" from="232" to="237" />
		</imprint>
	</monogr>
	<note type="raw_reference">Son, D.J.; Yu, C.H.; Kim, D.I. Resource allocation based on clustering for D2D communications in underlaying cellular networks. In Proceedings of the 2014 International Conference on Information and Communication Technology Convergence (ICTC), Busan, Republic of Korea, 22-24 October 2014; IEEE: Piscateville, NJ, USA, 2014; pp. 232-237.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_qBQ6qGs">D2D-assisted federated learning in mobile edge computing networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/wcnc49053.2021.9417459</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_N7WZ6eJ">Proceedings of the 2021 IEEE Wireless Communications and Networking Conference (WCNC)</title>
		<meeting>the 2021 IEEE Wireless Communications and Networking Conference (WCNC)<address><addrLine>Nanjing, China; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-04-01">29 March-1 April 2021. 2021</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, X.; Liu, Y.; Liu, J.; Argyriou, A.; Han, Y. D2D-assisted federated learning in mobile edge computing networks. In Proceedings of the 2021 IEEE Wireless Communications and Networking Conference (WCNC), Nanjing, China, 29 March-1 April 2021; IEEE: Piscateville, NJ, USA, 2021; pp. 1-7.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_hzuAMVV">Semi-Decentralized Federated Edge Learning for Fast Convergence on Non-IID Data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/wcnc51071.2022.9771904</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_evJm8de">Proceedings of the 2022 IEEE Wireless Communications and Networking Conference (WCNC)</title>
		<meeting>the 2022 IEEE Wireless Communications and Networking Conference (WCNC)<address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-04-13">10-13 April 2022</date>
			<biblScope unit="page" from="1898" to="1903" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sun, Y.; Shao, J.; Mao, Y.; Wang, J.H.; Zhang, J. Semi-Decentralized Federated Edge Learning for Fast Convergence on Non-IID Data. In Proceedings of the 2022 IEEE Wireless Communications and Networking Conference (WCNC), Austin, TX, USA, 10-13 April 2022; pp. 1898-1903. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_6b6PXT9">Semi-Decentralized Federated Learning With Cooperative D2D Local Model Aggregations</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hosseinalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Brinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Michelusi</surname></persName>
		</author>
		<idno type="DOI">10.1109/jsac.2021.3118344</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dr2JDRK">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="3851" to="3869" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lin, F.P.C.; Hosseinalipour, S.; Azam, S.S.; Brinton, C.G.; Michelusi, N. Semi-Decentralized Federated Learning With Cooperative D2D Local Model Aggregations. IEEE J. Sel. Areas Commun. 2021, 39, 3851-3869. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_vKTpMAk">Decentralized federated learning via SGD over wireless D2D networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simeone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bi</surname></persName>
		</author>
		<idno type="DOI">10.1109/spawc48557.2020.9154332</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eSGewSa">Proceedings of the 2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), Virtual</title>
		<meeting>the 2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), Virtual<address><addrLine>Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-05-29">26-29 May 2020. 2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="raw_reference">Xing, H.; Simeone, O.; Bi, S. Decentralized federated learning via SGD over wireless D2D networks. In Proceedings of the 2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC), Virtual, 26-29 May 2020; IEEE: Piscateville, NJ, USA, 2020; pp. 1-5.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_zcWJEeT">Learning-driven decentralized machine learning in resource-constrained wireless edge computing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/infocom42981.2021.9488817</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tBNWHEt">Proceedings of the IEEE INFOCOM 2021-IEEE Conference on Computer Communications</title>
		<meeting>the IEEE INFOCOM 2021-IEEE Conference on Computer Communications<address><addrLine>Vancouver, BC, Canada; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-05-13">10-13 May 2021. 2021</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note type="raw_reference">Meng, Z.; Xu, H.; Chen, M.; Xu, Y.; Zhao, Y.; Qiao, C. Learning-driven decentralized machine learning in resource-constrained wireless edge computing. In Proceedings of the IEEE INFOCOM 2021-IEEE Conference on Computer Communications, Vancouver, BC, Canada, 10-13 May 2021; IEEE: Piscateville, NJ, USA, 2021; pp. 1-10.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_v5HQNY5">Addressing algorithmic disparity and performance inconsistency in federated learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uRVhNsz">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="26091" to="26102" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cui, S.; Pan, W.; Liang, J.; Zhang, C.; Wang, F. Addressing algorithmic disparity and performance inconsistency in federated learning. Adv. Neural Inf. Process. Syst. 2021, 34, 26091-26102.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_9wGgNq6">Federated learning meets multi-objective optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shaloudegi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnse.2022.3169117</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Cpd7xP8">IEEE Trans. Netw. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2039" to="2051" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hu, Z.; Shaloudegi, K.; Zhang, G.; Yu, Y. Federated learning meets multi-objective optimization. IEEE Trans. Netw. Sci. Eng. 2022, 9, 2039-2051. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_7abjYBt">Pareto optimality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chinchuluun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karakitsiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mavrommati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Hu9xDym">Pareto Optimality, Game Theory and Equilibria</title>
		<meeting><address><addrLine>Berlin/Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="481" to="512" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chinchuluun, A.; Karakitsiou, A.; Mavrommati, A. Pareto optimality. In Pareto Optimality, Game Theory and Equilibria; Springer: Berlin/Heidelberg, Germany, 2008; pp. 481-512.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_jZrZNap">Evolutionary computation and convergence to a pareto front</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Van Veldhuizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Lamont</surname></persName>
		</author>
		<idno type="DOI">10.1007/bfb0027303</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7C5FPYg">Proceedings of the Late Breaking Papers at the Genetic Programming 1998 Conference</title>
		<meeting>the Late Breaking Papers at the Genetic Programming 1998 Conference<address><addrLine>Madison, WI, USA; Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1998-07-25">22-25 July 1998. 1998</date>
			<biblScope unit="page" from="221" to="228" />
		</imprint>
	</monogr>
	<note type="raw_reference">Van Veldhuizen, D.A.; Lamont, G.B. Evolutionary computation and convergence to a pareto front. In Proceedings of the Late Breaking Papers at the Genetic Programming 1998 Conference, Madison, WI, USA, 22-25 July 1998; Citeseer: Pittsburgh, PA, USA, 1998; pp. 221-228.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_9PpnhZb">Adaptive federated learning in resource constrained edge computing systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salonidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Makaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1109/jsac.2019.2904348</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Mm2ZaCV">IEEE J. Sel. Areas Commun</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1205" to="1221" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S.; Tuor, T.; Salonidis, T.; Leung, K.K.; Makaya, C.; He, T.; Chan, K. Adaptive federated learning in resource constrained edge computing systems. IEEE J. Sel. Areas Commun. 2019, 37, 1205-1221. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_hqn6mMd">Feasible D2D communication distance in D2D-enabled cellular networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccs.2014.7024754</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_grGmCzZ">Proceedings of the 2014 IEEE International Conference on Communication Systems</title>
		<meeting>the 2014 IEEE International Conference on Communication Systems<address><addrLine>Macau, China; Piscateville, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-11-21">19-21 November 2014. 2014</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ding, H.; Ma, S.; Xing, C. Feasible D2D communication distance in D2D-enabled cellular networks. In Proceedings of the 2014 IEEE International Conference on Communication Systems, Macau, China, 19-21 November 2014; IEEE: Piscateville, NJ, USA, 2014; pp. 1-5.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<idno type="DOI">10.1016/j.mehy.2008.11.003</idno>
		<title level="m" xml:id="_FKFR7p9">The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of</title>
		<imprint/>
	</monogr>
	<note>Disclaimer/Publisher&apos;s Note MDPI and/or the editor(s MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content</note>
	<note type="raw_reference">Disclaimer/Publisher&apos;s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
