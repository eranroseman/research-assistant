<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_rdx4N9f">Reinforcement learning for healthcare operations management: methodological framework, recent developments, and future research directions</title>
				<funder ref="#_aq2N27c">
					<orgName type="full">Health and Medical Research Fund of Health Bureau of Hong Kong</orgName>
				</funder>
				<funder ref="#_MwgnCP6">
					<orgName type="full">General Research Fund of Research Grants Council of Hong Kong</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">© The Author(s) 2025</p>
				</availability>
				<date type="published" when="2025-04-09">9 April 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qihao</forename><surname>Wu</surname></persName>
							<email>qihaowu@connect.hku.hkjiangxue</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Data and Systems Engineering , The University of Hong Kong , Hong Kong , China</note>
								<orgName type="department">Department of Data and Systems Engineering</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiangxue</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Data and Systems Engineering , The University of Hong Kong , Hong Kong , China</note>
								<orgName type="department">Department of Data and Systems Engineering</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yimo</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Data and Systems Engineering , The University of Hong Kong , Hong Kong , China</note>
								<orgName type="department">Department of Data and Systems Engineering</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong-Hong</forename><surname>Kuo</surname></persName>
							<email>yhkuo@hku.hkqihao</email>
							<idno type="ORCID">0000-0002-6170-324X</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Data and Systems Engineering , The University of Hong Kong , Hong Kong , China</note>
								<orgName type="department">Department of Data and Systems Engineering</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zuo-Jun</forename><forename type="middle">Max</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Faculty of Engineering and Business School , The University of Hong Kong , Hong Kong , China</note>
								<orgName type="department">Faculty of Engineering and Business School</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Department of Industrial Engineering &amp; Operations Research , University of California , Berkeley , Berkeley , California , USA</note>
								<orgName type="department">Department of Industrial Engineering &amp; Operations Research</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_mVWA6MG">Reinforcement learning for healthcare operations management: methodological framework, recent developments, and future research directions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-04-09">9 April 2025</date>
						</imprint>
					</monogr>
					<idno type="MD5">56F7A6AED74CFE854EF3EC84389349D4</idno>
					<idno type="DOI">10.1007/s10729-025-09699-6</idno>
					<note type="submission">Received: 27 March 2024 / Accepted: 8 February 2025 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_rZGQqbx">Reinforcement learning</term>
					<term xml:id="_EzhsDzd">Healthcare operations</term>
					<term xml:id="_NS4bbyd">Healthcare services delivery</term>
					<term xml:id="_mps8CCY">Markov decision process</term>
					<term xml:id="_Q7MuraT">Approximate dynamic programming</term>
					<term xml:id="_Ug59e38">Neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_XD3DeNC"><p xml:id="_wecPrre"><s xml:id="_CXPXPnu">With the advancement in computing power and data science techniques, reinforcement learning (RL) has emerged as a powerful tool for decision-making problems in complex systems.</s><s xml:id="_XYgG4x2">In recent years, the research on RL for healthcare operations has grown rapidly.</s><s xml:id="_e5GHx8K">Especially during the COVID-19 pandemic, RL has played a critical role in optimizing decisions with greater degrees of uncertainty.</s><s xml:id="_aSbm4mV">RL for healthcare applications has been an exciting topic across multiple disciplines, including operations research, operations management, healthcare systems engineering, and data science.</s><s xml:id="_yfN25JU">This review paper first provides a tutorial on the overall framework of RL, including its key components, training models, and approximators.</s><s xml:id="_EueBscY">Then, we present the recent advances of RL in the domain of healthcare operations management (HOM) and analyze the current trends.</s><s xml:id="_A9EKb8U">Our paper concludes by presenting existing challenges and future directions for RL in HOM.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_jKSVABK">Introduction</head><p xml:id="_SyuXFpY"><s xml:id="_VvSxxQD">Healthcare operations management (HOM) plays a crucial role in the effective functioning of healthcare systems, impacting public health, patient experience, and healthcare organizational goals.</s><s xml:id="_hccRAXB">It encompasses various practices that aim to ensure high-quality care, optimize healthcare delivery, and improve overall healthcare outcomes.</s><s xml:id="_rYnK7zH">These practices span a wide range of applications, including patient flow scheduling, medical resources distribution, healthcare process improvement, and pharmaceutical supply chain management <ref type="bibr" target="#b0">[1]</ref>.</s></p><p xml:id="_rGESab9"><s xml:id="_be5ZfuZ">In the field of operations research, traditional approaches to tackling these healthcare operations challenges involve mathematical programming techniques such as mixed-integer programming (MIP) and dynamic programming (DP).</s><s xml:id="_f4QDzct">These methods formulate the problems as mathematical models and seek optimal solutions by optimizing the specific objective(s).</s><s xml:id="_kz4BmAn">However, as the complexity and scale of these problems increase, conventional mathematical programming approaches may struggle to find optimal solutions due to the curse of dimensionality <ref type="bibr" target="#b1">[2]</ref>.</s></p><p xml:id="_brZq8aJ"><s xml:id="_fP8vuuU">With the emergence of computing and data science (DS) advancements, an abundance of accessible data and techniques has inundated healthcare operations.</s><s xml:id="_Qa6UPCg">Although this presents opportunities for HOM practices, it also poses unprecedented challenges <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_qhNfJEA">Mastery of machine learning (ML) and DS techniques is imperative to make good use of such data for better decisions.</s><s xml:id="_ZfMTwbk">As a branch of ML, reinforcement learning (RL) optimizes sequential decision-making problems by employing an intelligent agent to determine actions in a dynamic environment.</s><s xml:id="_8CUZPSc">Based on prior experience, the RL agent can learn how to make decisions given the current information, effectively mitigating the curse of dimensionality.</s><s xml:id="_65pbxWF">RL approaches are developed based on the framework of a Markov decision process (MDP), which is a typical modeling framework in the HOM context.</s><s xml:id="_tNeK57n">For instance, hospital operations managers determine inpatient admission and discharge on a daily basis given the available information (e.g., patients of different classes in the queues and inpatient beds available) while optimizing multiobjective factors such as health outcomes, operating costs, and benefits.</s><s xml:id="_XyrBapW">Therefore, RL has gained significant popularity and widespread adoption in HOM.</s></p><p xml:id="_Da8QFmf"><s xml:id="_zNKkUMX">The application of RL in the healthcare domain presents an attractive opportunity for improving healthcare operations.</s><s xml:id="_9Zz9rMb">From a broader perspective, existing surveys on RL in healthcare have discussed various instances across different domains, with a focus on dynamic treatment regimes, interventions, and automated medical diagnosis <ref type="bibr" target="#b3">[4]</ref> and clinical decision support <ref type="bibr" target="#b4">[5]</ref>.</s><s xml:id="_EDgsAQP">However, a review of RL in HOM is currently not available.</s><s xml:id="_53HJVJV">Furthermore, the COVID-19 pandemic has severely impacted the global health systems in recent years <ref type="bibr" target="#b5">[6]</ref>, resulting in various HOM problems due to limited resources such as insufficient COVID-19 test kits for distribution in communities and access block at hospitals <ref type="bibr" target="#b6">[7]</ref>.</s><s xml:id="_wcfHmCa">In response, a plethora of RL applications have been employed in corresponding HOM practices.</s><s xml:id="_JVzEBPH">Our paper aims to provide a comprehensive analysis of RL applications in HOM, reviewing the existing methodologies and state-of-the-art solutions.</s><s xml:id="_rjYE3dJ">To the best of our knowledge, this review is the first of its kind to analyze RL applications in HOM.</s><s xml:id="_f3j9YeC">Through this review, specific research questions can be answered:</s></p><p xml:id="_294g7HU"><s xml:id="_shUVnzw">1. Which types of HOM problems hold great potential for deploying RL methods, and how can RL contribute to solving these problems effectively?</s><s xml:id="_ZSAbNC2">2. Which RL methods are most appropriate and effective in addressing specific HOM challenges? 3. What are the main challenges associated with deploying RL in HOM, and how can these challenges be addressed?</s><s xml:id="_kNRXjKV">Furthermore, what are the future directions for RL in HOM, and how can researchers and practitioners leverage RL to enhance healthcare operations?</s></p><p xml:id="_9nYzVUC"><s xml:id="_YQUmM8r">Our review is structured as follows.</s><s xml:id="_GstRNsk">In Sect.</s><s xml:id="_7vpayA4">2, we offer a tutorial on the RL methodologies that are utilized in healthcare operations, providing a necessary understanding of the required methodologies.</s><s xml:id="_6ZWgyRs">Section 3 outlines the scope of our review and describes the methodology employed for the literature search.</s><s xml:id="_pXScPka">In Sect.</s><s xml:id="_pDux7vt"><ref type="bibr" target="#b3">4</ref>, we delve into the specific RL applications within the realm of HOM, classifying them according to their respective scopes and providing detailed analyses of each application.</s><s xml:id="_pbfvwYG">Section 5 summarizes the key insights gained from the reviewed applications and discusses future directions for the utilization of RL in HOM.</s><s xml:id="_WQMj279">We highlight potential areas of growth and identify challenges that need to be addressed in order to fully leverage the potential of RL in HOM.</s><s xml:id="_tdjBbKy">Finally, Sect.</s><s xml:id="_JrKEduM">6 presents the conclusion that brings together the key findings and contributions of our review.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_8WeT6jN">Methodologies</head><p xml:id="_28EsVfM"><s xml:id="_FYhwkYQ">This section serves as a tutorial that presents the fundamentals of RL relevant to the applications in HOM.</s><s xml:id="_rwdPHsN">Our tutorial is structured in a manner consistent with other tutorials in different application domains, such as RL for transportation and logistics operations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>.</s></p><p xml:id="_Zgdpt9a"><s xml:id="_P2dzsba">We start with the essential mathematical framework for modeling sequential decision-making, MDPs <ref type="bibr" target="#b9">[10]</ref>, which can typically be solved by DP methods using <ref type="bibr">Bellman's equation [11]</ref>.</s><s xml:id="_FFRVvTq">However, classical DP methods may easily fail due to the curse of dimensionality (e.g., in those largescale or intractable problems).</s><s xml:id="_ZwfE3Ed">In this case, RL approaches, which are closely related to DP <ref type="bibr" target="#b10">[12]</ref>, present an alternative class of methods for MDPs and sequential decision-making.</s><s xml:id="_Pnczw7J">Fundamentally, RL algorithms differ from classical DP methods in that they do not always require a complete system model <ref type="bibr" target="#b11">[13]</ref>.</s><s xml:id="_dnrsxSd">Hence, they are designed to handle larger-scale problems where traditional exact methods may face computational infeasibility <ref type="bibr" target="#b12">[14]</ref>.</s><s xml:id="_w8WZ8t4">Nevertheless, RL can also greatly benefit from system models, if applicable, with model-based methods <ref type="bibr" target="#b13">[15]</ref>.</s></p><p xml:id="_W2KPf9K"><s xml:id="_jtQE5Dq">The RL paradigm is rooted in the vast domain of MDP and sequential decision-making, which often intersects with different subjects and research communities such as operations research (OR), optimal control, computer science (CS), and artificial intelligence (AI).</s><s xml:id="_sU8KgEE">RL is known by various synonyms in different disciplines, such as approximate dynamic programming (ADP) <ref type="bibr" target="#b14">[16]</ref>, adaptive dynamic programming <ref type="bibr" target="#b15">[17]</ref>, neuro-dynamic programming <ref type="bibr" target="#b16">[18]</ref>, heuristic dynamic programming <ref type="bibr" target="#b17">[19]</ref>, and etc.</s><s xml:id="_GaEhF4f">From the perspective of the OR, researchers usually refer to such approaches as ADP.</s><s xml:id="_BsUp46j">Powell <ref type="bibr" target="#b14">[16]</ref> claimed that ADP is practiced under the umbrella of RL, and also classified classic RL methods (e.g., Q-learning <ref type="bibr" target="#b18">[20]</ref>) in ADP.</s><s xml:id="_K9EPHBM">From the perspective of the CS and AI community, RL is a popular terminology, especially with the recent advances in deep learning <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_jUShR93">The use of MDP in RL (i.e., MDP serves as the environment of RL) has been broadly adopted <ref type="bibr" target="#b10">[12]</ref>.</s></p><p xml:id="_kDGck6j"><s xml:id="_2aGqByh">In this section, we will walk through certain RL methods for HOM adopted by both the OR and CS communities.</s><s xml:id="_6Pj9vNp">Key terminologies in both communities, such as ADP and deep reinforcement learning (DRL), are discussed.</s><s xml:id="_baPcb9F">Section 2.1 first introduces MDP, which is considered the basic mathematical 123 foundation of RL <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b20">22]</ref>, and briefly discusses classical DP methods (e.g., value iteration <ref type="bibr">[11]</ref>).</s><s xml:id="_fVUyj9X">Section 2.2 reviews typical RL methods in HOM, by which the formalized MDP can be optimally solved.</s><s xml:id="_DbhUWDj">Our discussion on RL methodologies begins with standard ADP approaches <ref type="bibr" target="#b14">[16]</ref> to classic RL methods (e.g., temporal difference learning <ref type="bibr" target="#b21">[23]</ref>), followed by DRL with neural networks.</s><s xml:id="_ASfmSKQ">Then, other popular RL techniques, such as policy gradient and bandit problems, are presented <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_xC4e4ex">We also intend to present the evolution of RL from classical DP to the recent RL framework that conglomerates multiple aspects.</s><s xml:id="_rdw57Xv">Our paper focuses on RL in HOM, and this section only serves as a high-level tutorial on the RL methods.</s><s xml:id="_ZH9RSdn">The reader is recommended to refer to classic references in ADP <ref type="bibr" target="#b14">[16]</ref> and RL <ref type="bibr" target="#b19">[21]</ref> for more comprehensive discussions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_BA9v67F">Markov decision process</head><p xml:id="_Y2RzPzB"><s xml:id="_dSxVNvX">The principle of RL is built upon MDP, which models sequential decision-making problems.</s><s xml:id="_gmNxj5P">The decision stages (or stages, for simplicity) of the problem are discretized into T periods, where t = 0, 1, 2, . . .</s><s xml:id="_mAVu4En">, T and T is also known as the horizon (i.e., the time of termination of the MDP).</s><s xml:id="_XHqzBHc">MDP is typically represented by a tuple &lt; S, A, P, R, γ &gt; <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25]</ref>.</s><s xml:id="_djjRkzw">Specifically,</s></p><p xml:id="_xQuK3b3"><s xml:id="_FfAGMNB">• S is a set of states, and the state at stage t is denoted as S t ∈ S. The state reflects the system's or environment's behaviors at every stage via state variables.</s><s xml:id="_dU9JNYf">In a finitehorizon MDP, the state sequence is S 0 , S 1 , . . .</s><s xml:id="_BSHWMbP">, S T .</s><s xml:id="_DQ9AK3u">For the sake of illustrative purposes, this section focuses on finite-horizon MDPs; for MDPs with an infinite horizon, most terminologies and definitions presented here could possibly be generalized for T → ∞. • A is a set of actions, and the action at the t th stage is A t ∈ A. Based on the observed state S t at stage t, an action A t is chosen from a set of possible actions A. Similarly, in a finite-horizon MDP, the sequence of actions is denoted as A 0 , A 1 , . . .</s><s xml:id="_yTB32nK">, A T .</s><s xml:id="_PMxgqhT">• P is a transition probability matrix.</s><s xml:id="_Fxfjmt7">Its element, P a ss = P S t+1 = s | S t = s, A t = a , measures the probability of transitioning from the current state s with action a (at stage t) to the next state s (at stage t + 1).</s></p><formula xml:id="formula_0">• R is a reward function, R(s, a) = E[R t+1 | S t = s, A t = a].</formula><p xml:id="_g3PkmZ5"><s xml:id="_f6dgNZB">Here, R t+1 is the immediate reward generated from the environment given the state-action pair at stage t + 1.</s><s xml:id="_VD3WjsD">The state-action-reward sequence can be written as S 0 , A 0 , R 1 , S 1 , A 1 , R 2 , . .</s><s xml:id="_5vtjNYN">.. • γ is a discount factor, where γ ∈ [0, 1].</s><s xml:id="_cwuCfrP">It defines the discounted fraction of a future reward.</s><s xml:id="_x3ZCEnh">Specifically, for a reward r obtained after t + 1 stages in the future, its present value would be discounted as γ t r .</s></p><p xml:id="_gmr4ccD"><s xml:id="_anSW2AN">An essential characteristic of MDP is that the next state S t+1 of the system is only dependent on the current state S t of the system, and is independent of the history <ref type="bibr">[26]</ref>, such that P[S t+1 | S t ] = P[S t+1 | S 1 , . . .</s><s xml:id="_PtXeJmq">, S t ].</s></p><p xml:id="_3pT4HF8"><s xml:id="_RK8jFNJ">In MDP, a policy π defines the rules to choose an action given a state.</s><s xml:id="_DbJCeUx">A deterministic policy maps states to specific actions directly, i.e., adopting exactly the same action in each state.</s><s xml:id="_FBdwDe3">On the other hand, a stochastic policy can be regarded as a mapping from states to probabilities of choosing actions, i.e., π(a | s) = P[A t = a | S t = s] <ref type="bibr" target="#b24">[27]</ref>.</s><s xml:id="_PwJxgeh">Given policy π , the value (state-value) function v π <ref type="bibr">(s)</ref>, which evaluates the value of being in state s, can be written as Eq. 1.</s></p><formula xml:id="formula_1">v π (s) := E π R t+1 + γ v π (S t+1 ) | S t = s (1)</formula><p xml:id="_aJF97de"><s xml:id="_vRcPgVR">The action-value function, denoted q π (s, a), can be written as Eq. 2. The derivations are based on the widely used Bellman's equation <ref type="bibr">[11]</ref>.</s></p><formula xml:id="formula_2">q π (s, a) := E π [R t+1 + γ q π (S t+1 , A t+1 ) | S t = s, A t = a]<label>(2)</label></formula><p xml:id="_jE3guAQ"><s xml:id="_r9GCKEt">Eq. 3 presents the objective function of MDP, which aims to maximize the cumulative expected rewards collected over the entire horizon, i.e., 0, 1, 2,..., T .</s></p><p xml:id="_5muKUTR"><s xml:id="_Rk4gzDv">max</s></p><formula xml:id="formula_3">π E π T -1 t=0 γ t R t+1 S 0<label>(3)</label></formula><p xml:id="_3WrfA6q"><s xml:id="_88KrmU3">To solve this problem, the optimal state-value function v * (s) and the optimal action-value function q * (s, a) need to be identified, and these optimal value functions are associated with an optimal policy π * <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b25">28]</ref>, which can be determined with Eq. 4.</s></p><formula xml:id="formula_4">π * (a | s) = ⎧ ⎨ ⎩ 1 if a = argmax a ∈A q * (s, a ); 0 otherwise (4)</formula><p xml:id="_9UpTrdf"><s xml:id="_s5G32B6">where q * (s, a) = max π q π (s, a) is the optimal action-value function.</s><s xml:id="_bM4HdcZ">Similarly, v * (s) = max π v π (s) is the optimal statevalue function.</s></p><p xml:id="_dnc584K"><s xml:id="_N5VyuAS">There is a wide range of real-world problems that can be modeled as MDPs.</s><s xml:id="_YsCef4S">For example, there were reports of significantly long patient boarding times from emergency departments to inpatient wards <ref type="bibr" target="#b26">[29]</ref>, especially during the pandemic <ref type="bibr" target="#b6">[7]</ref>.</s><s xml:id="_tYXgET4">In <ref type="bibr">Dai and Shi [30]</ref>, an MDP framework considering inpatient overflow was proposed for patient boarding decisions.</s><s xml:id="_aV5t3Du">If the primary wards for the patient (i.e., the wards that offer the most effective medical services to the patient) are fully occupied, an overflow policy would allow transfers of patients to other non-primary wards.</s><s xml:id="_XxPyJMJ">In this case, the inpatient operations decisions can be determined by an MDP representing a multi-class, multi-pool queuing system.</s><s xml:id="_PJKNYtw">Every state encapsulates information such as patients in each queue at the moment and possible discharges of patients.</s><s xml:id="_e4NMW7P">Given the state, the action of assigning patients to different wards needs to be determined at each decision stage in the planning horizon.</s><s xml:id="_uq3bcGv">This action aims to balance the costs associated with patient overflow and prolonged patient boarding times.</s><s xml:id="_gMdH2HM">After an action is determined, the state transits to the next according to the transition probability matrix.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1" xml:id="_aHeSUZe">Dynamic programming</head><p xml:id="_btXeMx5"><s xml:id="_8mNwdDg">When the problem scale is manageable, classical DP methods could be deployed to solve the MDP.</s><s xml:id="_tDGfh2E">Value iteration is one of the most widely used approaches <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_tpF7nDQ">The principle of value iteration is to estimate the value function of each state via an iteration procedure.</s><s xml:id="_Q6Smcdb">An initialization procedure (at iteration n = 0) sets an estimated value of the function, denoted as v 0 (s), to zero.</s><s xml:id="_QMnsbrv">By using the Bellman's optimality equation <ref type="bibr">[11]</ref>,</s></p><formula xml:id="formula_5">v n+1 (s) = max a∈A E R t+1 + γ v n (S t+1 ) | S t = s, A t = a</formula><p xml:id="_wtFYvBS"><s xml:id="_5x2MyW6">At iteration n, the estimated value of the value function, v n (s), is updated for each s ∈ S accordingly <ref type="bibr" target="#b14">[16]</ref>:</s></p><formula xml:id="formula_6">v n (s) = max a∈A R(s, a) + γ s ∈S P s | s, a v n-1 s (5)</formula><p xml:id="_9nnhSuV"><s xml:id="_f3NqgW5">Value iteration <ref type="bibr" target="#b19">[21]</ref> claims the principle of optimality, v π (s) = v * (s), if and only if v π (s ) = v * (s ) for any state s' reachable from s <ref type="bibr" target="#b25">[28]</ref>.</s><s xml:id="_TnRw25a">Another DP approach, policy iteration <ref type="bibr" target="#b19">[21]</ref>, iteratively alternates between policy evaluation and policy improvement until the policy converges to the optimum.</s></p><p xml:id="_eJgMReB"><s xml:id="_AmG2yWp">Such recursive iterations could encounter computational challenges arising from the curse of dimensionality, given that the iterations may need to traverse all the combinations of available states, actions, and transitions <ref type="bibr" target="#b14">[16,</ref><ref type="bibr">26]</ref>.</s><s xml:id="_DYJJWqV">Therefore, methodologies with approximations would be essential to providing practical solutions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_dyFQZja">Reinforcement learning</head><p xml:id="_gYyvsud"><s xml:id="_Ke95d9w">Based on MDPs, RL approaches could be developed to address these dynamic sequential decision-making challenges.</s><s xml:id="_KUVNk7a">For comprehensive reviews of different classes of RL methods and their applications in various domains, we refer the reader to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">[32]</ref><ref type="bibr" target="#b29">[33]</ref><ref type="bibr" target="#b30">[34]</ref><ref type="bibr" target="#b31">[35]</ref><ref type="bibr" target="#b32">[36]</ref><ref type="bibr" target="#b33">[37]</ref><ref type="bibr" target="#b34">[38]</ref><ref type="bibr" target="#b35">[39]</ref>.</s><s xml:id="_QGBNxg2">In this section, we present the fundamentals of RL methods that have been applied in HOM.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1" xml:id="_tkRkqcB">Approximate dynamic programming</head><p xml:id="_WHWKgXj"><s xml:id="_ByPTM4j">Approximate dynamic programming (ADP) is designed for solving large-scale problems and overcoming the "curse of dimensionality" by constructing approximations of value functions (known as value function approximation).</s><s xml:id="_RAs2Xm2">This subsection introduces how the value function v(s) of state s can be approximated with Monte Carlo sampling.</s><s xml:id="_txvyag4">In RL, an episode refers to the sequence of agent-environment interactions starting from an initial state and ending in a terminal state, which can be used for learning or evaluating a policy <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_yTJbMch">The principle of Monte Carlo methods is learning from the episodic experience and updating the value estimates based on the average returns observed from episodes <ref type="bibr" target="#b19">[21]</ref>.</s></p><p xml:id="_fvJB6P7"><s xml:id="_wWsRBHu">Typically, ADP involves a rollout process.</s><s xml:id="_dShJxCB">In the first phase of ADP, states and possible actions are given to an approximator to derive an approximate solution.</s><s xml:id="_fA29TA8">The states and actions, as in the context of ADP, are usually simpler and of lower dimension than those in the original problem.</s><s xml:id="_k7Wk5qa">Once an approximate solution is derived, in the second phase, this approximation is iteratively updated and guides the decisionmaking process in the original problem <ref type="bibr" target="#b36">[40]</ref>.</s></p><p xml:id="_2HSxUXf"><s xml:id="_fgzr3tV">A standard ADP algorithm starts with an initial approximated value function V 0 t (S t ) for all states S t and iteratively updates it in a forward direction based on value iteration (i.e., Eq. 5).</s><s xml:id="_RJnKc6U">At iteration n, a sample path ω n , which refers to a sequence of exogenous information of the system that defines the realizations in all time periods <ref type="bibr" target="#b14">[16]</ref>, is generated (e.g., by Monte Carlo simulation).</s><s xml:id="_aM2kpPH">Given ω n , at any stage t, we would be at a realized state S n t and need to take an action a n t .</s><s xml:id="_vDVP2xP">A sampled value vn t at stage t is computed by solving the maximization problem defined in Eq. 6.</s></p><formula xml:id="formula_7">vn t = max a t ∈A R t+1 S n t , a t + γ s ∈S P s | S n t , a t V n-1 t+1 s (6)</formula><p xml:id="_nUDaAbF"><s xml:id="_pHnxMUc">where V n t (s) is the estimated value of the value function at state s after n sample observations.</s><s xml:id="_Z864d6G">From Eq. 6, a n t ∈ A is set to the optimal action for the maximization problem Eq. 6.</s><s xml:id="_6e98SuY">Also, V n t (s) can be updated via Eq.</s><s xml:id="_483EuBt">7:</s></p><formula xml:id="formula_8">V n t (S t ) = vn t , S t = S n t V n-1 t (S t ) , otherwise<label>(7)</label></formula><p xml:id="_WJkP6qy"><s xml:id="_mSQT3s6">After V n t (S t ) is updated for all t = 0, 1, 2, ..., T , n is advanced to n + 1.</s><s xml:id="_cHfAz6J">The iteration repeats until it reaches the preset number of iterations N .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4bgW2rk">Finding vn</head><p xml:id="_67W8CRU"><s xml:id="_ecMGDfd">t and a n t via solving the maximization problem Eqs. 6, 7 can be problematic.</s><s xml:id="_5Vhp258">It could be because the state space is huge and the problem is computationally challenging.</s><s xml:id="_jKAA7he">Another practical challenge is that the transition function or reward function may not even be known or not mathematically computable <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_fQ9nZYr">Thus, there are two classes of RL methods that aim to solve the maximization problem Eq. 6: model-based and model-free approaches.</s><s xml:id="_y8rXJJP">Model-based RL learns or has access to a model of the environment such that the transition dynamics and reward function can be modeled <ref type="bibr">[41]</ref>.</s><s xml:id="_BNqFtD5">On the contrary, without an explicit mathematical model of the environment, model-free RL directly learns from experiences or interactions with the environment through trial and error <ref type="bibr" target="#b8">[9]</ref>.</s></p><p xml:id="_p8xSfc7"><s xml:id="_n564Kvb">Researchers from the OR and CS communities may use different names (e.g., "approximate dynamic programming" and "reinforcement learning") to refer to similar RL paradigms <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b19">21]</ref>.</s><s xml:id="_3nXFn7P">OR researchers typically develop various ADP techniques using mathematically tractable formulations to solve MDPs <ref type="bibr" target="#b38">[42]</ref>.</s><s xml:id="_rgpgtyj">On the other hand, CS researchers focus on improving the algorithmic performance of RL methods and approximations in policy space, which will be discussed in subsequent subsections.</s><s xml:id="_TBDEmj6">We consider that the terminology "ADP" emphasizes more on the relationships with the system model, while "RL" emphasizes the approximations by learning <ref type="bibr">[41]</ref>.</s><s xml:id="_pUmXsja">In general, all these RL methods aim to address the challenge of handling high-dimensional problems by using approximations.</s><s xml:id="_xgAM4nB">As healthcare problems have become increasingly large-scale, complex, and dynamic, ADP offers an efficient approach to address various HOM applications <ref type="bibr" target="#b39">[43]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2" xml:id="_jWdPKm8">Temporal difference learning</head><p xml:id="_UqTqN5F"><s xml:id="_fveQ3x6">Temporal difference (TD) learning refers to a popular class of model-free RL algorithms that update the approximations of true value functions (i.e., v π (s) or q π (s, a)) based in part on other approximations (i.e., observed samples, for example, vn t ) <ref type="bibr" target="#b21">[23]</ref>.</s><s xml:id="_FGvz9UG">This general idea is bootstrapping <ref type="bibr" target="#b19">[21]</ref>, and the temporal difference (also known as the Bellman error) is the error in our approximations <ref type="bibr" target="#b14">[16]</ref>.</s><s xml:id="_xXPTEHJ">TD learning neither requires the episodic outcome nor the complete model of the environment <ref type="bibr" target="#b19">[21]</ref>.</s></p><p xml:id="_ygR3qJE"><s xml:id="_UgJUBA2">When approximating the state-value function, the simplest TD method is one-step TD that makes the update immediately based on observed vn t , as shown in Eq. 8, where α is the stepsize parameter <ref type="bibr" target="#b19">[21]</ref>.</s></p><formula xml:id="formula_9">V n t (S n t ) = (1 -α)V n-1 t (S n t ) + α vn t = V n-1 t (S n t )+ α R(S n t , a n t ) + γ V n-1 t+1 (S n t+1 ) -V n-1 t (S n t )<label>(8)</label></formula><p xml:id="_JS7sEAS"><s xml:id="_w46CewY">The need for α (i.e., smoothing) arises from the stochastic nature of vn t , which is a consequence of the way employed to estimate the expectation (i.e., model of the environment) <ref type="bibr" target="#b14">[16]</ref>.</s><s xml:id="_BBZRtXs">According to sampled exogenous information between t and t +1 <ref type="bibr" target="#b14">[16]</ref>,</s></p><formula xml:id="formula_10">vn t = R(S n t , a n t )+γ V n-1 t+1 (S n t+1</formula><p xml:id="_Qb9VcZx"><s xml:id="_BArCM9n">) is generated on the basis of the transition from S n t to S n t+1 using a policy (i.e., a n t ) and receiving the reward R(S n t , a n t ).</s><s xml:id="_vAReBnC">In Eq.8, the term R(S n t ,</s></p><formula xml:id="formula_11">a n t ) + γ V n-1 t+1 (S n t+1 ) -V n-1 t</formula><p xml:id="_eXjD4xj"><s xml:id="_BaAY7yV">(S n t ) refers to the temporal difference <ref type="bibr" target="#b14">[16]</ref>.</s></p><p xml:id="_ygKH7kC"><s xml:id="_DN7TR6f">Approximating the action-value function essentially follows similar approaches for approximating the state-value function previously presented <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_UdRYxwF">TD control first learns the action-value function rather than the state-value function.</s><s xml:id="_3Na6QFr">TD control can be implemented via on-policy or off-policy methods.</s><s xml:id="_yW8RJ3A">In RL, the policy guiding action selection and subsequent state transition is known as the behavior policy, realizing the outcome given the exogenous information.</s><s xml:id="_5QbK4zp">HOM applications usually utilize simulation techniques to generate sufficient sample paths <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b41">45]</ref>.</s><s xml:id="_Fu6prVM">In the context of simulation, behavior policy could be adopted to control the process of sampling states, which is referred to as sampling policy <ref type="bibr" target="#b14">[16]</ref>.</s><s xml:id="_UgkSjAR">The policy, which chooses the action that appears to be the best, is referred to as the target policy (or also known as the learning policy) <ref type="bibr" target="#b14">[16]</ref>.</s><s xml:id="_UJNvVVQ">On-policy methods, such as State-Action-Reward-State-Action (SARSA) <ref type="bibr" target="#b42">[46]</ref>, improve the target policy that is the same as the sampling policy, whereas off-policy methods improve the target policy that is different from the sampling policy <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b19">21]</ref>.</s></p><p xml:id="_mtU3rAX"><s xml:id="_YAstBdX">A typical off-policy method, Q-learning (QL) <ref type="bibr" target="#b18">[20]</ref> starts with the initialization of action-value function approximation Q 0 t (S t , a t ) for all states S t ∈ S and actions a t ∈ A and iteratively updates the values.</s><s xml:id="_xmFZX9T">At iteration n, a n t is determined by the sampling policy via Eq.</s><s xml:id="_4sfHNG9">9:</s></p><formula xml:id="formula_12">a n t = arg max a t ∈A Q n-1 t S n t , a t<label>(9)</label></formula><p xml:id="_fJN9mds"><s xml:id="_MgPMqez">Then, Q n t (S n t , a n t ) is updated via Eqs. 10 and 11:</s></p><formula xml:id="formula_13">qn t = R S n t , a n t + γ max a ∈A Q n-1 t+1 S n t+1 , a<label>(10)</label></formula><formula xml:id="formula_14">Q n t S n t , a n t = (1 -α)Q n-1 t S n t , a n t + α qn t = Q n-1 t S n t , a n t + α[R S n t , a n t + γ max a ∈A Q n-1 t+1 S n t+1 , a -Q n-1 t S n t , a n t ]<label>(11)</label></formula><p xml:id="_e5ETmhe"><s xml:id="_f6NeACR">Then, the transition to S n t+1 and the reward R(S n t , a n t ) are obtained based on the exogenous system information (e.g., from a sample path) observed at stage t.</s><s xml:id="_C6p4Ys4">In Eq. 10, QL (off-policy) includes a maximization problem</s></p><formula xml:id="formula_15">max a ∈A Q n-1 t+1 S n</formula><p xml:id="_7bdunK6"><s xml:id="_9pvHHhf">t+1 , a to select an action for the update.</s><s xml:id="_whSCqDB">Instead, SARSA (on-policy) replaces this problem with Q n-1 t+1 (S n t+1 , a n t+1 ), where a n t+1 is generated following the same policy that determines a n t (i.e., in Eq. 9) <ref type="bibr" target="#b14">[16]</ref>.</s><s xml:id="_bvC9PdW">Given a set of approximated Q (action-value) functions Q n (s, a), the approximated state-value function can be computed using Eq. 12 <ref type="bibr" target="#b14">[16]</ref>.</s></p><formula xml:id="formula_16">V n (s) = max a∈A Q n (s, a)<label>(12)</label></formula><p xml:id="_zjgb4mc"><s xml:id="_YTZsw4m">In this way, Eq. 10 can be reformulated as Eq. 13.</s></p><formula xml:id="formula_17">qn t = R S n t , a n t + γ V n-1 t+1 S n t+1 (<label>13</label></formula><formula xml:id="formula_18">)</formula><p xml:id="_BpQdcXD"><s xml:id="_h8kGrP4">When comparing qn t in Eq. 13 with vn t in Eq. 6, the embedded expectation over the downstream states that arise from action a t have to be calculated to identify vn t in Eq. 6; however, this step is always not computational efficiency.</s><s xml:id="_76pSykN">On the contrary, QL takes an action following the sampling policy and observes the downstream state given the exogenous information.</s><s xml:id="_65FNnWu">In HOM studies that feature finite and discrete actions, QL addresses problems that traditional DP can hardly resolve, such as routing problems in rescuing and emergency services <ref type="bibr" target="#b43">[47]</ref><ref type="bibr" target="#b44">[48]</ref><ref type="bibr" target="#b45">[49]</ref><ref type="bibr" target="#b46">[50]</ref>.</s></p><p xml:id="_GhWStN9"><s xml:id="_u9Zr6yX">TD methods leverage the advantages of the use of bootstrapping in DP and the sampling capabilities of Monte Carlo simulation <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_yD52KRB">TD learning methods can be unified as TD(λ) according to the use of eligibility traces <ref type="bibr" target="#b21">[23]</ref>, λ ∈ [0, 1], which represent the algorithmic discount to control the weights for expected rewards from different decision stages <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_YcbMb4n">For example, the a higher value of λ leads to greater weights of rewards that are from distant states and actions.</s><s xml:id="_zWNKjg9">TD(0) (i.e., one-step TD <ref type="bibr" target="#b19">[21]</ref>) uses one future reward R S n t , a n t to update the value function approximation (Eq.</s><s xml:id="_EWsgETv">8), while TD(1) implements a Monte Carlo algorithm <ref type="bibr" target="#b25">[28]</ref> that updates the value function approximation using episodic outcomes <ref type="bibr" target="#b19">[21]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3" xml:id="_D9SCdcX">Value function approximation</head><p xml:id="_dN67jAj"><s xml:id="_dwAgS6m">This subsection discusses several popular approximate solution methods in HOM.</s><s xml:id="_VzJxQrU">In basic settings of ADP and TD methods (as discussed in Sects.</s><s xml:id="_pa5p4Z8">2.2.1 and 2.2.2), the lookup table plays the role of approximator for value function approximations <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b19">21]</ref>.</s><s xml:id="_8PHYT36">For example, the Q table of QL records the values for each visited state-action pair in a tabular form during the iterations based on samples <ref type="bibr" target="#b10">[12]</ref>.</s><s xml:id="_YaMwXyt">Therefore, algorithm performance may still be constrained by the sizes of states and actions <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b19">21]</ref>.</s><s xml:id="_QaD5g3m">Fortunately, there are various kinds of function approximations rather than tables to address the curse of dimensionality in state space better.</s></p><p xml:id="_Bdb6btz"><s xml:id="_m6w67WV">In the context of ADP, a basis function φ f (S t ) maps state information from S t to a value of feature f by approximation <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b28">31]</ref>, where f ∈ F is a feature in the feature set F. In this way, the approximators of ADP could be constructed using a set of operators and transformation techniques, including lookup tables, aggregation, linear regression, kernel regression, and polynomial regression.</s><s xml:id="_vzGQymW">For example, linear value function approximation V θ (S t ) (which is a parametric model <ref type="bibr" target="#b14">[16]</ref>) with approximators' parameter vector θ can be written in Eq. 14:</s></p><formula xml:id="formula_19">V θ (S t ) = f ∈F θ f φ f (S t ) (14)</formula><p xml:id="_upzSnU5"><s xml:id="_E3PmVWV">Recently, RL using nonparametric models <ref type="bibr" target="#b14">[16]</ref>, such as neural networks, as approximators for value function approximation has drawn growing attention.</s><s xml:id="_FYGaYj2">As an instance of supervised learning, function approximation generalizes from samples of a desired function (e.g., value function) to formulate an approximate representation of the entire function <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_s3cjjsk">Those typical algorithms in HOM are studied in the following subsections.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4" xml:id="_ZFnXw84">Deep Q-network</head><p xml:id="_hzzwt6j"><s xml:id="_VH4dAHt">Generally speaking, deep Q-network is similar to QL but uses neural networks to approximate the value function, rather than the QL's tabular method.</s><s xml:id="_yYUTsQC">RL methods with function approximation by deep artificial neural networks are considered deep reinforcement learning (DRL) <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_qkWncnD">These deep neural networks (DNN) include multi-layer perception <ref type="bibr">[51]</ref>, convolutional neural networks (CNN) <ref type="bibr" target="#b47">[52]</ref>, and recurrent neural networks (RNN) <ref type="bibr" target="#b48">[53]</ref>.</s><s xml:id="_fmFwkCq">We refer the reader to <ref type="bibr" target="#b49">[54]</ref> for an inspiring discussion on neural networks and deep learning <ref type="bibr" target="#b49">[54]</ref>.</s><s xml:id="_hcTeBqv">Our study suggests that several renowned models, such as long short-term memory (LSTM) <ref type="bibr" target="#b50">[55]</ref>, graph neural network (GNN) <ref type="bibr" target="#b51">[56]</ref>, transformer, and attention mechanisms <ref type="bibr" target="#b52">[57]</ref>, have been successfully employed as approximators of reinforcement learning in HOM practices.</s></p><p xml:id="_ggCwMs3"><s xml:id="_8BPvvqE">Deep Q-network (DQN) is a widely applied DRL method that has been successfully applied in various industries.</s><s xml:id="_8VnTQ9m">It has even achieved human-level control in Atari video games <ref type="bibr" target="#b53">[58,</ref><ref type="bibr" target="#b54">59]</ref>.</s><s xml:id="_9XZjrEe">The primary principle of DQN is to replace the Q table approximator in Q-learning with neural networks.</s><s xml:id="_MbgA49h">In each decision stage, the state variables are fed into the DQN neural networks (referred to as Q-networks), which compute the approximated action-value function.</s><s xml:id="_NZ3Sfw9">The optimal action is then chosen by solving a maximization problem similar to Eq. 9.</s></p><p xml:id="_76vK8Ws"><s xml:id="_7DY764F">A key component of the DQN method is the use of experience replay <ref type="bibr" target="#b54">[59]</ref>.</s><s xml:id="_eDaBcSZ">This technique involves storing the agent's experiences, represented as transitions e t = (s t , a t , r t+1 , s t+1 ), to a dataset D t = {e 1 , . . .</s><s xml:id="_KQTSrEN">, e t } at each stage t.</s><s xml:id="_5BCUeF9">During the learning process, DQN performs Qlearning updates on batches of experience samples e = (s, a, r , s ) ∼ U (D), where U (D) denotes a uniform random sampling from the stored transitions <ref type="bibr" target="#b54">[59]</ref>.</s><s xml:id="_Jb4KuBZ">After an action is chosen, the DQN agent stores the newly generated transitions to the dataset.</s><s xml:id="_2ArATz3">Another important aspect of DQN is the concept of fixed Q-targets <ref type="bibr" target="#b54">[59]</ref>.</s><s xml:id="_87RNx8z">This mechanism controls the frequency at which the parameters θ of the Q-networks are updated.</s><s xml:id="_2Svdqa6">At predefined intervals, the target Q-network (also known as the fixed Q-network, which approximates the target Q-function Q θ , is synchronized with the latest parameters θ of the current Q-network.</s><s xml:id="_m8Bmxs9">The current Q-network is used for choosing the optimal action by approximating the current Q-function Q θ during sampling or making the decision (as shown in Eq. 9).</s></p><p xml:id="_BV734x5"><s xml:id="_jTvKzvt">DQN is a model-free off-policy method <ref type="bibr" target="#b53">[58]</ref>.</s><s xml:id="_UmvNDr8">The loss function utilized in the i-th update is as follows.</s></p><formula xml:id="formula_20">L i (θ i ) = E e∼U (D) (r + γ max a ∈A Q θ i (s , a ) -Q θ i (s, a)) 2<label>(15)</label></formula><p xml:id="_3eUyFZ7"><s xml:id="_uFm7JG8">In Eq. 15, the DQN agent computes the target Q-value, r +γ max a ∈A Q θ i (s , a ), of the batched samples e ∼ U (D) based on the target Q-network.</s><s xml:id="_2XKVBJq">Subsequently, stochastic gradient descent is implemented to minimize this loss function with respect to the parameter θ i .</s><s xml:id="_N7hmnU6">The experience replay and fixed Q-targets are designed to avoid autocorrelation and ensure the learning quality <ref type="bibr" target="#b54">[59]</ref>.</s></p><p xml:id="_6REYA2s"><s xml:id="_hwE6vSU">Several variants of DQN have been developed to address issues such as overestimation and difficulties in convergence.</s><s xml:id="_SeSNceu">One such variant is Double DQN (DDQN) <ref type="bibr" target="#b55">[60]</ref>.</s><s xml:id="_HUcyYBr">DDQN uses two function approximators: one to select the optimal actions and another to compute the target Q-value.</s><s xml:id="_yVKB4wx">The target Q-</s></p><formula xml:id="formula_21">value is computed as r + γ Q θ i s , argmax a ∈A Q θ i (s , a ) .</formula><p xml:id="_trH2Q9b"><s xml:id="_cCAya36">Using two separate function approximators, DDQN reduces the overestimation of action values and improves learning performance <ref type="bibr" target="#b55">[60]</ref>.</s></p><p xml:id="_mqfzHXH"><s xml:id="_t3jzuH8">Dueling DQN takes a different approach to constructing the target Q-value by summing the state-value function and the actions' advantage function <ref type="bibr" target="#b56">[61]</ref>.</s><s xml:id="_WXxtQv2">The advantage function has a size equal to the action space.</s><s xml:id="_gnUpnSX">The idea is to decompose the estimations of state and action, so as to improve learning convergence and performance.</s><s xml:id="_CWxuA6f">Dueling DDQN (D3QN) integrates the techniques of DQN, Double DQN, and Dueling DQN.</s><s xml:id="_zDjQBmR">This combination of methods has been shown to offer an effective decision-making approach in various domains, including transport and healthcare <ref type="bibr" target="#b57">[62]</ref><ref type="bibr" target="#b58">[63]</ref><ref type="bibr" target="#b59">[64]</ref>.</s></p><p xml:id="_8xszEGP"><s xml:id="_afJsFJT">Compared with QL, the DQN family is capable of handling HOM problems with larger state spaces because of using neural networks for generalization in function approxi-mation.</s><s xml:id="_4CeRFub">Hence, more complex routing problems in healthcare logistics <ref type="bibr" target="#b60">[65]</ref> and supply chain <ref type="bibr" target="#b61">[66]</ref> could be optimized.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5" xml:id="_cTWgHN7">Policy gradient</head><p xml:id="_de989DH"><s xml:id="_9nbcRYn">Unlike value-based methods (e.g., ADP, QL, and DQN) that update optimal policies according to approximated value functions, policy gradient <ref type="bibr" target="#b62">[67]</ref> directly optimizes the policy objective function J (ϑ) with respect to its policy's parameter ϑ, and determines actions based on the approximated probability distributions.</s><s xml:id="_zvMA6rn">Such an approach enables policy gradient to implement both discrete and continuous actions.</s><s xml:id="_yz4v5pA">In order to maximize the performance of the policy, the gradient of the value function with respect to the policy parameters, ∇J (ϑ), is utilized, as shown in Eq. 16 <ref type="bibr" target="#b19">[21]</ref>:</s></p><formula xml:id="formula_22">∇J (ϑ) = E π a∈A q π (S t , a) ∇π (a | S t , ϑ) (<label>16</label></formula><formula xml:id="formula_23">)</formula><p xml:id="_mfGw4e6"><s xml:id="_FTx9WsB">As a Monte Carlo method, the direct use of the typical REINFORCE (policy gradient) algorithm <ref type="bibr" target="#b63">[68]</ref> updates policies with the entire episode of transitions and return, while suffering from the large variance and slow learning <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_H9PatMc">Actor-critic (AC) algorithms, a class of model-free policy gradient RL methods that leverage the strengths of both policy-based and value-based approaches <ref type="bibr" target="#b64">[69,</ref><ref type="bibr" target="#b65">70]</ref>, can substantially reduce variance in learning.</s><s xml:id="_2FjCnSz">AC consists of two approximators: the actor (which determines the policy π ϑ (s, a) = π(a | s, ϑ)) and the critic (which estimates the value function Q θ (s, a)).</s><s xml:id="_QfY7DhE">Here, ϑ and θ are the respective parameters in actor and critic neural networks.</s><s xml:id="_haQjddU">AC's approximate policy gradient can be formulated as Eq. 17 <ref type="bibr" target="#b25">[28]</ref>:</s></p><formula xml:id="formula_24">∇ ϑ J (ϑ) ≈ E π ϑ ∇ ϑ log π ϑ (s, a)Q θ (s, a)<label>(17)</label></formula><p xml:id="_YupKyrn"><s xml:id="_7fb8MrC">In Eq. 17, Q θ (s, a) approximated by the critic neural network can also be replaced by the advantage function, which measures the relative advantage of taking action a in state s over the average.</s><s xml:id="_Rr8TdX7">The advantage function efficiently reduces the variance of policy updates.</s><s xml:id="_mVzMfqT">The AC algorithm with the advantage function is known as advantage actor-critic (A2C) <ref type="bibr" target="#b66">[71]</ref>, which can be extended to asynchronous advantage actorcritic (A3C) <ref type="bibr" target="#b66">[71,</ref><ref type="bibr" target="#b67">72]</ref> with parallel computations for multiple agents' interactions.</s><s xml:id="_GgECETj">These algorithms have also been widely adopted in HOM, such as hospital expansions <ref type="bibr" target="#b68">[73,</ref><ref type="bibr" target="#b69">74]</ref> and inventory control <ref type="bibr" target="#b70">[75]</ref>, in which the curse of dimensionality in both state and action spaces (e.g., determining the production and transportation capacities of regenerative medicine <ref type="bibr" target="#b70">[75]</ref>) could be effectively addressed.</s></p><p xml:id="_qvDamJx"><s xml:id="_EyPdHDx">Proximal policy optimization (PPO) <ref type="bibr">[76]</ref> is a widely used on-policy algorithm that builds on AC.</s><s xml:id="_jdfRnGY">PPO aims to address the instability and sensitivity issues associated with vanilla policy gradient methods <ref type="bibr" target="#b73">[77]</ref>.</s><s xml:id="_jZwrfqt">PPO enforces a constraint on the policy update to ensure that the new policy does not deviate too much from the old policy.</s><s xml:id="_wpgqEG8">This is achieved by introducing a clipped surrogate objective function that leverages the advantage function <ref type="bibr" target="#b74">[78]</ref> and Kullback-Leibler (KL) divergence <ref type="bibr" target="#b75">[79]</ref>.</s></p><p xml:id="_tF79H5S"><s xml:id="_BDAzQfW">Deep deterministic policy gradient (DDPG) is a modelfree, off-policy RL algorithm that combines the strengths of DQN <ref type="bibr" target="#b53">[58]</ref> with deterministic policy gradients <ref type="bibr" target="#b76">[80,</ref><ref type="bibr" target="#b77">81]</ref>.</s><s xml:id="_jY49s74">DDPG adopts the AC architecture, where the actor (neural network) learns a deterministic policy π ϑ (s) that maps states to actions, and the critic (neural network) learns the function Q θ (s, a) that maps state-action pairs to values.</s><s xml:id="_ZftMp5m">Here, ϑ and θ are the parameters of the actor and critic neural networks, respectively.</s><s xml:id="_VBkDxzQ">The objective is to maximize the expectation as shown in Eq. 18; the utilization of experience replay, e ∼ U (D), is the same as in Eq. 15 for DQN.</s></p><formula xml:id="formula_25">max ϑ E e∼U (D) Q θ (s, π ϑ (s)) (<label>18</label></formula><formula xml:id="formula_26">)</formula><p xml:id="_bwPbaAJ"><s xml:id="_B3UZf9j">DDPG incorporates two key techniques, experience replay, and fixed Q-targets, similar to those used in DQN <ref type="bibr" target="#b76">[80,</ref><ref type="bibr" target="#b77">81]</ref>.</s><s xml:id="_2amFyrv">These techniques are employed to enhance sample efficiency and stabilize the training process.</s></p><p xml:id="_HySReAW"><s xml:id="_xzXwgyu">The policy gradient algorithms mentioned above have been successfully applied in HOM, with many falling under the AC algorithm family.</s><s xml:id="_DtSxW3P">These algorithms were selected due to their robustness, ability to handle continuous action spaces, and high sample efficiency, which are crucial in healthcare settings <ref type="bibr" target="#b4">[5]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6" xml:id="_g7dBaMq">Exploration and exploitation</head><p xml:id="_bD3mUYX"><s xml:id="_jqjtMXv">In RL, algorithms are required to overcome the explorationexploitation trade-off dilemma <ref type="bibr" target="#b19">[21]</ref> when optimizing their decision policies.</s><s xml:id="_VXdQ7Nm">This trade-off arises from the need to balance between exploring uncertain actions to gain new knowledge about the system, such as the probability distributions of rewards, and exploiting the best actions given current already-known knowledge, in order to maximize the long-term rewards.</s></p><p xml:id="_S9maPFa"><s xml:id="_wcM8SYM">The exploration-exploitation trade-off is exemplified by the multi-armed bandit (MAB) problem <ref type="bibr" target="#b78">[82,</ref><ref type="bibr" target="#b79">83]</ref>, in which the agent is likened to a gambler who must choose "one arm of the bandit" from multiple options, each with unknown reward probabilities.</s><s xml:id="_wAqQXcr">The MAB problem is typically considered in a special case of RL, which has a single-state environment and immediate rewards.</s><s xml:id="_usGyXx4">This setting makes the required solution procedures more computationally efficient.</s><s xml:id="_CMwYSc3">Its goal is to maximize the cumulative rewards obtained <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_qbF5DmW">According to <ref type="bibr" target="#b80">[84]</ref>, there exist several bandit strategies that can be used to determine optimal actions.</s><s xml:id="_BPgeCE4">While the epsilon-greedy strategy <ref type="bibr" target="#b19">[21]</ref> is widely used, other approaches such as upper confi-dence bounds <ref type="bibr" target="#b81">[85]</ref>, Thompson sampling <ref type="bibr" target="#b82">[86]</ref>, and Gittins index <ref type="bibr" target="#b83">[87]</ref> have also been applied in the literature of HOM.</s><s xml:id="_qJucTeS">With an upper confidence bound approach, the action with the highest reward is chosen, while Thompson sampling (rooted in Bayesian methods) selects actions based on their posterior probabilities of being the best <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_9XvGvpv">These algorithms are widely applied in resource allocations in HOM, such as vaccine allocation <ref type="bibr" target="#b84">[88]</ref><ref type="bibr" target="#b85">[89]</ref><ref type="bibr" target="#b86">[90]</ref> and outpatient management <ref type="bibr" target="#b87">[91,</ref><ref type="bibr" target="#b88">92]</ref>, given their strong interpretability, sound theoretical support, and adaptivity in dynamic environments.</s></p><p xml:id="_TWTGksF"><s xml:id="_ZGPRuHw">Further, Bayesian RL <ref type="bibr" target="#b89">[93]</ref> is designed to address the exploration-exploitation trade-off.</s><s xml:id="_94H3JxK">By leveraging the prior probability distribution that represents uncertainty over value function approximations, Bayesian RL incorporates Bayesian inference to update the prior and obtain a posterior distribution based on observed transitions <ref type="bibr" target="#b90">[94]</ref>.</s><s xml:id="_6csZzXP">This approach allows the learning agent to explicitly incorporate uncertainty by treating the states of the MDP as hyper-states <ref type="bibr" target="#b91">[95]</ref> when making decisions.</s><s xml:id="_RqBPaqV">This integration of uncertainty with the states enables more effective exploration strategies.</s><s xml:id="_FJAFptx">The exploration-exploitation trade-off is naturally considered in Bayesian RL as the transitions occur among different hyper-states that involve uncertainty <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_geuSqpu">In this framework, Bayesian inference can serve as an approximator, and the knowledge about the prior distribution becomes more important <ref type="bibr" target="#b92">[96]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.7" xml:id="_jSKXtKn">Learning complex systems</head><p xml:id="_aMxPRq8"><s xml:id="_HJEGv8R">Multi-agent reinforcement learning (MARL) <ref type="bibr" target="#b93">[97]</ref> extends RL to handle HOM problems in more complex systems by involving multiple decision-making agents.</s><s xml:id="_MaCfx9z">In MARL, each agent learns its own local policy, and these individual policies are utilized to form a joint policy that maximizes the overall expected reward <ref type="bibr" target="#b94">[98]</ref>.</s><s xml:id="_vgGJrvr">The interactions among multiple agents can vary from cooperative settings to dynamic competitive games.</s><s xml:id="_dAc3ufK">For the details, we refer the reader to surveys on multi-agent systems <ref type="bibr" target="#b95">[99]</ref> and the Markov games framework [100] of MARL.</s><s xml:id="_vtZGCBZ">In this way, the curse of dimensionality in complex HOM problems, such as coordinating multiple emergency departments <ref type="bibr">[101]</ref> or emergency vehicles [102], could be addressed.</s></p><p xml:id="_qZkNxwr"><s xml:id="_v2dfZz4">Hierarchical RL [103] is a solution method that aims to solve complex problems efficiently by breaking them down into simpler structured subproblems.</s><s xml:id="_x4r8GCv">This approach involves organizing the problem into multiple levels of abstraction, each with its own set of policies.</s><s xml:id="_tGgr5Nz">At the high level, there are policies (also known as options) <ref type="bibr">[104]</ref>, which make decisions less frequently and focus on broader objectives, similar to the functions of managers.</s><s xml:id="_WWDdaA2">On the other hand, lowlevel policies are responsible for implementing immediate and finer-grained actions in the environment, similar to the functions of workers.</s><s xml:id="_v8Ta7da">This hierarchical approach is particu-larly effective in handling tasks with large state and action spaces, such as human-machine collaboration in ventilator production <ref type="bibr">[105]</ref>, as well as environments that provide sparse rewards <ref type="bibr">[106]</ref>.</s></p><p xml:id="_WPcPdRU"><s xml:id="_8TccbQh">Imitation learning [107], a methodology tailored for complex systems, involves recovering the reward function from expert demonstrations through the theory of inverse RL <ref type="bibr">[108]</ref>.</s><s xml:id="_NBMDJg9">Rather than relying solely on trial and error, an imitation learning agent can swiftly adopt decision-making policies from established human experts' policies <ref type="bibr">[107]</ref>.</s><s xml:id="_CcADh65">HOM utilizes behavioural cloning [109], an imitation learning technique that trains the RL agent to replicate the policies of experts based on the observed states, a process that parallels supervised learning <ref type="bibr">[110]</ref>.</s></p><p xml:id="_qjzTfcq"><s xml:id="_XUWSCa6">Furthermore, RL has been employed as an optimizer within complex algorithms, such as those used for predicting healthcare-related metrics during epidemics, including numbers of infections and inpatient admissions <ref type="bibr" target="#b69">[74,</ref><ref type="bibr">[111]</ref><ref type="bibr">[112]</ref><ref type="bibr">[113]</ref>.</s><s xml:id="_e82GeHM">In these scenarios, RL not only enhances prediction accuracy by optimizing the hyperparameters of the student-teacher curriculum learning <ref type="bibr">[111]</ref>, but it also identifies the key features that influence the system <ref type="bibr">[114]</ref>.</s><s xml:id="_rWrWh4j">While heuristic methods [115] can accomplish similar optimization tasks, a more promising approach lies in combining RL with heuristics to leverage their complementary strengths in combinatorial optimization <ref type="bibr">[116,</ref><ref type="bibr">117]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_9hCMbD8">Summary of key RL settings</head><p xml:id="_hJXWFRg"><s xml:id="_TKePZG4">Based on our previous discussion, we summarize the key RL settings in the context of HOM applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sRKJNkz">Model-based versus model-free</head><p xml:id="_AFX6EH9"><s xml:id="_2f7EMbA">Model-based RL makes use of the system model.</s><s xml:id="_4fZYu5C">Thus, the transitions and reward function could be explicitly incorporated into the solution framework <ref type="bibr">[41]</ref>.</s><s xml:id="_ScJA2Up">For example, in inpatient management <ref type="bibr" target="#b27">[30]</ref>, model-based RL could leverage the queuing network to infer value functions from waiting and overflow costs, by which the optimized policies could be built on analytical properties.</s><s xml:id="_SHfuxSa">However, model-free RL does not utilize the system model; rather, it learns directly from empirical interactions within the environments' simulations using an iterative trial-anderror approach.</s><s xml:id="_ZMpEXqr">It heavily relies on training samples and may suffer from poor sample complexity and convergence issues.</s><s xml:id="_SQGBj5R">Generally, leveraging domain knowledge and problem structure can accelerate convergence and reduce computational time for model-free methods <ref type="bibr">[105,</ref><ref type="bibr">118]</ref>.</s></p><p xml:id="_CxJu6Db"><s xml:id="_KYukGxQ">Tabular versus non-tabular Referring to Sect.</s><s xml:id="_T3AXYsV">2.2.3, value function approximation can be classified into tabular and non-tabular approaches.</s><s xml:id="_yWP4JDa">In problems consisting of only small numbers of states and actions, approximation could be completed with tabular methods.</s><s xml:id="_tgc6fTT">In the forms of arrays or tables, each row/column is associated with a state or state-action pair.</s><s xml:id="_jCmpdd8">Standard ADP and TD learning utilize tabular approximations that are derivative-free <ref type="bibr" target="#b28">[31]</ref>.</s><s xml:id="_M5FrcBd">However, in many HOM practices, the huge number of states may impose computational challenges in utilizing tabular approaches.</s><s xml:id="_AARuWR4">In these cases, RL methods using more compact and non-tabular forms of function representation are needed <ref type="bibr" target="#b19">[21]</ref>.</s></p><p xml:id="_vfbSQzA"><s xml:id="_UxqvQW7">Value-based versus policy-based As we have systematically introduced in Sect.</s><s xml:id="_TMc3Vyp">2.2.2, given state s (or state-action pair (s, a)), value-based approximation estimates v π (s) (or q π (s, a)) through value function approximation.</s><s xml:id="_FUdSR27">The optimal policy (Eq.</s><s xml:id="_NhNnK7h">4) is approximated by iteratively updating the approximated Q-value ( Q(s, a)) in Eq.9.</s><s xml:id="_j2MkSrZ">Value-based methods typically require explicit computations for each action.</s><s xml:id="_fTcebBW">Therefore, some studies on pandemic control <ref type="bibr">[119]</ref><ref type="bibr">[120]</ref><ref type="bibr">[121]</ref> considered discretized thresholds to represent lockdown policies based on state variables.</s><s xml:id="_uJBCKta">Referring to Sect.</s><s xml:id="_ZbZ9qyW">2.2.5, policy-based approximation parameterizes and determines the policy without using value functions.</s><s xml:id="_J42PMGe">It requires differentiability of the policy π(a | s, ϑ) to determine the parameter ϑ, so as to avoid solving the potentially intractable maximization problem in Eq. 4. In this way, policy-based methods can handle continuous action spaces.</s><s xml:id="_p7BMSkM">Stochastic policies are favored in policy-based methods due to their differentiability.</s><s xml:id="_DnBytNG">In many situations, policy-based methods can be combined with value-based methods, such as the AC algorithms, to reduce variance in updates <ref type="bibr" target="#b19">[21]</ref>.</s></p><p xml:id="_nrRZYbx"><s xml:id="_fCuxGxB">On-policy versus off-policy In Sect.</s><s xml:id="_ztW9YKD">2.2.2, we discuss that onpolicy methods focus on assessing or enhancing the policy that dictates decision-making, while off-policy methods aim to evaluate or refine a policy distinct from the one utilized to generate the data <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_YAMZRPs">Thus, on-policy methods are relatively simple and stable in their learning.</s><s xml:id="_9Jfsksp">Off-policy methods are flexible in learning a broader range of data, such as human experiences in HOM, but suffer from greater variance and slow convergence <ref type="bibr" target="#b19">[21]</ref>.</s><s xml:id="_jGkvZPU">Given that simulations can mitigate the lack of samples in HOM, we have observed extensive applications of both on-policy and off-policy methods in the following Sect.</s><s xml:id="_wqw7zVx">4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SmDfBrH">Online learning versus offline learning</head><p xml:id="_BEYuAma"><s xml:id="_vMkUjhW">Online learning continuously updates approximations' parameters as new data (e.g., states, actions, and rewards) arrive without re-training from scratch.</s><s xml:id="_rm35zXq">It is particularly suitable for dynamic environments where adaptability is essential, such as HOM applications <ref type="bibr" target="#b84">[88,</ref><ref type="bibr" target="#b88">92,</ref><ref type="bibr">122]</ref>.</s><s xml:id="_Ka4Z6mJ">Bandit problem <ref type="bibr" target="#b14">[16]</ref> (in Sect.</s><s xml:id="_8jKHpsz">2.2.6), focusing on single-step decisions with partial feedback, is a specific subclass of online learning.</s><s xml:id="_YHzbnF9">On the other hand, offline learning updates approximations' parameters according to the fixed entire dataset that is available at the time of training.</s><s xml:id="_FDJ59Yq">This process may iterate several rounds until approximations' performance stably achieves defined criteria.</s><s xml:id="_Ua3ZJhC">Some offline methods with experience replay <ref type="bibr" target="#b54">[59]</ref> are discussed in Sects.</s><s xml:id="_MGRsf8Z">2.2.4 and 2.2.5.</s><s xml:id="_KBEHN2q">One limitation of offline learning is that storing the entire training set may cause memory issues (from the computational resource point of view), especially when setting a large batch size or the state space has to be huge to describe HOM problems (e.g., in pandemic control application <ref type="bibr">[121]</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_YJSUpkF">Review scope and search</head><p xml:id="_uYYTykq"><s xml:id="_e2JTpwC">Our work adopts a scoping review approach [123] to review and analyze relevant research studies.</s><s xml:id="_wAbKwbK">We focus on HOM applications rather than clinical diagnostics; thus, publications in precision medicine development, medical imaging, and medical robotics are excluded.</s><s xml:id="_t4aGADX">Following the healthcare ecosystem map <ref type="bibr">[124]</ref>, we are able to identify keywords that are closely associated with our HOM scope.</s><s xml:id="_rpJEuWx">These keywords include "healthcare", "operations management", "hospital", "patient", "medical", "public health", "epidemic", "pandemic", "emergency", and "humanitarian".</s><s xml:id="_tHShnB9">We implemented a search strategy that contained a certain word "reinforcement learning" followed by these keywords on Scopus, and limited the subject area to "Decision Science".</s><s xml:id="_M7WEs8V">The Scopus query syntax is TITLE-ABS-KEY ( "reinforcement learning" ) AND TITLE-ABS-KEY ( "healthcare" OR "operations management" OR "hospital" OR "patient" OR "medical" OR "public health" OR "epidemic" OR "pandemic" OR "emergency" OR "humanitarian" ) AND ( LIMIT-TO ( SUBJAREA, "DECI" ) ).</s><s xml:id="_DnDYKRs">This initial search of articles (conducted on January 4, 2023, and updated on <ref type="bibr">January 22, 2024)</ref> resulted in 321 documents.</s><s xml:id="_eteBGxE">Based on our knowledge, we included additional relevant articles (e.g., those in arXiv and conference proceedings) since RL is also a widely researched area within the computer science community.</s><s xml:id="_mGH7yKv">After an initial checking of the abstracts, we considered a total of 144 articles for further analysis.</s><s xml:id="_a9bPdJs">In the subsequent round of detailed content analysis, we identified 117 relevant studies on RL in HOM for our review.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_ANUwPkX">Applications</head><p xml:id="_aPhxx4A"><s xml:id="_GHc4vnR">Following <ref type="bibr">[124]</ref>, we categorize the studies into macrolevel, mesolevel, and microlevel research thrusts.</s><s xml:id="_R46Rc8Q">We also adopt the terminologies, classification, and empirical results from previous studies such as <ref type="bibr">[125]</ref> and <ref type="bibr">[126]</ref>.</s><s xml:id="_jWS8UtN">We consider the healthcare ecosystem map presented in <ref type="bibr">[124]</ref> to structure our three thrusts of healthcare operations applications.</s><s xml:id="_GxRtvXT">The macrolevel research thrusts entail the overarching strategy and policy implemented by governments or authorities to harness the healthcare marketplace.</s><s xml:id="_njMn3Sc">The mesolevel research thrusts serve as a connector between the macrolevel and microlevel research thrusts.</s><s xml:id="_FFjr66W">For example, it encompasses the distribution and allocation of resources across multiple healthcare facilities.</s><s xml:id="_yH2svPX">Finally, the microlevel research thrusts pertain to the detailed operations involved in providing patient care services within a healthcare facility.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_Fv6yezF">Macrolevel research thrusts</head><p xml:id="_WPPNTES"><s xml:id="_XfjbJp7">Following the discussion in the literature <ref type="bibr">[124,</ref><ref type="bibr">127,</ref><ref type="bibr">128]</ref>, the applications in the macrolevel research thrusts revolve around the supply of and demand for healthcare services through various healthcare entities (e.g., hospitals, pharmacies, and governments) and on marketplaces.</s><s xml:id="_WHTVsRg">Examples include market mechanisms, organizational structures, healthcare network flows, and accessibility to health services.</s><s xml:id="_m4W7wvP">We analyze the relevant RL applications and identify that the majority of such applications focused on healthcare policies and strategies.</s><s xml:id="_UxreTJg">A portion of these RL applications was studied by <ref type="bibr">Weltz et al. [129]</ref> with a specific focus on respondent-driven sampling in public health, leaving a comprehensive review yet to be conducted.</s><s xml:id="_XCEJhRH">The global outbreak of COVID-19 has led to a surge in recent research focused on utilizing RL to determine optimal pandemic intervention policies.</s><s xml:id="_zRFqD56">Interestingly, we find that the RL studies in the macrolevel research thrust focus on infection modeling and control.</s><s xml:id="_m2HxDDU">We classify the studies into general measures and strategies, COVID-19 control policies, and mobility restriction policies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1" xml:id="_tbWSbuh">General measures and strategies</head><p xml:id="_FZDKMx4"><s xml:id="_abnsXcm">Prior to the outbreak of COVID-19, there were already research studies on sequential decision-making in public health, ranging from model-based simulation to ADP and DQN.</s><s xml:id="_N9R6Zp8">Back in 2008, <ref type="bibr">Das et al. [130]</ref> published a research study that developed a simulation model for analyzing largescale pandemic outbreaks to minimize the aggregated costs resulting from healthcare expenses and lost wages.</s><s xml:id="_BeHyvpB">Their study considered community, demographic, physiological, behavioral, and epidemiological features, such as indicators of infection, the stockpile of vaccines and drugs, as well as the hospital capacity.</s><s xml:id="_WrKUMAM">The decisions for the considered mitigation strategies encompass a range of actions and plans, including prophylaxis, quarantine plans, and hospital admissions.</s><s xml:id="_4AJ9m2R">RL was also proposed as a solution to the problem.</s><s xml:id="_PfTG4Hr">In a more recent study, <ref type="bibr">Shi et al. [132]</ref> conducted simulations on voluntary vaccination in social network settings and found that heterogeneous social connections demonstrate greater sensitivity to information regarding vaccination.</s><s xml:id="_hmSJ958">These simulations were specifically designed for an RL environment.</s></p><p xml:id="_DY2FC62"><s xml:id="_Y3Q6Q8R">Regarding school closure and vaccinated cohorts for controlling the H1N1 epidemic [151], <ref type="bibr">Yaesoubi et al. [131]</ref> adopted a partially observable Markov decision process (POMDP) <ref type="bibr">[152]</ref> in modeling hospitalizations and vaccinations.</s><s xml:id="_97u2gd6">The study concluded that an ADP approach guided by the latest information outperformed static policies.</s><s xml:id="_mj7huc8">Their results highlighted the significance of incorporating realtime data into decision-making processes.</s><s xml:id="_2rNBUgG"><ref type="bibr">Probert et al. [133]</ref> applied DQN to contain outbreaks of foot-and-mouth disease in farms using a Susceptible-Exposed-Infectious-Recovered (SEIR) model <ref type="bibr">[153]</ref>.</s><s xml:id="_pZgRWMe">Their approach modeled the RL state (e.g., infected and susceptible farms) on a discretized landscape, with a CNN serving as the approximator.</s><s xml:id="_7mbZy9x">The state-dependent actions involve selecting which farms to cull.</s></p><p xml:id="_7GEVHHH"><s xml:id="_ueKhP8V">In a recent study, <ref type="bibr">Liu et al. [134]</ref> developed an approach for adaptive control of the Ebola virus disease spreading across multiple locations.</s><s xml:id="_CRH924T">They utilized a combination of deep spatial fitted Q iteration <ref type="bibr">[154]</ref> with graph embeddings (a GNN approach), a semi-parametric variant [155] of Thompson sampling, and a tractable quadratic program <ref type="bibr">[156]</ref> to handle the search in a large action space.</s><s xml:id="_b9GdSwm">Comparisons with ad-hoc strategies and a susceptible-infected-susceptible (SIS) [157] model-based policy search showed that their proposed method achieved better control (resulting in more disease-free individuals) and higher robustness to model misspecification.</s><s xml:id="_dgnxcam">They also provided insightful discussions on the topics of causal inference <ref type="bibr">[158]</ref> and interpretability <ref type="bibr">[159]</ref> of the RL solutions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2" xml:id="_2TbMkg8">COVID-19 control policies</head><p xml:id="_F3zT5Xb"><s xml:id="_NdFqRPS">In late 2019, the COVID-19 pandemic broke out, quickly spreading worldwide and impacting billions of individuals.</s><s xml:id="_QENwabr">To address the unprecedented challenges posed by the COVID-19 pandemic, researchers have explored the application of RL in developing intervention policies and devising healthcare strategies.</s><s xml:id="_W7bthyQ">These strategies include testing, sanitization, and lockdown measures <ref type="bibr">[135]</ref>.</s></p><p xml:id="_ywpeCPH"><s xml:id="_dYCEq4G">By using SEIR models and DQN, <ref type="bibr">Arango et al. [136]</ref> and Miralles et al. <ref type="bibr">[137]</ref> determined optimal lockdown policies to optimize the number of available beds in intensive care units (ICUs) and the economic costs.</s><s xml:id="_xesSDte">Only infections were considered the state variable, while other variables depended on it.</s><s xml:id="_4B7Ap76">This approach aimed to approximate the disease transmission rate based on the number of infections.</s><s xml:id="_fEEgDNu">Their studies also suggested short lockdown cycles as solutions.</s><s xml:id="_8NRdWuH">In a later study, <ref type="bibr">Padmanabhan et al. [140]</ref> developed QL approaches to implement closed-loop control by sequentially determining intervention actions in Qatar.</s></p><p xml:id="_Wz9m47h"><s xml:id="_b2ArhwX">From a perspective of Bayesian inference, <ref type="bibr">Rathore et al. [141]</ref> proposed both Bayesian RL and control theory to reduce the impacts of respiratory infectious pandemics (such as .</s><s xml:id="_fSTwVme">They utilized a susceptible-infectiousrecovered (SIR) model and POMDP to study the infectious disease outbreak.</s><s xml:id="_QjS5nJ3">In the pandemic process, three states -pretrigger, increasing, and decreasing -were considered.</s><s xml:id="_k5jhq6c">The authors introduced a control knob represented by the reproduction number to indicate the on-off signals of actions.</s><s xml:id="_77zKxsd">This approach enabled the RL agent to leverage pre-trigger policies in an offline manner initially.</s><s xml:id="_ZfQkqKN">The policies were then transferred to an online exploration approach based on the information state and its associated likelihood.</s><s xml:id="_fg88r3m"><ref type="bibr">Wan et al. [120]</ref> developed an adaptive MARL approach to identify Pareto-optimal policies.</s><s xml:id="_yauAXe9">They established a Bayesian epidemiological model with online learning.</s><s xml:id="_2ztarYB">They employed a delayed MDP framework to generate a proxy state to capture the time-lag relationships between the number of infected and confirmed cases.</s><s xml:id="_KEpvxRz">In addition to DQN, they utilized Monte Carlo rollouts that considered real-life constraints, such as the severity of the spread, enhancing the interpretability of the results.</s><s xml:id="_CJ5FSWy">Their experiments suggested that these robust methods could effectively control epidemics in various cities with reduced costs.</s></p><p xml:id="_wGrZHG8"><s xml:id="_8Zbmdmu">Another line of research studies the impacts of pandemic control policies at an individual level.</s><s xml:id="_WxHcuXX">Several studies have incorporated weighted rewards to account for economic impacts and infections at an individual level.</s><s xml:id="_NynDJNt"><ref type="bibr">Ohi et al. [119]</ref> utilized LSTM and DDQN to determine optimal epidemic control policies for three levels of restriction policies.</s><s xml:id="_gMQYq9G">Based on population density and reproduction rates, they proposed placing a long lockdown during the first surge, followed by cyclic and short lockdowns to prevent resurgence.</s><s xml:id="_G7MPtdM"><ref type="bibr">Khadilkar et al. [138]</ref> factored in individual costs and developed a propagation model using network data.</s><s xml:id="_AHZDsPP">Using DQN, their proposed policy resulted in a higher peak of infections but a shorter epidemic lockdown duration than a static threshold policy.</s><s xml:id="_p53a6py"><ref type="bibr">Kompella et al. [139,</ref><ref type="bibr">160]</ref> extended the SEIR model by incorporating more detailed components related to locations, testing and tracing, and government regulations in their proposed AC approach.</s><s xml:id="_pDXxGyy">Their proposed method considered partially observed states capturing aggregated testing results and the number of hospitalizations.</s><s xml:id="_3ctZsYH">Their results suggested stratified actions consisting of combinations of government regulations.</s><s xml:id="_qd7RYwQ">Their experiments were scaled up to a population of 10,000 individuals while ensuring that actions were stable.</s><s xml:id="_Ym3pW6G">However, the computational expense of the proposed approach may pose a challenge when scaling up to a national epidemic control scenario at the macrolevel.</s></p><p xml:id="_RcqMSJE"><s xml:id="_hmG8Pcc">To date, the COVID-19 pandemic has presented an impetus for scholars and researchers to delve into the utilization of RL in the formulation and implementation of macrolevel healthcare policies.</s><s xml:id="_53Ap74S">The existing studies have demonstrated that policies derived from RL approaches provide more costeffective solutions <ref type="bibr">[143,</ref><ref type="bibr">144]</ref> than relying on heuristics or expert opinions when balancing saving lives and reducing economic impacts.</s><s xml:id="_YdS3XUg"><ref type="bibr">Guo et al. [121]</ref> built upon previous works such as <ref type="bibr">DQN [137]</ref> and agent-based FluTE simulation <ref type="bibr">[161]</ref>.</s><s xml:id="_Z25Jhsz">They expanded the established state variables, including vaccinations, the net monetary impacts of pandemic severity, and lockdown policy (strictness of the policy).</s><s xml:id="_qA5qcyv">A ProbSparse selfattention mechanism <ref type="bibr">[162]</ref> was integrated into the perceptron model to extract crucial information from complex epidemiological observations.</s><s xml:id="_6RzsUHY">This fusion facilitates the effective processing of high-dimensional data in the context of epidemiology.</s><s xml:id="_U6azWk6"><ref type="bibr">Bushaj et al. [142]</ref> emphasized the importance of increasing the number of healthy individuals in a population, early random vaccination of potential super spreaders, and quarantining high-risk individuals.</s><s xml:id="_SEq6fsP">They extended the Covasim simulation model <ref type="bibr">[163]</ref> by implementing random and age-based vaccination strategies.</s><s xml:id="_kVqXXBT">They integrated compartmental information, such as the population with the two-shot vaccine, into the state space of their DQN.</s><s xml:id="_22PEqnx">Additionally, the model included three additional vaccination-related interventions that can be activated based on vaccine availability.</s><s xml:id="_JppzJP2"><ref type="bibr">Yao et al. [144]</ref> utilized DDQN to identify adaptive nonpharmaceutical interventions for controlling COVID-19 outbreaks and other respiratory infectious diseases.</s><s xml:id="_36hZSTu">Using the required hospital beds to construct the state, they determined the threshold of available beds that would trigger stricter interventions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3" xml:id="_26VVQkS">Mobility restriction policies</head><p xml:id="_8YFdpUE"><s xml:id="_Seh7VVJ">There has been a growing interest in studying mobility and travel policies during the pandemic.</s><s xml:id="_gTmFW2U"><ref type="bibr">Libin et al. [146]</ref> investigated optimal policies for minimizing the number of susceptible individuals in different regions by integrating age groups within each region and mobility patterns between regions.</s><s xml:id="_HAwp8Dd">They developed a country-wide MARL framework.</s><s xml:id="_9UwBfHF">A PPO algorithm was employed, with the available budget as a crucial control factor for both open and closed actions and for constructing the state variables.</s><s xml:id="_2Q3xceq">Their results suggested that the joint MARL approach consistently yielded lower costs.</s><s xml:id="_yRVXajX"><ref type="bibr">Kwak et al. [147]</ref> treated different countries as homogeneous entities and formulated the problem as a susceptible-infectious-recovered-dead (SIRD) model.</s><s xml:id="_MRmB5bK">By adopting diminishing rates of new infections, their algorithm recommended an earlier implementation of intensity strategies compared to the degrees of travel restrictions implemented by the government in each country.</s></p><p xml:id="_UjXBSgH"><s xml:id="_Tqns9pD">In the context of urban mobility, <ref type="bibr">Zong et al. [149]</ref> developed a sophisticated algorithm called the multi-agent recurrent attention actor-critic algorithm.</s><s xml:id="_vqff5au">Their case study focused on optimizing lockdown policies for different states in the US.</s><s xml:id="_3bDt6wk">Their algorithm interacted with a susceptible-exposedasymptomatic-infected-recovered-death (SEAIRD) simulation model, incorporating heterogeneous locations such as schools, offices, and stores.</s><s xml:id="_n788XEF">The algorithm utilized a gated recurrent unit, setting it apart from and outperforming existing RL benchmarks such as <ref type="bibr">[164]</ref>.</s><s xml:id="_Xf2yHAb"><ref type="bibr">Song et al. [145]</ref> aimed to identify mobility-control policies in Beijing that could simultaneously minimize the costs of infections and retain mobility.</s><s xml:id="_B7JQYze">They achieved the objectives by developing a susceptible-infected-hospitalized-recovered (SIHR) model using real-world origin-destination (OD) data.</s><s xml:id="_pg5qRCz">The state, consisting of epidemic information and mobility demands, was fed into a GNN approximator within a DDPG framework.</s><s xml:id="_MGcTPfT">Their approach outperformed real-world expert policies in both early and late intervention scenarios by effectively addressing the life-or-economy dilemma, suppressing epidemics, and maintaining 76% of the mobility levels.</s><s xml:id="_TSM2Ckg"><ref type="bibr">Roy et al. [148]</ref> modulated zone mobility based on the healthcare system's budget, estimated using local GDP.</s><s xml:id="_gPJpGxk">They employed queueing theory to analyze the hospitals in different boroughs of New York City, utilizing inter-zone mobility matrices.</s><s xml:id="_2HFutFD">They proposed a QL algorithm to maximize mobility while considering the impact of high hospital occupancy.</s><s xml:id="_V38Bnge">Through hierarchical RL [103], <ref type="bibr">Du et al. [150]</ref> developed a multi-mode intervention strategy that integrates mobility constraints with medical resources and supplies as hierarchical actions to control the economic damage and contain the pandemic outbreaks.</s><s xml:id="_xWcVGDy">They also expanded a multilateralimpact-driven SEIR model to capture the impacts of different interventions.</s><s xml:id="_FRnzDtG">The optimal policies were assessed on two Chinese cities.</s></p><p xml:id="_cbjT4Jw"><s xml:id="_GBxD7t5">Table <ref type="table" target="#tab_0">1</ref> summarizes the research studies under the macrolevel research thrusts.</s><s xml:id="_jXT2abe">The applications of RL under the macrolevel research trust typically determine optimal healthcare policies, control critical epidemic conditions, and minimize overall costs within the constraints of available medical resources.</s><s xml:id="_CNspW2s">The applications aim to strike a balance between hospital occupancy, infections, and economic impacts.</s><s xml:id="_JgGQush">RL functions by utilizing states from epidemic models and determining actions that encompass a range of epidemic interventions, such as social and travel restrictions or different levels of lockdown intensity.</s><s xml:id="_BJuhdJV">In these applications, the action space is usually discrete, leading to a more popular choice of DQN as the method.</s><s xml:id="_uvNW9EQ">For infectious disease models, SEIR models <ref type="bibr">[119, 120, 136-140, 146, 148, 160]</ref> are the most popular class that simulates the dynamic behaviors of epidemics.</s></p><p xml:id="_EcUVKja"><s xml:id="_JB8HBNQ">One of the primary challenges in applying RL to healthcare policy is determining an effective reward function that accurately reflects real-world conditions.</s><s xml:id="_vHsVYmy">The impacts and rewards of interventions may be influenced by other factors, and validation can be expensive, with misspecification leading to incoherent learning.</s><s xml:id="_mfSJVDQ">Real-time model updating with real-world data calibration or robustness optimization with uncertainties are potential solutions to this challenge.</s><s xml:id="_Gj7Rtkv">Furthermore, as more complex problems arise, more sophisticated RL algorithms can be deployed, such as those addressing large-scale multiple-wave epidemics, partially observable problems <ref type="bibr">[131,</ref><ref type="bibr">139,</ref><ref type="bibr">141,</ref><ref type="bibr">160]</ref>, fine-grained policies, detecting super-spreaders, and immunity.</s><s xml:id="_cHPHPFc">However, practical implementations of DRL solutions in macrolevel applications remain rare in the real world.</s><s xml:id="_p7w3p7B">The rarity of real implementations is largely due to the high demands for transparency, trustworthiness, and regulatory compliance in these applications, prioritizing the interpretability of decision-making.</s><s xml:id="_pbKvbFA">Current studies only conduct sensitivity and statistical analyses of their policies.</s><s xml:id="_Vy29bhP">Designing interpretable RLs <ref type="bibr">[165,</ref><ref type="bibr">166]</ref> in low-dimensional representations that can address the dilemma of managing complex systems with strong interpretability remains a future direction.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_4aNNH85">Mesolevel research thrusts</head><p xml:id="_XjdMwQy"><s xml:id="_hW37tKv">The mesolevel research thrusts cover operations such as distribution, resource allocation, organization design, logistics, and supply chain management within the healthcare services domain.</s><s xml:id="_SHgSyZK">This level of analysis serves as a bridge between the macrolevel and microlevel research thrusts.</s><s xml:id="_MjnVGys">It operates within the framework of overall healthcare strategy but extends beyond the scope of a single healthcare institution <ref type="bibr">[124]</ref>.</s><s xml:id="_muqWKjw">The studies in this area can be classified into various domains, including humanitarian logistics, resource allocation during epidemics, and supply chain management in the healthcare industry.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1" xml:id="_HPdZCBv">Humanitarian logistics</head><p xml:id="_9QbpTM3"><s xml:id="_wCGUxec">Timely and effective decision-making is always crucial in providing relief after a disaster or mass casualty incident (MCI).</s><s xml:id="_czs6SNm">Those situations are often challenged by partial observability and a high degree of uncertainty.</s><s xml:id="_JUE6DnM">RL-based approaches have been developed to aid humanitarian logistics, encompassing tasks such as distribution, rescue path searching, and transportation.</s><s xml:id="_85cyKeF">These techniques facilitate humanitarian operations, enable rapid response, and enhance recovery efforts.</s><s xml:id="_XNsEPf6">Yu et al. <ref type="bibr" target="#b32">[36]</ref> utilized QL for humanitarian distribution planning.</s><s xml:id="_YDafCFp">Their objective was to minimize the delivery cost, the deprivation cost, and the terminal penalty cost.</s><s xml:id="_Zu8pWT6">The local response center (modeled as an agent in their RL framework) decided how to allocate supplies to areas affected by disasters.</s><s xml:id="_dJbK8Pk"><ref type="bibr">Fan et al. [169]</ref> developed a DQN approach that takes into account the scarcity of emergency supplies.</s><s xml:id="_K5vnw4J">Through numerical experiments, they demonstrated the effectiveness of RL in terms of computational time and objective values, particularly in tackling problems with high-dimensional spaces.</s><s xml:id="_Uj8Fb2x">In another study, <ref type="bibr">Van Steenbergen et al. [170]</ref> introduced Unmanned Aerial Vehicles (UAVs) to humanitarian complement trucks and optimized multi-vehicle, multi-trip, split-delivery routes under travel time uncertainty.</s><s xml:id="_4EDMDeX">By evaluating both value function approximation and policy function approximation, they validated that dynamic methods and UAV deployment significantly enhance operational performance and robustness, particularly in reaching remote locations.</s></p><p xml:id="_65nkc5r"><s xml:id="_VN8fnuH">In a problem of rescue path selection, Su et al. <ref type="bibr" target="#b43">[47]</ref> utilized a rectangular grid to represent the affected area.</s><s xml:id="_2mMH8WN">They implemented an RL framework where the rescue team was represented as an RL agent.</s><s xml:id="_XzhRrsn">The team aimed to find the most efficient path connecting two points, and a mechanism for escaping cyclic paths was incorporated into the design.</s><s xml:id="_ppj64NU">Nadi et al. <ref type="bibr" target="#b44">[48]</ref> improved a MARL framework by incorporating relief assessment and emergency response teams in an online setting.</s><s xml:id="_Xvn6aqh">The relief assessment teams utilized a prediction model to estimate the demands in affected areas.</s><s xml:id="_6UjZ8Bu">The response team then solved a vehicle routing problem (VRP), considering the requests' priorities and both teams' capacity and time window constraints.</s><s xml:id="_VcaB74r">Shen et al. <ref type="bibr" target="#b46">[50]</ref> modeled an aviation emergency rescue problem with a stochastic game process and employed MARL to determine task acceptance/rejection decisions at different locations.</s><s xml:id="_CbqFtXS">Yang et al. <ref type="bibr" target="#b45">[49]</ref> proposed a MARL approach, coined ResQ, for disaster response.</s><s xml:id="_hqpp2EG">This framework utilized Twitter data related to the specific disaster to map the geo-locations of volunteers and victims.</s><s xml:id="_AZrbchg">The states in the framework included the volunteers' spatial and temporal information, which served as inputs for the heuristic allocation strategy.</s><s xml:id="_xsRBDFM">The reward function, controlled by the total distances from agents to victims, was optimized using QL in a POMDP setting.</s></p><p xml:id="_fE5nKWn"><s xml:id="_xehc5Q4">Another aspect of humanitarian logistics is the transportation of patients to healthcare facilities after MCI.</s><s xml:id="_ZZpVRw2">Effective triage and prioritization are crucial to saving lives, but it is a computationally demanding task.</s><s xml:id="_v3pcqm2"><ref type="bibr">Li et al. [167]</ref> studied Whittle's restless bandits approach to learning triage and other relevant decisions over a finite but uncertain time horizon.</s><s xml:id="_3kmZYAa">The number of bandits would, therefore, change over time.</s><s xml:id="_Us3tmXd">Because of the stochastic nature of this problem, the authors proposed novel lagrangian relaxation methods to decompose the original problem, which have gained significantly higher performance.</s><s xml:id="_xme2x8c"><ref type="bibr">Lee et al. [101]</ref> developed a MARL framework powered by imitation learning to address the problem.</s><s xml:id="_acyEeXG">Their goal was to maximize the number of survivors in MCIs by optimizing the decisions related to patient admissions to emergency departments (EDs) and diversion of patients.</s><s xml:id="_X6aVQNN">Unlike previous studies focusing on individual patient assignment in outpatient care, this problem involved coordinating multiple homogeneous cooperative EDs (represented as agents in the MARL framework).</s><s xml:id="_f6fMqGV">Each agent only had partial information, such as the current patient arrivals, patient conditions, and its individual available beds (in the ED).</s><s xml:id="_WcvsbNG">Positive rewards were accumulated based on the survival probability of admitted patients, which was determined by their health conditions.</s><s xml:id="_V3yhGh7">An AC approach was used for a multi-agent setting, and the historical actions and realizations were inputted into an RNN to determine current actions.</s><s xml:id="_P6YTZWh">A policy gradient algorithm was implemented based on a generalized advantage estimator (GAE) <ref type="bibr" target="#b74">[78]</ref>.</s><s xml:id="_CAtDm7q">Behavioral cloning was employed as a conceptual optimization method using integer programming to pre-train the neural networks.</s><s xml:id="_Dzm8mAa">This imitation learning technique helps reduce computational time and yields a high-quality policy.</s><s xml:id="_pVAgmJ5">Additionally, a metaalgorithm, subspace partitioning, was utilized as another optimizing approach, as discussed by <ref type="bibr">Shin and Lee [168]</ref>.</s><s xml:id="_y2c4RnF">Another study by Al-Abbasi et al. <ref type="bibr" target="#b60">[65]</ref> also considered a patient transportation problem across heterogeneous medical facilities, where they used DQN to train their model.</s></p><p xml:id="_CNWUsKz"><s xml:id="_hK96Rbu">As presented in Table <ref type="table">2</ref>, the earlier applications in humanitarian logistics utilized QL for solving classes of VRP.</s><s xml:id="_E6rNBE6">Subsequent studies incorporated DNN to construct DRL frameworks.</s><s xml:id="_zwmrrPw">Neural networks' strong predictive capabilities enable the development of more sophisticated models with increased performance, for example, by integrating multiple agents and behavioral models to guide action selection.</s><s xml:id="_QCaz4Y4">The combination of MARL under the framework of POMDP has shown significant potential in disaster relief <ref type="bibr" target="#b44">[48]</ref><ref type="bibr" target="#b45">[49]</ref><ref type="bibr" target="#b46">[50]</ref><ref type="bibr">101]</ref>.</s><s xml:id="_aNPnAb7">Once these complex models are well-trained, RL can provide rapid responses in a short time.</s><s xml:id="_EPhJUMn">Moreover, leveraging imitation learning from an expert policy is anticipated to improve training efficiency.</s><s xml:id="_A4AvTWb">Learning from experts can also</s></p><p xml:id="_Vrp8ztW"><s xml:id="_Pws46Hv">Table 2 Summary of applications under humanitarian logistics Study Year Method Problem class Su et al. [47] 2011 QL VRP Nadi et al. [48] 2017 QL VRP Li et al. [167] 2020 Whittle's restless bandit Scheduling Yang et al. [49] 2020 QL VRP Shin et al. [168] 2020 TD Scheduling Lee et al. [101] 2021 AC (RNN), imitation learning Scheduling Al-Abbasi et al. [65] 2021 DQN Scheduling Yu et al. [36] 2021 QL Resource allocation Fan et al. [169] 2022 DQN Resource allocation Van Steenbergen et al. [170] 2023 Value-based, policy-based RL Resource allocation Shen et al. [50] 2023 QL VRP</s></p><p xml:id="_ekDCZ8C"><s xml:id="_mTzgFSc">aid in extracting domain knowledge, leading to improved interpretability.</s><s xml:id="_WadenxD">As such, DRL offers an efficient solution for tackling complex humanitarian logistics challenges.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2" xml:id="_UHkxeFx">Resource allocation in epidemics</head><p xml:id="_SEcwVdQ"><s xml:id="_UsRmEm8">While we primarily discussed research on epidemics within the macrolevel research thrusts in Sect.</s><s xml:id="_TyZcHtR">4.1, we also acknowledge a few RL applications for epidemics, specifically focusing on disaster response <ref type="bibr">[114]</ref>, as studies falling within the mesolevel research thrusts.</s><s xml:id="_zfaFWNE">In contrast to the epidemic control and healthcare strategies under the macrolevel research thrusts, the topics discussed in this subsection primarily focus on addressing logistics and resource allocation challenges during pandemics.</s><s xml:id="_VpdPwhy">During a pandemic, two crucially scarce medical resources are test kits and vaccines.</s><s xml:id="_QPaGJpt">Focusing on test kit allocations, <ref type="bibr">Bastani et al. [88]</ref> introduced "Eva" as a solution to allocate limited test kits to different groups of arrivals at Greek borders.</s><s xml:id="_3q9uH8H">The problem was initially formulated using MAB, where the prevalence of COVID-19 was estimated through an empirical Bayes approach.</s><s xml:id="_WEyGfMG">Subsequently, certainty-equivalent updates and an optimistic Gittins index were utilized to guide allocation decisions.</s><s xml:id="_y7b9bdZ">In the approximation phase, Lasso feature selection [180] was employed to handle the high dimensionality.</s><s xml:id="_2cUYM44">The "Eva" RL system was evaluated using counterfactual analysis based on inverse propensity weight scoring <ref type="bibr">[181]</ref>.</s><s xml:id="_xcvn6Ax">Additionally, the authors compared the predictive power of epidemiological metrics in gradient boosting <ref type="bibr">[182]</ref> by incorporating different combinations of features and conducting comprehensive estimations and validations.</s><s xml:id="_2WPP7Ka"><ref type="bibr">Gonsalves et al. [89]</ref> introduced an intrinsic conditional autoregressive prior distribution and a hierarchical Bayesian strategy.</s><s xml:id="_9d3yzJU">They utilized mobility data from UberMedia to identify potential testing locations.</s></p><p xml:id="_JdqBrhm"><s xml:id="_eUkw9g6">In the context of vaccine allocations during pandemics, RL agents consider information about various population groups categorized by geographical locations and ages.</s><s xml:id="_vKw7D5S">They utilize such information to determine the allocation of vaccines, considering resource scarcity.</s><s xml:id="_XXNpbgh">The objectives of these RL agents are mainly to minimize the number of infectious cases, maximize the number of critical patients treated, and optimize the economic impacts <ref type="bibr">[171]</ref>.</s><s xml:id="_8N6MAHb"><ref type="bibr">Hao et al. [172]</ref> introduced a hierarchical RL model that addresses the simultaneous allocation of vaccines and beds.</s><s xml:id="_DhUg95C">To mitigate computational costs, they implemented various ranking strategies to filter regions based on specific pandemic thresholds.</s><s xml:id="_Wftn8EW">Other studies have also explored different approaches to vaccine allocation.</s><s xml:id="_E5QxSwh"><ref type="bibr">Tan et al. [173]</ref> employed a random forest algorithm <ref type="bibr">[183]</ref> with real-world data to predict future infections before making vaccine allocation decisions.</s><s xml:id="_xBgeFrn"><ref type="bibr">Hao et al. [175]</ref> went beyond using a simple approach for simulation and relying solely on a black box approach.</s><s xml:id="_27k8ZEB">They instead incorporated expert solutions to enhance the performance of their RL model.</s><s xml:id="_FgR6PT2">Additionally, they conducted a sensitivity analysis to improve the model's explainability.</s><s xml:id="_4n2BRr4">More recent studies have focused on developing MARL methods for vaccine allocation.</s><s xml:id="_vnsjff9">Rey et al. <ref type="bibr" target="#b86">[90]</ref> employed a budget-sharing mechanism to improve performance with Thompson Sampling <ref type="bibr">[184]</ref>.</s></p><p xml:id="_HZWYFtK"><s xml:id="_wsM6kKa">By integrating a SIR model and a stochastic block model network, <ref type="bibr">Xia et al. [176]</ref> proposed a degree-based testing and vaccination model.</s><s xml:id="_brskadz">They employed both Pontryagin's maximum principle <ref type="bibr">[185]</ref> and DQN to optimize the control strategies.</s><s xml:id="_V4AcEB5"><ref type="bibr">Zeng et al. [178]</ref> enhanced the medical supplies dispatching process by incorporating additional states such as "asymptomatic", "hospitalized", and "deceased" into their SEIR model.</s><s xml:id="_xty7WsP">They utilized a DQN structure to optimize the dispatch decisions.</s><s xml:id="_eKtkDDU"><ref type="bibr">Thul et al. [177]</ref> introduced a stochastic optimization approach for vaccine allocation.</s><s xml:id="_UfJqWDe">They considered a collaborative environment where a vaccination agent and a learning agent interactively determine the allocation of stockpiles of vaccines and tests to a set of zones.</s><s xml:id="_qESB6ua">The learning agent makes decisions regarding the allocation of test kits and utilizes the belief state to inform the vaccination agent.</s><s xml:id="_WWp9Zeb">The authors proposed an optimal policy using a parameterized direct lookahead approximation based on Bayesian optimization.</s><s xml:id="_cfc2XmV">Their approach demonstrated superior performance compared to value function approximations, and greater scalability and robustness in both COVID-19 and nursing home contexts.</s></p><p xml:id="_y7QVg2u"><s xml:id="_KBgrj6q">Regarding the allocation of other medical resources during pandemics, <ref type="bibr">Bednarski et al. [174]</ref> and <ref type="bibr">Zhang et al. [179]</ref> explored the use of value-based RL for the redistribution of ventilators to alleviate shortages and reduce costs.</s><s xml:id="_7zPYgnC">They utilized LSTM models with logistics downtime to infer realtime demands across different states.</s><s xml:id="_qCq6rM6"><ref type="bibr">Shuvo et al. [73,</ref><ref type="bibr">186]</ref> optimized the decision-making process for hospital expansions by considering socioeconomic indicators and current capacities, applicable to both pandemic and non-pandemic scenarios.</s><s xml:id="_RtFS85M">Their studies aimed to minimize costs associated with capacity expansion while simultaneously reducing the occurrence of denial of service (DoS) situations.</s><s xml:id="_qWJE2t9">To forecast hospital occupancy, various regression models were employed, and the most suitable model was selected for downstream planning.</s></p><p xml:id="_AKUd9uu"><s xml:id="_pCn5Q7W">Table <ref type="table" target="#tab_2">3</ref> summarizes the applications that address decisionmaking problems related to resource allocation in epidemics, which mainly involve allocating medical resources based on spatial and temporal states.</s><s xml:id="_JkDwgz2">In addition to the popular valuebased and policy-based RL methods, we also highlight the widespread use of MAB algorithms in allocating test kits and vaccines during epidemics due to their scalability.</s><s xml:id="_aGhs4nw">Furthermore, the interpretability of MAB algorithms is enhanced through the use of cooperated linear models <ref type="bibr" target="#b84">[88,</ref><ref type="bibr" target="#b85">89]</ref>.</s><s xml:id="_kcnuyMd">In some cases <ref type="bibr">[176,</ref><ref type="bibr">177]</ref>, robust optimization methods have demonstrated superior performance compared to basic RL methods.</s></p><p xml:id="_QAfuhZH"><s xml:id="_5W3ncbF">This has inspired the exploration of combining robust optimization and RL, with the former pre-training and restricting the action space [187] for RL to achieve higher solving efficiency and rewards, especially in large-scale problems.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3" xml:id="_FMkHuU8">Healthcare supply chain management</head><p xml:id="_PM84g3v"><s xml:id="_AXP2RXN">While RL for healthcare supply chain management is a relatively nascent area, recent studies have begun to delve into various applications to aid decision-making in healthcare supply chain operations.</s></p><p xml:id="_SjBzdjE"><s xml:id="_aJQU3nt">One such paper is <ref type="bibr">Wu et al. [188]</ref>, which addressed a production scheduling problem for medical products.</s><s xml:id="_eW5Tuay">Their proposed algorithm utilizes LSTM as an approximator and policy gradient to schedule the production of medical masks.</s><s xml:id="_K6vYDJD">A study conducted by <ref type="bibr">Zhang et al. [105]</ref> introduced a system of flexible production scheduling specifically designed for ventilators.</s><s xml:id="_bNG9d8Y">Their proposed framework employed a hierarchical RL approach, utilizing heterogeneous digital twin agents to enhance collaboration efficiency between humans and machines.</s><s xml:id="_aNBjAX8">It is worth noting that this line of research often relies on simulation results, which can involve more realistic scenarios.</s><s xml:id="_F4ZE8QP"><ref type="bibr">Asadi et al. [190]</ref> studied the supply of critical medical needs, focusing on optimizing routing and delivery.</s><s xml:id="_pZwDrnJ">They utilized RL to tackle a medical drone delivery problem, where the RL agent considered battery levels and demands to optimize recharging actions.</s><s xml:id="_YdUHqZs">The objective was to maximize the expected satisfied demand.</s><s xml:id="_HzbS2Mg"><ref type="bibr">Seranilla and Löhndorf [192]</ref> considered the possibility of facility failures due to natural disasters.</s><s xml:id="_V4ngSwg">They utilized shadow price approximation for a multistage stochastic vaccine facility location problem.</s><s xml:id="_R2JshE8">Their proposed ADP approach successfully reduced operational and transportation costs by 30%.</s><s xml:id="_t8NXFQ2"><ref type="bibr">Wang et al. [66]</ref> addressed location routing problems for emergency facilities with a two-stage DRL approach.</s><s xml:id="_XAceqBx">Inventory management of medical products is another area of interest.</s><s xml:id="_T6rZHvz">Using DQN, <ref type="bibr">Ahmadi et al. [191]</ref> examined inventory decisions for perishable pharmaceutical products.</s><s xml:id="_2QeCU6D">Similarly, <ref type="bibr">Abu et al. [189]</ref> investigated a standard replenishment problem in a medical supply chain, where a DQN agent determines whether to refill or not to minimize refilling costs, storage costs, and shortage costs.</s><s xml:id="_CzwHFWc"><ref type="bibr">Tseng et al. [75]</ref> utilized AC to facilitate dynamic capacity planning of decentralized regenerative medicine.</s><s xml:id="_crNJyQA"><ref type="bibr">Van Vuchelen et al. [193]</ref> optimized health facility stock management through PPO.</s><s xml:id="_VVjKVVU">Their derived transshipment policies enhanced service level equity, particularly in resource-constrained environments, and were robust given demand seasonality.</s><s xml:id="_JJx8BQG">Recently, Abouee-Mehrizi et al. <ref type="bibr" target="#b41">[45]</ref> studied a stochastic perishable inventory control problem for blood platelets, where the shelf-life of delivered units is uncertain and potentially depends on the order size.</s><s xml:id="_bBT8NFN">Their ADP-based blood platelets ordering policy, which approximates a non-convex value function using basis functions and simulation-based policy iteration, significantly outperforms historical hospital performance and other benchmarks in a case study using real data from Canadian hospitals.</s></p><p xml:id="_RhAZv37"><s xml:id="_EWxgb9a">With comprehensive numerical experiments, their study has made valuable contributions to platelet inventory management under uncertainty.</s></p><p xml:id="_UZK9rtM"><s xml:id="_t4bAaq2">The applications of RL for healthcare supply chain management are summarized in Table <ref type="table" target="#tab_3">4</ref>.</s><s xml:id="_Dkz2Yyw">The table indicates that this field is an emerging area and presents diverse applications.</s><s xml:id="_BMXksp4">These applications span various areas, including production, routing, and inventory management.</s><s xml:id="_ytNDFKr">Moreover, they are solved through a variety of RL approaches.</s><s xml:id="_NdV3cmV">From our review, studies in healthcare supply chain management have utilized conventional value-based and policy-based RL methods to optimize medicine replenishment and transportation decisions.</s><s xml:id="_mFsxeaX">Recent research has also explored adopting a hierarchical framework <ref type="bibr">[105]</ref> and leveraged QL as an adaptive heuristic approach to accelerate the convergence of medical supplies scheduling <ref type="bibr">[118]</ref>.</s><s xml:id="_3J2fHdG">However, there is still significant potential for further exploration and applying more efficient methods.</s><s xml:id="_X4FkZvC">Such advancements are expected to yield substantial benefits for the healthcare service industry.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_5YrXs9n">Microlevel research thrusts</head><p xml:id="_fmWteTc"><s xml:id="_n6pxgzu">The most prominent level of HOM research is the microlevel research thrusts, as suggested by a number of studies [124,</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1" xml:id="_Q2DDpQv">Emergency medical services</head><p xml:id="_nczqB76"><s xml:id="_Y79su7e">Managing a fleet of emergency ambulances efficiently can be difficult due to their limited availability and the unpredictable distribution of emergency calls regarding location and time.</s></p><p xml:id="_75T4Ant"><s xml:id="_KQvQtGu">In the past, researchers mainly focused on static policies for ambulance dispatch.</s><s xml:id="_KGkPCVE">With technological advancements, there is a growing interest in studying dynamic vehicle operations.</s><s xml:id="_2dCGsPm">One popular method in the field is the development of ADP approaches using basis functions for approximation (as summarized in Table <ref type="table" target="#tab_4">5</ref>).</s><s xml:id="_mXXpDBm">Several formulations have been proposed to address problems in different scenarios.</s></p><p xml:id="_ZVv372c"><s xml:id="_HRCHZhA">In a pioneering study by <ref type="bibr">Maxwell et al. [194]</ref>, an ADPbased model and a greedy heuristic for dispatch assignments were proposed for ambulance redeployment.</s><s xml:id="_8x6UDVu">The paper also considered call center management, where a request is lost if all line pickers are busy.</s><s xml:id="_6tcyrAc">The objective was to simultaneously minimize the total number of missed calls, total response time, and relocation costs.</s><s xml:id="_cx69Yy8">The authors utilized direct search <ref type="bibr">[196]</ref> to fine-tune their ADP policies.</s><s xml:id="_vKWXX7j">Subsequent works aimed to incorporate ADP into both relocation and dispatch decisions.</s><s xml:id="_TBtgeJW"><ref type="bibr">Schmid et al. [195]</ref> proposed an ADP algorithm that dynamically relocates and dispatches vehicles, aiming to minimize the total response time of all requests under stochastic travel time and changing request volumes.</s><s xml:id="_uuTcCPq"><ref type="bibr">Nasrollahzadeh et al. [197]</ref> studied a similar problem and applied real data.</s><s xml:id="_MuSJxZZ">Another study utilized a first-order stochastic dominance method to enhance the robustness of solutions <ref type="bibr">[200]</ref>.</s><s xml:id="_PQHp5Ap">One of the challenges in this research area is the development of a comprehensive environment to simulate arrivals, relocation outcomes, and dispatch processes, which can be time-consuming.</s><s xml:id="_3er3yAq">To address the challenges, <ref type="bibr">Allen et al. [198]</ref> developed a complete gym-compatible environment for this problem.</s><s xml:id="_vBnDz6U">This environment involves multiple vehicles, dispatch centers, and patients, enabling the simulation of the entire ambulance dispatch process.</s><s xml:id="_gE63JpC">In recent work by <ref type="bibr">Gao et al. [201]</ref>, ambulances were effectively coordinated with UAVs using DNN-based policy iteration.</s><s xml:id="_DxMEsaF">The objective was to minimize EMS response times for better patient health outcomes.</s><s xml:id="_xGVq2A9">The action space was event-based, depending on the state constructed from queueing, temporal, and geographic properties.</s><s xml:id="_sKpsZtZ">The authors particularly emphasized their optimal policies when facing surge demands.</s></p><p xml:id="_tR5PwEd"><s xml:id="_9CM6ETb">Instead of focusing on the operations of ambulance fleets, <ref type="bibr">Benedetti et al. [199]</ref> studied the application of DQN to a traffic management problem with emergency vehicles.</s><s xml:id="_pwGzehF">Here, the DQN agent learns the status of the lane and controls traffic lights to reduce the waiting time for emergency vehicles.</s><s xml:id="_z89uc55"><ref type="bibr">Su et al. [102]</ref> designed a MARL framework that combines emergency vehicle routing with traffic signal control and minimizes travel times of both emergency vehicles and other vehicles by measuring their introduced lane pressure.</s><s xml:id="_8HRtvkQ"><ref type="bibr">Henderson et al. [202]</ref> highlighted the challenges faced by the EMS systems, including issues like traffic congestion, heterogeneous vehicles, and the growing volume of emergency calls.</s><s xml:id="_W9m7JhB">Their review provided an overview of widely utilized methods to address these challenges, including realtime optimization, offline optimization, stochastic DP, and ADP.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2" xml:id="_zxV55R6">Outpatient care</head><p xml:id="_v7mzhW4"><s xml:id="_mxadVuD">Outpatient care, also known as ambulatory care <ref type="bibr">[128,</ref><ref type="bibr">203]</ref>, refers to a range of medical services provided without requiring hospital admission.</s><s xml:id="_jjm3ym3">In an RL framework, one notable characteristic of outpatient care is that an episode representing patient care generally involves one or multiple visits to healthcare facilities within the same day.</s><s xml:id="_e7eRZ2Z">Typical examples of RL applications for outpatient care include patients visiting EDs, laboratories, surgical centers, or diagnostic centers.</s><s xml:id="_F6Mfats">In these settings, healthcare organizations aim to satisfy the demands for services.</s><s xml:id="_7hambpW">Given the capacity limitations and resource constraints in outpatient departments, optimization is needed.</s><s xml:id="_uH7YF3F">In recent years, researchers have developed RL and ADP approaches to address the challenges in outpatient care.</s><s xml:id="_ACgnMEM">These approaches have been applied in a range of applications to optimize resource allocation and improve the efficiency of outpatient services.</s><s xml:id="_mSMpgeG">The main challenge revolves around patient scheduling for outpatient resources or facilities, with the underlying objective of selecting or prioritizing patients effectively.</s><s xml:id="_nmNb6MR"><ref type="bibr">Patrick et al. [204]</ref> were among the first to employ ADP for cost-effectively achieving wait-time targets in patient scheduling for computerized tomography (CT) scanners.</s><s xml:id="_FYHT4aw">Their approach involved making decisions on available appointment slots to assign to waiting demand units, considering stochastic patient arrivals.</s><s xml:id="_eQzGBUw"><ref type="bibr">Huang et al. [205]</ref> extended the research by applying QL to a business process management model for resource allocation, using radiology CT-scan examination procedures as a case study.</s><s xml:id="_exgTQtF"><ref type="bibr">Lee et al. [210]</ref> focused on detecting hepatocellular carcinoma within the constraints of screening capacity.</s><s xml:id="_6T2GkHn">They employed greedy, interval estimation, and Boltzmann exploration techniques to maximize the number of detected cancers and generate risky ranks for patients.</s><s xml:id="_Nm6vmAM">They further improved their methodology by incorporating an MAB framework <ref type="bibr" target="#b87">[91]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_dusgEQy">123</head><p xml:id="_VjSzsPy"><s xml:id="_EBpCdbx">Each bandit represented a POMDP, and one patient was selected for screening in each decision epoch based on health state estimations.</s><s xml:id="_jqpAvKT">Transition matrices for screened and unscreened patients were constructed separately within the clinical system.</s><s xml:id="_5zAAUR2">The proposed optimal policy resulted in detecting 22% more early-stage cancer cases and suggested outpatient decision-making with a truncated planning horizon.</s><s xml:id="_nTje2Pq"><ref type="bibr">Lee et al. [211]</ref> applied DQN to make assignments of patients to different medical resources, including X-ray or CT scanners and consultants.</s><s xml:id="_gU6twdJ">The state information included patients' demands and acuity levels.</s><s xml:id="_BPZuX47">By adapting DQN, their approach prioritized risky patients and minimized waiting times, outperforming conventional scheduling rules.</s><s xml:id="_zeeQ5Ts">Recently, Zhalechian et al. <ref type="bibr" target="#b88">[92]</ref> made contributions to research in the application of online learning and for healthcare resource allocation.</s><s xml:id="_Nr797cF">They introduced a novel and generic framework that synergizes contextual learning with online allocation mechanisms to enable personalized decision-making under uncertainty.</s><s xml:id="_FxSQhu3">Besides the explorationexploitation trade-off, their proposed algorithms address critical challenges, such as adversarial customer arrivals, stochastic rewards and resource consumption, and delayed feedback, with performance guarantees.</s><s xml:id="_jVjMmRz">An online advance scheduling algorithm, which incorporates multiday booking and no-show behavior, demonstrates strong performance theoretically and empirically using real data from their collaborating health organization.</s><s xml:id="_vkhgYvc"><ref type="bibr">Astaraky et al. [209]</ref> presented a surgical scheduling problem, taking into account the availability of operating rooms and recovery beds.</s><s xml:id="_qEHFBju">Their objective was to minimize the complexity and cost of bookings by determining the number of advanced days for patients to book.</s><s xml:id="_WNvPnAG">They used a least-square iteration method to fine-tune the approximation parameters for state vectors, which include the master schedule, booking slate, hospital census, and waiting demand.</s><s xml:id="_hYG3qhV">This approach was compared to a FIFO scheduling policy, and their proposed ADP policy consistently outperformed the FIFO policy in both high and low system capacity scenarios.</s><s xml:id="_RW9DEWp"><ref type="bibr">Zhang et al. [213]</ref> designed a recursive least-squares TD algorithm to balance waiting times and the over-utilization of surgical resources.</s><s xml:id="_SBFqHM6">Decisions were made on a weekly basis to select which patients would be treated.</s><s xml:id="_DBrqPDW">The MDP state was defined by patients' groups, required specialties, maximum recommended waiting times, and the number of associated patients.</s><s xml:id="_ybDFvF4">The objective was to minimize surgery costs and delays.</s><s xml:id="_dG5jNxz">They also incorporated structural analysis into the ADP framework to improve efficiency by generating a feasible action subspace.</s><s xml:id="_F3hpxXY">In more recent studies, Xu et al. <ref type="bibr" target="#b40">[44]</ref> addressed the backlog of elective surgeries caused by disruptions during the pandemic.</s><s xml:id="_HQmSvJF">They applied a model-based piecewise decaying -greedy RL approach with an auxiliary system <ref type="bibr">[216]</ref> to minimize the time required to clear the surgical backlog and restore surgical activity.</s><s xml:id="_fndPZEj">A queueing network system consisting of a backlog queue and a newly arrived queue was formulated as a countable-state MDP.</s><s xml:id="_X3hFnJM">Dynamic patient scheduling for these two queues was implemented based on patients' clinical urgency.</s><s xml:id="_b8ntaTD">In the context of the pandemic, D'Aeth et al. <ref type="bibr">[215,</ref><ref type="bibr">217]</ref> developed an optimal nationwide prioritization scheme.</s><s xml:id="_akeZmKr">They modeled each individual as a DP considering each patient's health status and aggregated all individuals as a grouped weakly coupled DP with global constraints (e.g., hospital beds, doctors, and nurses).</s><s xml:id="_5DxhqBF">Treatment options, such as prioritizing specific disease patients, were determined for each individual to maximize the overall years of life gained nationwide.</s></p><p xml:id="_qU4pJht"><s xml:id="_sCxbVVQ">In appointment scheduling, <ref type="bibr">Lin et al. [206]</ref> utilized aggregation and Monte Carlo simulation to determine slot assignments for call-in patients with different no-show rates.</s><s xml:id="_jdMkVZQ"><ref type="bibr">Feldman et al. [208]</ref> investigated preference-based healthcare plans and customized appointments.</s><s xml:id="_HnDs8zT">They moved from a static model to a dynamic model that considers patients' no-show behavior and proposed a heuristic solution.</s><s xml:id="_3TpZPHj"><ref type="bibr">Diamant et al. [212]</ref> formulated a multistage patient scheduling problem as a rolling-horizon MDP.</s><s xml:id="_Bx6maee">Their approach described different types of patients undergoing specific care plans consisting of a series of assessments or treatments.</s><s xml:id="_wFtKuKz">The state provided patient-centered care plans, including no-shows and patients who rescheduled, to maximize the number of patients who could successfully complete all stages of treatments.</s><s xml:id="_RHnxw8Z">Patients' arrivals, referrals, and ineligibility rates were modeled using statistical distributions, and dual variable aggregation helped efficiently solve the large-scale linear programming model.</s><s xml:id="_vhxgdYS">This work is built upon earlier research on variable aggregation <ref type="bibr">[218]</ref>.</s><s xml:id="_hVCsJvG"><ref type="bibr">Schuetz et al. [207]</ref> considered the costs of rejecting a request, no-shows, and overtime in appointment scheduling.</s><s xml:id="_bdge9yP">They used ADP to decide whether to accept or reject a new request from a class-type combination (patient and examination classes).</s><s xml:id="_5XrFwSa"><ref type="bibr">Agrawal et al. [214]</ref> proposed an ADP approach that takes patients' requests of "dedicated," "flexible," and "urgent" (which must be met on the same day) to determine appointment decisions.</s><s xml:id="_8hFxNkz">Their objective was to maximize revenue and minimize physician overtime and idle time while satisfying as much demand as possible.</s></p><p xml:id="_sns7y2u"><s xml:id="_fZHaJXU">Table <ref type="table" target="#tab_5">6</ref> provides a summary of research studies in outpatient care discussed in this section.</s><s xml:id="_yT9uQA6">Among these applications, ADP is one of the most popular methods for optimizing outpatient service delivery.</s><s xml:id="_GGBjqSb">This model-based approach has been simulated and validated in clinics and hospitals of different scales <ref type="bibr">[204,</ref><ref type="bibr">209]</ref> and has consistently outperformed heuristic algorithms regarding total costs, while consuming less computing time than DP.</s><s xml:id="_NNeb45J">Different RL methods have also been compared in the existing studies.</s><s xml:id="_HuYVHPg">For example, in <ref type="bibr">Diamant et al. [212]</ref>, ADP outperformed A2C and greedy algorithms regarding rewards for the featured patient group.</s><s xml:id="_h7fZGcz">These findings suggest that RL approaches require more research efforts Integrating model-based <ref type="bibr" target="#b40">[44]</ref> and dimensionality reduction methods <ref type="bibr">[220]</ref> is expected to solve more specific and complex problems.</s><s xml:id="_hSD6STp">An interesting and important future direction is accommodating dynamic changes in factors such as hospital capacities, patient preferences, and doctor preferences to enable real-time operations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3" xml:id="_Va2TuQS">Inpatient care</head><p xml:id="_UudS5Rv"><s xml:id="_8abvQJ9">Inpatient care primarily encompasses the management of patient flow and related HOM that take place in inpatient wards.</s><s xml:id="_Dvx8Bgk">This includes admitting and discharging patients, transferring patients between specialty wards, and estimating patient LOS.</s><s xml:id="_um45sZN">In recent years, researchers <ref type="bibr">[221,</ref><ref type="bibr">222]</ref> have conducted extensive reviews of the latest modeling and analytical techniques for inpatient management.</s><s xml:id="_DQXHu5z">Our current review also finds that solutions utilizing ADP and RL have demonstrated substantial potential in enhancing inpatient care.</s><s xml:id="_SmWDJD3"><ref type="bibr">Samiedaluie et al. [223]</ref> developed a queue theory-based ADP approach to manage stroke patients in the neurology ward effectively.</s><s xml:id="_r5YN65P">The state information involved the number of patients with different severity levels and occupied beds.</s><s xml:id="_rvSYv9y">The objective was to minimize waiting and transferring costs, taking into account the quality of life determined by discharge destinations.</s><s xml:id="_KCjNXse">The authors also incorporated a priority cutoff policy during the experimental phase to facilitate the implementation of the ADP solution.</s><s xml:id="_WgmHngr">In a similar problem, Dai et al. <ref type="bibr" target="#b27">[30]</ref> modeled inpatient operations as a multi-pool queueing system and combined fluid control with singlepool approximation in their ADP approach.</s><s xml:id="_WxvmM36">Their aim was to minimize the costs associated with the inpatient overflow policy.</s><s xml:id="_EByW4CZ">To tackle the computational challenge, they utilized the basis function for the midnight time epoch to guide the basis functions for other time epochs, when approximating value functions using admission and discharge information.</s><s xml:id="_2K9D6mW"><ref type="bibr">Heydar et al. [227]</ref> formulated the patient-to-bed problem to determine the next-best decision when the most appropriate ward was unavailable, considering random arrivals and inpatient LOS.</s><s xml:id="_3es3khG">They employed linear approximations supported by features related to patients and wards in their ADP approach, while using phase-type distributions to model the LOS.</s><s xml:id="_b5kEEHw">In general, ADP policies demonstrated a significant reduction in boarding time from ED and effectively controlled total costs compared to popular existing strategies.</s><s xml:id="_85rvQZD"><ref type="bibr">Braverman et al. [225]</ref> created an ADP solution based on Stein's method <ref type="bibr">[230]</ref> and implemented it in an inpatient overflow experiment (presented in <ref type="bibr">Dai et al. [30]</ref>).</s><s xml:id="_bPgSe9U">The suboptimality of the solution was established conceptually using the Taylor equation.</s><s xml:id="_Ew6KhtZ">In another study, <ref type="bibr">Liu et al. [228]</ref> assessed their constrained linear bandits approach for managing inpatient overflow considering fairness.</s><s xml:id="_QFWERNR">Following their prior work on the application of MAB for outpatient <ref type="bibr" target="#b88">[92]</ref>, <ref type="bibr">Zhalechian et al. [122]</ref> proposed a data-driven algorithm for a hospitals' admission control problem where the patients' lengths of stay are uncertain, given limited reusable inpatient beds.</s><s xml:id="_nuG5JUw">Their data-driven admission control algorithm is designed to adaptively learn the readmission risk of different patients through batch learning with delayed feedback and choose the best care unit placement for a patient based on the observed information and the occupancy level of the care units.</s><s xml:id="_NM2VE7P">The performance measure of this online algorithm is Bayesian regret, and the Bayesian regret bound is also proved.</s><s xml:id="_dmPVby6">With experiments on data from a healthcare system, their results show an improved performance compared to traditional admission control methods.</s><s xml:id="_xjSenXW">Their paper highlights the potential benefits of using data-driven approaches in healthcare and suggests that this insightful approach can be further improved with enhanced data quality and volume and algorithms.</s></p><p xml:id="_XjMjk8c"><s xml:id="_fNEVtnM">In ICU management, <ref type="bibr">Prasad et al. [224]</ref> proposed a QL approach to optimize the weaning process of mechanical ventilation.</s><s xml:id="_EKJun8z">They considered a 32-dimensional representation of the patient state incorporating as many useful and easily accessible features as possible.</s><s xml:id="_P7hw83y">Actions to determine whether to have the patient off or on the ventilator and the level of sedation to be administered over the next 10-minute interval are determined at each stage.</s><s xml:id="_pPTV7A6">This innovative approach was tested on real patient data and has shown promising results in minimizing reintubation rates and regulating physiological stability.</s><s xml:id="_vzEShrn"><ref type="bibr">Shuvo et al. [186]</ref> conducted a study on determining the optimal timing for increasing the number of beds in hospitals for upgrade.</s><s xml:id="_GSp3Ru5">They considered the current capacity and the growth of the patient population, aiming to minimize costs associated with untreated patients and the maintenance of additional beds.</s><s xml:id="_d6wXnYv">With a comparison with myopic policies, their proposed A2C approach yielded the lowest costs.</s><s xml:id="_EPtKhuK">Subsequently, they extended their research by incorporating multiple hospitals in different geographic regions and including age information in the state space <ref type="bibr" target="#b68">[73]</ref>.</s><s xml:id="_fZmUue2">By utilizing real-world data, they were able to improve the effectiveness of their proposed approach using decision tree regression and predict population growth using models <ref type="bibr">[226]</ref>.</s></p><p xml:id="_gNZDPUR"><s xml:id="_DXCxZGt">RL has also been applied for staff scheduling problems for inpatient operations.</s><s xml:id="_fEh9qEe"><ref type="bibr">Lazebnik [229]</ref> enhanced staff schedules by employing agent-based simulation and policy gradient approaches with the rmsprop algorithm <ref type="bibr">[231]</ref>.</s><s xml:id="_WSgZ5E7">This approach demonstrated improved resilience to anomalies.</s><s xml:id="_rgBw9W8">The study also revealed a second-order polynomial relationship between successful treatment and budget.</s></p><p xml:id="_ZuaWSuQ"><s xml:id="_jEZBnAu">Table <ref type="table" target="#tab_6">7</ref> provides an overview of the applications of RL in inpatient care.</s><s xml:id="_dwFBg2s">The most popular approaches include ADP and A2C, which are well-suited for capturing the dynamic nature of inpatient operations, such as modeling inpatient flow.</s><s xml:id="_nAN8fT3">RL models often utilize queueing models to estimate queue lengths and waiting times, which are essential for making informed decisions regarding inpatient admission and discharge.</s><s xml:id="_Pv87Rq6">As we have reviewed in this subsection, the applications of RL for inpatient care have shown promise in recent studies.</s><s xml:id="_9NxduRH">The main objectives of these studies were to minimize patient boarding, reduce the time patients spend in the hospital, and avoid associated penalties while maintaining the quality of care and improving inpatient outcomes.</s><s xml:id="_74B2tRC">Accurate estimation of patient arrivals and demands is crucial, and various effective forecasting regressions and statistical inferences can be utilized.</s><s xml:id="_BYng426">Downstream optimization methods would also need to be designed so that estimation errors are considered.</s><s xml:id="_cEUfD4C">However, selecting the most appropriate basis function for ADP (or the approximator for RL) remains a challenge, as it depends on the characteristics of the inpatient operations.</s><s xml:id="_hSecrkv">Therefore, conducting experimental trials and comparisons is necessary to enhance the RL approaches' effectiveness.</s><s xml:id="_d57QcYA">Future research could combine inpatient, outpatient, and other hospital processes into a more complex interactive system to guide better decision-making.</s><s xml:id="_x4sxRNC">Additionally, incorporating human behaviors and preferences into modeling inpatient operations, as done in outpatient care studies, could be valuable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4" xml:id="_tuvcqhf">Residential care</head><p xml:id="_6dTyk2Z"><s xml:id="_dxBaM7y">Residential care involves providing personalized healthcare services to patients within the comfort of their own homes  <ref type="bibr">[232]</ref>.</s><s xml:id="_UrtyGHZ">This approach enables individuals to maintain their independence and enhance their quality of life <ref type="bibr">[233]</ref>.</s><s xml:id="_tz7q47c"><ref type="bibr">Cire and Diamant [232]</ref> developed an ADP approach to optimize the assignment of health practitioners (HPs) to patients.</s><s xml:id="_22heMjF">They compared four policies and found that the models based on fluid approximations <ref type="bibr">[234]</ref> outperformed those that utilized heuristics.</s><s xml:id="_rVka5up">Their methodology demonstrated superior performance compared to commonly used constrained versions of VRP when accounting for future uncertainty.</s><s xml:id="_XabCmbj">Their framework involved deciding whether to accept or reject a patient referral and assigning an HP to the patient if the decision is accepted while accounting for resources, care continuity, and time windows.</s><s xml:id="_KTYama9">The policy for arranging HPs working in a small set of adjacent regions aimed to maximize the expected long-term cost savings while minimizing the number of rejected referrals.</s><s xml:id="_8ztkCNg">In another study, <ref type="bibr">Salehi et al. [235]</ref> combined RL with a functional resonance analysis method (FRAM) to explore complex operations.</s><s xml:id="_YghCta5">They deployed an RL agent to examine 38 functions (such as "access the patient," "go home without services," "invite a caregiver," etc.) and incentivized it to select the optimal functional routes based on the patient's health improvement.</s></p><p xml:id="_QcGGBRH"><s xml:id="_b4cZVnp">In recent years, the Internet of Medical Things (IoMT) has been increasingly utilized in residential care <ref type="bibr">[236,</ref><ref type="bibr">237]</ref>.</s><s xml:id="_tYncvWy">IoMT refers to a network that integrates medical devices, sensors, learning algorithms, and mobile health technologies.</s><s xml:id="_es7YdtK">Through IoMT, healthcare institutes can collect real-time health information, provide remote services, and provide personalized interfaces <ref type="bibr">[238,</ref><ref type="bibr">239]</ref>.</s><s xml:id="_be6qWz2">To improve the quality of service (QoS) of IoMT facilities, a number of RL-based technologies, including blockchain [240], cloud systems <ref type="bibr">[241]</ref>, and fog computing <ref type="bibr">[242]</ref>, have been developed in the research community of telecommunications.</s><s xml:id="_QbyEVHp">RL-based wearable devices can also provide customized support for patients' rehabilitation <ref type="bibr">[233,</ref><ref type="bibr">243]</ref>.</s><s xml:id="_bU7dbGH">By reminding or alerting patients in their daily lives, RL assistance is expected to guarantee high-quality residential care for impaired patients and reduce the burden on their caregivers <ref type="bibr">[244]</ref>.</s><s xml:id="_aHfYs2s">In the OR community, queueing theory has been utilized to optimize the matching process between patients and medical resources, such as specialists, in cloud healthcare systems.</s><s xml:id="_NRyMb6R">The objective was to minimize the total medical costs <ref type="bibr">[245]</ref>.</s><s xml:id="_mEyGGfZ"><ref type="bibr">Tiwari et al. [246]</ref> utilized a combination of <ref type="bibr">MARL and Federated Learning [247]</ref> to minimize the latency of an IoMT system.</s><s xml:id="_XFhUr45"><ref type="bibr">Seid et al. [248]</ref> used a similar learning method to minimize the energy consumption of a drone-enabled healthcare system.</s><s xml:id="_U5yW9JF"><ref type="bibr">Chen et al. [249]</ref> optimized task offloading in wireless body area networks using a DDPG-based strategy and mobile edge computing servers for IoMT.</s></p><p xml:id="_mvxzZtn"><s xml:id="_9ySHGWt">Based on our review, we observe the number of studies with the deployment of model-based ADP and MARP techniques in residential care <ref type="bibr">[232,</ref><ref type="bibr">245,</ref><ref type="bibr">246,</ref><ref type="bibr">248]</ref>.</s><s xml:id="_9EAF77Y">These studies are also of interest to other disciplines, such as telecommunications and electronics.</s><s xml:id="_RbqMUQA">The rapidly growing and multi-disciplinary field of IoMT is expected to revolutionize residential care by facilitating remote patient monitoring, personalized medical recommendations, and the applications of OR for HOM.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4" xml:id="_cjyFZ38">Medical treatments</head><p xml:id="_tPY7SNY"><s xml:id="_G6uP9zr">It is important to distinguish HOM from some other similar areas where RL has also been widely used in recent years.</s><s xml:id="_z6p2Xbz">As stated in Sect.</s><s xml:id="_2wSWUMb">3, our review analyzes existing HOM research as described in the healthcare ecosystem map, where non-HOM research studies focusing on medical imaging and medical robotics for medical treatments are excluded.</s><s xml:id="_zrjQKkZ">These excluded studies often involve advanced computer vision and robotics techniques that may differ significantly from the use of ADP and RL in HOM.</s><s xml:id="_SA9BuZs">For more comprehensive reviews focusing on medical treatments, we refer the reader to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">[250]</ref><ref type="bibr">[251]</ref><ref type="bibr">[252]</ref>.</s></p><p xml:id="_PMzgeJS"><s xml:id="_qenyMRX">Another area that is related to, yet different from HOM, is dynamic treatment regimes, which pertain to detailed treatment strategies for patients in hospitals, healthcare facilities, and patient homes <ref type="bibr">[253]</ref>.</s><s xml:id="_hWQX7ed">RL-based clinical decision-making has proven beneficial in assisting medical staff with tasks such as determining dosing regimes for chemotherapy in clinical trials <ref type="bibr">[254]</ref>, split liver transplantation [255], treating Parkinson's disease <ref type="bibr">[243,</ref><ref type="bibr">256]</ref>, diagnosing skin cancer <ref type="bibr">[257]</ref>, and managing glycemic control in Type 2 diabetes <ref type="bibr">[258]</ref>.</s><s xml:id="_KWVnMwM"><ref type="bibr">Fatemi et al. [259]</ref> used DQN to identify medical deadends of patients' sequential treatments and avoid risky states for treatment security.</s><s xml:id="_gzpRFYT"><ref type="bibr">Bennett et al. [260]</ref> demonstrated the benefits of their proximal RL approach in a POMDP setting for sepsis management <ref type="bibr">[261]</ref>.</s></p><p xml:id="_wD97mSE"><s xml:id="_HZ3FxCD">Under the umbrella of medical decision-making, dynamic treatment regimes are having more and more RL applications.</s><s xml:id="_jYwwuSp">This section is only intended to exemplify a few insightful studies, as there are still numerous explorations and positive outcomes coming in this field.</s><s xml:id="_zDKcMWc">For more comprehensive reviews on this topic, we refer the reader to <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">39,</ref><ref type="bibr">262]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_aV2NMru">Statistics</head><p xml:id="_kmdyHmq"><s xml:id="_SF9Xmvd">Fig. <ref type="figure" target="#fig_0">1</ref> presents the trend of the number of publications related to RL applications in HOM.</s><s xml:id="_T2tcgCD">The earliest related studies date back to 2008.</s><s xml:id="_kr2SNaQ">These early studies initially utilized methods from optimal control and DP, which align with the RL paradigm.</s><s xml:id="_H6erQ33">The number of publications remained steady at around one to two studies per year until 2016 when RL algorithms demonstrated mastery in the game of <ref type="bibr">Go [263]</ref>.</s><s xml:id="_dgUMevk">In 2017, there was a peak in the use of RL in mesolevel and microlevel research thrusts.</s><s xml:id="_MUQBXAA">Since then, there has been exponential growth in publications, which has continued until the time of this review.</s><s xml:id="_tGtjGsv">This suggests that RL is becoming increasingly established and effective in solving HOM problems.</s></p><p xml:id="_PEQFGSt"><s xml:id="_b3FVKZe">In terms of the number of publications at each division level, we reviewed 24, 38, and 55 papers under macrolevel, mesolevel, and microlevel research thrusts, respectively.</s><s xml:id="_V4GWqvX">The rapid growth of publications under the marcolevel research thrusts started in 2019, which could be attributed to the COVID-19 pandemic.</s><s xml:id="_huwBVeV">Researchers actively explored the potential of RL in optimizing macrolevel policies associated with healthcare to manage this pandemic better.</s><s xml:id="_yFVkJYX">Similarly, mesolevel applications, which are mostly related to resource allocation and supply chain management, experienced a significant increase after 2019.</s><s xml:id="_JDSH5hw">It has been proven that RL can powerfully assist decision-making during pandemics in practice <ref type="bibr" target="#b84">[88]</ref>.</s></p><p xml:id="_dtXH5H6"><s xml:id="_Sk3SAyz">Our review also reveals that the applications of RL under the microlevel research thrusts have a longer inception period.</s><s xml:id="_UGRkBPn">In addition to the rapid growth observed after 2019, RL applications under the microlevel research thrusts have been consistently developed every year.</s><s xml:id="_QdHEmtZ">The majority of these applications utilize ADP to solve the associated MDP, as illustrated in Fig. <ref type="figure">2</ref>.</s><s xml:id="_SQxjTep">This is because applications under the microlevel research thrusts, such as surgical scheduling, typically have explicit MDP formulations that allow for the derivation of analytical structures.</s><s xml:id="_4TffVxB">These characteristics also make ADP a suitable approach.</s><s xml:id="_wPDJDjc">With the advancements in neural networks and deep learning, both ADP and DRL have become viable options for problems under microlevel research thrusts in HOM.</s></p><p xml:id="_e6eJHjG"><s xml:id="_7hwuzFK">Figures <ref type="figure">2</ref> and <ref type="figure">3</ref> offer further insights into the popularity of RL methods in different HOM applications.</s></p><p xml:id="_BNJHPMD"><s xml:id="_NGAu2MC">Figure <ref type="figure">2</ref> presents the mapping from HOM applications to RL learning methods.</s><s xml:id="_F2mKdA6">Among the 62 studies reviewed, value-based TD, QL, and DQN are the most popular choices.</s><s xml:id="_HmdAXZ6">These methods are particularly prevalent in applications such as COVID-19 control policies, humanitarian logistics, and resource allocation in epidemics.</s><s xml:id="_wkAFPmC">Additionally, more than half of the reviewed applications for residential care implemented QL or DQN.</s><s xml:id="_uhkQqjs">On the other hand, policy-based methods are widely dispersed across all three levels of HOM applications.</s></p><p xml:id="_vteMbpW"><s xml:id="_2QATx7W">Figure <ref type="figure">3</ref> presents the mapping from applications to learning approximation methods, which aligns with the results illustrated in Fig. <ref type="figure">2</ref>. Q table and DNN approximators account for the largest proportion of applications, totaling 73 studies.</s><s xml:id="_64khnVy">These approximators correspond to QL and DQN learning methods, respectively.</s><s xml:id="_mgHaccd">Regression approximators are Fig. <ref type="figure">2</ref> Mapping from applications to learning methods extensively utilized in EMSs, outpatient care, and inpatient care under the microlevel research thrusts.</s><s xml:id="_umtR2ke">This is because regression approximators provide an efficient approximation of the value functions of ADP, as shown in Fig. <ref type="figure">2</ref>. Bayesian inference is employed to estimate the values of actions in MAB frameworks and guide decision-making.</s></p><p xml:id="_j6mdzqc"><s xml:id="_jcnT4Kn">Given that the research on RL for HOM falls within the fields of OR and CS, it is interesting to investigate the evolution of methodologies, as discussed in Sect. 2. Figure <ref type="figure">4</ref> illustrates this evolution.</s><s xml:id="_PZQQ4Tw">OR researchers typically develop ADP methods, while classic RL methods such as TD and MAB focus on learning mechanisms rather than neural networks.</s><s xml:id="_PvjUdqD">On the other hand, CS researchers often use DRL methods like DQN and AC with neural networks.</s></p><p xml:id="_kxYE9BF"><s xml:id="_Dd5Uk2n">Our analysis reveals that ADP and classic RL methods have been applied for over a decade, with a steady but small number of ADP studies each year.</s><s xml:id="_PPdtZ86">Classic RL methods gained popularity during the COVID-19 pandemic.</s><s xml:id="_MMqyJ5P">This trend is consistent with DRL applications, which were first introduced as early as 2017 <ref type="bibr">[219]</ref>.</s><s xml:id="_BdySbhC">Prior to 2017, the amount of research on ADP and classic RL in HOM remained steady, where the two approaches were often used together.</s><s xml:id="_N2Ngdhv">However, since then, classic RL and DRL methods have become more dominant, surpassing ADP, especially after 2019.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_YUAHzrR">Current development</head><p xml:id="_5rsrUKk"><s xml:id="_EQ2Kuuf">Based on the discussions in Sects.</s><s xml:id="_76SbZ7Q">4 and 5.1, we summarize the current development of effective RL methods in HOM, referring to the key RL settings presented in Sect.</s><s xml:id="_SPQABZC">2.3.</s></p><p xml:id="_wWPxMbQ"><s xml:id="_D7RwJQx">One of the most critical discussions is the effectiveness of model-based and model-free RL in HOM.</s><s xml:id="_VK8mT9c">We have observed that model-free RL has been widely applied to macrolevel research thrust, while model-based methods are more applied to microlevel applications.</s><s xml:id="_NZzdfma">At the macrolevel, the system models usually utilize complicated compartmental models in epidemiology governed by ordinary differential equations <ref type="bibr">[153]</ref>.</s><s xml:id="_9dnXpyM">Most reviewed studies tend to rely on the power of "black box" neural networks to learn the system model and find suboptimal policies.</s><s xml:id="_yBPtdRX">This idea is like using complex methods to solve complex problems.</s><s xml:id="_mFrWv9g">Although satisfying results can be produced after sufficient iterations of RL's experiments and simulations, robustness and interpretability would also be essential for real-world deployments.</s><s xml:id="_fW8tYQc">At the microlevel, the system models usually refer to queueing models <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b40">44]</ref> or bandit problems <ref type="bibr" target="#b88">[92,</ref><ref type="bibr">122]</ref> under outpatient or inpatient scenarios, where strong theoretical supports are established.</s><s xml:id="_DJ4PXNV">In this way, model-based RL can leverage the structures of these models to derive properties Fig. <ref type="figure">3</ref> Mapping from applications to approximators of convergence, transitions, or optimal solutions.</s><s xml:id="_Q3Y44VG">The robustness, reliability, and interpretability can thus be strong.</s><s xml:id="_vuRpVFp">At the mesolevel, which is in between the macrolevel and microlevel in terms of problem scales and complexity, we have identified some studies that are pursuing model-based robust solutions [177] and realizing interpretable real-world deployments <ref type="bibr" target="#b84">[88]</ref>.</s><s xml:id="_HwsQV85">It emphasizes the importance of interpretable model-based methods when we are transferring Fig. <ref type="figure">4</ref> Evolution of RL methodologies used in HOM research to practice.</s><s xml:id="_z8Qqv9S">Therefore, model-free algorithms can be suitable for complex macrolevel HOM problems, where explicit models are not fully available.</s><s xml:id="_wrnA4qb">The success of modelbased methods in microlevel applications inspires us to make good use of system models in optimizing decision-making policies.</s><s xml:id="_dP6ySAF">If the system model of the HOM problem is reasonably accurate and theoretically well-defined, model-based RL could be a suitable choice.</s></p><p xml:id="_DbVBknB"><s xml:id="_gDSqHh4">The choice of tabular or non-tabular, value-based or policy-based methods for HOM depends on the state and action spaces of specific problems.</s><s xml:id="_vrXBbVw">Given the limitation on problem scales, tabular methods in HOM have only been effectively applied to some routing problems in humanitarian logistics <ref type="bibr" target="#b43">[47,</ref><ref type="bibr" target="#b45">49]</ref>.</s><s xml:id="_NyqkVJQ">They provide theoretical foundations for more advanced non-tabular methods and always serve as the benchmark for other RL algorithms' evaluation in HOM.</s><s xml:id="_Nug5WRf">Non-tabular methods would be required for addressing problems with high-dimensional state space and tractable action space, such as determining macrolevel discrete epidemic controls <ref type="bibr">[144,</ref><ref type="bibr">147]</ref>.</s><s xml:id="_9psZ2Rk">Figure <ref type="figure">2</ref> also indicates that value-based non-tabular ADP and DQN have been widely applied to HOM applications.</s><s xml:id="_uFPA3Up">If the action space is huge or continuous, such as mesolevel inventory decisions <ref type="bibr" target="#b70">[75,</ref><ref type="bibr">193]</ref>, pure valuebased methods may fail and policy-based methods (e.g., the AC family) are more appropriate.</s></p><p xml:id="_MYWjH7W"><s xml:id="_dHaSV7Q">Given the fact that most of HOM's system models are built with simulation techniques, both on-policy and off-policy methods were consistently applied in every research thrust of HOM.</s><s xml:id="_hk8jSDu">As discussed in Sect.</s><s xml:id="_gqygW4Y">2.3, off-policy methods can be used with existing expert experiences via imitation learning <ref type="bibr">[101]</ref>, by which the convergence issue is expected to be solved effectively.</s><s xml:id="_CyHYyJd">Further, online algorithms under the realm of MAB are particularly suitable for HOM with dynamic environments, where uncertainty is a main concern.</s><s xml:id="_6K9buwG">Successful applications include mesolevel resource allocation in epidemics <ref type="bibr" target="#b84">[88,</ref><ref type="bibr" target="#b86">90]</ref> and microlevel resource matching in outpatient <ref type="bibr" target="#b87">[91,</ref><ref type="bibr" target="#b88">92]</ref> and inpatient units <ref type="bibr">[122,</ref><ref type="bibr">228]</ref>.</s><s xml:id="_f59hxwB">Online learning's adaptability and interpretability are strengths for solving practical applications <ref type="bibr" target="#b84">[88]</ref>.</s><s xml:id="_xwzfesj">If sample paths for learning are lacking in some HOM applications, offline algorithms with experience replay can sufficiently learn from the limited samples and work out a stable policy.</s><s xml:id="_SwAFrQk">Typical RL methods, like DQN and DDPG, usually combine online and offline methods to achieve adaptability and stability simultaneously.</s><s xml:id="_SjMt45N">These methods have been utilized in finding macrolevel mobility restriction policy under pandemics, along with the SIHR model <ref type="bibr">[145]</ref>.</s></p><p xml:id="_xbK89tm"><s xml:id="_2seuyPq">Although the purpose of developing RL approaches is to solve large-scale applications, practical deployment of RL in real-world HOM problems remains a challenge.</s><s xml:id="_pbzvAwn">Most reviewed studies tended to utilize real-world data and simulation-based experiments to benchmark other approaches or real-world experts' policies.</s><s xml:id="_fJXt8kv">However, only a small num-ber of the studies solved large-scale problems in practice.</s><s xml:id="_hyB6VS9">Under the macrolevel research thrusts, the studies focus on the development of RL for optimal healthcare policies and strategies.</s><s xml:id="_bujTkuH">In the studies, RL has a superior performance to human experts' decisions.</s><s xml:id="_Jnw7djY">These studies have built largescale simulations (e.g., modeling populations of millions of people <ref type="bibr">[121,</ref><ref type="bibr">130,</ref><ref type="bibr">139]</ref>) and considered high-dimensional state space <ref type="bibr">[121,</ref><ref type="bibr">133,</ref><ref type="bibr">134,</ref><ref type="bibr">146]</ref> and action space <ref type="bibr">[134,</ref><ref type="bibr">146]</ref> (e.g., as large as 2.16 × 10 59 state-action pairs as in <ref type="bibr">[133]</ref>).</s><s xml:id="_mruJx8z">Our review has not identified any practical implementations of healthcare strategies (e.g., lockdown or mobility restriction policies) that solely rely on RL approaches.</s><s xml:id="_WUGt6EC">These macrolevel healthcare strategies are vital to massive stakeholders, and RL solutions are expected to provide references and assistance for the government and decision-makers.</s><s xml:id="_xAZFZwn">Under the mesolevel research thrusts, Bastani et al. <ref type="bibr" target="#b84">[88]</ref> have deployed their MAB framework to test kit allocations across 40 Greek borders during pandemics in the summer of 2020.</s><s xml:id="_quSMfFE">It is an astoundingly impressive large-scale, nationwide, real-world RL in HOM application.</s><s xml:id="_Cv3ZMCD">The proposed RL approach utilized at most 54,614 passenger locator forms a day, utilizing as many as 185,280 features (i.e., the state space in their problem).</s><s xml:id="_353Wbyv">Under microlevel research thrusts, D'Aeth et al. <ref type="bibr">[215,</ref><ref type="bibr">217]</ref> optimized a large-scale care prioritization scheme that involves 10 million patients in a case study of England.</s><s xml:id="_sHY9tCm">Their proposed weakly coupled DP had around 15 10,000,000 states and 6 10,000,000 actions.</s><s xml:id="_X8Dby5V">Notably, the authors highlighted the future improvements for real-world implementations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_rV6YscM">Challenges and directions</head><p xml:id="_d7WBxqh"><s xml:id="_yMZDEV9">Based on our scoping review of RL methods in HOM applications, summarized statistics, and current developments, we have identified key insights into the use of RL in HOM.</s><s xml:id="_BGA7Zvc">These insights are built upon the advantages of RL in efficiently solving complicated HOM optimization problems.</s></p><p xml:id="_7v4jCRQ"><s xml:id="_BhAwKk4">Complexity HOM applications can be complex.</s><s xml:id="_w6dwTS9">For instance, interactions at the macrolevel and mesolevel, and the integration of emergency care, outpatient care, and inpatient care at the microlevel.</s><s xml:id="_drEfqsg">Advanced RL algorithms with highdimensional representations make it possible to solve these complex systems.</s><s xml:id="_kRuYPQT">MARL is a promising framework incorporating multiple homogeneous or heterogeneous RL agents to achieve more precise and complex simulations.</s><s xml:id="_sMwmHv3">MARL has been successfully applied in a number of disaster and emergency response applications in HOM <ref type="bibr" target="#b45">[49,</ref><ref type="bibr" target="#b46">50,</ref><ref type="bibr">101,</ref><ref type="bibr">102,</ref><ref type="bibr">105,</ref><ref type="bibr">120,</ref><ref type="bibr">146,</ref><ref type="bibr">149,</ref><ref type="bibr">170,</ref><ref type="bibr">201]</ref>.</s><s xml:id="_HYGYKrQ">Another effective modeling approach for complex systems in HOM is the POMDP.</s><s xml:id="_MJVtkzj">In HOM applications, states are often partially observable, and observations can be influenced by unobserved factors such as confounding variables and biased estimations <ref type="bibr">[48, 123 49, 131]</ref>.</s><s xml:id="_R9KZa2H">Therefore, sophisticated algorithms, such as RL with causal inference, are anticipated to address the complexities inherent in the POMDP setting effectively <ref type="bibr">[260]</ref>.</s><s xml:id="_uRG7c7t">Under macrolevel and mesolevel research thrusts, hierarchical RL has demonstrated its advantage in efficiently learning and solving large-scale problems <ref type="bibr" target="#b85">[89,</ref><ref type="bibr">105,</ref><ref type="bibr">150,</ref><ref type="bibr">172]</ref>.</s><s xml:id="_R4PAMBZ">The reviewed three levels are interdependent and need to be considered in concert for integrated care to provide a coordinated and comprehensive healthcare delivery system.</s><s xml:id="_bz8YDkP">Advanced RL algorithms, which efficiently capture the patterns of the complex system with HOM data, will be a strong thrust in this campaign.</s></p><p xml:id="_smgGAZF"><s xml:id="_wbtaywG">Adaptability Given that the HOM applications are always dynamic, the need for flexible and adaptable RL algorithms that can capture the dynamic characteristics of problems and respond to emergency events promptly should be highlighted.</s><s xml:id="_ddkPE6q">Under macrolevel and mesolevel research thrusts, researchers have trained DRL algorithms on various infectious diseases at different stages to ensure their generality <ref type="bibr" target="#b69">[74,</ref><ref type="bibr">120]</ref>.</s><s xml:id="_kUzQSYW">MAB algorithms, known for their scalability, have demonstrated success in real-world epidemic resource allocation <ref type="bibr" target="#b84">[88]</ref> and hospital resource matching <ref type="bibr" target="#b88">[92,</ref><ref type="bibr">122]</ref>.</s><s xml:id="_u7Xwy5p">Another potential direction is the integration of transfer learning <ref type="bibr">[264]</ref> in the RL framework.</s><s xml:id="_mJZ8bNK">This approach allows for the utilization of previously learned HOM knowledge from neural networks to handle future similar tasks more effectively.</s><s xml:id="_dXkzuQK">These findings indicate that RL methods with more flexible adaptability will be promising in HOM.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_VkBrAfP">Robustness</head><p xml:id="_PhSXkxB"><s xml:id="_eU9YvTV">In the context of HOM, where we need to quantify some metrics related to human lives, robustness is always an essential topic.</s><s xml:id="_bKfHKYf">The estimated HOM-related metrics are typically used as inputs into downstream optimization and decision-making <ref type="bibr">[265]</ref>.</s><s xml:id="_8FJPZfW">Due to the uncertainties associated with these estimations, robust optimization <ref type="bibr">[177,</ref><ref type="bibr">266]</ref> can be used to ensure the worst-case performance.</s><s xml:id="_DDnvpDU">However, most RL approaches do not provide theoretical guarantees of the quality of the solutions.</s><s xml:id="_EnABKCQ">To address this, more advanced robust RL methods <ref type="bibr" target="#b69">[74,</ref><ref type="bibr">120,</ref><ref type="bibr">141,</ref><ref type="bibr">187,</ref><ref type="bibr">267,</ref><ref type="bibr">268]</ref> propelled by control theory show great promise.</s><s xml:id="_pewnseH">Safe RL <ref type="bibr">[268]</ref> incorporates constraints in the objective function or exploration process and is considered capable of achieving robustness under uncertainty.</s><s xml:id="_ywsCam9">Another approach is to develop distributionally robust optimization <ref type="bibr">[269]</ref> for MDP and benchmark it with RL methods.</s><s xml:id="_r4AnPXv">Optimization paradigms may also involve constraints (e.g., chance-constrained programming and threshold policies <ref type="bibr">[270,</ref><ref type="bibr">271]</ref>) to enhance the robustness of the solutions.</s><s xml:id="_wHaNtHa">Furthermore, there are combinations of optimization and learning <ref type="bibr">[272]</ref><ref type="bibr">[273]</ref><ref type="bibr">[274]</ref> that accelerate exact combinatorial optimization via RL.</s><s xml:id="_GV3tHx4">In HOM, the need for robustness is consistent with the need for adaptability.</s><s xml:id="_vh7CN7z">It means we need to seek optimal solutions under dynamic and uncertain HOM environments.</s></p><p xml:id="_dXpeQJx"><s xml:id="_Hjxv4Ah">Interpretability Communicating effective decisions to human decision-makers is vital in HOM.</s><s xml:id="_NAdEWv2">However, there is a dilemma between using "black box" neural networks [275] to solve complex systems and achieving good interpretability.</s><s xml:id="_pupHFMM">As a result, some choices, such as MAB algorithms without neural networks, are of greater popularity.</s><s xml:id="_FBp7ZEV">These methods approximate value functions using Bayesian or frequentist approaches, providing a level of interpretability.</s><s xml:id="_ypjjgwt">The prevalence of ADP in microlevel applications also highlights the importance of model-based RL, which allows for a deeper understanding of the underlying environments.</s><s xml:id="_tqQBJ2d">Multiple selected policy explanation approaches in other fields (mostly visual tasks), such as contrasting rollouts [276], determining critical states <ref type="bibr">[277]</ref>, utilizing attention mechanisms <ref type="bibr">[278]</ref>, programmatically interpretable RL [279], explaining through intended outcomes <ref type="bibr">[280]</ref>, and distal explanations with causal lens <ref type="bibr">[281]</ref>, can be extended to HOM.</s><s xml:id="_acQhyeQ">These approaches can be integrated into distillation and mimicking paradigms, as discussed in a comprehensive explainable DRL review <ref type="bibr">[165]</ref>.</s><s xml:id="_TS7frfp">Additionally, post hoc techniques can partially explain and inspect "black box" models in DRL, such as the Shapley Additive Explanations <ref type="bibr">[165,</ref><ref type="bibr">[282]</ref><ref type="bibr">[283]</ref><ref type="bibr">[284]</ref>. Exploring interpretable analysis in DRL will be an interesting and impactful direction for enhancing the practical implementation of decisions in HOM.</s></p><p xml:id="_gAGTw4w"><s xml:id="_na2Z7P9">Validation Validating the optimal results obtained from RL before deployment in HOM can be a challenging task.</s><s xml:id="_SShez93">Designing an effective measurement of rewards and benchmarking them is not straightforward.</s><s xml:id="_mpEYRbe">One approach is to compare the RL results with exact optimization methods and expert policies.</s><s xml:id="_gjXVeEq">Expert policies, which can serve as "supervisors" in imitation learning <ref type="bibr">[101,</ref><ref type="bibr">107]</ref>, can guide and accelerate RL training while also aiding in constructing rewards <ref type="bibr">[285]</ref>.</s><s xml:id="_mYPceXx">In addition, RL performance relies on off-policy evaluation methods <ref type="bibr">[286]</ref> as a means of validation, particularly in critical healthcare applications.</s><s xml:id="_UbR3qYN">Causal inference techniques can be used to validate RL decisions <ref type="bibr" target="#b84">[88,</ref><ref type="bibr">260]</ref>.</s><s xml:id="_7rKJprs">The combination of RL and causal inference in off-policy evaluation has shown great potential <ref type="bibr">[260,</ref><ref type="bibr">287]</ref>.</s><s xml:id="_Y7X7YFq">Validation is also closely related to the interpretability of RL <ref type="bibr">[165]</ref>.</s><s xml:id="_8JemTvb">Explicit and interpretable models, as well as model-based methods, have advantages in validating their results.</s><s xml:id="_SFPC4kc">This is because the optimality gap can be theoretically derived, providing a solid foundation for validating the performance of these methods.</s></p><p xml:id="_E5dr4Tk"><s xml:id="_smzNNFW">RL from human feedback Recently, trendy large language models (LLMs) have highlighted the importance of RLs with human advice <ref type="bibr">[288,</ref><ref type="bibr">289]</ref>.</s><s xml:id="_JwrxPCx">Under the umbrella of human-inthe-loop RL [290], these methods can perform tasks more aligned with human goals by preference-based RL <ref type="bibr">[291]</ref> and achieve effective imitation learning [107] or curriculum learning <ref type="bibr">[111]</ref>.</s><s xml:id="_KgrMRHz">If the data from human advice are of high quality, the training can be efficient even without the need for massive samples <ref type="bibr">[292]</ref>.</s><s xml:id="_t2SPbeV">The interaction between humans and RL can be at different levels depending on who dominates the control of the learning process <ref type="bibr">[293]</ref>.</s><s xml:id="_mfUpg4h">RL from human feedback (RLHF) can influence and be applied to every aspect of HOM.</s><s xml:id="_C3T2e88">The critical states, policies, and rewards of HOM applications can be shaped according to human advice.</s><s xml:id="_Ack8bAy">Critical constraints in HOM summarized by human experts can be integrated into safe RL <ref type="bibr">[268]</ref>.</s><s xml:id="_p2cgar9">The robustness and explorations of RL in HOM can be improved by handling uncertainty and trust regions <ref type="bibr">[294]</ref>.</s><s xml:id="_UnTF6Bc">RLHF can also substantially help promote the interpretability and validation of RL in HOM <ref type="bibr">[165,</ref><ref type="bibr">293]</ref>.</s><s xml:id="_gZ5nj5e">The concept of human-in-theloop and interoperability are tightly coupled with each other.</s><s xml:id="_RstsZ48">With RLHF, humans are able to have greater understanding and control over the generated RL policies.</s><s xml:id="_5Qh7J6N">Therefore, it is a promising direction for better practical deployment of RL policy in HOM.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FvvmqEt">Real-world implementations</head><p xml:id="_wddfYR7"><s xml:id="_aQUFJWe">As we have investigated in Sect.</s><s xml:id="_p9j5umK">5.2, RL's limited successful real-world applications in HOM can be attributed to the challenges abovementioned.</s><s xml:id="_pdFcbjV">Modern RL methods have advantages in advancing complex and large-scale HOM applications.</s><s xml:id="_4Av7CER">While, strong adaptability and robustness are pillars of effective modern RL methods, especially, when tackling emergent practical issues and ensuring the worst-scenario health outcomes.</s><s xml:id="_hkg2bkM">In terms of real-world implementations, interpretability is necessary to explain the optimal policies generated for human stakeholders' understanding.</s><s xml:id="_hwue5MS">Rigorous theory and validation of the methodologies and policies are also essential.</s><s xml:id="_DtvxW9v">Therefore, model-based methods with strong interpretability and theoretical performance guarantees are promising.</s><s xml:id="_faaN4JA">Furthermore, the use of RL in HOM is subject to strict regulatory, ethical, and safety requirements due to the importance of patient health outcomes.</s><s xml:id="_s9kZHPH">RL solutions with more human interactions are expected to make a difference.</s><s xml:id="_N6EUTR6">Only if the challenges of adaptability, robustness, interpretability, and validation are adequately addressed can modern RL methods be implemented in the real world.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_gd6W9TS">Conclusion</head><p xml:id="_v76y2Eh"><s xml:id="_E9GFsuY">RL is an approach that builds upon MDP for sequential decision-making and aims to address the challenges posed by the curse of dimensionality.</s><s xml:id="_jFHHtEH">Our paper begins with a tutorial on RL methodologies, ranging from MDP to ADP and DRL, followed by a comprehensive scoping review.</s><s xml:id="_6xFxxqA">Our review provides a detailed analysis of RL methodologies and their applications in different domains of HOM, which are classified into macrolevel, mesolevel, and microlevel research thrusts.</s><s xml:id="_sruCRVz">We analyze the performance of these RL methodologies in HOM.</s><s xml:id="_rMyQEhX">Given the significant impact of the COVID-19 pandemic on the world in recent years, our paper also provides a better understanding of the applications of RL in HOM and how these approaches can improve preparedness for future emergencies.</s><s xml:id="_ZZdPdk9">For example, RL has already been implemented in large-scale COVID-19 test kit allocation on Greek borders <ref type="bibr" target="#b84">[88]</ref>.</s><s xml:id="_7Ubx4uN">Finally, the paper presents statistics on trends, recent developments, and challenges, providing valuable insights into the current state of the field and potential avenues for future research.</s></p><p xml:id="_t5Hx2T2"><s xml:id="_FHrKKN6">Based on our review, we provide the answer to the research questions in Sect.</s><s xml:id="_CHpfZqs">1:</s></p><p xml:id="_hddg4bn"><s xml:id="_SAr4M2u">1. RL methods show great potential in solving complex HOM problems that involve MDP formulations and high dimensionality.</s><s xml:id="_8wF9U6v">Traditional optimization methods often struggle to find exact solutions for such problems in an acceptable time frame, while simple heuristic approaches may result in suboptimal solutions.</s><s xml:id="_egNCXMv">In this review, RL algorithms have been compared to various benchmarks, including MILP, heuristic methods, and real-world expert policies.</s><s xml:id="_G4FJ3am">The results demonstrate that RL can achieve good performance in terms of both solution effectiveness and computational efficiency.</s><s xml:id="_Uc3EYfC">Although RL training time can be long as problem scales grow, RL has the ability to learn problem-specific features during training and can be transferred to similar situations through transfer learning.</s><s xml:id="_39Bkruk">Additionally, imitation learning can provide a "warm start" for RL training.</s><s xml:id="_FBUCTgR">These characteristics and techniques make RL a suitable approach for tackling complex HOM problems.</s><s xml:id="_Ppr5uJt">2. Our comprehensive investigation of RL methods applied in HOM reveals that ADP and DRL approaches are among the most popular methods.</s><s xml:id="_RA3vyxe">However, the choice of the most suitable and effective RL methods depends on the specific HOM problems at hand.</s><s xml:id="_5CdeaB9">For highly complex HOM models, neural network approximators are expected to be effective in achieving desired outcomes.</s></p><p xml:id="_jNywHAQ"><s xml:id="_Y4Ggu9z">Conversely, when a model has an explicit planning framework, model-based methods can enhance robustness, interpretability, and validation in the face of uncertainty.</s><s xml:id="_2RbfYaE">According to our review, it is challenging to simultaneously achieve highly complex RL with "black box" approximators and model-based RL with strong interpretability and theoretical performance guarantee.</s></p><p xml:id="_85zFGpn"><s xml:id="_GZheUpf">Interpretable RL in HOM is, therefore, one of the most promising future directions.</s><s xml:id="_sJpFMvX">3.</s><s xml:id="_gMCrAqU">In Sect.</s><s xml:id="_6F7bHkk">5, we have discussed the recent developments, challenges, and potential future directions for RL in HOM.</s><s xml:id="_K2gqeqJ">Since the RL's high-dimensional representation can partly address the complexity in HOM applications, it is believed that developing RL for HOM purposes with a focus on developing adaptability, robustness, interpretability, validation, and RLHF holds promise.</s><s xml:id="_TnP8sVz">These five directions will enable better preparation and realworld large-scale solutions for future HOM problems.</s></p><p xml:id="_EZT7qmY"><s xml:id="_wbE7gZc">In conclusion, RL for HOM is an emerging field with significant potential.</s><s xml:id="_8GWcQFJ">The effective integration of RL methodologies and application modeling techniques is crucial for achieving optimal results.</s><s xml:id="_RGGPqau">The synergy between these two phases holds great promise for advancing the field of HOM.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc><div><p xml:id="_AFmjWMX"><s xml:id="_c5nawsa">Fig. 1 Number of publications related to RL for HOM by year</s></p></div></figDesc><graphic coords="23,183.26,56.10,360.72,258.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="24,183.26,56.58,360.72,290.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="25,184.26,56.42,359.52,301.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="25,183.26,454.84,360.72,258.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc><div><p><s xml:id="_6sJwb6U">Summary of applications under the macrolevel research thrustsMethod" refers to the learning algorithm (with approximator).</s><s xml:id="_kUQkWeZ">The approximator in the bracket will be omitted if it is a standard setting of the RL algorithm (e.g., the standard approximator of DQN is DNN) 2. Under macrolevel research thrusts, RL agents generally interact with the epidemic model and use the model outputs as the state 3. COVID-19 studies are excluded from "general measures and strategies"</s></p></div></figDesc><table><row><cell>Study</cell><cell>Year</cell><cell>Method 1</cell><cell>Epidemic model(s) 2 and data</cell></row><row><cell>General measures</cell><cell></cell><cell></cell><cell></cell></row><row><cell>and strategies 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Das et al. [130]</cell><cell>2008</cell><cell>Simulation</cell><cell>N/A</cell></row><row><cell>Yaesoubi et al. [131]</cell><cell>2016</cell><cell>ADP</cell><cell>SIRD</cell></row><row><cell>Shi et al. [132]</cell><cell>2019</cell><cell>Simulation</cell><cell>N/A</cell></row><row><cell>Probert et al. [133]</cell><cell>2019</cell><cell>DQN (CNN)</cell><cell>SEIR</cell></row><row><cell>Liu et al. [134]</cell><cell>2023</cell><cell>QL (GNN)</cell><cell>SIS</cell></row><row><cell>COVID-19 control policies</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Uddin et al. [135]</cell><cell>2020</cell><cell>DQN</cell><cell>N/A</cell></row><row><cell>Arango et al. [136]</cell><cell>2020</cell><cell>DQN</cell><cell>SEIR</cell></row><row><cell>Miralles et al. [137]</cell><cell>2020</cell><cell>DQN</cell><cell>SEIR</cell></row><row><cell>Ohi et al. [119]</cell><cell>2020</cell><cell>DDQN (LSTM)</cell><cell>SEIR</cell></row><row><cell>Khadilkar et al. [138]</cell><cell>2020</cell><cell>DQN</cell><cell>SEIR</cell></row><row><cell>Kompella et al. [139]</cell><cell>2020</cell><cell>AC</cell><cell>SEIR</cell></row><row><cell>Padmanabhan et al. [140]</cell><cell>2021</cell><cell>QL</cell><cell>SEIR</cell></row><row><cell>Rathore et al. [141]</cell><cell>2021</cell><cell>Bayesian</cell><cell>SIR</cell></row><row><cell>Wan et al. [120]</cell><cell>2021</cell><cell>DQN, Monte Carlo, MARL</cell><cell>SEIR, SIR</cell></row><row><cell>Guo et al. [121]</cell><cell>2022</cell><cell>DQN (Transformer)</cell><cell>SEIAR, FluTE</cell></row><row><cell>Bushaj et al. [142]</cell><cell>2022</cell><cell>DQN</cell><cell>Covasim</cell></row><row><cell>Nguyen et al. [143]</cell><cell>2022</cell><cell>PPO</cell><cell>Agent-based simulation</cell></row><row><cell>Yao et al. [144]</cell><cell>2023</cell><cell>DDQN</cell><cell>SEIR, SIR</cell></row><row><cell>Mobility restriction policies</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Song et al. [145]</cell><cell>2020</cell><cell>DDPG (GNN)</cell><cell>SIHR, OD Matrix</cell></row><row><cell>Libin et al. [146]</cell><cell>2021</cell><cell>PPO, MARL</cell><cell>SEIR, Mobility Flux</cell></row><row><cell>Kwak et al. [147]</cell><cell>2021</cell><cell>D3QN</cell><cell>SIRD</cell></row><row><cell>Roy et al. [148]</cell><cell>2021</cell><cell>QL</cell><cell>SEIRD, Zone Mobility</cell></row><row><cell>Zong et al. [149]</cell><cell>2022</cell><cell>AC (RNN, Attention), MARL</cell><cell>SEAIRDL</cell></row><row><cell>Du et al. [150]</cell><cell>2023</cell><cell>Hierarchical PPO</cell><cell>Multilateral-impact-driven SEIR</cell></row><row><cell>Note:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1. "</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc><div><p xml:id="_dMVG8A4"><s xml:id="_uRWjrpx">Summary of applications under resource allocation in epidemics</s></p></div></figDesc><table><row><cell>Study</cell><cell>Year</cell><cell>Method</cell><cell>Application</cell></row><row><cell>Wei et al. [171]</cell><cell>2021</cell><cell>QL, AC</cell><cell>Vaccine allocation</cell></row><row><cell>Hao et al. [172]</cell><cell>2021</cell><cell>QL (CNN), DQN</cell><cell>Vaccine allocation</cell></row><row><cell>Tan et al. [173]</cell><cell>2021</cell><cell>QL, DQN</cell><cell>Vaccine allocation</cell></row><row><cell>Bednarski et al. [174]</cell><cell>2021</cell><cell>QL, Value-based RL</cell><cell>Ventilator redistribution</cell></row><row><cell>Bastani et al. [88]</cell><cell>2021</cell><cell>MAB (Bayes), lasso, gradient boost</cell><cell>Test kits allocation</cell></row><row><cell>Gonsalves et al. [89]</cell><cell>2021</cell><cell>MAB (ICAR)</cell><cell>Testing priority</cell></row><row><cell>Hao et al. [175]</cell><cell>2022</cell><cell>PPO (GNN)</cell><cell>Vaccine allocation</cell></row><row><cell>Shuvo et al. [74]</cell><cell>2022</cell><cell>A2C, Pareto optimality</cell><cell>Hospital expansions</cell></row><row><cell>Xia et al. [176]</cell><cell>2022</cell><cell>DQN</cell><cell>Vaccine and test kits allocation</cell></row><row><cell>Rey et al. [90]</cell><cell>2023</cell><cell>MAB</cell><cell>Vaccine allocation</cell></row><row><cell>Thul et al. [177]</cell><cell>2023</cell><cell>Stochastic optimization</cell><cell>Vaccine and test kits allocation</cell></row><row><cell>Zeng et al. [178]</cell><cell>2023</cell><cell>DQN</cell><cell>Medical supplies allocation</cell></row><row><cell>Zhang et al. [179]</cell><cell>2023</cell><cell>DQN</cell><cell>Vaccine allocation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc><div><p xml:id="_phwYWJv"><s xml:id="_z8q2jPx">Summary of applications under healthcare supply chain management</s></p></div></figDesc><table><row><cell>Study</cell><cell>Year</cell><cell>Method</cell><cell>Application</cell></row><row><cell>Wu et al. [188]</cell><cell>2020</cell><cell>Policy gradient (LSTM)</cell><cell>Mask production scheduling</cell></row><row><cell>Abu Zwaida et al. [189]</cell><cell>2021</cell><cell>DQN (CNN)</cell><cell>Replenishment of medicine</cell></row><row><cell>Asadi et al. [190]</cell><cell>2022</cell><cell>Value iteration</cell><cell>Routing for delivery by drones</cell></row><row><cell>Ahmadi et al. [191]</cell><cell>2022</cell><cell>DQN</cell><cell>Inventory control of perishable medicine</cell></row><row><cell>Abouee-Mehrizi et al. [45]</cell><cell>2023</cell><cell>ADP</cell><cell>Inventory control of platelet</cell></row><row><cell>Zhang et al. [105]</cell><cell>2023</cell><cell>Hierarchical AC</cell><cell>Ventilator production scheduling</cell></row><row><cell>Seranilla and Löhndorf [192]</cell><cell>2023</cell><cell>ADP</cell><cell>Facility location for vaccine distribution</cell></row><row><cell>Wang et al. [66]</cell><cell>2023</cell><cell>DQN</cell><cell>Medical facility location routing</cell></row><row><cell>Tseng et al. [75]</cell><cell>2023</cell><cell>AC</cell><cell>Inventory control of regenerative medicine</cell></row><row><cell>Vanvuchelen et al. [193]</cell><cell>2023</cell><cell>PPO</cell><cell>Inventory control of malaria medicine</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc><div><p xml:id="_M7UeF8Z"><s xml:id="_3WWBhwB">At this level, HOM research problems are analyzed at the individual patient level within a single healthcare institution.</s><s xml:id="_BGEwsmr">Most of the approaches to tackling the problems take into account the specific needs of each patient and provide more detailed plans for healthcare service delivery.</s><s xml:id="_dNFzYeT">The studies under the microlevel research thrusts typically investigate four main categories of healthcare services, including emergency medical services (EMSs), outpatient care, inpatient care, and residential care.</s></p></div></figDesc><table><row><cell>Summary of applications in EMSs</cell><cell>Study</cell><cell>Year</cell><cell>Method</cell><cell>Application</cell></row><row><cell></cell><cell>Maxwell et al. [194]</cell><cell>2010</cell><cell>ADP</cell><cell>Ambulance redeployment</cell></row><row><cell></cell><cell>Schmid [195]</cell><cell>2012</cell><cell>ADP</cell><cell>Ambulance dispatch and relocation</cell></row><row><cell></cell><cell>Maxwell et al. [196]</cell><cell>2013</cell><cell>ADP</cell><cell>Ambulance redeployment</cell></row><row><cell></cell><cell>Nasrollahzadeh et al. [197]</cell><cell>2018</cell><cell>ADP</cell><cell>Ambulance dispatch and relocation</cell></row><row><cell></cell><cell>Allen et al. [198]</cell><cell>2021</cell><cell>Simulation</cell><cell>Ambulance location problem</cell></row><row><cell></cell><cell>Benedetti et al. [199]</cell><cell>2021</cell><cell>DQN</cell><cell>Traffic light timing</cell></row><row><cell></cell><cell>Yu et al. [200]</cell><cell>2021</cell><cell>ADP</cell><cell>Ambulance dispatch</cell></row><row><cell></cell><cell>Gao et al. [201]</cell><cell>2023</cell><cell>ADP (DNN)</cell><cell>Ambulance dispatch and relocation</cell></row><row><cell></cell><cell>Su et al. [102]</cell><cell>2023</cell><cell>A2C (LSTM)</cell><cell>Traffic signal control</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc><div><p xml:id="_d3DFwEE"><s xml:id="_zF7sVzA">Summary of applications in outpatient care</s></p></div></figDesc><table><row><cell>Study</cell><cell>Year</cell><cell>Method</cell><cell>Application</cell></row><row><cell>Patrick et al. [204]</cell><cell>2008</cell><cell>ADP</cell><cell>Diagnostic resource management</cell></row><row><cell>Huang et al. [205]</cell><cell>2011</cell><cell>QL</cell><cell>Diagnostic resource management</cell></row><row><cell>Lin et al. [206]</cell><cell>2011</cell><cell>ADP</cell><cell>Outpatient appointment scheduling</cell></row><row><cell>Schuetz et al. [207]</cell><cell>2012</cell><cell>ADP</cell><cell>Capacity allocation</cell></row><row><cell>Feldman et al. [208]</cell><cell>2014</cell><cell>ADP</cell><cell>Outpatient appointment scheduling</cell></row><row><cell>Astaraky et al. [209]</cell><cell>2015</cell><cell>ADP</cell><cell>Surgery scheduling</cell></row><row><cell>Lee et al. [210]</cell><cell>2015</cell><cell>Boltzmann exploration</cell><cell>Diagnostic resource management</cell></row><row><cell>Lee et al. [91]</cell><cell>2019</cell><cell>MAB</cell><cell>Diagnostic resource management</cell></row><row><cell>Lee et al. [211]</cell><cell>2020</cell><cell>DQN</cell><cell>Diagnostic resource management</cell></row><row><cell>Diamant et al. [212]</cell><cell>2021</cell><cell>ADP</cell><cell>Outpatient appointment scheduling</cell></row><row><cell>Zhang et al. [213]</cell><cell>2021</cell><cell>TD</cell><cell>Surgery scheduling</cell></row><row><cell>Zhalechian et al. [92]</cell><cell>2022</cell><cell>MAB</cell><cell>Diagnostic resource management</cell></row><row><cell>Agrawal et al. [214]</cell><cell>2023</cell><cell>ADP</cell><cell>Surgery scheduling</cell></row><row><cell>Xu et al. [44]</cell><cell>2023</cell><cell>Model-based RL</cell><cell>Surgery scheduling</cell></row><row><cell>D'Aeth et al. [215]</cell><cell>2023</cell><cell>DP (fluid approximation)</cell><cell>Care prioritization</cell></row><row><cell cols="3">to adapt to domain-specific settings in outpatient care [219].</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc><div><p xml:id="_u4qQmmA"><s xml:id="_rWCKqVV">Summary of applications in inpatient care</s></p></div></figDesc><table><row><cell>Study</cell><cell>Year</cell><cell>Method</cell><cell>Application</cell></row><row><cell>Samiedaluie et al. [223]</cell><cell>2017</cell><cell>ADP (queue theory)</cell><cell>Inpatient flow management</cell></row><row><cell>Prasad et al. [224]</cell><cell>2017</cell><cell>QL</cell><cell>Weaning of mechanical ventilation in ICU</cell></row><row><cell>Dai et al. [30]</cell><cell>2019</cell><cell>ADP (fluid control, single-pool approximation)</cell><cell>Inpatient flow management</cell></row><row><cell>Braverman et al. [225]</cell><cell>2020</cell><cell>ADP (Taylor expansion)</cell><cell>Inpatient flow management</cell></row><row><cell>Shuvo et al. [186]</cell><cell>2020</cell><cell>A2C</cell><cell>Hospital capacity expansion</cell></row><row><cell>Shuvo et al. [73]</cell><cell>2021</cell><cell>A2C, decision tree</cell><cell>Hospital capacity expansion</cell></row><row><cell>Kabir et al. [226]</cell><cell>2021</cell><cell>A2C (LSTM)</cell><cell>Hospital capacity expansion</cell></row><row><cell>Heydar et al. [227]</cell><cell>2021</cell><cell>ADP</cell><cell>Inpatient flow management</cell></row><row><cell>Liu et al. [228]</cell><cell>2021</cell><cell>MAB</cell><cell>Inpatient flow management</cell></row><row><cell>Lazebnik [229]</cell><cell>2023</cell><cell>Policy-based RL</cell><cell>Hospital staff scheduling</cell></row><row><cell>Zhalechian et al. [122]</cell><cell>2023</cell><cell>MAB</cell><cell>Inpatient flow management</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p xml:id="_pZbCUBH"><s xml:id="_y2bwMu6">Trends and directionsThrough our scoping review, we have collected statistics to visualize the overall trend of RL applications in HOM.</s><s xml:id="_tQmjMKk">In this section, a critical discussion of the current development that covers the performance of various RL methods for corresponding HOM problems is presented.</s><s xml:id="_Y9PbSbR">Additionally, we address the challenges faced in this field and discuss insightful future directions for RL applications in HOM.</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xml:id="_pnGwVYP"><p xml:id="_xYfPQK3"><s xml:id="_PwvNQEE">Acknowledgements The authors are grateful to <rs type="person">Tinglong Dai</rs> for his insights into HOM and helpful suggestions, and the Editors and Referees for their constructive comments.</s><s xml:id="_8yXASyW">This research is supported by <rs type="funder">Health and Medical Research Fund of Health Bureau of Hong Kong</rs> (Ref: <rs type="grantNumber">21222881</rs>) and <rs type="funder">General Research Fund of Research Grants Council of Hong Kong</rs> (<rs type="grantNumber">Ref: 17204823</rs>).</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aq2N27c">
					<idno type="grant-number">21222881</idno>
				</org>
				<org type="funding" xml:id="_MwgnCP6">
					<idno type="grant-number">Ref: 17204823</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_p8FYQyU">Data Availability</head><p xml:id="_X3wnRrw"><s xml:id="_83CBS6p">The authors confirm that the data supporting the findings of this study are available within the article.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_M7k5cFw">Declarations</head><p xml:id="_hph3VdG"><s xml:id="_367Rp5X">Ethical Approval None required.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BZCYQg3">Conflict of interest</head><p xml:id="_pP6fT79"><s xml:id="_G4f7uRA">The authors report that there is no Conflict of interest to declare.</s></p><p xml:id="_6tPNNbW"><s xml:id="_x3vz7BM">Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.</s><s xml:id="_FYMvtcJ">The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.</s><s xml:id="_9BMDPbc">If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</s><s xml:id="_Y4r4J9q">To view a copy of this licence, visit <ref type="url" target="http://creativecommons.org/licenses/by/4.0/">http://creativecomm  ons.org/licenses/by/4.0/</ref>.</s></p><p xml:id="_vfE72vC"><s xml:id="_WVPHaYT">Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_BWcEsrT">Healthcare operations management</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mclaughlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>AUPHA</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">McLaughlin DB (2008) Healthcare operations management. AUPHA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_39BaQsd">Dynamic programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Bellman RE (2010) Dynamic programming. Princeton University Press</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_xB4J2f3">Artificial intelligence and data mining in healthcare</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masmoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jarboui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siarry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Masmoudi M, Jarboui B, Siarry P (2021) Artificial intelligence and data mining in healthcare. Springer</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_AVRxQ2P">Reinforcement learning in healthcare: a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477600</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DftP6mh">ACM Comput Surv (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu C, Liu J, Nemati S et al (2021) Reinforcement learning in healthcare: a survey. ACM Comput Surv (CSUR) 55(1):1-36</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_HE97qAr">Reinforcement learning for clinical decision support in critical care: comprehensive review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rqbg7vJ">J Med Int Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">477</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu S, See KC, Ngiam KY et al (2020) Reinforcement learning for clinical decision support in critical care: comprehensive review. J Med Int Res 22(7):e18,477</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_q9TYXgP">A pneumonia outbreak associated with a new coronavirus of probable bat origin</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_szBkAms">Nature</title>
		<imprint>
			<biblScope unit="volume">579</biblScope>
			<biblScope unit="issue">7798</biblScope>
			<biblScope unit="page" from="270" to="273" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou P, Yang XL, Wang XG et al (2020) A pneumonia outbreak associated with a new coronavirus of probable bat origin. Nature 579(7798):270-273</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_nGemrtJ">Evaluating the patient boarding during Omicron surge in Hong Kong: time series analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Scl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Ttl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NX3Gfmt">J Med Syst</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu Q, Chan SCl, Lee TTl, et al (2023) Evaluating the patient boarding during Omicron surge in Hong Kong: time series anal- ysis. J Med Syst 47(1):1-10</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_VpWyXAz">Deep reinforcement learning in transportation research: a review</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Farazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ahamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HDBFH4E">Trans Res Interdiscip Perspectives</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">100</biblScope>
			<biblScope unit="page">425</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Farazi NP, Zou B, Ahamed T et al (2021) Deep reinforcement learning in transportation research: a review. Trans Res Interdiscip Perspectives 11(100):425</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_rVXUBFA">Reinforcement learning for logistics and supply chain management: methodologies, state of the art, and future opportunities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8ezyjtw">Trans Res Part E: Logist Trans Rev</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">102</biblScope>
			<biblScope unit="page">712</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yan Y, Chow AH, Ho CP et al (2022) Reinforcement learning for logistics and supply chain management: methodologies, state of the art, and future opportunities. Trans Res Part E: Logist Trans Rev 162(102):712</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_qZXdhJe">Markov decision processes: discrete stochastic dynamic programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hTPMCgv">Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3731</biblScope>
			<biblScope unit="page" from="34" to="37" />
			<date type="published" when="1966">2014. 1966</date>
		</imprint>
	</monogr>
	<note>Dynamic programming Wiley</note>
	<note type="raw_reference">Puterman ML (2014) Markov decision processes: discrete stochastic dynamic programming. Wiley 11. Bellman R (1966) Dynamic programming. Science 153(3731):34-37</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_7aKWXqD">Learning from delayed rewards. King&apos;s College</title>
		<author>
			<persName><forename type="first">Cjch</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<pubPlace>Cambridge United Kingdom</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Watkins CJCH (1989) Learning from delayed rewards. King&apos;s College, Cambridge United Kingdom</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_99j8BY2">Sequential decision problems and neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TTJjW9t">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Barto AG, Sutton RS, Watkins C (1989) Sequential decision prob- lems and neural networks. Adv Neural Inform Process Syst 2</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_xR8W8GD">Reinforcement learning for sequential decision and optimal control</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-19-7784-8_12</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Li SE (2023) Reinforcement learning for sequential decision and optimal control. Springer</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_zWnrqP6">Integrated architectures for learning, planning, and reacting based on approximating dynamic programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zcyjy88">Machine learning proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990">1990. 1990</date>
			<biblScope unit="page" from="216" to="224" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton RS (1990) Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In: Machine learning proceedings 1990. Elsevier, p 216-224</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main" xml:id="_Xk8Eesn">Approximate dynamic programming: solving the curses of dimensionality</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<biblScope unit="volume">703</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Powell WB (2007) Approximate dynamic programming: solving the curses of dimensionality, vol 703. Wiley</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_sdJXjSH">Reinforcement learning and adaptive dynamic programming for feedback control</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mVBpRUY">IEEE Circuits Syst Mag</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="32" to="50" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lewis FL, Vrabie D (2009) Reinforcement learning and adaptive dynamic programming for feedback control. IEEE Circuits Syst Mag 9(3):32-50</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_MZDExMc">Neuro-dynamic programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Athena Scientific</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Bertsekas D, Tsitsiklis JN (1996) Neuro-dynamic programming. Athena Scientific</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_TAP4MUu">Comparison of heuristic dynamic programming and dual heuristic programming adaptive critics for neurocontrol of a turbogenerator</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnn.2002.1000146</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7ZAUjPc">IEEE Trans Neural Networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="764" to="773" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Venayagamoorthy GK, Harley RG, Wunsch DC (2002) Com- parison of heuristic dynamic programming and dual heuristic programming adaptive critics for neurocontrol of a turbogener- ator. IEEE Trans Neural Networks 13(3):764-773</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_kqJedVU">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_c58KR9U">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Watkins CJ, Dayan P (1992) Q-learning. Mach Learn 8:279-292</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_3Y8eV4g">Reinforcement learning: an introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton RS, Barto AG (2018) Reinforcement learning: an intro- duction. MIT Press</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main" xml:id="_kB5vXB3">Learning machines: a unified view</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Andreae</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
		<respStmt>
			<orgName>Standard Telecommunications Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Andreae JH (1966) Learning machines: a unified view. Standard Telecommunications Laboratories</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_TDsaARQ">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HqPbk6Y">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton RS (1988) Learning to predict by the methods of temporal differences. Mach Learn 3:9-44</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_wtRwKMA">Convergent temporal-difference learning with arbitrary smooth function approximation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YWKGwcZ">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maei H, Szepesvari C, Bhatnagar S et al (2009) Conver- gent temporal-difference learning with arbitrary smooth function approximation. Adv Neural Inform Process Syst 22</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_gT7m6jZ">Rainbow: combining improvements in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v32i1.11796</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cGHMc6t">Proceedings of the AAAI conference on artificial intelligence 26</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</editor>
		<meeting>the AAAI conference on artificial intelligence 26</meeting>
		<imprint>
			<publisher>Athena Scientific</publisher>
			<date type="published" when="2012">2018. 2012</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Dynamic programming and optimal control: volume I</note>
	<note type="raw_reference">Hessel M, Modayil J, Van Hasselt H et al (2018) Rainbow: combining improvements in deep reinforcement learning. In: Pro- ceedings of the AAAI conference on artificial intelligence 26. Bertsekas D (2012) Dynamic programming and optimal control: volume I, vol 1. Athena Scientific</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_czBYDUB">Deep reinforcement learning for inventory control: a roadmap</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Boute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gijsbrechts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Jaarsveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.ejor.2021.07.016</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NptXSxF">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="401" to="412" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Boute RN, Gijsbrechts J, Van Jaarsveld W et al (2022) Deep rein- forcement learning for inventory control: a roadmap. Eur J Oper Res 298(2):401-412</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<ptr target="https://www.davidsilver.uk/teaching/" />
	</analytic>
	<monogr>
		<title level="s" xml:id="_pYhHScH">Lectures on reinforcement learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Silver D (2015) Lectures on reinforcement learning. https://www. davidsilver.uk/teaching/</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_mA9Ppwr">Models and insights for hospital inpatient operations: time-dependent ED boarding time</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vwZEKzw">Manage Sci</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shi P, Chou MC, Dai JG et al (2016) Models and insights for hospital inpatient operations: time-dependent ED boarding time. Manage Sci 62(1):1-28</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_T83VGpa">Inpatient overflow: an approximate dynamic programming approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1287/msom.2018.0730</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_trQsXth">Manuf Serv Oper Manag</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="894" to="911" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dai JG, Shi P (2019) Inpatient overflow: an approximate dynamic programming approach. Manuf Serv Oper Manag 21(4):894-911</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_3CKdVEA">Reinforcement learning and stochastic optimization: a unified framework for sequential decisions</title>
		<author>
			<persName><forename type="first">Wb ;</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781119815068</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gTHQu7f">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="237" to="285" />
			<date type="published" when="1996">2022. 1996</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>Reinforcement learning: a survey</note>
	<note type="raw_reference">Powell WB (2022) Reinforcement learning and stochastic opti- mization: a unified framework for sequential decisions. Wiley 32. Kaelbling LP, Littman ML, Moore AW (1996) Reinforcement learning: a survey. J Artif Intell Res 4:237-285</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_crX9Sux">Reinforcement learning: a tutorial survey and recent advances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gosavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sbYTdpu">INFORMS J Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="192" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gosavi A (2009) Reinforcement learning: a tutorial survey and recent advances. INFORMS J Comput 21(2):178-192</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_yJF9rAg">Deep reinforcement learning: a brief survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Arulkumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9w7HYuV">IEEE Signal Process Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="26" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arulkumaran K, Deisenroth MP, Brundage M et al (2017) Deep reinforcement learning: a brief survey. IEEE Signal Process Mag 34(6):26-38</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main" xml:id="_rkm2Xz5">Offline reinforcement learning: tutorial, review, and perspectives on open problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01643</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Levine S, Kumar A, Tucker G et al (2020) Offline reinforcement learning: tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_VSygxsX">Reinforcement learning approach for resource allocation in humanitarian logistics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2Rx6ddj">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">114</biblScope>
			<biblScope unit="page">663</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu L, Zhang C, Jiang J et al (2021) Reinforcement learning approach for resource allocation in humanitarian logistics. Expert Syst Appl 173(114):663</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_2G2KJW4">Deep learning, reinforcement learning, and world models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2022.03.037</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yPKEqx2">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="267" to="275" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matsuo Y, LeCun Y, Sahani M et al (2022) Deep learning, rein- forcement learning, and world models. Neural Netw 152:267-275</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_espZ5dX">A bibliometric analysis and review on reinforcement learning for transportation applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Tf6uqsJ">Transportmetrica B: Transport Dynamics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">461</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li C, Bai L, Yao L et al (2023) A bibliometric analysis and review on reinforcement learning for transportation applications. Trans- portmetrica B: Transport Dynamics 11(1):2179,461</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_mhcV5uE">Bias in reinforcement learning: a review in healthcare applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khojandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_n6XraKP">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Smith B, Khojandi A, Vasudevan R (2023) Bias in reinforcement learning: a review in healthcare applications. ACM Comput Surv 56(2):1-17</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_QRN7UGz">Handbook of learning and approximate dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kWHuVzd">Wiley</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Si J, Barto AG, Powell WB et al (2004) Handbook of learning and approximate dynamic programming, vol 2. Wiley 41</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main" xml:id="_u8Anfe5">Reinforcement learning and optimal control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.rico.2022.100121</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Athena Scientific</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Bertsekas D (2019) Reinforcement learning and optimal control. Athena Scientific</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_AWNsXdW">Steps toward artificial intelligence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Minsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ttNn3Hn">Proc IRE</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="30" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Minsky M (1961) Steps toward artificial intelligence. Proc IRE 49(1):8-30</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main" xml:id="_bhyZnDg">Approximate dynamic programming: health care applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nasrollahzadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Clemson University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">Nasrollahzadeh AA (2019) Approximate dynamic programming: health care applications. PhD thesis, Clemson University</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_bAntkqv">A reinforcement learningbased optimal control approach for managing an elective surgery backlog after pandemic disruption</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Chou</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10729-023-09636-5</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nVnZV38">Health Care Management Sci</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu H, Fang Y, Chou CA et al (2023) A reinforcement learning- based optimal control approach for managing an elective surgery backlog after pandemic disruption. Health Care Management Sci- ence pp 1-17</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main" xml:id="_XdGgkUg">Platelet inventory management with approximate dynamic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abouee-Mehrizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarhangian</surname></persName>
		</author>
		<idno type="DOI">10.1287/ijoc.2023.0245</idno>
		<idno type="arXiv">arXiv:2307.09395</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Abouee-Mehrizi H, Mirjalili M, Sarhangian V (2023) Platelet inventory management with approximate dynamic programming. arXiv preprint arXiv:2307.09395</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main" xml:id="_NKj2hrs">On-line Q-learning using connectionist systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">37</biblScope>
			<pubPlace>UK</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Rummery GA, Niranjan M (1994) On-line Q-learning using con- nectionist systems, vol 37. University of Cambridge, Department of Engineering Cambridge, UK</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_vh8tK7Q">Path selection in disaster response management based on Q-learning</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VVugswQ">Int J Autom Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="106" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Su ZP, Jiang JG, Liang CY et al (2011) Path selection in disaster response management based on Q-learning. Int J Autom Comput 8(1):100-106</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_8Hc9tCz">Adaptive multi-agent relief assessment and emergency response</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Edrisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_42Av5Ct">Int J Disaster Risk Reduction</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nadi A, Edrisi A (2017) Adaptive multi-agent relief assessment and emergency response. Int J Disaster Risk Reduction 24:12- 23</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_7FrReQZ">Coordinating disaster emergency response with heuristic reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/asonam49781.2020.9381416</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7snanMz">IEEE/ACM international conference on advances in social networks analysis and mining (ASONAM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="565" to="572" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yang Z, Nguyen L, Zhu J et al (2020) Coordinating disaster emer- gency response with heuristic reinforcement learning. In: 2020 IEEE/ACM international conference on advances in social net- works analysis and mining (ASONAM), IEEE, pp 565-572</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_AhSh4CW">A dynamic task assignment model for aviation emergency rescue based on multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cxBj2cA">J Saf Sci Resilience</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">2023. 1986. 6088</date>
		</imprint>
	</monogr>
	<note>Learning representations by back-propagating errors Nature</note>
	<note type="raw_reference">Shen Y, Wang X, Wang H et al (2023) A dynamic task assign- ment model for aviation emergency rescue based on multi-agent reinforcement learning. J Saf Sci Resilience 51. Rumelhart DE, Hinton GE, Williams RJ (1986) Learning rep- resentations by back-propagating errors. Nature 323(6088):533- 536</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_YZPxKx3">Recent advances in convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZbMWCcg">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="354" to="377" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gu J, Wang Z, Kuen J et al (2018) Recent advances in convolu- tional neural networks. Pattern Recogn 77:354-377</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_kPZ4S8M">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9Tp3FH7">IEEE Trans Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schuster M, Paliwal KK (1997) Bidirectional recurrent neural networks. IEEE Trans Signal Process 45(11):2673-2681</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_ktGk4AS">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GtqpAG3">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436-444</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_MQpDC2C">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_86AWNJv">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735-1780</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_NCTr9YB">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnn.2008.2005605</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DPNKreJ">IEEE Trans Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Scarselli F, Gori M, Tsoi AC et al (2008) The graph neural network model. IEEE Trans Neural Networks 20(1):61-80</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_z75eQ3f">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerjcs.1946/fig-10</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dKjRAeu">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vaswani A, Shazeer N, Parmar N et al (2017) Attention is all you need. Adv Neural Inform Process Syst 30</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main" xml:id="_C5xtNxZ">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Mnih V, Kavukcuoglu K, Silver D et al (2013) Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_pTs6Dtf">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dEz4y6b">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mnih V, Kavukcuoglu K, Silver D et al (2015) Human-level con- trol through deep reinforcement learning. Nature 518(7540):529- 533</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_ggDh9XU">Deep reinforcement learning with double Q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zFkcbfr">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Van Hasselt H, Guez A, Silver D (2016) Deep reinforcement learning with double Q-learning. In: Proceedings of the AAAI conference on artificial intelligence</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_Y7Z7RcF">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_R8ENDZc">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wang Z, Schaul T, Hessel M et al (2016) Dueling network architectures for deep reinforcement learning. In: International conference on machine learning, PMLR, pp 1995-2003</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_s9tDNWQ">Deep reinforcement learning for optimal critical care pain management with morphine using dueling double-deep q networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eschenfeldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ostvar</surname></persName>
		</author>
		<idno type="DOI">10.1109/embc.2019.8857295</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VC96p7z">2019 41st Annual international conference of the IEEE engineering in medicine and biology society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3960" to="3963" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lopez-Martinez D, Eschenfeldt P, Ostvar S et al (2019) Deep reinforcement learning for optimal critical care pain management with morphine using dueling double-deep q networks. In: 2019 41st Annual international conference of the IEEE engineering in medicine and biology society (EMBC), IEEE, pp 3960-3963</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_KEPy2VU">End-to-end autonomous driving through dueling double deep q-network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zXCxjSy">Automot Innov</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="328" to="337" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peng B, Sun Q, Li SE et al (2021) End-to-end autonomous driving through dueling double deep q-network. Automot Innov 4:328- 337</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_t8f7H3s">Designing van-based mobile battery swapping and rebalancing services for dockless ebikesharing systems based on the dueling double deep q-network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.trc.2022.103620</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AkfVqkz">Trans Res Part C: Emerging Technol</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">103</biblScope>
			<biblScope unit="page">620</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu M, Di Y, Zhu Z et al (2022) Designing van-based mobile battery swapping and rebalancing services for dockless ebike- sharing systems based on the dueling double deep q-network. Trans Res Part C: Emerging Technol 138(103):620</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_BXEY2D8">A unified framework for differentiated services in intelligent healthcare systems</title>
		<author>
			<persName><forename type="first">Al-Abbasi</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Samara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3Hdk55S">IEEE Trans Netw Sci Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="622" to="633" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Al-Abbasi AO, Samara L, Salem S et al (2021) A unified frame- work for differentiated services in intelligent healthcare systems. IEEE Trans Netw Sci Eng 9(2):622-633</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_Cy3dFEG">A new approach for solving location routing problems with deep reinforcement learning of emergency medical facility</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3615884.3629429</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jARRR2U">Proceedings of the 8th ACM SIGSPATIAL international workshop on security response using GIS</title>
		<meeting>the 8th ACM SIGSPATIAL international workshop on security response using GIS</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="50" to="53" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wang S, Zhou J, Liang H et al (2023) A new approach for solv- ing location routing problems with deep reinforcement learning of emergency medical facility. In: Proceedings of the 8th ACM SIGSPATIAL international workshop on security response using GIS, pp 50-53</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_FhWr978">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sh7REPY">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton RS, McAllester D, Singh S et al (1999) Policy gradient methods for reinforcement learning with function approximation. Adv Neural Inform Process Syst 12</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_UgcEBqg">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4615-3618-5_2</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uxadAyj">Reinforcement Learn</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
	<note type="raw_reference">Williams RJ (1992) Simple statistical gradient-following algo- rithms for connectionist reinforcement learning. Reinforcement Learn pp 5-32</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_z4uYj9X">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<idno type="DOI">10.1137/s0363012901385691</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_unHgcS8">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Konda V, Tsitsiklis J (1999) Actor-critic algorithms. Adv Neural Inform Process Syst 12</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_J3chj3e">Onactor-critic algorithms</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<idno type="DOI">10.1137/s0363012901385691</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ExvWKdB">SIAM J Control Optim</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1143" to="1166" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Konda VR, Tsitsiklis JN (2003) Onactor-critic algorithms. SIAM J Control Optim 42(4):1143-1166</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_abbq9JS">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZYNJjGp">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mnih V, Badia AP, Mirza M et al (2016) Asynchronous methods for deep reinforcement learning. In: International conference on machine learning, PMLR, pp 1928-1937</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main" xml:id="_NWv4YCc">Reinforcement learning through asynchronous advantage actor-critic on a GPU</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06256</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Babaeizadeh M, Frosio I, Tyree S et al (2016) Reinforcement learning through asynchronous advantage actor-critic on a GPU. arXiv preprint arXiv:1611.06256</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_hNp4zjs">Deep reinforcement learning based cost-benefit analysis for hospital capacity planning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shuvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Symum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZUQhW4M">2021 International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shuvo SS, Ahmed MR, Symum H et al (2021) Deep reinforce- ment learning based cost-benefit analysis for hospital capacity planning. In: 2021 International joint conference on neural net- works (IJCNN), IEEE, pp 1-7</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_kkuDxgu">Multi-objective reinforcement learning based healthcare expansion planning considering pandemic events</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shuvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Symum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ua4FW8j">IEEE J Biomed Health Inform</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shuvo SS, Symum H, Ahmed MR et al (2022) Multi-objective reinforcement learning based healthcare expansion planning con- sidering pandemic events. IEEE J Biomed Health Inform</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_KFdW6Cd">Deep reinforcement learning approach for dynamic capacity planning in decentralised regenerative medicine supply chains</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kCpAUYr">Int J Prod Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tseng CY, Li J, Lin LH et al (2023) Deep reinforcement learning approach for dynamic capacity planning in decentralised regen- erative medicine supply chains. Int J Prod Res pp 1-16</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main" xml:id="_dzpFvmm">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Schulman J, Wolski F, Dhariwal P et al (2017) Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">Q. Wu et al.</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_czSshYm">Benchmarking deep reinforcement learning for continuous control</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZZgUuzE">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1329" to="1338" />
		</imprint>
	</monogr>
	<note type="raw_reference">Duan Y, Chen X, Houthooft R et al (2016) Benchmarking deep reinforcement learning for continuous control. In: International conference on machine learning, PMLR, pp 1329-1338</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main" xml:id="_WKxPsrW">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Schulman J, Moritz P, Levine S et al (2015) High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sriram</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02286</idno>
		<title level="m" xml:id="_SDhqMDp">Emergence of locomotion behaviours in rich environments</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Heess N, Tb D, Sriram S et al (2017) Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main" xml:id="_seeuPJN">Deterministic policy gradient algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nMvmw4U">International conference on machine learning</title>
		<imprint>
			<publisher>Pmlr</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
	<note type="raw_reference">Silver D, Lever G, Heess N et al (2014) Deterministic policy gradi- ent algorithms. In: International conference on machine learning, Pmlr, pp 387-395</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main" xml:id="_FKexMyX">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Lillicrap TP, Hunt JJ, Pritzel A et al (2015) Continu- ous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_yxEGfzB">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JQWF9vt">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Auer P, Cesa-Bianchi N, Fischer P (2002) Finite-time analysis of the multiarmed bandit problem. Mach Learn 47:235-256</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_8hjJfht">The multi-armed bandit problem: decomposition and computation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Katehakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Veinott</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SEQ8cuK">Math Oper Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="268" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Katehakis MN, Veinott AF Jr (1987) The multi-armed ban- dit problem: decomposition and computation. Math Oper Res 12(2):262-268</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main" xml:id="_JJNhyYk">Survey on applications of multi-armed and contextual bandits</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MZXP9kb">IEEE Congress on evolutionary computation (CEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">Bouneffouf D, Rish I, Aggarwal C (2020) Survey on applications of multi-armed and contextual bandits. In: 2020 IEEE Congress on evolutionary computation (CEC), IEEE, pp 1-8</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main" xml:id="_UuYj7Xr">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hwfC27P">Proceedings of the 19th international conference on world wide web</title>
		<meeting>the 19th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">Li L, Chu W, Langford J et al (2010) A contextual-bandit approach to personalized news article recommendation. In: Proceedings of the 19th international conference on world wide web, pp 661-670</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_DBjZHDC">An empirical evaluation of Thompson sampling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zgWj3FY">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chapelle O, Li L (2011) An empirical evaluation of Thompson sampling. Advances in Neural Information Processing Systems 24</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_dcuFbTd">Bandit processes and dynamic allocation indices</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CSSYwPZ">J Roy Stat Soc: Ser B (Methodol)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="148" to="164" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gittins JC (1979) Bandit processes and dynamic allocation indices. J Roy Stat Soc: Ser B (Methodol) 41(2):148-164</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main" xml:id="_bNVaqC8">Efficient and targeted COVID-19 border testing via reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Drakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WTVkTHn">Nature</title>
		<imprint>
			<biblScope unit="volume">599</biblScope>
			<biblScope unit="issue">7883</biblScope>
			<biblScope unit="page" from="108" to="113" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bastani H, Drakopoulos K, Gupta V et al (2021) Efficient and targeted COVID-19 border testing via reinforcement learning. Nature 599(7883):108-113</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main" xml:id="_7jNeFUM">Maximizing the efficiency of active case finding for SARS-CoV-2 using bandit algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Gonsalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Copple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Paltiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XVdjTxg">Med Decis Making</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="970" to="977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gonsalves GS, Copple JT, Paltiel AD et al (2021) Maximizing the efficiency of active case finding for SARS-CoV-2 using bandit algorithms. Med Decis Making 41(8):970-977</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main" xml:id="_dudz3wG">Vaccine allocation policy optimization and budget sharing mechanism using reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Hammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saberi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fJhJW2m">Omega</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">102</biblScope>
			<biblScope unit="page">783</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rey D, Hammad AW, Saberi M (2023) Vaccine allocation policy optimization and budget sharing mechanism using reinforcement learning. Omega 115(102):783</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main" xml:id="_RvTsDJT">Optimal screening for hepatocellular carcinoma: a restless bandit model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lavieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Volk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Hn9YWQJ">Manuf Serv Oper Manag</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="198" to="212" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee E, Lavieri MS, Volk M (2019) Optimal screening for hepa- tocellular carcinoma: a restless bandit model. Manuf Serv Oper Manag 21(1):198-212</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main" xml:id="_HBpsU3e">Online resource allocation with personalized learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhalechian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keyvanshokooh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hVGHMcD">Oper Res</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2138" to="2161" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhalechian M, Keyvanshokooh E, Shi C et al (2022) Online resource allocation with personalized learning. Oper Res 70(4):2138-2161</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main" xml:id="_CfhvP7T">Bayesian reinforcement learning: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_c76j8Yw">Found Trends® Mach Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="359" to="483" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ghavamzadeh M, Mannor S, Pineau J et al (2015) Bayesian reinforcement learning: a survey. Found Trends® Mach Learn 8(5-6):359-483</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main" xml:id="_bNvzGaG">Bayesian Q-learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dearden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_W4UA7fu">Am Assoc Artif Intell (AAAI/IAAI)</title>
		<imprint>
			<biblScope unit="page" from="761" to="768" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dearden R, Friedman N, Russell S (1998) Bayesian Q-learning. Am Assoc Artif Intell (AAAI/IAAI) 1998:761-768</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main" xml:id="_hMUyZBJ">Bayesian reinforcement learning in factored pomdps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Oliehoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05612</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Katt S, Oliehoek F, Amato C (2018) Bayesian reinforcement learning in factored pomdps. arXiv preprint arXiv:1811.05612</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main" xml:id="_M8Q8kwN">Reinforcement learning with Gaussian processes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meir</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PPBPtYG">Proceedings of the 22nd international conference on machine learning</title>
		<meeting>the 22nd international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
	<note type="raw_reference">Engel Y, Mannor S, Meir R (2005) Reinforcement learning with Gaussian processes. In: Proceedings of the 22nd international con- ference on machine learning, pp 201-208</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main" xml:id="_u8tEBqN">Multi-agent reinforcement learning: a selective overview of theories and algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Başar</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60990-0_12</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Cd92bRW">Handbook of Reinforcement Learning and Control</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="321" to="384" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang K, Yang Z, Başar T (2021) Multi-agent reinforcement learning: a selective overview of theories and algorithms. Hand- book of Reinforcement Learning and Control pp 321-384</note>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main" xml:id="_N7u5aKh">Rollout, policy iteration, and distributed reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<idno type="DOI">10.1109/jas.2021.1003814</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Athena Scientific</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Bertsekas D (2021) Rollout, policy iteration, and distributed rein- forcement learning. Athena Scientific</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main" xml:id="_zEhnKVb">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FE8pdbk">Proceedings of the tenth international conference on machine learning</title>
		<meeting>the tenth international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tan M (1993) Multi-agent reinforcement learning: Independent vs. cooperative agents. In: Proceedings of the tenth international conference on machine learning, pp 330-337 100.</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main" xml:id="_S8n7jpc">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8zfwZd6">Machine learning proceedings</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1994">1994. 1994</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
	<note type="raw_reference">Littman ML (1994) Markov games as a framework for multi-agent reinforcement learning. In: Machine learning proceedings 1994. Elsevier, p 157-163 101.</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main" xml:id="_FKwtkjQ">Multi-agent reinforcement learning algorithm to solve a partially-observable multi-agent problem in disaster response</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ztvkKMc">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="296" to="308" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee HR, Lee T (2021) Multi-agent reinforcement learning algo- rithm to solve a partially-observable multi-agent problem in disaster response. Eur J Oper Res 291(1):296-308 102.</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main" xml:id="_PhfSphN">EMVLight: a multiagent reinforcement learning framework for an emergency vehicle decentralized routing and traffic signal control system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_v25ZJwj">Trans Res Part C: Emerg Technol</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">103</biblScope>
			<biblScope unit="page" from="955" to="103" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Su H, Zhong YD, Chow JY et al (2023) EMVLight: a multi- agent reinforcement learning framework for an emergency vehicle decentralized routing and traffic signal control system. Trans Res Part C: Emerg Technol 146(103):955 103.</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main" xml:id="_eAhBsps">Recent advances in hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WDgdkbJ">Discrete Event Dyn Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="41" to="77" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Barto AG, Mahadevan S (2003) Recent advances in hierarchical reinforcement learning. Discrete Event Dyn Syst 13(1-2):41-77 104.</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main" xml:id="_gQSmvKW">Hierarchical reinforcement learning by discovering intrinsic options</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06521105</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zhang J, Yu H, Xu W (2021) Hierarchical reinforcement learning by discovering intrinsic options. arXiv preprint arXiv:2101.06521 105.</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main" xml:id="_utNqVmk">A digital twin-driven flexible scheduling method in a human-machine collaborative workshop based on hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Thj86Wr">Flex Serv Manuf J</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang R, Lv J, Bao J et al (2023) A digital twin-driven flexible scheduling method in a human-machine collaborative workshop based on hierarchical reinforcement learning. Flex Serv Manuf J pp 1-23 106.</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main" xml:id="_4AbShMa">Latent space policies for hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra.2018.8460756</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7z7TrEE">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
	<note type="raw_reference">Haarnoja T, Hartikainen K, Abbeel P et al (2018) Latent space policies for hierarchical reinforcement learning. In: International conference on machine learning, PMLR, pp 1851-1860 107.</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main" xml:id="_pDXzmCF">Imitation learning: a survey of learning methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_su2Bzhx">ACM Comput Surv (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hussein A, Gaber MM, Elyan E et al (2017) Imitation learning: a survey of learning methods. ACM Comput Surv (CSUR) 50(2):1- 35 108.</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main" xml:id="_JDBrtsK">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_aQhFNMg">ICML</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="2" to="109" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ng AY, Russell S et al (2000) Algorithms for inverse reinforce- ment learning. In: ICML, p 2 109.</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main" xml:id="_wGQmvpR">A framework for behavioural cloning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sammut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8ZGaxnn">Mach Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="103" to="129" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bain M, Sammut C (1995) A framework for behavioural cloning. Mach Intell 15:103-129 110.</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main" xml:id="_KQPeBt3">Alvinn: an autonomous land vehicle in a neural network</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3FddVdH">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pomerleau DA (1988) Alvinn: an autonomous land vehicle in a neural network. Adv Neural Inform Process Syst 1 111.</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main" xml:id="_4ZEjAY9">Student-teacher curriculum learning via reinforcement learning: predicting hospital inpatient admission location</title>
		<author>
			<persName><forename type="first">R</forename><surname>El-Bouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Watkinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_myANmEg">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2848" to="2857" />
		</imprint>
	</monogr>
	<note type="raw_reference">El-Bouri R, Eyre D, Watkinson P et al (2020) Student-teacher curriculum learning via reinforcement learning: predicting hospi- tal inpatient admission location. In: International conference on machine learning, PMLR, pp 2848-2857 112.</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main" xml:id="_KxW8qyn">Designing a hybrid reinforcement learning based algorithm with application in prediction of the COVID-19 pandemic in Quebec</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khalilpourazari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hashemi</forename><surname>Doulabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AZxSxzp">Annal Oper Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Khalilpourazari S, Hashemi Doulabi H (2021) Designing a hybrid reinforcement learning based algorithm with application in pre- diction of the COVID-19 pandemic in Quebec. Annal Oper Res pp 1-45 113.</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main" xml:id="_65sBFaS">A data-driven hybrid ensemble AI model for COVID-19 infection forecast using multiple neural networks and reinforced learning</title>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zDUbCYP">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">146</biblScope>
			<biblScope unit="issue">105</biblScope>
			<biblScope unit="page" from="560" to="114" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jin W, Dong S, Yu C et al (2022) A data-driven hybrid ensemble AI model for COVID-19 infection forecast using multiple neural net- works and reinforced learning. Comput Biol Med 146(105):560 114.</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main" xml:id="_Pa563uB">Dynamic resource allocation during natural disasters using multi-agent environment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vereshchaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NQyMj6e">Social, cultural, and behavioral modeling: 12th international conference, SBP-BRiMS 2019</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-07-09">2019. July 9-12, 2019</date>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
	<note>Proceedings 12</note>
	<note type="raw_reference">Vereshchaka A, Dong W (2019) Dynamic resource allocation dur- ing natural disasters using multi-agent environment. In: Social, cultural, and behavioral modeling: 12th international conference, SBP-BRiMS 2019, Washington, DC, USA, July 9-12, 2019, Pro- ceedings 12, Springer, pp 123-132 115.</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main" xml:id="_AJYwufq">A review of populationbased meta-heuristic algorithms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Beheshti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smh</forename><surname>Shamsuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_waZETj7">Int J Adv Soft Comput Appl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beheshti Z, Shamsuddin SMH (2013) A review of population- based meta-heuristic algorithms. Int J Adv Soft Comput Appl 5(1):1-35 116.</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main" xml:id="_aVEMKQ7">Reinforcement learning for combinatorial optimization: a survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mazyavkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sviridov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ivanov</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cor.2021.105400</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gb4BwZE">Comput Oper Res</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">105</biblScope>
			<biblScope unit="page" from="400" to="117" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mazyavkina N, Sviridov S, Ivanov S et al (2021) Reinforcement learning for combinatorial optimization: a survey. Comput Oper Res 134(105):400 117.</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main" xml:id="_nd6AZwv">Deep reinforcement learning for crowdsourced urban delivery</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Farazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_K9VKhXN">Trans Res Part B: Methodol</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="227" to="257" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahamed T, Zou B, Farazi NP et al (2021) Deep reinforcement learning for crowdsourced urban delivery. Trans Res Part B: Methodol 152:227-257 118.</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main" xml:id="_P9Bf2Hr">Emergency medical supplies scheduling during public health emergencies: algorithm design based on AI techniques</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZS9ZAa4">Int J Prod Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xia H, Sun Z, Wang Y et al (2023) Emergency medical supplies scheduling during public health emergencies: algorithm design based on AI techniques. Int J Prod Res pp 1-23 119.</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main" xml:id="_JJcWZew">Exploring optimal control of epidemic spread using reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Q</forename><surname>Ohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mridha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Monowar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dyHbPK2">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="120" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ohi AQ, Mridha M, Monowar MM et al (2020) Exploring optimal control of epidemic spread using reinforcement learning. Sci Rep 10(1):22,106 120.</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main" xml:id="_GkDXABr">Multi-objective model-based reinforcement learning for infectious disease control</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_B3pvAvh">Proceedings of the 27th ACM SIGKDD conference on knowledge discovery data mining</title>
		<meeting>the 27th ACM SIGKDD conference on knowledge discovery data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1634" to="1644" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wan R, Zhang X, Song R (2021) Multi-objective model-based reinforcement learning for infectious disease control. In: Pro- ceedings of the 27th ACM SIGKDD conference on knowledge discovery data mining, pp 1634-1644 121.</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main" xml:id="_mDCjWNY">PaCAR: COVID-19 pandemic control decision making via large-scale agent-based modeling and deep reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EKD5uE7">Med Decis Making</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1064" to="1077" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Guo X, Chen P, Liang S et al (2022) PaCAR: COVID-19 pandemic control decision making via large-scale agent-based modeling and deep reinforcement learning. Med Decis Making 42(8):1064- 1077 122.</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main" xml:id="_3Cx3QUw">Datadriven hospital admission control: a learning approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhalechian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keyvanshokooh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1287/opre.2020.0481</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Y6yKGqw">Oper Res</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2111" to="2129" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhalechian M, Keyvanshokooh E, Shi C et al (2023) Data- driven hospital admission control: a learning approach. Oper Res 71(6):2111-2129 123.</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main" xml:id="_Nu7qBkK">Systematic review or scoping review? guidance for authors when choosing between a systematic or scoping review approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Munn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Wa9JTPT">BMC Med Res Methodol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Munn Z, Peters MD, Stern C et al (2018) Systematic review or scoping review? guidance for authors when choosing between a systematic or scoping review approach. BMC Med Res Methodol 18:1-7 124.</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main" xml:id="_JNsSeC4">OM forum-healthcare operations management: a snapshot of emerging research</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tayur</surname></persName>
		</author>
		<idno type="DOI">10.1287/msom.2019.0778</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4cm56G3">Manuf Serv Oper Manag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="869" to="887" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dai T, Tayur S (2020) OM forum-healthcare operations manage- ment: a snapshot of emerging research. Manuf Serv Oper Manag 22(5):869-887 125.</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main" xml:id="_t4vK8Hh">Taxonomic classification of planning decisions in health care: a structured review of the state of the art in OR/MS</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hulshof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kortbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Boucherie</surname></persName>
		</author>
		<idno type="DOI">10.1057/hs.2012.18</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_96v6D8e">Health Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="129" to="175" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hulshof PJ, Kortbeek N, Boucherie RJ et al (2012) Taxonomic classification of planning decisions in health care: a structured review of the state of the art in OR/MS. Health Syst 1:129-175 126.</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main" xml:id="_B5mXHuy">Empirical research in healthcare operations: past research, present understanding, and future opportunities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scholtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Terwiesch</surname></persName>
		</author>
		<idno type="DOI">10.1287/msom.2019.0826</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eWGfuUc">Manuf Serv Oper Manag</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="83" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kc DS, Scholtes S, Terwiesch C (2020) Empirical research in healthcare operations: past research, present understanding, and future opportunities. Manuf Serv Oper Manag 22(1):73-83 127.</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main" xml:id="_GBE8WmB">OM forum-the vital role of operations analysis in improving healthcare delivery</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aNSsj4U">Manuf Serv Oper Manag</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="488" to="494" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Green LV (2012) OM forum-the vital role of operations anal- ysis in improving healthcare delivery. Manuf Serv Oper Manag 14(4):488-494 128.</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main" xml:id="_CxqRJWG">Handbook of healthcare analytics: theoretical minimum for conducting 21st century research on healthcare operations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tayur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9Ug84xQ">Wiley</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dai T, Tayur S (2018) Handbook of healthcare analytics: theoret- ical minimum for conducting 21st century research on healthcare operations. Wiley 129.</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main" xml:id="_u4sWTNj">Reinforcement learning methods in public health</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weltz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Volfovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NvFsYGv">Clin Ther</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="154" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weltz J, Volfovsky A, Laber EB (2022) Reinforcement learning methods in public health. Clin Ther 44(1):139-154 130.</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main" xml:id="_yRRjBzt">A large-scale simulation model of pandemic influenza outbreaks for development of dynamic mitigation strategies</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Savachkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jc2cNPE">IIE Trans</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="893" to="905" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Das TK, Savachkin AA, Zhu Y (2008) A large-scale simula- tion model of pandemic influenza outbreaks for development of dynamic mitigation strategies. IIE Trans 40(9):893-905 131.</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main" xml:id="_7JxRey9">Identifying cost-effective dynamic policies to control epidemics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yaesoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yU8BPW4">Stat Med</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="5189" to="5209" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yaesoubi R, Cohen T (2016) Identifying cost-effective dynamic policies to control epidemics. Stat Med 35(28):5189-5209 132.</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main" xml:id="_Q9W7Du8">Voluntary vaccination through perceiving epidemic severity in social networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4HZmNhV">Complexity</title>
		<imprint>
			<biblScope unit="page">133</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shi B, Liu G, Qiu H et al (2019) Voluntary vaccination through perceiving epidemic severity in social networks. Complexity 2019 133.</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main" xml:id="_H7pPeNW">Context matters: using reinforcement learning to develop human-readable, state-dependent outbreak response policies</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Probert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lakkur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Fonnesbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NP8FVnF">Phil Trans R Soc B</title>
		<imprint>
			<biblScope unit="volume">374</biblScope>
			<biblScope unit="page" from="277" to="134" />
			<date type="published" when="1776">2019. 1776</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Probert WJ, Lakkur S, Fonnesbeck CJ et al (2019) Context mat- ters: using reinforcement learning to develop human-readable, state-dependent outbreak response policies. Phil Trans R Soc B 374(1776):20180,277 134.</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main" xml:id="_UCT879P">Deep spatial Q-learning for infectious disease control</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13253-023-00551-4</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jzTaBj4">J Agri, Biol Environ Stat</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu Z, Clifton J, Laber EB et al (2023) Deep spatial Q-learning for infectious disease control. J Agri, Biol Environ Stat pp 1-25 135.</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main" xml:id="_cqDmtzK">Optimal policy learning for disease prevention using reinforcement learning</title>
		<author>
			<persName><forename type="first">Alam</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_B6P7B4g">Sci Program</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alam Khan Z, Feng Z, Uddin MI et al (2020) Optimal policy learning for disease prevention using reinforcement learning. Sci Program 2020:1-13 136.</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main" xml:id="_8ARFn8t">COVID-19 pandemic cyclic lockdown optimization using reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04647137</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Arango M, Pelov L (2020) COVID-19 pandemic cyclic lock- down optimization using reinforcement learning. arXiv preprint arXiv:2009.04647 137.</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main" xml:id="_BPffbJb">A methodology based on deep Q-learning/genetic algorithms for optimizing COVID-19 pandemic government actions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Miralles-Pechuán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Wt2npvt">Proceedings of the 29th ACM international conference on information &amp; knowledge management</title>
		<meeting>the 29th ACM international conference on information &amp; knowledge management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note type="raw_reference">Miralles-Pechuán L, Jiménez F, Ponce H et al (2020) A methodol- ogy based on deep Q-learning/genetic algorithms for optimizing COVID-19 pandemic government actions. In: Proceedings of the 29th ACM international conference on information &amp; knowledge management, pp 1135-1144 138.</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main" xml:id="_emPUs5W">Optimising lockdown policies for epidemic control using reinforcement learning: an AI-driven control approach compatible with existing disease and network models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Khadilkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Seetharam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_n8DpsTu">Trans Indian Natl Acad Eng</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="132" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Khadilkar H, Ganu T, Seetharam DP (2020) Optimising lockdown policies for epidemic control using reinforcement learning: an AI-driven control approach compatible with existing disease and network models. Trans Indian Natl Acad Eng 5(2):129-132 139.</note>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Capobianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10560140</idno>
		<title level="m" xml:id="_dUPfVcy">Reinforcement learning for optimization of COVID-19 mitigation policies</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Kompella V, Capobianco R, Jong S et al (2020) Reinforcement learning for optimization of COVID-19 mitigation policies. arXiv preprint arXiv:2010.10560 140.</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main" xml:id="_bHfBJq6">Reinforcement learning-based decision support system for COVID-19</title>
		<author>
			<persName><forename type="first">R</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Khattab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YWuz7rc">Biomed Signal Process Control</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">102</biblScope>
			<biblScope unit="page" from="676" to="141" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Padmanabhan R, Meskin N, Khattab T et al (2021) Reinforcement learning-based decision support system for COVID-19. Biomed Signal Process Control 68(102):676 141.</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main" xml:id="_9yT6mK4">Using control theory and Bayesian reinforcement learning for policy management in pandemic situations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_epfMgND">2021 IEEE International conference on communications workshops (ICC Workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rathore H, Samant A (2021) Using control theory and Bayesian reinforcement learning for policy management in pandemic situa- tions. In: 2021 IEEE International conference on communications workshops (ICC Workshops), IEEE, pp 1-6 142.</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main" xml:id="_cytCX6B">A simulation-deep reinforcement learning (SiRL) approach for epidemic control optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bushaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beqiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mgsbkyz">Ann Oper Res</title>
		<imprint>
			<biblScope unit="volume">328</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="245" to="277" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bushaj S, Yin X, Beqiri A et al (2023) A simulation-deep reinforcement learning (SiRL) approach for epidemic control optimization. Ann Oper Res 328(1):245-277 143.</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main" xml:id="_VSTfW9Z">A general framework for optimising cost-effectiveness of pandemic response under partial intervention measures</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_azdjZu5">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="482" to="144" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nguyen QD, Prokopenko M (2022) A general framework for optimising cost-effectiveness of pandemic response under partial intervention measures. Sci Rep 12(1):19,482 144.</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main" xml:id="_chxcMgT">Optimal adaptive nonpharmaceutical interventions to mitigate the outbreak of respiratory infections following the COVID-19 pandemic: a deep reinforcement learning study in Hong Kong, China</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rRqrG9G">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1543" to="1551" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yao Y, Zhou H, Cao Z et al (2023) Optimal adaptive nonphar- maceutical interventions to mitigate the outbreak of respiratory infections following the COVID-19 pandemic: a deep reinforce- ment learning study in Hong Kong, China. J Am Med Inform Assoc 30(9):1543-1551 145.</note>
</biblStruct>

<biblStruct xml:id="b141">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01257146</idno>
		<title level="m" xml:id="_YrTCNaP">Reinforced epidemic control: saving both lives and economy</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Song S, Zong Z, Li Y et al (2020) Reinforced epidemic control: saving both lives and economy. arXiv preprint arXiv:2008.01257 146.</note>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main" xml:id="_vZ4YPsW">Deep reinforcement learning for large-scale epidemic control</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Libin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moonens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verstraeten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KrTkzSn">Machine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo Track: European Conference, ECML PKDD 2020</title>
		<meeting><address><addrLine>Ghent, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2021. September 14-18, 2020</date>
			<biblScope unit="page" from="155" to="170" />
		</imprint>
	</monogr>
	<note type="raw_reference">Libin PJ, Moonens A, Verstraeten T et al (2021) Deep rein- forcement learning for large-scale epidemic control. In: Machine Learning and Knowledge Discovery in Databases. Applied Data Science and Demo Track: European Conference, ECML PKDD 2020, Ghent, Belgium, September 14-18, 2020, Proceedings, Part V, Springer, pp 155-170 147.</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main" xml:id="_bDDvMKg">Deep reinforcement learning approaches for global public health strategies for COVID-19 pandemic</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_McQw5rf">PLoS One</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="148" />
			<date type="published" when="2021">2021. 0251</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kwak GH, Ling L, Hui P (2021) Deep reinforcement learning approaches for global public health strategies for COVID-19 pan- demic. PLoS One 16(5):e0251,550 148.</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main" xml:id="_sCMPdsd">Towards dynamic lockdown strategies controlling pandemic spread under healthcare resource budget</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yeZVb4w">Appl Netw Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Roy S, Dutta R, Ghosh P (2021) Towards dynamic lockdown strategies controlling pandemic spread under healthcare resource budget. Appl Netw Sci 6(1):1-15 149.</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main" xml:id="_NKYDmD2">Reinforcement learning based framework for COVID-19 resource allocation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mFpKWAn">Comput Ind Eng</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="issue">107</biblScope>
			<biblScope unit="page" from="960" to="150" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zong K, Luo C (2022) Reinforcement learning based framework for COVID-19 resource allocation. Comput Ind Eng 167(107):960 150.</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main" xml:id="_czz2djp">HRL4EC: Hierarchical reinforcement learning for multi-mode epidemic control</title>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_T6qr74M">N Engl J Med</title>
		<imprint>
			<biblScope unit="volume">640</biblScope>
			<biblScope unit="issue">119</biblScope>
			<biblScope unit="page" from="2605" to="2615" />
			<date type="published" when="2009">2023. 2009</date>
		</imprint>
	</monogr>
	<note>Team NSOIAHVI Emergence of a novel swine-origin influenza a (H1N1) virus in humans Inf Sci</note>
	<note type="raw_reference">Du X, Chen H, Yang B et al (2023) HRL4EC: Hierarchical reinforcement learning for multi-mode epidemic control. Inf Sci 640(119):065 151. Team NSOIAHVI (2009) Emergence of a novel swine-origin influenza a (H1N1) virus in humans. N Engl J Med 360(25):2605- 2615 152.</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main" xml:id="_ckZbcjv">State of the art-a survey of partially observable Markov decision processes: theory, models, and algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Monahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AxehZYv">Manage Sci</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Monahan GE (1982) State of the art-a survey of partially observ- able Markov decision processes: theory, models, and algorithms. Manage Sci 28(1):1-16 153.</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main" xml:id="_YgMEXbF">Global stability for the SEIR model in epidemiology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Muldowney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hQQ3C9h">Math Biosci</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="164" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li MY, Muldowney JS (1995) Global stability for the SEIR model in epidemiology. Math Biosci 125(2):155-164 154.</note>
</biblStruct>

<biblStruct xml:id="b149">
	<monogr>
		<title level="m" type="main" xml:id="_ubBrxYW">Reinforcement learning and dynamic programming using function approximators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CRC Press</publisher>
			<biblScope unit="page">155</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Busoniu L, Babuska R, De Schutter B et al (2017) Reinforcement learning and dynamic programming using function approxima- tors. CRC Press 155.</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main" xml:id="_xmZvbAM">Deep exploration via randomized value functions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BJEbCF5">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">124</biblScope>
			<biblScope unit="page" from="1" to="62" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Osband I, Van Roy B, Russo DJ et al (2019) Deep exploration via randomized value functions. J Mach Learn Res 20(124):1-62 156.</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main" xml:id="_3B9yXfQ">The unconstrained binary quadratic programming problem: a survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kochenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Wc7Cash">J Comb Optim</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="58" to="81" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kochenberger G, Hao JK, Glover F et al (2014) The unconstrained binary quadratic programming problem: a survey. J Comb Optim 28:58-81 157.</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main" xml:id="_dF3zNfH">On the asymptotic behavior of the stochastic and deterministic models of an epidemic</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dishon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TkPHFAv">Math Biosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="261" to="265" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weiss GH, Dishon M (1971) On the asymptotic behavior of the stochastic and deterministic models of an epidemic. Math Biosci 11(3-4):261-265 158.</note>
</biblStruct>

<biblStruct xml:id="b153">
	<monogr>
		<title level="m" type="main" xml:id="_XsxWS6B">Causal inference in statistics: an overview 159</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pearl J (2009) Causal inference in statistics: an overview 159.</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main" xml:id="_9fABbu5">Interpretable machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_U6ZSU4C">Lulu. com</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Molnar C (2020) Interpretable machine learning. Lulu. com 160.</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main" xml:id="_6Pch2fA">Agent-based Markov modeling for improved covid-19 mitigation policies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Capobianco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ault</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Hq6UXtx">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="953" to="992" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Capobianco R, Kompella V, Ault J et al (2021) Agent-based Markov modeling for improved covid-19 mitigation policies. J Artif Intell Res 71:953-992 161.</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main" xml:id="_Et9FAX3">FluTE, a publicly available stochastic influenza epidemic simulation model</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Halloran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Obenchain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MkJfPUC">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="656" to="162" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chao DL, Halloran ME, Obenchain VJ et al (2010) FluTE, a pub- licly available stochastic influenza epidemic simulation model. PLoS Computational Biology 6(1):e1000,656 162.</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main" xml:id="_zB4dHru">Informer: beyond efficient transformer for long sequence time-series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_R7A9NWG">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="115" to="163" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou H, Zhang S, Peng J et al (2021) Informer: beyond efficient transformer for long sequence time-series forecasting. In: Pro- ceedings of the AAAI conference on artificial intelligence, pp 11,106-11,115 163.</note>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main" xml:id="_aewuAMq">Covasim: an agentbased model of COVID-19 dynamics and interventions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mistry</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1009149</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ngtf2Fu">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">149</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kerr CC, Stuart RM, Mistry D et al (2021) Covasim: an agent- based model of COVID-19 dynamics and interventions. PLoS Comput Biol 17(7):e1009,149</note>
</biblStruct>

<biblStruct xml:id="b159">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">164</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Q. Wu et al. 164.</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main" xml:id="_wMTPSWW">Actor-attention-critic for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_B7Z7hPT">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2961" to="2970" />
		</imprint>
	</monogr>
	<note type="raw_reference">Iqbal S, Sha F (2019) Actor-attention-critic for multi-agent rein- forcement learning. In: International conference on machine learning, PMLR, pp 2961-2970 165.</note>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main" xml:id="_7gpMEqU">Explainable deep reinforcement learning: state of the art and challenges</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Vouros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4g3vnjn">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vouros GA (2022) Explainable deep reinforcement learning: state of the art and challenges. ACM Comput Surv 55(5):1-39 166.</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main" xml:id="_cJEeuB8">A survey on interpretable reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glanois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SQNKf4M">Mach Learn</title>
		<imprint>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Glanois C, Weng P, Zimmer M et al (2024) A survey on inter- pretable reinforcement learning. Mach Learn pp 1-44 167.</note>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main" xml:id="_r3zttNp">When to switch? index policies for resource scheduling in emergency response</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MqGBavG">Prod Oper Manag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="262" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li D, Ding L, Connor S (2020) When to switch? index policies for resource scheduling in emergency response. Prod Oper Manag 29(2):241-262 168.</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main" xml:id="_mpBuxG8">A meta algorithm for reinforcement learning: emergency medical service resource prioritization problem in an MCI as an example</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GhBZSD7">Health care systems engineering: HCSE</title>
		<imprint>
			<biblScope unit="page" from="103" to="115" />
			<date type="published" when="2020-05-30">2020. May 30-June 1, 2019 4</date>
			<publisher>Springer</publisher>
			<pubPlace>Montréal, Canada</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Shin K, Lee T (2020) A meta algorithm for reinforcement learn- ing: emergency medical service resource prioritization problem in an MCI as an example. In: Health care systems engineering: HCSE, Montréal, Canada, May 30-June 1, 2019 4, Springer, pp 103-115 169.</note>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main" xml:id="_NncYjbr">DHL: deep reinforcement learning-based approach for emergency supply distribution in humanitarian logistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mišić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6VtVqzb">Peer-to-Peer Netw Appl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2376" to="2389" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fan J, Chang X, Mišić J et al (2022) DHL: deep reinforce- ment learning-based approach for emergency supply distribution in humanitarian logistics. Peer-to-Peer Netw Appl 15(5):2376- 2389 170.</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main" xml:id="_2npW3bm">Reinforcement learning for humanitarian relief distribution with trucks and UAVs under travel time uncertainty</title>
		<author>
			<persName><forename type="first">R</forename><surname>Van Steenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Heeswijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fJYKuXx">Trans Res Part C: Emerg Technol</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="issue">104</biblScope>
			<biblScope unit="page" from="401" to="171" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">van Steenbergen R, Mes M, van Heeswijk W (2023) Reinforce- ment learning for humanitarian relief distribution with trucks and UAVs under travel time uncertainty. Trans Res Part C: Emerg Technol 157(104):401 171.</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main" xml:id="_6f4C7Jj">Deep reinforcement learning-based vaccine distribution strategies. 2021 2nd International conference on electronics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EYj9hjB">Commun Inform Technol (CECIT)</title>
		<imprint>
			<biblScope unit="page" from="427" to="436" />
			<date type="published" when="2021">2021</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Wei X, Pu C, He Z et al (2021) Deep reinforcement learning-based vaccine distribution strategies. 2021 2nd International conference on electronics. Commun Inform Technol (CECIT), IEEE, pp 427- 436 172.</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main" xml:id="_gpDt2bf">Hierarchical reinforcement learning for scarce medical resource allocation with imperfect information</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_h6auwj5">Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining</title>
		<meeting>the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2955" to="2963" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hao Q, Xu F, Chen L et al (2021) Hierarchical reinforce- ment learning for scarce medical resource allocation with imper- fect information. In: Proceedings of the 27th ACM SIGKDD conference on knowledge discovery &amp; data mining, pp 2955- 2963 173.</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main" xml:id="_bbcvt2u">COVID-19 vaccine distribution policy design with reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZHKj7k8">2021 5th International conference on advances in image processing (ICAIP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="103" to="108" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tan P (2021) COVID-19 vaccine distribution policy design with reinforcement learning. In: 2021 5th International conference on advances in image processing (ICAIP), pp 103-108 174.</note>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main" xml:id="_mHs97ZM">On collaborative reinforcement learning to optimize the redistribution of critical medical supplies throughout the COVID-19 pandemic</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Bednarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jc2k54Q">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="874" to="878" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bednarski BP, Singh AD, Jones WM (2021) On collaborative reinforcement learning to optimize the redistribution of critical medical supplies throughout the COVID-19 pandemic. J Am Med Inform Assoc 28(4):874-878 175.</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main" xml:id="_6ptB2AP">Reinforcement learning enhances the experts: large-scale COVID-19 vaccine allocation with multi-factor contact network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZhaaMtK">Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining</title>
		<meeting>the 28th ACM SIGKDD conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4684" to="4694" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hao Q, Huang W, Xu F et al (2022) Reinforcement learning enhances the experts: large-scale COVID-19 vaccine allocation with multi-factor contact network. In: Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data min- ing, pp 4684-4694 176.</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main" xml:id="_zvnCzWg">Controlling epidemics through optimal allocation of test kits and vaccine doses across networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Böttcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WFTbSBf">IEEE Trans Netw Sci Eng</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1422" to="1436" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xia M, Böttcher L, Chou T (2022) Controlling epidemics through optimal allocation of test kits and vaccine doses across networks. IEEE Trans Netw Sci Eng 9(3):1422-1436 177.</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main" xml:id="_dkar3wH">Stochastic optimization for vaccine and testing kit allocation for the COVID-19 pandemic</title>
		<author>
			<persName><forename type="first">L</forename><surname>Thul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vQvc9uy">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="325" to="338" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thul L, Powell W (2023) Stochastic optimization for vaccine and testing kit allocation for the COVID-19 pandemic. Eur J Oper Res 304(1):325-338 178.</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main" xml:id="_rCHQUHh">Deep reinforcement learning based medical supplies dispatching model for major infectious diseases: case study of COVID-19</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KYaYhGY">Oper Res Persp</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">100</biblScope>
			<biblScope unit="page" from="293" to="179" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zeng JY, Lu P, Wei Y et al (2023) Deep reinforcement learning based medical supplies dispatching model for major infectious diseases: case study of COVID-19. Oper Res Persp 11(100):293 179.</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main" xml:id="_bQTehfH">Management of resource sharing in emergency response using data-driven analytics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tutun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Anvaryazdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EDzAFcs">Annal Oper Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang J, Tutun S, Anvaryazdi SF et al (2023) Management of resource sharing in emergency response using data-driven analyt- ics. Annal Oper Res pp 1-30 180.</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main" xml:id="_f7a9VGr">Enumerate lasso solutions for feature selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CvAXGha">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">181</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Hara S, Maehara T (2017) Enumerate lasso solutions for feature selection. In: Proceedings of the AAAI conference on artificial intelligence 181.</note>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main" xml:id="_hCEsSTY">Use of stabilized inverse propensity scores as weights to directly estimate relative risk and its confidence intervals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Raebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DdHMAfH">Value in Health</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="277" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu S, Ross C, Raebel MA et al (2010) Use of stabilized inverse propensity scores as weights to directly estimate relative risk and its confidence intervals. Value in Health 13(2):273-277 182.</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main" xml:id="_yrsJXXH">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dHNqSxQ">Annal Stat</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Friedman JH (2001) Greedy function approximation: a gradient boosting machine. Annal Stat pp 1189-1232 183.</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main" xml:id="_zr6p6uN">A random forest guided tour</title>
		<author>
			<persName><forename type="first">G</forename><surname>Biau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Scornet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zRqWQhJ">TEST</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Biau G, Scornet E (2016) A random forest guided tour. TEST 25:197-227 184.</note>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main" xml:id="_ac9jHPB">Analysis of Thompson sampling for the multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XZeTPE4">Conference on learning theory, JMLR workshop and conference proceedings</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="39" to="40" />
		</imprint>
	</monogr>
	<note type="raw_reference">Agrawal S, Goyal N (2012) Analysis of Thompson sampling for the multi-armed bandit problem. In: Conference on learning the- ory, JMLR workshop and conference proceedings, pp 39-1 185.</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main" xml:id="_j2R2PmG">Pontryagin maximum principle</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kopp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GVay74g">Mathematics in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="255" to="279" />
			<date type="published" when="1962">1962</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Kopp RE (1962) Pontryagin maximum principle. In: Mathematics in Science and Engineering, vol 5. Elsevier, p 255-279 186.</note>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main" xml:id="_6ZCVxVT">Application of machine learning based hospital up-gradation policy for Bangladesh</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shuvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kabir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_m36qp9g">Proceedings of the 7th international conference on networking, systems and security</title>
		<meeting>the 7th international conference on networking, systems and security</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="18" to="24" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shuvo SS, Ahmed MR, Kabir SB, et al (2020) Application of machine learning based hospital up-gradation policy for Bangladesh. In: Proceedings of the 7th international conference on networking, systems and security, pp 18-24 187.</note>
</biblStruct>

<biblStruct xml:id="b183">
	<monogr>
		<title level="m" type="main" xml:id="_NAPwh7v">Robust policy optimization in deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.07536188</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Rahman MM, Xue Y (2022) Robust policy optimization in deep reinforcement learning. arXiv preprint arXiv:2212.07536 188.</note>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main" xml:id="_wNAPhHW">Real-time neural network scheduling of emergency medical mask production during COVID-19</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karatas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WPAnShs">Appl Soft Comput</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">106</biblScope>
			<biblScope unit="page" from="790" to="189" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu CX, Liao MH, Karatas M et al (2020) Real-time neural net- work scheduling of emergency medical mask production during COVID-19. Appl Soft Comput 97(106):790 189.</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main" xml:id="_9j7mdmV">Optimization of inventory management to prevent drug shortages in the hospital supply chain</title>
		<author>
			<persName><forename type="first">Abu</forename><surname>Zwaida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beauregard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2KtGsyS">Appl Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2726" to="2190" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abu Zwaida T, Pham C, Beauregard Y (2021) Optimization of inventory management to prevent drug shortages in the hospital supply chain. Appl Sci 11(6):2726 190.</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main" xml:id="_syZ6dMw">A Markov decision process approach for managing medical drone deliveries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Pinkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mes</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2022.117490</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ua8ete9">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="page" from="117490" to="117191" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Asadi A, Pinkley SN, Mes M (2022) A Markov decision pro- cess approach for managing medical drone deliveries. Expert Syst Appl p 117490 191.</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main" xml:id="_3bdgeHk">Intelligent inventory management approaches for perishable pharmaceutical products in a healthcare supply chain</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mosadegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maihami</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cor.2022.105968</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9b2P6sW">Comput Oper Res</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">105</biblScope>
			<biblScope unit="page" from="968" to="192" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahmadi E, Mosadegh H, Maihami R et al (2022) Intelligent inventory management approaches for perishable pharmaceuti- cal products in a healthcare supply chain. Comput Oper Res 147(105):968 192.</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main" xml:id="_ZaKACSv">Optimizing vaccine distribution in developing countries under natural disaster risk</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Seranilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Löhndorf</surname></persName>
		</author>
		<idno type="DOI">10.1002/nav.22143</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_R8Q2CxM">Nav Res Logist (NRL)</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Seranilla BK, Löhndorf N (2023) Optimizing vaccine distribution in developing countries under natural disaster risk. Nav Res Logist (NRL) 193.</note>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main" xml:id="_8sHDNs4">Cluster-based lateral transshipments for the Zambian health supply chain</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vanvuchelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Boeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.ejor.2023.08.005</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8xj26UV">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="373" to="386" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vanvuchelen N, De Boeck K, Boute RN (2024) Cluster-based lateral transshipments for the Zambian health supply chain. Eur J Oper Res 313(1):373-386 194.</note>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main" xml:id="_6HaCtnV">Approximate dynamic programming for ambulance redeployment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Restrepo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.1287/ijoc.1090.0345</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eWkX8NV">INFORMS J Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="281" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maxwell MS, Restrepo M, Henderson SG et al (2010) Approx- imate dynamic programming for ambulance redeployment. INFORMS J Comput 22(2):266-281 195.</note>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main" xml:id="_d3V35jM">Solving the dynamic ambulance relocation and dispatching problem using approximate dynamic programming</title>
		<author>
			<persName><forename type="first">V</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ejor.2011.10.043</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wBxvJcg">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="611" to="621" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schmid V (2012) Solving the dynamic ambulance relocation and dispatching problem using approximate dynamic programming. Eur J Oper Res 219(3):611-621 196.</note>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main" xml:id="_CgJTWjA">Tuning approximate dynamic programming policies for ambulance redeployment via direct search</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Topaloglu</surname></persName>
		</author>
		<idno type="DOI">10.1214/10-ssy020</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_J3XXFCH">Stoch Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="322" to="361" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maxwell MS, Henderson SG, Topaloglu H (2013) Tuning approx- imate dynamic programming policies for ambulance redeploy- ment via direct search. Stoch Syst 3(2):322-361 197.</note>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main" xml:id="_dSwgDJm">Real-time ambulance dispatching and relocation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nasrollahzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khademi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Mayorga</surname></persName>
		</author>
		<idno type="DOI">10.1287/msom.2017.0649</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kzFTKhN">Manuf Serv Oper Manag</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="480" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nasrollahzadeh AA, Khademi A, Mayorga ME (2018) Real-time ambulance dispatching and relocation. Manuf Serv Oper Manag 20(3):467-480 198.</note>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main" xml:id="_Gtsq3B5">Developing an OpenAI gymcompatible framework and simulation environment for testing deep reinforcement learning agents solving the ambulance location problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Monks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.04434199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Allen M, Pearn K, Monks T (2021) Developing an OpenAI gym- compatible framework and simulation environment for testing deep reinforcement learning agents solving the ambulance loca- tion problem. arXiv preprint arXiv:2101.04434 199.</note>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main" xml:id="_c2YtqKZ">Application of deep reinforcement learning for traffic control of road inter section with emergency vehicles</title>
		<author>
			<persName><forename type="first">G</forename><surname>Benedetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mangini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ePrfxQD">2021 IEEE International conference on systems, man, and cybernetics (SMC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="182" to="187" />
		</imprint>
	</monogr>
	<note type="raw_reference">Benedetti G, Fanti MP, Mangini AM, et al (2021) Application of deep reinforcement learning for traffic control of road inter section with emergency vehicles. In: 2021 IEEE International conference on systems, man, and cybernetics (SMC), IEEE, pp 182-187 200.</note>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main" xml:id="_2n3zgpt">Risk-averse flexible policy on ambulance allocation in humanitarian operations under uncertainty</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1080/00207543.2020.1735663</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Mv3xtqf">Int J Prod Res</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2588" to="2610" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu G, Liu A, Sun H (2021) Risk-averse flexible policy on ambu- lance allocation in humanitarian operations under uncertainty. Int J Prod Res 59(9):2588-2610 201.</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main" xml:id="_mM6YEva">Shortening emergency medical response time with joint operations of uncrewed aerial vehicles with ambulances</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="DOI">10.1287/msom.2022.0166</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XCB2ZFY">Manuf Serv Oper Manag</title>
		<imprint>
			<biblScope unit="volume">202</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gao X, Kong N, Griffin P (2023) Shortening emergency medical response time with joint operations of uncrewed aerial vehicles with ambulances. Manuf Serv Oper Manag 202.</note>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main" xml:id="_RurEWvR">Operations research tools for addressing current challenges in emergency medical services 203</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470400531.eorms0605</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Henderson SG (2011) Operations research tools for addressing current challenges in emergency medical services 203.</note>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main" xml:id="_wUXCnZY">An adaptive decision support system for outpatient appointment scheduling with heterogeneous service times</title>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Bg4U9MA">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="731" to="204" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Feng H, Jia Y, Huang T, et al (2024) An adaptive decision support system for outpatient appointment scheduling with heterogeneous service times. Sci Rep 14(1):27,731 204.</note>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main" xml:id="_CHaF9UZ">Dynamic multipriority patient scheduling for a diagnostic resource</title>
		<author>
			<persName><forename type="first">J</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Queyranne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MH4g3W5">Oper Res</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1507" to="1525" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Patrick J, Puterman ML, Queyranne M (2008) Dynamic multi- priority patient scheduling for a diagnostic resource. Oper Res 56(6):1507-1525 205.</note>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main" xml:id="_rPNy89b">Reinforcement learning based resource allocation in business process management</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RSFeaGP">Data Know Eng</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="145" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Huang Z, van der Aalst WM, Lu X et al (2011) Reinforcement learning based resource allocation in business process manage- ment. Data Know Eng 70(1):127-145 206.</note>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main" xml:id="_YuUk5Gv">Optimal and approximate algorithms for sequential clinical scheduling with no-shows</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muthuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7kvT5Y5">IIE Trans Healthc Syst Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="36" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lin J, Muthuraman K, Lawley M (2011) Optimal and approximate algorithms for sequential clinical scheduling with no-shows. IIE Trans Healthc Syst Eng 1(1):20-36 207.</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main" xml:id="_u92hf83">Approximate dynamic programming for capacity allocation in the service industry</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kolisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Z2mT9dK">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="250" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schuetz HJ, Kolisch R (2012) Approximate dynamic program- ming for capacity allocation in the service industry. Eur J Oper Res 218(1):239-250 208.</note>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main" xml:id="_BKjUnyu">Appointment scheduling under patient preference and no-show behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Topaloglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZDP5B3j">Oper Res</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="794" to="811" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Feldman J, Liu N, Topaloglu H et al (2014) Appointment schedul- ing under patient preference and no-show behavior. Oper Res 62(4):794-811 209.</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main" xml:id="_8XckjFT">A simulation based approximate dynamic programming approach to multi-class, multi-resource surgical scheduling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Astaraky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ugJZstG">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">245</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="309" to="319" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Astaraky D, Patrick J (2015) A simulation based approximate dynamic programming approach to multi-class, multi-resource surgical scheduling. Eur J Oper Res 245(1):309-319 210.</note>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main" xml:id="_XbSv9GH">Applying reinforce ment learning techniques to detect hepatocellular carcinoma under limited screening capacity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lavieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Volk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nAfqh3j">Health Care Manag Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="363" to="375" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee E, Lavieri MS, Volk ML et al (2015) Applying reinforce ment learning techniques to detect hepatocellular carcinoma under limited screening capacity. Health Care Manag Sci 18:363- 375 211.</note>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main" xml:id="_mbMH2bj">Improving emergency department efficiency by patient scheduling using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Wewf3t5">Healthcare, MDPI</title>
		<imprint>
			<biblScope unit="page" from="77" to="212" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee S, Lee YH (2020) Improving emergency department effi- ciency by patient scheduling using deep reinforcement learning. In: Healthcare, MDPI, p 77 212.</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main" xml:id="_Dsd7T5V">Dynamic multistage scheduling for patientcentered care plans</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diamant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Q3Hsktv">Health Care Manag Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="827" to="844" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Diamant A (2021) Dynamic multistage scheduling for patient- centered care plans. Health Care Manag Sci 24(4):827-844 213.</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main" xml:id="_5Cvxtjj">An approximate dynamic programming approach to the admission control of elective patients</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">El</forename><surname>Moudni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zPqpX2c">Comput Oper Res</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">105</biblScope>
			<biblScope unit="page" from="259" to="214" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang J, Dridi M, El Moudni A (2021) An approximate dynamic programming approach to the admission control of elective patients. Comput Oper Res 132(105):259 214.</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main" xml:id="_xtMJvxs">Preference based scheduling in a healthcare provider network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JsqkxZJ">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1318" to="1335" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Agrawal D, Pang G, Kumara S (2023) Preference based schedul- ing in a healthcare provider network. Eur J Oper Res 307(3):1318- 1335 215.</note>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main" xml:id="_VHq9xQU">Optimal hospital care scheduling during the SARS-CoV-2 pandemic</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>D'aeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kXJdZnp">Manag Sci</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D&apos;Aeth JC, Ghosal S, Grimm F et al (2023) Optimal hospital care scheduling during the SARS-CoV-2 pandemic. Manag Sci 216.</note>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main" xml:id="_p3hXkhy">Reinforcement learning for optimal control of queueing systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Modiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jbepxK3">2019 57th Annual allerton conference on communication, control, and computing</title>
		<meeting><address><addrLine>Allerton)</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">Liu B, Xie Q, Modiano E (2019) Reinforcement learning for opti- mal control of queueing systems. In: 2019 57th Annual allerton conference on communication, control, and computing (Allerton), IEEE, pp 663-670 217.</note>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main" xml:id="_sZMUaNN">Optimal national prioritization policies for hospital care during the SARS-CoV-2 pandemic</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>D'aeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VruVZhj">Nat Comput Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="521" to="531" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D&apos;Aeth JC, Ghosal S, Grimm F et al (2021) Optimal national prioritization policies for hospital care during the SARS-CoV-2 pandemic. Nat Comput Sci 1(8):521-531 218.</note>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main" xml:id="_SSxxpvd">Dynamic patient scheduling for multi-appointment health care programs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Diamant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Quereshy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CjZDKgx">Prod Oper Manag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="79" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Diamant A, Milner J, Quereshy F (2018) Dynamic patient scheduling for multi-appointment health care programs. Prod Oper Manag 27(1):58-79 219.</note>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main" xml:id="_cqzDG3J">Reinforcement learning for primary care e appointment scheduling</title>
		<author>
			<persName><forename type="first">Tsmt</forename><surname>Gomes</surname></persName>
		</author>
		<idno>da Informação 220</idno>
	</analytic>
	<monogr>
		<title level="s" xml:id="_xFEHu6w">Faculdade de Engenharia da Universidade do Porto Mestrado de Engenharia</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gomes TSMT (2017) Reinforcement learning for primary care e appointment scheduling. Faculdade de Engenharia da Universi- dade do Porto Mestrado de Engenharia da Informação 220.</note>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main" xml:id="_UuqDyfM">Strategic level proton therapy patient admission planning: a Markov decision process modeling approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rainwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dr5wX8w">Health Care Manag Sci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="286" to="302" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gedik R, Zhang S, Rainwater C (2017) Strategic level proton therapy patient admission planning: a Markov decision process modeling approach. Health Care Manag Sci 20:286-302 221.</note>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main" xml:id="_4xbdg4A">Recent modeling and analytical advances in hospital inpatient flow management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yKAdw9D">Prod Oper Manag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1838" to="1862" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dai JG, Shi P (2021) Recent modeling and analytical advances in hospital inpatient flow management. Prod Oper Manag 30(6):1838-1862 222.</note>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main" xml:id="_GbsjJNZ">A systematic review of research design and modeling techniques in inpatient bed management</title>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Madathil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oberoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_egYhwGY">Comput Ind Eng</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="451" to="466" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">He L, Madathil SC, Oberoi A et al (2019) A systematic review of research design and modeling techniques in inpatient bed man- agement. Comput Ind Eng 127:451-466 223.</note>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main" xml:id="_u5KzuPG">Managing patient admissions in a neurology ward</title>
		<author>
			<persName><forename type="first">S</forename><surname>Samiedaluie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kucukyazici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jYfVgCK">Oper Res</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="635" to="656" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Samiedaluie S, Kucukyazici B, Verter V et al (2017) Managing patient admissions in a neurology ward. Oper Res 65(3):635-656 224.</note>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chivers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06300225</idno>
		<title level="m" xml:id="_bEPxBAa">A reinforcement learning approach to weaning of mechanical ventilation in intensive care units</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Prasad N, Cheng LF, Chivers C et al (2017) A reinforcement learning approach to weaning of mechanical ventilation in inten- sive care units. arXiv preprint arXiv:1704.06300 225.</note>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main" xml:id="_hr5bFmb">On the Taylor expansion of value functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurvich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pDYfZa6">Oper Res</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="631" to="654" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Braverman A, Gurvich I, Huang J (2020) On the Taylor expansion of value functions. Oper Res 68(2):631-654 226.</note>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main" xml:id="_4Hyqrs9">Use of machine learning for long term planning and cost minimization in healthcare management</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shuvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_J779SJs">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2031" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kabir SB, Shuvo SS, Ahmed HU (2021) Use of machine learn- ing for long term planning and cost minimization in healthcare management. medRxiv pp 2021-10 227.</note>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main" xml:id="_XMMktgh">A stochastic model for the patient-bed assignment problem with random arrivals and departures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heydar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_M8Szd86">Annal Oper Res</title>
		<imprint>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Heydar M, O&apos;Reilly MM, Trainer E et al (2021) A stochas- tic model for the patient-bed assignment problem with random arrivals and departures. Annal Oper Res pp 1-33 228.</note>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main" xml:id="_KxYZzfY">An efficient pessimistic-optimistic algorithm for stochastic linear bandits with general constraints</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HDHRby5">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="86" to="229" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu X, Li B, Shi P et al (2021) An efficient pessimistic-optimistic algorithm for stochastic linear bandits with general constraints. Adv Neural Inform Process Syst 34:24,075-24,086 229.</note>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main" xml:id="_NYmwPU4">Data-driven hospitals staff and resources allocation using agent-based simulation and deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5Zhar6N">Eng Appl Artif Intell</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">106</biblScope>
			<biblScope unit="page" from="783" to="230" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lazebnik T (2023) Data-driven hospitals staff and resources allocation using agent-based simulation and deep reinforcement learning. Eng Appl Artif Intell 126(106):783 230.</note>
</biblStruct>

<biblStruct xml:id="b226">
	<monogr>
		<title level="m" type="main" xml:id="_WQbY2m7">Stein&apos;s method for steady-state diffusion approximations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Braverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">231</biblScope>
		</imprint>
		<respStmt>
			<orgName>Cornell University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">Braverman A (2017) Stein&apos;s method for steady-state diffusion approximations. PhD thesis, Cornell University 231.</note>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main" xml:id="_rjrnkY4">A sufficient condition for convergences of adam and rmsprop</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JAxFBvR">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="135" to="232" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zou F, Shen L, Jie Z et al (2019) A sufficient condition for convergences of adam and rmsprop. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recogni- tion, pp 11,127-11,135 232.</note>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main" xml:id="_fGeuZRy">Dynamic scheduling of home care patients to medical providers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Cire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diamant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dyN7uPX">Prod Oper Manag</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4038" to="4056" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cire AA, Diamant A (2022) Dynamic scheduling of home care patients to medical providers. Prod Oper Manag 31(11):4038- 4056 233.</note>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main" xml:id="_NRRZN3f">Continuous support for rehabilitation using machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Merkle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NdHExCj">Inform Technol</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="273" to="284" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Philipp P, Merkle N, Gand K et al (2019) Continuous support for rehabilitation using machine learning. Inform Technol 61(5- 6):273-284 234.</note>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main" xml:id="_CZ9P7VX">Requiem for second-order fluid approximations of traffic flow</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Daganzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bQ8jTVc">Trans Res Part B: Methodol</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="286" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Daganzo CF (1995) Requiem for second-order fluid approxima- tions of traffic flow. Trans Res Part B: Methodol 29(4):277-286 235.</note>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main" xml:id="_Xz2TDnj">A reinforcement learning development of the FRAM for functional reward-based assessments of complex systems performance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Salehi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Veitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cFPNuv4">Int J Ind Ergon</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">103</biblScope>
			<biblScope unit="page" from="271" to="236" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Salehi V, Tran T, Veitch B et al (2022) A reinforcement learn- ing development of the FRAM for functional reward-based assessments of complex systems performance. Int J Ind Ergon 88(103):271 236.</note>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main" xml:id="_b4kXAPu">Reinforcement learning for intelligent healthcare systems: a review of challenges, applications, and open research issues</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Abdellatif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mhaisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MyuqwAw">IEEE Int Things J</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abdellatif AA, Mhaisen N, Mohamed A et al (2023) Rein- forcement learning for intelligent healthcare systems: a review of challenges, applications, and open research issues. IEEE Int Things J 237.</note>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main" xml:id="_xdqZysE">Deep reinforcement learning-based precise prediction model for smart m-health system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jagannath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QjrJHsQ">Expert Syst</title>
		<imprint>
			<biblScope unit="page" from="13450" to="13238" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jagannath DJ, Dolly RJ, Dinesh Peter J (2023) Deep reinforce- ment learning-based precise prediction model for smart m-health system. Expert Syst p e13450 238.</note>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main" xml:id="_kNasJvQ">An adaptive user interface in healthcare</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Shakshuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Sheltami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mUuCX4M">Procedia Comput Sci</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shakshuki EM, Reid M, Sheltami TR (2015) An adaptive user interface in healthcare. Procedia Comput Sci 56:49-58 239.</note>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main" xml:id="_v6994v8">Dynamic healthcare interface for patients</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Shakshuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Sheltami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5NjMuEz">Procedia Comput Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="356" to="365" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shakshuki EM, Reid M, Sheltami TR (2015) Dynamic healthcare interface for patients. Procedia Comput Sci 63:356-365 240.</note>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main" xml:id="_SDnz29P">Reinforcement learning approaches for efficient and secure blockchain-powered smart health systems</title>
		<author>
			<persName><forename type="first">Al-Marridi Az</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">A</forename><surname>Erbad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fbZ5Ehd">Comput Netw</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">108</biblScope>
			<biblScope unit="page" from="279" to="241" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Al-Marridi AZ, Mohamed A, Erbad A (2021) Reinforcement learning approaches for efficient and secure blockchain-powered smart health systems. Comput Netw 197(108):279 241.</note>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main" xml:id="_3xWkcVs">Modeling, detecting, and mitigating threats against industrial healthcare systems: a combined software defined networking and reinforcement learning approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Radoglou-Grammatikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rompolos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sarigiannidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vvr4V5H">IEEE Trans Industr Inf</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2041" to="2052" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Radoglou-Grammatikis P, Rompolos K, Sarigiannidis P et al (2021) Modeling, detecting, and mitigating threats against indus- trial healthcare systems: a combined software defined networking and reinforcement learning approach. IEEE Trans Industr Inf 18(3):2041-2052 242.</note>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main" xml:id="_AmSJx8Z">Effective deep q-networks (EDQN) strategy for resource allocation based on optimized reinforcement learning algorithm</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Talaat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_W5mThu9">Multimed Tool Appl</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="961" to="243" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Talaat FM (2022) Effective deep q-networks (EDQN) strategy for resource allocation based on optimized reinforcement learning algorithm. Multimed Tool Appl 81(28):39,945-39,961 243.</note>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main" xml:id="_BYmsgzB">Optimizing patient-specific medication regimen policies using wearable sensors in Parkinson&apos;s disease</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baucum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khojandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RTJbRux">Manag Sci</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Baucum M, Khojandi A, Vasudevan R et al (2023) Optimizing patient-specific medication regimen policies using wearable sen- sors in Parkinson&apos;s disease. Manag Sci 244.</note>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main" xml:id="_acBYsgX">A reinforcement learning and deep learning based intelligent system for the support of impaired patients in home treatment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paragliola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coronato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9Y9dZD4">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">114</biblScope>
			<biblScope unit="page" from="285" to="245" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Naeem M, Paragliola G, Coronato A (2021) A reinforcement learning and deep learning based intelligent system for the sup- port of impaired patients in home treatment. Expert Syst Appl 168(114):285 245.</note>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main" xml:id="_ZbYmMdh">Optimal scheduling in cloud healthcare system using Q-learning algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9XMwny6">Complex Intell Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4603" to="4618" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li Y, Wang H, Wang N et al (2022) Optimal scheduling in cloud healthcare system using Q-learning algorithm. Complex Intell Syst 8(6):4603-4618 246.</note>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main" xml:id="_2ZZfFjq">Consumer-centric internet of medical things for cyborg applications based on federated reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lakhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Jhaveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ABneCyk">IEEE Trans Consum Electron</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tiwari P, Lakhan A, Jhaveri RH et al (2023) Consumer-centric internet of medical things for cyborg applications based on feder- ated reinforcement learning. IEEE Trans Consum Electron 247.</note>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main" xml:id="_Pnta8m7">Multi-agent federated reinforcement learning for resource allocation in UAV-enabled internet of medical things networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Glicksberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MbPZQCS">J Healthc Inform Res</title>
		<editor>
			<persName><surname>Wu</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021">2021. 2023</date>
		</imprint>
	</monogr>
	<note>Federated learning for healthcare informatics IEEE Int Thing J</note>
	<note type="raw_reference">Xu J, Glicksberg BS, Su C et al (2021) Federated learning for healthcare informatics. J Healthc Inform Res 5:1-19 Q. Wu et al. 248. Seid AM, Erbad A, Abishu HN et al (2023) Multi-agent federated reinforcement learning for resource allocation in UAV-enabled internet of medical things networks. IEEE Int Thing J 249.</note>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main" xml:id="_dpyxUcg">A deep reinforcement learning-based wireless body area network offloading optimization strategy for healthcare services</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_W3fpr9g">Health Inform Sci Syst</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="250" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen Y, Han S, Chen G et al (2023) A deep reinforcement learning-based wireless body area network offloading optimiza- tion strategy for healthcare services. Health Inform Sci Syst 11(1):8 250.</note>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main" xml:id="_cHcqMDx">Guidelines for reinforcement learning in healthcare</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aeRzgvC">Nat Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="18" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gottesman O, Johansson F, Komorowski M et al (2019) Guide- lines for reinforcement learning in healthcare. Nat Med 25(1):16- 18 251.</note>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main" xml:id="_xyB9Adk">Deep reinforcement learning in medical imaging: a literature review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KWXUVeg">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">102</biblScope>
			<biblScope unit="page" from="193" to="252" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou SK, Le HN, Luu K et al (2021) Deep reinforcement learn- ing in medical imaging: a literature review. Med Image Anal 73(102):193 252.</note>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main" xml:id="_7rWBErF">Deep reinforcement learning in computer vision: a comprehensive survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Rathour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VEpf2uR">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="page" from="1" to="87" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Le N, Rathour VS, Yamazaki K, et al (2022) Deep reinforcement learning in computer vision: a comprehensive survey. Artif Intell Rev pp 1-87 253.</note>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main" xml:id="_8nZjUYQ">Ambiguous dynamic treatment regimes: a reinforcement learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saghafian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bEDbwp6">Manag Sci</title>
		<imprint>
			<biblScope unit="volume">254</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Saghafian S (2023) Ambiguous dynamic treatment regimes: a reinforcement learning approach. Manag Sci 254.</note>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main" xml:id="_H9Butmw">Reinforcement learning with actionderived rewards for chemotherapy and clinical trial dosing regimen selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yauney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tyXdYam">Machine learning for healthcare conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="161" to="226" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yauney G, Shah P (2018) Reinforcement learning with action- derived rewards for chemotherapy and clinical trial dosing reg- imen selection. In: Machine learning for healthcare conference, PMLR, pp 161-226 255.</note>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main" xml:id="_jTGWNX6">Multi-armed bandits with endogenous learning and queueing: an application to split liver transplantation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Scheller-Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Tayur</surname></persName>
		</author>
		<idno>SSRN 3855206 256</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5ZcwGsU">Available</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tang YS, Scheller-Wolf AA, Tayur SR (2021) Multi-armed ban- dits with endogenous learning and queueing: an application to split liver transplantation. Available at SSRN 3855206 256.</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main" xml:id="_SZsZg6y">Optimizing individualized treatment planning for parkinson&apos;s disease using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khojandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_975cZeW">Annual international conference of the IEEE engineering in medicine Biol Soci (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="5406" to="5409" />
		</imprint>
	</monogr>
	<note type="raw_reference">Watts J, Khojandi A, Vasudevan R et al (2020) Optimizing indi- vidualized treatment planning for parkinson&apos;s disease using deep reinforcement learning. In: 2020 42nd Annual international con- ference of the IEEE engineering in medicine Biol Soci (EMBC), IEEE, pp 5406-5409 257.</note>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main" xml:id="_zmMWheK">A reinforcement learning model for AI-based decision support in skin cancer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Codella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZWzXReR">Nat Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1941" to="1946" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Barata C, Rotemberg V, Codella NC et al (2023) A reinforcement learning model for AI-based decision support in skin cancer. Nat Med 29(8):1941-1946 258.</note>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main" xml:id="_D2dwdp8">Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RyP2yey">Nat Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2633" to="2642" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang G, Liu X, Ying Z et al (2023) Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial. Nat Med 29(10):2633-2642 259.</note>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main" xml:id="_Xs7quJs">Medical deadends and learning to identify high-risk states and treatments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ye6bKZF">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4856" to="4870" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fatemi M, Killian TW, Subramanian J et al (2021) Medical dead- ends and learning to identify high-risk states and treatments. Adv Neural Inf Process Syst 34:4856-4870 260.</note>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main" xml:id="_4W8JamS">Proximal reinforcement learning: efficient off-policy evaluation in partially observed Markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PP3ce56">Oper Res</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bennett A, Kallus N (2023) Proximal reinforcement learning: effi- cient off-policy evaluation in partially observed Markov decision processes. Oper Res 261.</note>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main" xml:id="_usXZye8">A value-based deep reinforcement learning model with human expertise in optimal treatment of sepsis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bA2H4en">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="262" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu X, Li R, He Z et al (2023) A value-based deep reinforce- ment learning model with human expertise in optimal treatment of sepsis. NPJ Digit Med 6(1):15 262.</note>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main" xml:id="_dMuDmUM">Dynamic treatment regimes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NJNezwn">Annu Rev Stat Appl</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="447" to="464" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chakraborty B, Murphy SA (2014) Dynamic treatment regimes. Annu Rev Stat Appl 1:447-464 263.</note>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main" xml:id="_NBveZwc">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Sqc3nWc">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Silver D, Huang A, Maddison CJ et al (2016) Mastering the game of Go with deep neural networks and tree search. Nature 529(7587):484-489 264.</note>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main" xml:id="_4NZkCkE">Transfer learning for reinforcement learning domains: a survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YmCWKuS">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">265</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Taylor ME, Stone P (2009) Transfer learning for reinforcement learning domains: a survey. J Mach Learn Res 10(7) 265.</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main" xml:id="_XN7w7ty">Surgical scheduling via optimization and machine learning with long-tailed data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahdian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blanchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_enyJWj6">Health Care Manag Sci</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="692" to="718" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shi Y, Mahdian S, Blanchet J et al (2023) Surgical scheduling via optimization and machine learning with long-tailed data. Health Care Manag Sci 26(4):692-718 266.</note>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main" xml:id="_fhF2cCM">Recent advances in robust optimization: an overview</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gabrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Murat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fJuewrV">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="483" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gabrel V, Murat C, Thiele A (2014) Recent advances in robust optimization: an overview. Eur J Oper Res 235(3):471-483 267.</note>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main" xml:id="_w2mrKGG">Robust reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Doya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_948Sqpw">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Morimoto J, Doya K (2005) Robust reinforcement learning. Neu- ral Comput 17(2):335-359 268.</note>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main" xml:id="_J7DwPDn">A comprehensive survey on safe reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garcıa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WEKpprN">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1437" to="1480" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Garcıa J, Fernández F (2015) A comprehensive survey on safe reinforcement learning. J Mach Learn Res 16(1):1437-1480 269.</note>
</biblStruct>

<biblStruct xml:id="b264">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Rahimian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05659270</idno>
		<title level="m" xml:id="_UzErUjx">Distributionally robust optimization: a review</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Rahimian H, Mehrotra S (2019) Distributionally robust optimiza- tion: a review. arXiv preprint arXiv:1908.05659 270.</note>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main" xml:id="_pntT3S7">Robustness of proactive intensive care unit transfer policies</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grand-Clément</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_b6Y9eu7">Oper Res</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1653" to="1688" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grand-Clément J, Chan CW, Goyal V et al (2023) Robust- ness of proactive intensive care unit transfer policies. Oper Res 71(5):1653-1688 271.</note>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main" xml:id="_NMjFwG4">Distributionally robust chance-constrained Markov decision processes with random payoff</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AvJGZFv">Appl Math Optim</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page">272</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nguyen HN, Lisser A, Singh VV (2024) Distributionally robust chance-constrained Markov decision processes with random pay- off. Appl Math Optim 90(25) 272.</note>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main" xml:id="_6HFdNqq">Learning-based robust optimization: Procedures and statistical guarantees</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_u6vgC9C">Manage Sci</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3447" to="3467" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hong LJ, Huang Z, Lam H (2021) Learning-based robust opti- mization: Procedures and statistical guarantees. Manage Sci 67(6):3447-3467 273.</note>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main" xml:id="_Nzxy5kk">Learning to cut by looking ahead: cutting plane selection via imitation learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zarpellon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_czpmWnH">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="600" to="274" />
		</imprint>
	</monogr>
	<note type="raw_reference">Paulus MB, Zarpellon G, Krause A, et al (2022) Learning to cut by looking ahead: cutting plane selection via imitation learning. In: International conference on machine learning, PMLR, pp 17,584- 17,600 274.</note>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main" xml:id="_tJ3zK8n">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VTz2zwC">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Khalil E, Dai H, Zhang Y et al (2017) Learning combinatorial optimization algorithms over graphs. Adv Neural Inform Process Syst 30 275.</note>
</biblStruct>

<biblStruct xml:id="b270">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01780276</idno>
		<title level="m" xml:id="_bWHQa4e">Neural contextual bandits with deep representation and shallow exploration</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xu P, Wen Z, Zhao H et al (2020) Neural contextual bandits with deep representation and shallow exploration. arXiv preprint arXiv:2012.01780 276.</note>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main" xml:id="_xEEm3Gz">Improving robot controller transparency through autonomous policy explanation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_B8H8PFy">Proceedings of the 2017 ACM/IEEE international conference on human-robot interaction</title>
		<meeting>the 2017 ACM/IEEE international conference on human-robot interaction</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hayes B, Shah JA (2017) Improving robot controller transparency through autonomous policy explanation. In: Proceedings of the 2017 ACM/IEEE international conference on human-robot inter- action, pp 303-312 277.</note>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main" xml:id="_UDZDpvd">Establishing appropriate trust via critical states</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_y6wh3zC">IEEE/RSJ International conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3929" to="3936" />
		</imprint>
	</monogr>
	<note type="raw_reference">Huang SH, Bhatia K, Abbeel P, et al (2018) Establishing appro- priate trust via critical states. In: 2018 IEEE/RSJ International conference on intelligent robots and systems (IROS), IEEE, pp 3929-3936 278.</note>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main" xml:id="_RH9Ud73">Towards interpretable reinforcement learning using attention augmented agents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_W2T3ADZ">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">279</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mott A, Zoran D, Chrzanowski M et al (2019) Towards inter- pretable reinforcement learning using attention augmented agents. Adv Neural Inform Process Syst 32 279.</note>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main" xml:id="_69yg4xk">Programmatically interpretable reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GC6ba2A">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5045" to="5054" />
		</imprint>
	</monogr>
	<note type="raw_reference">Verma A, Murali V, Singh R, et al (2018) Programmatically inter- pretable reinforcement learning. In: International conference on machine learning, PMLR, pp 5045-5054 280.</note>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main" xml:id="_P6venWp">What did you think would happen? explaining agent behaviour through intended outcomes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VNA6amX">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="386" to="281" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yau H, Russell C, Hadfield S (2020) What did you think would happen? explaining agent behaviour through intended outcomes. Adv Neural Inform Process Syst 33:18,375-18,386 281.</note>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main" xml:id="_ghxM2Kj">Explainable reinforcement learning through a causal lens</title>
		<author>
			<persName><forename type="first">P</forename><surname>Madumal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sonenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4JkeenA">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2493" to="2500" />
		</imprint>
	</monogr>
	<note type="raw_reference">Madumal P, Miller T, Sonenberg L et al (2020) Explainable rein- forcement learning through a causal lens. In: Proceedings of the AAAI conference on artificial intelligence, pp 2493-2500 282.</note>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main" xml:id="_vtAMTSB">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" xml:id="_qaSWZbM">Adv Neural Inform Process Syst</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4765" to="4774" />
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Lundberg SM, Lee SI (2017) A unified approach to interpreting model predictions. In: Guyon I, Luxburg UV, Bengio S et al (eds) Adv Neural Inform Process Syst 30. Curran Associates, Inc., p 4765-4774 283.</note>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main" xml:id="_KfxYvhS">Explainable machine-learning predictions for the prevention of hypoxaemia during surgery</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Vavilala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZUjvkXX">Nature Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="749" to="284" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lundberg SM, Nair B, Vavilala MS et al (2018) Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nature Biomed Eng 2(10):749 284.</note>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main" xml:id="_NQ95SHD">From local explanations to global understanding with explainable AI for trees</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Erion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FTcqAQu">Nature Mach Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2522" to="5839" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lundberg SM, Erion G, Chen H et al (2020) From local explana- tions to global understanding with explainable AI for trees. Nature Mach Intell 2(1):2522-5839 285.</note>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main" xml:id="_nbzEsbY">An algorithmic perspective on imitation learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Osa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pajarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CjRgkHF">Found Trends® Robot</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="179" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Osa T, Pajarinen J, Neumann G et al (2018) An algorithmic perspective on imitation learning. Found Trends® Robot 7(1- 2):1-179 286.</note>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main" xml:id="_2JBcV2Q">Model selection for offline reinforcement learning: practical considerations for healthcare settings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Yyy7VYf">Machine learning for healthcare conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2" to="35" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tang S, Wiens J (2021) Model selection for offline reinforce- ment learning: practical considerations for healthcare settings. In: Machine learning for healthcare conference, PMLR, pp 2-35 287.</note>
</biblStruct>

<biblStruct xml:id="b282">
	<analytic>
		<title level="a" type="main" xml:id="_WxPHxs3">Reinforcement learning and causal models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tuF2V2W">The Oxford Handbook of Causal Reasoning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="295" to="288" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gershman SJ (2017) Reinforcement learning and causal models. The Oxford Handbook of Causal Reasoning 1:295 288.</note>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main" xml:id="_jUY5vRc">Deep reinforcement learning from human preferences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TRMQhAP">Adv Neural Inform Process Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">289</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Christiano PF, Leike J, Brown T et al (2017) Deep reinforcement learning from human preferences. Adv Neural Inform Process Syst 30 289.</note>
</biblStruct>

<biblStruct xml:id="b284">
	<monogr>
		<title level="m" type="main" xml:id="_JADCdnx">Fine-tuning language models from human preferences</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593290</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ziegler DM, Stiennon N, Wu J et al (2019) Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 290.</note>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main" xml:id="_jRn7raM">Human-in-the-loop reinforcement learning: a survey and position on requirements, challenges, and opportunities</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Retzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wayllace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_c7rENnK">J Artif Intell Res</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="359" to="415" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Retzlaff CO, Das S, Wayllace C et al (2024) Human-in-the-loop reinforcement learning: a survey and position on requirements, challenges, and opportunities. J Artif Intell Res 79:359-415 291.</note>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main" xml:id="_FExCUgu">Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kD6Kf7N">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3773" to="3793" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chen X, Zhong H, Yang Z, et al (2022) Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. In: International conference on machine learning, PMLR, pp 3773-3793 292.</note>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main" xml:id="_FBkVvBv">Learning to summarize with human feedback</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PnW59VD">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3008" to="3021" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Stiennon N, Ouyang L, Wu J et al (2020) Learning to summarize with human feedback. Adv Neural Inf Process Syst 33:3008-3021 293.</note>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main" xml:id="_Tge5vH8">Human-in-the-loop machine learning: a state of the art</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mosqueira-Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hernández-Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alonso-Ríos D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ns6jhXQ">Artif Intell Rev</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3005" to="3054" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mosqueira-Rey E, Hernández-Pereira E, Alonso-Ríos D et al (2023) Human-in-the-loop machine learning: a state of the art. Artif Intell Rev 56(4):3005-3054 294.</note>
</biblStruct>

<biblStruct xml:id="b289">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ndousse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m" xml:id="_bfBt3ah">Training a helpful and harmless assistant with reinforcement learning from human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Bai Y, Jones A, Ndousse K et al (2022) Training a helpful and harmless assistant with reinforcement learning from human feed- back. arXiv preprint arXiv:2204.05862</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
