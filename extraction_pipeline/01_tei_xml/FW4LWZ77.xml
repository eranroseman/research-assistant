<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_PXQYvrr">A Contextual-Bandit Approach to Personalized News Article Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2012-03-01">1 Mar 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
							<email>lihong@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
							<email>chuwei@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Langford</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>‡</label> Yahoo ! Labs</note>
								<orgName type="laboratory">Yahoo</orgName>
								<orgName type="institution">Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
							<email>schapire@cs.princeton.edu</email>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>+</label> Dept of Computer Science Princeton University</note>
								<orgName type="department">Dept of Computer Science</orgName>
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">Yahoo! Labs</note>
								<orgName type="laboratory">Yahoo! Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_n4xyq5F">A Contextual-Bandit Approach to Personalized News Article Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-03-01">1 Mar 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">2CC304348BA36B0422F214EE6124C88B</idno>
					<idno type="arXiv">arXiv:1003.0146v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_M58Cwnx">H.3.5 [Information Systems]: On-line Information Services; I.2.6 [Computing Methodologies]: Learning Algorithms</term>
					<term xml:id="_AeAxNy4">Experimentation Contextual bandit</term>
					<term xml:id="_SAhMMv2">web service</term>
					<term xml:id="_s8a5xuD">personalization</term>
					<term xml:id="_d9w8bFx">recommender systems</term>
					<term xml:id="_THJy7Th">exploration/exploitation dilemma</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_ynTAfaF"><p xml:id="_n7wdyb3"><s xml:id="_9S5dGvh">Personalized web services strive to adapt their services (advertisements, news articles, etc.) to individual users by making use of both content and user information.</s><s xml:id="_CmczG38">Despite a few recent advances, this problem remains challenging for at least two reasons.</s><s xml:id="_DG7qZcB">First, web service is featured with dynamically changing pools of content, rendering traditional collaborative filtering methods inapplicable.</s><s xml:id="_JCVJenz">Second, the scale of most web services of practical interest calls for solutions that are both fast in learning and computation.</s></p><p xml:id="_C9HQatX"><s xml:id="_Za85MEZ">In this work, we model personalized recommendation of news articles as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information about the users and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks.</s></p><p xml:id="_VveHna3"><s xml:id="_wbyD7Wt">The contributions of this work are three-fold.</s><s xml:id="_FCh4ujG">First, we propose a new, general contextual bandit algorithm that is computationally efficient and well motivated from learning theory.</s><s xml:id="_erCRMk2">Second, we argue that any bandit algorithm can be reliably evaluated offline using previously recorded random traffic.</s><s xml:id="_w3NNQvY">Finally, using this offline evaluation method, we successfully applied our new algorithm to a Yahoo!</s><s xml:id="_4SUnZkT">Front Page Today Module dataset containing over 33 million events.</s><s xml:id="_gnhBFUB">Results showed a 12.5% click lift compared to a standard context-free bandit algorithm, and the advantage becomes even greater when data gets more scarce.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_WePMfGn">INTRODUCTION</head><p xml:id="_8BUR5VZ"><s xml:id="_hVyaFCb">This paper addresses the challenge of identifying the most appropriate web-based content at the best time for individual users.</s><s xml:id="_aUgvnt6">Most * This work was done while R. Schapire visited Yahoo!</s><s xml:id="_smVUJHC">Labs.</s><s xml:id="_8USevXx"><ref type="bibr">April 26-30, 2010</ref>, Raleigh, North Carolina, USA. .</s><s xml:id="_Nd9kwKb">service vendors acquire and maintain a large amount of content in their repository, for instance, for filtering news articles <ref type="bibr" target="#b13">[14]</ref> or for the display of advertisements <ref type="bibr" target="#b4">[5]</ref>.</s><s xml:id="_uKgNNmy">Moreover, the content of such a web-service repository changes dynamically, undergoing frequent insertions and deletions.</s><s xml:id="_SV4C5Ws">In such a setting, it is crucial to quickly identify interesting content for users.</s><s xml:id="_kgQgejB">For instance, a news filter must promptly identify the popularity of breaking news, while also adapting to the fading value of existing, aging news stories.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hZVDx2U">A version of this paper appears at WWW 2010,</head><p xml:id="_TrNSCDD"><s xml:id="_JSun8hQ">It is generally difficult to model popularity and temporal changes based solely on content information.</s><s xml:id="_WbE8xGA">In practice, we usually explore the unknown by collecting consumers' feedback in real time to evaluate the popularity of new content while monitoring changes in its value <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_BNppHsf">For instance, a small amount of traffic can be designated for such exploration.</s><s xml:id="_Kqj7akK">Based on the users' response (such as clicks) to randomly selected content on this small slice of traffic, the most popular content can be identified and exploited on the remaining traffic.</s><s xml:id="_9jSHKZB">This strategy, with random exploration on an ǫ fraction of the traffic and greedy exploitation on the rest, is known as ǫ-greedy.</s><s xml:id="_pZqdAu3">Advanced exploration approaches such as EXP3 <ref type="bibr" target="#b7">[8]</ref> or UCB1 <ref type="bibr" target="#b6">[7]</ref> could be applied as well.</s><s xml:id="_yDZFTGC">Intuitively, we need to distribute more traffic to new content to learn its value more quickly, and fewer users to track temporal changes of existing content.</s></p><p xml:id="_feWX8GY"><s xml:id="_xxSa9GN">Recently, personalized recommendation has become a desirable feature for websites to improve user satisfaction by tailoring content presentation to suit individual users' needs <ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_ghAFexu">Personalization involves a process of gathering and storing user attributes, managing content assets, and, based on an analysis of current and past users' behavior, delivering the individually best content to the present user being served.</s></p><p xml:id="_hGQXkke"><s xml:id="_Ccx3pby">Often, both users and content are represented by sets of features.</s><s xml:id="_DWeRgTS">User features may include historical activities at an aggregated level as well as declared demographic information.</s><s xml:id="_Fmbdc2t">Content features may contain descriptive information and categories.</s><s xml:id="_UXUtac4">In this scenario, exploration and exploitation have to be deployed at an individual level since the views of different users on the same content can vary significantly.</s><s xml:id="_FeN5GGM">Since there may be a very large number of possible choices or actions available, it becomes critical to recognize commonalities between content items and to transfer that knowledge across the content pool.</s></p><p xml:id="_hq6TbWS"><s xml:id="_Zr4zNKk">Traditional recommender systems, including collaborative filtering, content-based filtering and hybrid approaches, can provide meaningful recommendations at an individual level by leveraging users' interests as demonstrated by their past activity.</s><s xml:id="_szYpZAA">Collaborative filtering <ref type="bibr" target="#b24">[25]</ref>, by recognizing similarities across users based on their consumption history, provides a good recommendation solution to the scenarios where overlap in historical consumption across users is relatively high and the content universe is almost static.</s><s xml:id="_ZrQtQ3S">Contentbased filtering helps to identify new items which well match an existing user's consumption profile, but the recommended items are always similar to the items previously taken by the user <ref type="bibr" target="#b19">[20]</ref>.</s><s xml:id="_SAVhybs">Hybrid approaches <ref type="bibr" target="#b10">[11]</ref> have been developed by combining two or more recommendation techniques; for example, the inability of collaborative filtering to recommend new items is commonly alleviated by combining it with content-based filtering.</s></p><p xml:id="_p23DNJe"><s xml:id="_XNAfCMU">However, as noted above, in many web-based scenarios, the content universe undergoes frequent changes, with content popularity changing over time as well.</s><s xml:id="_ctyTPwu">Furthermore, a significant number of visitors are likely to be entirely new with no historical consumption record whatsoever; this is known as a cold-start situation <ref type="bibr" target="#b20">[21]</ref>.</s><s xml:id="_5uCWRkf">These issues make traditional recommender-system approaches difficult to apply, as shown by prior empirical studies <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_wGubsZW">It thus becomes indispensable to learn the goodness of match between user interests and content when one or both of them are new.</s><s xml:id="_uDJSCxB">However, acquiring such information can be expensive and may reduce user satisfaction in the short term, raising the question of optimally balancing the two competing goals: maximizing user satisfaction in the long run, and gathering information about goodness of match between user interests and content.</s></p><p xml:id="_Z23TmdP"><s xml:id="_Fjfjpvv">The above problem is indeed known as a feature-based exploration/exploitation problem.</s><s xml:id="_yRZgQF6">In this paper, we formulate it as a contextual bandit problem, a principled approach in which a learning algorithm sequentially selects articles to serve users based on contextual information of the user and articles, while simultaneously adapting its article-selection strategy based on user-click feedback to maximize total user clicks in the long run.</s><s xml:id="_3qDJq8G">We define a bandit problem and then review some existing approaches in Section 2.</s><s xml:id="_G4ug2Gb">Then, we propose a new algorithm, LinUCB, in Section 3 which has a similar regret analysis to the best known algorithms for competing with the best linear predictor, with a lower computational overhead.</s><s xml:id="_QsfQb6K">We also address the problem of offline evaluation in Section 4, showing this is possible for any explore/exploit strategy when interactions are independent and identically distributed (i.i.d.), as might be a reasonable assumption for different users.</s><s xml:id="_tsaWZV2">We then test our new algorithm and several existing algorithms using this offline evaluation strategy in Section 5.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_7XbbS52">FORMULATION &amp; RELATED WORK</head><p xml:id="_T63Esuk"><s xml:id="_4seKQ5T">In this section, we define the K-armed contextual bandit problem formally, and as an example, show how it can model the personalized news article recommendation problem.</s><s xml:id="_DeXbg4d">We then discuss existing methods and their limitations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_2GTrKRT">A Multi-armed Bandit Formulation</head><p xml:id="_xfvKfnz"><s xml:id="_hARTcXJ">The problem of personalized news article recommendation can be naturally modeled as a multi-armed bandit problem with context information.</s><s xml:id="_GJZbp5z">Following previous work <ref type="bibr" target="#b17">[18]</ref>, we call it a contextual bandit. <ref type="foot" target="#foot_0">1</ref> Formally, a contextual-bandit algorithm A proceeds in discrete trials t = 1, 2, 3, . . .</s><s xml:id="_utYMsRz">In trial t:</s></p><p xml:id="_A5zyTyd"><s xml:id="_fY95HkA">1.</s><s xml:id="_Sr3uDKr">The algorithm observes the current user ut and a set At of arms or actions together with their feature vectors xt,a for a ∈ At.</s><s xml:id="_N2SSUDp">The vector xt,a summarizes information of both the user ut and arm a, and will be referred to as the context.</s><s xml:id="_CP6ZdgW">2. Based on observed payoffs in previous trials, A chooses an arm at ∈ At, and receives payoff rt,a t whose expectation depends on both the user ut and the arm at. 3. The algorithm then improves its arm-selection strategy with the new observation, (xt,a t , at, rt,a t ).</s><s xml:id="_Rh62U9x">It is important to em-phasize here that no feedback (namely, the payoff rt,a) is observed for unchosen arms a = at.</s><s xml:id="_fg6Ftwh">The consequence of this fact is discussed in more details in the next subsection.</s><s xml:id="_STBNXPS">In the process above, the total T -trial payoff of A is defined as T t=1 rt,a t .</s><s xml:id="_JCWSRUv">Similarly, we define the optimal expected T -trial payoff as E T t=1 r t,a * t , where a * t is the arm with maximum expected payoff at trial t.</s><s xml:id="_bw2GX25">Our goal is to design A so that the expected total payoff above is maximized.</s><s xml:id="_RZtFUf6">Equivalently, we may find an algorithm so that its regret with respect to the optimal arm-selection strategy is minimized.</s><s xml:id="_qVnc8T5">Here, the T -trial regret R A (T ) of algorithm A is defined formally by</s></p><formula xml:id="formula_0">R A (T ) def = E T t=1 r t,a * t -E T t=1 rt,a t .<label>(1)</label></formula><p xml:id="_hHXhahn"><s xml:id="_ayJU39E">An important special case of the general contextual bandit problem is the well-known K-armed bandit in which (i) the arm set At remains unchanged and contains K arms for all t, and (ii) the user ut (or equivalently, the context</s></p><formula xml:id="formula_1">(xt,1, • • • , xt,K))</formula><p xml:id="_t66j2TM"><s xml:id="_vm6pF7U">is the same for all t.</s><s xml:id="_MterBXQ">Since both the arm set and contexts are constant at every trial, they make no difference to a bandit algorithm, and so we will also refer to this type of bandit as a context-free bandit.</s></p><p xml:id="_HJu9HcU"><s xml:id="_4775kPB">In the context of article recommendation, we may view articles in the pool as arms.</s><s xml:id="_ydJrnVe">When a presented article is clicked, a payoff of 1 is incurred; otherwise, the payoff is 0. With this definition of payoff, the expected payoff of an article is precisely its clickthrough rate (CTR), and choosing an article with maximum CTR is equivalent to maximizing the expected number of clicks from users, which in turn is the same as maximizing the total expected payoff in our bandit formulation.</s></p><p xml:id="_37E8CxN"><s xml:id="_EUBdaHs">Furthermore, in web services we often have access to user information which can be used to infer a user's interest and to choose news articles that are probably most interesting to her.</s><s xml:id="_FSzWFJ5">For example, it is much more likely for a male teenager to be interested in an article about iPod products rather than retirement plans.</s><s xml:id="_e57hGX7">Therefore, we may "summarize" users and articles by a set of informative features that describe them compactly.</s><s xml:id="_sKuQpbT">By doing so, a bandit algorithm can generalize CTR information from one article/user to another, and learn to choose good articles more quickly, especially for new users and articles.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_gvapfmb">Existing Bandit Algorithms</head><p xml:id="_aX3k8Xf"><s xml:id="_QrFM2m3">The fundamental challenge in bandit problems is the need for balancing exploration and exploitation.</s><s xml:id="_gfkEKfv">To minimize the regret in Eq. ( <ref type="formula" target="#formula_0">1</ref>), an algorithm A exploits its past experience to select the arm that appears best.</s><s xml:id="_Z83PSEg">On the other hand, this seemingly optimal arm may in fact be suboptimal, due to imprecision in A's knowledge.</s><s xml:id="_yeYjJZK">In order to avoid this undesired situation, A has to explore by actually choosing seemingly suboptimal arms so as to gather more information about them (c.f., step 3 in the bandit process defined in the previous subsection).</s><s xml:id="_mHXuWCe">Exploration can increase short-term regret since some suboptimal arms may be chosen.</s><s xml:id="_Z5eZfpJ">However, obtaining information about the arms' average payoffs (i.e., exploration) can refine A's estimate of the arms' payoffs and in turn reduce long-term regret.</s><s xml:id="_RmXY5mu">Clearly, neither a purely exploring nor a purely exploiting algorithm works best in general, and a good tradeoff is needed.</s></p><p xml:id="_vSXtzXe"><s xml:id="_c7NhfBA">The context-free K-armed bandit problem has been studied by statisticians for a long time <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>.</s><s xml:id="_Qs79RGr">One of the simplest and most straightforward algorithms is ǫ-greedy.</s><s xml:id="_87ZNbTF">In each trial t, this algorithm first estimates the average payoff μt,a of each arm a.</s><s xml:id="_mcaY4tg">Then, with probability 1ǫ, it chooses the greedy arm (i.e., the arm with highest payoff estimate); with probability ǫ, it chooses a random arm.</s><s xml:id="_aVqKUjb">In the limit, each arm will be tried infinitely often, and so the payoff estimate μt,a converges to the true value µa with probability 1.</s><s xml:id="_yzhBT7G">Furthermore, by decaying ǫ appropriately (e.g., <ref type="bibr" target="#b23">[24]</ref>), the per-step regret, R A (T )/T , converges to 0 with probability 1.</s></p><p xml:id="_KBpUNwk"><s xml:id="_CDqvq6q">In contrast to the unguided exploration strategy adopted by ǫgreedy, another class of algorithms generally known as upper confidence bound algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref> use a smarter way to balance exploration and exploitation.</s><s xml:id="_AJ64FWV">Specifically, in trial t, these algorithms estimate both the mean payoff μt,a of each arm a as well as a corresponding confidence interval ct,a, so that |μt,a -µa| &lt; ct,a holds with high probability.</s><s xml:id="_TThzzSd">They then select the arm that achieves a highest upper confidence bound (UCB for short): at = arg maxa (μt,a + ct,a).</s><s xml:id="_FGYZrEb">With appropriately defined confidence intervals, it can be shown that such algorithms have a small total Ttrial regret that is only logarithmic in the total number of trials T , which turns out to be optimal <ref type="bibr" target="#b16">[17]</ref>.</s></p><p xml:id="_VSWMBR3"><s xml:id="_u7JK7GQ">While context-free K-armed bandits are extensively studied and well understood, the more general contextual bandit problem has remained challenging.</s><s xml:id="_gAQ7EAv">The EXP4 algorithm <ref type="bibr" target="#b7">[8]</ref> uses the exponential weighting technique to achieve an Õ( √ T ) regret,<ref type="foot" target="#foot_1">foot_1</ref> but the computational complexity may be exponential in the number of features.</s><s xml:id="_FWYFy8G">Another general contextual bandit algorithm is the epochgreedy algorithm <ref type="bibr" target="#b17">[18]</ref> that is similar to ǫ-greedy with shrinking ǫ.</s><s xml:id="_S7XWSxn">This algorithm is computationally efficient given an oracle optimizer but has the weaker regret guarantee of Õ(T 2/3 ).</s></p><p xml:id="_6DzRaNK"><s xml:id="_nPTeAMn">Algorithms with stronger regret guarantees may be designed under various modeling assumptions about the bandit.</s><s xml:id="_ZmAQ9wS">Assuming the expected payoff of an arm is linear in its features, Auer <ref type="bibr" target="#b5">[6]</ref> describes the LinRel algorithm that is essentially a UCB-type approach and shows that one of its variants has a regret of Õ( √ T ), a significant improvement over earlier algorithms <ref type="bibr" target="#b0">[1]</ref>.</s></p><p xml:id="_n3X7qkB"><s xml:id="_qtz9UV9">Finally, we note that there exist another class of bandit algorithms based on Bayes rule, such as Gittins index methods <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_TR6bQMg">With appropriately defined prior distributions, Bayesian approaches may have good performance.</s><s xml:id="_FZaV7F4">These methods require extensive offline engineering to obtain good prior models, and are often computationally prohibitive without coupling with approximation techniques <ref type="bibr" target="#b1">[2]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_fWDFqNQ">ALGORITHM</head><p xml:id="_B69ZnbH"><s xml:id="_yAEWyMG">Given asymptotic optimality and the strong regret bound of UCB methods for context-free bandit algorithms, it is tempting to devise similar algorithms for contextual bandit problems.</s><s xml:id="_cgv6UUD">Given some parametric form of payoff function, a number of methods exist to estimate from data the confidence interval of the parameters with which we can compute a UCB of the estimated arm payoff.</s><s xml:id="_gyJEwTc">Such an approach, however, is expensive in general.</s></p><p xml:id="_BRABnQ2"><s xml:id="_Eu4qRpA">In this work, we show that a confidence interval can be computed efficiently in closed form when the payoff model is linear, and call this algorithm LinUCB.</s><s xml:id="_6VBFQUd">For convenience of exposition, we first describe the simpler form for disjoint linear models, and then consider the general case of hybrid models in Section 3.2.</s><s xml:id="_6TFFZsZ">We note LinUCB is a generic contextual bandit algorithms which applies to applications other than personalized news article recommendation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_eD6aUqr">LinUCB with Disjoint Linear Models</head><p xml:id="_XHpnNCs"><s xml:id="_D2D4UHx">Using the notation of Section 2.1, we assume the expected payoff of an arm a is linear in its d-dimensional feature xt,a with some unknown coefficient vector θ θ θ * a ; namely, for all t,</s></p><formula xml:id="formula_2">E[rt,a|xt,a] = x ⊤ t,a θ θ θ * a . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p xml:id="_QXbZZ5c"><s xml:id="_eJDnjTg">This model is called disjoint since the parameters are not shared among different arms.</s><s xml:id="_sYaMEE7">Let Da be a design matrix of dimension m × d at trial t, whose rows correspond to m training inputs (e.g., m contexts that are observed previously for article a), and ba ∈ R m be the corresponding response vector (e.g., the corresponding m click/no-click user feedback).</s><s xml:id="_AC74pRC">Applying ridge regression to the training data (Da, ca) gives an estimate of the coefficients:</s></p><formula xml:id="formula_4">θ θ θa = (D ⊤ a Da + I d ) -1 D ⊤ a ca,<label>(3)</label></formula><p xml:id="_fAWArE4"><s xml:id="_R4gFtJJ">where I d is the d × d identity matrix.</s><s xml:id="_4W5Xbaa">When components in ca are independent conditioned on corresponding rows in Da, it can be shown <ref type="bibr" target="#b26">[27]</ref> that, with probability at least 1δ,</s></p><formula xml:id="formula_5">x ⊤ t,a θ θ θa -E[rt,a|xt,a] ≤ α x ⊤ t,a (D ⊤ a Da + I d ) -1 xt,a<label>(4)</label></formula><p xml:id="_Zu9KHyU"><s xml:id="_6ckDYsU">for any δ &gt; 0 and xt,a ∈ R d , where α = 1 + ln(2/δ)/2 is a constant.</s><s xml:id="_NkRT99D">In other words, the inequality above gives a reasonably tight UCB for the expected payoff of arm a, from which a UCBtype arm-selection strategy can be derived: at each trial t, choose</s></p><formula xml:id="formula_6">at def = arg max a∈At x ⊤ t,a θ θ θa + α x ⊤ t,a A -1 a xt,a ,<label>(5)</label></formula><p xml:id="_SZygMEf"><s xml:id="_JpUg9MP">where</s></p><formula xml:id="formula_7">Aa def = D ⊤ a Da + I d .</formula><p xml:id="_tduaSGJ"><s xml:id="_xyh4M93">The confidence interval in Eq. ( <ref type="formula" target="#formula_5">4</ref>) may be motivated and derived from other principles.</s><s xml:id="_fbdHFWF">For instance, ridge regression can also be interpreted as a Bayesian point estimate, where the posterior distribution of the coefficient vector, denoted as p(θ θ θa), is Gaussian with mean θ θ θa and covariance A -1 a .</s><s xml:id="_Zca7M3A">Given the current model, the predictive variance of the expected payoff x ⊤ t,a θ θ θ * a is evaluated as x ⊤ t,a A -1 a xt,a, and then x ⊤ t,a A -1 a xt,a becomes the standard deviation.</s><s xml:id="_EPcTR9m">Furthermore, in information theory <ref type="bibr" target="#b18">[19]</ref>, the differential entropy of p(θ θ θa) is defined as - 1  2 ln((2π) d det Aa).</s><s xml:id="_CH2nDK3">The entropy of p(θ θ θa) when updated by the inclusion of the new point xt,a then becomes - 1  2 ln((2π) d det (Aa + xt,ax ⊤ t,a )).</s><s xml:id="_mYay2Bh">The entropy reduction in the model posterior is 1  2 ln(1 + x ⊤ t,a A -1 a xt,a).</s><s xml:id="_eW2jpPj">This quantity is often used to evaluate model improvement contributed from xt,a.</s><s xml:id="_DxJ3yfp">Therefore, the criterion for arm selection in Eq. ( <ref type="formula" target="#formula_6">5</ref>) can also be regarded as an additive trade-off between the payoff estimate and model uncertainty reduction.</s></p><p xml:id="_68p5EKm"><s xml:id="_GrpP9hd">Algorithm 1 gives a detailed description of the entire LinUCB algorithm, whose only input parameter is α.</s><s xml:id="_wWb8jRh">Note the value of α given in Eq. ( <ref type="formula" target="#formula_5">4</ref>) may be conservatively large in some applications, and so optimizing this parameter may result in higher total payoffs in practice.</s><s xml:id="_VKcYFYS">Like all UCB methods, LinUCB always chooses the arm with highest UCB (as in Eq. ( <ref type="formula" target="#formula_6">5</ref>)).</s></p><p xml:id="_UvanNSA"><s xml:id="_CZwGjGU">This algorithm has a few nice properties.</s><s xml:id="_KTBC4SU">First, its computational complexity is linear in the number of arms and at most cubic in the number of features.</s><s xml:id="_vHTSVAY">To decrease computation further, we may update Aa t in every step (which takes O(d 2 ) time), but compute and cache Qa</s></p><formula xml:id="formula_8">def = A -1 a</formula><p xml:id="_rRfnxae"><s xml:id="_FK5zxZq">(for all a) periodically instead of in realtime.</s><s xml:id="_ykXJUGa">Second, the algorithm works well for a dynamic arm set, and remains efficient as long as the size of At is not too large.</s><s xml:id="_cu53GnK">This case is true in many applications.</s><s xml:id="_BVwNbst">In news article recommendation, for instance, editors add/remove articles to/from a pool and the pool size remains essentially constant.</s><s xml:id="_7chR8Tm">Third, although it is not the focus of the present paper, we can adapt the analysis from <ref type="bibr" target="#b5">[6]</ref> to show the following: if the arm set At is fixed and contains K arms, then the confidence interval (i.e., the right-hand side of Eq. ( <ref type="formula" target="#formula_5">4</ref>)) decreases fast enough with more and more data, and then prove the strong regret bound of Õ( √ KdT ), matching the state-of-the-art result <ref type="bibr" target="#b5">[6]</ref> for bandits satisfying Eq. ( <ref type="formula" target="#formula_2">2</ref>).</s><s xml:id="_7kWFUgu">These theoretical results indicate fundamental soundness and efficiency of the algorithm.</s></p><p xml:id="_af6s29y"><s xml:id="_8V8m6pj">Algorithm 1 LinUCB with disjoint linear models.</s><s xml:id="_YWpskwA">0: Inputs: α ∈ R+ 1: for t = 1, 2, 3, . . .</s><s xml:id="_GaU3nSG">, T do 2:</s></p><p xml:id="_96XwVrm"><s xml:id="_rfUprZy">Observe features of all arms a ∈ At: xt,a ∈ R d 3:</s></p><p xml:id="_WW8tkmH"><s xml:id="_GHWxdxQ">for all a ∈ At do 4:</s></p><p xml:id="_gsAanSn"><s xml:id="_JGJHWGN">if a is new then 5:</s></p><p xml:id="_yaGrjRX"><s xml:id="_SRbsqJf">Aa ← I d (d-dimensional identity matrix) 6:</s></p><p xml:id="_P578WwV"><s xml:id="_WeBQs4p">ba ← 0 d×1 (d-dimensional zero vector) 7:</s></p><formula xml:id="formula_9">end if 8: θ θ θa ← A -1 a ba 9: pt,a ← θ θ θ ⊤ a xt,a + α x ⊤ t,a A -1 a xt,a 10:</formula><p xml:id="_EcGTmx9"><s xml:id="_4NRAKnA">end for 11:</s></p><p xml:id="_vdh4Xfw"><s xml:id="_EBbMVcV">Choose arm at = arg maxa∈A t pt,a with ties broken arbitrarily, and observe a real-valued payoff rt 12:</s></p><formula xml:id="formula_10">Aa t ← Aa t + xt,a t x ⊤ t,at</formula><p xml:id="_8BBe8YB"><s xml:id="_BxwMPyQ">13: ba t ← ba t + rtxt,a t 14: end for Finally, we note that, under the assumption that input features xt,a were drawn i.i.d.</s><s xml:id="_KEvxkBP">from a normal distribution (in addition to the modeling assumption in Eq. ( <ref type="formula" target="#formula_2">2</ref>)), Pavlidis et al. <ref type="bibr" target="#b21">[22]</ref> came up with a similar algorithm that uses a least-squares solution θ θ θa instead of our ridge-regression solution ( θ θ θa in Eq. ( <ref type="formula" target="#formula_4">3</ref>)) to compute the UCB.</s><s xml:id="_mQ7ptJk">However, our approach (and theoretical analysis) is more general and remains valid even when input features are nonstationary.</s><s xml:id="_dnt4tgZ">More importantly, we will discuss in the next section how to extend the basic Algorithm 1 to a much more interesting case not covered by Pavlidis et al.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_QuBJa5E">LinUCB with Hybrid Linear Models</head><p xml:id="_j4J6qCb"><s xml:id="_63aYURt">Algorithm 1 (or the similar algorithm in <ref type="bibr" target="#b21">[22]</ref>) computes the inverse of the matrix, D ⊤ a Da + I d (or D ⊤ a Da), where Da is again the design matrix with rows corresponding to features in the training data.</s><s xml:id="_eGh792A">These matrices of all arms have fixed dimension d × d, and can be updated efficiently and incrementally.</s><s xml:id="_QQmY9N9">Moreover, their inverses can be computed easily as the parameters in Algorithm 1 are disjoint: the solution θ θ θa in Eq. ( <ref type="formula" target="#formula_4">3</ref>) is not affected by training data of other arms, and so can be computed separately.</s><s xml:id="_29QbSCQ">We now consider the more interesting case with hybrid models.</s></p><p xml:id="_S5pZ6RX"><s xml:id="_DzrqxSc">In many applications including ours, it is helpful to use features that are shared by all arms, in addition to the arm-specific ones.</s><s xml:id="_wPRJWn3">For example, in news article recommendation, a user may prefer only articles about politics for which this provides a mechanism.</s><s xml:id="_eG9VVT8">Hence, it is helpful to have features that have both shared and non-shared components.</s><s xml:id="_NA7XMMn">Formally, we adopt the following hybrid model by adding another linear term to the right-hand side of Eq. ( <ref type="formula" target="#formula_2">2</ref>):</s></p><formula xml:id="formula_11">E[rt,a|xt,a] = z ⊤ t,a β β β * + x ⊤ t,a θ θ θ * a ,<label>(6)</label></formula><p xml:id="_4GuaKdn"><s xml:id="_WfMmP3J">where zt,a ∈ R k is the feature of the current user/article combination, and β β β * is an unknown coefficient vector common to all arms.</s><s xml:id="_jAMWZya">This model is hybrid in the sense that some of the coefficients β β β * are shared by all arms, while others θ θ θ * a are not.</s><s xml:id="_ASthzhA">For hybrid models, we can no longer use Algorithm 1 as the confidence intervals of various arms are not independent due to the shared features.</s><s xml:id="_cRRm975">Fortunately, there is an efficient way to compute an UCB along the same line of reasoning as in the previous section.</s><s xml:id="_cSSrNcr">The derivation relies heavily on block matrix inversion techniques.</s><s xml:id="_3NTKMpA">Due to space limitation, we only give the pseudocode in Algorithm 2 (where lines 5 and 12 compute the ridge-regression solution of the coefficients, and line 13 computes the confidence interval), and leave detailed derivations to a full paper.</s><s xml:id="_jvTsH58">Here, we Algorithm 2 LinUCB with hybrid linear models.</s><s xml:id="_gPTPAAd">0: Inputs: α ∈ R+ 1: A0 ← I k (k-dimensional identity matrix) 2: b0 ← 0 k (k-dimensional zero vector) 3: for t = 1, 2, 3, . . .</s><s xml:id="_2BvydWr">, T do 4:</s></p><p xml:id="_ATSUNTM"><s xml:id="_Y8gY7DW">Observe features of all arms a ∈ At: (zt,a, xt,a) ∈ R k+d 5:</s></p><formula xml:id="formula_12">β β β ← A -1 0 b0 6:</formula><p xml:id="_kHC38hN"><s xml:id="_PVanhRH">for all a ∈ At do 7:</s></p><p xml:id="_kBRHAyC"><s xml:id="_ju78sCQ">if a is new then 8:</s></p><p xml:id="_78zumnQ"><s xml:id="_qfdpPFP">Aa ← I d (d-dimensional identity matrix) 9:</s></p><p xml:id="_XpdK2eC"><s xml:id="_GJGUpua">Ba ← 0 d×k (d-by-k zero matrix) 10:</s></p><p xml:id="_dP4DYhd"><s xml:id="_eNgHZgt">ba ← 0 d×1 (d-dimensional zero vector) 11:</s></p><formula xml:id="formula_13">end if 12: θ θ θa ← A -1 a ba -Ba β β β 13: st,a ← z ⊤ t,a A -1 0 zt,a -2z ⊤ t,a A -1 0 B ⊤ a A -1 a xt,a + x ⊤ t,a A -1 a xt,a + x ⊤ t,a A -1 a BaA -1 0 B ⊤ a A -1 a xt,a 14: pt,a ← z ⊤ t,a β β β + x ⊤ t,a θ θ θa + α √ st,</formula><p xml:id="_NvfjK3s"><s xml:id="_5zxNhXJ">a 15: end for 16: Choose arm at = arg maxa∈A t pt,a with ties broken arbitrarily, and observe a real-valued payoff rt 17: A0 ← A0 + B ⊤ at A -1 at Ba t 18: b0 ← b0 + B ⊤ at A -1 at ba t 19: Aa t ← Aa t + xt,a t x ⊤ t,at 20: Ba t ← Ba t + xt,a t z ⊤ t,at 21: ba t ← ba t + rtxt,a t 22:</s></p><formula xml:id="formula_14">A0 ← A0 + zt,a t z ⊤ t,at -B ⊤ at A -1 at Ba t 23: b0 ← b0 + rtzt,a t -B ⊤ at A -1</formula><p xml:id="_9jZDgpA"><s xml:id="_rBSFpsa">at ba t 24: end for only point out the important fact that the algorithm is computationally efficient since the building blocks in the algorithm (A0, b0, Aa, Ba, and ba) all have fixed dimensions and can be updated incrementally.</s><s xml:id="_wHbuCuR">Furthermore, quantities associated with arms not existing in At no longer get involved in the computation.</s><s xml:id="_2KrQNJS">Finally, we can also compute and cache the inverses (A -1 0 and A -1 a ) periodically instead of at the end of each trial to reduce the per-trial computational complexity to O(d 2 + k 2 ).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_3eJmuuh">EVALUATION METHODOLOGY</head><p xml:id="_hcgBhwB"><s xml:id="_zKRFuyv">Compared to machine learning in the more standard supervised setting, evaluation of methods in a contextual bandit setting is frustratingly difficult.</s><s xml:id="_VJkDdGS">Our goal here is to measure the performance of a bandit algorithm π, that is, a rule for selecting an arm at each time step based on the preceding interactions (such as the algorithms described above).</s><s xml:id="_r34kxFj">Because of the interactive nature of the problem, it would seem that the only way to do this is to actually run the algorithm on "live" data.</s><s xml:id="_K7DJPVK">However, in practice, this approach is likely to be infeasible due to the serious logistical challenges that it presents.</s><s xml:id="_GBJ92E4">Rather, we may only have offline data available that was collected at a previous time using an entirely different logging policy.</s><s xml:id="_92rJbVY">Because payoffs are only observed for the arms chosen by the logging policy, which are likely to often differ from those chosen by the algorithm π being evaluated, it is not at all clear how to evaluate π based only on such logged data.</s><s xml:id="_UVz6XSc">This evaluation problem may be viewed as a special case of the so-called "off-policy evaluation problem" in reinforcement learning (see, c.f., <ref type="bibr" target="#b22">[23]</ref>).</s></p><p xml:id="_McppGq4"><s xml:id="_RRUeBZV">One solution is to build a simulator to model the bandit process from the logged data, and then evaluate π with the simulator.</s><s xml:id="_PRTykVw">However, the modeling step will introduce bias in the simulator and so make it hard to justify the reliability of this simulator-based evalu-ation approach.</s><s xml:id="_BfMp6yc">In contrast, we propose an approach that is simple to implement, grounded on logged data, and unbiased.</s></p><p xml:id="_9nVYhSr"><s xml:id="_RNAgwpK">In this section, we describe a provably reliable technique for carrying out such an evaluation, assuming that the individual events are i.i.d., and that the logging policy that was used to gather the logged data chose each arm at each time step uniformly at random.</s><s xml:id="_aX4NQ8e">Although we omit the details, this latter assumption can be weakened considerably so that any randomized logging policy is allowed and our solution can be modified accordingly using rejection sampling, but at the cost of decreased efficiency in using data.</s></p><p xml:id="_4qH6zPt"><s xml:id="_RM9hyye">More precisely, we suppose that there is some unknown distribution D from which tuples are drawn i.i.d. of the form (x1, ..., xK, r1, . . .</s><s xml:id="_qphgZxy">, rK), each consisting of observed feature vectors and hidden payoffs for all arms.</s><s xml:id="_KMv39rf">We also posit access to a large sequence of logged events resulting from the interaction of the logging policy with the world.</s><s xml:id="_z36jmqS">Each such event consists of the context vectors x1, ..., xK , a selected arm a and the resulting observed payoff ra.</s><s xml:id="_uwM3V3E">Crucially, only the payoff ra is observed for the single arm a that was chosen uniformly at random.</s><s xml:id="_rvctbgX">For simplicity of presentation, we take this sequence of logged events to be an infinitely long stream; however, we also give explicit bounds on the actual finite number of events required by our evaluation method.</s></p><p xml:id="_v6zWsTF"><s xml:id="_BZDYjsj">Our goal is to use this data to evaluate a bandit algorithm π.</s><s xml:id="_jvDjn69">Formally, π is a (possibly randomized) mapping for selecting the arm at at time t based on the history ht-1 of t-1 preceding events, together with the current context vectors xt1, ..., xtK.</s></p><p xml:id="_JmTxYkf"><s xml:id="_9JyCFqS">Our proposed policy evaluator is shown in Algorithm 3. The method takes as input a policy π and a desired number of "good" events T on which to base the evaluation.</s><s xml:id="_RaMVyWf">We then step through the stream of logged events one by one.</s><s xml:id="_NSyTxKN">If, given the current history ht-1, it happens that the policy π chooses the same arm a as the one that was selected by the logging policy, then the event is retained, that is, added to the history, and the total payoff Rt updated.</s><s xml:id="_AzC9G4u">Otherwise, if the policy π selects a different arm from the one that was taken by the logging policy, then the event is entirely ignored, and the algorithm proceeds to the next event without any other change in its state.</s></p><p xml:id="_C9RfpDK"><s xml:id="_APZb2U8">Note that, because the logging policy chooses each arm uniformly at random, each event is retained by this algorithm with probability exactly 1/K, independent of everything else.</s><s xml:id="_uzCd2ja">This means that the events which are retained have the same distribution as if they were selected by D. As a result, we can prove that two processes are equivalent: the first is evaluating the policy against T real-world events from D, and the second is evaluating the policy using the policy evaluator on a stream of logged events.</s><s xml:id="_wsYmefs">THEOREM 1.</s><s xml:id="_bkMPhJT">For all distributions D of contexts, all policies π, all T , and all sequences of events hT ,</s></p><formula xml:id="formula_15">Pr Policy_Evaluator(π,S) (hT ) = Pr π,D<label>(hT )</label></formula><p xml:id="_aMRupGV"><s xml:id="_6H8EuUd">where S is a stream of events drawn i.i.d.</s><s xml:id="_zNVPvsK">from a uniform random logging policy and D. Furthermore, the expected number of events obtained from the stream to gather a history hT of length T is KT .</s></p><p xml:id="_6UjSrt7"><s xml:id="_WVtW7vc">This theorem says that every history hT has the identical probability in the real world as in the policy evaluator.</s><s xml:id="_vEHnUKz">Many statistics of these histories, such as the average payoff RT /T returned by Algorithm 3, are therefore unbiased estimates of the value of the algorithm π.</s><s xml:id="_p99j4ZV">Further, the theorem states that KT logged events are required, in expectation, to retain a sample of size T .</s></p><p xml:id="_KXkjRr6"><s xml:id="_EGMYGRg">PROOF.</s><s xml:id="_5hTCbgs">The proof is by induction on t = 1, . . .</s><s xml:id="_tfbsTEJ">, T starting with a base case of the empty history which has probability 1 when t = 0 Algorithm 3 Policy_Evaluator.</s><s xml:id="_pkMQDpk">0: Inputs: T &gt; 0; policy π; stream of events 1: h0 ← ∅ {An initially empty history} 2: R0 ← 0 {An initially zero total payoff} 3: for t = 1, 2, 3, . . .</s><s xml:id="_gH3atcV">, T do 4: repeat 5:</s></p><p xml:id="_Kb22pkT"><s xml:id="_DpAbgrR">Get next event (x1, ..., xK , a, ra) 6:</s></p><p xml:id="_RFkXFzr"><s xml:id="_mG2KNXg">until π(ht-1, (x1, ..., xK)) = a 7:</s></p><p xml:id="_zp6ErPw"><s xml:id="_W98gMzQ">ht ← CONCATENATE(ht-1, (x1, ..., xK , a, ra)) 8:</s></p><p xml:id="_FaXDEGA"><s xml:id="_uMsEMvF">Rt ← Rt-1 + ra 9: end for 10: Output: RT /T under both methods of evaluation.</s><s xml:id="_AmDWkgt">In the inductive case, assume that we have for all t -1:</s></p><formula xml:id="formula_16">Pr Policy_Evaluator(π,S) (ht-1) = Pr π,D (ht-1)</formula><p xml:id="_cdxz3cn"><s xml:id="_8H2s9B5">and want to prove the same statement for any history ht.</s><s xml:id="_dHtgWyt">Since the data is i.i.d. and any randomization in the policy is independent of randomization in the world, we need only prove that conditioned on the history ht-1 the distribution over the t-th event is the same for each process.</s><s xml:id="_xH4jHbU">In other words, we must show:</s></p><formula xml:id="formula_17">Pr Policy_Evaluator(π,S) ((xt,1, ..., xt,K, a, rt,a) | ht-1) = Pr D (xt,1, ..., xt,K, rt,a) Pr π(h t-1 ) (a | xt,1, ..., xt,K).</formula><p xml:id="_J6zzz93"><s xml:id="_DTnhwwa">Since the arm a is chosen uniformly at random in the logging policy, the probability that the policy evaluator exits the inner loop is identical for any policy, any history, any features, and any arm, implying this happens for the last event with the probability of the last event, PrD(xt,1, ..., xt,K, rt,a).</s><s xml:id="_8xHXNu3">Similarly, since the policy π's distribution over arms is independent conditioned on the history ht-1 and features (xt,1, ..., xt,K ), the probability of arm a is just Pr π(h t-1 ) (a|xt,1, ..., xt,K ).</s><s xml:id="_rgVBJm5">Finally, since each event from the stream is retained with probability exactly 1/K, the expected number required to retain T events is exactly KT .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_aFUSqZc">EXPERIMENTS</head><p xml:id="_wQzhXR8"><s xml:id="_HV5YXqE">In this section, we verify the capacity of the proposed LinUCB algorithm on a real-world application using the offline evaluation method of Section 4. We start with an introduction of the problem setting in Yahoo!</s><s xml:id="_4UugFnU">Today-Module, and then describe the user/item attributes we used in experiments.</s><s xml:id="_xTaj5Zg">Finally, we define performance metrics and report experimental results with comparison to a few standard (contextual) bandit algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_pPtXzHX">Yahoo! Today Module</head><p xml:id="_mxprGQq"><s xml:id="_sbeTFr9">The Today Module is the most prominent panel on the Yahoo!</s><s xml:id="_jRbT89u">Front Page, which is also one of the most visited pages on the Internet; see a snapshot in Figure <ref type="figure" target="#fig_0">1</ref>.</s><s xml:id="_pXN9WeK">The default "Featured" tab in the Today Module highlights one of four high-quality articles, mainly news, while the four articles are selected from an hourly-refreshed article pool curated by human editors.</s><s xml:id="_m3vzgEJ">As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, there are four articles at footer positions, indexed by F1-F4.</s><s xml:id="_QtKestp">Each article is represented by a small picture and a title.</s><s xml:id="_p68PB57">One of the four articles is highlighted at the story position, which is featured by a large picture, a title and a short summary along with related links.</s><s xml:id="_8tT3tMV">By default, the article at F1 is highlighted at the story position.</s><s xml:id="_nZSXwPA">A user can click on the highlighted article at the story position to read more details if she is interested in the article.</s><s xml:id="_nNQDYhE">The event is recorded as a story click.</s><s xml:id="_gPTngHR">To draw visitors' attention, we would like to rank available articles according to individual interests, and highlight the most attractive article for each visitor at the story position.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_7Yjb2Ua">Experiment Setup</head><p xml:id="_qUMbFfg"><s xml:id="_de2j6FU">This subsection gives a detailed description of our experimental setup, including data collection, feature construction, performance evaluation, and competing algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1" xml:id="_eWNYr2X">Data Collection</head><p xml:id="_kr2n4DN"><s xml:id="_aFF5GgK">We collected events from a random bucket in May 2009.</s><s xml:id="_DCc4Mww">Users were randomly selected to the bucket with a certain probability per visiting view. <ref type="foot" target="#foot_2">3</ref></s><s xml:id="_8HEsUfg">In this bucket, articles were randomly selected from the article pool to serve users.</s><s xml:id="_x4BsFUR">To avoid exposure bias at footer positions, we only focused on users' interactions with F1 articles at the story position.</s><s xml:id="_tAXZqfG">Each user interaction event consists of three components: (i) the random article chosen to serve the user, (ii) user/article information, and (iii) whether the user clicks on the article at the story position.</s><s xml:id="_DPeDpDX">Section 4 shows these random events can be used to reliably evaluate a bandit algorithm's expected payoff.</s></p><p xml:id="_2VGKCuv"><s xml:id="_z9Fcfb7">There were about 4.7 million events in the random bucket on May 01.</s><s xml:id="_vb3khZ6">We used this day's events (called "tuning data") for model validation to decide the optimal parameter for each competing bandit algorithm.</s><s xml:id="_XWQuVWW">Then we ran these algorithms with tuned parameters on a one-week event set (called "evaluation data") in the random bucket from May 03-09, which contained about 36 million events.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2" xml:id="_cBKfr4e">Feature Construction</head><p xml:id="_b7JbJzX"><s xml:id="_7uQzDqA">We now describe the user/article features constructed for our experiments.</s><s xml:id="_UEb7Ywj">Two sets of features for the disjoint and hybrid models, respectively, were used to test the two forms of LinUCB in Section 3 and to verify our conjecture that hybrid models can improve learning speed.</s></p><p xml:id="_qKTvbEv"><s xml:id="_XxUd6ts">We start with raw user features that were selected by "support".</s><s xml:id="_9dvzWCJ">The support of a feature is the fraction of users having that feature.</s><s xml:id="_ykTQZcX">To reduce noise in the data, we only selected features with high support.</s><s xml:id="_ZPAK6YF">Specifically, we used a feature when its support is at least 0.1.</s><s xml:id="_6BGmKuX">Then, each user was originally represented by a raw feature vector of over 1000 categorical components, which include: (i) demographic information: gender (2 classes) and age discretized into 10 segments; (ii) geographic features: about 200 metropolitan locations worldwide and U.S. states; and (iii) behavioral categories: about 1000 binary categories that summarize the user's consumption history within Yahoo! properties.</s><s xml:id="_d86Dwwg">Other than these features, no other information was used to identify a user.</s></p><p xml:id="_fBmpSqE"><s xml:id="_RFKtdrn">Similarly, each article was represented by a raw feature vector of about 100 categorical features constructed in the same way.</s><s xml:id="_27vYNBw">These features include: (i) URL categories: tens of classes inferred from the URL of the article resource; and (ii) editor categories: tens of topics tagged by human editors to summarize the article content.</s></p><p xml:id="_jQuj5zn"><s xml:id="_ewAQBvT">We followed a previous procedure <ref type="bibr" target="#b11">[12]</ref> to encode categorical user/article features as binary vectors and then normalize each feature vector to unit length.</s><s xml:id="_zyxvqJN">We also augmented each feature vector with a constant feature of value 1.</s><s xml:id="_2WdkEUS">Now each article and user was represented by a feature vector of 83 and 1193 entries, respectively.</s></p><p xml:id="_HKcjaWb"><s xml:id="_XHJPDMS">To further reduce dimensionality and capture nonlinearity in these raw features, we carried out conjoint analysis based on random exploration data collected in September 2008.</s><s xml:id="_9bmZc3x">Following a previous approach to dimensionality reduction <ref type="bibr" target="#b12">[13]</ref>, we projected user features onto article categories and then clustered users with similar preferences into groups.</s><s xml:id="_ufVKf5Z">More specifically:</s></p><p xml:id="_gJb9yFd"><s xml:id="_d47bE3M">• We first used logistic regression (LR) to fit a bilinear model for click probability given raw user/article features so that φ φ φ ⊤ u Wφ φ φa approximated the probability that the user u clicks on article a, where φ φ φu and φ φ φa were the corresponding feature vectors, and W was a weight matrix optimized by LR.</s></p><p xml:id="_R4g9G7b"><s xml:id="_2eqBpKv">• Raw user features were then projected onto an induced space by computing ψ ψ ψu</s></p><formula xml:id="formula_18">def = φ φ φ ⊤ u W.</formula><p xml:id="_zZsmW9p"><s xml:id="_mebGCrK">Here, the i th component in ψ ψ ψu for user u may be interpreted as the degree to which the user likes the i th category of articles.</s><s xml:id="_rUcFzJ6">K-means was applied to group users in the induced ψ ψ ψu space into 5 clusters.</s></p><p xml:id="_7raUzuS"><s xml:id="_AnmV4j5">• The final user feature was a six-vector: five entries corresponded to membership of that user in these 5 clusters (computed with a Gaussian kernel and then normalized so that they sum up to unity), and the sixth was a constant feature 1.</s><s xml:id="_TbqyEdx">At trial t, each article a has a separate six-dimensional feature xt,a that is exactly the six-dimensional feature constructed as above for user ut.</s><s xml:id="_rpRDUuj">Since these article features do not overlap, they are for disjoint linear models defined in Section 3.</s></p><p xml:id="_CtjmbfF"><s xml:id="_kagcuyN">For each article a, we performed the same dimensionality reduction to obtain a six-dimensional article feature (including a constant 1 feature).</s><s xml:id="_wsqaYeZ">Its outer product with a user feature gave 6 × 6 = 36 features, denoted zt,a ∈ R 36 , that corresponded to the shared features in Eq. ( <ref type="formula" target="#formula_11">6</ref>), and thus (zt,a, xt,a) could be used in the hybrid linear model.</s><s xml:id="_buSDAKy">Note the features zt,a contains user-article interaction information, while xt,a contains user information only.</s></p><p xml:id="_D6BkW2D"><s xml:id="_UAsYUZn">Here, we intentionally used five users (and articles) groups, which has been shown to be representative in segmentation analysis <ref type="bibr" target="#b12">[13]</ref>.</s><s xml:id="_ecREg5B">Another reason for using a relatively small feature space is that, in online services, storing and retrieving large amounts of user/article information will be too expensive to be practical.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_3xjactx">Compared Algorithms</head><p xml:id="_Sp3etDU"><s xml:id="_4TQjT7U">The algorithms empirically evaluated in our experiments can be categorized into three groups: I. Algorithms that make no use of features.</s><s xml:id="_UvB2Ek5">These correspond to the context-free K-armed bandit algorithms that ignore all contexts (i.e., user/article information).</s></p><p xml:id="_aDRqqFh"><s xml:id="_xad5HtC">• random: A random policy always chooses one of the candidate articles from the pool with equal probability.</s><s xml:id="_QtkZtTV">This algorithm requires no parameters and does not "learn" over time.</s><s xml:id="_q3yDP6h">• ǫ-greedy: As described in Section 2.2, it estimates each article's CTR; then it chooses a random article with probability ǫ, and chooses the article of the highest CTR estimate with probability 1ǫ.</s><s xml:id="_ZCMPkXR">The only parameter of this policy is ǫ.</s></p><p xml:id="_zMTSsnt"><s xml:id="_YGVtqqG">1 1.2 1.4 1.6 1.8 2 0 0.2 0.4 0.6 0.8 1 ctr ε ε-greedy ε-greedy (warm) ε-greedy (seg) ε-greedy (disjoint) ε-greedy (hybrid) omniscient (a) Deployment bucket. 1 1.2 1.4 1.6 1.8 2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 ctr α ucb ucb (warm) ucb (seg) linucb (disjoint) linucb (hybrid) omniscient (b) Deployment bucket. 1 1.2 1.4 1.6 1.8 2 0 0.2 0.4 0.6 0.8 1 ctr ε ε-greedy ε-greedy (warm) ε-greedy (seg) ε-greedy (disjoint) ε-greedy (hybrid) omniscient (c) Learning bucket. 1 1.2 1.4 1.6 1.8 2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 ctr α ucb ucb (warm) ucb (seg) linucb (simple) linucb (hybrid) omniscient (d) Learning bucket.</s><s xml:id="_FcJHQuc">• ucb: As described in Section 2.2, this policy estimates each article's CTR as well as a confidence interval of the estimate, and always chooses the article with the highest UCB.</s><s xml:id="_uf2NBgU">Specifically, following UCB1 <ref type="bibr" target="#b6">[7]</ref>, we computed an article a's confidence interval by ct,a = α √ nt,a , where nt,a is the number of times a was chosen prior to trial t, and α &gt; 0 is a parameter.</s></p><p xml:id="_N6RKHuD"><s xml:id="_RMQz8ac">• omniscient: Such a policy achieves the best empirical context-free CTR from hindsight.</s><s xml:id="_fycQHkK">It first computes each article's empirical CTR from logged events, and then always chooses the article with highest empircal CTR when it is evaluated using the same logged events.</s><s xml:id="_2azmUyp">This algorithm requires no parameters and does not "learn" over time.</s><s xml:id="_BXjYuPd">II.</s><s xml:id="_sbPQGS5">Algorithms with "warm start"-an intermediate step towards personalized services.</s><s xml:id="_KYPFUNG">The idea is to provide an offline-estimated user-specific adjustment on articles' context-free CTRs over the whole traffic.</s><s xml:id="_suKBVVJ">The offset serves as an initialization on CTR estimate for new content, a.k.a."warm start".</s><s xml:id="_3C8j7sh">We re-trained the bilinear logistic regression model studied in <ref type="bibr" target="#b11">[12]</ref> on Sept 2008 random traffic data, using features zt,a constructed above.</s><s xml:id="_Mwj9zZH">The selection criterion then becomes the sum of the context-free CTR estimate and a bilinear term for a user-specific CTR adjustment.</s><s xml:id="_6d4e6S8">In training, CTR was estimated using the context-free ǫ-greedy with ǫ = 1.</s></p><p xml:id="_yyakHEj"><s xml:id="_Unrsw9Q">• ǫ-greedy (warm): This algorithm is the same as ǫ-greedy except it adds the user-specific CTR correction to the article's context-free CTR estimate.</s><s xml:id="_F6zmAuW">• ucb (warm): This algorithm is the same as the previous one but replaces ǫ-greedy with ucb.</s><s xml:id="_YaRHyw5">III.</s><s xml:id="_b8K8aps">Algorithms that learn user-specific CTRs online.</s></p><p xml:id="_8qkNbYP"><s xml:id="_NTcPAsT">• ǫ-greedy (seg): Each user is assigned to the closest user cluster among the five constructed in Section 5.2.2, and so all users are partitioned into five groups (a.k.a.</s><s xml:id="_KymGkuW">user segments), in each of which a separate copy of ǫ-greedy was run.</s></p><p xml:id="_NTrZemX"><s xml:id="_nmBYrM6">• ucb (seg): This algorithm is similar to ǫ-greedy (seg) except it ran a copy of ucb in each of the five user segments.</s><s xml:id="_AKWY2ah">• ǫ-greedy (disjoint): This is ǫ-greedy with disjoint models, and may be viewed as a close variant of epoch-greedy <ref type="bibr" target="#b17">[18]</ref>.</s><s xml:id="_uUUEDBh">• linucb (disjoint): This is Algorithm 1 with disjoint models.</s></p><p xml:id="_VErYJHZ"><s xml:id="_eX2HWyb">• ǫ-greedy (hybrid): This is ǫ-greedy with hybrid models, and may be viewed as a close variant of epoch-greedy.</s><s xml:id="_f72Fvtw">• linucb (hybrid): This is Algorithm 2 with hybrid models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" xml:id="_7ZrZ2Da">Performance Metric</head><p xml:id="_8S4fDfm"><s xml:id="_s55PbKs">An algorithm's CTR is defined as the ratio of the number of clicks it receives and the number of steps it is run.</s><s xml:id="_NuNkPP8">We used all algorithms' CTRs on the random logged events for performance comparison.</s><s xml:id="_5js9Qy9">To protect business-sensitive information, we report an algorithm's relative CTR, which is the algorithm's CTR divided by the random policy's.</s><s xml:id="_ykgN56j">Therefore, we will not report a random policy's relative CTR as it is always 1 by definition.</s><s xml:id="_W6xcrFW">For convenience, we will use the term "CTR" from now on instead of "relative CTR".</s></p><p xml:id="_dSp35zs"><s xml:id="_BNNSJdG">For each algorithm, we are interested in two CTRs motivated by our application, which may be useful for other similar applications.</s><s xml:id="_AWsduhH">When deploying the methods to Yahoo!'s front page, one reasonable way is to randomly split all traffic to this page into two buckets <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_mQuzVgg">The first, called "learning bucket", usually consists of a small fraction of traffic on which various bandit algorithms are run to learn/estimate article CTRs.</s><s xml:id="_QfnH6cs">The other, called "deployment bucket", is where Yahoo!</s><s xml:id="_9n8pjFZ">Front Page greedily serves users using CTR estimates obained from the learning bucket.</s><s xml:id="_g2aN78r">Note that "learning" and "deployment" are interleaved in this problem, and so in every view falling into the deployment bucket, the article with the highest current (user-specific) CTR estimate is chosen; this estimate may change later if the learning bucket gets more data.</s><s xml:id="_C44jFcM">CTRs in both buckets were estimated with Algorithm 3. Since the deployment bucket is often larger than the learning bucket, CTR in the deployment bucket is more important.</s><s xml:id="_UmAd4FF">However, a higher CTR in the learning bucket suggests a faster learning rate (or equivalently, smaller regret) for a bandit algorithm.</s><s xml:id="_QTeYUBd">Therefore, we chose to report algorithm CTRs in both buckets.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5" xml:id="_q5WxenS">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1" xml:id="_t62V4C5">Results for Tuning Data</head><p xml:id="_M5HkH6x"><s xml:id="_Wz9qEqw">Each of the competing algorithms (except random and omniscient) in Section 5.3 requires a single parameter: ǫ for ǫ-greedy algorithms and α for UCB ones.</s><s xml:id="_Vdy3k2U">We used tuning data to optimize these parameters.</s><s xml:id="_dYXgCEe">Figure <ref type="figure" target="#fig_1">2</ref> shows how the CTR of each algorithm changes with respective parameters.</s><s xml:id="_n9qurGB">All results were obtained by a single run, but given the size of our dataset and the unbiasedness result in Theorem 1, the reported numbers are statistically reliable.</s></p><p xml:id="_VQR2cYB"><s xml:id="_fxHufFp">First, as seen from Figure <ref type="figure" target="#fig_1">2</ref>, the CTR curves in the learning buckets often possess the inverted U-shape.</s><s xml:id="_KZ6eDu9">When the parameter (ǫ or α) is too small, there was insufficient exploration, the algorithms failed to identify good articles, and had a smaller number of clicks.</s><s xml:id="_eUSCjZa">On the other hand, when the parameter is too large, the algorithms appeared to over-explore and thus wasted some of the opportunities to increase the number of clicks.</s><s xml:id="_swtfusd">Based on these plots on tuning data, we chose appropriate parameters for each algorithm and ran it once on the evaluation data in the next subsection.</s></p><p xml:id="_DSXvp99"><s xml:id="_TjJR27e">Second, it can be concluded from the plots that warm-start information is indeed helpful for finding a better match between user interest and article content, compared to the no-feature versions of ǫ-greedy and UCB.</s><s xml:id="_QYKMGrx">Specifically, both ǫ-greedy (warm) and ucb (warm) were able to beat omniscient, the highest CTRs achievable by context-free policies in hindsight.</s><s xml:id="_FKYWuCT">However, performance of the two algorithms using warm-start information is not as stable as algorithms that learn the weights online.</s><s xml:id="_qFkZJD5">Since the offline model for "warm start" was trained with article CTRs estimated on all random traffic <ref type="bibr" target="#b11">[12]</ref>, ǫ-greedy (warm) gets more stable performance in the deployment bucket when ǫ is close to 1.</s><s xml:id="_8dPPX9X">The warm start part also helps ucb (warm) in the learning bucket by selecting more attractive articles to users from scratch, but did not help ucb (warm) in determining the best online for deployment.</s><s xml:id="_rTnttp6">Since ucb relies on the a confidence interval for exploration, it is hard to correct the initialization bias introduced by "warm start".</s><s xml:id="_8B3vz6r">In contrast, all online-learning algorithms were able to consistently beat the omniscient policy.</s><s xml:id="_Vcv7bv7">Therefore, we did not try the warm-start algorithms on the evaluation data.</s></p><p xml:id="_Pk8vDVT"><s xml:id="_Rs2nayu">Third, ǫ-greedy algorithms (on the left of Figure <ref type="figure" target="#fig_1">2</ref>) achieved similar CTR as upper confidence bound ones (on the right of Figure <ref type="figure" target="#fig_1">2</ref>) in the deployment bucket when appropriate parameters were used.</s><s xml:id="_SAJUyBA">Thus, both types of algorithms appeared to learn comparable policies.</s><s xml:id="_Ph6UEvE">However, they seemed to have lower CTR in the learning bucket, which is consistent with the empirical findings of contextfree algorithms <ref type="bibr" target="#b1">[2]</ref> in real bucket tests.</s></p><p xml:id="_CXfqjTs"><s xml:id="_XfXYRkn">Finally, to compare algorithms when data are sparse, we repeated the same parameter tuning process for each algorithm with fewer data, at the level of 30%, 20%, 10%, 5%, and 1%.</s><s xml:id="_swfjYs7">Note that we still used all data to evaluate an algorithm's CTR as done in Algorithm 3, but then only a fraction of available data were randomly chosen to be used by the algorithm to improve its policy.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2" xml:id="_kWmN3nz">Results for Evaluation Data</head><p xml:id="_rGprZtu"><s xml:id="_EamEfje">With parameters optimized on the tuning data (c.f., Figure <ref type="figure" target="#fig_1">2</ref>), we ran the algorithms on the evaluation data and summarized the CTRs in Table <ref type="table" target="#tab_2">1</ref>.</s><s xml:id="_V9b52J2">The table also reports the CTR lift compared to the baseline of ǫ-greedy.</s><s xml:id="_TSVQxbP">The CTR of omniscient was 1.615, and so a significantly larger CTR of an algorithm indicates its effective use of user/article features for personalization.</s><s xml:id="_DV7Z4R3">Recall that the reported CTRs were normalized by the random policy's CTR.</s><s xml:id="_jWkJx7c">We examine the results more closely in the following subsections.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RdNPtyT">On the Use of Features.</head><p xml:id="_9RSfWeu"><s xml:id="_QmvVBZK">We first investigate whether it helps to use features in article recommendation.</s><s xml:id="_SWcRhf5">It is clear from Table <ref type="table" target="#tab_2">1</ref> that, by considering user features, both ǫ-greedy (seg/disjoint/hybrid) and UCB methods (ucb (seg) and linucb (disjoint/hybrid)) were able to achieve a CTR lift of around 10%, compared to the baseline ǫ-greedy.</s></p><p xml:id="_76tVhmb"><s xml:id="_72yEAwV">To better visualize the effect of features, Figure <ref type="figure" target="#fig_2">3</ref> shows how an article's CTR (when chosen by an algorithm) was lifted compared to its base CTR (namely, the context-free CTR). <ref type="foot" target="#foot_3">4</ref> Here, an article's base CTR measures how interesting it is to a random user, and was estimated from logged events.</s><s xml:id="_tSgPkEw">Therefore, a high ratio of the lifted and base CTRs of an article is a strong indicator that an algorithm does recommend this article to potentially interested users.</s><s xml:id="_zxgw8Zn">the other three plots show clear benefits by considering personalized recommendation.</s><s xml:id="_Pqj7cmg">In an extreme case (Figure <ref type="figure" target="#fig_2">3</ref>(c)), one of the article's CTR was lifted from 1.31 to 3.03-a 132% improvement.</s><s xml:id="_zRGKGVn">Furthermore, it is consistent with our previous results on tuning data that, compared to ǫ-greedy algorithms, UCB methods achieved higher CTRs in the deployment bucket, and the advantage was even greater in the learning bucket.</s><s xml:id="_bb88gYC">As mentioned in Section 2.2, ǫgreedy approaches are unguided because they choose articles uniformly at random for exploration.</s><s xml:id="_c2Kud7J">In contrast, exploration in upper confidence bound methods are effectively guided by confidence intervals-a measure of uncertainty in an algorithm's CTR estimate.</s><s xml:id="_YkXsWPW">Our experimental results imply the effectiveness of upper confidence bound methods and we believe they have similar benefits in many other applications as well.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KYJgEA7">On the Size of Data.</head><p xml:id="_CnVTVNu"><s xml:id="_9s5gTXy">One of the challenges in personalized web services is the scale of the applications.</s><s xml:id="_qUAcrKP">In our problem, for example, a small pool of news articles were hand-picked by human editors.</s><s xml:id="_yuAxGDn">But if we wish to allow more choices or use automated article selection methods to determine the article pool, the number of articles can be too large even for the high volume of Yahoo! traffic.</s><s xml:id="_z4muTER">Therefore, it becomes critical for an algorithm to quickly identify a good match between user interests and article contents when data are sparse.</s><s xml:id="_PsAa5Vp">In our experiments, we artificially reduced data size (to the levels of 30%, 20%, 10%, 5%, and 1%, respectively) to mimic the situation where we have a large article pool but a fixed volume of traffic.</s></p><p xml:id="_SsDmVmX"><s xml:id="_DaFPPs2">To better visualize the comparison results, we use bar graphs in Figure <ref type="figure" target="#fig_5">4</ref> to plot all algorithms' CTRs with various data sparsity levels.</s><s xml:id="_mdCmJWz">A few observations are in order.</s><s xml:id="_YPwjm9U">First, at all data sparsity levels, features were still useful.</s><s xml:id="_ebgZDzw">At the level of 1%, for instance, we observed a 10.3% improvement of linucb (hybrid)'s CTR in the deployment bucket (1.493) over ucb's (1.354).</s></p><p xml:id="_t2m79t2"><s xml:id="_9cYbNgv">Second, UCB methods consistently outperformed ǫ-greedy ones in the deployment bucket. <ref type="foot" target="#foot_5">5</ref></s><s xml:id="_hyuwERe">The advantage over ǫ-greedy was even more apparent when data size was smaller.</s></p><p xml:id="_qY2ujJX"><s xml:id="_E8U92rn">Third, compared to ucb (seg) and linucb (disjoint), linucb (hybrid) showed significant benefits when data size was small.</s><s xml:id="_JQrKXny">Recall that in hybrid models, some features are shared by all articles, making it possible for CTR information of one article to be "transferred" to others.</s><s xml:id="_bS57fwG">This advantage is particularly useful when the article pool is large.</s><s xml:id="_VhuQCUW">In contrast, in disjoint models, feedback of one article may not be utilized by other articles; the same is true for ucb (seg).</s><s xml:id="_yuGvf42">Figure <ref type="figure" target="#fig_5">4</ref>(a) shows transfer learning is indeed helpful when data are sparse.</s></p><p xml:id="_yB7HHyq"><s xml:id="_qNspvzW">Comparing ucb (seg) and linucb (disjoint).</s></p><p xml:id="_5cP8xaa"><s xml:id="_NK6KpjD">From Figure <ref type="figure" target="#fig_5">4</ref>(a), it can be seen that ucb (seg) and linucb (disjoint) had similar performance.</s><s xml:id="_5tjYg2q">We believe it was no coincidence.</s><s xml:id="_3QY3hv6">Recall that features in our disjoint model are actually normalized membership measures of a user in the five clusters described in Section 5.2.2.</s><s xml:id="_HNrF9tc">Hence, these features may be viewed as a "soft" version of the user assignment process adopted by ucb (seg).</s></p><p xml:id="_6EdasxA"><s xml:id="_TSYMXKA">Figure <ref type="figure" target="#fig_6">5</ref> plots the histogram of a user's relative membership measure to the closest cluster, namely, the largest component of the user's five, non-constant features.</s><s xml:id="_7bmvkba">It is clear that most users were quite close to one of the five cluster centers: the maximum membership of about 85% users were higher than 0.5, and about 40% of them were higher than 0.8.</s><s xml:id="_h3jcjbU">Therefore, many of these features have a highly dominating component, making the feature vector similar to the "hard" version of user group assignment.</s></p><p xml:id="_64yqfex"><s xml:id="_gBfCHhZ">We believe that adding more features with diverse components, such as those found by principal component analysis, would be necessary to further distinguish linucb (disjoint) from ucb (seg).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_aveMhcE">CONCLUSIONS</head><p xml:id="_UxKQRxx"><s xml:id="_7BhQ6b6">This paper takes a contextual-bandit approach to personalized web-based services such as news article recommendation.</s><s xml:id="_RQ8a5jv">We proposed a simple and reliable method for evaluating bandit algorithms directly from logged events, so that the often problematic simulator-building step could be avoided.</s><s xml:id="_ADBPm2k">Based on real Yahoo!</s><s xml:id="_9GyqKsk">Front Page traffic, we found that upper confidence bound methods generally outperform the simpler yet unguided ǫ-greedy methods.</s><s xml:id="_KEkh9dW">Furthermore, our new algorithm LinUCB shows advantages when data are sparse, suggesting its effectiveness to personalized web services when the number of contents in the pool is large.</s></p><p xml:id="_PqVhUMU"><s xml:id="_xCxRB7d">In the future, we plan to investigate bandit approaches to other similar web-based serviced such as online advertising, and compare our algorithms to related methods such as Banditron <ref type="bibr" target="#b15">[16]</ref>.</s><s xml:id="_fSpsfxB">A second direction is to extend the bandit formulation and algorithms in which an "arm" may refer to a complex object rather than an item (like an article).</s><s xml:id="_xuxFR4d">An example is ranking, where an arm corresponds to a permutation of retrieved webpages.</s><s xml:id="_tBXr9wZ">Finally, user interests change over time, and so it is interesting to consider temporal information in bandit algorithms.</s><s xml:id="_puVxtuM">0 0.05 0.1 0.15 0.2 0.25 0.3 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 maximum user membership feature</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc><div><p xml:id="_vSW9NyE"><s xml:id="_zj2hjvG">Figure 1: A snapshot of the "Featured" tab in the Today Module on Yahoo!</s><s xml:id="_UzcY5zk">Front Page.</s><s xml:id="_Qy2CSgN">By default, the article at F1 position is highlighted at the story position.</s></p></div></figDesc><graphic coords="6,66.17,53.09,213.35,130.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc><div><p xml:id="_ATJK8WD"><s xml:id="_GmDRtdX">Figure 2: Parameter tuning: CTRs of various algorithms on the one-day tuning dataset.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc><div><p xml:id="_NYcR4vs"><s xml:id="_pYKU52n">Figure 3: Scatterplots of the base CTR vs. lifted CTR (in the learning bucket) of the 50 most frequently selected articles when 100% evaluation data were used.</s><s xml:id="_nRWmDYs">Red crosses are for ǫ-greedy algorithms, and blue circles are for UCB algorithms.</s><s xml:id="_bV5zP6F">Note that the sets of most frequently chosen articles varied with algorithms; see the text for details.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc><div><p xml:id="_ZfGNbHf"><s xml:id="_5E3KExn">ucb ε-greedy (seg) ucb (seg) ε-greedy (disjoint) linucb (disjoint) ε-greedy (hybrid) linucb (hybrid) omniscient (a) CTRs in the deployment bucket.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc><div><p xml:id="_JuGgzyF"><s xml:id="_7V6UDWS">CTRs in the learning bucket.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc><div><p xml:id="_WQVjUfx"><s xml:id="_musJdC5">Figure 4: CTRs in evaluation data with varying data sizes.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc><div><p xml:id="_Z6Nvg8G"><s xml:id="_BXeWhrD">Figure 5: User maximum membership histogram.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : Performance evaluation: CTRs of all algorithms on the one-week evaluation dataset in the deployment and learning buckets (denoted by "deploy" and "learn" in the table, respectively). The numbers with a percentage is the CTR lift compared to</head><label>1</label><figDesc><div><p xml:id="_NM4FVnC"><s xml:id="_W7GxRjE">ǫ-greedy.</s></p></div></figDesc><table><row><cell>algorithm</cell><cell cols="2">size = 100% deploy learn</cell><cell cols="2">size = 30% deploy learn</cell><cell cols="2">size = 20% deploy learn</cell><cell cols="2">size = 10% deploy learn</cell><cell cols="2">size = 5% deploy learn</cell><cell cols="2">size = 1% deploy learn</cell></row><row><cell>ǫ-greedy</cell><cell>1.596 0%</cell><cell>1.326 0%</cell><cell>1.541 0%</cell><cell>1.326 0%</cell><cell>1.549 0%</cell><cell>1.273 0%</cell><cell>1.465 0%</cell><cell>1.326 0%</cell><cell>1.409 0%</cell><cell>1.292 0%</cell><cell>1.234 0%</cell><cell>1.139 0%</cell></row><row><cell>ucb</cell><cell>1.594 0%</cell><cell>1.569 18.3%</cell><cell>1.582 2.7%</cell><cell>1.535 15.8%</cell><cell>1.569 1.3%</cell><cell>1.488 16.9%</cell><cell>1.541 5.2%</cell><cell>1.446 9%</cell><cell>1.541 9.4%</cell><cell>1.465 13.4%</cell><cell>1.354 9.7%</cell><cell>1.22 7.1%</cell></row><row><cell>ǫ-greedy (seg)</cell><cell>1.742 9.1%</cell><cell>1.446 9%</cell><cell>1.652 7.2%</cell><cell>1.46 10.1%</cell><cell>1.585 2.3%</cell><cell>1.119 -12%</cell><cell>1.474 0.6%</cell><cell>1.284 -3.1%</cell><cell>1.407 0%</cell><cell>1.281 -0.8%</cell><cell>1.245 0.9%</cell><cell>1.072 -5.8%</cell></row><row><cell>ucb (seg)</cell><cell>1.781 11.6%</cell><cell>1.677 26.5%</cell><cell>1.742 13%</cell><cell>1.555 17.3%</cell><cell>1.689 9%</cell><cell cols="2">1.446 13.6% 11.7% 1.636</cell><cell>1.529 15.3%</cell><cell>1.532 8.7%</cell><cell>1.32 2.2%</cell><cell>1.398 13.3%</cell><cell>1.25 9.7%</cell></row><row><cell>ǫ-greedy (disjoint)</cell><cell cols="2">1.769 10.8% -1.2% 1.309</cell><cell>1.686 9.4%</cell><cell>1.337 0.8%</cell><cell>1.624 4.8%</cell><cell>1.529 20.1%</cell><cell>1.529 4.4%</cell><cell>1.451 9.4%</cell><cell>1.432 1.6%</cell><cell>1.345 4.1%</cell><cell>1.262 2.3%</cell><cell>1.183 3.9%</cell></row><row><cell>linucb (disjoint)</cell><cell>1.795 12.5%</cell><cell>1.647 24.2%</cell><cell cols="3">1.719 11.6% 13.7% 10.7% 1.507 1.714</cell><cell>1.384 8.7%</cell><cell>1.655 13%</cell><cell>1.387 4.6%</cell><cell cols="2">1.574 11.7% -3.5% 1.245</cell><cell>1.382 12%</cell><cell>1.197 5.1%</cell></row><row><cell>ǫ-greedy (hybrid)</cell><cell>1.739 9%</cell><cell>1.521 14.7%</cell><cell>1.68 9%</cell><cell>1.345 1.4%</cell><cell>1.636 5.6%</cell><cell>1.449 13.8%</cell><cell>1.58 7.8%</cell><cell>1.348 1.7%</cell><cell>1.465 4%</cell><cell>1.415 9.5%</cell><cell>1.342 8.8%</cell><cell>1.2 5.4%</cell></row><row><cell>linucb (hybrid)</cell><cell>1.73 8.4%</cell><cell>1.663 25.4%</cell><cell>1.691 9.7%</cell><cell>1.591 20%</cell><cell cols="3">1.708 10.3% 27.2% 14.3% 1.619 1.675</cell><cell>1.535 15.8%</cell><cell>1.588 12.7%</cell><cell>1.507 16.6%</cell><cell>1.482 20.1%</cell><cell>1.446 27%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p xml:id="_tcjykns"><s xml:id="_D8pQrja">In the literature, contextual bandits are sometimes called bandits with covariate, bandits with side information, associative bandits, and associative reinforcement learning.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p xml:id="_tFQwXdH"><s xml:id="_GJWEft6">Note Õ(•) is the same as O(•) but suppresses logarithmic factors.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p xml:id="_3x2rMCm"><s xml:id="_HPbbB3z">We call it view-based randomization.</s><s xml:id="_aMqX5Az">After refreshing her browser, the user may not fall into the random bucket again.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p xml:id="_PcNKaTt"><s xml:id="_6ycfKMU">To avoid inaccurate CTR estimates, only</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="50" xml:id="foot_4"><p xml:id="_hamudBa"><s xml:id="_p4s6xYe">articles that were chosen most often by an algorithm were included in its own plots.</s><s xml:id="_QhWuUmc">Hence, the plots for different algorithms are not comparable.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p xml:id="_va4WxM5"><s xml:id="_msBjnNF">In the less important learning bucket, there were two exceptions for linucb (disjoint).</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7." xml:id="_gBy2haK">ACKNOWLEDGMENTS</head><p xml:id="_6EkBThS"><s xml:id="_gQSYRcf">We thank <rs type="person">Deepak Agarwal</rs>, <rs type="person">Bee-Chung Chen</rs>, <rs type="person">Daniel Hsu</rs>, and <rs type="person">Kishore Papineni</rs> for many helpful discussions, <rs type="person">István Szita</rs> and <rs type="person">Tom Walsh</rs> for clarifying their algorithm, and <rs type="person">Taylor Xi</rs> and the anonymous reviewers for suggestions that improved the presentation of the paper.</s></p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_fZd9qh5">Reinforcement learning with immediate rewards and linear hypotheses</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Biermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00453-003-1038-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_76DsqCc">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="293" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Abe, A. W. Biermann, and P. M. Long. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica, 37(4):263-293, 2003.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_n7rqMab">Explore/exploit schemes for web content optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Elango</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdm.2009.52</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cQxGx4E">Proc. of the 9th International Conf. on Data Mining</title>
		<meeting>of the 9th International Conf. on Data Mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Agarwal, B.-C. Chen, and P. Elango. Explore/exploit schemes for web content optimization. In Proc. of the 9th International Conf. on Data Mining, 2009.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_7wkkvwP">Online models for content optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Elango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Motgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zachariah</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdm.2009.52</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZTWBUB7">Advances in Neural Information Processing Systems 21</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Agarwal, B.-C. Chen, P. Elango, N. Motgi, S.-T. Park, R. Ramakrishnan, S. Roy, and J. Zachariah. Online models for content optimization. In Advances in Neural Information Processing Systems 21, pages 17-24, 2009.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_MhhP3h4">Sample mean based index policies with o(log n) regret for the multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<idno type="DOI">10.2307/1427934</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kx4T2Qe">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1054" to="1078" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Agrawal. Sample mean based index policies with o(log n) regret for the multi-armed bandit problem. Advances in Applied Probability, 27(4):1054-1078, 1995.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_87ap46G">Just-in-time contextual advertising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1321440.1321488</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EJkYs8G">Proc. of the 16th ACM Conf. on Information and Knowledge Management</title>
		<meeting>of the 16th ACM Conf. on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Anagnostopoulos, A. Z. Broder, E. Gabrilovich, V. Josifovski, and L. Riedel. Just-in-time contextual advertising. In Proc. of the 16th ACM Conf. on Information and Knowledge Management, pages 331-340, 2007.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_hrZhaRE">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9mtH3KX">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 3:397-422, 2002.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_RrfFVvV">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1013689704352</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_spjB4yg">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235-256, 2002.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_GEsJUah">The nonstochastic multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="DOI">10.1137/s0097539701398375</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GWpEcFU">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="77" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_GJPAmVG">Bandit Problems: Sequential Allocation of Experiments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fristedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_exAu8rg">Monographs on Statistics and Applied Probability</title>
		<imprint>
			<publisher>Chapman and Hall</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. A. Berry and B. Fristedt. Bandit Problems: Sequential Allocation of Experiments. Monographs on Statistics and Applied Probability. Chapman and Hall, 1985.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_hUZDgGz">The Adaptive Web -Methods and Strategies of Web Personalization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brusilovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kobsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Nejdl</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-72079-9</idno>
	</analytic>
	<monogr>
		<title level="s" xml:id="_SxSAjyw">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4321</biblScope>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin / Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Brusilovsky, A. Kobsa, and W. Nejdl, editors. The Adaptive Web - Methods and Strategies of Web Personalization, volume 4321 of Lecture Notes in Computer Science. Springer Berlin / Heidelberg, 2007.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_Yqzf2s8">Hybrid systems for personalized recommendations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burke</surname></persName>
		</author>
		<idno type="DOI">10.1007/11577935_7</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ju3mwTq">Intelligent Techniques for Web Personalization</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Mobasher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Anand</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Burke. Hybrid systems for personalized recommendations. In B. Mobasher and S. S. Anand, editors, Intelligent Techniques for Web Personalization. Springer-Verlag, 2005.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_xaKzkdc">Personalized recommendation on dynamic content using predictive bilinear models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1145/1526709.1526802</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7urP2Mv">Proc. of the 18th International Conf. on World Wide Web</title>
		<meeting>of the 18th International Conf. on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="691" to="700" />
		</imprint>
	</monogr>
	<note type="raw_reference">W. Chu and S.-T. Park. Personalized recommendation on dynamic content using predictive bilinear models. In Proc. of the 18th International Conf. on World Wide Web, pages 691-700, 2009.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_RFwuNaE">A case study of behavior-driven conjoint analysis on Yahoo!: Front Page Today Module</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beaupre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Motgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phadke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zachariah</surname></persName>
		</author>
		<idno type="DOI">10.1145/1557019.1557138</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cQBtYSd">Proc. of the 15th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the 15th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1097" to="1104" />
		</imprint>
	</monogr>
	<note type="raw_reference">W. Chu, S.-T. Park, T. Beaupre, N. Motgi, A. Phadke, S. Chakraborty, and J. Zachariah. A case study of behavior-driven conjoint analysis on Yahoo!: Front Page Today Module. In Proc. of the 15th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining, pages 1097-1104, 2009.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_283rp8j">Google news personalization: scalable online collaborative filtering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242610</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9DYV7qv">Proc. of the 16th International World Wide Web Conf</title>
		<meeting>of the 16th International World Wide Web Conf</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: scalable online collaborative filtering. In Proc. of the 16th International World Wide Web Conf., 2007.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_w3NTzzE">Bandit processes and dynamic allocation indices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gittins</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2517-6161.1979.tb01068.x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cSx2w3W">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="148" to="177" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society. Series B (Methodological), 41:148-177, 1979.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_XeZR6WK">Efficient bandit algorithms for online multiclass prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390212</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BTNrT2S">Proc. of the 25th International Conf. on Machine Learning</title>
		<meeting>of the 25th International Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. M. Kakade, S. Shalev-Shwartz, and A. Tewari. Efficient bandit algorithms for online multiclass prediction. In Proc. of the 25th International Conf. on Machine Learning, pages 440-447, 2008.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_4438Zkb">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<idno type="DOI">10.1016/0196-8858(85)90002-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RyMExZV">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. L. Lai and H. Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4-22, 1985.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_HxX68s5">The epoch-greedy algorithm for contextual multi-armed bandits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZrD7n4f">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Langford and T. Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In Advances in Neural Information Processing Systems 20, 2008.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main" xml:id="_TxadWau">Information Theory, Inference, and Learning Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">D. J. C. MacKay. Information Theory, Inference, and Learning Algorithms. Cambridge University Press, 2003.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_5Au3edg">Text-learning and related intelligent agents: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mladenic</surname></persName>
		</author>
		<idno type="DOI">10.1109/5254.784084</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QszJPZt">IEEE Intelligent Agents</title>
		<imprint>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Mladenic. Text-learning and related intelligent agents: A survey. IEEE Intelligent Agents, pages 44-54, 1999.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_bRwQq3p">Naïve filterbots for robust cold-start recommendations</title>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pennock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150490</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nMPgnZJ">Proc. of the 12th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the 12th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="699" to="705" />
		</imprint>
	</monogr>
	<note type="raw_reference">S.-T. Park, D. Pennock, O. Madani, N. Good, and D. DeCoste. Naïve filterbots for robust cold-start recommendations. In Proc. of the 12th ACM SIGKDD International Conf. on Knowledge Discovery and Data Mining, pages 699-705, 2006.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_GQxQBHb">Simulation studies of multi-armed bandits with covariates</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Pavlidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Tasoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<idno type="DOI">10.1109/uksim.2008.86</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XS2kb6x">Proceedings on the 10th International Conf. on Computer Modeling and Simulation</title>
		<meeting>on the 10th International Conf. on Computer Modeling and Simulation</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="493" to="498" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. G. Pavlidis, D. K. Tasoulis, and D. J. Hand. Simulation studies of multi-armed bandits with covariates. In Proceedings on the 10th International Conf. on Computer Modeling and Simulation, pages 493-498, 2008.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_mwRZShF">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_F2fNY3H">Proc. of the 17th Interational Conf. on Machine Learning</title>
		<meeting>of the 17th Interational Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Precup, R. S. Sutton, and S. P. Singh. Eligibility traces for off-policy policy evaluation. In Proc. of the 17th Interational Conf. on Machine Learning, pages 759-766, 2000.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_6wpBGb8">Some aspects of the sequential design of experiments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<idno type="DOI">10.1090/s0002-9904-1952-09620-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4D94SpH">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="527" to="535" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58(5):527-535, 1952.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_kmVx4RS">Recommender systems in e-commerce</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedi</surname></persName>
		</author>
		<idno type="DOI">10.1145/336992.337035</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_58dprXk">Proc. of the 1st ACM Conf. on Electronic Commerce</title>
		<meeting>of the 1st ACM Conf. on Electronic Commerce</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. B. Schafer, J. Konstan, and J. Riedi. Recommender systems in e-commerce. In Proc. of the 1st ACM Conf. on Electronic Commerce, 1999.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_EmzvAqV">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/25.3-4.285</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BCgTECA">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285-294, 1933.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_9trquqk">Exploring compact reinforcement-learning representations with linear regression</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Szita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9EUVPVa">Proc. of the 25th Conf. on Uncertainty in Artificial Intelligence</title>
		<meeting>of the 25th Conf. on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. J. Walsh, I. Szita, C. Diuk, and M. L. Littman. Exploring compact reinforcement-learning representations with linear regression. In Proc. of the 25th Conf. on Uncertainty in Artificial Intelligence, 2009.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
