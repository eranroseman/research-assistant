<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_JK3dB4H">Ensuring Fairness in Machine Learning to Advance Health Equity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2019-06-26">2019 June 26.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>MD</roleName><forename type="first">Alvin</forename><surname>Rajkomar</surname></persName>
							<email>alvinrajkomar@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Mountain</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName><surname>View</surname></persName>
						</author>
						<author>
							<persName><roleName>PhD</roleName><forename type="first">Michaela</forename><surname>Hardt</surname></persName>
						</author>
						<author>
							<persName><roleName>Mountain View, California</roleName><surname>Google</surname></persName>
						</author>
						<author>
							<persName><roleName>MD, MPH, Google, Mountain View, California</roleName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Howell</surname></persName>
						</author>
						<author>
							<persName><roleName>PhD</roleName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
						</author>
						<author>
							<persName><roleName>Mountain View, California</roleName><forename type="first">Marshall</forename><forename type="middle">H</forename><surname>Chin</surname></persName>
						</author>
						<author>
							<persName><roleName>Hardt, Howell</roleName><forename type="first">Drs</forename><surname>Rajkomar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of California , San Francisco , San Francisco , California</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Francisco</addrLine>
									<settlement>San Francisco</settlement>
									<region>California</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">University of Chicago , Chicago , Illinois</note>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>Illinois</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">Google LLC , 1600 Amphitheatre Way , Mountain View , CA 94043 ;</note>
								<orgName type="institution">Google LLC</orgName>
								<address>
									<addrLine>1600 Amphitheatre Way</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">Google LLC , 1600 Amphitheatre Way , Mountain View , CA 94043 . Dr .</note>
								<orgName type="department">Dr</orgName>
								<orgName type="institution">Google LLC</orgName>
								<address>
									<addrLine>1600 Amphitheatre Way</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
									<country></country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<note type="raw_affiliation">University of Chicago , 5841 South Maryland Avenue , MC2007 , Chicago , IL 60637.</note>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<addrLine>5841 South Maryland Avenue</addrLine>
									<postCode>MC2007 60637.</postCode>
									<settlement>Chicago</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_wu44m4b">Ensuring Fairness in Machine Learning to Advance Health Equity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-26">2019 June 26.</date>
						</imprint>
					</monogr>
					<idno type="MD5">3E292C10FF8F7FEF3A4C2D8CB8101B6F</idno>
					<idno type="DOI">10.7326/m18-1990</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_BFUkCu7"><p xml:id="_HpYFFJG"><s xml:id="_94B4Wfn">Machine learning is used increasingly in clinical care to improve diagnosis, treatment selection, and health system efficiency.</s><s xml:id="_aNR88SW">Because machine-learning models learn from historically collected data, populations that have experienced human and structural biases in the past-called protected groups-are vulnerable to harm by incorrect predictions or withholding of resources.</s><s xml:id="_knWKACQ">This article describes how model design, biases in data, and the interactions of model predictions with clinicians and patients may exacerbate health care disparities.</s><s xml:id="_CHpjX7C">Rather than simply guarding against these harms passively, machine-learning systems should be used proactively to advance health equity.</s><s xml:id="_h6QTqPw">For that goal to be achieved, principles of distributive justice must be incorporated into model design, deployment, and evaluation.</s><s xml:id="_Cfa4R23">The article describes several technical implementations of distributive justice-specifically those that ensure equality in patient outcomes, performance, and resource allocation-and guides clinicians as to when they should prioritize each principle.</s><s xml:id="_yXbmWSq">Machine learning is providing increasingly sophisticated decision support and population-level monitoring, and it should encode principles of justice to ensure that models benefit all patients.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_R9DmXq9"><p xml:id="_EgrcNUC"><s xml:id="_9zJPst6">Machine learning can identify the statistical patterns of data generated by tens of thousands of physicians and billions of patients to train computers to perform specific tasks with sometimes superhuman ability, such as detecting diabetic eye disease better than retinal specialists <ref type="bibr" target="#b0">(1)</ref>.</s><s xml:id="_s8RqdfM">However, historical data also capture patterns of health care disparities, and machine-learning models trained on these data may perpetuate these inequities.</s><s xml:id="_CsYnxDv">This concern is not just academic.</s><s xml:id="_ndaQaE6">In a model used to predict future crime on the basis of historical arrest records, African American defendants who did not reoffend were classified as high risk at a substantially higher rate than white defendants who did not reoffend <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b2">3)</ref>.</s><s xml:id="_KSwsc9b">Similar biases have been observed in predictive policing <ref type="bibr" target="#b3">(4)</ref> and identifying which calls to a child protective services agency required an in-person investigation <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref>.</s><s xml:id="_9KWwKGv">The implications for health care led the American Medical Association to pass policy recommendations to "promote development of thoughtfully designed, high-quality, clinically validated health care AI [artificial or augmented intelligence, such as machine learning] that … identifies and takes steps to address bias and avoids introducing or exacerbating health care disparities including when testing or deploying new AI tools on vulnerable populations" <ref type="bibr" target="#b6">(7)</ref>.</s></p><p xml:id="_H66E4C6"><s xml:id="_bZutSFq">We argue that health care organizations and policymakers should go beyond the American Medical Association's position of doing no harm and instead proactively design and use machine-learning systems to advance health equity.</s><s xml:id="_efaHKSR">Whereas much health disparities work has focused on discriminatory decision making and implicit biases by clinicians, policymakers, organizational leaders, and researchers are increasingly focusing on the ill health effects of structural racism and classism-how systems are shaped in ways that harm the health of disempowered, marginalized populations <ref type="bibr" target="#b7">(8)</ref>.</s><s xml:id="_RW4Zxx8">For example, the United States has a shameful history of purposive decisions by government and private businesses to segregate housing.</s><s xml:id="_jKZPCXe">Zoning laws, discrimination in mortgage lending, prejudicial practices by real estate agents, and the ghettoization of public housing all contributed to the concentration of urban African Americans in inferior housing that has led to poor health <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b9">10)</ref>.</s><s xml:id="_HQUk6sH">Even when the goal of decision makers is not outright discrimination against disadvantaged groups, actions may lead to inequities.</s><s xml:id="_vDrUzKj">For example, if the goal of a machine-learning system is to maximize efficiency, that might come at the expense of disadvantaged populations.</s></p><p xml:id="_KEM84zz"><s xml:id="_hZH36ry">As a society, we value health equity.</s><s xml:id="_ApzD42Y">For example, the Healthy People 2020 vision statement aims for "a society in which all people live long, healthy lives," and one of the mission's goals is to "achieve health equity, eliminate disparities, and improve the health of all groups" <ref type="bibr" target="#b10">(11)</ref>.</s><s xml:id="_6Fv7XGu">The 4 classic principles of Western clinical medical ethics are justice, autonomy, beneficence, and nonmaleficence.</s><s xml:id="_MBucHQx">However, health equity will not be attained unless we purposely design our health and social systems, which increasingly will be infused with machine learning <ref type="bibr" target="#b11">(12)</ref>, to achieve this goal.</s></p><p xml:id="_fddqJM5"><s xml:id="_8PnNUgt">To ensure fairness in machine learning, we recommend a participatory process that involves key stakeholders, including frequently marginalized populations, and considers distributive justice within specific clinical and organizational contexts.</s><s xml:id="_Fn6shcf">Different technical approaches can configure the mathematical properties of machine-learning models to render predictions that are equitable in various ways.</s><s xml:id="_yFJMnFQ">The existence of mathematical levers must be supplemented with criteria for when and why they should be used-each tool comes with tradeoffs that require ethical reasoning to decide what is best for a given application.</s></p><p xml:id="_ajvbMfj"><s xml:id="_HqaAnuv">We propose incorporating fairness into the design, deployment, and evaluation of machinelearning models.</s><s xml:id="_x7ugtuP">We discuss 2 clinical applications in which machine learning might harm protected groups by being inaccurate, diverting resources, or worsening outcomes, especially if the models are built without consideration for these patients.</s><s xml:id="_MbDB5CY">We then describe the mechanisms by which a model's design, data, and deployment may lead to disparities; explain how different approaches to distributive justice in machine learning can advance health equity; and explore what contexts are more appropriate for different equity approaches in machine learning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MZrspHC">CASE STUDY 1: INTENSIVE CARE UNIT MONITORING</head><p xml:id="_dsy5nqp"><s xml:id="_BdrRkb7">A common area of predictive modeling research focuses on creating a monitoring systemfor example, to warn a rapid response team about inpatients at high risk for deterioration <ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref>, requiring their transfer to an intensive care unit within 6 hours.</s><s xml:id="_HdSCybh">How might such a system inadvertently result in harm to a protected group?</s><s xml:id="_qgPHxgV">In this thought experiment, we consider African Americans as a protected group.</s></p><p xml:id="_e7EzJd3"><s xml:id="_Ef3vQea">To build the model, our hypothetical researchers collected historical records of patients who had clinical deterioration and those who did not.</s><s xml:id="_uhT5938">The model acts like a "diagnostic test" of risk for intensive care unit transfer.</s><s xml:id="_tJeMj22">However, if too few African American patients were included in the training data-the data used to construct the model-the model might be inaccurate for them.</s><s xml:id="_6RhXmau">For example, it might have a lower sensitivity and miss more patients at risk for deterioration.</s><s xml:id="_gukJY6T">African American patients might be harmed if clinical teams started relying on alerts to identify at-risk patients without realizing that the prediction system underdetects patients in that group (automation bias) <ref type="bibr" target="#b15">(16)</ref>.</s><s xml:id="_kYHgdjK">If the model had a lower positive predictive value for African Americans, it might also disproportionately harm them through dismissal bias-a generalization of alert fatigue in which clinicians may learn to discount or "dismiss" alerts for African Americans because they are more likely to be false-positive <ref type="bibr" target="#b16">(17)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qW4YStJ">CASE STUDY 2: REDUCING LENGTH OF STAY</head><p xml:id="_gXDpZ6D"><s xml:id="_Gqz4D4Y">Imagine that a hospital created a model with clinical and social variables to predict which inpatients might be discharged earliest so that it could direct limited case management resources to them to prevent delays.</s><s xml:id="_KRj9wVT">If residence in ZIP codes of socioeconomically depressed or predominantly African American neighborhoods predicted greater lengths of stay <ref type="bibr" target="#b17">(18)</ref>, this model might disproportionately allocate case management resources to patients from richer, predominantly white neighborhoods and away from African Americans in poorer ones.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_tPR8Pyc">WHAT IS MACHINE LEARNING?</head><p xml:id="_jMzQtRd"><s xml:id="_VdvyWpR">Traditionally, computer systems map inputs to outputs according to manually specified "ifthen" rules.</s><s xml:id="_AUTPScQ">With increasingly complex tasks, such as language translation, manually specifying rules becomes infeasible, and instead the mapping (or model) is learned by the system given only input examples represented through a set of features together with their desired output, referred to as labels.</s></p><p xml:id="_ssSc9J8"><s xml:id="_bVz9w9v">The quality of a model is assessed by computing evaluation metrics on data not used to build the model, such as sensitivity, specificity, or the c-statistic, which measures the ability of a model to distinguish patients with a condition from those without it <ref type="bibr" target="#b18">(19,</ref><ref type="bibr" target="#b19">20)</ref>.</s><s xml:id="_nFGfcce">Once the model's quality is deemed satisfactory, it can be deployed to make predictions on new examples for which the label is unknown when the prediction is made.</s><s xml:id="_fn85Su9">The quality of the models on retrospective data must be followed with tests of clinical effectiveness, safety, and comparison with current practice, which may require clinical trials <ref type="bibr" target="#b20">(21)</ref>.</s></p><p xml:id="_WwrVTVY"><s xml:id="_YJDJdJ5">Traditionally, statistical models for prediction, such as the pooled-cohort equation <ref type="bibr" target="#b21">(22)</ref>, have used few variables to predict clinical outcomes, such as cardiovascular risk <ref type="bibr" target="#b22">(23)</ref>.</s><s xml:id="_UPQBcMt">Modern machine-learning techniques, however, can consider many more features.</s><s xml:id="_y5Xb3jH">For example, a recent model to predict hospital readmissions examined hundreds of thousands of pieces of information, including the free text of clinical notes <ref type="bibr" target="#b23">(24)</ref>.</s><s xml:id="_yt4h6rZ">Complex data and models can drive more personalized and accurate predictions but may also make algorithms hard to understand and trust <ref type="bibr" target="#b24">(25)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_uGawMmQ">WHAT CAN CAUSE A MACHINE-LEARNING SYSTEM TO BE UNFAIR?</head><p xml:id="_GMRPrCr"><s xml:id="_gw7Y4kd">The Glossary lists key biases in the design, data, and deployment of a machine-learning model that may perpetuate or exacerbate health care disparities if left unchecked.</s><s xml:id="_K69W4AW">The Figure reveals how the various biases relate to one another and how the interactions of model predictions with clinicians and patients may exacerbate health care disparities.</s><s xml:id="_BApH6bS">Biases may arise during the design of a model.</s><s xml:id="_BUqTVDM">For example, if the label is marred by health care disparities, such as predicting the onset of clinical depression in environments where protected groups have been systematically misdiagnosed, then the model will learn to perpetuate this disparity.</s><s xml:id="_ce2WNk2">This represents a generalization of test-referral bias (26) that we refer to as label bias.</s><s xml:id="_zZ37Z3a">Moreover, the data on which the model is developed may be biased.</s><s xml:id="_a4KhzAS">Data on patients in the protected group might be distributed differently from those in the nonprotected group because of biological or nonbiological variation <ref type="bibr" target="#b8">(9,</ref><ref type="bibr" target="#b26">27)</ref>.</s><s xml:id="_wdR8NKu">For example, the data may not contain enough examples from a group to properly tailor the predictions to them (minority bias) <ref type="bibr" target="#b27">(28)</ref>, or the data set of the protected group may be less informative because features are missing not at random as a result of more fragmented care <ref type="bibr" target="#b28">(29,</ref><ref type="bibr" target="#b29">30)</ref>.</s></p><p xml:id="_Dw8tyTq"><s xml:id="_2mx6rbk">The immediate effect of these differences is that the model may not be as accurate for patients in the protected class, but the effects on patient outcomes and resource allocation are usually mediated through how clinicians and administrators interact with the model.</s><s xml:id="_d2CYVrz">For example, do clinicians trust the model even when it is wrong (automation bias) or ignore it when they should not (dismissal bias)?</s><s xml:id="_MJWvumM">Will administrators use a flawed model to determine which patients are at high risk for poor outcomes and who should then receive more assistance?</s></p><p xml:id="_pKAf7D8"><s xml:id="_uPgeefY">Patients in the protected group may also be negatively affected by privilege bias if models are not built for diseases that disproportionately affect them or if models are disproportionately deployed to areas where they do not seek care (for example, concierge practice vs. safety-net clinic) <ref type="bibr" target="#b30">(31)</ref>.</s><s xml:id="_cDhGCxW">They also may be affected by informed mistrust if protected groups distrust using models for their own care <ref type="bibr" target="#b31">(32)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_F6W5dmU">DISTRIBUTIVE JUSTICE OPTIONS IN MACHINE LEARNING</head><p xml:id="_kuhPZKZ"><s xml:id="_jjkRxKc">What can be done to mitigate the biases that make a model unfair?</s><s xml:id="_Zq6EMxh">We propose using 3 central axes inspired by principles of distributive justice to understand fairness in machine learning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Ejgwge7">Equal Outcomes</head><p xml:id="_nMpGwZc"><s xml:id="_wBgWZMk">Equal patient outcomes refers to the assurance that protected groups have equal benefit in terms of patient outcomes from the deployment of machine-learning models <ref type="bibr" target="#b32">(33)</ref>.</s><s xml:id="_6ky7RGC">A weak form of equal outcomes is ensuring that both the protected and nonprotected groups benefit similarly from a model (equal benefit); a stronger form is making sure that both groups benefit and any outcome disparity is lessened (equalized outcomes).</s><s xml:id="_br8bw7J">Ensuring equal outcomes is the most critical aspect of fairness and can be advanced by interventions proactively designed to reduce disparities <ref type="bibr" target="#b33">(34,</ref><ref type="bibr" target="#b34">35)</ref>.</s><s xml:id="_ppxwMmA">It may be hard to know in advance, though, if any well-intentioned general, nontailored intervention, whether a quality improvement project or a machine-learning system, might disproportionately harm or benefit a protected group.</s><s xml:id="_Dp46KeF">However, besides equal outcomes, other options that might advance health equity can be analyzed and addressed prospectively.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jd6fGDu">Equal Performance</head><p xml:id="_sTqr8BB"><s xml:id="_apXTb2v">If a model systematically makes errors disproportionately for patients in the protected group, it is likely to lead to unequal outcomes.</s><s xml:id="_4BVZSmB">Equal performance refers to the assurance that a model is equally accurate for patients in the protected and nonprotected groups.</s><s xml:id="_2Wa2tuz">Equal performance has 3 commonly discussed types: equal sensitivity (also known as equal opportunity <ref type="bibr" target="#b35">[36]</ref>), equal sensitivity and specificity (also known as equalized odds), and equal positive predictive value (commonly referred to as predictive parity <ref type="bibr" target="#b36">[37]</ref>).</s><s xml:id="_bz5E2rR">Not only can these metrics be calculated, but techniques exist to force models to have one of these properties <ref type="bibr" target="#b35">(36,</ref><ref type="bibr" target="#b37">(38)</ref><ref type="bibr" target="#b38">(39)</ref><ref type="bibr" target="#b39">(40)</ref><ref type="bibr" target="#b40">(41)</ref>.</s></p><p xml:id="_uV9FVTj"><s xml:id="_HBU9BSJ">When should each type of equal performance be considered?</s><s xml:id="_ctEVS7D">A higher false-negative rate in the protected group in case 1 would mean African American patients were missing the opportunity to be identified; in this case, equal sensitivity is desirable.</s><s xml:id="_qn6kxz7">A higher falsepositive rate might be especially deleterious by leading to potentially harmful interventions (such as unnecessary biopsies), motivating equal specificity.</s><s xml:id="_gG9q84R">When the positive predictive value for alerts in the protected group is lower than in the nonprotected groups, clinicians may learn that the alerts are less informative for them and act on them less (a situation known as class-specific alert fatigue).</s><s xml:id="_y3eRa37">Ensuring equal positive predictive value is desirable in this case.</s></p><p xml:id="_Nd5fjMM"><s xml:id="_hxtH7UN">Equal performance, however, may not necessarily translate to equal outcomes.</s><s xml:id="_xpvsmXA">First, the recommended treatment informed by the prediction may be less effective for patients in the protected group (for example, because of different responses to medications and a lack of research on heterogeneous treatment effects <ref type="bibr" target="#b41">[42]</ref>).</s><s xml:id="_wZqZT6V">Second, even if a model is inaccurate for a group, clinicians might compensate with additional vigilance, overcoming the model's deficiencies.</s></p><p xml:id="_aqJ5hjQ"><s xml:id="_ZTxQT2V">Third, forcing a model's predictions to have one of the equal performance characteristics may have unexpected consequences.</s><s xml:id="_EtSD6Vm">In case 1, ensuring that a model will detect African American and non-African American patients at equal rates (equal sensitivity) could be straightforwardly accomplished by lowering the threshold for the protected class to receive the intervention.</s><s xml:id="_AvnC9rh">This simultaneously increases the false-positive rate for this group, manifesting as more false alarms and subsequent class-specific alert fatigue.</s><s xml:id="_AZ9Xb4w">Likewise, equalized odds can be achieved by lowering accuracy for the nonprotected group, which undermines the principle of beneficence.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NGAC9XR">Equal Allocation</head><p xml:id="_KbEynDN"><s xml:id="_ZWMJeu6">Predictions are often used to allocate resources, such as in case 2, in which some patients are given additional case management.</s><s xml:id="_UmuFpmu">The third type of equity is equal allocation (also known as demographic parity <ref type="bibr" target="#b42">[43]</ref>), which ensures that the resources are proportionately allocated to patients in the protected group.</s><s xml:id="_vMmMVTz">Because the comorbidity distribution may differ across groups, the desired allocation might first be adjusted for relevant variables <ref type="bibr" target="#b43">(44)</ref>.</s><s xml:id="_FTBy3eG">This is distinct from equal performance, because allocation is determined by the rate of positive predictions (such as predictions above a threshold) without regard to their accuracy.</s></p><p xml:id="_E6gaNjP"><s xml:id="_bKFBk8b">In some cases, judging accuracy is misleading when labels have historical bias, explaining why equal allocation may be preferable.</s><s xml:id="_fGzZ23E">Consider a model to identify which patients presenting emergently with chest pain should automatically activate a cardiac catheterization team.</s><s xml:id="_3A5DkC7">If African American women were historically sent for this procedure at inappropriately low rates compared with white men <ref type="bibr" target="#b44">(45)</ref>, then "correct" predictions (based on historical data) would underidentify these women.</s><s xml:id="_WgsPdpF">Equal allocation could be used to lower the threshold for African American women so that the catheterization laboratory would be activated at equal rates across groups, thereby correcting for past bias.</s><s xml:id="_vb7aNVq">This may not necessarily translate to equal outcomes if it leads to a higher rate of false-positive activations of the laboratory with respect to actual clinical need or to a continuation of lower true-positive rates if clinicians dismiss the predictions because of the underlying bias against recommending the procedure for women.</s><s xml:id="_gSudUNK">Whether the net effect of the model is a reduction in health care disparities, especially compared with not implementing a model, is uncertain.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9PPakSm">Tradeoffs</head><p xml:id="_9CDDSQ8"><s xml:id="_dn6CcCk">The computer science community was rocked when a machine-learning model used to help predict which criminal defendants were at risk for committing a future crime was found to be unfair with respect to equalized odds: African American defendants who did not reoffend were classified as high risk at a substantially higher rate than white defendants who did not reoffend.</s><s xml:id="_DceT8jv">The model builders, however, asserted that the model had equal positive and negative predictive value across the groups <ref type="bibr" target="#b45">(46)</ref>.</s><s xml:id="_KmbF77H">Subsequent analysis revealed that various types of fairness are sometimes incompatible: A model may be fair with respect to equal positive and negative predictive value but unfair with respect to equalized odds (or vice versa), but it is impossible for any model to satisfy both.</s><s xml:id="_wsHa9fM">This impossibility also holds for equalized odds and equal allocation, and for equal allocation and equal positive and negative predictive value <ref type="bibr" target="#b36">(37)</ref>.</s><s xml:id="_VzTvmFC">Machine-learning fairness is not just for machine-learning specialists to understand; it requires clinical and ethical reasoning to determine which type of fairness is appropriate for a given application and what level of it is satisfactory.</s><s xml:id="_rT3WY7M">Although no cookiecutter solution exists, the examples and recommendations provide a starting point for this reasoning.</s><s xml:id="_eZqrbRs">We believe that in practice, satisfactory levels of the desired fairness types can be achieved.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rU6c8nX">RECOMMENDATIONS</head><p xml:id="_NjqbAr9"><s xml:id="_fRZQSdp">In the Table , we present recommendations for how to incorporate fairness into machine learning.</s><s xml:id="_76WQzdr">Researchers should consider how prior health care disparities may affect the design and data of a model.</s><s xml:id="_xk7wcfr">For example, if advanced-stage melanoma is diagnosed more frequently in patients with dark skin than in other groups, might a skin cancer detection model fail to detect early-stage disease in patients with dark skin <ref type="bibr" target="#b46">(47,</ref><ref type="bibr" target="#b47">48)</ref>?</s><s xml:id="_2jC7p4T">During training and evaluation, researchers should measure any deviations from equal accuracy and equal allocation, and consider mitigating them by using techniques during training <ref type="bibr" target="#b37">(38)</ref><ref type="bibr" target="#b38">(39)</ref><ref type="bibr" target="#b39">(40)</ref> or by postprocessing a trained model <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41)</ref>.</s><s xml:id="_yj2PH6a">Before deployment, launch reviews should formally assess model performance and allocation of resources across groups.</s><s xml:id="_BHWS8MY">The reviews should determine whether a model promotes equal outcomes, broadly defined as "the patient's care experience, functional status, and quality of life, as well as… personalization of care and resource stewardship" <ref type="bibr" target="#b48">(49)</ref>.</s><s xml:id="_c8Npa6b">If a model is deployed, the performance of the model and outcome measurements should be monitored, possibly through formal trial design (such as stepped-wedge trials <ref type="bibr" target="#b49">[50]</ref>).</s><s xml:id="_TqU2v8n">Moreover, the model may be improved over time by collecting more representative or less biased data.</s></p><p xml:id="_HUT7T3x"><s xml:id="_wJwkdYa">We purposefully do not recommend the commonly discussed fairness principle of "unawareness," which states that a model should not use the membership of the group as a feature.</s><s xml:id="_8hbjy9j">Complex models can infer a protected attribute even if it is not explicitly coded in a data set, such as a model identifying a patient's self-reported sex from a retinal image even though ophthalmologists cannot <ref type="bibr" target="#b50">(51)</ref>.</s><s xml:id="_K8ppFy2">Moreover, removing features may lead to poorer performance for all patients.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Exqdxbm">CONCLUSIONS</head><p xml:id="_F9qtMhb"><s xml:id="_e3nHNNA">Consideration of fairness in machine learning allows us to reexamine historical bias and proactively promote a more equitable future.</s><s xml:id="_pvsjddW">We are optimistic that machine learning can substantially improve the care delivered to patients if it is thoughtfully designed and deployed.</s><s xml:id="_SfTdBed">Case 2 is based on a University of Chicago Medicine example in which data scientists from the Center for Healthcare Delivery Science and Innovation collaborated with experts from the Diversity and Equity Committee to identify the equity problem and to design a local checklist for model building and deployment that advances equity.</s></p><p xml:id="_q2bT8Uy"><s xml:id="_cdrGtAu">Machine-learning fairness is not just about preventing a model from harming a protected group; it may also help focus care where it is really needed.</s><s xml:id="_Xt2yeHn">Models could be used to provide translation services where inperson interpreters are scarce, provide medical expertise in areas with a limited number of specialists, and even improve diagnostic accuracy for rare conditions that are often misdiagnosed.</s><s xml:id="_ea6BGAY">By including fairness as a central consideration in how the models are designed, deployed, and evaluated, we can ensure that all patients benefit from this technology.</s></p><p xml:id="_vGdZJUK"><s xml:id="_Qwr2ynR">Grant Support: Dr. Chin was supported in part by the Chicago Center for Diabetes Translation Research (grant NIDDK P30 DK092949), Robert Wood Johnson Foundation Finding Answers: Solving Disparities Through Payment and Delivery Reform Program Office, and Merck Foundation Bridging the Gap: Reducing Disparities in Diabetes Care National Program Office.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WwpGqHn">Glossary</head><p xml:id="_F5YuXqe"><s xml:id="_zKjthMw">Biases in model design Label bias: A label that does not mean the same thing for all patients because it is an imperfect proxy that is subject to health care disparities rather than an adjudicated truth.</s><s xml:id="_ysUpmqK">This is a generalization of test-referral and test-interpretation bias in the statistics literature Cohort bias: Defaulting to traditional or easily measured groups without considering other potentially protected groups or levels of granularity (e.g., whether sex is recorded as male, female, or other or more granular categories)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GTtjM3T">Biases in training data</head><p xml:id="_4jZVZJj"><s xml:id="_4rNHH6b">Minority bias: The protected group may have insufficient numbers of patients for a model to learn the correct statistical patterns Missing data bias: Data may be missing for protected groups in a nonrandom fashion, which makes an accurate prediction hard to render (e.g., a model may underdetect clinical deterioration in patients under contact isolation because they have fewer vital signs) Informativeness bias: Features may be less informative to render a prediction in a protected group (e.g., identifying melanoma from an image of a patient with dark skin may be more difficult)</s></p><p xml:id="_Xspqjd3"><s xml:id="_X6MV64D">Training-serving skew: The model may be deployed on patients whose data are not similar to the data on which the model was trained.</s><s xml:id="_EZmWXJB">The training data may not be representative (i.e., selection bias), or the deployment data may differ from the training data (e.g., a lack of unified methods for data collection or not recording data with standardized schemas)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_xkD57BF">Biases in interactions with clinicians</head><p xml:id="_zgBvDfQ"><s xml:id="_N9ZhjkT">Automation bias: If clinicians are unaware that a model is less accurate for a specific group, they may trust it too much and inappropriately act on inaccurate predictions Feedback loops: If the clinician accepts the recommendation of a model even when it is incorrect to do so, the model's recommended versus administered treatments will always match.</s><s xml:id="_qSZaRnz">The next time the model is trained, it will learn to continue these mistakes Dismissal bias: Conscious or unconscious desensitization to alerts that are systematically incorrect for a protected group (e.g., an early-warning score for patients with sepsis).</s><s xml:id="_MdmppEu">Alert fatigue is a form of this Allocation discrepancy: If the protected group has disproportionately fewer positive predictions, then resources allocated by the predictions (e.g., extra clinical attention or social services) are withheld from that group Biases in interactions with patients Privilege bias: Models may be unavailable in settings where protected groups receive care or require technology/sensors disproportionately available to the nonprotected class Informed mistrust: Given historical exploitation and unethical practices, protected groups may believe that a model is biased against them.</s><s xml:id="_w5bTUzB">These patients may avoid seeking care from clinicians or systems that use the model or deliberately omit information.</s><s xml:id="_bSy7RJH">The protected group may be harmed by not receiving appropriate care  During model development, differences in the distribution of features used to predict a label between the protected and nonprotected groups may bias a model to be less accurate for protected groups.</s><s xml:id="_BNXjtbb">Moreover, the data used to develop a model may not generalize to the data used during model deployment (training-serving skew).</s><s xml:id="_XcndDuK">Biases in model design and data affect patient outcomes through the model's interaction with clinicians and patients.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_37cRr8J"><s xml:id="_ZggtqFp">Agency bias: Protected groups may not have input into the development, use, and evaluation of models.</s><s xml:id="_dQu4JaD">They may not have the resources, education, or political influence to detect biases, protest, and force correction Distributive justice options for machine learning Equal patient outcomes: The model should lead to equal patient outcomes across groups Equal performance: The model performs equally well across groups for such metrics as accuracy, sensitivity, specificity, and positive predictive value Equal allocation: Allocation of resources as decided by the model is equal across groups, possibly after controlling for all relevant factors</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure .</head><label>.</label><figDesc><div><p xml:id="_d2xNsaH"><s xml:id="_ABdDWsE">Figure.</s><s xml:id="_sAjNhPX">Conceptual framework of how various biases relate to one another.</s></p></div></figDesc><graphic coords="13,89.76,62.00,492.48,233.65" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_CZjgeUP"><s xml:id="_5D4Q7nf">Ann Intern Med.</s><s xml:id="_PtAPxUf">Author manuscript; available in PMC 2019 June 26.</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_UAjdxwU">Acknowledgment:</head><p xml:id="_6AKRBfw"><s xml:id="_kjYCBWf">The authors thank <rs type="person">Meredith Whittaker</rs>, <rs type="person">Roxanne Pinto</rs>, <rs type="person">Moritz Hardt</rs>, <rs type="person">Gerardo Flores</rs>, <rs type="person">Charina Chou</rs>, <rs type="person">Kathryn Rough</rs>, <rs type="person">Ashley Hayes</rs>, <rs type="person">Jutta Williams</rs>, <rs type="person">Katherine Chou</rs>, <rs type="person">Andrew Smart</rs>, <rs type="person">Alex Beutel</rs>, <rs type="person">Valeria Espinosa</rs>, <rs type="person">Adam Sadilek</rs>, <rs type="person">Kaspar Molzberger</rs>, and <rs type="person">Yoni Halpern</rs> for insightful comments on the interplay of fairness in building and deploying machine learning and for helpful comments on early versions of the manuscript.</s><s xml:id="_qsR9cBQ">They also thank <rs type="person">John Fahrenbach</rs>, <rs type="person">James Williams</rs>, and their colleagues from the <rs type="institution">Chicago Center for Diabetes Translation Research</rs> for providing critical analysis of the ideas of the manuscript applied to real clinical prediction tasks.</s></p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6ueqv6y">Table.</head><p xml:id="_5vJ7Bqm"><s xml:id="_KYryVjr">Recommendations Design Determine the goal of a machine-learning model and review it with diverse stakeholders, including protected groups.</s><s xml:id="_v27yX9H">Ensure that the model is related to the desired patient outcome and can be integrated into clinical workflows.</s><s xml:id="_BM4KFv6">Discuss ethical concerns of how the model could be used.</s><s xml:id="_We8fkJm">Decide what groups to classify as protected.</s><s xml:id="_Jt8X5bn">Study whether the historical data are affected by health care disparities that could lead to label bias.</s><s xml:id="_gx9bKv2">If so, investigate alternative labels.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_quB58sG">Data collection</head><p xml:id="_5ddYWXU"><s xml:id="_dJvfxmj">Collect and document training data to build a machine-learning model.</s><s xml:id="_fmF8vst">Ensure that patients in the protected group can be identified (weighing cohort bias against privacy concerns).</s><s xml:id="_726aJQh">Assess whether the protected group is represented adequately in terms of numbers and features.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wPM2eHR">Training</head><p xml:id="_Qb2RRjt"><s xml:id="_rr5dKmt">Train a model taking into account the fairness goals.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_a4MCKhq">Evaluation</head><p xml:id="_6F59Fhc"><s xml:id="_eG3J7j8">Measure important metrics and allocation across groups.</s><s xml:id="_AczxsWK">Compare deployment data with training data to ensure comparability.</s><s xml:id="_CU74MAy">Assess the usefulness of predictions to clinicians initially without affecting patients.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8DGF4J8">Launch review</head><p xml:id="_duKCAYZ"><s xml:id="_x7KZKfZ">Evaluate whether a model should be launched with all stakeholders, including representatives from the protected group.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9afUFhZ">Monitored deployment</head><p xml:id="_X6tQn5T"><s xml:id="_sFMTb2x">Systematically monitor data and important metrics throughout deployment.</s><s xml:id="_Dje5QuM">Gradually launch and continuously evaluate metrics with automated alerts.</s><s xml:id="_nCCjXuB">Consider a formal clinical trial design to assess patient outcomes.</s><s xml:id="_448aK3Q">Periodically collect feedback from clinicians and patients.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_jSFfEqN">Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahimy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Widner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ophtha.2018.01.034</idno>
		<idno type="PMID">29548646</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2GyD5J6">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="1264" to="1272" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krause J, Gulshan V, Rahimy E, Karth P, Widner K, Corrado GS, et al. Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy. Ophthalmology 2018;125:1264-72. doi:10.1016/j.ophtha.2018.01.034 [PubMed: 29548646]</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_Q4XP3a3">Machine bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mattu</surname></persName>
		</author>
		<ptr target="www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencingon13" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_R8pvrqk">ProPublica</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016-12">2016. December 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Angwin J, Larson J, Kirchner L, Mattu S. Machine bias. ProPublica 23 5 2016 Accessed at www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing on 13 December 2017.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_wnembB9">Inherent trade-offs in algorithmic fairness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219617.3219634</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_gjxVnpc">Abstracts of the 2018 Association for Computing Machinery International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting><address><addrLine>Irvine, California; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018-06-22">18 -22 June 2018. 2018</date>
			<biblScope unit="page">40</biblScope>
		</imprint>
	</monogr>
	<note>Abstract</note>
	<note type="raw_reference">Kleinberg J. Inherent trade-offs in algorithmic fairness [Abstract].. Abstracts of the 2018 Association for Computing Machinery International Conference on Measurement and Modeling of Computer Systems; Irvine, California. 18 -22 June 2018; New York: Association for Computing Machinery; 2018. 40</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_DqxNcdn">To predict and serve?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Isaac</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1740-9713.2016.00960.x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hF5yzv9">Significance</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lum K, Isaac W. To predict and serve? Significance 2016;13:14-9.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_FXhgJnv">A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Benavides-Prado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fialko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vaithianathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MnZnjZR">Proc Mach Learn Res</title>
		<imprint>
			<biblScope unit="page" from="134" to="148" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chouldechova A, Benavides-Prado D, Fialko O, Vaithianathan R. A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions. Proc Mach Learn Res 2018: 134-48.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_csxm7An">Can an algorithm tell when kids are in danger?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hurley</surname></persName>
		</author>
		<ptr target="www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in-danger.html" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_8dzSKcW">The New York Times Magazine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018-01">2018. January 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hurley D Can an algorithm tell when kids are in danger? The New York Times Magazine 2 1 2018 Accessed at www.nytimes.com/2018/01/02/magazine/can-an-algorithm-tell-when-kids-are-in- danger.html on 2 January 2018.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<idno type="DOI">10.1001/amajethics.2019.188</idno>
		<ptr target="www.ama-assn.org/ama-passes-first-policy-recommendations-augmented-" />
		<title level="m" xml:id="_jYPfkMg">AMA passes first policy recommendations on augmented intelligence</title>
		<imprint>
			<publisher>American Medical Association</publisher>
			<date type="published" when="2018-07-06">2018. 6 July 2018</date>
		</imprint>
	</monogr>
	<note>Accessed at</note>
	<note type="raw_reference">American Medical Association. AMA passes first policy recommendations on augmented intelligence 2018 Accessed at www.ama-assn.org/ama-passes-first-policy-recommendations- augmented-intelligence on 6 July 2018.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_Mx4UJd8">Lessons for achieving health equity comparing Aotearoa/New Zealand and the United States</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Ameratunga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Muramatsu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.healthpol.2018.05.001</idno>
		<idno type="PMID">29961558</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cANNbM9">Health Policy</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="837" to="853" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chin MH, King PT, Jones RG, Jones B, Ameratunga SN, Muramatsu N, et al. Lessons for achieving health equity comparing Aotearoa/New Zealand and the United States. Health Policy 2018; 122:837-53. doi:10.1016/j.healthpol.2018.05.001 [PubMed: 29961558]</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Smedley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Stith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Nelson</surname></persName>
		</author>
		<title level="m" xml:id="_wSzVBsz">Institute of Medicine. Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care Washington</title>
		<meeting><address><addrLine>DC</addrLine></address></meeting>
		<imprint>
			<publisher>National Academies Pr</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Smedley BD, Stith AY, Nelson AR, eds; Institute of Medicine. Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care Washington, DC: National Academies Pr; 2003.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Rothstein</surname></persName>
		</author>
		<title level="m" xml:id="_usMhVHq">The Color of Law: A Forgotten History of How Our Government Segregated America</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Liveright</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rothstein R The Color of Law: A Forgotten History of How Our Government Segregated America New York: Liveright; 2017.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno type="DOI">10.1596/31215</idno>
		<ptr target="www.healthypeople.gov/2020/About" />
		<title level="m" xml:id="_FMGUeAe">Healthy People 2020 About Healthy People</title>
		<imprint>
			<date type="published" when="2018-10-09">2018. 9 October 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Healthy People 2020 About Healthy People. 2018 Accessed at www.healthypeople.gov/2020/ About-Healthy-People on 9 October 2018.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_U9khbWt">Deep learning-a technology with the potential to transform health care</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2018.11100</idno>
		<idno type="PMID">30178065</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9KV5zmK">JAMA</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="1101" to="1102" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hinton G Deep learning-a technology with the potential to transform health care. JAMA 2018;320:1101-2. doi:10.1001/jama.2018.11100 [PubMed: 30178065]</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_meF8rQ2">Piloting electronic medical record-based early detection of inpatient deterioration in community hospitals</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ragins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hoberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Levine</surname></persName>
		</author>
		<idno type="DOI">10.1002/jhm.2652</idno>
		<idno type="PMID">27805795</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ymQAhgr">J Hosp Med</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">Suppl 1</biblScope>
			<biblScope unit="page" from="18" to="24" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Escobar GJ, Turk BJ, Ragins A, Ha J, Hoberman B, LeVine SM, et al. Piloting electronic medical record-based early detection of inpatient deterioration in community hospitals. J Hosp Med 2016;11 Suppl 1:S18-24. doi:10.1002/jhm.2652 [PubMed: 27805795]</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_Y4B8Drv">Finding patients before they crash: the next major opportunity to improve patient safety [Editorial]</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zimlichman</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjqs-2014-003499</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KP7f7eh">BMJ Qual Saf</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bates DW, Zimlichman E. Finding patients before they crash: the next major opportunity to improve patient safety [Editorial]. BMJ Qual Saf 2015;24:1-3. doi:10.1136/bmjqs-2014-003499</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_5AcbYwV">Big data in health care: using analytics to identify and manage high-risk and high-cost patients</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ohno-Machado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Escobar</surname></persName>
		</author>
		<idno type="DOI">10.1377/hlthaff.2014.0041</idno>
		<idno type="PMID">25006137</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2mQ9KnZ">Health Aff (Millwood)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1123" to="1131" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bates DW, Saria S, Ohno-Machado L, Shah A, Escobar G. Big data in health care: using analytics to identify and manage high-risk and high-cost patients. Health Aff (Millwood) 2014;33:1123-31. doi:10.1377/hlthaff.2014.0041 [PubMed: 25006137]</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_T3Vauwa">Automation bias and verification complexity: a systematic review</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lyell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Coiera</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocw105</idno>
		<idno type="PMID">27516495</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qQgEr8j">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="423" to="431" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lyell D, Coiera E. Automation bias and verification complexity: a systematic review. J Am Med Inform Assoc 2017;24:423-31. doi:10.1093/jamia/ocw105 [PubMed: 27516495]</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_x6bbFsM">Insights into the problem of alarm fatigue with physiologic monitor devices: a comprehensive observational study of consecutive intensive care unit patients</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Zègre-Hemsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mammone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salas-Boni</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0110274</idno>
		<idno type="PMID">25338067</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xzkKhDn">PLoS One</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">110274</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Drew BJ, Harris P, Zègre-Hemsey JK, Mammone T, Schindler D, Salas-Boni R, et al. Insights into the problem of alarm fatigue with physiologic monitor devices: a comprehensive observational study of consecutive intensive care unit patients. PLoS One 2014;9: e110274. doi:10.1371/ journal.pone.0110274 [PubMed: 25338067]</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_FbAgCHB">The association of patients&apos; socioeconomic characteristics with the length of hospital stay and hospital charges within diagnosis-related groups</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tognetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Begg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cumella</forename><forename type="middle">E</forename><surname>Jr</surname></persName>
		</author>
		<idno type="DOI">10.1056/nejm198806163182405</idno>
		<idno type="PMID">3131674</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8cN74jg">N Engl J Med</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="page" from="1579" to="1585" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Epstein AM, Stern RS, Tognetti J, Begg CB, Hartley RM, Cumella E Jr, et al. The association of patients&apos; socioeconomic characteristics with the length of hospital stay and hospital charges within diagnosis-related groups. N Engl J Med 1988;318:1579-85. [PubMed: 3131674]</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_uTWJkqq">Use and misuse of the receiver operating characteristic curve in risk prediction</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.1161/circulationaha.106.672402</idno>
		<idno type="PMID">17309939</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uf4R7sW">Circulation</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="928" to="935" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cook NR. Use and misuse of the receiver operating characteristic curve in risk prediction. Circulation 2007;115:928-35. [PubMed: 17309939]</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_MmbCzGH">Statistical evaluation of prognostic versus diagnostic models: beyond the ROC curve</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Cook</surname></persName>
		</author>
		<idno type="DOI">10.1373/clinchem.2007.096529</idno>
		<idno type="PMID">18024533</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6zXMXG9">Clin Chem</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="17" to="23" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cook NR. Statistical evaluation of prognostic versus diagnostic models: beyond the ROC curve. Clin Chem 2008;54:17-23. [PubMed: 18024533]</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_aH2MKRd">With an eye to AI and autonomous diagnosis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-018-0048-y</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hdeq2F2">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Keane PA, Topol EJ. With an eye to AI and autonomous diagnosis. NPJ Digit Med 28 8 2018;1:40.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_43RMyPK">Validation of the atherosclerotic cardiovascular disease pooled cohort risk equations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Muntner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Colantonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName><surname>Jr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Howard</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2014.2630</idno>
		<idno type="PMID">24682252</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8bnUTp6">JAMA</title>
		<imprint>
			<biblScope unit="volume">311</biblScope>
			<biblScope unit="page" from="1406" to="1415" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Muntner P, Colantonio LD, Cushman M, Goff DC Jr, Howard G, Howard VJ, et al. Validation of the atherosclerotic cardiovascular disease pooled cohort risk equations. JAMA 2014;311:1406-15. doi:10.1001/jama.2014.2630 [PubMed: 24682252]</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_zE363jZ">Big data and machine learning in health care</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Beam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.18391</idno>
		<idno type="PMID">29532063</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JV3pg7q">JAMA</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page" from="1317" to="1318" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beam AL, Kohane IS. Big data and machine learning in health care. JAMA 2018;319:1317-8. doi: 10.1001/jama.2017.18391 [PubMed: 29532063]</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_caCMBEp">Scalable and accurate deep learning with electronic health records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajkomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><forename type="middle">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Hajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wUXRP9r">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rajkomar A, Oren E, Chen K, Dai AM, Hajaj N, Hardt M, et al. Scalable and accurate deep learning with electronic health records. NPJ Digit Med 8 5 2018;1:18.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_ttDmkQk">Unintended consequences of machine learning in medicine</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cabitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rasoini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Gensini</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.7797</idno>
		<idno type="PMID">28727867</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KjbWWK4">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="page" from="517" to="518" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cabitza F, Rasoini R, Gensini GF. Unintended consequences of machine learning in medicine. JAMA 2017;318:517-8. doi:10.1001/jama.2017.7797 [PubMed: 28727867]</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_zJrnuVB">Biomedical decision making: probabilistic clinical reasoning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Sox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VNqGcCH">Biomedical Informatics</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Shortliffe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Cimino</surname></persName>
		</editor>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="67" to="107" />
		</imprint>
	</monogr>
	<note type="raw_reference">Owens DK, Sox HC. Biomedical decision making: probabilistic clinical reasoning. In: Shortliffe EH, Cimino JJ, eds. Biomedical Informatics London: Springer-Verlag; 2014:67-107.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_rgdBgS8">In the era of precision medicine and big data, who is normal?</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Manrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jpa</forename><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2018.2009</idno>
		<idno type="PMID">29710130</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7zK9pZP">JAMA</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page" from="1981" to="1982" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Manrai AK, Patel CJ, Ioannidis JPA. In the era of precision medicine and big data, who is normal? JAMA 2018;319:1981-2. doi:10.1001/jama.2018.2009 [PubMed: 29710130]</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_kf3V4Bt">Fairness in precision medicine</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pitcan</surname></persName>
		</author>
		<idno type="DOI">10.1201/9780429295010-2</idno>
		<ptr target="https://datasociety.net/research/fairness-precision-medicineon31" />
		<imprint>
			<date type="published" when="2018-05">2018. May 2018</date>
			<publisher>Data &amp; Society</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Ferryman K, Pitcan M. Fairness in precision medicine. Data &amp; Society 2018 Accessed at https:// datasociety.net/research/fairness-precision-medicine on 31 May 2018.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_SEez6cT">Reduction of peripartum racial and ethnic disparities: a conceptual framework and maternal safety consensus bundle</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brumley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Caughey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Cornell</surname></persName>
		</author>
		<idno type="DOI">10.1097/aog.0000000000002475</idno>
		<idno type="PMID">29683895</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5t8TezK">Obstet Gynecol</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="770" to="782" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Howell EA, Brown H, Brumley J, Bryant AS, Caughey AB, Cornell AM, et al. Reduction of peripartum racial and ethnic disparities: a conceptual framework and maternal safety consensus bundle. Obstet Gynecol 2018;131:770-82. doi:10.1097/AOG.0000000000002475 [PubMed: 29683895]</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_KrCqZ4p">Potential biases in machine learning algorithms using electronic health record data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gianfrancesco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tamang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yazdany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schmajuk</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamainternmed.2018.3763</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PThGQrG">JAMA Intern Med</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gianfrancesco MA, Tamang S, Yazdany J, Schmajuk G. Potential biases in machine learning algorithms using electronic health record data. JAMA Intern Med 2018. doi:10.1001/ jamainternmed.2018.3763</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_msWehcG">Good intentions are not enough: how informatics interventions can worsen inequality</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Veinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ancker</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocy052</idno>
		<idno type="PMID">29788380</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XbEv7RE">J Am Med Inform Assoc</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1080" to="1088" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Veinot TC, Mitchell H, Ancker JS. Good intentions are not enough: how informatics interventions can worsen inequality. J Am Med Inform Assoc 2018;25:1080-8. doi:10.1093/jamia/ocy052 [PubMed: 29788380]</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_Stz9rpq">Digital phenotyping: technology for a new science of behavior</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Insel</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.11295</idno>
		<idno type="PMID">28973224</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YmthU8M">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="page" from="1215" to="1216" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Insel TR. Digital phenotyping: technology for a new science of behavior. JAMA 2017;318:1215-6. doi:10.1001/jama.2017.11295 [PubMed: 28973224]</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_T5ZVFZC">The legal and ethical concerns that arise from using complex predictive analytics in health care</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amarasingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lo</surname></persName>
		</author>
		<idno type="DOI">10.1377/hlthaff.2014.0048</idno>
		<idno type="PMID">25006139</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WwpJUAT">Health Aff (Millwood)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1139" to="1147" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cohen IG, Amarasingham R, Shah A, Xie B, Lo B. The legal and ethical concerns that arise from using complex predictive analytics in health care. Health Aff (Millwood) 2014;33:1139-47.doi: 10.1377/hlthaff.2014.0048 [PubMed: 25006139]</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_gTK86KP">A roadmap and best practices for organizations to reduce racial and ethnic disparities in health care</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Nocon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Casey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Goddu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Keesecker</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11606-012-2082-9</idno>
		<idno type="PMID">22798211</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tNTPM3P">J Gen Intern Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="992" to="1000" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chin MH, Clarke AR, Nocon RS, Casey AA, Goddu AP, Keesecker NM, et al. A roadmap and best practices for organizations to reduce racial and ethnic disparities in health care. J Gen Intern Med 2012; 27:992-1000. doi:10.1007/s11606-012-2082-9 [PubMed: 22798211]</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" xml:id="_AZj3FJh">Systems Practices for the Care of Socially At-Risk Populations</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>National Academies Pr</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>National Academies of Sciences, Engineering, and Medicine</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">National Academies of Sciences, Engineering, and Medicine, ed. Systems Practices for the Care of Socially At-Risk Populations Washington, DC: National Academies Pr; 2016.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_Gs8xms6">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4DbvWAq">Proceedings from the Conference on Advances in Neural Information Processing Systems 2016</title>
		<meeting>from the Conference on Advances in Neural Information Processing Systems 2016<address><addrLine>Barcelona, Spain; La Jolla, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems</publisher>
			<date type="published" when="2017-12-10">5-10 December 2017. 2017</date>
			<biblScope unit="page" from="3315" to="3323" />
		</imprint>
	</monogr>
	<note>Abstract</note>
	<note type="raw_reference">Hardt M, Price E, Srebro N. Equality of opportunity in supervised learning [Abstract]. In: Proceedings from the Conference on Advances in Neural Information Processing Systems 2016, Barcelona, Spain, 5-10 December 2017 La Jolla, CA: Neural Information Processing Systems; 2017:3315-23.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_zpZWkyK">Fair prediction with disparate impact: a study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chouldechova</surname></persName>
		</author>
		<idno type="DOI">10.1089/big.2016.0047</idno>
		<idno type="PMID">28632438</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AbndkxW">Big Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chouldechova A Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. Big Data 2017;5:153-63. doi:10.1089/big.2016.0047 [PubMed: 28632438]</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_Yfys2kY">Learning non-discriminatory predictors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Ohannessian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8bscNEz">Proc Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1920" to="1953" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Woodworth B, Gunasekar S, Ohannessian MI, Srebro N. Learning non-discriminatory predictors. Proc Mach Learn Res 2017;65: 1920-53.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_eKDjvK5">Data decisions and theoretical implications when adversarially learning fair representations Accessed</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1707.00075.pdf" />
		<imprint>
			<date type="published" when="2018-05-08">8 May 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beutel A, Chen J, Zhao Z, Chi EH. Data decisions and theoretical implications when adversarially learning fair representations Accessed at https://arxiv.org/pdf/1707.00075.pdf on 8 May 2018.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_rfPUMh5">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1801.07593on8" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_GrWrQ9f">Artificial Intelligence, Ethics, and Society</title>
		<imprint>
			<date type="published" when="2018-05">2018. May 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang BH, Lemoine B, Mitchell M. Mitigating unwanted biases with adversarial learning. Artificial Intelligence, Ethics, and Society 2018 Accessed at http://arxiv.org/abs/1801.07593 on 8 May 2018</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_WR72bHX">Probabilities for SV machines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/1113.003.0008</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wsfqPwn">Advances in Large Margin Classifiers</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Pr</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
	<note type="raw_reference">Platt JC. Probabilities for SV machines. In: Smola AJ, Bartlett PJ, Schuurmans D, Schölkopf B, eds. Advances in Large Margin Classifiers Cambridge, MA: MIT Pr; 1999:61-74.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_8TjBFpY">Inclusion of demographic-specific information in studies supporting US Food &amp; Drug Administration approval of high-risk medical devices</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Dhruva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Mazure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Redberg</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamainternmed.2017.3148</idno>
		<idno type="PMID">28738116</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Z8vdhWK">JAMA Intern Med</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="1390" to="1391" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dhruva SS, Mazure CM, Ross JS, Redberg RF. Inclusion of demographic-specific information in studies supporting US Food &amp; Drug Administration approval of high-risk medical devices. JAMA Intern Med 2017;177:1390-1. doi:10.1001/jamainternmed.2017.3148 [PubMed: 28738116]</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_c2Kr9jp">On fairness and calibration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_X622kag">Proceedings from the Conference on Advances in Neural Information Processing Systems 2017</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<meeting>from the Conference on Advances in Neural Information Processing Systems 2017<address><addrLine>Long Beach, California; La Jolla, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems</publisher>
			<date type="published" when="2017-12-09">4-9 December 2017. 2017</date>
			<biblScope unit="page" from="5680" to="5689" />
		</imprint>
	</monogr>
	<note type="raw_reference">Pleiss G, Raghavan M, Wu F, Kleinberg J, Weinberger KQ. On fairness and calibration. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan, et al., eds. Proceedings from the Conference on Advances in Neural Information Processing Systems 2017, Long Beach, California, 4-9 December 2017 La Jolla, CA: Neural Information Processing Systems; 2017:5680-9.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_DMdudDP">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7503.001.0001</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EPE9UmZ">Proceedings from the Conference on Advances in Neural Information Processing Systems 2017</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<meeting>from the Conference on Advances in Neural Information Processing Systems 2017<address><addrLine>Long Beach, CA; La Jolla, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Neural Information Processing Systems</publisher>
			<date type="published" when="2017-12-09">4-9 December 2017. 2017</date>
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kilbertus N, Rojas-Carulla M, Parascandolo G, Hardt M, Janzing D, Schölkopf B. Avoiding discrimination through causal reasoning. In: Guyon I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, et al., eds. Proceedings from the Conference on Advances in Neural Information Processing Systems 2017, Long Beach, CA, 4-9 December 2017 La Jolla, CA: Neural Information Processing Systems; 2017:656-66.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_sNtnP36">The effect of race and sex on physicians&apos; recommendations for cardiac catheterization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Berlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Harless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Kerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sistrunk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Gersh</surname></persName>
		</author>
		<idno type="DOI">10.1056/nejm199902253400806</idno>
		<idno type="PMID">10029647</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CzvguAc">N Engl J Med</title>
		<imprint>
			<biblScope unit="volume">340</biblScope>
			<biblScope unit="page" from="618" to="626" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schulman KA, Berlin JA, Harless W, Kerner JF, Sistrunk S, Gersh BJ, et al. The effect of race and sex on physicians&apos; recommendations for cardiac catheterization. N Engl J Med 1999;340:618-26. [PubMed: 10029647]</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_4YUQ86R">COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity Golden</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dieterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RaarqYw">North-pointe</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016">2016</date>
			<pubPlace>CO</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Dieterich W, Mendoza C, Brennan T. COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity Golden, CO: North-pointe; 7 2016.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_U7Xc248">Machine learning and health care disparities in dermatology</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Adamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamadermatol.2018.2348</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hYZvWxw">JAMA Dermatol</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Adamson AS, Smith A. Machine learning and health care disparities in dermatology. JAMA Dermatol 2018. doi:10.1001/jamadermatol.2018.2348</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_eR4c8xU">Skin cancer in skin of color</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gloster</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Neal</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jaad.2005.08.063</idno>
		<idno type="PMID">17052479</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_taBQqnw">J Am Acad Dermatol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="741" to="760" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gloster HM Jr, Neal K. Skin cancer in skin of color. J Am Acad Dermatol 2006;55:741-60. [PubMed: 17052479]</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_9VDgKt6">Emerging from EHR purgatory-moving from process to outcomes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Goroll</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMp1700601</idno>
		<idno type="PMID">28538132</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qN8uYdf">N Engl J Med</title>
		<imprint>
			<biblScope unit="volume">376</biblScope>
			<biblScope unit="page" from="2004" to="2006" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Goroll AH. Emerging from EHR purgatory-moving from process to outcomes. N Engl J Med 2017;376:2004-6. doi:10.1056/NEJMp1700601 [PubMed: 28538132]</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_kENuCb2">How to study improvement interventions: a brief overview of possible study types</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Portela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Pronovost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Woodcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dixon-Woods</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjqs-2014-003620</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UUAKqwN">BMJ Qual Saf</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="325" to="336" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Portela MC, Pronovost PJ, Woodcock T, Carter P, Dixon-Woods M. How to study improvement interventions: a brief overview of possible study types. BMJ Qual Saf 2015;24:325-36. doi: 10.1136/bmjqs-2014-003620</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_nzsv7pd">Predicting cardiovascular risk factors from retinal fundus photographs using deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Mcconnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_D89bPqz">Nat Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="158" to="164" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Poplin R, Varadarajan AV, Blumer K, Liu Y, McConnell MV, Corrado GS, et al. Predicting cardiovascular risk factors from retinal fundus photographs using deep learning. Nat Biomed Eng 2017;2: 158-64.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
