<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_EuDq47u">Recognizing Unintentional Touch on Interactive Tabletop</title>
				<funder>
					<orgName type="full">Beijing Key Lab of Networked Multimedia</orgName>
				</funder>
				<funder ref="#_pGzRxqJ">
					<orgName type="full">National Key Research and Development Plan</orgName>
				</funder>
				<funder ref="#_jPaA6nE #_Vp2n4Tt">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
							<email>xuhaixu@uw.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chun</forename><surname>Yu</surname></persName>
							<email>chunyu@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yuntao</forename><surname>Wang</surname></persName>
							<email>yuntaowang@mail.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yuanchun</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">Tsinghua University and University of Washington Tsinghua University Tsinghua University , Search Results 30 Shuangqing Rd , Beijing , Beijing , 100084 ,</note>
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<orgName type="institution" key="instit4">Tsinghua University</orgName>
								<address>
									<addrLine>Search Results 30 Shuangqing Rd</addrLine>
									<postCode>100084</postCode>
									<settlement>Beijing Beijing</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">University of Washington , 1410 NE Campus Parkway , Seattle , WA , 98195 ; Chun Yu ,</note>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<addrLine>1410 NE Campus Parkway</addrLine>
									<postCode>98195</postCode>
									<settlement>Seattle Chun Yu</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_FYB9Ejp">Recognizing Unintentional Touch on Interactive Tabletop</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9C5640DE5DE653AEF879F1315DC51BC3</idno>
					<idno type="DOI">10.1145/3381011</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T07:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_qPgn388">CCS Concepts:</term>
					<term xml:id="_7SA5HfU">Human-centered computing Ubiquitous and mobile computing; Empirical studies in HCI Unintentional input</term>
					<term xml:id="_yRNw38T">Interactive tabletop</term>
					<term xml:id="_cMZj37w">Behavior pattern</term>
					<term xml:id="_RNkz9Xt">Gaze and head</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_3rat9rv"><p xml:id="_m5Wh8Gf"><s xml:id="_ScUtF2v">A multi-touch interactive tabletop is designed to embody the benefits of a digital computer within the familiar surface of a physical tabletop.</s><s xml:id="_cbvjxVr">However, the nature of current multi-touch tabletops to detect and react to all forms of touch, including unintentional touches, impedes users from acting naturally on them.</s><s xml:id="_H4ZVfHM">In our research, we leverage gaze direction, head orientation and screen contact data to identify and filter out unintentional touches, so that users can take full advantage of the physical properties of an interactive tabletop, e.g., resting hands or leaning on the tabletop during the interaction.</s></p><p xml:id="_YWCdsGM"><s xml:id="_TBmrnjP">To achieve this, we first conducted a user study to identify behavioral pattern differences (gaze, head and touch) between completing usual tasks on digital versus physical tabletops.</s><s xml:id="_gPntcwE">We then compiled our findings into five types of spatiotemporal features, and train a machine learning model to recognize unintentional touches with an F1 score of 91.3%, outperforming the state-of-the-art model by 4.3%.</s><s xml:id="_cpXVQ5Y">Finally we evaluated our algorithm in a real-time filtering system.</s><s xml:id="_ct2R7dK">A user study shows that our algorithm is stable and the improved tabletop effectively screens out unintentional touches, and provide more relaxing and natural user experience.</s><s xml:id="_Sv9F2j4">By linking their gaze and head behavior to their touch behavior, our work sheds light on the possibility of future tabletop technology to improve the understanding of users' input intention.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_EMctPP7">INTRODUCTION</head><p xml:id="_T9wwmGu"><s xml:id="_eYAvqCp">Nowadays, interactive tabletops are widely used in exhibition <ref type="bibr" target="#b48">[49]</ref>, education <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>, military <ref type="bibr" target="#b3">[4]</ref> and emergency control <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26]</ref>, etc.</s><s xml:id="_BTnuDts">Such a tabletop is expected to combine the advantages of a physical table with a digital computer, by allowing users to directly touch the table's surface to issue input commands.</s><s xml:id="_WsbMa9A">A touchable and unmovable tabletop is distinguished from mobile touchable devices by providing benefits such as body support <ref type="bibr" target="#b9">[10]</ref>, large visual information display <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b48">49]</ref> and simultaneous multiperson collaboration <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</s></p><p xml:id="_3aH8Mmy"><s xml:id="_Uy8E2bt">An interactive tabletop allows for tangible interaction, but not all contacts on the tabletop surface are intended to trigger a digital response.</s><s xml:id="_dtGjFD5">For example, when writing or drawing on the surface, a user may support her body, or rest her palms, wrists, and forearms on the surface to reduce fatigue <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_H6CEeum">In such cases, an ideal tabletop system should have the intelligence to filter unintentional touches.</s><s xml:id="_JdpdsFR">We define unintentional touches as those touches that Fig. <ref type="figure">1</ref>.</s><s xml:id="_T8VXV9N">Intentional and unintentional touches on a tabletop.</s><s xml:id="_HQvKGtC">Face point is the intersection of the tabletop surface and the "face ray", which is emitted from the center of the face, in the forward direction.</s><s xml:id="_kKtp3TK">Gaze point is the midpoint of two interactions of the surface and "gaze rays", which are emitted from the center of two pupils respectively, in the gaze direction.</s></p><p xml:id="_Tayz6rk"><s xml:id="_GW33HhA">do not contribute to any interaction goal.</s><s xml:id="_UakFVHV">Previous works also use accidental/unwanted touches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref>.</s><s xml:id="_qSRV9hF">In our paper, we use them interchangeably with the same definition as unintentional touches.</s></p><p xml:id="_rsg9neV"><s xml:id="_98AXPC7">However, currently, the issue of avoiding unintentional touch on tabletops has not been systematically studied in academia; a tabletop system usually recognizes any contact of a human body with the tabletop surface as a touch event.</s><s xml:id="_rKDYF3Y">As a result, users tend to behave carefully and prudently on tabletops to avoid triggering unwanted touch events.</s><s xml:id="_KpBjuxr">For example, Annett et al. <ref type="bibr" target="#b0">[1]</ref> found that users floated their hand over the screen when handling a stylus and drawing on a tablet to avoid accidental palm touches.</s><s xml:id="_FyrHn5a">In other words, it is hard for users to exhibit many of their natural behaviors on interactive tabletops, e.g., resting hands/arms on the surface, stretching arms/elbows to support the body, gesticulating tentatively during contemplation, etc. Users may get even more frustrated when committing unintentional touches disrupts their workflow, despite their effort to avoid them.</s></p><p xml:id="_8tT6w7x"><s xml:id="_wmv9RSj">In literature, the issue of unintentional touches has been extensively studied <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b64">65]</ref>, but mostly on mobile devices such as phones and tablets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36]</ref>.</s><s xml:id="_Eb7F97n">For instance, some research has looked into palm rejection on mobile devices during tasks such as note-taking and drawing <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b51">52]</ref>.</s><s xml:id="_BMCHJh5">However, the interaction paradigm on a large-scale tabletop is different from that on a mobile device because of its physical size and lack of mobility.</s><s xml:id="_SW9mHTn">For example, users can hold and move the mobile phone or tablet when needed, but when interacting with a tabletop, users must move themselves.</s><s xml:id="_ua5RBcG">In our study involving two usual tasks on multi-touch tabletops, we found that on average, 17.1%/45.3% of touches are unintentional on an interactive/physical tabletop.</s><s xml:id="_hT5KTG8">The high frequency of unintentional touches on tabletops motivates our research.</s></p><p xml:id="_qvzbtJv"><s xml:id="_cBBAQjw">In this research, we propose a novel approach that leverages the intrinsic relationships between the gaze and face behavior and contact behavior to recognize unintentional touches on interactive tabletops (Figure <ref type="figure">1</ref>).</s><s xml:id="_4UmWNzY">This is inspired by the previous observation that face and gaze direction usually indicates where people's interest and attention lie <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b73">74]</ref>.</s><s xml:id="_gpjAzjk">Our work sheds light on the possibility of future tabletop technology to better understand users and their input intention by incorporating the information conveyed by their gaze and head behavior.</s></p><p xml:id="_UJVMBmW"><s xml:id="_9jmN4pv">This research has three phases, each contributing to answer one of the three research questions.</s></p><p xml:id="_Et8N2V4"><s xml:id="_KvVcSJB">(1) RQ1) What are the differences between users' gaze, head, and touch behavior patterns on a digital interactive tabletop and those on a physical tabletop?</s><s xml:id="_abzmXjx">We conduct a user study to empirically investigate and compare users' gaze, head, and touch behavior on a digital interactive tabletop versus on a physical tabletop, for completing two usual tasks (map navigation and photo categorization).</s><s xml:id="_7WDPFZn">We identify significant differences in terms of the number of unintentional touches, as well as coordination patterns of gaze and head between the two modalities.</s><s xml:id="_c6NwGcZ">(2) RQ2) How to filter out unintentional touches on a digital tabletop to allow relaxing postures on the surface?</s></p><p xml:id="_f68utVb"><s xml:id="_7aQhpMm">We propose a set of features according to the spatiotemporal characteristics of gaze, head and touch data.</s></p><p xml:id="_R3bHj6P"><s xml:id="_GwmsWg7">Based on these features, we train and compare several machine learning models to recognize unintentional touches.</s><s xml:id="_ErQVPhr">The performance of the best Gradient Boosting model (0.914, 0.913 and 0.913 on average precision, recall, and F1 score, respectively) significantly outperforms baseline models.</s></p><p xml:id="_FvHKBDT"><s xml:id="_9HqGV2T">(3) RQ3) What is the stability of the algorithm and the usability of the new interactive tabletop?</s><s xml:id="_ydhQYD8">We implement a system that can recognize and filter unintentional touches on an interactive tabletop in real-time.</s><s xml:id="_VSuZqap">A user study validates the stability of our algorithm (F1 score equals 90.6%), and shows that our new system can effectively screen out unintentional touches, and provide more natural and relaxing user experience.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_4ATnBpe">BACKGROUND</head><p xml:id="_R7fkuuq"><s xml:id="_KDrf7c5">We review related work on addressing unintentional touches on mobile devices, how the face/gaze direction indicates a human's intention, and how to use gaze to facilitate touch during interactions.</s><s xml:id="_nhZXPub">In addition, we also review another body of related work that leverages unintentional touches as an alternative type of input method.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_Fy9UtDV">Recognizing Unintentional Touch on Interactive Devices</head><p xml:id="_7Wbm4g3"><s xml:id="_87aUEyK">Several works have been conducted to recognize unintentional input on small-scale touchscreen devices.</s><s xml:id="_PE6SBZe">Matero and Colley <ref type="bibr" target="#b35">[36]</ref> tried to classify accidental touch events on mobile phones in daily usage.</s><s xml:id="_9tpCj5u">They investigated numerous typical operations on mobile devices (e.g., sweeping on the screen, device handling and phone calls) and proposed a rule-based classification algorithm according to the contact area and touch duration.</s><s xml:id="_28cUCyq">Their best algorithm can eliminate 79.6% of unintentional touches with a 0.8% false-positive rate.</s><s xml:id="_CU6um6p">Lu and Li <ref type="bibr" target="#b31">[32]</ref> analyzed accidental touches during the standby mode of mobile phones.</s><s xml:id="_GMgew7m">They compared intentional touch gestures and unintentional touches on a turned-off screen, e.g., putting the device in a pocket.</s><s xml:id="_eHrtVxm">They used a number of touch spatiotemporal features such as duration, touch area trajectory, pressure level on the screen, and acceleration deviation from the motion sensor to classify whether a touch on a turned-off screen was accidental.</s><s xml:id="_wvRzuQE">Their final decision tree model achieved 98.2% on precision and 97.6% on recall.</s></p><p xml:id="_ThGGfYa"><s xml:id="_nkKMGfv">Another big family of touch-based devices are tablets.</s><s xml:id="_FvxRZha">Annett et al. <ref type="bibr" target="#b0">[1]</ref> compared the unintended touches that occurred when using a stylus on digital versus media.</s><s xml:id="_aGYZfgP">They found that users maintained some unnatural and tiring postures on the digital tablets to avoid unintentional touches caused by their palms, e.g., floating their hands over the screen.</s><s xml:id="_Cchy3pd">They further investigated different algorithms to remove accidental touches on stylusbased tablets <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_7G2peZy">Their best algorithm used the distance between the screen and the stylus as the threshold and achieved approximately 86% accuracy for the classification.</s><s xml:id="_vaqRQK4">Julia Schwarz et al. <ref type="bibr" target="#b51">[52]</ref> focused on unwanted interaction triggered by palms on tablets.</s><s xml:id="_86hZCy7">They used similar spatiotemporal features in <ref type="bibr" target="#b31">[32]</ref> to distinguish palm touches from stylus input.</s><s xml:id="_CmjzVaM">Their decision forest model achieved a precision of 97.9% and 0.016 errors/stroke.</s></p><p xml:id="_T5cT5St"><s xml:id="_YZC6kfa">To our knowledge, previous works on recognizing unintentional touch were performed on mobile phones and tablets.</s><s xml:id="_H8t5Aum">However, tabletops can accommodate distinct interaction activities.</s><s xml:id="_Phk5NXS">Beyond that, they are more prone to unintentional touches due to their increased surface area and a wider range of possible inputs.</s><s xml:id="_wmCgEMv">For instance, users can easily lean on the tabletop or put an arm on the surface to support themselves.</s><s xml:id="_grvSVXq">These actions would be far less common when interacting with smartphones or tablets.</s><s xml:id="_KHZ9k8u">Although the interactions on tablets have some overlap with those of tabletops, previous works focus on the distinction between the stylus and accidental finger/palm touches.</s><s xml:id="_FmrJ5Gh">This relieves the technical difficulty, because the stylus has unique properties such as small static touch area, as pointed out in <ref type="bibr" target="#b51">[52]</ref>.</s><s xml:id="_PvaXpgf">The difference on the interface will also lead to the difference in features such as touch trajectories <ref type="bibr" target="#b39">[40]</ref>.</s><s xml:id="_qQnwFNS">In this research, we address the gap by investigating unintentional touches on digital tabletops.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_u7d7HqH">Gaze and Face for Interaction and Intention</head><p xml:id="_ggA2XqP"><s xml:id="_Hy5dGuJ">People primarily direct their gaze towards regions of interest <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b73">74]</ref>.</s><s xml:id="_drjmNk3">Several works evaluated the coordination patterns of mouse cursor and gaze during daily interaction on PCs.</s><s xml:id="_76pjqN5">Huang et al. <ref type="bibr" target="#b18">[19]</ref> investigated the behavioral patterns of gaze and mouse cursor during web searches.</s><s xml:id="_sNuKMjM">They found that while users' gaze-cursor alignment varied a lot, users lagged their cursor behind their gaze by at least 250 ms and on average 700 ms.</s><s xml:id="_tJ2gP8K">Liebling and Dumais <ref type="bibr" target="#b29">[30]</ref> looked into users' daily work on PC in the wild.</s><s xml:id="_yAvQqfd">In contrast to the previous findings, they suggested that the gaze leads mouse only in two-thirds of the time.</s><s xml:id="_cQt7FHk">However, the results in <ref type="bibr" target="#b29">[30]</ref> still supported the consistency between the cursor and gaze during intentional interactions.</s></p><p xml:id="_4MkCEA9"><s xml:id="_Q2wtCsz">The consistency between gaze and intention inspired a number of new interaction techniques.</s><s xml:id="_NkAVYpg">Jacob <ref type="bibr" target="#b19">[20]</ref> proposed several novel interaction methods using eye gaze as the independent channel, e.g., object selection on a PC screen.</s><s xml:id="_tmq8seP">Since then, many gaze-enhanced input approaches have been proposed.</s><s xml:id="_BEyvgEG">Researchers have tried to facilitate pointing/touch interaction with the gaze.</s><s xml:id="_p7YUSvW">For example, Zhai et al. <ref type="bibr" target="#b75">[76]</ref> proposed MAGIC pointing, which used gaze for object suggestions on screens and hand-control for confirmation.</s><s xml:id="_aSB7XDq">Stellmach et al. <ref type="bibr" target="#b57">[58]</ref> designed a gaze-supported selection method for distant display.</s><s xml:id="_XmKKvUU">They used gaze for selection and hand gestures on a handheld device for manipulation.</s><s xml:id="_qgzwhve">Sidenmark et al. <ref type="bibr" target="#b53">[54]</ref> leveraged the coordination of gaze and head to acquire gaze targets in virtual reality.</s><s xml:id="_ku9mF5S">Turner et al. <ref type="bibr" target="#b61">[62]</ref> extended the idea with more gaze-touch combination patterns, such as performing RST (rotate, scale, translate) by touch on the trajectory suggested by gaze.</s><s xml:id="_nUaJGPf">Voelker et al. <ref type="bibr" target="#b63">[64]</ref> developed an indirect input system, where a user touched on a horizontal surface and looked at a vertical screen.</s><s xml:id="_TZ5tAZ9">Both Turner et al. <ref type="bibr" target="#b62">[63]</ref> and Mauderer et al. <ref type="bibr" target="#b38">[39]</ref> suggested selecting out-of-reach objects through combination of far gaze and close touch on a tabletop.</s><s xml:id="_DBYsSxt">Pfeuffer et al. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b44">45]</ref> proposed several scenarios on a tablet screen, using gaze for object selection or button selection and using touch for RST.</s><s xml:id="_Zy7fd3Y">Sidenmark et al. <ref type="bibr" target="#b54">[55]</ref> utilized eye-hand coordination patterns on interacted objects for eye-tracking calibration in the virtual reality setting.</s><s xml:id="_MP34qs6">All these works employed gaze as a viable input channel, showing that gaze could play a significant role in enhancing interaction across mobile devices such as tablets and phones or non-mobile devices such as tabletops.</s></p><p xml:id="_Wk97xEC"><s xml:id="_FT4PpKc">However, these works mainly employed gaze as an active channel during interactions.</s><s xml:id="_Xy2jcYv">Fewer works discussed using gaze as a cue for human intention.</s><s xml:id="_p94fCng">Schwarz and her colleagues <ref type="bibr" target="#b50">[51]</ref> used the gaze and body direction to determine whether the user was engaged in a Kinect game.</s><s xml:id="_uR42v8q">Mariakakis et al. <ref type="bibr" target="#b33">[34]</ref> detected users' gaze to determine whether they were focusing on their phones.</s><s xml:id="_zmdQ4yE">Pfeuffer et al. <ref type="bibr" target="#b43">[44]</ref> proposed a mechanism to switch between direct and indirect input mode based on the alignment of the input area and the user's visual attention.</s><s xml:id="_a9zcc94">Compared to gaze direction, a relatively lower-cost alternative is to use facial orientation, which also proves to be a powerful proxy for attention.</s><s xml:id="_jXub8Yj">Hollands et al. <ref type="bibr" target="#b17">[18]</ref> suggested that in approximately 70 percent of scenarios, the gaze direction and face direction are the same.</s><s xml:id="_x32NW98">Liao <ref type="bibr" target="#b28">[29]</ref> used facial direction as one of the clues to determine a user's most interested picture in a photo collection.</s><s xml:id="_T6Bhua2">Maglio et al. <ref type="bibr" target="#b32">[33]</ref> built a system that used face direction to determine where the user intends to interact with the environment.</s><s xml:id="_jSf6GnA">All these works indicate that eye gaze and head pose can be used to predict regions of interest.</s><s xml:id="_x33A2b6">To the best of our knowledge, this is the first work evaluating the performance using gaze and face features for recognizing the unintentional inputs on touchscreens.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_9DEmqEc">Leveraging Unintentional Touches as Intentional Input Methods</head><p xml:id="_6cqSSmP"><s xml:id="_p65WAYu">Our goal is to remove unwanted effects of unintentional touches on tabletops.</s><s xml:id="_FZzAfGR">However, some literature interprets the nature of these touches differently: instead of "ignoring" unintentional interactions on tabletops, they tried to "use" those interactions.</s><s xml:id="_aPUdAE7">For example, Koura et al. <ref type="bibr" target="#b23">[24]</ref> proposed to use the forearm, which was usually considered problematic (e.g., incorrect recognition and occlusions), as a new interaction technique for menu manipulation and data storage.</s><s xml:id="_dHxdAr5">Le et al. <ref type="bibr" target="#b27">[28]</ref> proposed to interpret leaning into a new class of gestures to enhance interaction.</s><s xml:id="_U4vNfvY">Matulic et al. <ref type="bibr" target="#b37">[38]</ref> extended hand interactions from fingertips to the whole hand in hand-shape based interaction.</s><s xml:id="_vWEpJJq">Zhang et al. <ref type="bibr" target="#b76">[77]</ref> proposed to leverage various hand postures such as using the palm to augment pen and touch interactions.</s><s xml:id="_hYwMsMG">These works provide another perspective to deal with unintentional touches.</s><s xml:id="_XTXaJVb">However, any interaction technique requires additional attention from users.</s><s xml:id="_mE4tgRG">These works do not solve users' concerns about accidentally creating unwanted interaction.</s><s xml:id="_Y24AYgG">Recently, Serim and Jacucci had an interesting discussion on the distinct definition of explicit vs. implicit interaction, and identified new considerations for design and evaluation of implicit interaction <ref type="bibr" target="#b52">[53]</ref>.</s><s xml:id="_zB2FfTF">Similarly, our work tries to obviate the concerns about accidental actions and enable users to operate on tabletops freely.</s></p><p xml:id="_qEaKGQ3"><s xml:id="_5gqsQcJ">In the rest of the paper, we try to answer the three research questions one by one.</s><s xml:id="_gxUpSYS">We begin by answering RQ1 via Study 1.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_KHnQWRW">STUDY 1: INTERACTION BEHAVIOR ON TABLETOPS</head><p xml:id="_EGk3Zh2"><s xml:id="_vADb9SF">We conducted Study 1 to obtain empirical knowledge on how frequently unintentional touches occurred while completing common daily tasks on tabletops.</s><s xml:id="_ty9nQa8">We also wanted to investigate the differences among touch behaviors on the two types of tabletops: digital tabletops equipped with multi-touch interactive touch screens versus physical tabletops with unresponsive surfaces.</s><s xml:id="_FrkGarz">Measuring the difference helped us understand how users changed their interaction behaviors when using touch-sensitive technology.</s><s xml:id="_PEzxhBM">In addition, the data collected from in this study was leveraged to train classification models to recognize and filter unintentional touches.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_msTPzna">Participants</head><p xml:id="_zyJdnPw"><s xml:id="_SgrGg3P">We recruited 12 participants (8 males, 4 females, Age = 23.2 ± 1.47) from a local university through email.</s><s xml:id="_Tw7xawn">All participants had at least four years of experience with touchscreen devices such as smartphones and tablets, and used mobile phones on a daily basis.</s><s xml:id="_snU6qYP">None of them had used an interactive large-scale tabletop before.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_DUzdynZ">Apparatus</head><p xml:id="_CNbNa3B"><s xml:id="_NZh3EGc">Figure <ref type="figure">2</ref> illustrates the apparatus used in this study.</s><s xml:id="_UQYHkfz">We used a customized interactive tabletop as the experimental platform.</s><s xml:id="_ZSpc2TG">The size of the system was 250 cm × 160 cm × 80 cm, with a 220cm × 135cm screen at the center of the table surface, powered by a built-in computer with Windows 10 OS.</s><s xml:id="_aR6jATh">Its capacitive screen was realized via an iPCT transparent touch foil <ref type="bibr" target="#b74">[75]</ref> that could sense up to 20 touch events simultaneously at 100Hz, recording the timestamp, location and state (touch down/up/move) of each touch point.</s><s xml:id="_6PtneT5">Here we defined a touch event as a detected touch point from being put down to lifted up.</s><s xml:id="_fUqZsfu">Note that the system did not provide contact area information.</s><s xml:id="_7JSuXQB">A wide touch area registered as multiple simultaneous touch events.</s><s xml:id="_3grGDMf">Therefore, if a palm was put on the surface unintentionally, there would be a group of unintentional touch events.</s><s xml:id="_m7d9YRW">Moreover, it did not have any pre-filtering algorithm and registered all touch events, which served as a good platform for answering our research questions.</s></p><p xml:id="_ZtnxpKR"><s xml:id="_wTNsWMC">Additionally, we used a head-mounted gaze tracker, Binocular 120Hz Pupillab Eye-tracker <ref type="bibr" target="#b20">[21]</ref> to track users' gaze direction relative to the gaze tracker.</s><s xml:id="_NV5NZMp">The Pupillab was calibrated with its marker system for each participant.</s></p><p xml:id="_cywbcKF"><s xml:id="_epWmmz8">We employed an Optitrack system with 12 cameras <ref type="bibr" target="#b45">[46]</ref> to capture the 3D location and angle of the gaze tracker at 120 Hz.</s><s xml:id="_XXggqnZ">The direction of the gaze trackers indicated the head pose and face direction.</s><s xml:id="_q4VJA2R">It could then be used to calculate the face/gaze fixation points.</s><s xml:id="_nw5uz7X">The fixation position on the tabletop where a user was facing/looking at was computed in real-time in Unity 5.5.4. Figure <ref type="figure">1</ref> shows the definition and the calculation of the face point and gaze point.</s><s xml:id="_dkqZTTS">We tested the eye-tracking on two authors with 9 markers (3 × 3, aspect ratio, for all regions of the screen) on the table, with an average gaze angle of 25 °and an average vision distance of 50 cm.</s><s xml:id="_8H3b2gc">The (a) Digital, Stand, Map Task (b) Physical, Sit, Photo Task Fig. 3. Experiment Setting Examples tracking accuracy is 2.39 ± 1.48 °and the precision is 1.97 ± 1.05 °.</s><s xml:id="_7xrwT6V">The tabletop, Pupillab, and Optitrack were three separate systems.</s><s xml:id="_UNstdru">All data collected by different devices were synchronized using timestamps in Unity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_tbjgjQf">Design</head><p xml:id="_DVah58t"><s xml:id="_pvMDA7z">We used a two-factor within-subject design.</s><s xml:id="_y5pkjKh">The two independent variables are the type of the tabletop (digital vs. physical) and the user posture (sitting vs. standing, initially in the middle of the bottom edge of the tabletop).</s></p><p xml:id="_T4jeYgP"><s xml:id="_2USJsaC">The presentation order of the conditions is counterbalanced using a Latin Square design.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1" xml:id="_USU5xMU">Tasks.</head><p xml:id="_USwEPKH"><s xml:id="_G746MWh">We experimented with two typical tasks on tabletops: map navigation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> and photo categorization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b59">60]</ref>.</s><s xml:id="_2pQvtTq">These two tasks were commonly used to investigate interactions in previous research.</s><s xml:id="_WnP7Ba6">They involve the most common interactions on the surface, such as tapping, dragging, zooming, rotating, etc.</s></p><p xml:id="_UhUmpAY"><s xml:id="_Ff95hVq">In the map navigation task, a mid-sized city map is shown on the tabletop, initially at the same size as the screen and centered.</s><s xml:id="_DXXPJhf">None of the participants were familiar with the city.</s><s xml:id="_WEGSB9Q">Two subtasks needed to be finished under each condition.</s><s xml:id="_Z2nfAHk">In the first subtask, participants were asked to find three landmarks on the map, after which they were instructed to draw a route connecting them.</s><s xml:id="_vS2KRpA">In the second subtask, participants needed to find two landmarks and design a subway route from one to the other.</s><s xml:id="_VacuAnS">There were several candidate routes, and the subtask could be completely by sketching out any one of them.</s><s xml:id="_AhwFqUY">To avoid memory effect, the details of the subtasks (e.g., the landmarks) between the digital and physical tabletops were different.</s><s xml:id="_W7XeHyJ">But the tasks are kept the same among all participants to maintain the difficulty level.</s></p><p xml:id="_sKFwB46"><s xml:id="_JUyXj3x">In the photo categorization task, 40 fixed-size photos (standard A5 size, 148 x 210 mm) were displayed on the surface.</s><s xml:id="_UxsxkFg">These photos belonged to four different groups such as forest, snowfield, beach, and house.</s><s xml:id="_EYUr79y">10 samples from each category were randomly scattered on the table in a reachable region.</s><s xml:id="_KC9b5T3">Participants were required to sort them into four piles by category.</s><s xml:id="_gzEg4kj">We prepared 16 different categories to avoid the learning effect.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2" xml:id="_fXUfR2B">Setup of The Digital Tabletop.</head><p xml:id="_5C3E9M2"><s xml:id="_p9AacZP">For the interactive tabletop, the operations in the map navigation task included 1) moving and zooming the map in the dragging mode, with at least one touch point for moving and at least two touch points for zooming in/out; 2) drawing lines/markers on the map in the drawing mode, where a marker would appear with a tap and a line would be drawn following a finger's movement; 3) erasing lines in the clearing mode, where any line crossed by a finger trajectory would be removed.</s><s xml:id="_3UndQRg">Participants could press a button to switch between the modes.</s></p><p xml:id="_d3UJEwz"><s xml:id="_ET3pNmh">The operations in the photo categorization task were simple.</s><s xml:id="_XeXgRWB">The photos could be moved by at least one finger dragging and rotated by at least two fingers rotating.</s><s xml:id="_tfZJ4CE">The photos were not zoomable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3" xml:id="_NhCms8d">Setup of The Physical Tabletop.</head><p xml:id="_teyVavr"><s xml:id="_6uNX3Wv">We set up the physical interface on the same interactive tabletop (Figure <ref type="figure">2b</ref>), on which we disabled all touch responses and displayed a pure black wallpaper to serve as a plain physical table.</s><s xml:id="_5HDrhZ3">All electronic elements on the digital tabletop were substituted with paper materials.</s><s xml:id="_hCQZ9uB">We verified the robustness of the capacitive touchscreen of our interactive tabletop to sense touch through papers.</s><s xml:id="_Xj7DPHS">Therefore, all touch behaviors on the physical tabletop could also be recorded.</s></p><p xml:id="_Yf5Ba35"><s xml:id="_zehagAa">For the map navigation task, we placed a paper map with identical size (before zooming) and color on top of the surface.</s><s xml:id="_fNyMbH4">Participants were asked to finish the same two sub-tasks as described.</s><s xml:id="_tSSaUxV">They needed to move fingers along the route for the route drawing task.</s><s xml:id="_ZBa78za">For the photo categorization task, we placed 40 printed photos of the same size as the digital photos on the tabletop.</s><s xml:id="_UH66ghw">To retain consistency with the digital tabletop, participants were free to move the paper map and photos on the desktop but not allowed to pick them up from the surface.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" xml:id="_fwRf9hK">Procedures</head><p xml:id="_k6cRDvy"><s xml:id="_5wwc5zQ">Participants first signed the consent form.</s><s xml:id="_K33vFA8">Before the experiment, we notified the participants to act naturally and not to worry about the task performance.</s><s xml:id="_yTDtPug">The only requirement was to finish each task in 10 minutes.</s><s xml:id="_d9NZsV6">Participants first put on the Pupillab device and went through the calibration procedure.</s><s xml:id="_eRQacQm">They were given 3 minutes to familiarize themselves with the interactions on the tabletop.</s><s xml:id="_JThDMWh">After that, they performed the aforementioned tasks under different conditions.</s><s xml:id="_EXPrHJS">On average a map navigation task took about 5 minutes and a photo categorization task took roughly 3 minutes.</s><s xml:id="_3f59Vsh">The full experiment with four conditions, two digital and two physical, lasted approximately 30 minutes.</s><s xml:id="_UNNert6">After the experiment, we briefly interviewed each participant about their subjective feelings during the tasks, especially on the perceived difference between the digital and the physical tabletops.</s><s xml:id="_DB3FvvA">Finally, participants were thanked and dismissed.</s><s xml:id="_6EtCWcv">Each participant was offered $10 as compensation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5" xml:id="_wGs8zMJ">Data Collection and Annotation</head><p xml:id="_G5JXyUR"><s xml:id="_cX7ARZK">During the experiment, participants' direction of face and gaze (as described in Section 3.2), face/gaze points (Figure <ref type="figure">1</ref>) and touch points were recorded for each frame.</s><s xml:id="_dFchYqS">On average, about 100,000 data points were collected per participant.</s><s xml:id="_rMfAVvm">Furthermore, we recorded the table screen and videotaped their behavior throughout the experiment.</s><s xml:id="_ZEJM7F7">After data collection, we smoothed and resampled the gaze trajectory using Stampe's two-stage filter <ref type="bibr" target="#b56">[57]</ref> and two sample weighted average <ref type="bibr" target="#b29">[30]</ref>.</s></p><p xml:id="_rMSXk3x"><s xml:id="_Dq4Asjn">We collected a total of 1,228,250 frames and 27,384 touch events from twelve participants.</s><s xml:id="_URZyBmw">Each touch event had 97.1 frames of data (SD = 211.9)</s><s xml:id="_qXRqDpY">on average.</s><s xml:id="_a8KpKWU">Two authors independently annotated every touch event (intentional or not) throughout the data using a simple self-developed tool, which visualized and replayed the participants' behavior (see Figure <ref type="figure" target="#fig_1">4</ref>).</s><s xml:id="_A2YyEz5">Cohen's Kappa inter-rater reliability equaled 0.73.</s><s xml:id="_xvsw3eA">Conflicts were solved by a collective review of the two authors.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6" xml:id="_fmvpXBd">Results</head><p xml:id="_xp6maue"><s xml:id="_AuNJdBY">Participants spent similar amounts of time on digital and physical tables to finish the tasks (t 11 = -0.19,</s><s xml:id="_GKd2ZvE">p = 0.85).</s><s xml:id="_Y8jZZZc">Overall, we discovered a large proportion of unintentional touches: 17.1 (SD = 13.0)</s><s xml:id="_rS9aPNY">/ 45.3 (SD = 10.5)</s><s xml:id="_SCTQRcF">percent of touches were unintentional on an interactive/physical tabletop.</s><s xml:id="_e4WP5Hd">We observed interesting differences between the two types of tabletops, in terms of behavior patterns (Section 3.6.1),</s><s xml:id="_FSZtd4a">number of unintentional touches (Section 3.6.2),</s><s xml:id="_REYq6XM">and mental models (Section 3.6.3).</s><s xml:id="_7mvymwa">We summarize these differences one by one.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1" xml:id="_vmxzN4U">Different Behavior Patterns on Physical and Digital</head><p xml:id="_EMHFg5P"><s xml:id="_fKh85gn">Tabletops.</s><s xml:id="_CCnUUhv">We found remarkable differences in touch behavior between the digital and the physical tabletops.</s><s xml:id="_EJnqBrU">Figure <ref type="figure">5</ref> depicts the typical behavior patterns on these two forms of tabletop.</s><s xml:id="_pMUzACk">The specific difference is summarized in Table <ref type="table">1</ref>.</s><s xml:id="_6uQedkn">Although participants were instructed to behave as natural as possible, they tended to be quite cautious on the digital tabletop, which is similar to the findings on tablets <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_yAxukBB">8 out of 12 participants mentioned "careful" or "safe" during the interview.</s><s xml:id="_Y5FQm7f">"It always came to my mind that I need to be careful with the screen, otherwise I will trigger unwanted touch events" (P2).</s><s xml:id="_CG7M74A">"I soon got to float my hands above the surface.</s><s xml:id="_DXByqdU">That's safe" (P10).</s><s xml:id="_SmXDnCA">Therefore, only a small number of unintentional (a) Patterns on Digital Table (b) Patterns on Physical Table Fig. 5. Typical Behavioral Patterns on the Two Tabletops</s></p><p xml:id="_Yf8X8eA"><s xml:id="_AKGVV8C">Table <ref type="table">1</ref>.</s><s xml:id="_mckMAEk">Summary of the behavior difference between the digital and physical tabletop.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Qa76WcN">Digital Tabletop Physical Tabletop</head><p xml:id="_f3dt7jT"><s xml:id="_EWNfSB8">Intentional Touch</s></p><p xml:id="_MfFEmRU"><s xml:id="_nYVMNtW">• Users are cautious, with arms floating over the surface.</s></p><p xml:id="_uZM7YNE"><s xml:id="_gGAvSxk">• Mostly one or two fingers are used for interaction.</s></p><p xml:id="_6ysWVSn"><s xml:id="_EhX8BQx">• Other parts of hand/arm (except the finger involved in interactions) stay far away from the screen.</s><s xml:id="_WjV94uh">• The unused hand rest on the edge of the tabletop.</s></p><p xml:id="_dYHQDSy"><s xml:id="_WmRQsG9">• Usually gaze leads or stays in line with the touch.</s></p><p xml:id="_dBGGEU6"><s xml:id="_48yvjX6">• Users are casual, with arms often resting on the surface.</s></p><p xml:id="_xFxpgJd"><s xml:id="_mpjS5au">• Mostly two or three fingers are used for interaction.</s></p><p xml:id="_asr5waS"><s xml:id="_tePNEvj">• Other parts of hand/arm (except the finger involved in interactions) lay on the tabletop surface.</s><s xml:id="_AqdNTTQ">• The unused hand rest on the surface casually.</s></p><p xml:id="_gqeH5HV"><s xml:id="_5wQ8bWS">• Gaze mostly stays in line with touch.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AZxRQkp">Unintentional Touch</head><p xml:id="_KvjszdK"><s xml:id="_b7kKRkh">• Touches are less frequent than those on the physical tabletop due to the carefulness.</s><s xml:id="_v2BC52x">• Edge accidental touches are inevitable and usually triggered by the palm/elbow resting on the edge.</s><s xml:id="_hbQcVGg">• Usually touches are relatively far from the area of the participants' attention.</s></p><p xml:id="_xFbnzXw"><s xml:id="_4GqgDUc">• Many touches are static and ephemeral.</s></p><p xml:id="_AA6BUjq"><s xml:id="_UqsrZgz">• Gaze does not stay in line with touches.</s></p><p xml:id="_mkT2A5N"><s xml:id="_mmvePC9">• Touches are very common and appear with intentional touches simultaneously.</s><s xml:id="_uKwETnw">• Many touches are triggered by the hand/arm resting on the surface (no matter the hand is being used or not).</s><s xml:id="_CdanZkh">• Usually touches gather together if triggered by the hand that is being used, and are far from the attentive area if they are triggered by the hand that is not being used.</s><s xml:id="_GvGT7aE">• Touches are more dynamic and long-lasting than those on the digital tabletop.</s><s xml:id="_RrfbZ89">• Gaze does not stay in line with touches.</s></p><p xml:id="_xEZbqtW"><s xml:id="_Ef4UV6V">touch events were recorded on the digital surface.</s><s xml:id="_wbPJrdn">In most cases, participants used only one or two fingers to operate, keeping the rest of their hands and arms far from the surface.</s><s xml:id="_tTbCse5">In contrast, on the physical table, users were more relaxed.</s><s xml:id="_4Kn8DEy">They naturally rested their arms on the surface.</s><s xml:id="_T72rhfT">"I did the same as what I will do with my wooden desk" (P2).</s><s xml:id="_RaqC6fV">When users pointed at the target (usually with two or three fingers), other fingers or even the palm were often put on the tabletop casually (see Figure <ref type="figure">3b</ref>).</s><s xml:id="_7tvcKUZ">These touches were unwanted but very common on the physical table.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2" xml:id="_VK2wKM8">More Unintentional Touches When Operating with</head><p xml:id="_St7H7Z7"><s xml:id="_RdqKtWp">The Physical Tabletop.</s><s xml:id="_Sm6Anjr">We first ran two-way ANOVAs (table type × posture) according to the experiment design on the number of intentional touches and unintentional touches separately.</s><s xml:id="_ZPyq3yq">Neither table type nor posture had a significant effect on the number of intentional touches (F t abl e (1, 11) = 0.826, p = 0.38, F postur e (1, 11) = 0.13, p = 0.73).</s><s xml:id="_tSyVguu">The effect of the two main factors' interaction was also not significant (F t abl e×postur e (1, 11) = 2.67, p = 0.13).</s><s xml:id="_NPbJeFw">As for the number of unintentional touches, the results indicated significance on both main factors and their interaction (F t abl e (1, 11) = 45.16,</s><s xml:id="_9j54vmf">p &lt; 0.01, F postur e (1, 11) = 14.13, p &lt; 0.01, F t abl e×postur e (1, 11) = 29.54,</s><s xml:id="_uUvC8vn">p &lt; 0.01).</s><s xml:id="_qfJcyVR">More unintentional touches were observed on the physical tabletop and in the sitting posture.</s></p><p xml:id="_GC7dhmF"><s xml:id="_yK9zJ9J">The posture factor was designed to incorporate a variety of different behavior patterns on the tabletop.</s><s xml:id="_Bp6v7T7">We hence combine the data of the standing and sitting postures.</s><s xml:id="_N7cSwFM">In Figure <ref type="figure" target="#fig_2">6</ref>, the boxplot shows the 12 participants' average number of the intentional/unintentional touches on the digital/physical table respectively.</s><s xml:id="_39RHHfR">We ran two paired t-tests to compare the number of intentional and unintentional touches between two tabletops.</s><s xml:id="_z4jpj9t">The results do not show significance of tabletop types on the number of intentional touches (t 11 = -0.73,</s><s xml:id="_4DaDARX">p = 0.48), but revealed significance on unintentional touches (t 11 = -5.86,</s><s xml:id="_e39kMAD">p &lt; 0.01).</s><s xml:id="_fvWTGdH">Similar numbers of intentional touches were triggered on two tabletops, but more unintentional touch events appeared on the physical tabletop than the digital tabletop.</s><s xml:id="_jdDx6rs">In addition, we found an interesting case where two participants exhibited noteceably higher numbers of unintentional touches on the digital tabletop.</s><s xml:id="_EV5DQj9">From the video, the two participants made attempts to "imitate" operations on the physical table by placing their palm on the surface, according to their understanding of "being natural".</s><s xml:id="_56yY7g6">This also reflects the difference in behavior patterns between the digital and physical settings.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3" xml:id="_fVwd45Z">Different Mental Models Towards Two Types of Interface.</head><p xml:id="_5cuKCUE"><s xml:id="_R9R6VYT">The interview results are also interesting: although participants behaved quite differently on two surfaces, they did not mind the differences: it was natural to be careful on the digital screen and casual on the physical surface.</s><s xml:id="_eEHazwY">Almost all users (11 out of 12) expressed that they would feel uncomfortable if they were asked to interact with the touch screen in the same way as the plain table.</s><s xml:id="_bW9gNjV">"I never touch screens…in the way on ordinary tables, otherwise, it will annoy me with a lot of unwanted triggers!"</s><s xml:id="_nqg8ZKZ">(P1).</s><s xml:id="_q2zvNx6">"It does sometimes make me tired, but compared to unwanted triggers, I'd rather be careful" (P11).</s><s xml:id="_chJCutn">One of the two outliers said "Although I tried to operate naturally on the digital screen, I cannot resist the temptation to raise my hands.</s><s xml:id="_GxpbNwN">It is an awkward experience…" (P7).</s><s xml:id="_Ferwqj5">We speculate this phenomenon is caused by different mental models towards digital and physical tables.</s><s xml:id="_c4dj6sR">Users are accustomed to use digital screens more cautiously.</s><s xml:id="_XeUJUUj">In order to avoid triggering unwanted touch events, they are willing to pay more attention to their actions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_ExtmkK4">TOUCH INTENTION IDENTIFICATION</head><p xml:id="_YFxpgyK"><s xml:id="_UWFN5uW">Our goal was to build an intelligent tabletop that allows users to operate on an interactive tabletop in a similar manner as on a physical table.</s><s xml:id="_63WaQ2f">To answer RQ2, we extracted features from user behavior data and trained a binary classifier.</s><s xml:id="_6dkQ5QP">We blended the data of two tabletops, only focusing on whether the touches were intentional or not.</s><s xml:id="_4pnrvhh">The analysis of the behavior patterns (Table <ref type="table">1</ref>) provided insights for feature extraction, which shed light on the features that were useful for classification.</s><s xml:id="_rAq6Tsk">Specifically, the difference between intentional and unintentional touches is summarized as follows:</s></p><p xml:id="_TYvrkz5"><s xml:id="_7kpT5EZ">• The majority of the gaze is in line with intentional touches, but not with unintentional ones (Gaze/Face Distance) • A large number of unintentional touches are more static, ephemeral and closer to the edge compared to intentional ones (Side Distance) • Intentional touches usually involve one to three fingers while unintentional ones often appear in groups (Clustering) • Historical behavior affects current touches.</s><s xml:id="_pnYnWjq">E.g., sometimes gaze leads intentional touches shortly, the unintentional touches clustering area often spawn more unintentional touches subsequently (History)</s></p><p xml:id="_waWWXKs"><s xml:id="_9HQyFXJ">We randomly split our dataset into an optimization set (30%) for feature analysis, and a traintest set (70%) for model training and testing.</s><s xml:id="_2FP7zbj">In Section 4.1, we introduce the features extracted from the gaze, face and touch data (as summarized in Table <ref type="table" target="#tab_0">2</ref>).</s><s xml:id="_JCvCAP8">We present an overview of these features through descriptive statistics, using data from the optimization set.</s><s xml:id="_DsEt9Ju">Note that we purposefully only extract features that are compatible with real-time system implementations.</s><s xml:id="_AD4Dpv3">Other features, e.g., the lifetime of a touch event, are not included in our analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_VtFMY6m">Feature Definition</head><p xml:id="_e5Z3zd9"><s xml:id="_7KXN2TP">4.1.1</s><s xml:id="_PHVhv6j">Gaze/Face Distance.</s><s xml:id="_GpywYuM">Within each frame, the distance between each touch point and face/gaze point (defined in Figure <ref type="figure">1</ref>) is named as face/gaze distance.</s><s xml:id="_b2PbbF6">The distribution of distances of four categories (intentional/unintentional × digital/physical) is shown in Figure <ref type="figure">7</ref>.</s><s xml:id="_Zxc5SbH">In comparison with unintentional touches, intentional touches have smaller gaze/face distance in both digital and physical conditions.</s><s xml:id="_59CFpcH">Compared with Figure <ref type="figure">7b</ref>, Figure <ref type="figure">7a</ref> has a more pronounced difference between intentional and unintentional touches.</s><s xml:id="_CbzaW7c">This indicates that the gaze point stays more in line with touches than the face point.</s></p><p xml:id="_3zUKRvw"><s xml:id="_KKspAc8">Figure <ref type="figure">8a</ref> shows a heatmap of touch locations.</s><s xml:id="_Af3YmHz">Intentional touches are distributed in the center of the surface while the majority of unintentional touches are scattered near the bottom edge.</s><s xml:id="_T3pmgzE">Side distance is defined as the perpendicular distance of a touch point to the nearest screen edge.</s><s xml:id="_2xgVDBW">The near-margin property of unintentional touches on both tabletops is salient (Figure <ref type="figure">8b</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2" xml:id="_9jn7FGz">Clustering.</head><p xml:id="_7VYgWHk"><s xml:id="_HgBTAJn">We observed obvious spatial clusters of touch points during the study, especially on the physical table.</s><s xml:id="_XAjD83d">This may be explained by users' casualness: users would usually rest their hand or even their arm on the table, which triggered a wide contacting region.</s><s xml:id="_bFfFhnR">Owing to the hardware properties of the customized tabletop, the system recognized a large contacting region as a group of separate touch points rather than a continuous area.</s><s xml:id="_jT7MRxp">A dynamic distance matrix was created.</s><s xml:id="_uVTsNtQ">Each row and column represents a touch point at the frame.</s><s xml:id="_3aczj5R">Touch (a) Heatmaps of touches (b) Side Distance/cm distance, the mean of the element of the matrix, is the distance between two touch points.</s><s xml:id="_DrBHUuc">Number of adjacent touches of each touch point is defined as the count of other touches whose distance to this touch point is smaller than some particular threshold.</s><s xml:id="_UqjkCHj">Figure <ref type="figure">9</ref> suggests apparent clustering of unintentional touches on both tabletops: most intentional touches have zero or one adjacent touch point, while the majority of unintentional touches have at least three adjacent touch points.</s><s xml:id="_utZeb3q">Several distance thresholds were tested (Figure <ref type="figure">9</ref> only shows two examples).</s></p><p xml:id="_AF47sZZ"><s xml:id="_jbTbVTg">Compared to a 10 cm threshold (see Figure <ref type="figure">9a</ref>) and other values, the threshold of 20 cm appeared to have the strongest splitting power between intentional and unintentional touches (see Figure <ref type="figure">9b</ref>).</s><s xml:id="_WMd4ymK">Hence, the clustered threshold is set to 20 cm, which is approximately the width of a stretched palm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3" xml:id="_w9w7PSZ">History.</head><p xml:id="_uGVmsuG"><s xml:id="_NJFvjXF">As found by previous literature, the gaze leads hands in many cases <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30]</ref>.</s><s xml:id="_cEhHuAD">This indicates that the position of current intentional touches may be close to previous gaze points.</s><s xml:id="_nKGy99F">Moreover, we observe the phenomenon of clustering not only on the spatial dimension but also on the temporal dimension: if a location is clustered with a group of touches, a new touch will be more likely to appear within or near the clustering area shortly afterward, especially when users are unaware of the touches.</s><s xml:id="_wFmEnF7">Hence a historical time window may carry useful information.</s><s xml:id="_G7Y8TW3">700 ms is selected as the width of the window according to <ref type="bibr" target="#b18">[19]</ref>, counted backward from the down time of a touch event.</s><s xml:id="_fBmKV8Z">Historical gaze/face distance represents the distance between the current touch point and previous gaze/face points in the window.</s><s xml:id="_N4jUku4">Historical touch distance and historical number of adjacent touches can be similarly obtained based on the distance between the current touch point and previous ones.</s><s xml:id="_nKPRF2U">Figure <ref type="figure">10</ref> shows the historical gaze/face distance.</s><s xml:id="_nYW7bPk">It is similar to Figure <ref type="figure">7</ref> but with a higher degree of separation.</s><s xml:id="_EGkCkG5">We categorized all features into four different groups based on their characteristics (see Table <ref type="table" target="#tab_0">2</ref>).</s><s xml:id="_u5QsPQE">Some features appear more than once because they belong to different types simultaneously.</s><s xml:id="_eV6RSru">Then we trained machine learning models based on these features.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_pQz454T">Model Comparison and Feature Comparison</head><p xml:id="_5CefZjw"><s xml:id="_q9K8QkS">To obtain the best binary classifier for unintentional touch identification, we compared different machine learning models using our features in Section 4.2.1.</s><s xml:id="_kMgVGQx">After obtaining the best model, we compared it with other baseline models in Section 4.2.2.</s><s xml:id="_Aw5Kuau">In addition, we also conducted a feature ablation study in Section 4.2.3 to investigate the relative importance among different feature types.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1" xml:id="_QQ6ZtPK">Model Selection.</head><p xml:id="_zzx7SsN"><s xml:id="_TDcPZSE">A good classifier is capable of identifying intentional touches and therefore filtering unintentional ones.</s><s xml:id="_Aq8WyK7">Yet more significantly, the effect of each type of feature in the model is also worthy of exploration.</s><s xml:id="_tcujdYQ">This indicates the importance of features during classification, which can provide insights into human behavior patterns.</s></p><p xml:id="_kXBwB58"><s xml:id="_daMWTmA">Six machine learning models were compared on the traintest set, including LR (Logistic Regression), NB (Naïve Bayes), KNN (K-Nearest Neighbor), RF (Random Forest), GB (Gradient Boosting) <ref type="bibr" target="#b10">[11]</ref>, and MLP (Multi-Layer Perceptron).</s><s xml:id="_g6xnbpN">The first three models are typical approaches to classification problems.</s><s xml:id="_GfnxP2p">Both Random Forest and Gradient Boosting employ decision trees as weak classifiers.</s><s xml:id="_vR8Fu2u">They are suitable for feature selection <ref type="bibr" target="#b6">[7]</ref>.</s><s xml:id="_zVyHcby">MLP also proves to be a suitable model for complex problems where the relative importance of features is unknown <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_GRZJDmk">Every person's behavioral pattern has consistency, thus merging all users' data for training will lead to information leaks, which makes the classification naive and impractical.</s><s xml:id="_djm6REx">Therefore, we trained models using the leave-one-user-out method, where the models were trained on the data of 11 participants and tested on the remaining one.</s><s xml:id="_xC4NAT2">This can better measure the model's generalizability.</s><s xml:id="_sy8hc7D">The tuning of the hyperparameters of each model was conducted within the leave-one-user-out set using an inner five-fold cross-validation loop.</s><s xml:id="_CZkWrsm">After the best parameters were chosen, the ignored user's data was used for testing.</s><s xml:id="_yXkFvdb">Final metrics (precision, recall and F1 score) were the average of 12 models (same as the number of participants).</s><s xml:id="_5fathaz">Table <ref type="table" target="#tab_1">3</ref> summarizes the testing result of all models.</s><s xml:id="_RdSTZhH">We chose the GB model for our later analysis since this model significantly outperformed than other models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2" xml:id="_5FdtrUj">Baseline Model Comparison.</head><p xml:id="_G3mrRC6"><s xml:id="_gu3zYkC">We further compared our best model with two baseline models:</s></p><p xml:id="_YZ3MuaN"><s xml:id="_H4UJaj3">(1) A naive threshold-based model using gaze distance.</s><s xml:id="_GbEgC4F">The threshold was determined from the optimization set as 23.9cm which maximizes the split of the two types of touches.</s><s xml:id="_X7b49mx">(2) A modified version of the state-of-the-art decision-tree model from <ref type="bibr" target="#b51">[52]</ref>.</s><s xml:id="_VTvVvaW">Although this model is intended for distinguishing palm touches from stylus input, many features are transferable to our system.</s><s xml:id="_9tBJ8CB">We implemented the instant version for a fair comparison.</s><s xml:id="_xmWuYcG">The features include touch distance, touch points travelling speed and acceleration.</s><s xml:id="_GFNS2cP">As our tabletop does not provide touch area information, we omitted this feature type.</s><s xml:id="_aW8UGtQ">Table 4. Results of Baseline Comparison.</s><s xml:id="_hw6WXBq">A paired t-test on the cross-validation F1 scores between the full GB and the Palm rejection model shows significance p &lt; 0.01.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HDWUmvk">Models</head><p xml:id="_mXKbdfd"><s xml:id="_66uYazH">Prec Rec F1 Gaze-threshold-based 0.794 ± 0.026 0.791 ± 0.047 0.793 ± 0.033</s></p><p xml:id="_ejzAf4e"><s xml:id="_etpaqjk">Palm rejection <ref type="bibr" target="#b51">[52]</ref> 0.873 ± 0.057 0.869 ± 0.021 0.870 ± 0.032 Full GB 0.914 ± 0.020 0.913 ± 0.026 0.913 ± 0.014 Table <ref type="table">4</ref> summarizes the results on the test set.</s><s xml:id="_63tEKFS">Our model significantly outperforms baseline 1 by 12.0% and baseline 2 by 4.3% on the average F1 score.</s><s xml:id="_b2gWSus">The big gap between the result of the reimplemented model from <ref type="bibr" target="#b51">[52]</ref> and that of the original paper can be caused by a few reasons.</s><s xml:id="_BD4VVNB">The interaction paradigms on tabletops are different from tablets.</s><s xml:id="_hfYNuJM">The features of a stylus's input in <ref type="bibr" target="#b51">[52]</ref> can be very different from a finger's input.</s><s xml:id="_b9tnqdp"><ref type="bibr" target="#b51">[52]</ref> considers palm as the major source of unintentional touches, while in our case other parts of the hand and the arm were also a source.</s><s xml:id="_tZx92UZ">For example, Figure <ref type="figure">5a</ref> shows that the thumb can often trigger unwanted touches.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3" xml:id="_tUPBHfR">Feature Ablation</head><p xml:id="_cyrYdSw"><s xml:id="_tcKeZWG">Comparison.</s><s xml:id="_CvrSBnw">We enhanced our findings through a feature ablation study to investigate the effect of gaze and face features, i.e., removing certain feature types and observing the performance decrease of the model.</s></p><p xml:id="_XcjPqVb"><s xml:id="_bPR4Md7">The removal of the gaze features, including any features that involved gaze information, both in Gaze and History feature types, led to a significant drop in the model performance.</s><s xml:id="_7ESDaTa">This emphasized the importance of gaze-based features, which is a reflection of previous research concluding that people tend to look at the place where they have interest <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b73">74]</ref>.</s><s xml:id="_4ZQFSJH">Our results further reveal its positive effect in classification and show that Gaze is a strong clue that conveys human intention.</s><s xml:id="_yZXbcFt">These findings are consistent with previous studies on human gaze-attention correlation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b58">59]</ref>.</s><s xml:id="_wRYQcAT">The intuition of "where we look is where we have interests, therefore where we touch" is validated again by our study in the context of tabletop interaction.</s></p><p xml:id="_dUDQGhc"><s xml:id="_dD8cXdY">Surprisingly, compared to Gaze, the removal of Face did not cause much decrease in the performance.</s><s xml:id="_ZCMvx8E">Compared to the model with gaze features ablated, the model with all gaze and face features removed just decreased by 0.009 on the F1 score.</s><s xml:id="_m5CusH8">These results suggest that Face may not be an appropriate alternative for Gaze.</s><s xml:id="_bgAMvpZ">This indicates that there will be a significant drop in classification performance by substituting gaze tracking with a lower-cost head tracking.</s><s xml:id="_3nwK5MY">This may be explained by the lack of head-gaze consistency during the interaction on the large-scale surface.</s></p><p xml:id="_2hRM4ds"><s xml:id="_gzasY65">To further understand the consistency, we investigated the included angle between the face ray and gaze ray during the experiment.</s><s xml:id="_JR4vQtu">Figure <ref type="figure" target="#fig_4">11</ref> shows the heatmaps of the average included angle between the two rays throughout the Study 1.</s><s xml:id="_Zxzk44U">The angle is calculated frame by frame and takes the face point as the position of the  value.</s><s xml:id="_zSdYb95">Two heatmaps of the sitting and standing postures share similar patterns.</s><s xml:id="_YuFy3FU">Users' gaze direction has a greater deviation from their face direction in the main working area.</s><s xml:id="_BVKzEMU">The figure of standing posture has a wider red region than that of sitting because of the larger movement space for the upper limbs during the standing posture.</s><s xml:id="_WF3xf4K">The heatmaps indicate that when people are focusing on tasks within their reaching range, they may have more gaze movement rather than head movement.</s><s xml:id="_dXk9Fm3">The gaze direction greatly deviates from the normal direction of the face -users do not always stare strictly forward, but often move their eyes around, especially when they are operating on the surface close to them.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_HuBCgZu">Misclassification Analysis</head><p xml:id="_5rHuCmd"><s xml:id="_xY7E8qV">We thoroughly investigated the misclassified samples during training and testing.</s><s xml:id="_WNuFGwv">This can equip us with more insights about the similarities between intentional and unintentional touches that fool our algorithm.</s></p><p xml:id="_utNauPF"><s xml:id="_NXfPmJN">Close inspection shows that some unintentional touches have almost the same values on most features as some intentional ones or vice versa.</s><s xml:id="_N5Yq959">Table <ref type="table">6</ref> summarizes the relatively frequent patterns of wrongly classified samples in the GB full model.</s><s xml:id="_s4Rqbjn">These samples indicate the shortage of the model.</s><s xml:id="_8pB7hKD">The reasons may lie in the fact that some features that hide deep beneath the data are not extracted effectively, or even some patterns that can not be captured by the current feature set.</s></p><p xml:id="_6gtgsDv"><s xml:id="_aaUh8Yr">The comparison between the GB model with human annotation can reveal some information that is not observable from the data.</s><s xml:id="_UetaYSd">The videotape allowed the authors to obtain the context information of operations with two additional dimensions, both historical and future, either task-specific or person-specific.</s><s xml:id="_D7wZDry">These two dimensions can further provide insights into the misclassification issue.</s><s xml:id="_af4cVyC">For instance, "The participant just found the start place on the map.</s><s xml:id="_kbH9HVE">So in the next step, he needed to find the destination, so he was searching at that time.</s><s xml:id="_2P8c9b7">" (Author 1, historical and task-specific).</s><s xml:id="_u76qVPS">"Several seconds after this touch, he switched his attention to another place far from the current position.</s><s xml:id="_p32D3bK">Then he got excited since he found the target, which was quite different from his current state.</s><s xml:id="_FpfVvx8">" (Author 2, future and user-specific).</s><s xml:id="_Jx5Af7W">These messages cannot be reflected in the data from the tabletop or eye tracker because both future and task-specific information are missing.</s><s xml:id="_eaFPMFm">This indicates the limitation of just leveraging gaze and touch information.</s><s xml:id="_AJ6HM4G">The task information and emotional state (e.g., getting excited) can be two promising candidate features that worth further exploration to achieve better classification performance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_vf4ury7">STUDY 2: USABILITY STUDY</head><p xml:id="_dRG8U6N"><s xml:id="_T5EYKV4">To answer RQ3 regarding the viability of the algorithm, we implemented a real-time filtering system by integrating the GB model with the tabletop.</s><s xml:id="_QRaF6Qm">This tabletop provided a new experience to users.</s><s xml:id="_VCyrAYD">How well does the algorithm perform in real-time?</s><s xml:id="_BzQQrRK">Will users accept it?</s><s xml:id="_AaengBu">Will they feel the benefit?</s><s xml:id="_etFH2ff">In Study 2 we answer these questions by testing the system's usability and effectiveness.</s><s xml:id="_qSARU3V">(c) Touch Close to The Bottom Edge: Touches near the bottom edge of the tabletop.</s><s xml:id="_BgVh2fT">This may be biased by the large number of unintentional touches that appear near the edge of the tabletop.</s></p><p xml:id="_Pstd6CE"><s xml:id="_jnAcMVd">(d) Two-hand Close Cooperation: Close hands cooperation with many simultaneous touches.</s><s xml:id="_Hp6GdUG">Touches may be misclassified because of small touch distances.</s></p><p xml:id="_E5Wnwga"><s xml:id="_FvbfBeW">(e) Two Touches with Gaze Shifting: Two touch points are far from each other, with gaze switching focus between them.</s><s xml:id="_SUGVSPw">The possible reason of misclassification is large gaze distance when the gaze moves away from one touch point to another.</s></p><p xml:id="_9nhvTWF"><s xml:id="_kPf9fnq">(f) Resting Hand with Gaze Passing by: Fingers resting on the surface with unconsciously close gaze point when the user is searching target or thinking.</s><s xml:id="_PZZWraH">The small gaze distance can lead to misclassification.</s></p><p xml:id="_VmECBCE"><s xml:id="_sqY2KDq">(g) Touch Close to Intentional Area: Touches caused by a part of the palm or redundant fingers of the hand being used, close to the intentional area.</s><s xml:id="_wbzeE7T">The gaze may be close to unintentional touches, which is a common feature for intentional ones.</s></p><p xml:id="_Vy6E3um"><s xml:id="_xM8uYZ7">(h) Hand Flying over the Surface: Unintentionally flying over fingers or transient resting palms on the surface.</s><s xml:id="_B96TvzM">The combination of moderate touch duration and gaze distance may confuse the classifier.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_JYXSstj">System Implementation</head><p xml:id="_HQgMmDF"><s xml:id="_7qjWQpm">We obtained our final GB model by training on the whole dataset collected in Study 1 with the same hyperparameters.</s><s xml:id="_mEcTwCM">When a touch event is registered, it will pass through the classifier to determine whether it is intentional.</s></p><p xml:id="_HEY65f5"><s xml:id="_sabZDu5">If the event is classified as intentional, then it will be processed as a normal operation.</s><s xml:id="_AfYnb4T">Otherwise, it will be removed immediately and won't change anything on the surface.</s><s xml:id="_gxKuyvs">The average computational time of our algorithm is 83.3 ms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_m87jvHb">Participants and Apparatus</head><p xml:id="_fSsP9v2"><s xml:id="_eN5RZ5t">We invited another 12 participants from the local university through email (7 males, 5 females, Age = 22.7 ± 2.1).</s><s xml:id="_ph8p5zw">They were all familiar with mobile phones/tablets.</s><s xml:id="_xmRtjGa">None of them attended Study 1.</s><s xml:id="_fw2vswQ">The equipment remained unchanged.</s><s xml:id="_z8rFG9K">The only difference was the new classifier integrated with the tabletop.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_bB3GNfR">Design</head><p xml:id="_vkHh45n"><s xml:id="_85eA2sh">We used a one-factor within-subject design.</s><s xml:id="_yYW75N3">The independent factor was the classifier being enabled/disabled.</s><s xml:id="_QrqrAEu">The study consisted of two sessions: one session with our classifier (namely with session) and one without the classifier (namely without session).</s><s xml:id="_tUQSFnm">The order of the two sessions was counterbalanced.</s></p><p xml:id="_yVky6jA"><s xml:id="_bHZrCeK">In Study 2, participants were only asked to finish tasks on the digital surface in the sitting posture.</s><s xml:id="_kHXBw2w">As found in Study 1, this posture condition provoked users to trigger more accidental touches.</s><s xml:id="_zQ3fYUT">Similar to Study 1, the tasks consisted of map navigation and photo categorization.</s><s xml:id="_VyVrmAd">In each session of Study 2, participants needed to finish a long map navigation task and a long photo categorization task with a balanced order.</s><s xml:id="_PRzzhhW">The operations in two tasks were the same as Study 1.</s><s xml:id="_kFSPw6d">A 5-minute warm-up stage was scheduled before each session for users to get familiar with the system.</s><s xml:id="_KbAdFbk">Both tasks were designed to take approximately 15 minutes and a 2-minute break was inserted between the two tasks.</s><s xml:id="_Wy8ADZq">Participants needed to perform each task in the sitting posture for a long time, henceforth their arms would easily be affected by fatigue, which could make them more "willing" to rest their arms on the surface.</s><s xml:id="_F3ad9YG">These modifications could provide more "opportunities" for users to evaluate the performance of this new system and have a better sense to compare the systems in the two sessions.</s></p><p xml:id="_8Y8mDhc"><s xml:id="_2GQ2Fsr">We manipulated users' expectations of the tabletop in Study 2. In both sessions, the experiment was introduced to the participants as an inspecting study.</s><s xml:id="_MwjFyvU">Participants were told that the tabletop was expected to be intelligent so that it could recognize and filter any unintentional touches.</s><s xml:id="_vhuPFUt">However, the system was not yet perfect.</s><s xml:id="_PncSxVJ">They were asked to perform either carefully or casually as long as they were comfortable with the system, and report any touches they deemed "wrong" during the experiment, which can reflect users' subjective evaluation on system's performance in practice.</s><s xml:id="_ZWp6XYp">The word "wrong" is used to describe those touches that ideally should have been removed (false positive) or should have appeared (false negative), but are noticed by users.</s><s xml:id="_kHsmxWX">Specifically, in the with session, the "wrong" touches represent those misclassified touches observed by participants (either intentional touches are classified as unintentional ones or vice versa).</s><s xml:id="_zbxBTxh">In the without session, the "wrong" touches mean the accidental touches that trigger responses in the system and are noticed by users.</s><s xml:id="_46DCRm8">Whenever participants found anything wrong, they needed to speak up to the instructor immediately.</s></p><p xml:id="_ZJx47kZ"><s xml:id="_E6pkAsf">At the end of each session, users rated the system with a questionnaire.</s><s xml:id="_mvE88Mx">The questions were all rated on a 7-point Likert Scale (1: Not at all -7: A lot), consisting of the following parts.</s></p><p xml:id="_QZUNSj3"><s xml:id="_Z75hd5t">• One question for the subjective feeling of intuitiveness of the system during the operation • Two questions for the efficiency and learnability of the system from Perceived Usefulness and Ease of Use Questionnaire <ref type="bibr" target="#b7">[8]</ref> • Five questions for task load during the experiment from the NASA Task Load Index (NASA-TLX) <ref type="bibr" target="#b15">[16]</ref> • Two questions for the system's capability of intention recognition and error prevention from Nielsen's Heuristic Evaluation <ref type="bibr" target="#b41">[42]</ref> (only in the with session)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" xml:id="_ECfpdbA">Procedure</head><p xml:id="_ycjGKCM"><s xml:id="_4JDM5Cy">In the study, participants first went through the adjustment and the calibration of the Pupillab, and finished two tasks in each session, with a 3-minute break in between.</s><s xml:id="_DGUqah5">After completing each session, participants were asked to answer the questionnaire to evaluate the system.</s><s xml:id="_CQPth4V">Each participant was offered $15 as compensation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5" xml:id="_RyhvY2k">Result</head><p xml:id="_WDMBQm3"><s xml:id="_UkgYhS5">We compared the numeric results between the best model and baseline models to validate the advantages of our method (Section 5.5.1).</s><s xml:id="_TxGg5ze">We observed an interesting phenomenon that users did not notice a number of misclassified touches (Section 5.5.2).</s><s xml:id="_p3pmp29">We then compared the two sessions in terms of the number of "wrong" touches (Section 5.5.3) and users feedback (Section 5.5.4).</s><s xml:id="_FcStVg4">The results revealed the advantage of our algorithm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1" xml:id="_C4GYEPD">Validation of</head><p xml:id="_dpV8jQD"><s xml:id="_xNjwfWf">The New Algorithm.</s><s xml:id="_MwqKdAa">We first evaluated our best model with Study 2 data.</s><s xml:id="_TqHgnEV">Like Study 1, two authors manually annotated the data with the tool and solved conflicts with a collective review.</s><s xml:id="_ydC7tVT">Then, we applied our model and two baselines to the Study 2 data.</s><s xml:id="_xy3RbD6">Table <ref type="table">7</ref> summarized the results.</s><s xml:id="_J9tr2Qn">All models had similar performance as Table <ref type="table">4</ref> and our model consistently outperformed the two baselines.</s><s xml:id="_8Hj2vQj">In the real-time system, the model achieved an F-1 score of 0.906.</s><s xml:id="_a27NE8Q">Compared to the model modified from <ref type="bibr" target="#b51">[52]</ref>, our model still had an advantage of 4.0% on the F1 score (with statistical significance).</s><s xml:id="_w2Crnjg">This indicates that our algorithm is stable for new users in the real-time system.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2" xml:id="_N6dFU5u">Good Numeric Results</head><p xml:id="_DusySG3"><s xml:id="_jwdB7cT">According to User-report Data.</s><s xml:id="_d9F5JBD">Alternatively, user subject reports on the false positive/negative touch event also provided another perspective of the system performance.</s><s xml:id="_hwmCvj7">In the with session, the average subjective F1 score among 12 participants achieved a surprisingly high value (0.956, 0.954, 0.955 for precision, recall and F1 score, respectively).</s><s xml:id="_ZWptFSH">Inspection of the data reveals that some misclassified touches were not perceived by participants.</s><s xml:id="_PGFHNQw">For instance, when a user operates with multiple fingers intentionally (moving or rotating), one finger's touch events may be filtered (false negative) but the operation can still continue normally.</s></p><p xml:id="_FkJURtj"><s xml:id="_7FeUkh2">In another example, some unintentional touches are indeed classified as intentional (false positive), but the duration of the touches is too short to affect the normal operation.</s><s xml:id="_YU4PhZc">These gesture-related or task-related misclassified touches do not change the procedure of the on-going operation, and thus are difficult to be noticed by users.</s><s xml:id="_HFbZd3s">In other words, users only detect misclassification when superficial operations do not go as expected.</s><s xml:id="_6RqmtWY">Therefore, they will ignore lots of misclassification cases in the system.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3" xml:id="_wEK7tCB">Less "</head><p xml:id="_K3442DZ"><s xml:id="_HrpSyE8">Wrong" Touches on The New Tabletop.</s><s xml:id="_SuSHXNf">We compared the percentage of the "wrong" touches in two sessions, i.e., the perceived classification error rate in the with session and the perceived accidental touch rate in the without session.</s><s xml:id="_sgZYX3B">A Wilcoxon signed-rank test shows that the ratio of "wrong" touches in the without session is significantly higher than that of the with session (8.0% vs. 4.7%, V = 3, p = 0.002).</s><s xml:id="_3mrQYUN">This indicates that our new system can reduce the number of noticeable unintentional touches considerably.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4" xml:id="_y2HCjje">Positive Feedbacks of</head><p xml:id="_RJqcDmJ"><s xml:id="_pmDnJH8">The New Tabletop.</s><s xml:id="_APrpVNH">We ran Wilcoxon signed-rank tests on the questionnaire results (see Figure <ref type="figure">12</ref>).</s><s xml:id="_YthpJkp">The intuitiveness ratings in the with session were significantly higher than the scores in the without session (V = 37.0, p &lt; 0.05).</s><s xml:id="_r8eJ26z">And the mental load scores in the with sessions were significantly lower than those in the without session (V = 9.0, p &lt; 0.05).</s><s xml:id="_NU6DnjY">In addition, the comparison of physical load and effort in two sessions showed marginal significance.</s><s xml:id="_zhHk6P5">Participants had slightly lower physical load and effort in the with session (for physical load, V = 9.0, p = 0.09 &lt; 0.1, for effort, V = 15.5, p = 0.06 &lt; 0.1).</s><s xml:id="_UnM9gmB">However, the ratings of the system's efficiency and learnability, as well as the performance and frustration in the two sessions did not show any significance.</s><s xml:id="_cyEq5dn">Participants gave positive feedback on the two metrics of intention recognition and error prevention for the with session.</s><s xml:id="_YXju9Wn">Most participants agreed that the system could correctly classify their intention (with the average score as 6.1 ± 0.9).</s><s xml:id="_PNb7qyg">They also rated 5.1 ± 1.1 scores on average for evaluating the system's ability for error prevention.</s><s xml:id="_BrRB5sG">These results show that the purpose of the new system was recognized by the participants.</s><s xml:id="_tN2dvVz">Our new system organically integrates the advantages of digital and physical tabletops.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_aktKjde">DISCUSSION 6.1 Generalizability of the Model for Other Scenarios</head><p xml:id="_dMT6pMe"><s xml:id="_JCFSauU">In this section, we discuss how the feature sets and classification models can be generalized.</s><s xml:id="_E3g8YsQ">We first summarize the similarities and differences between the features of digital tabletops and mobile devices.</s><s xml:id="_Y2eAraB">We then discuss the relationships between our model based on the capacitive technique and other digital table technologies, and show the potential of generalizing our model to other technologies.</s><s xml:id="_3qyydBH">We also discuss how our single-user scenario can provide insights for multi-user scenarios.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1" xml:id="_TNarCwb">Similarities and Differences of Features on Mobile Devices.</head><p xml:id="_D28cwE3"><s xml:id="_gdYX7hw">We investigated five spatio-temporal feature types for unintentional touches classification on multi-touch tabletops (see Table <ref type="table" target="#tab_0">2</ref>).</s><s xml:id="_xCW9Mb6">Our results support the importance of the Gaze for intention classification on a large-scale touchscreen.</s><s xml:id="_J4cRCQS">Comparatively, Face does not have such a great performance.</s></p><p xml:id="_HyXxzDG"><s xml:id="_SJPAwN8">Other features have been discussed in previous literature on mobile phones and tablets.</s><s xml:id="_4AwBbSH">Our findings suggest that those features share similarities between tabletops and mobile devices.</s><s xml:id="_ttvPXT6">For example, unintentional touches are more prone to cluster together (Clustering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">52]</ref>).</s><s xml:id="_FHGTBKr">They appear more often near the edge of the device, in spite of the size of the touch screen (Side <ref type="bibr" target="#b31">[32]</ref>).</s><s xml:id="_eS6W5s6">The distribution of the unintentional touch duration has a long tail <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36]</ref>.</s><s xml:id="_s5JMfXw">These similarities suggest the potential to transfer our method to mobile devices.</s><s xml:id="_6nwfkJF"><ref type="table">Technology</ref>.</s><s xml:id="_rNEShAA">Our tabletop used capacitive sensing technology to register touch events.</s><s xml:id="_uXs4GBY">However, the features used in our model are actually independent of the sensing technique.</s><s xml:id="_EkRtUVN">Features in Table <ref type="table" target="#tab_0">2</ref> only require positional and temporal information of touch events.</s><s xml:id="_NS3hJRg">E.g., Gaze distance can be calculated once the positions of both touch points and gaze points are obtained, History can be calculated once the timestamps of touch points are recorded.</s><s xml:id="_C97FWtV">As long as a system's technology can provide accurate information, it will be compatible with our model.</s><s xml:id="_qKDz46M">Therefore, our system has the potential to be generalized to tabletops with other technologies such as FTIR <ref type="bibr" target="#b14">[15]</ref>, diffused illumination <ref type="bibr" target="#b49">[50]</ref>, etc.</s><s xml:id="_Zn7QfVS">However, we point out that some technical issues related to the sensing accuracy of the positional and temporal information are beyond the scope of our paper, e.g., the noise effect of environment lightness on FTIR/DI desktops.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2" xml:id="_m7VkegN">Features with Other Digital Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3" xml:id="_HrDBukh">Multi-user Scenarios.</head><p xml:id="_V7tAgUQ"><s xml:id="_pzGSbte">In this paper, we only investigated single-user scenarios.</s><s xml:id="_pHjXB83">A multi-user scenario is another major use case of large-scale interactive surfaces.</s><s xml:id="_XmN4pWw">If different users' interaction areas have minute overlap, each user's interactions can be treated as a single-user case and our model can be generalized easily.</s><s xml:id="_tbuQjCu">The major differences between multi-user and single-user scenarios lie in two aspects <ref type="bibr" target="#b34">[35]</ref>: 1) users can have close collaboration, where their touch points are mixed together.</s><s xml:id="_SsYFsHR">2) users can have human-human interaction, where their attention is drawn away from the surface.</s><s xml:id="_HxZvKNZ">These greatly increase the complexity of the system.</s><s xml:id="_awCBTWy">For the first aspect, there are some works trying to distinguish users when multiple users are interacting with a tabletop (e.g., <ref type="bibr" target="#b9">[10]</ref>).</s><s xml:id="_xYaXq4g">This may be one of the promising methods to leverage our model, i.e., dividing a multi-user scenario into several single-user cases, so that our model can be applied.</s><s xml:id="_Cgb7DFN">As for the second aspect, the intuition of "where we look is where we have interests" holds in social interaction as well <ref type="bibr" target="#b21">[22]</ref>.</s><s xml:id="_mtGRHtP">Therefore, gaze can still be an important feature showing users' intention for interaction.</s><s xml:id="_JZHJ6qv">We expect more exploration in the future work.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_dNthPG8">Awareness of Unintentional Touch</head><p xml:id="_42hYn6Z"><s xml:id="_W3uxjEX">There are actually two types of unintentional touches during the interaction on the tabletop.</s><s xml:id="_9bYzyyj">Some touches are unconscious.</s><s xml:id="_nTb7T5u">Users do not realize that they triggered an event.</s><s xml:id="_qk4eG7z">If the system does not respond to such contact, users will not notice it.</s><s xml:id="_smzJJSk">For example, users will only notice the unintentional touches triggered by the thenar (the bases of the thumb) when they see accidental reactions happening around it.</s></p><p xml:id="_fNeXz6U"><s xml:id="_3Vd9BEZ">On the other hand, some unintentional touches are inevitable.</s><s xml:id="_pZ3X7DK">Users "have to" do some specific operations during the interaction to achieve their goal, even having the expectation of the consequence that it will lead to accidental touches.</s><s xml:id="_gTrDUrb">For example, users "have to" put the palms on the surface to support their body while reaching out to distant targets, or they "have to" rest their arms because of fatigue.</s></p><p xml:id="_JRhTvJA"><s xml:id="_aeV69Tv">The mental model essentially works only on the inevitable touches but not those unconscious touches, since the model works only when users are conscious of their touches, either explicitly or implicitly, as reflected in our Study 2. However, designers and developers should be careful about unconscious touches.</s><s xml:id="_MzH5VUm">If they are not filtered, it will have a more significant effect on user experience than the unfiltered inevitable touches, since users do not expect any responses from these unconscious touches.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" xml:id="_ePCTy5J">Limitations and Future Work</head><p xml:id="_66gGuVP"><s xml:id="_xv9Djdy">First, in this research, we explore user touch behavior in only two tasks: map navigation and photo categorization.</s><s xml:id="_sVzN5Hh">Although these tasks are common on multitouch tabletops, there is other unintentional touch behavior we did not investigate which may appear in other interactive scenarios such as drawing sketches.</s><s xml:id="_sZFf7JD">Task-specific patterns may also affect the filtering, which will make the classification more complicated.</s><s xml:id="_bDmffCg">Studies on more tasks will be involved in future work.</s><s xml:id="_39UdNak">Our technique leverages the coordination between touch and gaze.</s><s xml:id="_CDmYcmV">There are some cases where touch input is less guided by visual attention, such as blind typing.</s><s xml:id="_uGDeFMc">The technique might also conflict with other interaction paradigms such as remote object manipulation or peripheral-vision-based interaction <ref type="bibr" target="#b46">[47]</ref>, as summarized in Section 2.3.</s><s xml:id="_UApKXKN">In these tasks, the benefit of our method is diminished.</s><s xml:id="_RynYC2v">In addition, although with a limited number of users we found a consistent result (see the 2nd and 4th box in <ref type="bibr">Fig 6)</ref>, 12 people is a relatively small number for a two-factor within-subject design.</s><s xml:id="_b3yDpzx">The manually labeling of intentional and unintentional touches can introduce potential bias.</s><s xml:id="_xFP4P7S">In the future, the number of participants needs to be increased to incorporate more diverse behavior patterns across different users.</s><s xml:id="_cRWxbpA">Better palm-rejection models <ref type="bibr" target="#b51">[52]</ref> and more sophisticated deep learning (e.g., LSTM) methods are worth exploring.</s></p><p xml:id="_T4yFNJA"><s xml:id="_x5HvAdW">Second, due to the hardware properties of the tabletop, the screen does not have the ability to measure the contact area and pressure level of any touch events <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b69">70]</ref>.</s><s xml:id="_qRAH9Zm">Although the touch area is partially reflected by the Clustering features, the lack of these features limits the performance of our system.</s><s xml:id="_khkQ8QD">For example, the accuracy of our system is not as good as previous work on tablets which leveraged contact size features (91.3% vs. 97.6%</s><s xml:id="_8QmUs8E"><ref type="bibr" target="#b51">[52]</ref>).</s><s xml:id="_Db8zU6N">Regardless of the different interaction paradigms on tablets <ref type="bibr" target="#b51">[52]</ref> and tabletops, it reflects how the contact size feature can improve recognition.</s><s xml:id="_pxxDcHv">Meanwhile, touch pressure can also reflect interaction intention to some extent <ref type="bibr" target="#b60">[61]</ref>.</s><s xml:id="_p77pkU8">These pieces of literature indicate the potential of enhancing our model by including these features.</s><s xml:id="_7qnrRDT">There are some commercial multi-touch tabletops that can provide contact images (e.g., Microsoft Pix-elSense <ref type="bibr" target="#b37">[38]</ref>) or pressure distribution (in the foreseeable future) with fairly high resolution, which needs further exploration.</s><s xml:id="_b3Cbyg8">However, nowadays there are a great number of tabletops that cannot provide area or pressure information, our feature sets can be applied for those tables to support intention recognition.</s></p><p xml:id="_u6YxHqM"><s xml:id="_9CWZcSz">Third, to obtain accurate head orientation and gaze direction, a head-mounted eye tracker is still necessary, which hinders the direct implementation of our model into an ordinary digital surface.</s><s xml:id="_f8b37cM">This reduces the system's viability and scalability.</s><s xml:id="_nppxuu7">Some previous researches have worked on head/gaze direction estimation directly from a camera <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b47">48]</ref>.</s><s xml:id="_9KpZbwP">It is enticing to obviate the necessity of additional devices.</s><s xml:id="_84kmNrY">Our work provides an example of improving user understanding by leveraging other aspects of user behavior <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b72">73]</ref>.</s><s xml:id="_ANMDcRR">More behaviors beyond gaze and face are worth exploration in the future.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_mnyxerw">CONCLUSION</head><p xml:id="_VUBCzAJ"><s xml:id="_Cv6tqGk">In this paper, we investigated the face-gaze-touch coordination patterns of touches on large-scale multi-touch tabletops.</s><s xml:id="_rMg3f4W">In the first user study, we collected empirical behavioral data from 12 participants' performing usual daily tasks on both digital and physical tabletops.</s><s xml:id="_tZXXWXA">Inspection of the data indicates a significant difference between the two types of tabletop: users tend to be more careful and prudent when interacting with the digital tabletop than the physical table.</s><s xml:id="_eBkDhXz">We then extracted five types of spatio-temporal features to classify real-time unintentional touches.</s><s xml:id="_Fq2DM9e">A Gradient Boosting model was trained on the data and achieved 0.914, 0.913 and 0.913 on precision, recall and F1 score respectively.</s><s xml:id="_aDfGmzr">It significantly outperformed the start-of-the-art model by 4.3% on the F1 score.</s><s xml:id="_9XSJWjk">The model reveals the importance of gaze features for unintentional touch recognition.</s><s xml:id="_NRVCm5r">Our second user study evaluated our model in a real-time system.</s><s xml:id="_bJp7D9j">The results validated the advantage of our system over baselines.</s><s xml:id="_QCtaZjC">Participants' positive subjective feedback indicates the effectiveness of our algorithm.</s><s xml:id="_FKTWUPm">provide a user friendly experience by reducing the cognitive barrier preventing users from interacting naturally with the table.</s><s xml:id="_ucbQ4me">This work sheds light on the possibility of future tabletop technology to improve the understanding of users' input intention by linking their gaze and head behavior to their touch behavior.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>33: 6 •Fig. 3 .</head><label>63</label><figDesc><div><p xml:id="_g34vfuE"><s xml:id="_FzUM59g">Fig. 2. System Apparatus</s></p></div></figDesc><graphic coords="6,80.70,104.63,110.41,82.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc><div><p xml:id="_2STQGfa"><s xml:id="_fPGX2kc">Fig. 4. GUI of Annotation Tool: The video of the tabletop screen (left part) and the participant (right part) are presented after calibration.</s><s xml:id="_pKKq2bx">Annotators can navigate back and forth to any time frame.</s><s xml:id="_9kZgpSF">All touch events and face/gaze point are visualized in the left part at frame level.</s></p></div></figDesc><graphic coords="8,265.45,125.88,130.27,92.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 .</head><label>6</label><figDesc><div><p xml:id="_Usm67NP"><s xml:id="_CrXBVHv">Fig. 6.</s><s xml:id="_y42uBQe">Boxplot of touches on two tabletops.</s><s xml:id="_2eQkX5j">Data of sitting and standing postures are merged.</s><s xml:id="_fDwTeP5">"D/P" refers to digital/physical tabletop settings.</s><s xml:id="_HrAjbXX">"Int/Unint" refers to intentional/unintentional touches.</s><s xml:id="_7PV3JGB">The same below.</s></p></div></figDesc><graphic coords="9,238.41,104.60,135.18,104.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .Fig. 9 .Fig. 10 .</head><label>8910</label><figDesc><div><p xml:id="_4hf4Zja"><s xml:id="_vPXFkfj">Fig. 7. Gaze/Face Distance</s></p></div></figDesc><graphic coords="10,81.71,525.05,110.41,86.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 11 .</head><label>11</label><figDesc><div><p xml:id="_dEAZtCf"><s xml:id="_G3S69F3">Fig. 11.</s><s xml:id="_vQj7Gks">Face-gaze average included angle (in degree unit) heatmaps on the tabletop.</s><s xml:id="_V3EEatg">The gaze/face points outside the screen are discarded.</s><s xml:id="_5THY2Un">Left) sitting posture.</s><s xml:id="_6QpGc2u">Right) standing posture</s></p></div></figDesc><graphic coords="14,159.61,104.57,135.18,66.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Table 6 .</head><label>6</label><figDesc><div><p xml:id="_pwnEQrn"><s xml:id="_jR4jttB">Misclassification Pattern Visualization and SummaryPatterns Details Patterns Details (a) Proficient Operation Without Gaze: Proficient operations without the need of gaze.</s><s xml:id="_Fa5EZNp">The misclassification may be caused by large gaze distances since proficient operations do not involve gaze extensively.</s><s xml:id="_m6prERg">(b) Whole Palm Continuous Interaction: Using the whole palm for moving, rotating and zooming in/out.</s><s xml:id="_kC4RKyN">Close touch distances and large numbers of touch points are common features of unintentional touches.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Table 7 .Fig. 12 .</head><label>712</label><figDesc><div><p xml:id="_may22TK"><s xml:id="_WEFDph5">Fig. 12. Results of The Questionnaire.</s><s xml:id="_RhPQfxJ">Error bar indicates the standard error.</s><s xml:id="_xvYPpX6">"w" represents the with session and "wo" represents the without session.</s><s xml:id="_NsRAJUt">+ means marginally significance (p &lt; 0.1) and * means significance (p &lt; 0.05).</s></p></div></figDesc><graphic coords="17,312.23,210.27,157.71,113.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc><div><p xml:id="_GVGVUv5"><s xml:id="_agyZmcG">Feature Groups for Classification</s></p></div></figDesc><table><row><cell>Gaze/Face</cell><cell>Side</cell><cell>Clustering</cell><cell>History</cell></row><row><cell></cell><cell></cell><cell>Touch Distance</cell><cell>Historical Gaze Distance</cell></row><row><cell>Gaze/Face Distance Historical Gaze/Face Distance</cell><cell>Side Distance</cell><cell>Historical Touch Distance Number of Adjacent Touches (20cm)</cell><cell>Historical Face Distance Historical Touch Distance</cell></row><row><cell></cell><cell></cell><cell>Historical Number of Adjacent Touches (10cm)</cell><cell>Historical Number of Adjacent Touches (10cm)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc><div><p xml:id="_6gQzpRB"><s xml:id="_rRAdYPU">Results of Model Comparison.</s><s xml:id="_Fzq8xsv">A paired t-test on the cross-validation F1 scores between the GB and the RF shows significance p &lt; 0.001.</s><s xml:id="_wcQKFqF">The hyperparameters of the GB model among cross-validation is consistent.</s><s xml:id="_Ytea4vg">The best GB model has 100 as the number estimator and 3 as the maximum depth of each tree.</s></p></div></figDesc><table><row><cell>Models</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell>NB</cell><cell cols="3">0.832 ± 0.016 0.836 ± 0.020 0.833 ± 0.008</cell></row><row><cell>KNN</cell><cell cols="3">0.842 ± 0.029 0.844 ± 0.026 0.843 ± 0.027</cell></row><row><cell>MLP</cell><cell cols="3">0.850 ± 0.018 0.848 ± 0.033 0.848 ± 0.025</cell></row><row><cell>LR</cell><cell cols="3">0.874 ± 0.020 0.869 ± 0.012 0.872 ± 0.009</cell></row><row><cell>RF</cell><cell cols="3">0.893 ± 0.020 0.892 ± 0.011 0.893 ± 0.009</cell></row><row><cell>GB</cell><cell cols="3">0.914 ± 0.020 0.913 ± 0.026 0.913 ± 0.014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc><div><p xml:id="_zGnv758"><s xml:id="_fT6qjrE">Results of Feature Ablation.</s><s xml:id="_2VTEUYK">A paired t-test on the cross-validation F1 scores between the full model and the model with Face feature ablated shows significance p &lt; 0.05.</s></p></div></figDesc><table><row><cell>Feature Ablated</cell><cell>Prec</cell><cell>Rec</cell><cell>F1</cell></row><row><cell>-(Full)</cell><cell cols="3">0.914 ± 0.020 0.913 ± 0.026 0.913 ± 0.014</cell></row><row><cell>Face</cell><cell cols="3">0.900 ± 0.021 0.892 ± 0.020 0.895 ± 0.018</cell></row><row><cell>Gaze</cell><cell cols="3">0.842 ± 0.029 0.837 ± 0.031 0.838 ± 0.022</cell></row><row><cell cols="4">Gaze&amp;Face 0.831 ± 0.032 0.826 ± 0.022 0.829 ± 0.026</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_enam3ks"><s xml:id="_73rHrPz">Proc.</s><s xml:id="_mWdXraJ">ACM Interact.</s><s xml:id="_gq7TH3X">Mob.</s><s xml:id="_7WmMYYp">Wearable Ubiquitous Technol., Vol. 4, No. 1, Article 33.</s><s xml:id="_GnGWcee">Publication date: March 2020.</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_qGcCPRc">ACKNOWLEDGMENTS</head><p xml:id="_CvyaM2h"><s xml:id="_cqWgEAP">This work is supported by the <rs type="funder">National Key Research and Development Plan</rs> under Grant No. <rs type="grantNumber">2016YFB1001402</rs>, the <rs type="funder">Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">61521002</rs>, No. <rs type="grantNumber">61672314</rs>, and also by <rs type="funder">Beijing Key Lab of Networked Multimedia</rs>.</s><s xml:id="_ZWrpudt">We thank <rs type="person">Tianheng Wang</rs> for helping code development and <rs type="person">Gero Bergk</rs> for the proof-reading.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pGzRxqJ">
					<idno type="grant-number">2016YFB1001402</idno>
				</org>
				<org type="funding" xml:id="_jPaA6nE">
					<idno type="grant-number">61521002</idno>
				</org>
				<org type="funding" xml:id="_Vp2n4Tt">
					<idno type="grant-number">61672314</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_w2X8WhE">The Pen is Mightier: Understanding Stylus Behaviour While Inking on Tablets</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Annett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781003059325-25</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=2619648.2619680" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_zVsPv4f">Proceedings of Graphics Interface 2014 (GI &apos;14)</title>
		<meeting>Graphics Interface 2014 (GI &apos;14)<address><addrLine>Toronto, Ont., Canada, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Canadian Information Processing Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
	<note type="raw_reference">Michelle Annett, Fraser Anderson, Walter F. Bischof, and Anoop Gupta. 2014. The Pen is Mightier: Understanding Stylus Behaviour While Inking on Tablets. In Proceedings of Graphics Interface 2014 (GI &apos;14). Canadian Information Processing Society, Toronto, Ont., Canada, Canada, 193-200. http://dl.acm.org/citation.cfm?id=2619648.2619680</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_AtPwWzU">Exploring and Understanding Unintended Touch During Direct Pen Interaction</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Annett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.1145/2674915</idno>
		<ptr target="https://doi.org/10.1145/2674915" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_zvvBVeM">ACM Trans. Comput.-Hum. Interact</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2014-11">2014. Nov. 2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michelle Annett, Anoop Gupta, and Walter F. Bischof. 2014. Exploring and Understanding Unintended Touch During Direct Pen Interaction. ACM Trans. Comput.-Hum. Interact. 21, 5, Article 28 (Nov. 2014), 39 pages. https://doi.org/10.1145/2674915</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_r3gPShR">Fluid Grouping: Quantifying Group Engagement Around Interactive Tabletop Exhibits in the Wild</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hammerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Spiegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brenda</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Margaret</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702231</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702231" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_rbB9egV">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
	<note type="raw_reference">Florian Block, James Hammerman, Michael Horn, Amy Spiegel, Jonathan Christiansen, Brenda Phillips, Judy Diamond, E. Margaret Evans, and Chia Shen. 2015. Fluid Grouping: Quantifying Group Engagement Around Interactive Tabletop Exhibits in the Wild. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15). ACM, New York, NY, USA, 867-876. https://doi.org/10.1145/2702123.2702231</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_MWcPB3X">The Effect of View Techniques on Collaboration and Awareness in Tabletop Map-Based Tasks</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Bortolaso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Oskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Gutwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Nicholas Graham</surname></persName>
		</author>
		<idno type="DOI">10.1145/2669485.2669504</idno>
		<ptr target="https://doi.org/10.1145/2669485.2669504" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_wbdC4Aa">Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;14)</title>
		<meeting>the Ninth ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
	<note type="raw_reference">Christophe Bortolaso, Matthew Oskamp, Greg Phillips, Carl Gutwin, and T.C. Nicholas Graham. 2014. The Effect of View Techniques on Collaboration and Awareness in Tabletop Map-Based Tasks. In Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;14). ACM, New York, NY, USA, 79-88. https://doi.org/10.1145/2669485.2669504</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_te6A7EN">A Comparison of User Preferences for Tangible Objects vs Touch Buttons with a Map-based Tabletop Application</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winyu</forename><surname>Chinthammit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paddy</forename><surname>Nixon</surname></persName>
		</author>
		<idno type="DOI">10.1145/2686612.2686645</idno>
		<ptr target="https://doi.org/10.1145/2686612.2686645" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_d2xNkyS">Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design (OzCHI &apos;14)</title>
		<meeting>the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design (OzCHI &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="212" to="215" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mark Brown, Winyu Chinthammit, and Paddy Nixon. 2014. A Comparison of User Preferences for Tangible Objects vs Touch Buttons with a Map-based Tabletop Application. In Proceedings of the 26th Australian Computer-Human Interaction Conference on Designing Futures: The Future of Design (OzCHI &apos;14). ACM, New York, NY, USA, 212-215. https://doi.org/10.1145/2686612.2686645</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_XhVMtt3">Supporting Situation Awareness in Collaborative Tabletop Systems with Automation</title>
		<author>
			<persName><forename type="first">Y.-L. Betty</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stacey</forename><forename type="middle">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="DOI">10.1145/2669485.2669496</idno>
		<ptr target="https://doi.org/10.1145/2669485.2669496" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_FkwVQ6Y">Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;14)</title>
		<meeting>the Ninth ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y.-L. Betty Chang, Stacey D. Scott, and Mark Hancock. 2014. Supporting Situation Awareness in Collaborative Tabletop Systems with Automation. In Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;14). ACM, New York, NY, USA, 185-194. https://doi.org/10.1145/2669485.2669496</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_waVhBth">Feature selection for classification</title>
		<author>
			<persName><forename type="first">Manoranjan</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1088-467x(97)00008-5</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_axrBVgv">Intelligent data analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="131" to="156" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Manoranjan Dash and Huan Liu. 1997. Feature selection for classification. Intelligent data analysis 1, 1-4 (1997), 131-156.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_egxfqcz">Perceived usefulness, perceived ease of use, and user acceptance of information technology</title>
		<author>
			<persName><forename type="first">Fred D</forename><surname>Davis</surname></persName>
		</author>
		<idno type="DOI">10.2307/249008</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TytYC4R">MIS quarterly</title>
		<imprint>
			<biblScope unit="page" from="319" to="340" />
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fred D Davis. 1989. Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS quarterly (1989), 319-340.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_ff5dEhh">Interactive Prototyping of Tabletop and Surface Applications</title>
		<author>
			<persName><forename type="first">Tulio</forename><surname>De Souza Alcantara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Maurer</surname></persName>
		</author>
		<idno type="DOI">10.1145/2494603.2480313</idno>
		<ptr target="https://doi.org/10.1145/2494603.2480313" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_zyJaPg8">Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS &apos;13)</title>
		<meeting>the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tulio de Souza Alcantara, Jennifer Ferreira, and Frank Maurer. 2013. Interactive Prototyping of Tabletop and Surface Applications. In Proceedings of the 5th ACM SIGCHI Symposium on Engineering Interactive Computing Systems (EICS &apos;13). ACM, New York, NY, USA, 229-238. https://doi.org/10.1145/2494603.2480313</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_M7d6Z4T">Group Touch: Distinguishing Tabletop Users in Group Settings via Statistical Modeling of Touch Pairs</title>
		<author>
			<persName><forename type="first">Abigail</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">O</forename><surname>Wobbrock</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025793</idno>
		<ptr target="https://doi.org/10.1145/3025453.3025793" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_pXBNZ2e">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI &apos;17)</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems (CHI &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="47" />
		</imprint>
	</monogr>
	<note type="raw_reference">Abigail C. Evans, Katie Davis, James Fogarty, and Jacob O. Wobbrock. 2017. Group Touch: Distinguishing Tabletop Users in Group Settings via Statistical Modeling of Touch Pairs. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI &apos;17). ACM, New York, NY, USA, 35-47. https://doi.org/10.1145/3025453.3025793</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_8CHCu2J">Greedy function approximation: a gradient boosting machine</title>
		<author>
			<persName><surname>Jerome H Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7DyrWX4">Annals of statistics</title>
		<imprint>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001), 1189-1232.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_DrMwZb7">Artificial neural networks (the multilayer perceptron)-a review of applications in the atmospheric sciences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><surname>Dorling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9MxdTQx">Atmospheric environment</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2627" to="2636" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matt W Gardner and SR Dorling. 1998. Artificial neural networks (the multilayer perceptron)-a review of applications in the atmo- spheric sciences. Atmospheric environment 32, 14 (1998), 2627-2636.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_JU3e96F">Can &quot;Touch&quot; Get Annoying?</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Gerken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Christian</forename><surname>Jetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toni</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Reiterer</surname></persName>
		</author>
		<idno type="DOI">10.1145/1936652.1936704</idno>
		<ptr target="https://doi.org/10.1145/1936652.1936704" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_rhD9ehC">ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;10)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="257" to="258" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jens Gerken, Hans-Christian Jetter, Toni Schmidt, and Harald Reiterer. 2010. Can &quot;Touch&quot; Get Annoying?. In ACM International Con- ference on Interactive Tabletops and Surfaces (ITS &apos;10). ACM, New York, NY, USA, 257-258. https://doi.org/10.1145/1936652.1936704</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_xnXzuTH">Touch screen palm input rejection</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Griffin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_puz4CpS">US Patent App</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">469</biblScope>
			<biblScope unit="page">354</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jason Tyler Griffin. 2013. Touch screen palm input rejection. US Patent App. 13/469,354.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_fCcDfHT">Low-cost multi-touch sensing through frustrated total internal reflection</title>
		<author>
			<persName><surname>Jefferson Y Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wfSf7Mp">Proceedings of the 18th annual ACM symposium on User interface software and technology</title>
		<meeting>the 18th annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="115" to="118" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jefferson Y Han. 2005. Low-cost multi-touch sensing through frustrated total internal reflection. In Proceedings of the 18th annual ACM symposium on User interface software and technology. ACM, 115-118.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_8fBMS4U">Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lowell</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><surname>Staveland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DE66seZ">Advances in psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="139" to="183" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sandra G Hart and Lowell E Staveland. 1988. Development of NASA-TLX (Task Load Index): Results of empirical and theoretical research. Advances in psychology 52 (1988), 139-183.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_j8DdbGw">Human gaze control during real-world scene perception</title>
		<author>
			<persName><surname>John M Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6xfFtNs">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="498" to="504" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">John M Henderson. 2003. Human gaze control during real-world scene perception. Trends in cognitive sciences 7, 11 (2003), 498-504.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_b8aPDzQ">Look where you&apos;re going!&quot;: gaze behaviour associated with maintaining and changing the direction of locomotion</title>
		<author>
			<persName><forename type="first">Aftab</forename><forename type="middle">E</forename><surname>Mark A Hollands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><forename type="middle">N</forename><surname>Patla</surname></persName>
		</author>
		<author>
			<persName><surname>Vickers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BqXbcjA">Experimental brain research</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="230" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mark A Hollands, Aftab E Patla, and Joan N Vickers. 2002. &quot;Look where you&apos;re going!&quot;: gaze behaviour associated with maintaining and changing the direction of locomotion. Experimental brain research 143, 2 (2002), 221-230.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_Gzq9C9A">User See, User Point: Gaze and Cursor Alignment in Web Search</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryen</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Buscher</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208591</idno>
		<ptr target="https://doi.org/10.1145/2207676.2208591" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_7uETCdb">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jeff Huang, Ryen White, and Georg Buscher. 2012. User See, User Point: Gaze and Cursor Alignment in Web Search. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;12). ACM, New York, NY, USA, 1341-1350. https://doi.org/10. 1145/2207676.2208591</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_uYkhEss">The Use of Eye Movements in Human-computer Interaction Techniques: What You Look at is What You Get</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Jacob</surname></persName>
		</author>
		<idno type="DOI">10.1145/123078.128728</idno>
		<ptr target="https://doi.org/10.1145/123078.128728" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_63C3yGa">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="169" />
			<date type="published" when="1991-04">1991. April 1991</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Robert J. K. Jacob. 1991. The Use of Eye Movements in Human-computer Interaction Techniques: What You Look at is What You Get. ACM Trans. Inf. Syst. 9, 2 (April 1991), 152-169. https://doi.org/10.1145/123078.128728</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_DgwafVc">Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Patera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<idno type="DOI">10.1145/2638728.2641695</idno>
		<ptr target="https://doi.org/10.1145/2638728.2641695" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_WB9wWXK">Adjunct Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp &apos;14 Adjunct)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1151" to="1160" />
		</imprint>
	</monogr>
	<note type="raw_reference">Moritz Kassner, William Patera, and Andreas Bulling. 2014. Pupil: An Open Source Platform for Pervasive Eye Tracking and Mobile Gaze-based Interaction. In Adjunct Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp &apos;14 Adjunct). ACM, New York, NY, USA, 1151-1160. https://doi.org/10.1145/2638728.2641695</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_Q6WyuZP">Some functions of gaze-direction in social interaction</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kendon</surname></persName>
		</author>
		<idno type="DOI">10.1016/0001-6918(67)90005-4</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nBxGH8K">Acta psychologica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="22" to="63" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Adam Kendon. 1967. Some functions of gaze-direction in social interaction. Acta psychologica 26 (1967), 22-63.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_ucatDJx">Tables in the Wild: Lessons Learned from a Large-scale Multi-tabletop Deployment</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Kharrufa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeline</forename><surname>Balaam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Heslop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Leat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Olivier</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2466130</idno>
		<ptr target="https://doi.org/10.1145/2470654.2466130" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_qXQYjJA">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ahmed Kharrufa, Madeline Balaam, Phil Heslop, David Leat, Paul Dolan, and Patrick Olivier. 2013. Tables in the Wild: Lessons Learned from a Large-scale Multi-tabletop Deployment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13). ACM, New York, NY, USA, 1021-1030. https://doi.org/10.1145/2470654.2466130</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_jSuF5hZ">Amazing Forearm As an Innovative Interaction Device and Data Storage on Tabletop Display</title>
		<author>
			<persName><forename type="first">Seiya</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asako</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumihisa</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideyuki</forename><surname>Tamura</surname></persName>
		</author>
		<idno type="DOI">10.1145/2396636.2396706</idno>
		<ptr target="https://doi.org/10.1145/2396636.2396706" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_M6BPYQP">Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;12)</title>
		<meeting>the 2012 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="383" to="386" />
		</imprint>
	</monogr>
	<note type="raw_reference">Seiya Koura, Shunsuke Suo, Asako Kimura, Fumihisa Shibata, and Hideyuki Tamura. 2012. Amazing Forearm As an Innovative Inter- action Device and Data Storage on Tabletop Display. In Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;12). ACM, New York, NY, USA, 383-386. https://doi.org/10.1145/2396636.2396706</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_9tZr35B">Combining Observations of Intentional and Unintentional Behaviors for Human-computer Interaction</title>
		<author>
			<persName><forename type="first">Yoshinori</forename><surname>Kuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoyuki</forename><surname>Ishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoru</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshiaki</forename><surname>Shirai</surname></persName>
		</author>
		<idno type="DOI">10.1145/302979.303051</idno>
		<ptr target="https://doi.org/10.1145/302979.303051" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Y3fgtvF">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;99)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;99)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yoshinori Kuno, Tomoyuki Ishiyama, Satoru Nakanishi, and Yoshiaki Shirai. 1999. Combining Observations of Intentional and Unin- tentional Behaviors for Human-computer Interaction. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;99). ACM, New York, NY, USA, 238-245. https://doi.org/10.1145/302979.303051</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_fADqHd3">Tangible Tabletops for Emergency Response: An Exploratory Study</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Landgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Evren Yantaç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Pawełwoźniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Sárosi</surname></persName>
		</author>
		<author>
			<persName><surname>Fjeld</surname></persName>
		</author>
		<idno type="DOI">10.1145/2500342.2500352</idno>
		<ptr target="https://doi.org/10.1145/2500342.2500352" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Qfnv8GM">Proceedings of the International Conference on Multimedia, Interaction, Design and Innovation (MIDI &apos;13)</title>
		<meeting>the International Conference on Multimedia, Interaction, Design and Innovation (MIDI &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note>Article 10</note>
	<note type="raw_reference">Andreas Kunz, Ali Alavi, Jonas Landgren, Asim Evren Yantaç, PawełWoźniak, Zoltán Sárosi, and Morten Fjeld. 2013. Tangible Tabletops for Emergency Response: An Exploratory Study. In Proceedings of the International Conference on Multimedia, Interaction, Design and Innovation (MIDI &apos;13). ACM, New York, NY, USA, Article 10, 8 pages. https://doi.org/10.1145/2500342.2500352</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_EdxQ9HT">PhysicsBox: Playful Educational Tabletop Games</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Langner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Brosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimund</forename><surname>Dachselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheelagh</forename><surname>Carpendale</surname></persName>
		</author>
		<idno type="DOI">10.1145/1936652.1936712</idno>
		<ptr target="https://doi.org/10.1145/1936652.1936712" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_h9KdkUt">ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;10)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="273" to="274" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ricardo Langner, John Brosz, Raimund Dachselt, and Sheelagh Carpendale. 2010. PhysicsBox: Playful Educational Tabletop Games. In ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;10). ACM, New York, NY, USA, 273-274. https://doi.org/10. 1145/1936652.1936712</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_uYQsXGR">Towards Leaning Aware Interaction with Multitouch Tabletops</title>
		<author>
			<persName><forename type="first">Khanh-Duy</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Paknezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paweł</forename><forename type="middle">W</forename><surname>Woźniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Azh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabrielė</forename><surname>Kasparavičiūtė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Fjeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengdong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1145/2971485.2971553</idno>
		<ptr target="https://doi.org/10.1145/2971485.2971553" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_9Kar2j7">Proceedings of the 9th Nordic Conference on Human-Computer Interaction (NordiCHI &apos;16)</title>
		<meeting>the 9th Nordic Conference on Human-Computer Interaction (NordiCHI &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Article 4, 4</note>
	<note type="raw_reference">Khanh-Duy Le, Mahsa Paknezhad, Paweł W. Woźniak, Maryam Azh, Gabrielė Kasparavičiūtė, Morten Fjeld, Shengdong Zhao, and Michael S. Brown. 2016. Towards Leaning Aware Interaction with Multitouch Tabletops. In Proceedings of the 9th Nordic Conference on Human-Computer Interaction (NordiCHI &apos;16). ACM, New York, NY, USA, Article 4, 4 pages. https://doi.org/10.1145/2971485.2971553</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_yxeQB7c">A Framework for Attention-based Personal Photo Manager</title>
		<author>
			<persName><forename type="first">Wen-Hung</forename><surname>Liao</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1732003.1732067" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_sYKYDQ7">Proceedings of the 2009 IEEE International Conference on Systems, Man and Cybernetics (SMC&apos;09)</title>
		<meeting>the 2009 IEEE International Conference on Systems, Man and Cybernetics (SMC&apos;09)<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2128" to="2132" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wen-Hung Liao. 2009. A Framework for Attention-based Personal Photo Manager. In Proceedings of the 2009 IEEE International Con- ference on Systems, Man and Cybernetics (SMC&apos;09). IEEE Press, Piscataway, NJ, USA, 2128-2132. http://dl.acm.org/citation.cfm?id= 1732003.1732067</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_FRuwjvf">Gaze and Mouse Coordination in Everyday Work</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Liebling</surname></persName>
		</author>
		<author>
			<persName><surname>Dumais</surname></persName>
		</author>
		<idno type="DOI">10.1145/2638728.2641692</idno>
		<ptr target="https://doi.org/10.1145/2638728.2641692" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_wS7NJXR">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (UbiComp &apos;14 Adjunct)</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (UbiComp &apos;14 Adjunct)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1141" to="1150" />
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel J. Liebling and Susan T. Dumais. 2014. Gaze and Mouse Coordination in Everyday Work. In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication (UbiComp &apos;14 Adjunct). ACM, New York, NY, USA, 1141-1150. https://doi.org/10.1145/2638728.2641692</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_qczy6qv">Permulin: Mixed-focus Collaboration on Multi-view Tabletops</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Lissermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jochen</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schmitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Steimle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Mühlhäuser</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557405</idno>
		<ptr target="https://doi.org/10.1145/2556288.2557405" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_UKmEa8R">Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)</title>
		<meeting>the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3191" to="3200" />
		</imprint>
	</monogr>
	<note type="raw_reference">Roman Lissermann, Jochen Huber, Martin Schmitz, Jürgen Steimle, and Max Mühlhäuser. 2014. Permulin: Mixed-focus Collaboration on Multi-view Tabletops. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14). ACM, New York, NY, USA, 3191-3200. https://doi.org/10.1145/2556288.2557405</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_7HceG4Q">Gesture On: Enabling Always-On Touch Gestures for Fast Mobile Access from the Device Standby Mode</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702610</idno>
		<ptr target="https://doi.org/10.1145/2702123.270261033:22•Xuetal" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_RmmGvsF">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hao Lu and Yang Li. 2015. Gesture On: Enabling Always-On Touch Gestures for Fast Mobile Access from the Device Standby Mode. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15). ACM, New York, NY, USA, 3355-3364. https://doi.org/10.1145/2702123.2702610 33:22 • Xu et al.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_gWKA3Nx">Gaze and Speech in Attentive User Interfaces</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">P</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teenie</forename><surname>Matlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">S</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barton</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-40063-x_1</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=645524.656806" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Fu687WS">Proceedings of the Third International Conference on Advances in Multimodal Interfaces (ICMI &apos;00)</title>
		<meeting>the Third International Conference on Advances in Multimodal Interfaces (ICMI &apos;00)<address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="raw_reference">Paul P. Maglio, Teenie Matlock, Christopher S. Campbell, Shumin Zhai, and Barton A. Smith. 2000. Gaze and Speech in Attentive User Interfaces. In Proceedings of the Third International Conference on Advances in Multimodal Interfaces (ICMI &apos;00). Springer-Verlag, London, UK, UK, 1-7. http://dl.acm.org/citation.cfm?id=645524.656806</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_mzv2fgX">SwitchBack: Using Focus and Saccade Tracking to Guide Users&apos; Attention for Mobile Task Resumption</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mariakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Tanvir Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shwetak</forename><forename type="middle">N</forename><surname>Aumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">O</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><surname>Wobbrock</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702539</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702539" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_2yQP3Z2">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015. 2953-2962</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Mariakakis, Mayank Goel, Md Tanvir Islam Aumi, Shwetak N. Patel, and Jacob O. Wobbrock. 2015. SwitchBack: Using Focus and Saccade Tracking to Guide Users&apos; Attention for Mobile Task Resumption. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15). ACM, New York, NY, USA, 2953-2962. https://doi.org/10.1145/2702123.2702539</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_n5vXQCM">Rethinking &apos;Multi-user&apos;: An In-the-wild Study of How Groups Approach a Walk-up-and-use Tabletop Interface</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kreitmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Davies</surname></persName>
		</author>
		<idno type="DOI">10.1145/1978942.1979392</idno>
		<ptr target="https://doi.org/10.1145/1978942.1979392" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_7EuUyzZ">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;11)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3033" to="3042" />
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Marshall, Richard Morris, Yvonne Rogers, Stefan Kreitmayer, and Matt Davies. 2011. Rethinking &apos;Multi-user&apos;: An In-the-wild Study of How Groups Approach a Walk-up-and-use Tabletop Interface. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;11). ACM, New York, NY, USA, 3033-3042. https://doi.org/10.1145/1978942.1979392</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_6ySQ7MS">Identifying Unintentional Touches on Handheld Touch Screen Devices</title>
		<author>
			<persName><forename type="first">Juha</forename><surname>Matero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashley</forename><surname>Colley</surname></persName>
		</author>
		<idno type="DOI">10.1145/2317956.2318031</idno>
		<ptr target="https://doi.org/10.1145/2317956.2318031" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_dxGKfwd">Proceedings of the Designing Interactive Systems Conference (DIS &apos;12)</title>
		<meeting>the Designing Interactive Systems Conference (DIS &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="506" to="509" />
		</imprint>
	</monogr>
	<note type="raw_reference">Juha Matero and Ashley Colley. 2012. Identifying Unintentional Touches on Handheld Touch Screen Devices. In Proceedings of the Designing Interactive Systems Conference (DIS &apos;12). ACM, New York, NY, USA, 506-509. https://doi.org/10.1145/2317956.2318031</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_DumrsYB">Empirical Evaluation of Uni-and Bimodal Pen and Touch Interaction Properties on Digital Tabletops</title>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Matulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moira</forename><surname>Norrie</surname></persName>
		</author>
		<idno type="DOI">10.1145/2396636.2396659</idno>
		<ptr target="https://doi.org/10.1145/2396636.2396659" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_98T9H3R">Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;12)</title>
		<meeting>the 2012 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;12)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
	<note type="raw_reference">Fabrice Matulic and Moira Norrie. 2012. Empirical Evaluation of Uni-and Bimodal Pen and Touch Interaction Properties on Digital Tabletops. In Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces (ITS &apos;12). ACM, New York, NY, USA, 143-152. https://doi.org/10.1145/2396636.2396659</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_YCxeXbH">Hand Contact Shape Recognition for Posture-Based Tabletop Widgets and Interaction</title>
		<author>
			<persName><forename type="first">Fabrice</forename><surname>Matulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimund</forename><surname>Dachselt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3132272.3134126</idno>
		<ptr target="https://doi.org/10.1145/3132272.3134126" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_XSMxuv6">Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces (ISS &apos;17)</title>
		<meeting>the 2017 ACM International Conference on Interactive Surfaces and Spaces (ISS &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
	<note type="raw_reference">Fabrice Matulic, Daniel Vogel, and Raimund Dachselt. 2017. Hand Contact Shape Recognition for Posture-Based Tabletop Widgets and Interaction. In Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces (ISS &apos;17). ACM, New York, NY, USA, 3-11. https://doi.org/10.1145/3132272.3134126</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_37kP6rq">Combining Touch and Gaze for Distant Selection in a Tabletop Setting</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mauderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Daiber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Mauderer and Florian Daiber. 2013. Combining Touch and Gaze for Distant Selection in a Tabletop Setting. (2013).</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_FuwZFgk">WUW -Wear Ur World: A Wearable Gestural Interface</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pattie</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1145/1520340.1520626</idno>
		<ptr target="https://doi.org/10.1145/1520340.1520626" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_NygkMJd">CHI &apos;09 Extended Abstracts on Human Factors in Computing Systems (CHI EA &apos;09)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="4111" to="4116" />
		</imprint>
	</monogr>
	<note type="raw_reference">Pranav Mistry, Pattie Maes, and Liyan Chang. 2009. WUW -Wear Ur World: A Wearable Gestural Interface. In CHI &apos;09 Extended Abstracts on Human Factors in Computing Systems (CHI EA &apos;09). ACM, New York, NY, USA, 4111-4116. https://doi.org/10.1145/1520340.1520626</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_6RMAQtC">Head pose estimation in computer vision: A survey</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><forename type="middle">Manubhai</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wvkG2eX">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="607" to="626" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Erik Murphy-Chutorian and Mohan Manubhai Trivedi. 2009. Head pose estimation in computer vision: A survey. IEEE transactions on pattern analysis and machine intelligence 31, 4 (2009), 607-626.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_QzMWMhn">Usability inspection methods</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Nielsen</surname></persName>
		</author>
		<idno type="DOI">10.1145/259963.260531</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rT2gzpZ">Conference companion on Human factors in computing systems</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="413" to="414" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jakob Nielsen. 1994. Usability inspection methods. In Conference companion on Human factors in computing systems. ACM, 413-414.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_VfadHby">Gaze-touch: Combining Gaze with Multi-touch for Interaction on the Same Surface</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Ki</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2642918.2647397</idno>
		<ptr target="https://doi.org/10.1145/2642918.2647397" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_WEJx3Va">Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;14)</title>
		<meeting>the 27th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ken Pfeuffer, Jason Alexander, Ming Ki Chong, and Hans Gellersen. 2014. Gaze-touch: Combining Gaze with Multi-touch for Interaction on the Same Surface. In Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology (UIST &apos;14). ACM, New York, NY, USA, 509-518. https://doi.org/10.1145/2642918.2647397</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_HwKRAqc">Gaze-Shifting: Direct-Indirect Input with Pen and Touch Modulated by Gaze</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Ki</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2807442.2807460</idno>
		<ptr target="https://doi.org/10.1145/2807442.2807460" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_kVCyv4J">Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;#38; Technology (UIST &apos;15)</title>
		<meeting>the 28th Annual ACM Symposium on User Interface Software &amp;#38; Technology (UIST &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="373" to="383" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ken Pfeuffer, Jason Alexander, Ming Ki Chong, Yanxia Zhang, and Hans Gellersen. 2015. Gaze-Shifting: Direct-Indirect Input with Pen and Touch Modulated by Gaze. In Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;#38; Technology (UIST &apos;15). ACM, New York, NY, USA, 373-383. https://doi.org/10.1145/2807442.2807460</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_gGJtaVX">Gaze and Touch Interaction on Tablets</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Pfeuffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2984511.2984514</idno>
		<ptr target="https://doi.org/10.1145/2984511.2984514" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Cz63xQe">Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST &apos;16)</title>
		<meeting>the 29th Annual Symposium on User Interface Software and Technology (UIST &apos;16)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="301" to="311" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ken Pfeuffer and Hans Gellersen. 2016. Gaze and Touch Interaction on Tablets. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST &apos;16). ACM, New York, NY, USA, 301-311. https://doi.org/10.1145/2984511.2984514</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main" xml:id="_Pbs3N3a">Optitrack</title>
		<ptr target="http://www.naturalpoint.com/optitrack/" />
		<imprint>
			<date type="published" when="0222">2011. 22 2 2014. 2011</date>
			<publisher>Natural Point, Inc</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Natural Point. 2011. Optitrack. Natural Point, Inc.,[Online]. Available: http://www. naturalpoint. com/optitrack/.[Accessed 22 2 2014] (2011).</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_WTA5aRt">SuperVision: Playing with Gaze Aversion and Peripheral Vision</title>
		<author>
			<persName><forename type="first">Argenis</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gomez</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300703</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300703" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_byxVKMD">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">473</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Argenis Ramirez Gomez and Hans Gellersen. 2019. SuperVision: Playing with Gaze Aversion and Peripheral Vision. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19). ACM, New York, NY, USA, Article 473, 12 pages. https: //doi.org/10.1145/3290605.3300703</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_E6p83kC">Estimating gaze direction from low-resolution faces in video</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eEYHwft">Computer Vision-ECCV</title>
		<imprint>
			<biblScope unit="volume">2006</biblScope>
			<biblScope unit="page" from="402" to="415" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Neil Robertson and Ian Reid. 2006. Estimating gaze direction from low-resolution faces in video. Computer Vision-ECCV 2006 (2006), 402-415.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_vmDuhPH">Designing Interactive Advertisements for Public Displays</title>
		<author>
			<persName><forename type="first">Hasibullah</forename><surname>Sahibzada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Hornecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Echtler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">Tobias</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3025453.3025531</idno>
		<ptr target="https://doi.org/10.1145/3025453.3025531" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_DVq8bYJ">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI &apos;17)</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems (CHI &apos;17)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1518" to="1529" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hasibullah Sahibzada, Eva Hornecker, Florian Echtler, and Patrick Tobias Fischer. 2017. Designing Interactive Advertisements for Public Displays. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI &apos;17). ACM, New York, NY, USA, 1518-1529. https://doi.org/10.1145/3025453.3025531</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_59VdqaS">Multi-touch surfaces: A technical guide</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Schöning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Brandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Daiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Echtler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Löchtefeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Motamedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Olivier</surname></persName>
		</author>
		<idno type="DOI">10.1080/2151237x.2009.10129285</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gbqKucp">IEEE Tabletops and Interactive Surfaces</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johannes Schöning, Peter Brandl, Florian Daiber, Florian Echtler, Otmar Hilliges, Jonathan Hook, Markus Löchtefeld, Nima Motamedi, Laurence Muller, Patrick Olivier, et al. 2008. Multi-touch surfaces: A technical guide. IEEE Tabletops and Interactive Surfaces 2, 11 (2008).</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_4p965eZ">Combining Body Pose, Gaze, and Gesture to Determine Intention to Interact in Vision-based Interfaces</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Claudius</forename><surname>Marais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommer</forename><surname>Leyvand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Mankoff</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2556989</idno>
		<ptr target="https://doi.org/10.1145/2556288.2556989" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_KqTP65D">Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)</title>
		<meeting>the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3443" to="3452" />
		</imprint>
	</monogr>
	<note type="raw_reference">Julia Schwarz, Charles Claudius Marais, Tommer Leyvand, Scott E. Hudson, and Jennifer Mankoff. 2014. Combining Body Pose, Gaze, and Gesture to Determine Intention to Interact in Vision-based Interfaces. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14). ACM, New York, NY, USA, 3443-3452. https://doi.org/10.1145/2556288.2556989</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_WvTcYBk">Probabilistic Palm Rejection Using Spatiotemporal Touch Features and Iterative Classification</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Harrison</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556288.2557056</idno>
		<ptr target="https://doi.org/10.1145/2556288.2557056" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_aa2Y3f3">Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)</title>
		<meeting>the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2014. 2009-2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Julia Schwarz, Robert Xiao, Jennifer Mankoff, Scott E. Hudson, and Chris Harrison. 2014. Probabilistic Palm Rejection Using Spatiotem- poral Touch Features and Iterative Classification. In Proceedings of the 32Nd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;14). ACM, New York, NY, USA, 2009-2012. https://doi.org/10.1145/2556288.2557056</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_6wMZnra">Explicating &amp;#34;Implicit Interaction&amp;#34;: An Examination of the Concept and Challenges for Research</title>
		<author>
			<persName><forename type="first">Barış</forename><surname>Serim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Jacucci</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300647</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300647" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_DjxFknh">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">417</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Barış Serim and Giulio Jacucci. 2019. Explicating &amp;#34;Implicit Interaction&amp;#34;: An Examination of the Concept and Challenges for Research. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19). ACM, New York, NY, USA, Article 417, 16 pages. https://doi.org/10.1145/3290605.3300647</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_SpMpmHr">Eye&amp;#38;Head: Synergetic Eye and Head Movement for Gaze Pointing and Selection</title>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Sidenmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3332165.3347921</idno>
		<ptr target="https://doi.org/10.1145/3332165.3347921" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_vaaZwqh">Proceedings of the 32Nd Annual ACM Symposium on User Interface Software and Technology (UIST &apos;19)</title>
		<meeting>the 32Nd Annual ACM Symposium on User Interface Software and Technology (UIST &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1161" to="1174" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ludwig Sidenmark and Hans Gellersen. 2019. Eye&amp;#38;Head: Synergetic Eye and Head Movement for Gaze Pointing and Selection. In Proceedings of the 32Nd Annual ACM Symposium on User Interface Software and Technology (UIST &apos;19). ACM, New York, NY, USA, 1161-1174. https://doi.org/10.1145/3332165.3347921</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_97BanGr">Gaze Behaviour on Interacted Objects During Hand Interaction in Virtual Reality for Eye Tracking Calibration</title>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Sidenmark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Lundström</surname></persName>
		</author>
		<idno type="DOI">10.1145/3314111.3319815</idno>
		<ptr target="https://doi.org/10.1145/3314111.3319815" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_vaKQ5tK">Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications (ETRA &apos;19)</title>
		<meeting>the 11th ACM Symposium on Eye Tracking Research &amp; Applications (ETRA &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Article 6, 9</note>
	<note type="raw_reference">Ludwig Sidenmark and Anders Lundström. 2019. Gaze Behaviour on Interacted Objects During Hand Interaction in Virtual Reality for Eye Tracking Calibration. In Proceedings of the 11th ACM Symposium on Eye Tracking Research &amp; Applications (ETRA &apos;19). ACM, New York, NY, USA, Article 6, 9 pages. https://doi.org/10.1145/3314111.3319815</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_PjAeMpN">VMotion: Designing a Seamless Walking Experience in VR</title>
		<author>
			<persName><forename type="first">Misha</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aske</forename><surname>Mottelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pattie</forename><surname>Maes</surname></persName>
		</author>
		<idno type="DOI">10.1145/3196709.3196792</idno>
		<ptr target="https://doi.org/10.1145/3196709.3196792" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Z4Q6X6a">Proceedings of the 2018 Designing Interactive Systems Conference (DIS &apos;18)</title>
		<meeting>the 2018 Designing Interactive Systems Conference (DIS &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
	<note type="raw_reference">Misha Sra, Xuhai Xu, Aske Mottelson, and Pattie Maes. 2018. VMotion: Designing a Seamless Walking Experience in VR. In Proceedings of the 2018 Designing Interactive Systems Conference (DIS &apos;18). ACM, New York, NY, USA, 59-70. https://doi.org/10.1145/3196709. 3196792</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_efJSpRe">Heuristic filtering and reliable calibration methods for video-based pupil-tracking systems</title>
		<author>
			<persName><forename type="first">Dave</forename><forename type="middle">M</forename><surname>Stampe</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03204486</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MfCJwsU">Behavior Research Methods, Instruments, &amp; Computers</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="142" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dave M Stampe. 1993. Heuristic filtering and reliable calibration methods for video-based pupil-tracking systems. Behavior Research Methods, Instruments, &amp; Computers 25, 2 (1993), 137-142.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_svEUCxF">Still Looking: Investigating Seamless Gaze-supported Selection, Positioning, and Manipulation of Distant Targets</title>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Stellmach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raimund</forename><surname>Dachselt</surname></persName>
		</author>
		<idno type="DOI">10.1145/2470654.2470695</idno>
		<ptr target="https://doi.org/10.1145/2470654.2470695" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_qy7HnyA">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sophie Stellmach and Raimund Dachselt. 2013. Still Looking: Investigating Seamless Gaze-supported Selection, Positioning, and Ma- nipulation of Distant Targets. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;13). ACM, New York, NY, USA, https://doi.org/10.1145/2470654.2470695</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_2FFWycR">From gaze to focus of attention</title>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Finke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3pPekS8">Visual Information and Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="765" to="772" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rainer Stiefelhagen, Michael Finke, Jie Yang, and Alex Waibel. 1999. From gaze to focus of attention. In Visual Information and Infor- mation Systems. Springer, 765-772.</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_XT9YqGD">Affordances for Manipulation of Physical Versus Digital Media on Interactive Surfaces</title>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Terrenghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Sellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<idno type="DOI">10.1145/1240624.1240799</idno>
		<ptr target="https://doi.org/10.1145/1240624.1240799" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_suGqtPV">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;07)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;07)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1157" to="1166" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lucia Terrenghi, David Kirk, Abigail Sellen, and Shahram Izadi. 2007. Affordances for Manipulation of Physical Versus Digital Media on Interactive Surfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;07). ACM, New York, NY, USA, 1157-1166. https://doi.org/10.1145/1240624.1240799</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_5SGJ6xR">Unintentional touch rejection</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Reed L Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">P</forename><surname>Kolmykov-Zotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">D</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QT5qx8K">US Patent</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Reed L Townsend, Alexander J Kolmykov-Zotov, Steven P Dodge, and Bryan D Scott. 2011. Unintentional touch rejection. US Patent 8,018,440.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_kGX95gq">Gaze+RST: Integrating Gaze and Multitouch for Remote Rotate-Scale-Translate Tasks</title>
		<author>
			<persName><forename type="first">Jayson</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2702123.2702355</idno>
		<ptr target="https://doi.org/10.1145/2702123.2702355" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_YJC8SvB">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4179" to="4188" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jayson Turner, Jason Alexander, Andreas Bulling, and Hans Gellersen. 2015. Gaze+RST: Integrating Gaze and Multitouch for Remote Rotate-Scale-Translate Tasks. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI &apos;15). ACM, New York, NY, USA, 4179-4188. https://doi.org/10.1145/2702123.2702355</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_nKP3R4V">Combining Gaze with Manual Interaction to Extend Physical Reach</title>
		<author>
			<persName><forename type="first">Jayson</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Gellersen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2029956.2029966</idno>
		<ptr target="https://doi.org/10.1145/2029956.2029966" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_qwCu7JU">Proceedings of the 1st International Workshop on Pervasive Eye Tracking &amp;#38; Mobile Eye-based Interaction (PETMEI &apos;11)</title>
		<meeting>the 1st International Workshop on Pervasive Eye Tracking &amp;#38; Mobile Eye-based Interaction (PETMEI &apos;11)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jayson Turner, Andreas Bulling, and Hans Gellersen. 2011. Combining Gaze with Manual Interaction to Extend Physical Reach. In Proceedings of the 1st International Workshop on Pervasive Eye Tracking &amp;#38; Mobile Eye-based Interaction (PETMEI &apos;11). ACM, New York, NY, USA, 33-36. https://doi.org/10.1145/2029956.2029966</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_nraAfuB">Combining Direct and Indirect Touch Input for Interactive Workspaces Using Gaze Input</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrii</forename><surname>Matviienko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Schöning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Borchers</surname></persName>
		</author>
		<idno type="DOI">10.1145/2788940.2788949</idno>
		<ptr target="https://doi.org/10.1145/2788940.2788949" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_PsxngAx">Proceedings of the 3rd ACM Symposium on Spatial User Interaction (SUI &apos;15)</title>
		<meeting>the 3rd ACM Symposium on Spatial User Interaction (SUI &apos;15)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
	<note type="raw_reference">Simon Voelker, Andrii Matviienko, Johannes Schöning, and Jan Borchers. 2015. Combining Direct and Indirect Touch Input for Inter- active Workspaces Using Gaze Input. In Proceedings of the 3rd ACM Symposium on Spatial User Interaction (SUI &apos;15). ACM, New York, NY, USA, 79-88. https://doi.org/10.1145/2788940.2788949</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_bgaJMXU">Occlusion-aware Interfaces</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravin</forename><surname>Balakrishnan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753365</idno>
		<ptr target="https://doi.org/10.1145/1753326.1753365" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_BK6dGBS">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;10)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel Vogel and Ravin Balakrishnan. 2010. Occlusion-aware Interfaces. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;10). ACM, New York, NY, USA, 263-272. https://doi.org/10.1145/1753326.1753365</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_hjHms9R">Understanding User Behavior For Document Recommendation</title>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farheen</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Popp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Routhwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farnaz</forename><surname>Jahanbakhsh</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380071</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380071" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_hA2y5S9">The World Wide Web Conference (WWW &apos;20)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Xuhai Xu, Ahmed Hassan Awadallah, Susan T. Dumais, Farheen Omar, Bogdan Popp, Robert Routhwaite, and Farnaz Jahanbakhsh. 2020. Understanding User Behavior For Document Recommendation. In The World Wide Web Conference (WWW &apos;20). Association for Computing Machinery, New York, NY, USA, 7. https://doi.org/10.1145/3366423.3380071</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_AM8jghp">Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students</title>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prerna</forename><surname>Chikersal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afsaneh</forename><surname>Doryab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniella</forename><forename type="middle">K</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janine</forename><forename type="middle">M</forename><surname>Dutcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Tumminia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheldon</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasey</forename><forename type="middle">G</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">David</forename><surname>Creswell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351274</idno>
		<ptr target="https://doi.org/10.1145/3351274" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_tKQuwwt">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2019-09">2019. Sept. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xuhai Xu, Prerna Chikersal, Afsaneh Doryab, Daniella K. Villalba, Janine M. Dutcher, Michael J. Tumminia, Tim Althoff, Sheldon Cohen, Kasey G. Creswell, J. David Creswell, and et al. 2019. Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 3, 3, Article Article 116 (Sept. 2019), 33 pages. https://doi.org/10.1145/3351274</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_wCxRbyM">Hand Range Interface: Information Always at Hand with a Body-centric Mid-air Input Surface</title>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Dancu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pattie</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suranga</forename><surname>Nanayakkara</surname></persName>
		</author>
		<idno type="DOI">10.1145/3229434.3229449</idno>
		<ptr target="https://doi.org/10.1145/3229434.3229449" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_qp6g3ac">Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;18)</title>
		<meeting>the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>Article 5</note>
	<note type="raw_reference">Xuhai Xu, Alexandru Dancu, Pattie Maes, and Suranga Nanayakkara. 2018. Hand Range Interface: Information Always at Hand with a Body-centric Mid-air Input Surface. In Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI &apos;18). ACM, New York, NY, USA, Article 5, 12 pages. https://doi.org/10.1145/3229434.3229449</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_XAdSPjJ">EarBuddy: Enabling On-Face Interaction via Wireless Earbuds</title>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mariakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376836</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376836" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_qepq6vD">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI &apos;20)</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems (CHI &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Xuhai Xu, Haitian Shi, Xin Yi, Wenjia Liu, Yukang Yan, Yuanchun Shi, Alex Mariakakis, Jennifer Mankoff, and Anind K. Dey. 2020. EarBuddy: Enabling On-Face Interaction via Wireless Earbuds. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI &apos;20). Association for Computing Machinery, New York, NY, USA, 14. https://doi.org/10.1145/3313831.3376836</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_QUWFtGC">Clench Interface: Novel Biting Input Techniques</title>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Mankoff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300505</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300505" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_fCYa5gK">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">275</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Xuhai Xu, Chun Yu, Anind K. Dey, and Jennifer Mankoff. 2019. Clench Interface: Novel Biting Input Techniques. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19). ACM, New York, NY, USA, Article 275, 12 pages. https: //doi.org/10.1145/3290605.3300505</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_rWkNpZS">HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays</title>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingtian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3380983</idno>
		<ptr target="https://doi.org/10.1145/3380983" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_yt9Tewb">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">22</biblScope>
			<date type="published" when="2020-03">2020. March 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yukang Yan, Yingtian Shi, Chun Yu, and Yuanchun Shi. 2020. HeadCross: Exploring Head-Based Crossing Selection on Head-Mounted Displays. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 1 (March 2020), 22. https://doi.org/10.1145/3380983</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main" xml:id="_AyHZqjT">VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval</title>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3173574.3173652</idno>
		<ptr target="https://doi.org/10.1145/3173574.3173652" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_8XFa4gE">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI &apos;18)</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems (CHI &apos;18)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Yukang Yan, Chun Yu, Xiaojuan Ma, Xin Yi, Ke Sun, and Yuanchun Shi. 2018. VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI &apos;18). ACM, New York, NY, USA, Article 78, 13 pages. https://doi.org/10.1145/3173574.3173652</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_f7a4Smt">FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions</title>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengrui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376810</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376810" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_WwUYuzH">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI &apos;20)</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems (CHI &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Yukang Yan, Chun Yu, Wengrui Zheng, Ruining Tang, Xuhai Xu, and Yuanchun Shi. 2020. FrownOnError: Interrupting Responses from Smart Speakers by Facial Expressions. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI &apos;20). Association for Computing Machinery, New York, NY, USA, 14. https://doi.org/10.1145/3313831.3376810</note>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main" xml:id="_N6ehzaV">Eye movements during perception of complex objects</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alfred</surname></persName>
		</author>
		<author>
			<persName><surname>Yarbus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Alfred L Yarbus. 1967. Eye movements during perception of complex objects. Springer.</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main" xml:id="_hMufYTa">Nanometer touch film production method</title>
		<author>
			<persName><forename type="first">Yaning</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejiang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yaning Luo Zejiang Liu. 2012. Nanometer touch film production method.</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_8fesCY3">Manual and Gaze Input Cascaded (MAGIC) Pointing</title>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Ihde</surname></persName>
		</author>
		<idno type="DOI">10.1145/302979.303053</idno>
		<ptr target="https://doi.org/10.1145/302979.303053" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_YbmcvQJ">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;99)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;99)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shumin Zhai, Carlos Morimoto, and Steven Ihde. 1999. Manual and Gaze Input Cascaded (MAGIC) Pointing. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;99). ACM, New York, NY, USA, 246-253. https://doi.org/10.1145/ 302979.303053</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main" xml:id="_K4GqhJU">Sensing Posture-Aware Pen+Touch Interaction on Tablets</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Pahud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gierad</forename><surname>Laput</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mcguffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mittereder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Hinckley</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300285</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300285" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_S4A8UW3">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang Zhang, Michel Pahud, Christian Holz, Haijun Xia, Gierad Laput, Michael McGuffin, Xiao Tu, Andrew Mittereder, Fei Su, William Buxton, and Ken Hinckley. 2019. Sensing Posture-Aware Pen+Touch Interaction on Tablets. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19). ACM, New York, NY, USA, Article 55, 14 pages. https://doi.org/10.1145/3290605.3300285</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
