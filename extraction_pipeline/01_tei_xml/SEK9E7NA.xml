<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_4muF4x9">Designing Reinforcement Learning Algorithms for Digital Interventions: Pre-Implementation Guidelines</title>
				<funder ref="#_zEWWZ4B">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000002</idno>
				</funder>
				<funder ref="#_tUpFFZD">
					<orgName type="full">National Science Foundation Graduate Research Fellowship Program</orgName>
					<orgName type="abbreviated">GRFP</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100023581</idno>
				</funder>
				<funder ref="#_uvNBVU6 #_EzPZ2t4 #_5SHq5aU #_mVRQRS4">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_sT5YVAn #_t4sC39e">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">Â© 2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).</p>
				</availability>
				<date type="published" when="2022-07-22">22 July 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Anna</forename><forename type="middle">L</forename><surname>Trella</surname></persName>
							<email>annatrella@g.harvard.edu</email>
							<idno type="ORCID">0000-0003-4779-9115</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA;</note>
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02420</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kel</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
							<email>kellywzhang@seas.harvard.edu</email>
							<idno type="ORCID">0000-0003-0850-4978</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA;</note>
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02420</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Inbal</forename><surname>Nahum-Shani</surname></persName>
							<idno type="ORCID">0000-0001-6138-9089</idno>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Institute for Social Research , University of Michigan , Ann Arbor , MI 48109 , USA;</note>
								<orgName type="department">Institute for Social Research</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vivek</forename><surname>Shetty</surname></persName>
							<email>vshetty@ucla.edu</email>
							<idno type="ORCID">0000-0002-3167-3318</idno>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Schools of Dentistry &amp; Engineering , University of California , Los Angeles , CA 90095 , USA;</note>
								<orgName type="department">Schools of Dentistry &amp; Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
							<email>finale@seas.harvard.edu</email>
							<idno type="ORCID">0000-0003-2886-3898</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA;</note>
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02420</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
							<email>samurphy@fas.harvard.edu</email>
							<idno type="ORCID">0000-0002-2032-4286</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA;</note>
								<orgName type="department">School of Engineering and Applied Sciences</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<postCode>02420</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country>USA;</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_2n2mt6V">Designing Reinforcement Learning Algorithms for Digital Interventions: Pre-Implementation Guidelines</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-22">22 July 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">FF45A182222B042EF9A8619EC3BE450A</idno>
					<idno type="DOI">10.3390/a15080255</idno>
					<note type="submission">Received: 1 June 2022 Accepted: 19 July 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T12:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_S6GaxWw">reinforcement learning (RL)</term>
					<term xml:id="_yWteNQr">online learning</term>
					<term xml:id="_qXPQDsH">mobile health</term>
					<term xml:id="_hSd97qS">algorithm design</term>
					<term xml:id="_BRFuCyx">algorithm evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_e5vRRGg"><p xml:id="_jw6GbZu"><s xml:id="_NAuDzzw">Online reinforcement learning (RL) algorithms are increasingly used to personalize digital interventions in the fields of mobile health and online education.</s><s xml:id="_tgPcswQ">Common challenges in designing and testing an RL algorithm in these settings include ensuring the RL algorithm can learn and run stably under real-time constraints, and accounting for the complexity of the environment, e.g., a lack of accurate mechanistic models for the user dynamics.</s><s xml:id="_z9JaZQm">To guide how one can tackle these challenges, we extend the PCS (predictability, computability, stability) framework, a data science framework that incorporates best practices from machine learning and statistics in supervised learning to the design of RL algorithms for the digital interventions setting.</s><s xml:id="_XkZ5AHs">Furthermore, we provide guidelines on how to design simulation environments, a crucial tool for evaluating RL candidate algorithms using the PCS framework.</s><s xml:id="_G8BQthb">We show how we used the PCS framework to design an RL algorithm for Oralytics, a mobile health study aiming to improve users' tooth-brushing behaviors through the personalized delivery of intervention messages.</s><s xml:id="_mqjBbB4">Oralytics will go into the field in late 2022.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_Eqj7kbn">Introduction</head><p xml:id="_gCYF2Vc"><s xml:id="_Gtq9rZV">There is growing interest in using online reinforcement learning (RL) to optimize the delivery of messages or other forms of prompts in digital interventions.</s><s xml:id="_peJ6WUE">In mobile health, RL algorithms have been used to increase the effectiveness of the content and timing of intervention messages designed to promote physical activity <ref type="bibr">[1,</ref><ref type="bibr" target="#b5">2]</ref> and to manage weight loss <ref type="bibr">[3]</ref>.</s><s xml:id="_Msmr2uK">In other areas, including the social sciences and education, RL algorithms are used to provide pretrial nudges to encourage court hearing attendance <ref type="bibr" target="#b7">[4]</ref>, to personalize math explanations <ref type="bibr">[5]</ref>, and to deliver quiz questions during lecture videos <ref type="bibr">[6]</ref>.</s><s xml:id="_wCNst4Q">Unlike games and work in some areas of robotics, digital intervention studies can be extremely costly to run.</s><s xml:id="_KGx97bY">Furthermore, when the study is a preregistered clinical trial, once initiated, the trial protocol (including any online algorithms) cannot be altered without jeopardizing trial validity.</s><s xml:id="_MSqBnph">Thus, design decisions are a "one-way door" <ref type="bibr" target="#b11">[7]</ref>; once we commit to a set of design decisions, they are irreversible for the duration of the trial.</s><s xml:id="_A8N7r3h">To prevent poor design decisions that could be detrimental to the effectiveness and the validity of study results, RL algorithms must undergo a thorough design and testing process before deployment.</s></p><p xml:id="_wMb3APe"><s xml:id="_zAubtPv">The development of an RL algorithm for digital interventions requires a multitude of design decisions.</s><s xml:id="_HV4mQ6a">These decisions include how best to accommodate the lack of mechanistic models for dynamic human responses to digital interventions and how to ensure the robustness of the algorithm to potentially nonstationary/non-Markovian outcome distributions.</s><s xml:id="_kdRgEVq">Furthermore, one must ensure not only that the RL algorithm learns and quickly optimizes interventions but also that the algorithm runs stably and autonomously online within constrained amounts of time.</s><s xml:id="_SmNTUGM">One must also ensure that the RL algorithm can obtain data in a timely manner.</s><s xml:id="_Htt3cCc">Time and budgetary considerations may restrict the complexity of the RL algorithm that can be implemented.</s><s xml:id="_W3myhsX">Furthermore, it is important to ensure the data collected by the RL algorithm can be used to inform future studies and address scientific, causal inference questions.</s><s xml:id="_q6vyTEq">Addressing these challenges in a reproducible, replicable manner is critical if RL algorithms are to play a role in optimizing digital interventions.</s><s xml:id="_9Gp7vZa">Therefore, we need a framework for making design decisions for RL algorithms intended to optimize digital interventions.</s></p><p xml:id="_dZ7xeRb"><s xml:id="_9ECEqzg">The primary contributions of this work are twofold:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1." xml:id="_vDGWdhB">Decision Times</head><p xml:id="_m2fY2Vp"><s xml:id="_8j9CD7k">These are the times, indexed by t, at which the RL algorithm may deliver a treatment (via a smart device such as a desktop computer, smartwatch, smartphone, smart speaker, wearable, etc.).</s><s xml:id="_QjGytuV">The cadence of the decision times (minute level, hourly, daily, etc.) depends on the type of digital intervention.</s><s xml:id="_NpubABB">For example, in Oralytics, we have two decision times per day, namely, one hour prior to the set morning and evening brushing windows specified by the user.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2." xml:id="_c3xm9YU">State</head><p xml:id="_h9bf7td"><s xml:id="_hhp9rqb">S i,t â R d represents the ith user's state at decision time t.</s><s xml:id="_weC2dAB">d is the number of features describing the user's state (e.g., current location, recent adherence to medication, current social setting, recent engagement with the intervention application, etc.).</s><s xml:id="_5XPjcCF">See Section 5.2 for the state definition for Oralytics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3." xml:id="_dBjjxV5">Action</head><p xml:id="_S3pte5E"><s xml:id="_uYsnnNB">A i,t â A represents the decision made by the RL algorithm for the ith user at decision time t.</s><s xml:id="_4QBf8Kb">Treatment actions in digital interventions frequently include the action of not delivering any treatment at time t.</s><s xml:id="_XTfEyrb">For Oralytics, the action space is A := {0, 1}, where A i,t = 1 represents sending the user an engagement message and A i,t = 0 represents not sending an engagement message.</s><s xml:id="_9UajSRv">See Section 5.1 for descriptions of the types of messages that can be sent in Oralytics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4." xml:id="_xzaDMfS">Reward</head><p xml:id="_sUzPGzb"><s xml:id="_SDB3VwW">R i,t â R is the reward for the ith user at decision time t observed after taking action A i,t .</s><s xml:id="_dMaMWRG">The definition of the reward depends on the type of digital intervention.</s><s xml:id="_eJBxm7V">Examples include successfully completing a math problem, taking a medication, and increasing physical activity.</s><s xml:id="_gAKw2yT">In Oralytics, the reward is the subsequent brushing duration; see Section 5.2 for further discussion of the reward in Oralytics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5." xml:id="_K67swf2">Online RL Algorithms</head><p xml:id="_Yf7HbxW"><s xml:id="_MGr6FQs">Generally, online RL algorithms are composed of two parts: (a) fitting a model of the user and (b) an action selection strategy.</s><s xml:id="_GDVT4wm">The simplest type of user model is a model for the reward function, E[R i,t |S i,t , A i,t ].</s><s xml:id="_r8wfz8K">In more general cases, a model for the sum of future rewards, conditional on the current state, S i,t , and action, A i,t , is also learned.</s><s xml:id="_xHcAGQz">The action selection strategy of the RL algorithm uses the user's current state S i,t , along with the learned user model and outputs the treatment action A i,t at each decision time t.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6." xml:id="_R7fqFc2">Update Times</head><p xml:id="_VP36RuQ"><s xml:id="_hBM766G">These are the times at which the RL algorithm is updated.</s><s xml:id="_vFuM4x5">Updating typically includes updating a model of the user (e.g., a model of the user's reward function).</s><s xml:id="_jZxUYeD">The RL algorithm updates using user i's current history of past states, actions, and rewards up to time t, denoted by H i,t-1 = {S i,s , A i,s , R i,s } t-1 s=1 .</s><s xml:id="_7V2JTUk">If the algorithm pools data across users, then the history of other users in the study, H j,t-1 for i = j, is used to update the model for user i.</s><s xml:id="_9BrNaEx">These updates can occur after each decision time or at longer time scales.</s><s xml:id="_wm47gdg">For example, in <ref type="bibr">[1]</ref>, the decision times are 5 times per day, but the update times are only nightly.</s><s xml:id="_H4tDfVY">For Oralytics, the update cadence is once a week.</s></p><p xml:id="_wBGHBae"><s xml:id="_TfHC5qD">An online RL algorithm should quickly learn which action to deliver in which states for each user.</s><s xml:id="_DtwQr98">One of the most widely used and simplest RL algorithms is a contextual bandit algorithm <ref type="bibr" target="#b15">[11]</ref><ref type="bibr" target="#b16">[12]</ref><ref type="bibr" target="#b17">[13]</ref>.</s><s xml:id="_RUbwBh4">As data accrues, a contextual bandit algorithm incrementally learns which action will lead to maximal reward in each state.</s><s xml:id="_6FZNzBR">The online algorithm sequentially updates its estimate of the reward function (the mean of the reward conditional on state and action) and selects actions.</s><s xml:id="_QUXTMwH">The performance of the algorithm is often measured by the sum of rewards-the faster the algorithm learns, the greater the sum.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_Hhtbf56">PCS Framework for Designing RL Algorithms for Digital Intervention Development</head><p xml:id="_dJWgjtT"><s xml:id="_VTrSDEg">The PCS Framework <ref type="bibr" target="#b12">[8]</ref> incorporates best practices from machine learning and applied statistics to provide a set of guidelines for assessing the quality of a prediction algorithm when used to address problems in real life.</s><s xml:id="_CdSYZB6">The goal is to enhance the scientific community's confidence in the prediction algorithm's performance in terms of predictability of results, computability in the implementation of the algorithm, and stability in the performance results of the learning algorithm across perturbations.</s><s xml:id="_GSkFhGY">PCS has been adopted and extended to other domains; see Section 4.3 for further discussion.</s></p><p xml:id="_uv6ZFek"><s xml:id="_5RqyWqx">As in the prediction setting, there are a variety of design decisions one needs to make before deploying an RL algorithm, e.g., choosing the model class to use to approximate the reward function.</s><s xml:id="_Jv3WNMp">While many of the original PCS principles can be used in the development and evaluation of RL algorithms, RL algorithm development also introduces new challenges for the PCS framework, particularly in the online digital intervention setting.</s><s xml:id="_XNGs5n5">First, the main task is not prediction, but rather in RL, the main goal is to select intervention actions so that average rewards across time are maximized for each user.</s><s xml:id="_35AV946">We call this the goal of personalization <ref type="bibr" target="#b18">[14]</ref>.</s><s xml:id="_TK93Kbm">We generalize the PCS framework to include an evaluation of the ability of an online RL algorithm to personalize.</s><s xml:id="_8kH3naa">Second, in digital intervention settings, it is important to evaluate the ability of the online RL algorithm to maximize rewards under realworld constraints.</s><s xml:id="_hnHt7tV">For example, there are often time constraints on computations, budgetary constraints on software engineering development, and constraints on the algorithm in terms of obtaining data in real time.</s><s xml:id="_d9BQC9Y">Furthermore, the algorithm must run stably online without constant human monitoring and adjustment.</s><s xml:id="_JyEcNTa">The current PCS framework does not provide evaluation tools that deal with the above needs.</s><s xml:id="_Zh4xy9b">We extend the PCS framework to the context of designing and evaluating online RL algorithms.</s><s xml:id="_XhjpAuZ">Our extended framework focuses on providing confidence that the online RL algorithm will lead to greater effectiveness under real-world constraints and with stability.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." xml:id="_GCEvbqt">Personalization (P)</head><p xml:id="_KYs79Sh"><s xml:id="_Spna7HG">The original PCS Framework uses predictability (P) to ensure that models used in data science have good predictive accuracy on both seen and unseen data.</s><s xml:id="_Qczevec">Predictive accuracy is a simple, commonly used metric for evaluating such models, but in some cases, multiple evaluation metrics or domain-specific metrics are more appropriate.</s><s xml:id="_MHUMTXB">In our setting, the main task is personalization.</s><s xml:id="_P3vTza4">Namely, the online RL algorithm should learn to select actions to maximize each user's average rewards.</s><s xml:id="_TUfsDZn">Instead of a predictive accuracy metric, we want a metric to validate the extent of personalization.</s><s xml:id="_zbMeEqx">For example, when choosing a metric to evaluate RL algorithms for multiple users, one may be interested not just in the average over the users' sums of rewards but in other metrics that capture the variation in the sum of rewards across users.</s><s xml:id="_3BVVtM5">Let N be the total number of users with T total decision times.</s><s xml:id="_zS6XPyX">We suggest the following metrics:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CdWmN7b">â¢</head><p xml:id="_mdk3Kvu"><s xml:id="_2YgDFTV">Average of Users' Average (Across Time) Rewards: This metric is the average of all N users' rewards averaged across all T decision times, defined as 1</s></p><formula xml:id="formula_0">N â N i=1 1 T â T t=1 R i,t .</formula><p xml:id="_krg4Vfh"><s xml:id="_YnG5vtQ">The metric serves as a global measure of the RL algorithm's performance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_w7wRVbB">â¢</head><p xml:id="_zuzpSHS"><s xml:id="_Hw5NwMD">The 25th Percentile of Users' Average (Across Time) Rewards: To compute this metric, first compute the average reward across time for each user, 1 T â T t=1 R i,t for each i = 1, 2, . . .</s><s xml:id="_3w52mef">, N; this metric is the lower 25th percentile of these average rewards across the N users.</s><s xml:id="_eKX2Ads">The metric shows how well an RL algorithm performs for the worst-off users, namely users in the lower quartile of average rewards across time.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ng2F28R">â¢</head><p xml:id="_cTUNrT9"><s xml:id="_jSt4qDc">Average Reward For Multiple Time Points: This metric is the average users' rewards across time for multiple time points t 0 = 1, 2, ..., T, defined as 1</s></p><formula xml:id="formula_1">N â N i=1 1 t 0 â t 0</formula><p xml:id="_mw2hcyU"><s xml:id="_FqknBsp">t=1 R i,t for each t 0 .</s><s xml:id="_EzkmQ6k">These metrics can be used to assess the speed at which the RL algorithm learns across weeks in the trial.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2." xml:id="_HWRaC9P">Computability (C)</head><p xml:id="_nuuYJmZ"><s xml:id="_WeQJmXV">Computability has to do with the efficiency and scalability of algorithms, decisions, and processes.</s><s xml:id="_HejTAjT">While the original PCS framework focused on the computability of training and evaluating models, we also consider computability to include the ability to implement the algorithm within the constraints of the study.</s><s xml:id="_GZYA3Xs">In the online RL setting, computability encompasses all issues related to ensuring that the RL algorithm can select actions and update in a timely manner while running online.</s><s xml:id="_wRYK2UD">The performance of the online RL algorithm must be evaluated under the constraints of the study; key RL algorithm design constraints that could arise include:</s></p><p xml:id="_WSmdq6C"><s xml:id="_DzudXC7">â¢ Timely Access to Reward and State Information: The investigators may have an ideal definition of the reward or state features for the algorithm; however, due to delays in communication between sensors, the digital application, and the cloud storage, the investigators' first choice may not be reliably available.</s><s xml:id="_uEuKxjR">Since RL algorithms for digital interventions must make decisions online, the development team must choose state features that will be reliably available to the algorithm at each decision time.</s><s xml:id="_Hg9Fhnd">Additionally, the team must also choose rewards that are reliably available to the algorithm at update times.</s><s xml:id="_zCJqntx">â¢ Engineering Budget: One should consider the engineering budget, supporting software needed, and time available to deliver a production-ready algorithm.</s><s xml:id="_NwYJUJ3">If there are significant constraints, a simpler algorithm may be preferred over a sophisticated one because it is easier to implement, test, and set up monitoring systems for.</s><s xml:id="_BQCUhrH">â¢ Off-Policy Evaluation and Causal Inference Considerations:</s></p><p xml:id="_zfPYkNJ"><s xml:id="_JY6kGgV">The investigative team often not only cares about the RL algorithm's ability to learn but also about being able to use data collected by the RL algorithm to answer scientific questions after the study is over.</s><s xml:id="_un2EydR">These scientific questions can include topics such as off-policy evaluation <ref type="bibr" target="#b19">[15,</ref><ref type="bibr" target="#b20">16]</ref> and causal inference <ref type="bibr" target="#b21">[17,</ref><ref type="bibr" target="#b22">18]</ref>.</s><s xml:id="_aP6q87T">Thus, the algorithm may be constrained to select actions probabilistically with probabilities that are bounded away from zero and one.</s><s xml:id="_QBPmhRm">This enhances the ability of investigators to use the resulting data to address scientific questions with sufficient power <ref type="bibr" target="#b23">[19]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3." xml:id="_7zx8DnD">Stability (S)</head><p xml:id="_qyk9vzg"><s xml:id="_eAmhKrm">Stability concerns how an RL algorithm's results change with minor perturbations and the documentation and reproducibility of results.</s><s xml:id="_xaqZRuc">In online RL, stability plays two roles.</s><s xml:id="_bCuxGs2">First, the RL algorithm must run stably and automatically without the need for constant human monitoring and adjustment.</s><s xml:id="_dXuj6Rd">This is particularly critical as users abandon digital interventions that have inconsistent functionality (unstable RL algorithm) <ref type="bibr" target="#b25">[20,</ref><ref type="bibr" target="#b26">21]</ref>.</s><s xml:id="_feEyeQK">Second, the RL algorithm should perform well across a variety of potential real-world environments.</s><s xml:id="_6zMjxsH">A critical tool in assessing stability to perturbations of the environment is the use of simulation test beds.</s><s xml:id="_6kT2ZpW">Test beds include a variety of plausible environmental variants, each of which encodes different concerns of the investigative team.</s><s xml:id="_FBYg47H">The following are challenging attributes of probable environments in digital intervention problems that one could design test beds for:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WRJYER7">â¢</head><p xml:id="_ujQZTUZ"><s xml:id="_8M8GkTj">User Heterogeneity: There is likely some amount of user heterogeneity in response to actions, even when users are in the same context.</s><s xml:id="_2Ac7rW2">User heterogeneity can be partially due to unobserved user traits (e.g., factors that are stable or change slowly over time, like family composition or personality type).</s><s xml:id="_WaBRkYq">The amount of between-user heterogeneity impacts whether an RL algorithm that pools data (partially or using clusters) across users to select actions will lead to improved rewards.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pBme8QK">â¢</head><p xml:id="_Q7uUkbB"><s xml:id="_JhMGQXx">Non-Stationarity: Unobserved factors common to all users such as societal changes (e.g., a new wave of the pandemic), and time-varying unobserved treatment burden (e.g., a user's response to a digital intervention may depend on how many days the user has experienced the intervention) may make the distribution of the reward appear to vary with time, i.e., non-stationary.</s></p><p xml:id="_f6vXMN9"><s xml:id="_kYcrTYG">â¢ High-Noise Environments: Digital interventions typically deliver treatments to users in highly noisy environments.</s><s xml:id="_T6uM7Zg">This is in part because digital interventions deliver treatments to users in daily life, where many unobserved factors (e.g., social context, mood, or stress) can affect a user's responsiveness to an intervention.</s><s xml:id="_Th9VxSG">If unobserved, these factors produce noise.</s><s xml:id="_tjyupmA">Moreover, the effect of digital prompts on a near-term reward tends to be small due to the nature of the intervention.</s><s xml:id="_VsxgVfy">Therefore, it is important to evaluate the algorithm's ability to personalize even in highly noisy, low signal-tonoise ratio environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4." xml:id="_A2VkuvT">Simulation Environments for PCS Evaluation</head><p xml:id="_XbUg8Da"><s xml:id="_6YncWGr">To utilize the PCS framework, we advocate for using a simulation environment for designing and evaluating RL algorithms.</s><s xml:id="_tkSEFKD">We aim to compare RL algorithm candidates under real-world constraints (computability).</s><s xml:id="_Xka7YDa">Thus, we build multiple variants of the simulation environment, each reflecting plausible user dynamics (i.e., state transitions and reward distributions) (stability).</s><s xml:id="_PGBebaC">We then simulate digital intervention studies for each simulation environment variant and RL algorithm candidate pairing.</s><s xml:id="_s6WWReB">Finally, we use multiple metrics to evaluate the performance of the RL algorithm candidates (personalization).</s></p><p xml:id="_7PgTBnW"><s xml:id="_MmeuJSK">In the case of digital interventions, there is often no mechanistic model or physical process for user behavioral dynamics, which makes it difficult to accurately model transitions (e.g., modeling a user's future level of physical activity as a function of their past physical activity, location, local weather).</s><s xml:id="_jaVCfmJ">Note that the goal of developing the simulators is not to conduct model-based RL <ref type="bibr" target="#b27">[22]</ref>.</s><s xml:id="_7qVNGAK">Rather, here, the simulators represent a variety of plausible environments to facilitate the evaluation of the performance of potential RL algorithms in terms of personalization, computability, and stability across these environments.</s><s xml:id="_Z8nH3VW">Existing data and domain expertise is most naturally used to construct the simulation environments.</s><s xml:id="_qQSFWZ3">However, as is the case for Oralytics, the previously collected data may be scarce, i.e., we have few data points per user.</s><s xml:id="_MvzHPsQ">Moreover, the data may only be partially informative, e.g., the data was collected under only a subset of the actions.</s><s xml:id="_S9zbvYY">Next, we provide guidelines for how to build an environment simulator in such challenging settings.</s></p><p xml:id="_Pz3VgcC"><s xml:id="_d7NHpUv">Base Environment Simulator: To have the best chance possible of accurately evaluating how well different RL candidates will perform, we recommend first building a base environment simulator that mimics the existing data to the greatest extent possible.</s><s xml:id="_7v8Psh7">This involves carefully choosing a set of time-varying features and reward-generating model class that will be expressive enough to model the true reward distribution well.</s><s xml:id="_Yg2tdX3">To check how well the simulated data generated by the model of the environment mimics the observed data, we recommend a variety of ways to compare distributions.</s><s xml:id="_fx3JDKd">This includes visual comparisons such as plotting histograms; comparing measures of the real data such as mean reward, between-user variance, and within-user variance to the same measures of the simulated data; and measuring how well the base model captured the variance in the data.</s><s xml:id="_g2wrybr">Examples of these checks done for Oralytics are in Appendix A. <ref type="bibr" target="#b7">4</ref>.</s></p><p xml:id="_fWrePJW"><s xml:id="_qsc6awd">Variant Environment Simulators: We recommend considering many variants or perturbed simulation environments to evaluate the stability of RL algorithms across multiple plausible environments.</s><s xml:id="_XybkyJg">These variants can be used to address the concerns of the investigative team.</s><s xml:id="_ycNN4Ka">For example, if the base simulator generates stationary rewards and the investigative team is concerned that the real reward distribution may not be stationary, a variant could incorporate nonstationarity into the environment dynamics.</s></p><p xml:id="_UpAfvnT"><s xml:id="_7ZVdNd8">If the previously collected data does not include particular actions, as was the case for Oralytics, we recommend consulting domain experts for a range of potential realistic effect sizes (differences in mean reward under the new action versus a baseline action).</s><s xml:id="_7xh4ZRm">For example, in Oralytics, we only have data under no intervention and do not have data on rewards under the intervention.</s><s xml:id="_dwfFGkT">Thus, using the input of the domain experts on the team, we imputed several plausible treatment effects (varying by certain state features and the amount of heterogeneity in treatment effects across users).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_FX55RuF">Related Works 4.1. Digital Intervention Case Studies</head><p xml:id="_U8t2Xp4"><s xml:id="_FB746HY">Liao et al. <ref type="bibr">[1]</ref> describes the development of an online RL algorithm for HeartSteps V2, a physical activity mobile health application.</s><s xml:id="_cVpFCgW">The authors highlight how their design decisions address specific challenges in designing RL algorithms by, for example, adjusting for longer-term effects of the current action and accommodating noisy data.</s><s xml:id="_TsawnMG">However, they do not provide general guidelines for making design decisions for RL algorithms in digital intervention development.</s></p><p xml:id="_Znc7JFT"><s xml:id="_SmRt4YP">Another related work is that of Figueroa et al. <ref type="bibr" target="#b28">[23]</ref>, which provides an in-depth case study of the design decisions, and the associated challenges and considerations, for an RL algorithm for text messaging in a physical activity mobile application serving patients with diabetes and depression.</s><s xml:id="_XphHJEs">This case study provides guidelines to others developing RL algorithms for mobile health applications.</s><s xml:id="_G9NT4SG">Specifically, the authors first categorize the challenges they faced into 3 major themes: (1) choosing a model for decision making, (2) data handling, and (3) weighing algorithm performance versus effectiveness in the real world.</s><s xml:id="_DcqjKsx">They describe how they dealt with each challenge in the design process of their RL algorithm.</s><s xml:id="_zQb53TS">In contrast, by expanding the PCS framework, this work introduces general guidelines for comprehensively evaluating RL algorithms.</s><s xml:id="_2dUbMC6">Moreover, we make recommendations for how to design a variety of simulation test beds even using only sparse and partially informative existing data, in service of PCS.</s><s xml:id="_zTmGJMw">The generality of the PCS framework makes it more widely applicable.</s><s xml:id="_XmGWszy">For example, Figueroa et al. <ref type="bibr" target="#b28">[23]</ref> has an existing dataset for all actions, which makes its recommendations less applicable to those designing algorithms with existing data only under a subset of actions.</s><s xml:id="_J7PU3a4">The PCS framework allows us to move beyond suggesting solutions to a specific set of challenges for a particular study by offering holistic guidelines for addressing challenges in developing simulation environments and evaluating algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2." xml:id="_37xp3ft">Simulation Environments in Reinforcement Learning</head><p xml:id="_zmRfepA"><s xml:id="_Va3GN4k">In RL, simulators (generative models) may be used to derive a policy from the generative model underlying the simulator (model-based learning).</s><s xml:id="_E9HauYt">Agarwal et al. <ref type="bibr" target="#b27">[22]</ref> uses simulation as an intermediate step to learn personalized policies in a data-sparse regime with heterogeneous users, where they only observe a single trajectory per user.</s><s xml:id="_cFamqGn">Wei et al. <ref type="bibr" target="#b29">[24]</ref> proposes a framework for simulating in a data-sparse setting by using imitation learning to better interpolate traffic trajectories in an autonomous driving setting.</s><s xml:id="_axsDuYr">In contrast, in PCS, the simulator is used as a crucial tool for using the framework to design, compare, and evaluate RL algorithm candidates for use in a particular problem setting.</s></p><p xml:id="_jc6Rv7G"><s xml:id="_W9hgTmD">There exist many resources aiming to improve the design and evaluation of RL algorithms through simulation; however, in contrast to this work, they do not provide guidelines for designing plausible simulation environments using existing data.</s><s xml:id="_Gf4fByZ">RecSim <ref type="bibr" target="#b30">[25]</ref> gives a general framework but does not advise on the quality of the environment nor on how to make critical design decisions such as reward construction, defining the state space, simulating unobserved actions, etc. MARS-Gym <ref type="bibr" target="#b31">[26]</ref> provides a full end-to-end pipeline process (data processing, model design, optimization, evaluation) and open-source code for a simulation environment for marketplace recommender systems.</s><s xml:id="_VNXDmv6">OpenAI Gym <ref type="bibr" target="#b32">[27]</ref> is a collection of benchmark environments in classical control, games, and robotics where the public can run and compare the performance of RL algorithms.</s></p><p xml:id="_zQytxE4"><s xml:id="_3HPAzG5">There are also a handful of papers that build simulation environment test beds using real data.</s><s xml:id="_Bgs36yn">Wang et al. <ref type="bibr" target="#b33">[28]</ref> evaluates their algorithm for promoting running activity with a simulation environment built using two datasets.</s><s xml:id="_JE6E64j">Singh et al. <ref type="bibr" target="#b34">[29]</ref> develops a simulation environment using movie recommendations to evaluate their safe RL approach.</s><s xml:id="_8EVYS9H">Korzepa et al. <ref type="bibr" target="#b35">[30]</ref> uses a simulation environment to guide the design of personalized algorithms that optimize hearing aid settings.</s><s xml:id="_AySXnwu">Hassouni et al. <ref type="bibr" target="#b36">[31,</ref><ref type="bibr" target="#b37">32]</ref> fits a realistic simulation environment using the U.S. timekeeping research project data.</s><s xml:id="_sDa4QJJ">Their simulation environment creates daily schedules of activities for each user (i.e., sleep, work, workout, etc.)</s></p><p xml:id="_9BRP3ca"><s xml:id="_4wtzGhz">where each user is one of many different user profiles (i.e., workaholic, athlete, retiree) for the task of improving physical activity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3." xml:id="_mQ8zykN">PCS Framework Extensions</head><p xml:id="_99KtVVb"><s xml:id="_zyzHsGt">The PCS framework has been extended to other learning domains such as causal inference <ref type="bibr" target="#b38">[33]</ref>, network analysis <ref type="bibr" target="#b39">[34]</ref>, and algorithm interpretability <ref type="bibr" target="#b40">[35]</ref>.</s><s xml:id="_mpwaqZB">Despite the variety of these tasks, they can all be framed as supervised learning problems in batch data settings that can be evaluated in terms of prediction accuracy on a hold-out dataset.</s><s xml:id="_maESvqt">PCS has not been extended to provide guidelines for developing an online decision-making algorithm.</s><s xml:id="_4CSFfKP">This extension is needed because of the additional considerations, discussed above, present in a real-world RL setting.</s><s xml:id="_CfEUvgt">Additionally, while these papers focus on evaluating how well a model accurately predicts the outcome on training and hold-out datasets, we extend the framework to evaluate how well an algorithm personalizes to each user.</s><s xml:id="_xbttRm8">Dwivedi et al. and Ward et al. <ref type="bibr" target="#b38">[33,</ref><ref type="bibr" target="#b39">34]</ref> implement the original computability principle by considering algorithm and process efficiency and scalability.</s><s xml:id="_bTM7HyC">Margot et al. <ref type="bibr" target="#b40">[35]</ref> provides a new principle, simplicity, which is based on the sum of the lengths of generated rules.</s><s xml:id="_XyFEQHb">We extend computability to include the constraints of the study.</s><s xml:id="_WxFdcgx">Finally, these papers consider the stability of results across different changes to the data (e.g., bootstrapping or cross-validation) or design decisions (e.g., choice of representation space or the embedding dimension).</s><s xml:id="_qxdEX5b">Our framework focuses on how stable an algorithm is in plausible real-world environments that may be complex (e.g., due to user heterogeneity, nonstationary, high noise).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_ctcm3AB">Case Study: Oral Health</head><p xml:id="_gkGkhfP"><s xml:id="_59j59R6">In this case study, we demonstrate the use of PCS principles in designing an RL algorithm for Oralytics.</s><s xml:id="_U3jArqK">Two main challenges are (1) we do not have timely access to many features and the reward is relatively noisy and (2) we have sparse, partially informative data to inform the construction of our simulation environment test bed.</s><s xml:id="_7DfTnnp">In addition, there are several study constraints.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rXRNDKK">1.</head><p xml:id="_MextqWf"><s xml:id="_zuWe97Y">Once the study is initiated, the trial protocol and algorithm cannot be altered without jeopardizing trial validity.</s><s xml:id="_jDTfQAn">2.</s></p><p xml:id="_BWAtVHE"><s xml:id="_hSjWR6B">We are using an online algorithm, so we may not have timely access to certain desirable state features or rewards.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rcqrxbS">3.</head><p xml:id="_Kd8yjaa"><s xml:id="_MpHJJXJ">We have a limited engineering budget.</s><s xml:id="_WgZd2DY"><ref type="bibr" target="#b7">4</ref>.</s></p><p xml:id="_dzb6HXd"><s xml:id="_rRYtqHT">We must answer post-study scientific questions that require causal inference or offpolicy evaluation.</s></p><p xml:id="_FBR43gH"><s xml:id="_AD7xvmY">We highlight how we handle these challenges by using the PCS framework, despite being in a highly constrained setting.</s><s xml:id="_RVS8fN6">The case study is organized as follows.</s><s xml:id="_2K25hq7">In Section 5.1, we give background context and motivation for the Oralytics study.</s><s xml:id="_vcZzwQn">In Section 5.2, we explain the Oralytics sequential decision-making problem.</s><s xml:id="_fWM8aNy">In Section 5.3, we describe our process for designing RL algorithm candidates that can stably learn despite having a severely constrained features space and noisy rewards.</s><s xml:id="_sFfDqAK">Finally, in Section 5.4, we describe how we designed the simulation environment variants to evaluate the RL algorithm candidates; throughout, we offer recommendations for designing realistic environment variants and for constructing such environments using data for only a subset of actions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1." xml:id="_CWHmbv9">Oralytics</head><p xml:id="_nBxEmt2"><s xml:id="_VDYSnVC">Oralytics is a digital intervention for improving oral health.</s><s xml:id="_pwzJctH">Each user is provided a commercially available electric toothbrush with integrated sensors and Bluetooth connectivity as well as the Oralytics mobile application for their smartphone.</s><s xml:id="_SxT5hAX">There are two decision times per day (prior to the user's morning and evening brushing windows) when a message may or may not be delivered to the user via their smartphone.</s><s xml:id="_9AHBF8u">The types of messages focus on winning a gift for oneself, winning a gift for one's favorite charity, feedback on prior brushing, and educational information.</s><s xml:id="_P5MMHkH">Once a message is delivered to the user, the app records it so that a user is highly unlikely to receive the same message twice.</s><s xml:id="_EM2P3X6">Oralytics will be implemented with approximately 70 users in a clinical trial where the participant duration is 10 weeks; this means each user has T = 140 decision times.</s><s xml:id="_NMhrbDa">The study duration is 2 years and the expected weekly incremental recruitment rate is around 4 users.</s><s xml:id="_XrF8bFx">The Oralytics mobile app will use an online RL algorithm to optimize message delivery (i.e., treatment actions) to maximize an oral health-related reward (see below).</s><s xml:id="_TURc5e2">To inform the RL algorithm design, we have access to data from a prior oral health study, ROBAS 2 <ref type="bibr" target="#b41">[36]</ref>, and input from experts in oral and behavioral health.</s><s xml:id="_VMhpT6C">The ROBAS 2 study used earlier versions of both the electric toothbrush and the Oralytics application to track the brushing behaviors of 32 users over 28 days.</s><s xml:id="_QnzeQTF">Importantly, in ROBAS 2, no intervention messages were sent to the users.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2." xml:id="_9d6wCU3">The Oralytics Sequential Decision-Making Problem</head><p xml:id="_qJzxURZ"><s xml:id="_r8R9EHK">We now discuss how we designed the state space and rewards for our RL problem in collaboration with domain experts and the software team while considering various constraints.</s><s xml:id="_JgEcKNx">These decisions must be communicated and agreed upon with the software development team because they build the systems that provide the RL algorithm with the necessary data at decision and update times and execute actions selected by the RL algorithm.</s></p><p xml:id="_2fGj63t"><s xml:id="_7UyECU6">1. Choice of Decision Times: We chose the decision times to be prior to each user's specified morning and evening brushing windows, as the scientific team thought this would be the best time to influence users' brushing behavior.</s></p><p xml:id="_YRsWYaG"><s xml:id="_eSxwfM3">2. Choice of Reward: The research team's first choice of reward was a measure of brushing quality derived from the toothbrush sensor data from each brushing episode.</s><s xml:id="_PwxYc4M">However, the brushing quality outcome is often not reliably obtainable because it requires (1) that the toothbrush dock be plugged in and ( <ref type="formula">2</ref>) that the user be standing within a few feet of the toothbrush dock when brushing their teeth.</s><s xml:id="_AMVS2G2">Users may fail to meet these two requirements for a variety of reasons, e.g., the user brushes their teeth in a shared bathroom where they cannot conveniently leave the dock plugged in.</s><s xml:id="_2rjaQJv">Thus, we selected brushing duration in seconds as the reward (personalization) since 120 s is the dentist-recommended brushing duration and brushing duration is a necessary factor in calculating the brushing quality score.</s><s xml:id="_Y3MpqYz">Additionally, brushing duration is expected to be reliably obtainable even when the user is far from the toothbrush dock when brushing (computability).</s><s xml:id="_9fpKNB8">Note that in Figure <ref type="figure" target="#fig_1">2</ref>, a small number of user-brushing episodes have durations over the recommended 120 s.</s><s xml:id="_AwhnHX6">Hence, we truncate the brushing time to avoid optimizing for overbrushing.</s><s xml:id="_kKkZcXA">Let D i,t denote the user's brushing duration.</s><s xml:id="_bwMbURP">The reward is defined as R i,t := min(D i,t , 180).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_uxWpT7f">Choice of State Features At Decision Time:</head><p xml:id="_7vqHGhy"><s xml:id="_KtMvnN8">To provide the best personalization, an RL algorithm ideally has access to as many relevant state features as possible to inform a decision, e.g., recent brushing, location, user's schedule, etc.</s><s xml:id="_zY9cqDB">However, our choice of the state space is constrained by the need to get features reliably before decision and update times, as well as by our limited engineering budget.</s><s xml:id="_zRbD525">For example, we originally wanted a feature for the evening decision time to be the morning's brushing outcome; however, this feature may not be accessible in a timely manner.</s><s xml:id="_ShuRGgu">This is because in order for the algorithm to receive the morning brushing data, the Oralytics smartphone app requires the user to open the app and we do not expect most users to reliably open the app after every morning brush time before the evening brushing window.</s><s xml:id="_P8X4zwc">Further discussion of our choice of decision time state features can be found in Appendix B.1.</s></p><p xml:id="_QPKfaTe"><s xml:id="_5ah9sR6">4. Choice of Algorithm Update Times: In our simulations, we update the algorithm weekly.</s><s xml:id="_YFNkeKr">In terms of speed of learning (at least in idealized settings), it is best to update the algorithm after each decision time.</s><s xml:id="_nGuyVQ3">However, due to computability considerations, we chose a slower update cadence.</s><s xml:id="_m6pvnFg">Specifically, for the Oralytics app, the consideration was that we can only update the policy used to select actions when the user opens the app.</s><s xml:id="_edA8Akb">If the user did not open the app for many days, we would be unable to update the app after each decision time.</s><s xml:id="_nUUvF6y">Users may well fail to open the app for a few days at a time, so we chose weekly updates.</s><s xml:id="_8zwyFzz">In the future, we will explore other update cadences as well, e.g., once a day.</s></p><p xml:id="_BZyQwZw"><s xml:id="_y7yccyH">0 100 200 300 Brushing Duration in Seconds 0% 5% 10% 15% 20% 25% 30% 35% 40% Percentage of Brushing Sessions The ROBAS 2 study had 32 users total and each user had 56 brushing windows (2 brushing windows per day for 28 days).</s><s xml:id="_aUns4Td">If a user did not brush during a brushing window, their brushing duration is recorded as zero seconds.</s><s xml:id="_J9GtYvV">Note in the figure above that across all users and brushing windows, about 40% of brushing sessions had no brushing, that is, a brushing duration of zero seconds.</s><s xml:id="_nADVtGn">The ROBAS 2 brushing durations are highly zero-inflated.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3." xml:id="_EYfhqVe">Designing the RL Algorithm Candidates</head><p xml:id="_HEkTVQF"><s xml:id="_GcxHpJb">Here, we discuss our use of the PCS framework to guide and evaluate the following design decisions for the RL algorithm candidates.</s><s xml:id="_xxq9nxa">There are some decisions that we have already made and other decisions that we encode as axes for our algorithm candidates to test in the simulation environment.</s><s xml:id="_Jhv8dmn">See Appendix B for further details regarding the RL algorithm candidates.</s></p><p xml:id="_bUwafB3"><s xml:id="_as5JnjR">1. Choice of using a Contextual Bandit Algorithm Framework: We understand that actions will likely affect a user's future states and rewards, e.g., sending an intervention message the previous day may affect how receptive a user is to an intervention message today.</s><s xml:id="_5zjKFyt">This suggests that an RL algorithm that models a full Markov decision process (MDP) may be more suitable than a contextual bandit algorithm.</s><s xml:id="_YbwW4R8">However, the highly noisy environment and the limited data to learn from (140 decision times per user total) make it difficult for the RL algorithm to accurately model state transitions.</s><s xml:id="_eFM6eqH">Due to errors in the state transition model, the estimates of the delayed effects of actions used in MDP-based RL algorithms can often be highly noisy or inaccurate.</s><s xml:id="_ekWVCEP">This issue is exacerbated by our severely constrained state space (i.e., we have few features and the features we get are relatively noisy).</s><s xml:id="_e3aT9az">As a result, an RL algorithm that fits a full MDP model may not learn much during the study, which could compromise personalization and offer a poor user experience.</s><s xml:id="_6SqAzBX">To mitigate these issues, we use contextual bandit algorithms, which fit a simpler model of the environment.</s><s xml:id="_3nFHnPM">Using a lower discount factor (a form of regularization) has been shown to lead to learning a better policy than using the true discount factor, especially in data-scarce settings <ref type="bibr" target="#b42">[37]</ref>.</s><s xml:id="_s7JZ3NH">Thus, a contextual bandit algorithm can be interpreted as an extreme form of this regularization where the discount factor is zero.</s><s xml:id="_8Z37H9S">Finally, contextual bandits are the simplest algorithm for sequential decision making (computability) and have been used to personalize digital interventions in a variety of areas <ref type="bibr">[1,</ref><ref type="bibr" target="#b5">2,</ref><ref type="bibr">5,</ref><ref type="bibr" target="#b28">23]</ref>.</s></p><p xml:id="_VrtYVXJ"><s xml:id="_cBJnMQd">2. Choice of a Bayesian Framework: We consider contextual bandit algorithms that use a Bayesian framework, specifically posterior (Thompson) sampling algorithms <ref type="bibr" target="#b43">[38]</ref>.</s><s xml:id="_GZpMcR3">Posterior sampling involves placing a prior on the parameters of the reward approximating function and updating the posterior distribution of the reward function parameters at each algorithm update time.</s><s xml:id="_nPsDpap">This allows us to incorporate prior data and domain expertise into the initialization of the algorithm parameters.</s><s xml:id="_Kz6GHYp">In addition, Thompson sampling algorithms are stochastic (action selections are a not deterministic function of the data), which better facilitate causal inference analyses later on using the data collected in the study.</s></p><p xml:id="_P3Fhe7W"><s xml:id="_dRDzVU9">3. Choice of Constrained Action Selection Probabilities: We constrain the action selection probabilities to be bounded away from zero and one in order to facilitate offpolicy and causal inference analyses once the study is over (computability).</s><s xml:id="_MYxvU3s">With help from the domain experts, we decided to constrain the action selection probabilities of the algorithm to be in the interval [0.35, 0.75].</s></p><p xml:id="_8sJXsRY"><s xml:id="_dsua3xc">The following are decisions we will test using the simulation environment.</s><s xml:id="_xcSE7EY">4. Choice of the Reward Approximating Function: An important decision in designing the contextual bandit algorithm is how to approximate the reward function.</s><s xml:id="_X3v8mzq">We consider two types of approximations, a Bayesian linear regression model (BLR) and a Bayesian zero-inflated Poisson regression model (ZIP), which are both relatively simple, well studied, and well understood.</s><s xml:id="_B2tTTre">For BLR, we implement action centering in the linear model <ref type="bibr">[1]</ref>.</s><s xml:id="_Bnd6Vhg">The linear model for the reward function is easily interpretable by domain experts and allows them to critique and inform the model.</s><s xml:id="_t8Uut6W">We consider the ZIP because of the zero-inflated nature of brushing durations in our existing dataset ROBAS 2; see Figure <ref type="figure" target="#fig_1">2</ref>. We expect the ZIP to provide a better fit to the reward function by the contextual bandit and thus lead to increased average rewards.</s><s xml:id="_AmdYxRb">Formal specifications for BLR and ZIP as reward functions can be found in Appendix B.2.1 and Appendix B.2.2, respectively.</s></p><p xml:id="_Yfx6XeK"><s xml:id="_EwMvYNA">To perform posterior sampling, both the BLR and ZIP models are Bayesian with uninformative priors.</s><s xml:id="_eTb9G7w">From the perspective of computability and stability, the posterior for the BLR has a closed form, which makes it easier to write software that performs efficient and stable updates.</s><s xml:id="_nN3VCbC">In contrast, for the ZIP, the posterior distribution must be approximated and the approach used to approximate the posterior is another aspect of the algorithm design that the scientific team needs to consider.</s><s xml:id="_tHQGEsJ">See Appendix C for further discussion on how to update the RL algorithm candidates.</s></p><p xml:id="_2W8sMcK"><s xml:id="_VcYZPJN">5. Choice of Cluster Size: We consider clustering users with cluster sizes K = 1 (no pooling), K = 4 (partial pooling), and K = N = 72 (full pooling) to determine whether clustering in our setting will lead to higher sums of rewards (personalization).</s><s xml:id="_jT9Uqvv">Note that 72 is the approximate expected sample size for the Oralytics study.</s><s xml:id="_8mJyRTM">Clustering-based algorithms pool data from multiple users to learn an algorithm per cluster (i.e., at update times, the algorithm uses H i,t-1 for all users i in the same cluster, and at decision times, the same algorithm is used to select actions for all users in the cluster).</s><s xml:id="_TUC46bW">Clustering-based algorithms have been empirically shown to perform well when users within a cluster are similar <ref type="bibr" target="#b44">[39,</ref><ref type="bibr" target="#b45">40]</ref>.</s><s xml:id="_y8ZXPPB">In addition, we believe that clustering will facilitate learning within environments that have noisy within-user rewards <ref type="bibr" target="#b46">[41,</ref><ref type="bibr" target="#b47">42]</ref>.</s><s xml:id="_Zg3tRfg">There is a trade-off between no pooling and full pooling.</s><s xml:id="_FKB3Y7W">No pooling may learn a policy more specific to the user later on in the study but may not learn as well earlier in the study when there is not a lot of data for that user.</s><s xml:id="_5KdTAP9">Full pooling may learn well earlier in the study because it can take advantage of all users' data but may not personalize as well as a no-pooling algorithm, especially if users are heterogeneous.</s><s xml:id="_XkZ23A8">We consider K = 4 for the balance partial pooling offers between the advantages and disadvantages between no pooling and full pooling.</s><s xml:id="_HsJhrj2">Moreover, four is the study's expected weekly recruitment rate and the update cadence is also weekly.</s><s xml:id="_zD6W4JH">We consider the two extremes and partial pooling as a way to explore this trade-off.</s><s xml:id="_8EMY9rR">A further discussion on choices of cluster size can be found in Appendix B.3.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4." xml:id="_RZvPU3N">Designing the Simulation Environment</head><p xml:id="_GSkMxks"><s xml:id="_mjfPhVX">We build a simulator that considers multiple variants for the environment, each encoding a concern by the research team.</s><s xml:id="_4MYjSb2">The simulator allows us to evaluate the stability of results for each RL algorithm across the environmental variants (stability).</s></p><p xml:id="_Md7HzSg"><s xml:id="_7q96CpQ">Fitting Base Models: Recall that the ROBAS 2 study did not involve intervention messages.</s><s xml:id="_mDpzPWK">However, we can still use the ROBAS 2 dataset to fit the base model for the simulation environment, i.e., a model for the reward (brushing duration in seconds) under no action.</s><s xml:id="_RpCnQD2">Two main approaches for fitting zero-inflated data are the zero-inflated model and the hurdle model <ref type="bibr" target="#b48">[43]</ref>.</s><s xml:id="_ebQj8Nv">Both the zero-inflated model and the hurdle model have (i) a Bernoulli component and (ii) a nonzero component.</s><s xml:id="_ufVxUTk">The zero-inflated model's Bernoulli component is latent and represents the user's intention to brush, while the hurdle model's Bernoulli component is observed and represents whether the user brushed or not.</s><s xml:id="_Bn4VeTb">Therefore, the zero-inflated model's nonzero component models the user's brushing duration when the user intends to brush, and the hurdle model's nonzero component models the user's brushing duration conditional on whether the user brushed or not.</s><s xml:id="_6QCEmcH">Throughout the model fitting process, we performed various checks on the quality of the model to determine whether the fitted model was sufficient (Appendix A.4).</s><s xml:id="_EzeP8r3">This included checking whether the percentage of zero brush times simulated by our model was comparable to that of the original ROBAS 2 dataset.</s><s xml:id="_hQkemrp">Additionally, we checked whether the model accurately captured the mean and variance of the nonzero brushing durations across users.</s></p><p xml:id="_NNRQJQd"><s xml:id="_X6QM9m7">The first approach we took was to choose one model class (zero-inflated Poisson) and fit a single population-level model for all users in the ROBAS 2 study.</s><s xml:id="_zqWJzhu">However, a single population-level model was insufficient for fitting all users due to the high level of user heterogeneity (i.e., the between-user and within-user variance of the simulated brushing durations from the fitted model was smaller than the between-user and withinuser variance of brushing durations in the ROBAS 2 data).</s><s xml:id="_rBJnnSX">Thus, next, we decided to maintain one model class, but fit one model per user for all users.</s><s xml:id="_KxCyb8F">However, when we fit a zero-inflated Poisson to each user, we found that the model provided an adequate fit for some users but not for users who showed more variability in their brushing durations.</s><s xml:id="_2K7wtWp">The within-user variance simulated rewards from the model fit on those users was still lower than the within-user variance of the ROBAS 2 user data used to fit the model.</s><s xml:id="_gDzn8pY">Therefore, we considered a hurdle model <ref type="bibr" target="#b48">[43]</ref> because it is more flexible than the zero-inflated Poisson.</s><s xml:id="_6hBFSNp">For Poisson distributions, the mean and variance are equal, whereas the hurdle model does not conflate the mean and variance.</s></p><p xml:id="_x3Z4KcX"><s xml:id="_7AaTAnZ">Ultimately, for each user, we considered three model classes: (1) a zero-inflated Poisson, (2) a hurdle model with a square root transform, and (3) a hurdle model with a log transform, and chose one of these model classes for each user (Appendix A.2). Specifically, to select the model class for user i, we fit all three model classes using each user's data from ROBAS 2.</s><s xml:id="_WNeMJMg">Then, we chose the model class that had the lowest root mean squared error (RMSE) (Appendix A.3). Additionally, along with the base model that generates stationary rewards, we include an environmental variant with a nonstationary reward function; here, "day in study" is used as a feature in the environment's reward generating model (Appendix A.1).</s></p><p xml:id="_jpW4frd"><s xml:id="_q4Z8ugk">Imputing Treatment Effect Sizes: To construct a model of rewards for when an intervention message is sent (a case for which we have no data), we impute plausible treatment effects with the interdisciplinary team and modify the fitted base model with these effects.</s><s xml:id="_9yMWWet">Specifically, we impute treatment effects on the Bernoulli component and the nonzero component.</s><s xml:id="_6rTfPeE">We impute both types of treatment effects because the investigative team's intervention messages were developed to encourage users to brush more frequently and to brush for the recommended duration.</s><s xml:id="_fw4pwKX">Furthermore, because the research team believes that the users may respond differently to the engagement messages depending on the context and depending on the user, we included context-aware, population-level, and user-heterogeneous effects of the engagement messages as environmental variants (Appendix A.5).</s></p><p xml:id="_hGfpBag"><s xml:id="_c2VCc8X">We use the following guidelines to guide the design of the effect sizes:</s></p><p xml:id="_3uyWkbr"><s xml:id="_ebygb6f">1.</s></p><p xml:id="_yVxdhpn"><s xml:id="_5drTTXR">In general, for mobile health digital interventions, we expect the effect (magnitude of weight) of actions to be smaller than (or on the order of) the effect for baseline features, which include time of day and the user's previous day brushing duration (all features are specified in Appendix A.1).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZWEh3dc">2.</head><p xml:id="_mWfNWwh"><s xml:id="_Ee42dtV">The variance in treatment effects (weights representing the effect of actions) across users should be on the order of the variance in the effect of features across users (i.e., variance in parameters of fitted user-specific models).</s></p><p xml:id="_dSfpapj"><s xml:id="_4Grbr6y">Following guideline 1 above, to set the population level effect size, we take the absolute value of the weights (excluding that for the intercept term) of the base models fitted for each ROBAS 2 user and the average across users and features (e.g., the average absolute value of weight for time of day and previous day brushing duration).</s><s xml:id="_3havFEc">For the heterogeneous (user-specific) effect sizes, for each user, we draw a value from a normal centered at the population effect sizes.</s><s xml:id="_MGKGE7Q">Following guideline 2, the variance of the normal distributions is found by again taking the absolute value of the weights of the base models fitted for each user, averaging the weights across features, and taking the empirical variance across users.</s><s xml:id="_z95dxcp">In total, there are eight environment variants, which are summarized in Table <ref type="table" target="#tab_1">1</ref>.</s><s xml:id="_96uVxaC">See Appendix A for further details regarding the development of the simulation environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_WjdhXHt">Experiment and Results</head><p xml:id="_bfNUaHV"><s xml:id="_6mca4d6">We evaluate the RL algorithm candidates in each of the environment variants (stability).</s><s xml:id="_xJYjxA5">Specifically, the RL algorithm candidates will be comprised of a posterior sampling algorithm with two different reward models: (i) a Bayesian linear regression model (BLR) and (ii) a zero-inflated Poisson regression model (ZIP); see Appendices B and C for more discussion of these algorithms.</s><s xml:id="_57wqdEr">Additionally, for each of these two reward approximating functions, we will consider different cluster sizes (with k = 1, 4, N users).</s><s xml:id="_GmSff3p">Each cluster will have one RL algorithm instantiation per cluster (no data shared across clusters).</s><s xml:id="_AQTadWc">We cluster users by their entry date into the study (e.g., the first k users are in the first cluster, the next k users are in the second cluster, and so on).</s><s xml:id="_b2uWXzV">Since for the real Oralytics study we will incrementally recruit users into the study at a rate of about four users per week, for our experiments, we also simulate four users entering the study every week.</s><s xml:id="_5kVbyeH">To simulate a study, we draw N = 72 users (approximately the expected sample size for the Oralytics study) with replacement and cluster them by their entry date into the simulated study.</s><s xml:id="_wMARNbz">The algorithms for each cluster are updated weekly with the first update taking place after one week (at decision time t = 14 for each cluster).</s></p><p xml:id="_MSfhRUD"><s xml:id="_ybbDUtK">To evaluate personalization, we use the following metrics to compare algorithms: average rewards (average across users and time) and the 25th percentile of average rewards (averaged over time) across users.</s><s xml:id="_GTRFhbp">The purpose of looking at the 25th percentile of average rewards across users is to evaluate how well the algorithms perform on the worst off users (i.e., users who have a lower average reward than the average user).</s><s xml:id="_FM4DD29">We ran 100 Monte Carlo trials for each environmental variant and algorithm candidate pairing.</s><s xml:id="_rGYzPBN">Table <ref type="table">2</ref> shows the average and the 25th percentile of users' average (across time) rewards.</s><s xml:id="_UEpNkym">Figure <ref type="figure" target="#fig_2">3</ref> shows the average reward over time.</s></p><p xml:id="_xV8mThr"><s xml:id="_AscpupM">We highlight the following takeaways from our experiments:</s></p><p xml:id="_XZ3as5A"><s xml:id="_VNU3HtQ">1.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_5M9kDuD">BLR vs. ZIP:</head><p xml:id="_XQk58xs"><s xml:id="_ZK7hSzQ">We prefer BLR to ZIP.</s><s xml:id="_qjRprgQ">BLR with cluster size k = N results in higher user rewards than all other RL algorithm candidates in all environments in terms of average reward and 25th percentile reward (Table <ref type="table">2</ref>) and for average reward across all user decision times (Figure <ref type="figure" target="#fig_2">3</ref>).</s><s xml:id="_e6bAY2z">It is interesting to note that BLR with cluster size k = 4 performs comparably to ZIP for all cluster sizes k (Table <ref type="table">2</ref>, Figure <ref type="figure" target="#fig_2">3</ref>).</s><s xml:id="_bJE6by7">Originally, we hypothesized that ZIP would perform better than BLR because the ZIP-based algorithms can better model the zero-inflated nature of the rewards.</s><s xml:id="_hHub9fP">We believe that the ZIP-based algorithms suffered in performance because they require fitting more parameters and thus require more data to learn effectively.</s><s xml:id="_DX7CYzb">On the other hand, the BLR model trades off bias and variance more effectively in our data-sparse settings.</s></p><p xml:id="_UujSxGJ"><s xml:id="_hr9wxcJ">Beyond considerations of their ability to personalize, we also prefer the BLR-based RL algorithms because they have an easy-to-compute closed-form posterior update (computability and stability).</s><s xml:id="_JT6ZK7v">The ZIP-based algorithms involve using approximate posterior sampling, which is more computationally intensive and numerically unstable.</s><s xml:id="_zKByjP8">In addition, BLR with action centering is robust, namely, it is guaranteed to be unbiased even when the baseline reward model is incorrect <ref type="bibr">[1]</ref>.</s><s xml:id="_ywFeThk">BLR with action centering specifically does not require the knowledge of the baseline features at decision time (See Appendix C.1.1).</s><s xml:id="_SvhJWxD">This means that baseline features only need to be available at update time and we can incorporate more features that were not available in real time at the decision time.</s></p><p xml:id="_r6CGsSE"><s xml:id="_hH9keRM">Table <ref type="table">2</ref>. Average and 25th Percentile Rewards.</s><s xml:id="_WVQE6AY">Average and 25th percentile rewards are defined in Section 3.1.</s><s xml:id="_QtfApUG">The naming convention for environment variants is found in Table <ref type="table" target="#tab_1">1</ref>.</s><s xml:id="_FnqnvWm">"k" refers to the cluster size.</s><s xml:id="_fYT3bza">Average rewards are averaged across time, users, and 100 trials.</s><s xml:id="_aeWcMD2">For the 25th percentile rewards, we average rewards for each user across time, find the lower 25th percentile across N = 72 users, and then average that across 100 trials.</s><s xml:id="_2J7VxZU">The value in the parenthesis is the standard error of the mean.</s><s xml:id="_4ke9cVH">The best performing algorithm candidate in each environment variant is bolded.</s><s xml:id="_WFV9G8e">BLR (k = N) performs better than other algorithm candidates across all simulated environments.</s><s xml:id="_yMbKZN8">Notice that the average rewards are lower than the 120-s dentist-recommended brushing duration.</s><s xml:id="_z44SsEm">This is because of the zero-inflated nature of our setting (i.e., the user does not brush).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_nEVqwcw">RL Algorithm Candidates</head><note type="other" xml:id="_y4SHpnD">Average</note><p xml:id="_NXqURn8"><s xml:id="_vvaaenB">Rewards RL Algorithm S_Het NS_Het S_Pop NS_Pop ZIP k = 1 100.038</s><s xml:id="_WuRMPvq">(0.597) 102.566 (0.526) 107.184 (0.626) 109.379</s><s xml:id="_HctZMFY">(0.552) ZIP k = 4 100.463</s><s xml:id="_feuJqv8">(0.586) 103.035 (0.539) 108.217 (0.609) 110.242 (0.562) ZIP k = N 100.791</s><s xml:id="_cpUKCkB">(0.596) 103.391 (0.546) 108.410 (0.617) 110.542 (0.554) BLR k = 1 97.196 (0.585) 99.691 (0.527) 103.692 (0.615) 105.590 (0.546) BLR k = 4 99.772 (0.590) 102.310 (0.547) 107.568 (0.619) 109.454</s><s xml:id="_VHmD3uw">(0.547) BLR k = N 101.267</s><s xml:id="_Jr6Jt5w">(0.590) 104.024 (0.542) 108.974 (0.610) 111.201 (0.546) 25th Percentile Rewards RL Algorithm S_Het NS_Het S_Pop NS_Pop ZIP k = 1 67.907 (1.150) 73.830 (0.403) 74.898 (1.016) 78.651 (0.556) ZIP k = 4 68.865</s><s xml:id="_sfBzghm">(1.067) 73.836 (0.464) 75.933 (1.114) 80.413 (0.629) ZIP k = N 69.448 (1.201) 74.580 (0.475) 76.312 (1.122) 80.424 (0.648) BLR k = 1 65.600 (1.139) 70.703 (0.457) 70.915 (1.024) 74.782 (0.596) BLR k = 4 68.045</s><s xml:id="_f2h3yfw">(1.122) 73.322 (0.505) 75.766 (1.097) 79.809 (0.622) BLR k = N 69.757 (1.171) 75.393 (0.427) 77.272 (1.096) 81.675 (0.583) ZIP (k = 1) ZIP (k = 4) ZIP (k = N) BLR (k = 1) BLR (k = 4) BLR (k = N) 20 40 60 80 100 120 140 User Decision Times 90 92 94 96 98 100 102 Average Reward</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Nsc8FKQ">2.</head><p xml:id="_k9wzga2"><s xml:id="_bef4mZZ">Cluster Size: RL algorithms with larger cluster sizes k perform better overall, especially for simulation environments with population-level treatment effects (rather than heterogeneous treatment effects).</s><s xml:id="_TfB9pwJ">At first glance, one might think that algorithms with smaller cluster sizes may perform better because they can learn more personalized policies for users (especially in environments with heterogeneous treatment effects).</s><s xml:id="_8K3Rp8Y">Interestingly, though, the algorithms with larger cluster sizes performed better across all environments in terms of average reward (average across users and time) and the 25th-percentile of the average reward (average over time) across users (Table <ref type="table">2</ref>); this means that the RL algorithm candidates with larger cluster sizes performed better for both the average user and for the worst off users.</s><s xml:id="_AFTT4vQ">The better performance of algorithms with larger cluster sizes is likely due to their ability to reduce noise and learn faster by leveraging the data of multiple users to learn.</s><s xml:id="_AsDjaWy">Even though the algorithms with larger cluster sizes are less able to model and learn the heterogeneity across users, this is outweighed by the immense benefit of sharing data across users to learn faster and reduce noise.</s></p><p xml:id="_HBBhgpa"><s xml:id="_kswRPx8">There are some limitations to these experiments.</s><s xml:id="_VznhSE9">Fixed Reward Noise Variance for BLR: The BLR algorithm includes a noise variance hyperparameter (Î· 2 in Equation (A3)).</s><s xml:id="_gUcXjgc">In our experiments, we set Î· 2 to the reward variance observed in the ROBAS 2 data set.</s><s xml:id="_AkkpKjC">Assuming that Î· 2 is known is unrealistic; in the future, we plan to learn Î· 2 along with other BLR algorithm parameters in the real study.</s><s xml:id="_wptHKuR">The known value of Î· 2 could be a reason that BLR performed comparably to ZIP.</s></p><p xml:id="_DMd38wz"><s xml:id="_qcNdvRF">More Distinct and Complex Simulation Environments: We may not be looking widely enough across environment variants to find settings where these algorithms perform differently.</s><s xml:id="_4qa2MDG">With sufficient data per user in a highly heterogeneous user environment, we expect cluster size k = 1 to do the best.</s><s xml:id="_EQqY5a2">In future work, we aim to add simulation environments with greater heterogeneity and less noise to see if large cluster sizes still perform well, and we aim to create more complex simulation environment variants that are more distinct (e.g., environments where users may differ by heterogeneous demographic features like age and gender).</s><s xml:id="_BMY7GGn">Additionally, we want to impute state features of interest in the real study that were not present in the data set, such as phone engagement.</s></p><p xml:id="_CkmbJ3y"><s xml:id="_gXVbNeF">Additional RL Algorithm Candidate Considerations.</s><s xml:id="_R5T8r9h">We also aim to consider other axes for algorithm candidates such as algorithms with other update cadences (e.g., every night or biweekly) and algorithms with an informative prior.</s><s xml:id="_YjRxRAj">In initial simulations using algorithms with informative priors, we found that since the same (limited amount) of ROBAS 2 data was used to build both the simulation environment and the prior, the algorithms did not need to learn much to perform well.</s><s xml:id="_pGA96mb">An open question is how to develop both simulation environments and informative priors in a realistic way using a limited amount of data.</s><s xml:id="_ejquMCZ">Finally, we will also explore additional design decisions such as how to carefully design the feature space for the RL algorithm.</s></p><p xml:id="_BNzYnXC"><s xml:id="_wUNQCDn">These investigations will determine the final algorithm that goes into the actual study.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7." xml:id="_h2Rs7bh">Discussion and Future Work</head><p xml:id="_crQ23vn"><s xml:id="_jBfmbyr">In this paper, we present the first extension of the PCS framework for designing RL algorithms in digital intervention settings.</s><s xml:id="_eQJDHZ5">The case study demonstrates how to use the PCS framework to make design decisions and highlights our ongoing work in designing the Oralytics RL algorithm.</s><s xml:id="_wpk5Swq">This work helps fellow researchers understand and balance the benefits and drawbacks of certain aspects of the RL algorithm they use for their digital intervention studies.</s><s xml:id="_rmE9g7Z">We consider three model classes: (1) a zero-inflated Poisson, (2) a hurdle model with a square root transform, and (3) a hurdle model with a log transform, and choose one out of these three model classes for each user.</s><s xml:id="_a5Guazk">We define these three model classes below.</s><s xml:id="_TXJZXsk">Additionally, below g(S) is the baseline feature vector of the current state defined in Appendix A.1, w i,b , w i,p , w i,Âµ are user-specific weight vectors, Ï 2 i,u is the user-specific variance for the normal component, and sigmoid(x) = 1 1+e -x is the sigmoid function.</s></p><p xml:id="_zh439Bk"><s xml:id="_kkgUBqV">( Note that the nonzero component of this model, Y 2 , can also be represented as a constant times a noncentral chi-squared, where the noncentrality parameter is the square of the mean of the normal distribution.</s></p><p xml:id="_Y9GquTB"><s xml:id="_DG2wBnV">(3) Hurdle Model with Log Transform for Brushing Duration</s></p><formula xml:id="formula_3">Z â¼ Bernoulli 1 -Ï g(S) T w i,b Y â¼ Lognormal g(S) T w i,Âµ , Ï 2 i,u</formula><p xml:id="_pusBh8n"><s xml:id="_gFG5tRb">Brushing Duration in Seconds : D = ZY Since we want to simulate brushing duration in seconds, we also round outputs of the hurdle models to the nearest whole integer.</s><s xml:id="_Mtgq4qJ">Notice that the zero-inflated Poisson model is a mixture model with a latent state.</s><s xml:id="_99qwzKW">The Bernoulli draw Z is latent and represents the user's intention to brush, and the Poisson models the user's brushing duration when they intend to brush (this is because the brush time can still be zero when the user intends to brush).</s><s xml:id="_xdwbqtB">On the other hand, the hurdle model provides a model for brushing duration conditional on whether the user brushed or not.</s><s xml:id="_GaPnXQQ">The Bernoulli draw Z in the hurdle model is observed.</s></p><p xml:id="_4WpFSdu"><s xml:id="_dJhH5dQ">Note that the hurdle model is used for the simulation environment only and not the RL algorithm.</s><s xml:id="_pu4kDaX">The hurdle model conditions on a collider (e.g., whether the person brushes their teeth), thus potentially leading to causal bias <ref type="bibr" target="#b49">[44,</ref><ref type="bibr" target="#b50">45]</ref>.</s><s xml:id="_jhnnvTr">For example, consider an unobserved cause U, intervention A, whether the user brushed or not Z, brushing duration D, and a directed acyclic graph with A â Z, U â Z, U â D, and Z â D. Then conditioning on collider Z of treatment opens a pathway from A to D through U <ref type="bibr" target="#b51">[46]</ref>.</s><s xml:id="_ZAmyAfe">Suppose in reality A only impacts whether the user brushes their teeth but not the duration.</s><s xml:id="_y8kY2k4">Then, if we condition on Z to evaluate the impact of A on D, we may erroneously learn that A impacts the duration of brushing.</s><s xml:id="_UqQ5sAf">This makes the hurdle unsuitable as a model for an RL algorithm that aims to learn causal effects.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CMsB7mv">Appendix A.3. Fitting the Environment Base Models</head><p xml:id="_UZ2g3nR"><s xml:id="_pz95XWj">We use ROBAS 2 data to fit the brushing duration model under action 0 (no message).</s><s xml:id="_sPDQa3p">For all model classes, we fit one model per user.</s><s xml:id="_GNkZa2v">All models were fit using MAP with a prior w i,b , w i,p , w i,Âµ â¼ N (0, I) as a form of regularization because we have sparse data for each user.</s><s xml:id="_VfzttUH">Weights were chosen by running random restarts and selecting the weights with the highest log posterior density.</s></p><p xml:id="_WcYEeC5"><s xml:id="_SVMU7hg">Fitting Hurdle Models: For fitting hurdle models for user i, we fit the Bernoulli component and the nonzero brushing duration component separately.</s><s xml:id="_FpNHECv">We use D i,t to denote the ith ROBAS 2 user's brushing duration in seconds at the t time point.</s><s xml:id="_yWBy7gE">Set Z i,t = 1 if the original observation D i,t &gt; 0 and 0 otherwise.</s><s xml:id="_NgXMvZG">We fit a model for this Bernoulli component.</s><s xml:id="_SqaZbB2">We then fit a model for the normal component to either the square root transform Y i,t = D i,t or to inverse-log transform Y i,t = exp(D i,t ) of the ith user's nonzero brushing duration.</s></p><p xml:id="_CCrBEdt"><s xml:id="_eSzjDGe">Fitting Zero-Inflated Poisson Models: For the zero-inflated Poisson model, we jointly fit parameters for both the Bernoulli and the Poisson components.</s><s xml:id="_aGqR5tQ">Since the brushing durations in the ROBAS 2 data were integer values, we did not have to transform the observation to fit the zero-inflated Poisson model.</s></p><p xml:id="_EzrrxjH"><s xml:id="_m9srkqh">The fitted parameters for the environment base models can be accessed at: <ref type="url" target="https://github.com/StatisticalReinforcementLearningLab/pcs-for-rl/tree/main/sim_env_data">https://  github.com/StatisticalReinforcementLearningLab/pcs-for-rl/tree/main/sim_env_data</ref></s><s xml:id="_CWvSH5H">(accessed on 1 June 2022).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_M6JHQBR">Selecting the Model Class for Each User</head><p xml:id="_4exBcbP"><s xml:id="_M4Q43JG">To select the model class for user i, we fit all three model classes using user i's data from ROBAS 2. We then chose the model class that had the RMSE.</s><s xml:id="_NvwKSUa">Namely, we choose the model class with the lowest L i , where:</s></p><formula xml:id="formula_4">L i := T â t=1 (D i,t -Ã[D i,t |S i,t ]) 2</formula><p xml:id="_d5kaY22"><s xml:id="_q4DBHfy">Recall that D i,t is the brush time in seconds for user i at decision time t.</s><s xml:id="_rZjCyV5">Definitions of Ã[D i,t |S i,t ] for each model class are specified below in Table <ref type="table" target="#tab_1">A1</ref>.</s></p><formula xml:id="formula_5">Table A1. Definitions of E[D i,t |S i,t ] for each model class. E[D i,t |S i,t ] is the mean of user model i fitted using data {(S i,t , D i,t )} T t=1 . Model Class E[D i,t |S i,t ] Zero-Inflated Poisson 1 -sigmoid g(S i,t ) T w i,b â¢ exp g(S i,t ) T w i,p Hurdle (Square Root) 1 -sigmoid g(S i,t ) T w i,b â¢ Ï 2 i,u + (g(S i,t ) T w i,Âµ ) 2 Hurdle (Log) 1 -sigmoid(g(S i,t ) T w i,b ) â¢ exp g(S i,t ) T w i,Âµ + Ï 2 i,u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_YpnQMvS">Average of Variances Within-User Brushing Durations:</head><p xml:id="_n7f52w4"><s xml:id="_DkFJbsJ">This metric measures the average amount of within-user variance.</s></p><p xml:id="_fCnFzjF"><s xml:id="_MymE4Fd">1</s></p><formula xml:id="formula_6">N N â i=1 Var {D i,t } T t=1</formula><p xml:id="_wG9uNkM"><s xml:id="_vf9cNm2">The base models slightly overestimate the proportion of missed brushing windows in the ROBAS 2 data set.</s><s xml:id="_UAjGte5">Our base models also slightly underestimate the average brushing duration.</s><s xml:id="_NTPd2CK">Our base models also for the most part slightly overestimate the between-user and within-user variance of rewards.</s></p><p xml:id="_p2Rfzwt"><s xml:id="_Q9uY47C">Table A4.</s><s xml:id="_GkpCQTU">Comparing Moments Between Base Models and ROBAS 2 Data Set.</s><s xml:id="_rDg3EEe">Above, we use BDs to abbreviate Brushing Durations.</s><s xml:id="_7VHZYvt">Values for the Stationary and Nonstationary base models are averaged across 100 trials.</s><s xml:id="_hFpXUvb">Metrics ROBAS 2 Stationary Non-Stationary Proportion of Missed Brushing Windows 0.376674 0.403114 0.397812 Average Nonzero BDs 137.768129 131.308445 134.676955</s><s xml:id="_A75SUVe">Variance of Nonzero BDs 2326.518304</s><s xml:id="_WZGpJAd">2392.955018</s><s xml:id="_V2C3Gad">2253.177853</s><s xml:id="_nay98Yg">Variance of Average User BDs 1415.920148</s><s xml:id="_z8bbgmG">1699.126897</s><s xml:id="_6wr6eUp">1399.615330</s><s xml:id="_nxjkZjv">Average of Variances of Within-User BDs 1160.723506</s><s xml:id="_QeUFXYu">1405.944459</s><s xml:id="_bs2YgjC">1473.239769</s><s xml:id="_CeKdcyh">Appendix A.4.2.</s><s xml:id="_yFnYwRz">Measuring If a Base Model Captures the Variance in the Data</s></p><p xml:id="_KGEuzjb"><s xml:id="_3kYpEST">We measure how well the fitted base models captured (1) whether or not the user brushed and (2) the variance of the brush time when the users did brush.</s><s xml:id="_35adfK5">To measure point <ref type="bibr">(1)</ref> for each user model i, we calculate the statistic:</s></p><formula xml:id="formula_7">U i := 1 T T â t=1 I[D i,</formula><p xml:id="_2NRasrV"><s xml:id="_mFKu8U4">t &gt; 0] -E[I[D i,t &gt; 0]|S i,t ] Var[I[D i,t &gt; 0]|S i,t ] 2 (A1) where E[I[D i,t &gt; 0]|S i,t ] = 1sigmoid(S T i,t w i,b ) and Var[I[D i,t &gt; 0]|S i,t ] = E[I[D i,t &gt; 0]|S i,t ] â¢ sigmoid(S T i,t w i,b ).</s><s xml:id="_Zym4XyY">To measure point (2) for each user model i, we calculate the statistic: U i := 1 â T t=1 I[D i,t &gt; 0] T â t=1 I[D i,t &gt; 0] D i,t -E[D i,t |S i,t , D i,t &gt; 0] Var[D i,t |S i,t , D i,t &gt; 0] 2 (A2) Definitions of E[D i,t |S i,t , D i,t &gt; 0] and Var[D i,t |S i,t , D i,t &gt; 0] for the nonzero component of each model class are specified in Table <ref type="table">A2</ref>.</s><s xml:id="_hceSTKQ">For a user model to capture the variance in the data, U i should be close to 1.</s><s xml:id="_kjVwgbr">We calculate the empirical mean U = 1 N â N i=1 U i and standard deviation Ï U = std(U i ), and the approximate 95% confidence interval is</s></p><formula xml:id="formula_8">U Â± 1.96 Ã Ï U â N</formula><p xml:id="_DU5p4Jx"><s xml:id="_eq37abB">. Results are in Table <ref type="table">A5</ref>.</s><s xml:id="_yVv3XtT">We can see that after computing the statistic for each user, the confidence interval is close to 1.</s><s xml:id="_56Bam4b">We understand that the confidence intervals do not contain 1, which implies that we are overestimating the amount of variance in I[D &gt; 0] and underestimating the amount of variance in D|D &gt; 0. In the future, we hope to improve upon this statistic by considering nonlinear components in our base models.</s></p><p xml:id="_SkfxxEp"><s xml:id="_BZudv6Y">Table A5.</s><s xml:id="_EZP3fB4">Statistic i for Capturing Variance in the Data.</s><s xml:id="_BuydmHZ">Values are rounded to the nearest 3 decimal places.</s><s xml:id="_JFYjt9j">Metric Stationary Non-Stationary Equation (A1) U 0.811 0.792 Equation (A1) Ï U 0.146 0.150 Equation (A1) Confidence Interval (0.760, 0.861) (0.739, 0.844) Equation (A2) U 3.579 3.493 Equation (A2) Ï U 4.861 4.876 Equation (A2) Confidence Interval (1.895, 5.263) (1.803, 5.182) 0.8 0.6 0.4 0.2 0.0 Sizes 0% 5% 10% 15% 20% 25% Percentage B of Hurdle effect sizes B = -0.473</s><s xml:id="_fbPc7PX">(a) 0.0 0.5 1.0 1.5 2.0 Effect Sizes 0% 2% 5% 8% 10% 12% 15% 18% Percentage N of Hurdle (Square Root) effect sizes N = 1.020</s><s xml:id="_jjpeeGb">(b) 0.2 0.0 0.2 0.4 0.6 Effect Sizes 0% 5% 10% 15% 20% 25% Percentage N of Hurdle (Log) effect sizes N = 0.193 (c) 0.8 0.6 0.4 0.2 Effect Sizes 0% 5% 10% 15% 20% 25% Percentage B of ZIP effect sizes B = -0.473</s><s xml:id="_kHCEDVb">(d) 0.0 0.2 0.4 0.6 Effect Sizes 0% 5% 10% 15% 20% 25% Percentage N of ZIP effect sizes N = 0.166 Recall that we have normal priors on Î¸ i where Î¸ i â¼ N (Âµ prior , Î£ prior ), where Âµ prior = 0 â R 3+3+4 and Î£ prior = diag(Ï 2 prior I 3 , Ï 2 prior I 3 , Ï 2 prior I 4 ).</s><s xml:id="_nwS3Vmw">The posterior distribution of the weights given current history H i,t-1 , p(Î¸ i |H i,t-1 ) is conjugate and is also normal.</s></p><formula xml:id="formula_9">Î¸ i |H i,</formula><p xml:id="_fFUmBjJ"><s xml:id="_JWS4rJg">t-1 â¼ N (Âµ posterior i,t-1 , Î£ posterior i,t-1 ) Î£ posterior i,t-1 = 1 Î· 2 Î¦ T i,1:t-1 Î¦ i,1:t-1 + Î£ -1 prior -1 Âµ posterior i,t-1 = Î£ posterior i,t-1 1 Î· 2 Î¦ T i,1:t-1 R i,1:t-1 + Î£ -1 prior Âµ prior Note that we fit Î· 2 to the ROBAS 2 dataset and fixed it for all of our experiments.</s><s xml:id="_eAnnwKd">For the real study, we are considering assigning a conjugate prior on Î· 2 and updating it at update times.</s><s xml:id="_gZvM8SE">Appendix C.1.2.</s><s xml:id="_svaNGvE">Zero-Inflated Poisson Regression Model</s></p><p xml:id="_rS9Qz98"><s xml:id="_7MSHfNA">For the zero-inflated Poisson regression model, the posterior distribution of the weights Î¸ i = {Î± i,b , Î² i,b , Î± i,p , Î² i,p } given data H i,t-1 , p(Î¸ i |H i,t-1 ) does not have a closed form.</s><s xml:id="_62UbEe7">Therefore, we use Metropolis-Hastings (MH) with a normal proposal distribution as an approximate posterior sampling method.</s></p><p xml:id="_3ZBpV9a"><s xml:id="_QfdhgyN">Posterior Density:</s></p><p xml:id="_8h5pQvf"><s xml:id="_8t2HubR">The log-likelihood of the zero-inflated Poisson regression model is: log f (R i,t |S i,t , A i,t ; Î¸ i ) = log((1p) + p exp(-Î»)) R = 0 log pÎ» + R log Î»log R! R = 1, 2, 3, ... where p = 1sigmoid m(S i,t ) T Î± i,b + A i,t â¢ f (S i,t ) T Î² i,b is the probability of the user intending to brush, and Î» = exp m(S i,t ) T Î± i,p + A i,t â¢ f (S i,t ) T Î² i,p is the expected Poisson count.</s></p><p xml:id="_5HVxfxd"><s xml:id="_ycdM4Hw">Therefore, the log posterior density is: log p(Î¸ i |H i,t-1 ) â N â n=1 log f (R i,t |S i,t , A i,t ; Î¸ i ) + log p(Î¸ i )</s></p><p xml:id="_tTsKGvB"><s xml:id="_TcsfDgc">Proposal Distribution:</s></p><p xml:id="_Nt9gQqy"><s xml:id="_EAbFzbm">We choose a normal distribution for our proposal distribution.</s><s xml:id="_JE6YKPf">At each step of MH, we propose a new sample given the old sample, Î¸ k prop â¼ N (Î¸ k old , Î³ 2 I), where Î¸ k denotes the kth value of Î¸.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ANxH6xX">Metropolis-Hastings Acceptance Ratio:</head><p xml:id="_TrYGccG"><s xml:id="_xjj4P3p">The Metropolis-Hastings acceptance ratio given a proposed sample Î¸ prop and an old sample Î¸ old is defined as: Î±(Î¸ prop , Î¸ old ) := min 1, p(Î¸ prop )/q(Î¸ prop |Î¸ old ) p(Î¸ old )/q(Î¸ old |Î¸ prop )</s></p><p xml:id="_sY99jZk"><s xml:id="_gpyjYTh">Since our proposal distribution is symmetric, the log acceptance ratio becomes: log Î±(Î¸ prop , Î¸ old ) := min(0, log p(Î¸ prop )log p(Î¸ old ))</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc><div><p xml:id="_wv5f9F9"><s xml:id="_GGyhMuq">Figure 2.Histogram of brushing durations in seconds for all user brushing sessions in ROBAS 2. The ROBAS 2 study had 32 users total and each user had 56 brushing windows (2 brushing windows per day for 28 days).</s><s xml:id="_XSsXTWV">If a user did not brush during a brushing window, their brushing duration is recorded as zero seconds.</s><s xml:id="_C5MjeQz">Note in the figure above that across all users and brushing windows, about 40% of brushing sessions had no brushing, that is, a brushing duration of zero seconds.</s><s xml:id="_yrMKrkV">The ROBAS 2 brushing durations are highly zero-inflated.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc><div><p xml:id="_WhKxcS5"><s xml:id="_Nc7s8XH">Figure 3. Average User Rewards Over Time.</s><s xml:id="_vk5vBC4">Above, we show simulation results of the six candidate algorithms (BLR and ZIP respectively for different cluster sizes k) across the four simulation environments.</s><s xml:id="_AfZM2Ft">The y-axis is the mean and Â±1.96 â¢ standard error of the average user rewards ( R = 1 72 â 72 i=1 1 t 0 â t 0 s=1 R i,s ) for decision times t 0 â [20, 40, 60, 80, 100, 120, 140] across 100 Monte Carlo simulated trials.</s><s xml:id="_aJEJ56G">Standard error is Ï â 100 where Ï is the sample variance of the 100 Rs.</s><s xml:id="_fza8za9">(a) Stationary Base Model and Heterogeneous Effect Size; (b) Nonstationary Base Model and Heterogeneous Effect Size; (c) Stationary Base Model and Population Effect Size; (d) Nonstationary Base Model and Population Effect Size.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Normalized</head><figDesc><div><p xml:id="_3DGvGWG"><s xml:id="_mhWrp4C">Total Brushing Duration in Seconds = (Brushing Duration -172)/118 Normalized Day in Study When Fitting Model = (Day -14.5)/13.5 Normalized Day in Study When Generating Rewards = (Day -35.5)/34.5 Appendix A.2. Environment Base Model</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc><div><p xml:id="_Q9NMSjA"><s xml:id="_x7bUTt5">) Zero-Inflated Poisson Model for Brushing Duration Z â¼ Bernoulli 1sigmoid(g(S) T w i,b ) Y â¼ Poisson exp g(S) T w i,p Brushing Duration in Seconds : D = ZY (2) Hurdle Model with Square Root Transform for Brushing Duration Z â¼ Bernoulli 1sigmoid g(S) T w i,b Y â¼ N g(S) T w i,Âµ , Ï 2 i,u Brushing Duration in Seconds : D = ZY 2</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure A1 .</head><label>A1</label><figDesc><div><p xml:id="_anWJnZB"><s xml:id="_Xq6NJZm">Figure A1.</s><s xml:id="_uMsEqY5">Effect sizes â i,B 's, â i,N 's, Âµ B , Âµ N for each base model class.</s><s xml:id="_X7NymB8">The effect sizes are used to generate rewards under action A = 1 for the simulation environment.</s><s xml:id="_QgTVwej">(a) Bernoulli component (hurdle); (b) Nonzero component (square root); (c) Nonzero component (log); (d) Bernoulli component (ZIP); (e) Poisson component (ZIP).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc><div><p xml:id="_YxCGjwf"><s xml:id="_q85Y5C3">Four Environment Variants.</s><s xml:id="_3hskVbd">We consider two environment base models (stationary and nonstationary) and two effect sizes (population effect size, heterogeneous effect size).</s></p></div></figDesc><table><row><cell>S_Pop: Stationary Base Model, Popula-</cell><cell>NS_Pop: Nonstationary Base Model, Popu-</cell></row><row><cell>tion Effect Size</cell><cell>lation Effect Sizes</cell></row><row><cell>S_Het: Stationary Base Model, Heteroge-</cell><cell>NS_Het: Nonstationary Base Model, Het-</cell></row><row><cell>neous Effect Size</cell><cell>erogeneous Effect Sizes</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_nWkGEq4">Acknowledgments:</head><p xml:id="_4sByZe5"><s xml:id="_uTrJJNb">We are grateful for the guidance and support of <rs type="person">Wei Wei Pan</rs>, <rs type="person">Jiayu Yao</rs>, <rs type="person">Jessamyn Jackson</rs>, and <rs type="person">Doug Ezra Morrison</rs> throughout this project.</s></p></div>
			</div>
			<div type="funding">
<div xml:id="_4cjmQCz"><p xml:id="_yt7kpRM"><s xml:id="_AAAhxre">Funding: This research was funded by <rs type="funder">NIH</rs> grants <rs type="grantNumber">IUG3DE028723</rs>, <rs type="grantNumber">P50DA054039</rs>, <rs type="grantNumber">P41EB028242</rs>, <rs type="grantNumber">U01CA229437</rs>, <rs type="grantNumber">UH3DE028723</rs>, and <rs type="grantNumber">R01MH123804</rs>.</s><s xml:id="_sSDUgRc">KWZ is also supported by the <rs type="funder">National Science Foundation</rs> grant number <rs type="funder">NSF</rs> <rs type="grantNumber">CBET-2112085</rs> and by the <rs type="funder">National Science Foundation Graduate Research Fellowship Program</rs> under Grant No. <rs type="grantNumber">DGE1745303</rs>.</s><s xml:id="_yr4mGY5">Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the <rs type="funder">National Science Foundation</rs>.</s></p></div>
<div><head xml:id="_bSUTHgw">Institutional Review Board Statement: Not applicable.</head><p xml:id="_EMuQez4"><s xml:id="_7htpHs7">Informed Consent Statement: Not applicable.</s><s xml:id="_kbSBmZA">Data is de-identified.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zEWWZ4B">
					<idno type="grant-number">IUG3DE028723</idno>
				</org>
				<org type="funding" xml:id="_uvNBVU6">
					<idno type="grant-number">P50DA054039</idno>
				</org>
				<org type="funding" xml:id="_EzPZ2t4">
					<idno type="grant-number">P41EB028242</idno>
				</org>
				<org type="funding" xml:id="_5SHq5aU">
					<idno type="grant-number">U01CA229437</idno>
				</org>
				<org type="funding" xml:id="_mVRQRS4">
					<idno type="grant-number">UH3DE028723</idno>
				</org>
				<org type="funding" xml:id="_sT5YVAn">
					<idno type="grant-number">R01MH123804</idno>
				</org>
				<org type="funding" xml:id="_t4sC39e">
					<idno type="grant-number">CBET-2112085</idno>
				</org>
				<org type="funding" xml:id="_tUpFFZD">
					<idno type="grant-number">DGE1745303</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_g4kAPAT">Data Availability Statement:</head><p xml:id="_SSxFzug"><s xml:id="_fkvsAAy">The ROBAS 2 training data is available online at <ref type="url" target="https://github.com/ROBAS-UCLA/ROBAS.2/blob/master/inst/extdata/robas_2_data.csv">https://github.com/  ROBAS-UCLA/ROBAS.2/blob/master/inst/extdata/robas_2_data.csv</ref> (accessed on 31 May 2022).</s><s xml:id="_Ehwah5X">The source code and all other supplementary resources are available online at <ref type="url" target="https://github.com/StatisticalReinforcementLearningLab/pcs-for-rl">https://github.com/  StatisticalReinforcementLearningLab/pcs-for-rl</ref> (accessed on 1 June 2022).</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_TJPGHpH"><p xml:id="_Yt7YQvm"><s xml:id="_rknm8mh">Author Contributions: Conceptualization, A.L.T. and K.W.Z.; methodology, A.L.T., K.W.Z. and S.A.M.; software, A.L.T.; validation, A.L.T., K.W.Z., I.N.-S., V.S., F.D.-V.</s><s xml:id="_h9gnESa">and S.A.M.; formal analysis, A.L.T. and K.W.Z.; investigation, A.L.T.; resources A.L.T., K.W.Z., I.N.-S., V.S., F.D.-V.</s><s xml:id="_ArAAzkr">and S.A.M.; data curation, A.L.T. and K.W.Z.; writing-original draft preparation, A.L.T. and K.W.Z.; writing-review and editing, A.L.T., K.W.Z., I.N.-S., V.S., F.D.-V.</s><s xml:id="_A4HX44x">and S.A.M.; visualization, A.L.T.; supervision, F.D.-V.</s><s xml:id="_p4xEzxT">and S.A.M.; project administration, I.N.-S., V.S. and S.A.M.; funding acquisition, I.N.-S., V.S., F.D.-V.</s><s xml:id="_YHuHeY4">and S.A.M.</s><s xml:id="_C6CWTNc">All authors have read and agreed to the submitted version of the manuscript.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZDD5SC2">Conflicts of Interest:</head><p xml:id="_UhsDvMQ"><s xml:id="_FfN3PTP">The authors declare no conflict of interest.</s><s xml:id="_4AHawqH">The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript, or in the decision to publish the results.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GfBn8DG">Abbreviations</head><p xml:id="_4r5TrVv"><s xml:id="_TGQCZ8G">The following abbreviations are used in this manuscript: The ROBAS 2 dataset has a variety of features that we anticipate to be associated with brushing duration.</s><s xml:id="_ucRTwuh">These include the time of day (morning vs. evening), weekday vs. weekend, and summaries of the user's past brushing behavior.</s><s xml:id="_NCAMXPr">Together with domain experts in behavioral health and dentistry, we chose the following features to use to fit a model of the reward.</s><s xml:id="_KEhvwhZ">Recall that the ROBAS 2 dataset only includes data under no intervention, so for now we are only fitting a model for the baseline reward model (i.e., the brushing duration under action A i,t = 0).</s><s xml:id="_tw2ZPMJ">In Appendix A.5 we discuss how to model brushing duration under action 1.</s><s xml:id="_6XWWagA">Hurdle (Square Root)</s></p><p xml:id="_qbxutsS"><s xml:id="_qrF37UY">2 ) Zero-Inflated Poisson exp(g(S i,t ) T w i,p ) exp(exp(g(S i,t ) T w i,p ))</s></p><p xml:id="_axJkNAv"><s xml:id="_UaZZsdW">exp(exp(g(S i,t ) T w i,p ))-1</s></p><p xml:id="_Z5maNq2"><s xml:id="_kpSYgTD">Table <ref type="table">A3</ref> lists the number of model classes for all users in the ROBAS 2 study that we obtained after the procedure was run.</s><s xml:id="_SqZ46Ff">Using the chosen user-specific models, we simulate 100 trials.</s><s xml:id="_3ZNsmA2">In each trial, for each user in ROBAS 2, we use their respective model to generate a data trajectory (S i,t , R i,t ) 56 t=1 (note that the ROBAS 2 study had two brushing windows per day for 28 days for a total of 56 brushing windows).</s><s xml:id="_eA9AP7A">We then compute the following metrics for each of the trials and averaged across trials:</s></p><p xml:id="_kaFhAWR"><s xml:id="_na8te33">1.</s></p><p xml:id="_W3rZeJR"><s xml:id="_CHsBMD3">Proportion of Missed Brushing Windows:</s></p><p xml:id="_7Y8pSKt"><s xml:id="_DEbR7uW">Average Nonzero Brushing Duration: The following choice of the treatment effect (advantage) feature space was made after discussion with domain experts of which features are most likely to interact with the intervention (action).</s><s xml:id="_xwMbsEX">The Stationary model uses state h(S) â R 4 corresponding to the following features:</s></p><p xml:id="_k8nPUvd"><s xml:id="_UrhPMme">Time of Day (Morning/Evening) â {0, 1} 3.</s></p><p xml:id="_rFBxSUy"><s xml:id="_Bn3bHPy">Prior Day Total Brushing Duration (Normalized) â R 4.</s></p><p xml:id="_Hx5QRx6"><s xml:id="_qa7Q4Fq">Weekend Indicator (Weekday/Weekend) â {0, 1}</s></p><p xml:id="_FwUwYvh"><s xml:id="_esHrhAu">The Non-Stationary model uses state h(S) â R 5 corresponding to all of the above features as well as the following:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KTHrVfb">5.</head><p xml:id="_ZH8XxMu"><s xml:id="_7AM3gz3">Day in Study (Normalized) â R Appendix A.5.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_aXada8M">Imputation Approach</head><p xml:id="_2spx79r"><s xml:id="_Gu2kTHC">For the zero-inflated Poisson model, we impute treatment effects on both the user's intent to brush (Bernoulli component) and the user's brushing duration when they intend to brush (Poisson component).</s><s xml:id="_4GjkVcp">Similarly, for the hurdle models, we impute treatment effects on both whether the user's brushing duration is zero (Bernoulli component) and the user's brushing duration when the duration is nonzero.</s></p><p xml:id="_ZxVDEuz"><s xml:id="_gcA67Y7">After incorporating effect sizes, brushing duration under action A in state S is D where:</s></p><p xml:id="_XYtwW3a"><s xml:id="_Kxesfg5">are user-specific effect sizes; we will also consider population-level effect sizes (same across all users), which we denote as â B , â N .</s><s xml:id="_6Sbyphz">g(S) is the baseline feature vector as described in Appendix A.1, and h(S) is the feature vector that interacts with the effect size as specified above.</s></p><p xml:id="_jQpkQue"><s xml:id="_yUm3BJ2">Appendix A. <ref type="bibr">5</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_egvU8x5">.3. Heterogeneous Population-Level Effect Size</head><p xml:id="_qn8Gw2S"><s xml:id="_6Zkyz32">We consider realistic heterogeneous effect sizes (each user has a unique effect size) and a realistic population-level effect size (all users who share the same base model class also share the same effect size).</s></p><p xml:id="_JRqdVE4"><s xml:id="_R7auQuj">Population-Level Effect Sizes: Recall that for the stationary base model we fit models for Y and Z, and get user-specific parameters w i,b , w i,p â R 5 (zero-inflated Poisson model) and parameters w i,b , w i,Âµ â R 5 (hurdle models).</s><s xml:id="_qPqHdc2">Values of the fitted parameters can be accessed at: <ref type="url" target="https://github.com/StatisticalReinforcementLearningLab/pcs-for-rl/tree/main/sim_env_data">https://github.com/StatisticalReinforcementLearningLab/pcs-for-rl/tree/  main/sim_env_data</ref> (accessed on 1 June 2022).</s><s xml:id="_MRJdYHm">We use these parameters to form the population effect sizes as follows:</s></p><p xml:id="_CS2X3Z9"><s xml:id="_tNdAyXW">Zero-Inflated Models' Effect Sizes:</s></p><p xml:id="_9T4F9HP"><s xml:id="_qjXK3bz">i,p |.</s><s xml:id="_Vgmn8EY">Hurdle Models' Effect Sizes:</s></p><p xml:id="_88q37qa"><s xml:id="_7dSz2rW">We use w</s></p><p xml:id="_sDHAk4N"><s xml:id="_G2AEAYW">i,u to denote the d th dimension of the vector w i,b , w i,p , w i,u respectively; we take the minimum over all dimensions excluding d = 1, which represents the weight for the bias/intercept term.</s></p><p xml:id="_SaZSeY5"><s xml:id="_PmsSEkf">Heterogeneous Effect Sizes: To calculate the heterogeneous effect sizes, we again group users by their chosen base model (zero-inflated, hurdle square-root, hurdle log).</s><s xml:id="_W2RrRJv">We then draw effect sizes for each user from a normal distribution specific to their base model:</s></p><p xml:id="_szkcHmQ"><s xml:id="_4d9uggm">are set to the population-level effect sizes for that base model class as described above.</s><s xml:id="_HYWVBHn">To set Ï 2 B , Ï 2 N , we do the following: Zero-Inflated Models:</s></p><p xml:id="_q7sk4pZ"><s xml:id="_gFyy8X4">i,p |.</s><s xml:id="_eWBs97j">Hurdle Models:</s></p><p xml:id="_WsHb2Wh"><s xml:id="_Mrpeadx">i,Âµ |.</s><s xml:id="_n4TxGjn">After the procedure described above, we set Ï B = 0.192 for the hurdle models, Ï B = 0.193 for the zero-inflated model, Ï N = 0.576 for the hurdle square-root model, Ï N = 0.173 for the hurdle log model, and Ï N = 0.163 for the zero-inflated model (values are rounded to the nearest 3 decimal places).</s><s xml:id="_vMn8ZmK">Histograms of â i,B , â i,N and values of Âµ B , Âµ N for each base model class are specified in Figure <ref type="figure">A1</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_uWN9aPK">Appendix B. RL Algorithm Candidates</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZP8DK72">Appendix B.1. Feature Space for the RL Algorithm Candidates</head><p xml:id="_3V3RNHN"><s xml:id="_dfzAXJ2">We use f (S) â R 3 to denote the feature space used by our RL algorithm candidates to predict the advantage (i.e., the immediate treatment effect).</s><s xml:id="_zUQjPFT">f (S) contains the following features:</s></p><p xml:id="_HVvn3K7"><s xml:id="_Dh2WjnK">1.</s></p><p xml:id="_qqUmSqC"><s xml:id="_dVsGuBH">Bias/Intercept Term â R 2.</s></p><p xml:id="_JgDgdPV"><s xml:id="_4CD9Tnb">Time of Day (Morning/Evening) â {0, 1} 3.</s></p><p xml:id="_pTG2Un8"><s xml:id="_bCbqCnz">Prior Day Total Brushing Duration (Normalized) â R The normalization procedure for Prior Day Brushing Duration is the same as the one described in Appendix A.1.</s></p><p xml:id="_auAz5Jy"><s xml:id="_DXqGwwS">We use m(S) â R 4 to denote the feature space used by the RL algorithm candidates to approximate the baseline reward function.</s><s xml:id="_AEDYBrJ">m(S) contains all the above features as well as the following:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Y3dBASj">4.</head><p xml:id="_2Xejfmx"><s xml:id="_fVvgVaT">Weekend Indicator (Weekday/Weekend) â {0, 1}</s></p><p xml:id="_4Va5CjC"><s xml:id="_DhTx8HW">Note that the feature space used by the RL algorithm candidates is different than the feature space used to model the reward in the simulation environments, specified in Appendices A.1 and A.5; this means that the RL algorithms will have a misspecified reward model.</s><s xml:id="_fYkrsEu">Namely, the baseline feature space for the simulation environment has an additional Proportion of Nonzero Brushing Sessions Over Past 7 Days feature and the non-stationary variant has the Day in Study (Normalized) feature.</s><s xml:id="_aVqTZaq">The treatment effect feature space for the simulation environment has an additional Weekend Indicator (Weekday/Weekend) and the non-stationary variant has the Day in Study (Normalized) feature.</s></p><p xml:id="_uhnnsD4"><s xml:id="_AaDWreB">The rationale for not including the Day in Study (Normalized) feature is although we wanted to capture potential non-stationarity in brushing outcomes in order to create a realistic simulation environment, our RL algorithm candidates do not have reward functions that vary arbitrarily over time.</s><s xml:id="_phBdmXG">We do not include Proportion of Nonzero Brushing Sessions Over Past 7 Days and Weekend Indicator (Weekday/Weekend) to detect the robustness of RL algorithm candidates to a misspecified reward model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BdArE3D">Appendix B.2. Decision 1: Reward Approximating Function</head><p xml:id="_EGMjnwG"><s xml:id="_zkEnhgu">The first decision in designing the RL algorithm is the choice between using a linear model or a zero-inflated Poisson model as the reward approximating function used by the posterior sampler (note that this is separate from the reward model used to generate the environment).</s><s xml:id="_DUStW6Z">More information on how the posterior sampling algorithm performs action selection can be found in Appendix C.2. Appendix C.1 provides information about how the algorithm updates at update times.</s></p><p xml:id="_ZNHWEhy"><s xml:id="_SB3R4YV">Note that the function m for the RL algorithm's baseline reward model is only used at update times.</s><s xml:id="_KesCVF6">The function f for the RL algorithm's advantage model is used at both decision and update times.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_b2dbXBC">Appendix B.2.1. Bayesian Linear Regression Model</head><p xml:id="_amgtk5K"><s xml:id="_UQuQTmM">The first candidate is to use the following reward generating model with action centering used in <ref type="bibr">[1]</ref> for the posterior sampler:</s></p><p xml:id="_fWFfcsE"><s xml:id="_UmJ4xsZ">where Î± i,0 â R 4 and Î± i,1 , Î² i â R 3 .</s><s xml:id="_Kdh6K3z">Ï i,t is the probability that action A i,t = 1 is selected by the RL algorithm for user i in state S i,t ; we discuss how to compute this probability in Appendix C.2.</s><s xml:id="_y88k2Vs">The RL algorithm models i,t as being drawn from N (0, Î· 2 ) (the choice of Î· 2 is informed by the ROBAS 2 dataset).</s><s xml:id="_7ajyRng">Additionally, we put uninformative normal priors on the parameters: Î± i,0 â¼ N (0, Ï prior I 4 ), Î± i,1 â¼ N (0, Ï prior I 3 ), Î² i â¼ N (0, Ï prior I 3 ), where Ï prior = 5.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NKReUV8">Appendix B.2.2. Zero-Inflated Poisson Regression Model</head><p xml:id="_jKrpHHQ"><s xml:id="_fd2m7Uc">The second candidate is to use the zero-inflated reward generating model for the posterior sampler:</s></p><p xml:id="_vFVpfRB"><s xml:id="_FuwR4Yb">The above model closely resembles the zero-inflated Poisson model class used to develop the simulation environment in Appendix A.2; however, recall that the feature space used by the RL algorithm and the model to generate the environment is different.</s><s xml:id="_RNSv8KB">Additionally, here, we directly model the reward, rather than the raw brushing duration.</s></p><p xml:id="_UfChaJH"><s xml:id="_YEfffwg">Additionally, the posterior sampling algorithm will put the following uninformative normal priors on the parameters: Î± i,b , Î± i,p â¼ N (0, Ï prior I 4 ) and Î² i,b , Î² i,p â¼ N (0, Ï prior I 3 ), where Ï prior = 5.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cn4ZgBC">Appendix B.3. Decision 2: Cluster Size</head><p xml:id="_hjKwQs5"><s xml:id="_sZBTt9f">Clustering involves grouping k users together and pooling all their data together for the RL algorithm.</s><s xml:id="_TUmC7pK">This gives us one RL algorithm instantiation per cluster (no data shared across clusters).</s><s xml:id="_jgXdsQX">For our experiments, we draw N = 72 simulated users (the expected sample size for the Oralytics study) with replacement and cluster these users at random (every possible cluster is equally likely).</s><s xml:id="_HW6bEQR">We then keep these cluster assignments fixed across the trials.</s></p><p xml:id="_e2dNvGr"><s xml:id="_akyfF2H">For simplicity in running our experiments, we consider randomly formed clusters, but we are thinking of clustering by entry date in the real study.</s><s xml:id="_dNntXuy">Recall that we want to cluster users who are similar to each other.</s><s xml:id="_Yh3Ze3a">A natural approach is to cluster users by a baseline feature, but we cannot predict how many users who share the same baseline feature will join within a relatively short period of time (e.g., we cannot depend on there being four females within the first two weeks).</s><s xml:id="_3KReWtX">Entry date is a reasonable clustering criterion because domain experts believe that users who enter the study around the same time will be similar.</s><s xml:id="_qT7NqXn">Users who enter near the end of the study may be very different from users who enter near the beginning because of societal factors (e.g., pandemic restrictions being lifted), seasonal influences (e.g., differences in a user's mood in spring and midwinter), and fidelity (e.g., quality of onboarding procedures and staff experience may improve over time).</s><s xml:id="_A5VQcvu">One natural approach is to cluster by baseline features, but that is not feasible for a study with a slow recruitment rate, like Oralytics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_tqrsANV">Appendix C. RL Algorithm Posterior Updates and Posterior Sampling Action Selection</head><p xml:id="_6DPUQwp"><s xml:id="_SREwhBx">Appendix C.1.</s><s xml:id="_DM95jBa">Posterior Updates to the RL Algorithm at Update Time During the update step, the reward approximating function will update the posterior with newly collected data.</s><s xml:id="_GNqNRhV">Additionally, we make M draws of the parameters from the updated posterior and use them for all decision times until the next update time.</s><s xml:id="_UasbtMg">Here are the procedures for how the Bayesian linear regression model and the zero-inflated Poisson model perform posterior updating.</s></p><p xml:id="_xJUhgHf"><s xml:id="_AGtW43x">Appendix C.1.1.</s><s xml:id="_huyjCH4">Bayesian Linear Regression Model Suppose we are selecting actions for decision time t.</s><s xml:id="_wZQHq5J">Let Ï(S i,t , A i,t ) = [m(S i,t ), Ï i,t f (S i,t ), (A i,tÏ i,t ) f (S i,t )] be the joint feature vector and Î¸ i = [Î± i,0 , Î± i,1 , Î² i ] be the joint weight vector.</s><s xml:id="_59Uyynf">Notice that Equation (A3) can be vectorized in the form: R i,t = Ï(S i,t , A i,t ) T Î¸ i + i,t .</s><s xml:id="_ngru7Dr">Now let Î¦ i,1:t-1 be the matrix of all stacked vectors {Ï(S i,s , A i,s )} t-1 s=1 , and R i,1:t-1 be a vector of stacked rewards {R i,s } t-1 s=1 , where we have batch data of the t -1 decision times before the current update time.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_nQgnwP4">Appendix C.2. Action Selection at Decision Time</head><p xml:id="_Gq972cP"><s xml:id="_mZDaqRC">Our action selection scheme at decision time selects action A i,t â¼ Bern(Ï i,t ) where Ï i,t = clip( Ïi,t ).</s><s xml:id="_wbrMEXD">clip is the clipping function defined in Appendix C.2.2 and Ïi,t is the posterior probability that A i,t = 1 is optimally defined in Appendix C. <ref type="bibr" target="#b5">2</ref> Note that the randomness in the probability above is only over the draw of Î² from the posterior distribution.</s></p><p xml:id="_HfF963c"><s xml:id="_g3JTszp">Zero-Inflated Poisson Model: Based on the zero-inflated Poisson model of the reward, specified by Equation (A4): Ïi,t = Pr Î±i,b ,Î± i,p , Î²i,b , Î²i,p Zi,t á»¸i,t &gt; 0 S i,t , H i,t-1</s></p><p xml:id="_kUQKjy3"><s xml:id="_ykZvWCJ">where Zi,t â¼ Bernoulli 1sigmoid(m(S i,t ) T Î±i,b + A i,t â¢ f (S i,t ) T Î²i,b ) and á»¸i,t â¼ Poisson exp m(S i,t ) T Î±i,p + A i,t â¢ f (S i,t ) T Î²i,p .</s><s xml:id="_9nNdjmG">Note that the randomness in the probability above is only over the draw of (Î± i,b , Î±i,p , Î²i,b , Î²i,p ) from the posterior distribution.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_bAU7kr3">Appendix C.2.2. Clipping to Form Action Selection Probabilities</head><p xml:id="_6WcFRW7"><s xml:id="_w3JazCF">Since we want to facilitate after-study analyses, we clip action selection probabilities using the action clipping function for some Ï min , Ï max where 0 &lt; Ï min â¤ Ï max &lt; 1 is chosen by the scientific team: clip(Ï) = min(Ï max , max(Ï, Ï min )) â [Ï min , Ï max ] (A5)</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_RpKHBZ2">Bias/Intercept Term â R 2. Time of Day</title>
		<idno type="DOI">10.22501/rc.1166516</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5CbMhms">Morning/Evening) â {0</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">Bias/Intercept Term â R 2. Time of Day (Morning/Evening) â {0, 1}</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<idno type="DOI">10.7717/peerj.4248/fig-2</idno>
		<title level="m" xml:id="_a35wGrm">Prior Day Total Brushing Duration (Normalized) â R 4. Weekend Indicator (Weekday/Weekend) â {0</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">Prior Day Total Brushing Duration (Normalized) â R 4. Weekend Indicator (Weekday/Weekend) â {0, 1}</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_a3uf6D7">Proportion of Nonzero Brushing Sessions Over Past 7 Days â</title>
		<idno type="DOI">10.1787/888933835022</idno>
		<imprint>
			<biblScope unit="volume">0</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Proportion of Nonzero Brushing Sessions Over Past 7 Days â [0, 1]</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" xml:id="_dmGR9UF">Normalization of State Features We normalize features to ensure that state features are all in a similar range. The Prior Day Total Brushing Duration feature is normalized using z-score normalization (subtract mean and divide by standard deviation) and the Day in Study feature</title>
		<imprint/>
	</monogr>
	<note>The Stationary model of the base environment uses the state function g(S i,t ) â R 5 that only includes the first five features above The Non-Stationary model of the base environment uses state g(S i,t ) â R 6 that corresponds to all of the above features Day in Study (Normalized) â [-1, 1] We use these features to generate two types of base reward environments (Stationary and Non-Stationary) originally in the range</note>
	<note type="raw_reference">Day in Study (Normalized) â [-1, 1] We use these features to generate two types of base reward environments (Stationary and Non-Stationary). The Stationary model of the base environment uses the state function g(S i,t ) â R 5 that only includes the first five features above. The Non-Stationary model of the base environment uses state g(S i,t ) â R 6 that corresponds to all of the above features. Normalization of State Features We normalize features to ensure that state features are all in a similar range. The Prior Day Total Brushing Duration feature is normalized using z-score normalization (subtract mean and divide by standard deviation) and the Day in Study feature (originally in the range</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_X8rEavg">Personalized HeartSteps: A Reinforcement Learning Algorithm for Optimizing Physical Activity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3381007</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5JE7fF2">Proc. ACM Interact. Mob. Wearable Ubiquitous Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liao, P.; Greenewald, K.H.; Klasnja, P.V.; Murphy, S.A. Personalized HeartSteps: A Reinforcement Learning Algorithm for Optimizing Physical Activity. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2020, 4, 1-22. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_chMvTz2">Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozdoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hochberg</surname></persName>
		</author>
		<idno type="DOI">10.2196/jmir.7994</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uc7JtyR">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">338</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yom-Tov, E.; Feraru, G.; Kozdoba, M.; Mannor, S.; Tennenholtz, M.; Hochberg, I. Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system. J. Med. Internet Res. 2017, 19, e338. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_8KsKyd3">Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Forman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kerrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Butryn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Juarascio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>OntaÃ±Ã³n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Dallal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Crochiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moskow</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10865-018-9964-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fbtmv5c">J. Behav. Med</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="276" to="290" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Forman, E.M.; Kerrigan, S.G.; Butryn, M.L.; Juarascio, A.S.; Manasse, S.M.; OntaÃ±Ã³n, S.; Dallal, D.H.; Crochiere, R.J.; Moskow, D. Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss? J. Behav. Med. 2019, 42, 276-290. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_487GQXG">Stanford Computational Policy Lab Pretrial Nudges</title>
		<author>
			<persName><forename type="first">S</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.24148/cdrb2022-4</idno>
		<ptr target="https://policylab.stanford.edu/projects/nudge.html" />
		<imprint>
			<date type="published" when="2022-06-01">2022. 1 June 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Allen, S. Stanford Computational Policy Lab Pretrial Nudges. 2022. Available online: https://policylab.stanford.edu/projects/ nudge.html (accessed on 1 June 2022).</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_Fq65ath">Bandit algorithms to personalize educational chatbots</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-021-05983-y</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g5K4Kpz">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="2389" to="2418" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cai, W.; Grossman, J.; Lin, Z.J.; Sheng, H.; Wei, J.T.Z.; Williams, J.J.; Goel, S. Bandit algorithms to personalize educational chatbots. Mach. Learn. 2021, 110, 2389-2418. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_E8RSpvn">Bandit Learning with Implicit Feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_E25Dz4x">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">Qi, Y.; Wu, Q.; Wang, H.; Tang, J.; Sun, M. Bandit Learning with Implicit Feedback. In Advances in Neural Information Processing Systems;</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><surname>Eds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inc</forename><surname>Curran Associates</surname></persName>
		</author>
		<idno type="DOI">10.2172/7218131</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Red Hook</publisher>
			<biblScope unit="volume">31</biblScope>
			<pubPlace>NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R., Eds.; Curran Associates, Inc.: Red Hook, NY, USA, 2018; Volume 31.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_z3jAPcf">Letter to Amazon Shareholders</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bezos</surname></persName>
		</author>
		<ptr target="https://www.sec.gov/Archives/edgar/data/1018724/000119312516530910/d168744dex991.htm" />
		<imprint>
			<date type="published" when="1997-06-01">1997. 1997. 1 June 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bezos, J.P. 1997 Letter to Amazon Shareholders. 1997. Available online: https://www.sec.gov/Archives/edgar/data/1018724/ 000119312516530910/d168744dex991.htm (accessed on 1 June 2022).</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_kcRtRDx">Veridical data science</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumbier</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1901326117</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_szbVhdr">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="3920" to="3929" />
			<date type="published" when="2020">2020</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu, B.; Kumbier, K. Veridical data science. Proc. Natl. Acad. Sci. USA 2020, 117, 3920-3929. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_tmuMBNw">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton, R.S.; Barto, A.G. Reinforcement Learning: An Introduction; MIT Press: Cambridge, MA, USA, 2018.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_6DG8MfW">Reinforcement learning for personalization: A systematic literature review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hengst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Grua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>El Hassouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<idno type="DOI">10.3233/DS-200028</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4eVWw8K">Data Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="107" to="147" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">den Hengst, F.; Grua, E.M.; el Hassouni, A.; Hoogendoorn, M. Reinforcement learning for personalization: A systematic literature review. Data Sci. 2020, 3, 107-147. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_9vWYqcM">Bandit problems with side observations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Poor</surname></persName>
		</author>
		<idno type="DOI">10.1109/TAC.2005.844079</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CrcaPyF">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="338" to="355" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, C.C.; Kulkarni, S.R.; Poor, H.V. Bandit problems with side observations. IEEE Trans. Autom. Control 2005, 50, 338-355. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_h9ar6pt">The epoch-greedy algorithm for contextual multi-armed bandits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_exntK7q">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="96" to="103" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Langford, J.; Zhang, T. The epoch-greedy algorithm for contextual multi-armed bandits. Adv. Neural Inf. Process. Syst. 2007, 20, 96-103.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_YXG6T8C">From ads to interventions: Contextual bandits in mobile health</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-51394-2_25</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AUqkXkM">Mobile Health</title>
		<meeting><address><addrLine>Berlin/Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="495" to="517" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tewari, A.; Murphy, S.A. From ads to interventions: Contextual bandits in mobile health. In Mobile Health; Springer: Berlin/Heidelberg, Germany, 2017; pp. 495-517.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_gB7j9yg">What is personalization? Perspectives on the design and implementation of personalization in information systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Poole</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15327744joce1603&amp;4_2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UzMFdZu">J. Organ. Comput. Electron. Commer</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="179" to="202" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fan, H.; Poole, M.S. What is personalization? Perspectives on the design and implementation of personalization in information systems. J. Organ. Comput. Electron. Commer. 2006, 16, 179-202. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_KSgeB5h">Data-efficient off-policy policy evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-96136-1_26</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_775xYUc">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>PMLR, New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="2139" to="2148" />
		</imprint>
	</monogr>
	<note type="raw_reference">Thomas, P.; Brunskill, E. Data-efficient off-policy policy evaluation for reinforcement learning. In Proceedings of the International Conference on Machine Learning. PMLR, New York, NY, USA, 20-22 June 2016; pp. 2139-2148.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main" xml:id="_GMp9tq6">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01643</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Levine, S.; Kumar, A.; Tucker, G.; Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv 2020, arXiv:2005.01643.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_KkJQKTD">Assessing time-varying causal effect moderation in mobile health</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boruvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2017.1305274</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_42P7fUM">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="1112" to="1121" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Boruvka, A.; Almirall, D.; Witkiewitz, K.; Murphy, S.A. Assessing time-varying causal effect moderation in mobile health. J. Am. Stat. Assoc. 2018, 113, 1112-1121. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_A4V2x2F">Confidence intervals for policy evaluation in adaptive experiments</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hadad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hirshberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2014602118</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Gr7eaWF">Proc. Natl. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">2014602118</biblScope>
			<date type="published" when="2021">2021</date>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Hadad, V.; Hirshberg, D.A.; Zhan, R.; Wager, S.; Athey, S. Confidence intervals for policy evaluation in adaptive experiments. Proc. Natl. Acad. Sci. USA 2021, 118, e2014602118. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_cbyETWt">Power Constrained Bandits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2AaNTVN">Proceedings of the 6th Machine Learning for Healthcare Conference</title>
		<meeting>the 6th Machine Learning for Healthcare Conference<address><addrLine>PMLR, Virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-08-07">6-7 August 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yao, J.; Brunskill, E.; Pan, W.; Murphy, S.; Doshi-Velez, F. Power Constrained Bandits. In Proceedings of the 6th Machine Learning for Healthcare Conference, PMLR, Virtual, 6-7 August 2021;</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sendak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sjoding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="209" to="259" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jung, K., Yeung, S., Sendak, M., Sjoding, M., Ranganath, R., Eds.; 2021; Volume 149, pp. 209-259.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_qwrRcpY">Mobile Health Apps: Adoption, Adherence, and Abandonment</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Murnane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huffaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kossinets</surname></persName>
		</author>
		<idno type="DOI">10.1145/2800835.2800943</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_h4radXA">Proceedings of the Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers</title>
		<meeting>the Adjunct the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the 2015 ACM International Symposium on Wearable Computers<address><addrLine>Osaka, Japan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015-09-11">7-11 September 2015. 2015</date>
			<biblScope unit="page" from="261" to="264" />
		</imprint>
	</monogr>
	<note>UbiComp/ISWC&apos;15 Adjunct</note>
	<note type="raw_reference">Murnane, E.L.; Huffaker, D.; Kossinets, G. Mobile Health Apps: Adoption, Adherence, and Abandonment. In Proceedings of the Adjunct Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2015 ACM International Symposium on Wearable Computers, Osaka, Japan, 7-11 September 2015; UbiComp/ISWC&apos;15 Adjunct; Association for Computing Machinery: New York, NY, USA, 2015; pp. 261-264. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_mAMFPxF">Opportunities and Challenges for Smartphone Applications in Supporting Health Behavior Change: Qualitative Study</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dennison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yardley</surname></persName>
		</author>
		<idno type="DOI">10.2196/jmir.2583</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SuHqcze">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dennison, L.; Morrison, L.; Conway, G.; Yardley, L. Opportunities and Challenges for Smartphone Applications in Supporting Health Behavior Change: Qualitative Study. J. Med. Internet Res. 2013, 15, e86. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_Ksqh78k">PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alumootil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06961</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Agarwal, A.; Alomar, A.; Alumootil, V.; Shah, D.; Shen, D.; Xu, Z.; Yang, C. PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators. arXiv 2021, arXiv:2102.06961.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_F6YYENf">Adaptive learning algorithms to optimize mobile applications for behavioral health: Guidelines for design decisions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Modiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Deliu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jay Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Lyles</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocab001</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ShmDFX5">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1225" to="1234" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Figueroa, C.A.; Aguilera, A.; Chakraborty, B.; Modiri, A.; Aggarwal, J.; Deliu, N.; Sarkar, U.; Jay Williams, J.; Lyles, C.R. Adaptive learning algorithms to optimize mobile applications for behavioral health: Guidelines for design decisions. J. Am. Med. Inform. Assoc. 2021, 28, 1225-1234. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_AnC3cdk">Learning to simulate on sparse trajectory data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4yBQKMM">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Berlin/Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="530" to="545" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wei, H.; Chen, C.; Liu, C.; Zheng, G.; Li, Z. Learning to simulate on sparse trajectory data. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases; Springer: Berlin/Heidelberg, Germany, 2020; pp. 530-545.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main" xml:id="_jqmZt98">RecSim: A Configurable Simulation Platform for Recommender Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mladenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
		<idno>arXiv:cs.LG/1909.04847</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Ie, E.; Hsu, C.W.; Mladenov, M.; Jain, V.; Narvekar, S.; Wang, J.; Wu, R.; Boutilier, C. RecSim: A Configurable Simulation Platform for Recommender Systems. arXiv 2019, arXiv:cs.LG/1909.04847.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main" xml:id="_TqRBhN9">MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R O</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H F</forename><surname>Camargo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>BrandÃ£o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Caetano</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdmw51313.2020.00035</idno>
		<idno>arXiv:cs.IR/2010.07035</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Santana, M.R.O.; Melo, L.C.; Camargo, F.H.F.; BrandÃ£o, B.; Soares, A.; Oliveira, R.M.; Caetano, S. MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces. arXiv 2020, arXiv:cs.IR/2010.07035.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Openai</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><surname>Gym</surname></persName>
		</author>
		<idno type="DOI">10.1063/pt.5.029931</idno>
		<idno>arXiv:cs.LG/1606.01540</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; Zaremba, W. OpenAI Gym. arXiv 2016, arXiv:cs.LG/1606.01540.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_PtjMmT8">Optimizing Adaptive Notifications in Mobile Health Interventions Systems: Reinforcement Learning from a Data-driven Behavioral Simulator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>KrÃ¶se</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10916-021-01773-0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_F26w5UU">J. Med. Syst</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S.; Zhang, C.; KrÃ¶se, B.; van Hoof, H. Optimizing Adaptive Notifications in Mobile Health Interventions Systems: Reinforcement Learning from a Data-driven Behavioral Simulator. J. Med. Syst. 2021, 45, 1-8. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_nFgyHbH">Building healthy recommendation sequences for everyone: A safe reinforcement learning approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZaHhxEk">Proceedings of the FAccTRec Workshop</title>
		<meeting>the FAccTRec Workshop<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
			<biblScope unit="page" from="26" to="27" />
		</imprint>
	</monogr>
	<note type="raw_reference">Singh, A.; Halpern, Y.; Thain, N.; Christakopoulou, K.; Chi, E.; Chen, J.; Beutel, A. Building healthy recommendation sequences for everyone: A safe reinforcement learning approach. In Proceedings of the FAccTRec Workshop, Online, 26-27 September 2020.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_U8sfHb3">Simulation Environment for Guiding the Design of Contextual Personalization Systems in the Context of Hearing Aids</title>
		<author>
			<persName><forename type="first">M</forename><surname>Korzepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>MÃ¸rup</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386392.3399291</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rrbyCqH">Proceedings of the Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization</title>
		<meeting>the Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization<address><addrLine>Genoa, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020-07-17">14-17 July 2020. 2020</date>
			<biblScope unit="page" from="293" to="298" />
		</imprint>
	</monogr>
	<note>UMAP &apos;20 Adjunct</note>
	<note type="raw_reference">Korzepa, M.; Petersen, M.K.; Larsen, J.E.; MÃ¸rup, M. Simulation Environment for Guiding the Design of Contextual Person- alization Systems in the Context of Hearing Aids. In Proceedings of the Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization, Genoa, Italy, 14-17 July 2020; UMAP &apos;20 Adjunct; Association for Computing Machinery: New York, NY, USA, 2020; pp. 293-298. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_wq2cZjh">Personalization of Health Interventions Using Cluster-Based Reinforcement Learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Otterlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barbaro</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-03098-8_31</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3sGXnyh">PRIMA 2018: Principles and Practice of Multi-Agent Systems</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Oren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Sakurai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Noda</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">T R</forename><surname>Savarimuthu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Cao Son</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hassouni, A.E.; Hoogendoorn, M.; van Otterlo, M.; Barbaro, E. Personalization of Health Interventions Using Cluster-Based Reinforcement Learning. In PRIMA 2018: Principles and Practice of Multi-Agent Systems; Miller, T., Oren, N., Sakurai, Y., Noda, I., Savarimuthu, B.T.R., Cao Son, T., Eds.; Springer International Publishing: Cham, Switzerland, 2018; pp. 467-475.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Otterlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Eiben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Muhonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barbaro</surname></persName>
		</author>
		<idno type="DOI">10.1145/3350546.3352527</idno>
		<idno type="arXiv">arXiv:1804.03592</idno>
		<title level="m" xml:id="_sezuhJt">A clustering-based reinforcement learning approach for tailored personalization of e-Health interventions</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">Hassouni, A.E.; Hoogendoorn, M.; van Otterlo, M.; Eiben, A.E.; Muhonen, V.; Barbaro, E. A clustering-based reinforcement learning approach for tailored personalization of e-Health interventions. arXiv 2018, arXiv:1804.03592.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_K8qSruR">Stable Discovery of Interpretable Subgroups via Calibration in Causal Studies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1111/insr.12427</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_a32xa86">Int. Stat. Rev</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="135" to="S178" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dwivedi, R.; Tan, Y.S.; Park, B.; Wei, M.; Horgan, K.; Madigan, D.; Yu, B. Stable Discovery of Interpretable Subgroups via Calibration in Causal Studies. Int. Stat. Rev. 2020, 88, S135-S178. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_zKXvBWf">Next waves in veridical network embedding</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">G</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1002/sam.11486</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dBBGqNW">Stat. Anal. Data Min. ASA Data Sci. J</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ward, O.G.; Huang, Z.; Davison, A.; Zheng, T. Next waves in veridical network embedding. Stat. Anal. Data Min. ASA Data Sci. J. 2021, 14, 5-17. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_MMcczst">A new method to compare the interpretability of rule-based algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Margot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luta</surname></persName>
		</author>
		<idno type="DOI">10.3390/ai2040037</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qDpun9f">AI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="621" to="635" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Margot, V.; Luta, G. A new method to compare the interpretability of rule-based algorithms. AI 2021, 2, 621-635. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_XYC8KCR">A Scalable System for Passively Monitoring Oral Health Behaviors Using Electronic Toothbrushes in the Home Setting: Development and Feasibility Study</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Belin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.2196/17347</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vMxJMSP">JMIR Mhealth Uhealth</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17347</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shetty, V.; Morrison, D.; Belin, T.; Hnat, T.; Kumar, S. A Scalable System for Passively Monitoring Oral Health Behaviors Using Electronic Toothbrushes in the Home Setting: Development and Feasibility Study. JMIR Mhealth Uhealth 2020, 8, e17347. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_KPrK48F">The dependence of effective planning horizon on model accuracy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24804-2</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Rc5ru6b">Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 2015 International Conference on Autonomous Agents and Multiagent Systems<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-08">4-8 May 2015</date>
			<biblScope unit="page" from="1181" to="1189" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jiang, N.; Kulesza, A.; Singh, S.; Lewis, R. The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems, Istanbul, Turkey, 4-8 May 2015; pp. 1181-1189.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main" xml:id="_BwgsRrH">A Tutorial on Thompson Sampling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazerouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<idno type="DOI">10.1561/9781680834710</idno>
		<ptr target="http://xxx.lanl.gov/abs/1707.02038" />
		<imprint>
			<date type="published" when="2022-06-01">1 June 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Russo, D.; Roy, B.V.; Kazerouni, A.; Osband, I. A Tutorial on Thompson Sampling. Available online: http://xxx.lanl.gov/abs/17 07.02038 (accessed on 1 June 2022).</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_PYAnVKk">Group-driven reinforcement learning for personalized mhealth intervention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00928-1_67</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ArzzXMD">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<meeting><address><addrLine>Berlin/Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhu, F.; Guo, J.; Xu, Z.; Liao, P.; Yang, L.; Huang, J. Group-driven reinforcement learning for personalized mhealth intervention. In International Conference on Medical Image Computing and Computer-Assisted Intervention; Springer: Berlin/Heidelberg, Germany, 2018; pp. 590-598.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_NPvSfCq">IntelligentPooling: Practical Thompson sampling for mHealth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-021-05995-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NnBarhA">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="2685" to="2727" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tomkins, S.; Liao, P.; Klasnja, P.; Murphy, S. IntelligentPooling: Practical Thompson sampling for mHealth. Mach. Learn. 2021, 110, 2685-2727. [CrossRef] [PubMed]</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_QVsw3uy">Multi-task learning for contextual bandits</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gezY6vS">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4848" to="4856" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Deshmukh, A.A.; Dogan, U.; Scott, C. Multi-task learning for contextual bandits. Adv. Neural Inf. Process. Syst. 2017, 30, 4848-4856.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_AaFFS2G">Horde of bandits using gaussian markov random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lakshmanan</surname></persName>
		</author>
		<idno type="DOI">10.19070/2377-8075-1700086</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_j7HzrBU">Proceedings of the Artificial Intelligence and Statistics</title>
		<meeting>the Artificial Intelligence and Statistics<address><addrLine>PMLR, Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04">April 2017</date>
			<biblScope unit="page" from="690" to="699" />
		</imprint>
	</monogr>
	<note type="raw_reference">Vaswani, S.; Schmidt, M.; Lakshmanan, L. Horde of bandits using gaussian markov random fields. In Proceedings of the Artificial Intelligence and Statistics, PMLR, Fort Lauderdale, FL, USA, 20-22 April 2017; pp. 690-699.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_zRzk6SB">A comparison of zero-inflated and hurdle models for modeling zero-inflated count data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40488-021-00121-4</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zVZvSws">J. Stat. Distrib. Appl</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Feng, C.X. A comparison of zero-inflated and hurdle models for modeling zero-inflated count data. J. Stat. Distrib. Appl. 2021, 8, 1-19. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_ZZa5Ypj">Illustrating bias due to conditioning on a collider</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Schisterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Westreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poole</surname></persName>
		</author>
		<idno type="DOI">10.1093/ije/dyp334</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FCmzRZu">Int. J. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="417" to="420" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cole, S.R.; Platt, R.W.; Schisterman, E.F.; Chu, H.; Westreich, D.; Richardson, D.; Poole, C. Illustrating bias due to conditioning on a collider. Int. J. Epidemiol. 2010, 39, 417-420. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_7WCpt7K">Educational Note: Paradoxical collider effect in the analysis of non-communicable disease epidemiological data: A reproducible illustration and web application</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Luque-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schomaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Redondo-Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jose Sanchez Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Schnitzer</surname></persName>
		</author>
		<idno type="DOI">10.1093/ije/dyy275</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_whsWZU8">Int. J. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="640" to="653" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luque-Fernandez, M.A.; Schomaker, M.; Redondo-Sanchez, D.; Jose Sanchez Perez, M.; Vaidya, A.; Schnitzer, M.E. Educational Note: Paradoxical collider effect in the analysis of non-communicable disease epidemiological data: A reproducible illustration and web application. Int. J. Epidemiol. 2019, 48, 640-653. [CrossRef]</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_4JJzMHY">Endogenous selection bias: The problem of conditioning on a collider variable</title>
		<author>
			<persName><forename type="first">F</forename><surname>Elwert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winship</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-soc-071913-043455</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YvTNCZj">Annu. Rev. Sociol</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="31" to="53" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Elwert, F.; Winship, C. Endogenous selection bias: The problem of conditioning on a collider variable. Annu. Rev. Sociol. 2014, 40, 31-53. [CrossRef]</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
