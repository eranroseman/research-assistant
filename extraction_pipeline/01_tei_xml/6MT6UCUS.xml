<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_qWjNVcx">AI-driven multi-agent reinforcement learning framework for real-time monitoring of physiological signals in stress and depression contexts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Thanveer</forename><surname>Shaik</surname></persName>
							<email>thanveer.shaik@unisq.edu.au</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Mathematics, Physics &amp; Computing , University of Southern Queensland , Toowoomba , Australia.</note>
								<orgName type="department">School of Mathematics, Physics &amp; Computing</orgName>
								<orgName type="institution">University of Southern Queensland</orgName>
								<address>
									<settlement>Toowoomba</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohui</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Mathematics, Physics &amp; Computing , University of Southern Queensland , Toowoomba , Australia.</note>
								<orgName type="department">School of Mathematics, Physics &amp; Computing</orgName>
								<orgName type="institution">University of Southern Queensland</orgName>
								<address>
									<settlement>Toowoomba</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> School of Computer and Artificial Intel- ligence , Wuhan University of Technology , Wuhan , China.</note>
								<orgName type="department">School of Computer and Artificial Intel- ligence</orgName>
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Division of Artificial Intelligence , School of Data Science , Lingnan University , Hong Kong , China.</note>
								<orgName type="department" key="dep1">Division of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Data Science</orgName>
								<orgName type="institution">Lingnan University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong-Ning</forename><surname>Dai</surname></persName>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> Department of Computer Science , Hong Kong Baptist University , Hong Kong , China.</note>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
							<affiliation key="aff4">
								<note type="raw_affiliation"><label>5</label> Huazhong University of Science and Technology , Wuhan , China.</note>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianming</forename><surname>Yong</surname></persName>
							<affiliation key="aff5">
								<note type="raw_affiliation"><label>6</label> School of Business , University of Southern Queensland , Toowoomba , Australia.</note>
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">University of Southern Queensland</orgName>
								<address>
									<settlement>Toowoomba</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">†</forename><forename type="middle">Xiaohui</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main" xml:id="_FTYdjvy">AI-driven multi-agent reinforcement learning framework for real-time monitoring of physiological signals in stress and depression contexts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A6B2345BD2C651D15383C3FB00575AAA</idno>
					<idno type="DOI">10.1186/s40708-025-00262-1</idno>
					<note type="submission">Received: 17 January 2025 Accepted: 25 May 2025</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T06:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_VQ7wurG">Behavior patterns</term>
					<term xml:id="_taUSneV">Decision making</term>
					<term xml:id="_BJxnT2P">Patient monitoring</term>
					<term xml:id="_Ej4938S">Reinforcement learning</term>
					<term xml:id="_qrAey5h">Vital signs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_bGRtUtA"><p xml:id="_JdfcjWy"><s xml:id="_fjQnJDs">Purpose Effective patient monitoring is crucial for timely healthcare interventions and improved outcomes, especially in managing conditions influenced by stress and depression, which can manifest through physiological changes.</s><s xml:id="_XCh3yv8">Traditional monitoring systems often struggle with the complexity and dynamic nature of such conditions, leading to delays in identifying critical scenarios.</s><s xml:id="_5QugnRm">This study proposes a novel multi-agent deep reinforcement learning (DRL) framework to address these challenges by monitoring vital signs and providing real-time decision-making capabilities.</s></p><p xml:id="_7GUMpxh"><s xml:id="_RmTFeru">Methods Our framework deploys multiple learning agents, each dedicated to monitoring specific physiological features such as heart rate, respiration, and temperature.</s><s xml:id="_vWJDrQG">These agents interact with a generic healthcare monitoring environment, learn patients' behavior patterns, and estimate the level of emergency to alert Medical Emergency Teams (METs) accordingly.</s><s xml:id="_7HAmXTV">The study evaluates the proposed system using two real-world datasets-PPG-DaLiA and WESAD-designed to capture physiological and stress-related data.</s><s xml:id="_a5CuArA">The performance is compared with baseline models, including Q-Learning, PPO, Actor-Critic, Double DQN, and DDPG, as well as existing monitoring frameworks like WISEML and CA-MAQL.</s><s xml:id="_v3xDX46">Hyperparameter optimization is also performed to fine-tune learning rates and discount factors.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RJMh6aB">Results</head><p xml:id="_2rBCcQS"><s xml:id="_zSgdBRh">Experimental results demonstrate that the proposed multi-agent DRL framework outperforms baseline models in accurately monitoring patients' vital signs under stress and varying conditions.</s><s xml:id="_3kyK67X">The optimized agents adapt effectively to dynamic environments, ensuring timely detection of critical health deviations.</s><s xml:id="_6FqAFPk">Comparative evaluations reveal superior performance in metrics related to decision-making accuracy and response efficiency, highlighting the robustness of the framework.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zHtGWub">Conclusions</head><p xml:id="_TGpySYr"><s xml:id="_5DMfBBp">The proposed AI-driven monitoring system offers significant advancements over traditional methods by handling complex and uncertain environments, adapting to varying patient conditions influenced by stress and depression, and making autonomous, real-time decisions.</s><s xml:id="_Mkd7uK7">While the framework demonstrates high accuracy and adaptability, challenges related to data scale and future vital sign prediction remain.</s><s xml:id="_bvVXKSx">Future research will focus on extending predictive capabilities to further enhance proactive healthcare interventions.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_9fxFvkw">Introduction</head><p xml:id="_JdHvP4f"><s xml:id="_kuNuuR8">Mental health disorders, particularly depression and stress, are among the most pervasive global health challenges today, significantly impacting individuals' wellbeing and productivity <ref type="bibr" target="#b1">[1]</ref>.</s><s xml:id="_3PErRqT">These conditions, often referred to as"silent epidemics,"require early detection and timely interventions to mitigate their effects.</s><s xml:id="_fu7cTvD">However, traditional approaches to mental health management have often been reactive, addressing symptoms only after they manifest significantly.</s><s xml:id="_BN3Y63v">This underscores the urgent need for proactive monitoring and assessment frameworks that can identify early warning signs, enabling clinicians to intervene effectively before the conditions escalate.</s></p><p xml:id="_GKbhDCa"><s xml:id="_WvzZUKF">The physiological and behavioral manifestations of depression and stress, such as changes in heart rate, respiration patterns, and body temperature, provide measurable indicators of an individual's mental health state <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>.</s><s xml:id="_Y3E2UXd">Modern advancements in wearable technology and Internet of Things (IoT)-enabled systems now make it possible to continuously monitor these indicators in real time, presenting new opportunities to enhance mental health care.</s><s xml:id="_puNHGDx">However, the challenge lies in effectively analyzing the complex, multi-dimensional data generated by these systems and deriving actionable insights to guide clinical decisions.</s></p><p xml:id="_kz75UXr"><s xml:id="_ybEjjAK">Traditional machine learning (ML) techniques have been extensively employed in this domain to classify physiological signals, identify patterns, and predict health outcomes <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>.</s><s xml:id="_HCEJfTd">These methods have laid a strong foundation for developing monitoring frameworks but are inherently limited by their reliance on static models that do not adapt to changing conditions or learn dynamically from ongoing data streams.</s><s xml:id="_FXCQBBH">They are primarily observational, suggesting potential courses of action without the ability to autonomously adapt or act in response to the observed patterns.</s></p><p xml:id="_v8Yf9vN"><s xml:id="_MT7Jmq4">Reinforcement Learning (RL) represents a paradigm shift in this context by enabling autonomous agents to actively interact with their environment, learn from feedback, and optimize their actions to achieve predefined objectives <ref type="bibr" target="#b6">[6]</ref>.</s><s xml:id="_Zyhseb2">Unlike traditional ML models, RL agents leverage a reward-driven approach where each action taken by the agent is evaluated through a reward mechanism that reinforces favorable behaviors and discourages undesirable ones.</s><s xml:id="_EnVUHPk">This iterative learning process allows RL agents to adapt dynamically to complex, uncertain, and ever-evolving environments, making RL an ideal candidate for healthcare applications that require precision, adaptability, and responsiveness.</s></p><p xml:id="_q5XzTVR"><s xml:id="_TEAgRz7">RL has already demonstrated its potential in various domains, including dynamic treatment optimization, diagnostic decision-making, and medication scheduling <ref type="bibr">[7]</ref><ref type="bibr">[8]</ref><ref type="bibr" target="#b9">[9]</ref>.</s><s xml:id="_hwR4Rrq">For instance, RL algorithms have been used to optimize the timing and dosage of medications, ensuring that treatments are administered at the most effective intervals.</s><s xml:id="_55FETNd">The analogy of RL agents acting as virtual clinicians, continuously monitoring a patient's state and making decisions based on observed changes, highlights the transformative potential of this technology in healthcare <ref type="bibr" target="#b10">[10]</ref>.</s></p><p xml:id="_yET5JWH"><s xml:id="_z5T8HKV">In this study, we propose a novel monitoring framework that utilizes multi-agent Deep Reinforcement Learning (DRL) to address the complexities associated with monitoring depression and stress.</s><s xml:id="_qaqSvP5">The framework is designed to analyze and interpret real-time physiological data, enabling clinicians to detect deviations from normal patterns and respond proactively.</s><s xml:id="_ASTMtPV">Each DRL agent is dedicated to monitoring a specific physiological parameter, such as heart rate, respiration rate, or body temperature, and learns optimal thresholds based on Modified Early Warning Scores (MEWS) <ref type="bibr" target="#b11">[11]</ref>.</s><s xml:id="_KA87HEE">By introducing a clinically-informed reward mechanism, the framework enables these agents to continuously refine their decision-making capabilities, ensuring timely and accurate alerts to medical teams (Fig. <ref type="figure">1</ref>).</s></p><p xml:id="_2BqtbVT"><s xml:id="_A7bh93S">The proposed framework represents a significant advancement over traditional RL models by employing a multi-agent architecture that allows simultaneous monitoring of multiple vital signs.</s><s xml:id="_JJ3NGDn">This distributed approach enhances the system's scalability, enabling it to handle the complexities of real-world healthcare scenarios where multiple parameters must be monitored concurrently.</s><s xml:id="_emxhK6n">Furthermore, the novel reward system ensures that the agents are aligned with clinically relevant objectives, optimizing their behavior to support timely medical interventions.</s></p><p xml:id="_2eveK8Y"><s xml:id="_mn2qaqM">The contributions of this study are summarized as follows:</s></p><p xml:id="_wEkHqzc"><s xml:id="_HKJHXUH">• Introduction of a clinically-informed reward mechanism tailored to support RL agents in learning behavior patterns indicative of depression and stress.</s></p><p xml:id="_sdJA6QE"><s xml:id="_Hq3F7PZ">• Development of a generic, multi-agent monitoring environment that enables simultaneous tracking of various physiological parameters.</s><s xml:id="_z8JVqbU">• Establishment of a novel paradigm for remote monitoring of mental health conditions, leveraging multiagent DRL to provide actionable insights in real-time.</s><s xml:id="_zWhhHWB">The rest of this paper is organized as follows: Sect. 2 reviews related literature on RL applications in healthcare and mental health monitoring.</s><s xml:id="_PCg6PR9">Section 3 provides a detailed description of the research problem, technical background, and proposed methodology.</s><s xml:id="_qZWYG4w">Experimental setup and evaluation metrics are discussed in Sect.</s><s xml:id="_VRRgSgK">4, followed by an analysis of the results and insights in Sect. 5. Applications and implications are discussed in Sect.</s><s xml:id="_CJvdAHc">6, and Sect.</s><s xml:id="_uWC3WtQ">7 concludes the paper by outlining limitations and future directions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_9jhAtcc">Related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_8tVV2G3">Machine learning in healthcare</head><p xml:id="_hkTPbQZ"><s xml:id="_cTDUydy">Machine learning (ML) has transformed healthcare by providing predictive, diagnostic, and monitoring solutions across various domains <ref type="bibr" target="#b12">[12]</ref>.</s><s xml:id="_P8Uegb8">Supervised learning algorithms, in particular, leverage labeled datasets to make predictions and classifications based on input features <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>.</s><s xml:id="_Dx6jAnk">For instance, ML and deep learning techniques have been employed to predict vital signs like heart rate and classify physical activities <ref type="bibr" target="#b15">[15]</ref>.</s><s xml:id="_zAzdjRN">In the context of mental health, ML models have demonstrated efficacy in detecting stress and depression through the analysis of physiological and behavioral data <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>.</s><s xml:id="_uWbajEu">Stress and depression are critical public health concerns, often linked to chronic conditions like cardiovascular disease and diabetes.</s><s xml:id="_zmyRp6t">Early detection of these conditions can significantly improve outcomes by facilitating timely interventions.</s></p><p xml:id="_jKVk8pA"><s xml:id="_BvtQRaT">Oyeleye et al. <ref type="bibr" target="#b18">[18]</ref> investigated ML and deep learning models to estimate heart rate using wearable devices, comparing various regression algorithms including linear regression, k-nearest neighbor (kNN), decision tree, and LSTM.</s><s xml:id="_8pqtVzh">Similarly, Luo et al. <ref type="bibr" target="#b19">[19]</ref> utilized LSTM models to predict heart rate by integrating factors such as gender, age, physical activity <ref type="bibr" target="#b20">[20]</ref>, and mental state, highlighting the relevance of mental well-being in monitoring overall health.</s></p><p xml:id="_U3YuHA2"><s xml:id="_nWJDHsp">Unsupervised learning algorithms further contribute by deriving patterns from unlabeled data, employing clustering and association techniques <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>.</s><s xml:id="_KG3MwF9">Sheng and Huber <ref type="bibr" target="#b21">[21]</ref> proposed an encoder-decoder framework to cluster physical activity data, achieving high accuracy by learning behavioral embeddings.</s><s xml:id="_BYCGvVW">Despite their strengths, these traditional ML techniques face limitations in dynamically adapting to uncertain environments or integrating diverse data sources.</s></p><p xml:id="_2QZTMuw"><s xml:id="_tusZ7Vf">Reinforcement Learning (RL) addresses these gaps by enabling systems to learn through interaction with their environment <ref type="bibr" target="#b23">[23]</ref>.</s><s xml:id="_nm6KVuH">Unlike supervised approaches, RL relies on rewards or penalties to optimize decision-making processes, making it particularly suited for real-time and sequential decision-making tasks <ref type="bibr" target="#b24">[24]</ref>.</s><s xml:id="_mXncCdr">This capability is critical for monitoring complex conditions like stress and depression, where continuous data-driven interventions can prevent deterioration.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_QHdQ6TZ">Mimicking human behavior patterns</head><p xml:id="_cxWhbC9"><s xml:id="_SghtZPX">Understanding human behavior is vital for developing personalized healthcare solutions, especially for stress and depression management.</s><s xml:id="_EN8qba2">Stressful events and depressive episodes often manifest through changes in physical activity, sleep patterns, and physiological responses <ref type="bibr" target="#b16">[16]</ref>.</s><s xml:id="_rHdGTw3">Tirumala et al. <ref type="bibr" target="#b25">[25]</ref> explored probabilistic trajectory models to analyze human movement and interactions, proposing a hierarchical reinforcement learning (HRL) framework for identifying behavior patterns.</s><s xml:id="_zJRn5qa">Janssen et al. <ref type="bibr">[26]</ref> extended this concept by segmenting complex biological behaviors into manageable subtasks using HRL, which organizes sequential actions into logical options.</s></p><p xml:id="_RTwDXbH"><s xml:id="_5Svm2Ha">Tsiakas et al. <ref type="bibr" target="#b26">[27]</ref> proposed a human-centric cyberphysical systems (CPS) framework for personalized human-robot collaboration and training, which focused on minimally intrusive methods to predict human attention.</s><s xml:id="_mGG7c68">Similarly, Kubota et al. <ref type="bibr" target="#b27">[28]</ref> examined robots' Fig. <ref type="figure">1</ref> Human monitoring framework for tracking vital signs and alerting medical teams in emergencies adaptability to cognitive impairments, exploring therapeutic and assistive applications.</s><s xml:id="_EAuEXWg">Such frameworks emphasize the importance of understanding both highlevel behaviors (e.g., emotional and cognitive states) and low-level behaviors (e.g., speech, gestures, and physiological signals) <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_Crd5Wch">This research forms the foundation for developing systems that can effectively address mental health challenges like stress and depression.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_rUKtWTs">Reinforcement learning in healthcare</head><p xml:id="_BDVtDgs"><s xml:id="_anqFfhm">Reinforcement Learning (RL) has emerged as a transformative tool in healthcare for its ability to handle complex, dynamic, and uncertain environments.</s><s xml:id="_4crHRWE">Lisowska et al. <ref type="bibr" target="#b29">[30]</ref> demonstrated how RL could optimize the timing of interventions for cancer patients, employing models such as Deep Q-Learning (DQL), Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO) to develop virtual coaches for personalized prompts.</s><s xml:id="_spyU2pf">Personalized interventions, including messaging for diabetes patients, have shown efficacy in increasing physical activity and improving mental health <ref type="bibr" target="#b30">[31]</ref>.</s></p><p xml:id="_dj9MMkT"><s xml:id="_qVFk9cK">Li et al. <ref type="bibr" target="#b31">[32]</ref> leveraged RL to analyze electronic health records (EHRs) for sequential decision-making, employing a model-free Deep Q-Networks (DQN) algorithm for clinical decision support.</s><s xml:id="_2TjzJYq">Guo et al. <ref type="bibr" target="#b32">[33]</ref> proposed a dynamic weight assignment network inspired by advanced RL algorithms, demonstrating its application in human activity recognition.</s><s xml:id="_2cGKYdQ">RL's ability to integrate multi-agent frameworks further enhances its potential for mental health monitoring by enabling concurrent learning across multiple parameters.</s><s xml:id="_PPzWDqd">Despite RL's success in areas like gaming and assistive robotics, its deployment in healthcare, especially for mental health conditions, poses unique challenges.</s><s xml:id="_sbXABYr">Traditional approaches struggle with the safety and uncertainty inherent in dynamic healthcare environments.</s><s xml:id="_cYfCQTd">Stress and depression monitoring, for example, require systems that can adapt to fluctuating physiological and behavioral data.</s><s xml:id="_Nw4UdcM">Our study introduces a multi-agent reinforcement learning (MARL) framework designed specifically for these challenges.</s><s xml:id="_Yx5Uaqq">Unlike single-agent systems, MARL allows for concurrent monitoring of multiple physiological parameters, each modeled by a specialized agent.</s><s xml:id="_H8nbBVM">This framework is particularly suited for stress and depression monitoring, where indicators such as heart rate variability, sleep disruptions, and activity levels must be continuously analyzed.</s></p><p xml:id="_WEZcbna"><s xml:id="_Q6N64Xn">By incorporating a clinically-informed reward mechanism, our MARL framework aligns agent behavior with healthcare objectives, ensuring timely interventions.</s><s xml:id="_gqnRjpd">This approach not only addresses safety concerns but also enhances the scalability and adaptability of mental health monitoring systems, providing a novel contribution to AI-driven healthcare.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_rDpF3Qq">DRL monitoring framework</head><p xml:id="_Aaep9Ja"><s xml:id="_C2hFAPp">In this section, the design of a human behavior monitoring system, DRL monitoring framework, that uses R is presented in detail.</s><s xml:id="_eestbN8">The aim of the system is to monitor vital signs to learn human behavior patterns and ensure clinical safety in an uncertain environment.</s><s xml:id="_YGy2huV">The proposed framework involves a multi-agent system where each vital sign state is observed by an individual agent, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. A DRL algorithm, DQN, is used to learn effective strategies in the sequential decision-making process without prior knowledge through trial-and-error interactions with the environment <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_Ub3RVHh">Technical background</head><p xml:id="_sbQ7Veb"><s xml:id="_SJaFwDh">The challenge addressed in this research is the development of a multi-agent framework for real-time health status monitoring by learning and interpreting patterns in vital signs through wearable sensors.</s><s xml:id="_HHJYxe6">The agents must detect deviations from normal vital sign patterns that exceed Modified Early Warning Scores (MEWS) thresholds and alert the emergency team accordingly.</s></p><p xml:id="_fQSG9KG"><s xml:id="_HFexvR3">To formulate this problem, we leverage the framework of Markov Decision Processes (MDP), expressed as a 5-tuple M = (S, A, P, R, γ ) .</s><s xml:id="_dRFBSsT">Here, S represents the finite state space, where each state s t ∈ S corresponds to a distinct combination of vital sign readings at time t.</s><s xml:id="_WamNYeC">The action set A comprises potential alerts the agents can issue based on the observed vital signs.</s><s xml:id="_SrpA5dp">The transition function P(s, a, s ′ ) models the probability of moving from state s to state s ′ upon taking action a, reflecting the dynamic nature of human vital signs.</s></p><p xml:id="_Dv8qU4N"><s xml:id="_wGVqyJu">Central to our approach is the reward function R(s, a), which is defined to prioritize actions that lead to the early detection of potential health risks, thereby enabling timely intervention.</s><s xml:id="_k8zqPUF">This is mathematically represented as:</s></p><p xml:id="_V75X6Kn"><s xml:id="_Z4Gcq5x">where γ is the discount factor that balances the impor- tance of immediate versus future rewards, ensuring the agents' actions are aligned with long-term health monitoring objectives.</s></p><p xml:id="_Tj9sNm3"><s xml:id="_rrnqTJF">The goal is to discover an optimal policy π(s t ) that maximizes the expected reward by selecting the most appropriate action a t in any given state s t .</s><s xml:id="_8rtZFCt">This optimi- zation is achieved through the iterative update of the Q-function, as outlined in the Bellman equation:</s></p><formula xml:id="formula_0">(1) R(s t , a t ) = ∞ t=0 γ t r t ,</formula><p xml:id="_F44aETv"><s xml:id="_AgcrsRE">where α represents the learning rate, influencing the inte- gration of new information into the Q-function.</s><s xml:id="_SCzvgmG">Through this process, the agents continually refine their decisionmaking strategy, enhancing the system's capability to monitor and respond to emerging health risks effectively.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_cZupTAX">Monitoring environment</head><p xml:id="_2zHP2ph"><s xml:id="_US62beA">A custom RL monitoring system based on MDP has been created to have human vital signs data serve as the observation space S, action space A for learning agents to make decisions, and rewards R for the agents' actions as depicted in Fig. <ref type="figure" target="#fig_0">2</ref>.</s><s xml:id="_N3fJ886">This study introduces a novel isolated multi-agent MDP framework that allows multi-agents to share the same environment and make decisions based on the health parameters they are monitoring, receiving</s></p><formula xml:id="formula_1">Q new (s t , a t ) ← (1 -α)Q(s t , a t ) + α r t + γ max a Q(s t+1 , a) ,<label>(2)</label></formula><p xml:id="_mt29CyH"><s xml:id="_7XK9rwk">rewards without being influenced by the decisions of other agents.</s><s xml:id="_ZrvGvYN">The goal of all agents in this environment is to monitor the health of patients using the predefined MEWS, as shown in Tab. 1.</s><s xml:id="_emtHDB6">In healthcare, each vital sign plays a critical role in determining a person's clinical safety.</s></p><p xml:id="_JzxNBuT"><s xml:id="_ZJZ6Wyf">In the current framework, we have implemented three RL agents to monitor heart rate, respiration, and temperature.</s><s xml:id="_vyDg7Mb">These agents operate primarily in cooperative mode, sharing information about the patient's health status and working together to ensure timely interventions.</s><s xml:id="_U6RRudn">Cooperation allows the agents to pool rewards from collective actions, improving overall system learning.</s><s xml:id="_rN9n6cz">However, when multiple patients are being monitored or resource constraints arise (e.g., limited access to medical personnel), the agents may enter competitive mode.</s><s xml:id="_BrQsx2F">In this mode, agents prioritize the most critical health states and may compete for resources by adjusting the urgency of alerts based on the patient's condition.</s><s xml:id="_stcfpST">As the number of agents increases, the framework is designed to scale effectively.</s><s xml:id="_cEGdeWb">Each additional agent monitors new physiological parameters or additional patients, with the system adjusting the reward mechanism and communication strategy to maintain efficient performance.</s><s xml:id="_mncz2Kv">The system remains modular, enabling easy expansion without significantly impacting computational load or decision-making speed.</s><s xml:id="_UNtzy2f">Importantly, the system's ability to operate in both cooperative and competitive modes ensures flexibility, allowing it to adapt to various healthcare scenarios, including large-scale monitoring in hospitals.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1" xml:id="_bp2umZ8">Observation space</head><p xml:id="_TzMhepk"><s xml:id="_aUfR6e5">The environment in Fig. <ref type="figure" target="#fig_0">2</ref> has a state, represented by s i t ǫS , where i = 0, 1, 2, ...n , refers to observations at time t.</s><s xml:id="_MdtFVp8">The aim is to divide the state into observations and allocate them to multi-agents.</s><s xml:id="_YQb43B9">Suppose S represents the state of the human body, and there are three observations, s 0 t , s 1 t , s 2 t ǫS , that represent different internal vital signs of the human body at time t.</s><s xml:id="_qEnYazk">The human health status is controlled by multiple internal vital signs, each with a different threshold as shown in MEWS Tab. 1.</s><s xml:id="_8VaRFnp">Using a single agent to monitor all the vital signs can result in a sparse rewards challenge <ref type="bibr" target="#b16">[16]</ref>, where the environment might produce few useful rewards and hinders the learning of an agent.</s><s xml:id="_kq6M5rU">Therefore, multi-agents need to be deployed for each human to monitor the critical vital signs.</s><s xml:id="_R8hdhyr">The expected return E π of a policy π in a state s can be defined by state-value Eq. 3 in the multi-agent setting, where i = 0, 1, 2, 3, ...n is a finite number of observations n in the state.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2" xml:id="_uN7kDek">Action space</head><p xml:id="_mZQMehC"><s xml:id="_TtRMNSZ">The action space of the monitoring environment is defined based on the MEWS <ref type="bibr" target="#b35">[36]</ref> as shown in Tab. 1.</s><s xml:id="_cPBGW62">The table presents early warning scores of adults' vital signs with the appropriate Medical Emergency Team (MET) to contact if any escalations in the health parameters.</s><s xml:id="_EYpr4WM">Based on the MEWS as threshold values, the action space has been segmented to have five discrete actions to communicate the vital signs to MET-0, MET-1, MET-2, MET-3, and MET-4.</s><s xml:id="_Dh7Q3M4">Each of these actions will be taken by agents based on the current state of the vital signs they are monitoring.</s><s xml:id="_jBjR4kH">The expected return E π for taking an action a in a state s under a policy π can be measured using the action-value function Q π (s, a) defined in Eq. 4.</s></p><p xml:id="_S6jYpp7"><s xml:id="_NjBjkBG">(3)</s></p><formula xml:id="formula_2">V π (s i ) = E π ∞,n t=0,i=0 γ t R(s t , π(s t ))|s 0 0 = s</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3" xml:id="_XeeVpqV">Rewards</head><p xml:id="_f8fx7MD"><s xml:id="_79x7sQf">The reward policy is designed to incentivize accurate monitoring and timely alerts.</s><s xml:id="_pgp4K3W">Agents are positively rewarded for actions aligned with MEWS thresholds (Table <ref type="table" target="#tab_2">2</ref>), ensuring critical conditions like stress-induced hyperthermia or depression-related bradycardia are prioritized.</s><s xml:id="_qXsTXTg">Rewards are categorized for each action, as shown in Eq. 6.</s><s xml:id="_dWz36Ew">This encourages agents to maximize cumulative rewards and learn behavior patterns, crucial for addressing mental health risks.</s></p><p xml:id="_8rXeMBN"><s xml:id="_vyY7Gn5">The goal of RL is to maximize cumulative rewards obtained through the actions of learning agents in an environment.</s><s xml:id="_6NCYSsn">In traditional RL, an agent is rewarded based on its action that leads to a transition from state s t to s t+1 .</s><s xml:id="_jAx8Bhq">In this study, the objective of the learning agent is to learn patterns in human vital signs.</s><s xml:id="_geHdtVb">This is achieved through the design of an effective reward policy.</s><s xml:id="_BTQJyy7">The reward policy, as defined in this study, is calculated using Eq. 5.</s><s xml:id="_8PDFd4t">The agents are positively rewarded if they monitor vital signs in a state and take the correct action from the action space to communicate with the correct MET as defined in MEWS Tab. 1.</s><s xml:id="_Gc8Tkss">On the other hand, if the agent takes the wrong action, it is negatively rewarded.</s><s xml:id="_stApzz7">The rewards are split into five categories for the five actions in the action space based on the MET from MEWS Tab. 1.</s><s xml:id="_3aGTTcH">The full rewards for each action selected by the agents are presented in Tab. 2. The reward policy utilizes the DRL agents' desire to maximize rewards in each learning iteration, making them learn the behavior patterns.</s><s xml:id="_vg8tke3">Under each category, different levels of rewards were configured.</s><s xml:id="_ujRacwr">For example, an observation s 1 t ǫS at the time t is related to heart rate falling under MET-4, the rewards are shown in Eq. 6.</s></p><p xml:id="_MRExttw"><s xml:id="_mSW7ZSG">(4) Correctness Determination in Reward Design The clinically-informed reward mechanism in our framework is designed to reflect the accuracy of agent decisions with respect to established triage protocols.</s><s xml:id="_7E4yc5A">Each physiological observation is assigned a severity band based on the Modified Early Warning Scores (MEWS), which are widely used in clinical settings to determine escalation levels.</s><s xml:id="_fdPvEHu">If the agent selects the correct Medical Emergency Team (MET) level that corresponds to the MEWSderived threshold (for instance, selecting MET-3 when the heart rate exceeds 130 bpm), it receives a high positive reward (+10).</s><s xml:id="_ebaFmSz">In contrast, if the agent overestimates or underestimates the appropriate escalation level, it is penalized proportionally (e.g., -1 to-4) based on the deviation from the correct action.</s><s xml:id="_eHjh6Fm">This graded reward policy allows agents to learn both clinical accuracy and escalation sensitivity, supporting a balance between safety (avoiding false negatives) and efficiency (avoiding false positives).</s></p><formula xml:id="formula_3">Q π (s, a) = E π ∞ t=0 γ t R(s t , a t , π(s t ))|s 0 = s, a 0 = a (5) R(s t , a t ) = +reward if action = MET -reward if action � = MET</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_fH9UbSx">Learning agent</head><p xml:id="_G5hDeNp"><s xml:id="_rcYrbB3">In this study, a game learning agent DQN algorithm is employed.</s><s xml:id="_6dwpgBF">The DQN algorithm was first introduced by DeepMind, a subsidiary of Google, for playing Atari games.</s><s xml:id="_JhrwPQe">It allows the agent to play games by simply observing the screen, without any prior training or knowledge about the games.</s><s xml:id="_ACEM6mZ">The DQN algorithm approximates the Q-Learning function using neural networks, and the learning agent is rewarded based on the neural network's prediction of the best action for the current state.</s><s xml:id="_bwDRrMQ">For this research, the reward policy is described in more detail in Sect.</s><s xml:id="_3gfgXks">3.2.3.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1" xml:id="_Tryhkbw">Function approximation</head><p xml:id="_chbu9YD"><s xml:id="_TnAWg8B">The neural network used in this study to estimate the Q-values for each action has three layers: an input layer, a hidden layer, and an output layer.</s><s xml:id="_d6z4etJ">The input layer has a node for each vital sign in a state and the output layer has a node for each action in the action space.</s><s xml:id="_TgmNATb">The model is configured with a relu activation function, mean square error as the loss function, and an Adam optimizer.</s><s xml:id="_Hu84FMY">The model is trained on the states and their corresponding rewards and, once trained, it can predict the accumulated reward.</s></p><p xml:id="_NSEaryx"><s xml:id="_9xHz77Y">The learning agent takes an action a t ∈ A in a transi- tion from state s t to s ′ t and receives a reward R. In this transition, the maximum Q-function value is calculated</s></p><formula xml:id="formula_4">(6) R(s 1 t , a t ) =          10 if MET = 4&amp;action = 4 -1 if MET = 4&amp;action = 3 -2 if MET = 4&amp;action = 2 -3 if MET = 4&amp;action = 1 -4 if MET = 4&amp;action = 0</formula><p xml:id="_xF45eZs"><s xml:id="_H6QF5xs">according to Eq. 4, and the calculated value is discounted by a discount factor γ to prioritize immediate rewards over future rewards.</s><s xml:id="_rDqgHeG">The discounted future reward is combined with the current reward to obtain the target value.</s><s xml:id="_jCHt8dD">The difference between the prediction from the neural network and the target value forms the loss function, which is a measure of the deviation of the predicted value from the target value and can be estimated using Eq. 7. The square of the loss function penalizes the agent for large loss values.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2" xml:id="_CqP44vd">Memorize and replay</head><p xml:id="_qW5H5z4"><s xml:id="_kqskCus">The basic neural network model has a limitation in its memory capacity and can forget previous observations as they are overwritten by new observations.</s><s xml:id="_HtffySg">To mitigate this issue, a memory array that stores the previous observations including the current state s t , action a t , reward R, and next state s ′ t is used.</s><s xml:id="_v84mySw">This memory array enables the neural network to be retrained using the replay method, where a random sample of previous observations from the memory is selected for training.</s><s xml:id="_EWnHXUx">In this study, the neural network model was retained by using a batch size of 32 previous observations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3" xml:id="_FvwyhTR">Exploration and exploitation</head><p xml:id="_KSCZwFC"><s xml:id="_QHFTZNT">The exploration-exploitation trade-off in RL refers to the balancing act between trying out new actions to gather information and exploiting the actions that lead to the highest rewards.</s><s xml:id="_dFYV9rE">This balance can be modeled mathematically using the ǫ-greedy algorithm, which defines a probability ǫ of choosing a random action and a prob- ability 1 -ǫ of choosing the action believed to lead to the highest reward based on the current knowledge of the action-value function Q(s t , a) .</s><s xml:id="_eXrt63Y">The equation to determine the action taken at time t is as follows:</s></p><p xml:id="_KGPxUmx"><s xml:id="_KSy8UPn">where the greedy action is defined as:</s></p><p xml:id="_Uc7TXFG"><s xml:id="_cNgJmjV">The value of ǫ determines the level of exploration versus exploitation, with smaller values leading to more exploitation and larger values leading to more exploration.</s><s xml:id="_YUNwp6E">Over time, as the action-value function becomes more accurate, ǫ can be decreased to allow for more exploita- tion and convergence to the optimal policy.</s></p><p xml:id="_3hjCVp7"><s xml:id="_VTg9cFk">(7</s></p><formula xml:id="formula_5">) loss = (R + γ • max(Q π * (s, a)) target_value -Q π (s, a) predicted_value ) 2 (8) a t =</formula><p xml:id="_V3PK5qG"><s xml:id="_8fWWy3s">random(a t ) with probability ǫ greedy(a t ) with probability 1 -ǫ (9)</s></p><formula xml:id="formula_6">a t = arg max a Q(s t , a)</formula><p xml:id="_55e8xkm"><s xml:id="_CGxTuX8">In this study, we emphasize the importance of balancing exploration and exploitation for effective patient monitoring.</s><s xml:id="_CG2uaG7">Exploration allows agents to discover better monitoring strategies, while exploitation ensures timely alerts by acting on learned knowledge.</s><s xml:id="_sBSTPjA">Through empirical testing, we found that an exploration rate ǫ between 0.1 and 0.2 provided the optimal balance in our healthcare environment.</s><s xml:id="_xHFn3rm">This range ensured that agents could adapt to changing patient conditions while still providing timely and accurate interventions.</s><s xml:id="_tNwXdFd">In critical situations with frequent health deviations, a higher exploitation rate proved beneficial, whereas environments with fewer critical events required more exploration to discover new monitoring patterns.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4" xml:id="_nyAujyh">Hyper parameters</head><p xml:id="_MbQt266"><s xml:id="_YmvkRkS">Other than the parameters defined for the neural networks, a set of hyperparameters has to supply for the RL process.</s><s xml:id="_QUtbzMz">They are as follows:</s></p><p xml:id="_3npdR6B"><s xml:id="_agz32ZQ">• episodes ( M ): This is a gaming term that means the number of times an agent has to execute the learning process.</s><s xml:id="_DwGBGmh">• learning_rate(α ): Learning rate is to determine much information neural networks learn in an iteration.</s><s xml:id="_3ZxzRaq">• discount_factor(γ ): Discount factor ranges from 0 to 1 to limit future rewards and focus on immediate rewards.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_7FyCYfh">Algorithm 1 Multi-agents monitoring</head><p xml:id="_qErQY44"><s xml:id="_aFMta7Y">Algorithm 1 implements the proposed multi-agent human monitoring framework.</s><s xml:id="_EnQmk88">It takes as input a set of subjects C = 1, 2, . . .</s><s xml:id="_UcG4qMx">, C and a set of vital signs V = 1, 2, . . .</s><s xml:id="_rwZvsP6">, V , along with the number of episodes M = 1, 2, . . .</s><s xml:id="_VQsazGW">, M .</s><s xml:id="_6maMP9t">The algorithm outputs the rewards achieved by agents in each episode.</s><s xml:id="_xx44r6Z">Lines 1-2 initializes all the parameters needed for monitoring the environment and learning agent.</s><s xml:id="_uYtWumx">Lines 3-7 present the reward policy.</s><s xml:id="_EBT7gqq">Lines 8-14 present the function approximation using the neural networks model, memorize &amp; replay, exploration &amp; exploitation of the DRL agent.</s><s xml:id="_bdzR4Ez">Lines 15-28 are nested for loops with conditional statements to check if the episode is completed or not.</s><s xml:id="_hpqQFDe">The outer loop is to iterate each episode while resetting the environment to initial values and score to zero.</s><s xml:id="_pXDqC8r">The inner loop is to iterate timesteps which denote the time of the current state and calls the methods.</s></p><p xml:id="_TbmcgEp"><s xml:id="_sBRdhJn">The patient monitoring system operates with multiple agents, each tasked with monitoring specific vital signs such as heart rate, respiration rate, and temperature.</s><s xml:id="_GJaJdhZ">The agents are initialized with a basic action set, which includes triggering alerts, adjusting monitoring intervals, and taking no action based on the patient's condition.</s><s xml:id="_4GaMe5d">At each time step, agents receive vital sign data as input and evaluate the patient's state.</s><s xml:id="_SZbVxTM">Based on the current state and the agent's policy, an action is selected.</s><s xml:id="_frghvUA">The reward function provides feedback based on the timeliness and accuracy of the action: positive rewards are given for correct, timely interventions, while penalties are applied for false alarms or missed emergencies.</s><s xml:id="_FBXnpGp">Over time, the agents improve their performance through continuous learning and collaboration, ensuring that vital signs are monitored comprehensively and interventions are timely (Fig. <ref type="figure">3</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_pjzAVxZ">Experiment</head><p xml:id="_8tjBdPx"><s xml:id="_2egM9Dw">In this study, the proposed multi-agent framework was evaluated by deploying an agent for each physiological feature of a different set of subjects.</s><s xml:id="_eHNSjGq">The aim of the learning agents was to monitor their respective vital signs, communicate with the corresponding MET based on the estimated level of emergency, and learn the subjects' behavior patterns.</s><s xml:id="_EG7sr6j">All the experiments were conducted using Python programming language version 3.7.6 and related libraries such as TensorFlow, Keras, Open Gym AI, and stable_baselines3.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_9rJZ5uV">Dataset</head><p xml:id="_7UUx632"><s xml:id="_rMQq78b">• PPG-DaLiA <ref type="bibr" target="#b36">[37]</ref>: The dataset contains physiological and motion data of 15 subjects, recorded from both a wrist-worn device and a chest-worn device while the subjects were performing a wide range of activities under conditions close to real life.</s><s xml:id="_QVGhTAD">• WESAD <ref type="bibr" target="#b37">[38]</ref>: The WESAD (Wearable Stress and Affect Detection) dataset includes multimodal physiological signals such as ECG, PPG, GSR, respiration, and body temperature, recorded from 15 subjects while they performed a series of stress-inducing and affective tasks under laboratory conditions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_ZXdTZAT">Baseline models</head><p xml:id="_XYEGpFs"><s xml:id="_5GkkXrh">• WISEML <ref type="bibr" target="#b38">[39]</ref>: Mallozzi et al. proposed an RL framework for runtime monitoring to prevent dangerous and safety-critical actions in safety-critical applications.</s><s xml:id="_qQDz8QS">In this framework, runtime monitoring is used to enforce properties to the agent and shape its reward during learning.</s><s xml:id="_CSNmZsU">• CA-MQL [40]: Chen et al. proposed constrained action-based MQL (CA-MQL) for UAVs to autonomously make flight decisions that consider the uncertainty of the reference point location.</s><s xml:id="_ckAfHrB">• MADDPG [41]: Lowe et al. introduced a deep reinforcement learning framework for multi-agent environments.</s><s xml:id="_bjqwuyh">This framework uses an adaptation of actor-critic methods to coordinate agents in both 3 Experiemental Design cooperative and competitive settings by accounting for other agents' policies.</s><s xml:id="_PH2g6WD">It highlights the difficulty of traditional algorithms in multi-agent scenarios and introduces policy ensembles for more robust learning.</s><s xml:id="_8Cnvw3g">• QMIX [42]: Rashid et al. developed QMIX, a valuebased multi-agent RL algorithm that factors joint action-values into per-agent values, allowing for decentralised policies while training in a centralised manner.</s><s xml:id="_em5yUm3">QMIX demonstrated superior performance on challenging StarCraft II tasks by ensuring consistency between centralised and decentralised learning.</s><s xml:id="_dfGGrtt">• Existing RL baseline models by Li et al. [32] were deployed to optimize sequential treatment strategies based on Electronic Health Records (EHRs) for chronic diseases using DQN.</s><s xml:id="_949f7Fr">The multi-agent framework results were compared with Q-Learning and Double DQN.</s><s xml:id="_vtYNZum">• Similarly, RL was deployed to recognize human activity using a dynamic weight assignment network architecture with TD3 (a combination of Deep Deterministic Policy Gradient (DDPG), Actor-Critic, and DQN) by Guo et al. [33].</s><s xml:id="_K9vEa3J">• Yom et al. [31] used Advantage Actor-Critic (A2C)</s></p><p xml:id="_pbuePw7"><s xml:id="_KAa375R">and Proximal Policy Optimization (PPO) algorithms to act as virtual coaches in decision-making and send personalized messages.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_j5RAJkf">Performance measures</head><p xml:id="_AY6krc8"><s xml:id="_kR5K4Tw">In the initial phase, Cumulative Rewards were selected as the primary performance metric because they offer a direct reflection of the RL agents' success in achieving healthcare objectives.</s><s xml:id="_yQqbWzH">These cumulative rewards quantify the agents' ability to make correct decisions based on real-time physiological data, which is essential for ensuring timely medical interventions.</s><s xml:id="_DTCAkxt">Given the critical nature of healthcare systems, focusing on cumulative rewards allowed for the evaluation of how well the agents were trained to detect early signs of health deterioration.</s></p><p xml:id="_T65EcqT"><s xml:id="_aFTpc4B">To provide a more holistic evaluation, we introduced additional performance metrics:</s></p><p xml:id="_pbc5525"><s xml:id="_xbNrF42">• Learning Rate: This metric evaluates how quickly the agents converge to an optimal policy, which is vital in healthcare applications where rapid adaptation to changing patient conditions is crucial.</s><s xml:id="_P536Fxd">Faster learning ensures that the agents can respond to emergencies in real time, improving the effectiveness of the system.</s><s xml:id="_QZt3gjD">• Computational Complexity: This metric assesses the system's processing demands, particularly in terms of CPU/GPU time.</s><s xml:id="_Q9WGEHy">Minimizing computational complexity is essential in healthcare settings with resource constraints, such as hospitals or wearable monitoring devices.</s><s xml:id="_b8ne9Yw">Lower complexity ensures that the system can run efficiently without causing delays in decision-making.</s><s xml:id="_awkHdwA">• Memory Usage: As the system scales to monitor multiple physiological parameters across various agents, memory usage becomes a key factor.</s><s xml:id="_QnTE9c9">Efficient memory utilization is critical for deploying the framework on resource-constrained devices like wearables, ensuring scalability and adaptability without compromising performance.</s></p><p xml:id="_4ur4ZBr"><s xml:id="_5Qd6BYT">Incorporating these metrics provides a more comprehensive evaluation of the proposed framework, ensuring not only its effectiveness in terms of rewards but also its efficiency, scalability, and real-world deployment potential in healthcare environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_KSR8UU6">Experiment results and analysis</head><p xml:id="_SPey8y9"><s xml:id="_5vtt7Fr">The advantage of RL for monitoring systems is that it can learn to handle complex, dynamic environments.</s><s xml:id="_PaE2JSw">Many monitoring tasks involve making decisions based on incomplete, uncertain information, and the optimal decision may depend on the context of the situation <ref type="bibr" target="#b42">[43]</ref>.</s><s xml:id="_cMEmv9b">RL can learn to make decisions in these types of problems by considering the current state of the system and past experience.</s><s xml:id="_MnauvKu">In this study, the aim is to leverage the RL capability to optimize the decision-making process in patient monitoring.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_9KMtCza">DRL agents performance</head><p xml:id="_EWMWBFu"><s xml:id="_zjz3pRY">The performance of the proposed DRL framework was evaluated using two datasets, with a focus on cumulative rewards, learning rate, computational complexity, and memory usage.</s><s xml:id="_AR9XHVd">Additionally, we expanded our comparison to include multi-agent RL frameworks, MADDPG and QMIX, to assess how well these frameworks handle the complexities of real-time health monitoring tasks.</s></p><p xml:id="_AGMz84J"><s xml:id="_E7EvqN6">The results are summarized in Tab. 3, which includes the performance of single-agent RL methods (Q-Learning, PPO, A2C, and DDPG) and multi-agent RL models (MADDPG and QMIX).</s><s xml:id="_3qFBGW4">The proposed DRL framework consistently outperforms all other models in terms of cumulative rewards, with significant improvements over the baseline methods.</s></p><p xml:id="_XnxKX3G"><s xml:id="_u6b6cKe">As shown in Tab. 3, the proposed DRL framework surpasses both MADDPG and QMIX in cumulative rewards for both datasets, particularly excelling in agent 1's performance on the PPG-DaLia dataset.</s><s xml:id="_xkHZS7z">This indicates that our framework's design, which includes a tailored reward mechanism based on Modified Early Warning Scores (MEWS), enables more efficient learning in healthcare environments.</s><s xml:id="_ewdG4M5">Additionally, the exploration-exploitation trade-off in our system is better optimized for the variability of physiological data.</s></p><p xml:id="_nHd2gcZ"><s xml:id="_6fxH9hr">To provide a more intuitive assessment of the framework's performance, we evaluated the classification accuracy of the agents by comparing their actions against MEWS-derived ground truth escalation levels.</s><s xml:id="_xURngmn">Accuracy was computed as the ratio of correct escalation actions (e.g., MET-2 chosen when MEWS score corresponds to MET-2) to the total number of decisions made across episodes.</s><s xml:id="_z24rU8y">This metric offers a clinically relevant view of agent performance, especially for practitioners accustomed to discrete outcome measures.</s><s xml:id="_KtAtjT4">The proposed DRL agents achieved an average decision accuracy of 88.3% across all episodes and subjects, outperforming baseline models such as PPO (79.1%),</s><s xml:id="_JVtDMgz">A2C (76.4%), and Double DQN (81.6%).</s><s xml:id="_AGFqgxA">These results demonstrate that the agents not only maximize cumulative rewards but also maintain high decision accuracy in real-time physiological monitoring tasks.</s></p><p xml:id="_wXmfsv6"><s xml:id="_XtCwp5C">Beyond cumulative rewards, we evaluated the proposed DRL framework against baseline models using additional performance metrics, including learning rate, computational complexity, and memory usage, as shown in Tab. 4. The proposed DRL framework showed superior performance across all these metrics, indicating its suitability for real-time applications in resource-constrained healthcare environments.</s></p><p xml:id="_E9J966M"><s xml:id="_6AxQBmb">In terms of learning rate, the proposed DRL framework converged after 850 epochs, outperforming all baseline models, including Q-Learning (1200 epochs) and Double DQN (1100 epochs).</s><s xml:id="_sq3SkWS">This faster convergence demonstrates the DRL framework's enhanced efficiency in learning complex healthcare scenarios.</s><s xml:id="_8S8PCjz">Faster learning is especially critical in healthcare, where timely interventions directly impact patient outcomes.</s><s xml:id="_hC2ZrFM">The use of multiple agents, each dedicated to a specific physiological metric, accelerates policy optimization and enhances responsiveness in dynamic, real-world environments.</s></p><p xml:id="_gC3n39S"><s xml:id="_NEg6rGP">For computational complexity, the proposed DRL framework exhibited a significantly lower iteration time of 0.70 s, outperforming more complex multi-agent models like CA-MQL (1.30 s) and PPO (1.10 s).</s><s xml:id="_KpwRZBK">This indicates that the framework is computationally efficient, making it ideal for real-time healthcare monitoring where decision delays could compromise patient safety.</s><s xml:id="_hfq38p3">This improved efficiency is due to an optimized reward structure and action space, which reduces the time required for decision-making without compromising accuracy.</s></p><p xml:id="_4qjgSkn"><s xml:id="_c2wFtuV">In terms of memory usage, the DRL framework consumed 110MB, which is lower than all other baseline models, such as DDPG (160MB) and CA-MQL (175MB).</s><s xml:id="_h7kyA4v">This low memory footprint is crucial for deploying the framework on resource-constrained hardware like wearable devices or low-power hospital systems.</s><s xml:id="_7y6pEF5">The efficient memory usage ensures the system can scale with additional agents without overloading system resources, making the framework suitable for large-scale healthcare applications.</s></p><p xml:id="_DNnhy7C"><s xml:id="_HQaGDuf">All three learning agents were fed with physiological features such as heart rate, respiration, and temperature, respectively, from the PPG-DaLiA dataset.</s><s xml:id="_4AzdrFb">Based on the observation space, action space, and reward policy defined for a customized gym environment for human behavior monitoring, the learning agents were run for 10 episodes, as shown in Fig. <ref type="figure">4</ref>. In the results, agent 1 refers to the heart rate monitoring agent, which showed a constant increase in scores for each episode for most of the subjects except subjects 5 and 6.</s><s xml:id="_WNKgyPw">The intermittent low scores in agent 1 performance are due to the exploration rate in DQN learning, where the algorithm tries exploring all the actions randomly instead of relying on neural networks' predictions.</s><s xml:id="_g2RBjH7">Similarly, agent 2 and agent 3 monitor two other physiological features, respiration and temperature, respectively.</s><s xml:id="_JYX482x">agent 2 performed better than the other two agents and achieved consistent scores for all subjects.</s><s xml:id="_c9rwFG3">Out of all agents, agent 3, temperature monitoring performance, was poor.</s><s xml:id="_9W4dzyY">This issue was traced back to the data level, where the units of the temperature thresholds in the MEWS table and the input body temperature data from the dataset were different.</s><s xml:id="_UeQfapU">Still, agent 3 achieved high scores in monitoring subjects 9, 8, 4, and 10.</s></p><p xml:id="_hUCsdvv"><s xml:id="_y8FsN5F">The reward policy designed in the proposed multiagent framework enables agents to learn the human physiological feature patterns.</s><s xml:id="_gKVQJwH">For example, if a subject's heart rate is 139 beats per minute, agent 1 takes Action 3 to communicate the message to MET-3.</s><s xml:id="_A8mCY7u">The agent will get rewarded with +10 points only if Action 3 is taken; otherwise, the agent gets negatively rewarded according to the reward policy (Table <ref type="table" target="#tab_2">2</ref>).</s><s xml:id="_9V63jYg">With this example, the results in Fig. <ref type="figure">4</ref> can be interpreted better.</s><s xml:id="_Qd4YrnJ">An increase in scores episode by episode, with the exception of the exploration rate, actually infers an increase in the learning curve of the agents in terms of human physiological patterns.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_A7JCqVr">Hyper-parameters optimization</head><p xml:id="_53BuQek"><s xml:id="_EGdMSvK">The DRL agents were further evaluated by hyperparameters optimization.</s><s xml:id="_w89dAZq">Out of all the hyperparameters discussed in this study, two hyperparameters, learning rate ( α ) and discount factor ( γ ), were optimized for all three agents, and the results are shown in Figs. <ref type="figure">5</ref> and <ref type="figure">6</ref>.</s><s xml:id="_DrzNP2C">The learning rate determines how much information neural networks learn in an iteration to predict action and approximate the rewards.</s><s xml:id="_faftZ5e">The discount factor measures how much RL agents focus on future rewards relative to those in the immediate rewards.</s><s xml:id="_dZ6PKNJ">In Fig. <ref type="figure">5</ref>, Fig. <ref type="figure">5a</ref> show the agents' performance while optimizing α of neural net- works.</s><s xml:id="_bVnvvcv">The x-axis of the plots represents scores (cumulative rewards) achieved by an agent in each episode shown on the y-axis.</s><s xml:id="_NCm4W7r">The bar plots show that the learning rate α = 0.01 is a more optimized value in all the monitoring agents.</s><s xml:id="_3KQgQfp">Similarly, Figs.</s><s xml:id="_SXbXJHu">6a present the γ optimization of agent 1, agent 2, and agent 3, respectively.</s><s xml:id="_kecxH8z">The discount factors γ = 0.9 and γ = 0.75 are the more optimized values for agents 2 and 3, respectively, after 10 episodes of training.</s></p><p xml:id="_jyHTshR"><s xml:id="_UyYP8KV">Convergence Visualization and Hyperparameter Effectiveness.</s><s xml:id="_7Xubk28">To provide a clearer view of how different hyperparameters affect model performance and convergence speed, we conducted additional experiments and visualized the episode-wise cumulative rewards under varying values of learning rate ( α ) and discount factor ( γ ).</s><s xml:id="_rT9M8aH">As shown in Figs. <ref type="figure">5</ref> and <ref type="figure">6</ref>, learning rate α = 0.01 led to faster and more stable convergence compared to higher or lower values, which either caused slower learning or higher variance across episodes.</s><s xml:id="_VwDtDxS">Similarly, γ = 0.9 4 DQN Agents Performance resulted in optimal long-term reward accumulation, balancing future reward consideration with immediate decision-making.</s><s xml:id="_Df6JbaF">These visualizations offer intuitive insights into the convergence dynamics of the proposed DRL framework and reinforce our hyperparameter selection strategy.</s></p><p xml:id="_BM3g8Sf"><s xml:id="_qbWzSmc">Clinical Relevance of Cumulative Rewards The cumulative rewards obtained by the DRL agents are not arbitrary metrics but are directly linked to the agents' ability to make timely and clinically relevant decisions.</s><s xml:id="_pm6nSWp">Each reward is assigned based on how well an agent's action aligns with the MEWS-defined threshold for a given vital sign.</s><s xml:id="_cHvxjSR">For instance, if an agent detects an elevated heart rate indicative of stress-induced tachycardia and correctly escalates the condition to the appropriate MET level, it receives a positive reward.</s><s xml:id="_DyvSmmX">Conversely, a delayed or incorrect escalation results in a penalty.</s><s xml:id="_mpg2qvw">Over time, higher cumulative rewards indicate that the agents are successfully learning to respond to physiological deviations in ways that mirror clinical priorities.</s><s xml:id="_G8jfKCg">Thus, cumulative rewards in this framework serve as a quantitative proxy for the agents' effectiveness in the proactive monitoring and assessment of stress-and depression-linked health indicators.</s></p><p xml:id="_6DSkZVy"><s xml:id="_4KxuZAC">Generalization Across Heterogeneous Conditions The ability to generalize across varying physiological patterns is essential for any real-world stress and depression monitoring system.</s><s xml:id="_C5TxabU">While this study uses Modified Early Warning Scores (MEWS) to establish clinically informed reward boundaries, the reinforcement learning agents are not bound by fixed rules.</s><s xml:id="_JccXGyx">Instead, they learn adaptive policies by interacting with dynamically evolving input states.</s><s xml:id="_mYyB9VQ">To assess generalization, we employed two publicly available and heterogeneous datasets-PPG-DaLiA and WESAD-which differ in sensor configurations, experimental settings, and stress elicitation protocols.</s></p><p xml:id="_MR5GmmK"><s xml:id="_7ZzSAyE">The consistent performance of our DRL agents across both datasets suggests promising generalizability.</s><s xml:id="_SzKjYVC">However, we acknowledge that additional validation on datasets encompassing richer behavioral modalities and more diverse populations is necessary to further substantiate this claim.</s><s xml:id="_kRHmTyp">Future extensions will focus on integrating multimodal data sources and deploying the framework in cross-domain learning environments to evaluate transferability and robustness under real-world conditions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_Z6E9ge2">Discussion</head><p xml:id="_rrtWqV6"><s xml:id="_kf5AUjd">This study introduces an innovative approach to patient monitoring within the unpredictable environment of healthcare settings, employing adaptive multi-agent deep reinforcement learning (DRL) to ensure timely healthcare interventions.</s><s xml:id="_FnUbNJG">The fluctuating nature of vital signs, crucial indicators of patient health, necessitates a robust system capable of real-time analysis and decisionmaking.</s><s xml:id="_QkGf8Hc">Stress and depression, increasingly prevalent in modern healthcare contexts, are known to significantly impact vital signs such as heart rate, respiration, and temperature <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>.</s><s xml:id="_YhyZ5nW">By addressing these conditions, the proposed framework enhances early detection and intervention capabilities, which are critical for mitigating the physical and mental health risks associated with stressinduced tachycardia or depression-related bradycardia.</s></p><p xml:id="_WEmEmYD"><s xml:id="_ySS4szR">By leveraging the sequential decision-making prowess of RL algorithms, we have established a framework where each vital sign is monitored by a dedicated DRL agent.</s><s xml:id="_M6ANYt7">These agents operate within a cohesive monitoring environment, guided by meticulously defined reward policies to identify and respond to potential health emergencies based on MEWS and MET standards.</s><s xml:id="_MBg9NNP">This approach extends traditional patient monitoring by integrating the capacity to dynamically adapt to physiological changes influenced by mental health stressors, thereby providing a more comprehensive solution.</s></p><p xml:id="_HMwEepg"><s xml:id="_VNNYTms">A notable aspect of our research is the emphasis on the design of the observation space for each DRL agent.</s><s xml:id="_PwsgDkQ">This design is pivotal in ensuring the accuracy and effectiveness of the learning process, as it directly impacts the agent's ability to interpret vital sign data and make informed decisions.</s><s xml:id="_3B5MyUc">The challenge encountered with DRL agent 3, responsible for monitoring body temperature, underscores the importance of data consistency and the need for a harmonized observation space.</s><s xml:id="_EHBbtKd">The discrepancy between the temperature units in the MEWS table and the dataset highlighted a critical area for improvement, emphasizing the need for standardized data inputs to enhance agent performance and ensure reliability in detecting stress or depression-related anomalies.</s></p><p xml:id="_Fj8TSkJ"><s xml:id="_ZAwNbQv">The autonomous decision-making capability inherent in RL represents a significant advancement in 5 Hyper Parameters -α optimization supporting clinicians.</s><s xml:id="_wG2nphe">By providing real-time updates on patient health, the DRL framework facilitates a proactive approach to patient care, extending its applicability beyond hospital settings to include home monitoring and specialized care environments.</s><s xml:id="_gyqgnWS">This adaptability is further enhanced by the strategic optimization of hyperparameters, which fine-tunes the learning process of DRL agents to achieve optimal performance.</s><s xml:id="_u9wPZZT">Our investigation into hyperparameters such as the learning rate and discount factor reveals the critical balance between immediate and future rewards, a balance that is essential for the effective monitoring of patient health, particularly in cases where stress or depression can cause delayed yet significant physiological effects.</s></p><p xml:id="_N2w9rsb"><s xml:id="_kVjwvZC">Comparatively, traditional supervised learning algorithms, while accurate in predicting vital signs, fall short in dynamic healthcare environments due to their reliance on extensive labeled datasets and external supervision.</s><s xml:id="_QTwvrTp">The DRL approach, free from the constraints of labeled data, offers a more flexible and efficient solution for patient monitoring.</s><s xml:id="_WxEDzJe">However, it is essential to acknowledge the considerable effort required in data preparation and model tuning within supervised learning frameworks, which, despite their limitations, contribute 6 Hyper Parameters -γ optimization significantly to the development of informed clinical decisions.</s></p><p xml:id="_kC7kTMu"><s xml:id="_tkShKeJ">The adaptive multi-agent DRL framework proposed in this study represents a paradigm shift in patient monitoring, offering a dynamic, efficient, and scalable solution for timely healthcare interventions <ref type="bibr" target="#b43">[44]</ref>.</s><s xml:id="_HpNsrMv">By addressing both the physical and mental health challenges posed by stress and depression, this framework introduces a holistic approach to patient monitoring.</s><s xml:id="_GK9Z5sk">The challenges and insights gleaned from this research pave the way for future advancements in the field, promising to enhance the quality of patient care through innovative technological solutions.</s></p><p xml:id="_zCVzQJ8"><s xml:id="_Q4cwYbD">Scope of Physiological Monitoring.</s><s xml:id="_acfUznT">We acknowledge that stress and depression are highly complex psychophysiological conditions that cannot be comprehensively diagnosed through the monitoring of only three physiological parameters.</s><s xml:id="_WRnhF4B">In this study, the use of heart rate, respiration rate, and body temperature was intended as a proof-of-concept for evaluating the feasibility and performance of the proposed multi-agent DRL framework in a controlled setting.</s><s xml:id="_tg378Kx">These variables were selected due to their well-documented correlation with acute stress responses and their widespread availability in wearable sensor systems.</s><s xml:id="_aj82VA2">However, they serve as proxies for physiological arousal rather than definitive indicators of mental health status.</s><s xml:id="_ZWyqxdN">The modular nature of our framework allows for the seamless integration of additional biosignals (e.g., GSR, HRV, EEG) or behavioral indicators (e.g., sleep disruption, speech features) in future work.</s><s xml:id="_6JDnCGk">As such, the current implementation should be viewed as a foundational step toward building a more comprehensive and multimodal system for mental health monitoring.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_arj64qp">Explainability</head><p xml:id="_wDv7Ane"><s xml:id="_US76QJg">While the proposed multi-agent DRL framework demonstrates strong adaptability and decision-making performance in patient monitoring, ensuring explainability remains a vital aspect for clinical adoption.</s><s xml:id="_r5YzcCK">To this end, we suggest incorporating agent-specific decision traceability as a foundational mechanism.</s><s xml:id="_cQFvjjv">Each agent can log transitions in Q-values alongside corresponding MEWS thresholds and selected actions, providing a transparent record of decision rationale over time.</s><s xml:id="_R3neB3w">Such traceability supports retrospective audits by clinicians and aligns with the interpretability expectations of healthcare AI systems.</s><s xml:id="_n3WSuuD">Furthermore, future extensions of this work will explore the integration of model-agnostic interpretability techniques, such as SHapley Additive exPlanations (SHAP), to assess the contribution of each physiological feature to the agents' actions in real time.</s><s xml:id="_PVz4Cf6">This dual approach-combining Q-value trajectory logging with post-hoc feature attribution-has the potential to enhance clinician trust, uncover failure points, and guide improvements in agent design.</s><s xml:id="_nEg4sf2">Emphasizing explainability is particularly important in sensitive contexts such as stress and depression monitoring, where transparent and accountable AI systems are essential for safe and ethical deployment.</s></p><p xml:id="_JAj6wGN"><s xml:id="_fEgvCJj">Dataset Size and Generalizability.</s><s xml:id="_6zxj2Qr">Although the proposed framework was evaluated using two widely recognized datasets-PPG-DaLiA and WESAD-each comprising 15 subjects, the size of these cohorts reflects an ongoing challenge in stress-related physiological research.</s><s xml:id="_dYmsTfD">Collecting high-quality, multimodal data under controlled conditions involving stress and affect remains inherently complex and resource-intensive, often limiting sample sizes across benchmark studies in this domain.</s><s xml:id="_53zfJvt">Despite this constraint, the framework consistently demonstrated reliable policy convergence and adaptive learning across multiple agents and subjects, providing strong evidence of its robustness and effectiveness in modeling temporal patterns in physiological signals.</s></p><p xml:id="_XxqDNza"><s xml:id="_5vAP6qv">Importantly, the controlled nature of the datasets allowed for reproducible experimentation and precise evaluation of the technical capabilities of the multi-agent DRL system.</s><s xml:id="_fX69fsR">Nonetheless, future work will aim to expand validation efforts using larger and more diverse datasets, potentially integrating synthetic data augmentation and transfer learning techniques to improve generalizability.</s><s xml:id="_GvvZe48">These steps will ensure broader applicability of the proposed monitoring framework in real-world healthcare settings, while preserving the methodological rigor established in this study.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_MaZUxfZ">Conclusion</head><p xml:id="_nACRZWD"><s xml:id="_ZNrHqwA">This study has pioneered an adaptive framework for healthcare interventions using multi-agent DRL to dynamically monitor vital signs, establishing a novel approach in patient care.</s><s xml:id="_BbjKpEW">By considering the significant influence of stress and depression on vital signs, this research underscores the importance of addressing mental health challenges in conjunction with physical health monitoring.</s><s xml:id="_eS5jyPt">Through the development of a generic monitoring environment coupled with a strategic reward policy, the DRL agents were empowered to learn from and adapt to vital sign fluctuations, enabling timely interventions by healthcare professionals.</s><s xml:id="_qrsAFwb">The ability of these agents to detect stress-induced or depressionrelated anomalies demonstrates the potential of this system to provide a comprehensive and proactive approach to healthcare.</s><s xml:id="_8vQyUfJ">Despite its innovative contributions, the research faced challenges, such as discrepancies in body temperature data scales and the absence of predictive capabilities for future vital sign trends, which limited the effectiveness of one DRL agent and the overall predictive potential of the system.</s><s xml:id="_w5jsXag">These limitations highlight the need for enhanced data standardization and the integration of predictive analytics to anticipate trends in vital signs influenced by mental health conditions.</s><s xml:id="_bTN6ywx">Future research will focus on overcoming these challenges by augmenting the framework with predictive modeling capabilities, enabling DRL agents to forecast vital sign trends and anticipate health emergencies.</s></p><p xml:id="_Dg5dcVG"><s xml:id="_q7zJMJW">This advancement aims to revolutionize patient monitoring by facilitating proactive healthcare measures, significantly reducing the risk of critical health episodes associated with stress and depression.</s><s xml:id="_2bzRduK">The future direction of this research will extend the scope to include multi-agent DRL frameworks capable of predicting future health trajectories, thereby enhancing the integration of mental and physical health monitoring in adaptive patient care systems.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6DMVDhm">Funding</head><p xml:id="_mdaMhWh"><s xml:id="_8gT6QCe">Not applicable.</s><s xml:id="_8Hv3ud2">No specific funding was received for this research.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc><div><p xml:id="_ET5Gp7K"><s xml:id="_ftupzTY">Fig. 2 Multi-agent monitoring framework</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc><div><p><s xml:id="_rqZw69J"><ref type="bibr" target="#b35">[36]</ref>fied Early Warning Scores[36]</s></p></div></figDesc><table><row><cell>MEWS</cell><cell>4/MET</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4/MET</cell></row><row><cell>Respiratory Rate</cell><cell>≤4</cell><cell>5-8</cell><cell></cell><cell></cell><cell>9-20</cell><cell>21-24</cell><cell>25-30</cell><cell>31-35</cell><cell>≥36</cell></row><row><cell>Oxygen Saturation</cell><cell>≤84</cell><cell>85-89</cell><cell>90-92</cell><cell>93-94</cell><cell>≥95</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Temperature</cell><cell></cell><cell>≤34.0</cell><cell>34.1-35.0</cell><cell>35.1-36.0</cell><cell>36.1-37.9</cell><cell>38.0-38.5</cell><cell>≥38.6</cell><cell></cell><cell></cell></row><row><cell>Heart Rate</cell><cell>≤39</cell><cell></cell><cell></cell><cell>40-49</cell><cell>50-99</cell><cell>100-109</cell><cell>110-129</cell><cell>130-139</cell><cell>≥140</cell></row><row><cell>Sedation Score</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Awake</cell><cell></cell><cell>Mild</cell><cell>Moderate</cell><cell>Severe</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc><div><p xml:id="_sjNpedM"><s xml:id="_mQbp3RC">Rewards Policy</s></p></div></figDesc><table><row><cell>MEWS</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell><cell>0</cell></row><row><cell>Action 0</cell><cell>-4</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>10</cell></row><row><cell>Action 1</cell><cell>-4</cell><cell>-3</cell><cell>-2</cell><cell>10</cell><cell>-1</cell></row><row><cell>Action 2</cell><cell>-4</cell><cell>-3</cell><cell>10</cell><cell>-1</cell><cell>-2</cell></row><row><cell>Action 3</cell><cell>-4</cell><cell>10</cell><cell>-1</cell><cell>-2</cell><cell>-3</cell></row><row><cell>Action 4</cell><cell>10</cell><cell>-3</cell><cell>-2</cell><cell>-1</cell><cell>-4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc><div><p xml:id="_DkC5zRc"><s xml:id="_TN67bJn">Comparison of DRL and MARL Frameworks on Cumulative Rewards</s></p></div></figDesc><table><row><cell>Method</cell><cell>PPG-DaLiA Dataset</cell><cell></cell><cell></cell><cell>WESAD Dataset</cell><cell></cell></row><row><cell></cell><cell>Agent 1</cell><cell>Agent 2</cell><cell>Agent 3</cell><cell>Agent 1</cell><cell>Agent 2</cell><cell>Agent 3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc><div><p xml:id="_7NPJJTj"><s xml:id="_6v86yyB">Evaluation of DRL Framework and Baseline Models on Additional Metrics</s></p></div></figDesc><table><row><cell>RL Method</cell><cell>Learning Rate</cell><cell>Computational</cell><cell>Memory</cell></row><row><cell></cell><cell>(Epochs to</cell><cell>Complexity (Time</cell><cell>Usage</cell></row><row><cell></cell><cell>Converge)</cell><cell>in Seconds)</cell><cell>(MB)</cell></row><row><cell>Q-Learning</cell><cell>1200</cell><cell>0.85s per iteration</cell><cell>120MB</cell></row><row><cell>PPO [31]</cell><cell>900</cell><cell>1.10s per iteration</cell><cell>150MB</cell></row><row><cell>A2C [31]</cell><cell>1000</cell><cell>1.05s per iteration</cell><cell>140MB</cell></row><row><cell cols="2">Double DQN [32] 1100</cell><cell>0.95s per iteration</cell><cell>135MB</cell></row><row><cell>DDPG [33]</cell><cell>950</cell><cell>1.20s per iteration</cell><cell>160MB</cell></row><row><cell>WISEML [39]</cell><cell>900</cell><cell>1.15s per iteration</cell><cell>145MB</cell></row><row><cell>CA-MQL [40]</cell><cell>1000</cell><cell>1.30s per iteration</cell><cell>175MB</cell></row><row><cell>MADDPG [41]</cell><cell>950</cell><cell>1.25s per iteration</cell><cell>155MB</cell></row><row><cell>QMIX [42]</cell><cell>1100</cell><cell>1.20s per iteration</cell><cell>165MB</cell></row><row><cell>Proposed DRL</cell><cell>850</cell><cell>0.70s per iteration</cell><cell>110MB</cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_E249N5j">Data Availability</head><p xml:id="_QtQpJAz"><s xml:id="_QQS54vJ">The datasets used and analyzed during the current study are publicly available.</s><s xml:id="_zxhGaEP">Specifically, the PPG-DaLiA dataset <ref type="bibr" target="#b36">[37]</ref> and the WESAD <ref type="bibr" target="#b37">[38]</ref> were used in this research.</s><s xml:id="_7Cgk6A6">Detailed instructions on accessing these datasets are provided in their respective publications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_7rf3S59">Materials Availability</head><p xml:id="_4xZFvFb"><s xml:id="_Ev5t2yJ">Not applicable.</s><s xml:id="_NkzpX2S">This study does not rely on specialized materials requiring</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DNP7CgE">Conflict of interest.</head><p xml:id="_mMWvxD6"><s xml:id="_ZhHBvWa">Code availability he source code used to implement the multi-agent DRL monitoring framework, including the environment configuration and training routines, is publicly available at: <ref type="url" target="https://github.com/Thanveer-Analyst/multi-agent-health-monitoring.git">https:// github. com/ Thanv eer-Analy st/ multi-agent- health-monit oring. git</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Rqqzc3X">Declarations Ethics approval and consent to participate</head><p xml:id="_qdRXMCJ"><s xml:id="_yJGPYeJ">Not applicable.</s><s xml:id="_Z9S6APk">This study does not involve human participants or animal studies requiring ethical approval.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rFsu54e">Consent for Publication</head><p xml:id="_bpqZPDB"><s xml:id="_bEv7Suv">All authors have provided their consent for the publication of this manuscript.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2DAsYcH">Competing interests</head><p xml:id="_tNr9qDZ"><s xml:id="_qH4FwXq">Author Xiaohui Tao is a member of the Editorial Board of the Journal Brain Informatics.</s><s xml:id="_34Q7aMA">The paper was handled by another Editor and has undergone a rigorous peer review process.</s><s xml:id="_9R9g7ym">Author Xiaohui Tao was not involved in the journal's peer review of, or decisions related to, this manuscript.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_naZTFTD">Publisher's Note</head><p xml:id="_gFYGcEU"><s xml:id="_jtCvYJd">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>41] 42500 29870 36015 41200 28560 35345 QMIX [42] 44800 30520 37600 43200 29230 36980 Proposed DRL 48354 30019 38651 47794 29056 37786</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rHbzUNT">Q-Learning</title>
		<imprint/>
	</monogr>
	<note>WISEML MADDPG</note>
	<note type="raw_reference">Q-Learning 25878 17304 23688 25318 16341 22823 PPO [31] 23688 20367 17688 23128 19404 16823 A2C [31] 24717 13707 24369 24157 12744 23504 Double DQN [32] 25569 15360 20367 25009 14397 19502 DDPG [33] 26760 20754 23967 26200 19791 23102 WISEML [39] 28654 25789 33669 28094 24826 32804 CA-MQL [40] 32985 27856 34685 32425 26893 33820 MADDPG [41] 42500 29870 36015 41200 28560 35345 QMIX [42] 44800 30520 37600 43200 29230 36980 Proposed DRL 48354 30019 38651 47794 29056 37786 References</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_Q88a7r3">Mental health and well-being at work: A systematic review of literature and directions for future research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Syed</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.hrmr.2023.100998</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eGcjxEg">Human Resour Manag Rev</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">100998</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Khalid A, Syed J (2024) Mental health and well-being at work: A system- atic review of literature and directions for future research. Human Resour Manag Rev 34(1):100998</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_egY7E6H">Mobile health in remote patient monitoring for chronic diseases: principles, trends, and challenges</title>
		<author>
			<persName><forename type="first">N</forename><surname>El-Rashidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>El-Sappagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Bakry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abdelrazek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.3390/diagnostics11040607</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BpbKVTK">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">607</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">El-Rashidy N, El-Sappagh S, Islam SR, El-Bakry M, H, Abdelrazek S. (2021) Mobile health in remote patient monitoring for chronic diseases: princi- ples, trends, and challenges. Diagnostics 11(4):607</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_VYps8GU">Ai enabled rpm for mental health facility</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gururajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3556551.3561191</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RduNzaK">Proceedings of the 1st ACM Workshop on Mobile and Wireless Sensing for Smart Healthcare</title>
		<meeting>the 1st ACM Workshop on Mobile and Wireless Sensing for Smart Healthcare</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shaik T, Tao X, Higgins N, Xie H, Gururajan R, Zhou X (2022) Ai enabled rpm for mental health facility. In: Proceedings of the 1st ACM Workshop on Mobile and Wireless Sensing for Smart Healthcare, pp. 26-32</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_R78MP7h">E-commerce application with analytics for pharmaceutical industry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Pattanayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Pooja</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-19-3590-9_22</idno>
		<ptr target="https://doi.org/10.1007/978-981-19-3590-9_22" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Mwv3apf">Advances in Intelligent Systems and Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
	<note type="raw_reference">Pattanayak RK, Kumar VS, Raman K, Surya MM, Pooja MR (2022) E-commerce application with analytics for pharmaceutical industry. In: Advances in Intelligent Systems and Computing, pp. 291-298. Springer, ??? . https:// doi. org/ 10. 1007/ 978-981-19-3590-9_ 22</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_49xwXwr">Towards computational solutions for precision medicine based big data healthcare system using deep learning models: A review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thirunavukarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gpd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Gopikrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palanisamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2022.106020</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2tKSq5h">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page">106020</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thirunavukarasu R, C GPD, R G, Gopikrishnan M, Palanisamy V, (2022) Towards computational solutions for precision medicine based big data healthcare system using deep learning models: A review. Comput Biol Med 149:106020</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_9ZpuAp4">Deep reinforcement learning for autonomous driving: A survey</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talpaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaa</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<idno type="DOI">10.1109/tits.2021.3054625</idno>
		<ptr target="https://doi.org/10.1109/tits.2021.30546" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_vmAnYxM">IEEE Trans Intell Transp Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4909" to="4926" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kiran BR, Sobh I, Talpaert V, Mannion P, Sallab AAA, Yogamani S, Perez P (2022) Deep reinforcement learning for autonomous driving: A survey. IEEE Trans Intell Transp Syst 23(6):4909-4926. https:// doi. org/ 10. 1109/ tits. 2021. 30546</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_B6xtb97">Optimizing individualized treatment planning for parkinson&apos;s disease using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khojandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramdhani</surname></persName>
		</author>
		<idno type="DOI">10.1109/embc44109.2020.9175311</idno>
		<ptr target="https://doi.org/10.1109/embc44109.2020.91753" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_zRcGstz">Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC). IEEE</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Watts J, Khojandi A, Vasudevan R, Ramdhani R (2020) Optimizing individualized treatment planning for parkinson&apos;s disease using deep reinforcement learning. In: 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC). IEEE, ??? . https:// doi. org/ 10. 1109/ embc4 4109. 2020. 91753</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_MWBTyFH">A reinforcement learning and deep learning based intelligent system for the support of impaired patients in home treatment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paragliola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coronato</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.114285</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pPShC2M">Exp Syst Appl</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page">114285</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Naeem M, Paragliola G, Coronato A (2021) A reinforcement learning and deep learning based intelligent system for the support of impaired patients in home treatment. Exp Syst Appl 168:114285</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_z4fpGjB">State of the art of machine learning-enabled clinical decision support in intensive care units: Literature review</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.2196/28781</idno>
		<ptr target="https://doi.org/10.2196/28781" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Sb5gV59">JMIR Med Inf</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28781</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hong N, Liu C, Gao J, Han L, Chang F, Gong M, Su L (2022) State of the art of machine learning-enabled clinical decision support in intensive care units: Literature review. JMIR Med Inf 10(3):28781. https:// doi. org/ 10. 2196/ 28781</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_G5uqsDQ">Probabilistic machine learning for healthcare</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-biodatasci-092820-033938</idno>
		<ptr target="https://doi.org/10.1146/annurev-biodatasci-092820-033938" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_AbwXgUh">Ann Rev Biomed Data Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="393" to="415" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen IY, Joshi S, Ghassemi M, Ranganath R (2021) Probabilistic machine learning for healthcare. Ann Rev Biomed Data Sci 4(1):393-415. https:// doi. org/ 10. 1146/ annur ev-bioda tasci-092820-033938</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_JaksK9x">Reinforcement learning for clinical applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Khezeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ozrazgat-Baslanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bihorac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rashidi</surname></persName>
		</author>
		<idno type="DOI">10.2215/cjn.0000000000000084</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EhAPeAk">Clin J Am Soc Nephrol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="521" to="523" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Khezeli K, Siegel S, Shickel B, Ozrazgat-Baslanti T, Bihorac A, Rashidi P (2023) Reinforcement learning for clinical applications. Clin J Am Soc Nephrol 18(4):521-523</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_bN2QWmu">Role of machine learning in healthcare sector</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Vijarania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.4195384</idno>
		<ptr target="https://doi.org/10.2139/ssrn.4195384" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_yTH44Bj">SSRN Electr J</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rastogi M, Vijarania DM, Goel DN (2022) Role of machine learning in healthcare sector. SSRN Electr J. https:// doi. org/ 10. 2139/ ssrn. 41953 84</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_b5xF59v">Machine learning algorithms-a review</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mahesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uqZAs89">Int J Sci Res (IJSR)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="381" to="386" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mahesh B (2020) Machine learning algorithms-a review. Int J Sci Res (IJSR) 9:381-386</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_YRa8HWV">Implications of big data analytics in developing healthcare frameworks-a review</title>
		<author>
			<persName><forename type="first">V</forename><surname>Palanisamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thirunavukarasu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jksuci.2017.12.007</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_a2Zp24y">J King Saud Univ-Comput Inf Sci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Palanisamy V, Thirunavukarasu R (2019) Implications of big data analytics in developing healthcare frameworks-a review. J King Saud Univ-Comput Inf Sci 31(4):415-425</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_EYbwDpq">Physical activity monitoring and classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Alsareii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Alasmari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raza</surname></persName>
		</author>
		<idno type="DOI">10.3390/life12081103</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mN95FYs">Life</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1103</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alsareii SA, Awais M, Alamri AM, AlAsmari MY, Irfan M, Aslam N, Raza M (2022) Physical activity monitoring and classification using machine learning techniques. Life 12(8):1103</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_k54juEn">Deep-reinforcement-learningbased autonomous uav navigation with sparse rewards</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gcnqf7j">IEEE Int Things J</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6180" to="6190" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang C, Wang J, Wang J, Zhang X (2020) Deep-reinforcement-learning- based autonomous uav navigation with sparse rewards. IEEE Int Things J 7(7):6180-6190</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_Xb8ed84">Applications of artificial intelligencemachine learning for detection of stress: a critical overview</title>
		<author>
			<persName><forename type="first">A-Fa</forename><surname>Mentis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roussos</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41380-023-02047-6</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ESBj37C">Mol Psychiatr</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1882" to="1894" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mentis A-FA, Lee D, Roussos P (2024) Applications of artificial intelligence- machine learning for detection of stress: a critical overview. Mol Psychiatr 29(6):1882-1894</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_UHPb5nr">A predictive analysis of heart rates using machine learning techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oyeleye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Titarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniou</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijerph19042417</idno>
		<ptr target="https://doi.org/10.3390/ijerph19042417" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_NrTDnFb">Int J Environ Res Pub Health</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2417</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Oyeleye M, Chen T, Titarenko S, Antoniou G (2022) A predictive analysis of heart rates using machine learning techniques. Int J Environ Res Pub Health 19(4):2417. https:// doi. org/ 10. 3390/ ijerp h1904 2417</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_NfC2psh">Heart rate prediction model based on neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1088/1757-899x/715/1/012060</idno>
		<ptr target="https://doi.org/10.1088/1757-899x/715/1/012060" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Mt25zGw">IOP Conf Series: Mater Sci Eng</title>
		<imprint>
			<biblScope unit="volume">715</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12060</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luo M, Wu K (2020) Heart rate prediction model based on neural net- work. IOP Conf Series: Mater Sci Eng 715(1):012060. https:// doi. org/ 10. 1088/ 1757-899x/ 715/1/ 012060</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_SZPxZkN">Sensor-based and vision-based human activity recognition: A comprehensive survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Piran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rQSHRgS">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">107561</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dang LM, Min K, Wang H, Piran MJ, Lee CH, Moon H (2020) Sensor-based and vision-based human activity recognition: A comprehensive survey. Pattern Recognit 108:107561</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_a29ESGF">Unsupervised embedding learning for human activity recognition using wearable sensor data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Nbw59eJ">The Thirty-Third International Flairs Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sheng T, Huber M (2020) Unsupervised embedding learning for human activity recognition using wearable sensor data. In: The Thirty-Third International Flairs Conference</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_kPTMn3f">Synthetic sensor data generation for health applications: A supervised deep learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Norgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gebremedhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zMXsB2w">2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1164" to="1167" />
		</imprint>
	</monogr>
	<note type="raw_reference">Norgaard S, Saeedi R, Sasani K, Gebremedhin AH (2018) Synthetic sensor data generation for health applications: A supervised deep learning approach. In: 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 1164-1167 . IEEE</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_9mZ2CKA">Machine learning: Algorithms, real-world applications and research directions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Sarker</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42979-021-00592-x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UVyQgfT">SN Computer Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">160</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sarker IH (2021) Machine learning: Algorithms, real-world applications and research directions. SN Computer Sci 2(3):160</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_VRtbcbs">Application of machine learning in ocean data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00530-020-00733-x</idno>
		<ptr target="https://doi.org/10.1007/s00530-020-00733-x" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_aqJtAHV">Multimed Syst</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lou R, Lv Z, Dang S, Su T, Li X (2021) Application of machine learning in ocean data. Multimed Syst. https:// doi. org/ 10. 1007/ s00530-020-00733-x</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_k6qdRST">Hierarchical reinforcement learning, sequential behavior, and the dorsal frontostriatal system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galashov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_01869</idno>
		<ptr target="https://doi.org/10.1162/jocn_a_01869" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_rhSW5Jm">J Cognit Neurosci</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1307" to="1325" />
			<date type="published" when="2020">2020. 2022</date>
		</imprint>
	</monogr>
	<note>Behavior priors for efficient reinforcement learning 26</note>
	<note type="raw_reference">Tirumala D, Galashov A, Noh H, Hasenclever L, Pascanu R, Schwarz J, Desjardins G, Czarnecki WM, Ahuja A, Teh YW, et al. (2020) Behavior priors for efficient reinforcement learning 26. Janssen M, LeWarne C, Burk D, Averbeck BB (2022) Hierarchical reinforce- ment learning, sequential behavior, and the dorsal frontostriatal system. J Cognit Neurosci 34(8):1307-1325. https:// doi. org/ 10. 1162/ jocn_a_ 01869</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_NvBWyvz">An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tsiakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theofanidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Makedon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3056540.3076191</idno>
		<ptr target="https://doi.org/10.1145/3056540.3076191" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_46HduYD">Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments</title>
		<meeting>the 10th International Conference on PErvasive Technologies Related to Assistive Environments</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tsiakas K, Papakostas M, Theofanidis M, Bell M, Mihalcea R, Wang S, Burzo M, Makedon F (2017) An interactive multisensing framework for personal- ized human robot collaboration and assistive training using reinforce- ment learning. In: Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments. ACM, ??? . https:// doi. org/ 10. 1145/ 30565 40. 30761 91</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_pjhh3Nc">Methods for robot behavior adaptation for cognitive neurorehabilitation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kubota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Riek</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-control-042920-093225</idno>
		<ptr target="https://doi.org/10.1146/annurev-control-042920-093225" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_gTVvM6Z">Ann Rev Control Robot Auton Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="135" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kubota A, Riek LD (2022) Methods for robot behavior adaptation for cognitive neurorehabilitation. Ann Rev Control Robot Auton Syst 5(1):109-135. https:// doi. org/ 10. 1146/ annur ev-contr ol-042920-093225</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_hDxXKth">Exploring the landscape of ubiquitous in-home health monitoring: a comprehensive survey</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pourpanah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Etemad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KPvQn54">ACM Trans Comput Healthc</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pourpanah F, Etemad A (2024) Exploring the landscape of ubiquitous in-home health monitoring: a comprehensive survey. ACM Trans Comput Healthc 5(4):1-43</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_xUhG72E">From personalized timely notification to healthy habit formation: a feasibility study of reinforcement learning approaches on synthetic data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peleg</surname></persName>
		</author>
		<idno type="DOI">10.1109/cbms52027.2021.00061</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wCsJq4Z">SMARTERCARE@ AI* IA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7" to="18" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lisowska A, Wilk S, Peleg M (2021) From personalized timely notification to healthy habit formation: a feasibility study of reinforcement learning approaches on synthetic data. In: SMARTERCARE@ AI* IA, pp. 7-18</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_Nv2TnVz">Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozdoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hochberg</surname></persName>
		</author>
		<idno type="DOI">10.2196/jmir.7994</idno>
		<ptr target="https://doi.org/10.2196/jmir.7994" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_sbXknMu">J Med Int Res</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">338</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yom-Tov E, Feraru G, Kozdoba M, Mannor S, Tennenholtz M, Hochberg I (2017) Encouraging physical activity in patients with diabetes: Interven- tion using a reinforcement learning system. J Med Int Res 19(10):338. https:// doi. org/ 10. 2196/ jmir. 7994</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_Q5WDGyz">Electronic health records based reinforcement learning for treatment optimizing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.is.2021.101878</idno>
		<ptr target="https://doi.org/10.1016/j.is.2021.101878" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_8PgUxQG">Inf Syst</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">101878</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li T, Wang Z, Lu W, Zhang Q, Li D (2022) Electronic health records based reinforcement learning for treatment optimizing. Inf Syst 104:101878. https:// doi. org/ 10. 1016/j. is. 2021. 101878</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_fs9NE9z">A deep reinforcement learning method for multimodal data fusion in action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="DOI">10.1109/lsp.2021.3128379</idno>
		<ptr target="https://doi.org/10.1109/lsp.2021.3128379" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_5ZtrV74">IEEE Signal Process Lett</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="120" to="124" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Guo J, Liu Q, Chen E (2022) A deep reinforcement learning method for multimodal data fusion in action recognition. IEEE Signal Process Lett 29:120-124. https:// doi. org/ 10. 1109/ lsp. 2021. 31283 79</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_KQPxG6H">Reinforcement learning in healthcare: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477600</idno>
		<ptr target="https://doi.org/10.1145/3477600" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_4R4C3ZD">ACM Comput Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu C, Liu J, Nemati S, Yin G (2021) Reinforcement learning in healthcare: A survey. ACM Comput Surv 55:1. https:// doi. org/ 10. 1145/ 34776 00</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2024.3486109</idno>
		<idno type="arXiv">arXiv:2305.06360</idno>
		<title level="m" xml:id="_mZCzBbv">Exploring the landscape of machine unlearning: A comprehensive survey and taxonomy</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Shaik T, Tao X, Xie H, Li L, Zhu X, Li Q (2023) Exploring the landscape of machine unlearning: A comprehensive survey and taxonomy. arXiv preprint arXiv: 2305. 06360</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_5RZjVeT">Canberra hospital and health services clinical procedure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Signs</surname></persName>
		</author>
		<idno type="DOI">10.47363/jimrr/2023(2)124</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Signs V (2021) Canberra hospital and health services clinical procedure</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_E9TtPuT">Deep PPG: Largescale heart rate estimation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Indlekofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Laerhoven</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19143079</idno>
		<ptr target="https://doi.org/10.3390/s19143079" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_bPAha2h">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">3079</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Reiss A, Indlekofer I, Schmidt P, Laerhoven KV (2019) Deep PPG: Large- scale heart rate estimation with convolutional neural networks. Sensors 19(14):3079. https:// doi. org/ 10. 3390/ s1914 3079</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_MDPygym">Introducing wesad, a multimodal dataset for wearable stress and affect detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duerichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Laerhoven</surname></persName>
		</author>
		<idno type="DOI">10.1145/3242969.3242985</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CK5DTm9">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="400" to="408" />
		</imprint>
	</monogr>
	<note type="raw_reference">Schmidt P, Reiss A, Duerichen R, Marberger C, Van Laerhoven K (2018) Introducing wesad, a multimodal dataset for wearable stress and affect detection. In: Proceedings of the 20th ACM International Conference on Multimodal Interaction, pp. 400-408</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_MrQ2F4S">A runtime monitoring framework to enforce invariants on reinforcement learning agents exploring complex environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mallozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pelliccione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tei</surname></persName>
		</author>
		<idno type="DOI">10.1109/rose.2019.00011</idno>
		<ptr target="https://doi.org/10.1109/rose.2019.00011" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_z7dVEpg">IEEE/ACM 2nd International Workshop on Robotics Software Engineering (RoSE). IEEE</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mallozzi P, Castellano E, Pelliccione P, Schneider G, Tei K (2019) A runtime monitoring framework to enforce invariants on reinforcement learning agents exploring complex environments. In: 2019 IEEE/ACM 2nd Inter- national Workshop on Robotics Software Engineering (RoSE). IEEE, ??? . https:// doi. org/ 10. 1109/ rose. 2019. 00011</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_uSBpq75">Autonomous tracking using a swarm of UAVs: A constrained multi-agent reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D-K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvt.2020.3023733</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_t9MC6vj">IEEE Trans Veh Technol</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13702" to="13717" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen Y-J, Chang D-K, Zhang C (2020) Autonomous tracking using a swarm of UAVs: A constrained multi-agent reinforcement learning approach. IEEE Trans Veh Technol 69(11):13702-13717</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_fm28ve9">Multiagent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamar</forename><forename type="middle">A</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mordatch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4MuKsRj">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lowe R, Wu YI, Tamar A, Harb J, Pieter Abbeel O, Mordatch I (2017) Multi- agent actor-critic for mixed cooperative-competitive environments. Adv Neural Inf Process Syst 30:1</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_ZBBFS55">Monotonic value function factorisation for deep multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Witt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QjBaafN">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">178</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rashid T, Samvelyan M, De Witt CS, Farquhar G, Foerster J, Whiteson S (2020) Monotonic value function factorisation for deep multi-agent reinforcement learning. J Mach Learn Res 21(178):1-51</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_y7W2jP7">Optimizing decision-making processes in times of covid-19: using reflexivity to counteract information-processing failures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Schippers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_X7ZFMKe">Front Psychol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">650525</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schippers MC, Rus DC (2021) Optimizing decision-making processes in times of covid-19: using reflexivity to counteract information-processing failures. Front Psychol 12:650525</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_WyjC84Z">Graph-enabled reinforcement learning for time series forecasting with adaptive intelligence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/tetci.2024.3398024</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rkfYGwn">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shaik T, Tao X, Xie H, Li L, Yong J, Li Y (2024) Graph-enabled reinforce- ment learning for time series forecasting with adaptive intelligence. IEEE Transactions on Emerging Topics in Computational Intelligence</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
