<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_W2fAy6F">IntelligentPooling: practical Thompson sampling for mHealth</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">Â© The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature 2021</p>
				</availability>
				<date type="published" when="2021-06-21">21 June 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sabina</forename><surname>Tomkins</surname></persName>
							<email>stomkins@stanford.edu</email>
							<idno type="ORCID">0000-0002-2632-8173</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Stanford University , Stanford , United States of America</note>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Liao</surname></persName>
							<email>pengliao@g.harvard.edu</email>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Harvard University , Cambridge , United States of America</note>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Predrag</forename><surname>Klasnja</surname></persName>
							<email>klasnja@umich.edu</email>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> University of Michigan , Ann Arbor , United States of America</note>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><surname>Murphy</surname></persName>
							<email>samurphy@fas.harvard.edu</email>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Harvard University , Cambridge , United States of America</note>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">United States of America</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alborz</forename><surname>Geramifard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Csaba</forename><surname>Szepesvari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main" xml:id="_BW6Gtj4">IntelligentPooling: practical Thompson sampling for mHealth</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-21">21 June 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">B9A7ECB710042D6ED600BC8F7C87B7EC</idno>
					<idno type="DOI">10.1007/s10994-021-05995-8</idno>
					<note type="submission">Received: 16 May 2020 / Revised: 10 December 2020 / Accepted: 11 May 2021 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_nvNcSFP">Thompson sampling</term>
					<term xml:id="_gMYZUFh">Mobile health</term>
					<term xml:id="_mCAxsN4">Clinical trial</term>
					<term xml:id="_jaC7aFH">Physical activity</term>
					<term xml:id="_KcpG2Kf">Nonstationary environment</term>
					<term xml:id="_dyjrntq">Mixed effects</term>
					<term xml:id="_s5wccDE">Bayesian reward model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_BaS9Uff"><p xml:id="_DzwcS8v"><s xml:id="_z5dQgXz">In mobile health (mHealth) smart devices deliver behavioral treatments repeatedly over time to a user with the goal of helping the user adopt and maintain healthy behaviors.</s><s xml:id="_zTFx7Aa">Reinforcement learning appears ideal for learning how to optimally make these sequential treatment decisions.</s><s xml:id="_skk5p8m">However, significant challenges must be overcome before reinforcement learning can be effectively deployed in a mobile healthcare setting.</s><s xml:id="_BT2BaKW">In this work we are concerned with the following challenges: (1) individuals who are in the same context can exhibit differential response to treatments (2) only a limited amount of data is available for learning on any one individual, and (3) non-stationary responses to treatment.</s><s xml:id="_3CZQ6VB">To address these challenges we generalize Thompson-Sampling bandit algorithms to develop Intel-lIgentPoolIng. IntellIgentPoolIng learns personalized treatment policies thus addressing challenge one.</s><s xml:id="_sPN9t3D">To address the second challenge, IntellIgentPoolIng updates each user's degree of personalization while making use of available data on other users to speed up learning.</s><s xml:id="_BJyRp9s">Lastly, IntellIgentPoolIng allows responsivity to vary as a function of a user's time since beginning treatment, thus addressing challenge three.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="40" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="41" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="42" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="43" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_sjM8DrB">Introduction</head><p xml:id="_8JpFfaK"><s xml:id="_tbSnNm5">Mobile health (mHealth) applications deliver treatments in users' everyday lives to support healthy behaviors.</s><s xml:id="_q2amzeE">These mHealth applications offer an opportunity to impact health across a diverse range of domains from substance use <ref type="bibr" target="#b45">(Rabbi et al. 2017)</ref>, to disease self-management <ref type="bibr" target="#b24">(Hamine et al. 2015)</ref> to physical inactivity <ref type="bibr" target="#b14">(Consolvo et al. 2008)</ref>.</s><s xml:id="_dqE49XY">For example, to help users increase their physical activity, an mHealth application might send walking suggestions at the times and in the contexts (e.g.</s><s xml:id="_GJf27Rm">current location or recent physical activity) when a user is likely to be able to pursue the suggestions.</s><s xml:id="_Jge4kkr">A goal of mHealth applications is to provide treatments in contexts in which users need support while avoiding overtreatment.</s><s xml:id="_AmZzR79">Over-treatment can lead to user disengagement <ref type="bibr" target="#b40">(Nahum-Shani et al. 2017)</ref>, for example users might ignore treatments or even delete the application.</s><s xml:id="_gCq3SkZ">Consequently, the goal is to be able to learn an optimal policy for when and how to intervene for each user and context without over-treating.</s></p><p xml:id="_2qj9H9Q"><s xml:id="_SJTgrHh">Contextual bandit algorithms appear ideal for this task.</s><s xml:id="_9qJt3yT">Contextual bandit algorithms have been successful in a range of application settings from news recommendations <ref type="bibr" target="#b33">(Li et al. 2010)</ref> to education <ref type="bibr" target="#b42">(Qi et al. 2018)</ref>.</s><s xml:id="_VfQBw7T">However, as we discuss below, many challenges remain to adapt contextual bandit algorithms for mHealth settings.</s><s xml:id="_SEJH4Jz">Thompson sampling offers an attractive framework for addressing these challenges.</s><s xml:id="_938pNCV">In their seminal work <ref type="bibr" target="#b2">(Agrawal and Goyal 2013)</ref>, <ref type="bibr">Agrawal and</ref> Goyal show that Thompson sampling for contextual bandits, which works well in practice, can also achieve strong theoretical guarantees.</s><s xml:id="_Kt7qjHd">In our work, we propose Thompson sampling contextual bandit algorithm which introduces a mixed effects structure for the weights on the feature vector, an algorithm we call Intel-lIgentPoolIng.</s><s xml:id="_jvCz5vB">We demonstrate empirically that IntellIgentPoolIng has many advantages.</s><s xml:id="_HE6PdmU">We also derive a high-probability regret bound for our approach which achieves similar regret to <ref type="bibr" target="#b2">(Agrawal and Goyal 2013)</ref>.</s><s xml:id="_6HRfu4a">Unlike <ref type="bibr" target="#b2">(Agrawal and Goyal 2013)</ref>, our regret bound depends on the variance components introduced by the mixed effects structure which is at the center of our approach.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1" xml:id="_zPJc7Ac">Challenges</head><p xml:id="_gHFYV2F"><s xml:id="_6AGsk5M">There are significant challenges to learning optimal policies in mHealth.</s><s xml:id="_D5fGeeE">This work primarily addresses the challenge of learning personalized user policies from limited data.</s><s xml:id="_5QWADwX">Contextual bandit algorithms can be viewed as algorithms that use the user's context to adapt treatment.</s><s xml:id="_aGG8Qwj">While this approach can have advantages compared to ignoring the user's context, it fails to address that users can respond differentially to treatments even when they appear to be in the same context.</s><s xml:id="_VVbuFND">This occurs since sensors on smart devices are unlikely to record all aspects of a user's context that affect their health behaviors.</s><s xml:id="_NCjjpD9">For example, the context may not include social constraints on the user (e.g., care-giving responsibilities), which may influence the user's ability to be active.</s><s xml:id="_awwVWuG">Thus, algorithms that can learn from the differential responsiveness to treatment are desirable.</s><s xml:id="_zFD9ZmD">This motivates the need for an algorithm that not only incorporates contextual information, but that can also learn personalized policies.</s><s xml:id="_YDpWwd4">A natural first approach would be to use the algorithm separately for each user, but the algorithm is likely to learn very slowly if data on a user is sparse and/or noisy.</s><s xml:id="_ksaWAsB">However, typically in mHealth studies multiple users are using the application at any given time.</s><s xml:id="_uqNcX5X">Thus an algorithm that pools data over users intelligently so as to speed up learning of personalized policies is desirable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_9BFVKYe">3</head><p xml:id="_5G8w77w"><s xml:id="_HcrNPyP">An additional challenge is non-stationary responses to treatment (e.g.</s><s xml:id="_vwsfdTt">non-stationary reward function).</s><s xml:id="_5GG4mef">For example, in the beginning of a study, a user might be excited to receive a treatment, however after a few weeks this excitement can wane.</s><s xml:id="_gbGrwZ8">This motivates the need for algorithms that can learn time-varying treatment policies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2" xml:id="_cyykbVY">Contributions</head><p xml:id="_vSdqaNA"><s xml:id="_nVy9mhW">We develop IntellIgentPoolIng, a type of Thompson sampling contextual bandit algorithm specifically designed to overcome the above challenges.</s><s xml:id="_mR3aDXS">Our main contributions are:</s></p><p xml:id="_YwFUf5s"><s xml:id="_wuECkuV">â¢ IntellIgentPoolIng: A Thompson sampling contextual bandit algorithm for rapid personalization in limited data settings.</s><s xml:id="_tJUFjMX">This algorithm employs classical random effects in the reward function <ref type="bibr" target="#b46">(Raudenbush and Bryk 2002;</ref><ref type="bibr" target="#b31">Laird and Ware 1982)</ref> and empirical <ref type="bibr">(Bayes Morris 1983;</ref><ref type="bibr" target="#b9">Casella 1985)</ref> to adaptively adjust the degree to which policies are personalized to each user.</s><s xml:id="_z9HVQsc">We present an analysis of this adaptivity in <ref type="bibr">Sect. 3.5</ref> showing that IntellIgentPoolIng can learn to personalize to a user as a function of the observed variance in the treatment effect both between and within users.</s><s xml:id="_tmD2cem">â¢ A high probability regret bound for IntellIgentPoolIng.</s></p><p xml:id="_EmyyHR6"><s xml:id="_DQUrD8j">â¢ An empirical evaluation of IntellIgentPoolIng in a simulation environment constructed from mHealth data.</s><s xml:id="_AtxJrmG">IntellIgentPoolIng not only achieves 26% lower regret than state-of-the-art approaches, it also is better able to adapt to the degree of heterogeneity present in a population than this approach.</s><s xml:id="_sxfpdak">â¢ Feasibility of IntellIgentPoolIng from a pilot study in a live clinical trial.</s><s xml:id="_hTn3mjJ">We demonstrate that IntellIgentPoolIng can be executed in a real-time online environment and show preliminary evidence of this method's effectiveness.</s><s xml:id="_fSP6MWJ">â¢ We show how to modify IntellIgentPoolIng to learn in non-stationary environments.</s></p><p xml:id="_YAXPdm6"><s xml:id="_w5uQSc8">Next, in Sect. 2 we discuss relevant related work.</s><s xml:id="_rhQGX9S">In Sect. 3 we present IntellIgentPoolIng and provide a high-probability regret bound for this algorithm.</s><s xml:id="_3HmeE3s">We then describe how we use historical data to construct a simulation environment and evaluate our approach against state-of-the-art in Sect. 4. Next, in Sect. 5 we introduce the feasibility study and provide preliminary evidence into the benefits of this approach.</s><s xml:id="_mcpffJG">We then discuss how to extend this work to include time-varying effects in Sect.</s><s xml:id="_GuQ739D">6.</s><s xml:id="_nFrVzVK">Finally, we discuss the limitations with our approach in Sect.</s><s xml:id="_95VBUKD">7 before concluding.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_F66JgkF">Related work</head><p xml:id="_ZCKXqn3"><s xml:id="_sxrAGWH">To put the proposed work in a broader healthcare perspective, an overview of similar work in mHealth is provided by Sect.</s><s xml:id="_UEXVvKU">2.1.</s><s xml:id="_T4BuwNG">Next, we discuss the extent to which reinforcement learning/bandit algorithms have been deployed in mHealth settings (Sect.</s><s xml:id="_Y49pcym">2.1).</s><s xml:id="_GyQKyPn">IntellI-gentPoolIng has similarities with several modeling approaches, here we discuss the most relevant: multi-task learning, meta-learning, Gaussian processes for Thompson Sampling contextual bandits, and time-delayed bandits.</s><s xml:id="_BTUrn3C">These topics are discussed in Sects.</s><s xml:id="_vpDNe7G">2.2-2.4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_kzU8ggs">Connections to Bandit algorithms in mHealth</head><p xml:id="_krzAtgs"><s xml:id="_BYV7HbJ">Bandit algorithms in mHealth have typically used one of two approaches.</s><s xml:id="_dZ6NXnG">The first approach is person specific, that is, an algorithm is deployed separately on each user, such as in <ref type="bibr" target="#b44">Rabbi et al. 2015;</ref><ref type="bibr" target="#b25">Jaimes et al. 2016;</ref><ref type="bibr" target="#b20">Forman et al. 2018 and</ref><ref type="bibr" target="#b36">Liao et al. 2020</ref>.</s><s xml:id="_rt7jeyT">This approach makes sense when users are highly heterogeneous, that is, their optimal policies differ greatly one from another.</s><s xml:id="_qKJbCef">However, this approach can present challenges for policy learning when data is scarce and/or noisy, as in our motivating example of encouraging activity in an mHealth study where only a few decision time-points occur each day (see <ref type="bibr" target="#b58">(Xia 2018)</ref> for an empirical evaluation of the shortcomings of Thompson sampling for personalized contextual bandits in mHealth settings).</s><s xml:id="_fju4wAm">The second approach completely pools users' data, that is one algorithm is used on all users so as to learn a common treatment policy both in bandit algorithms <ref type="bibr" target="#b41">(Paredes et al. 2014;</ref><ref type="bibr" target="#b59">Yom-Tov et al. 2017)</ref>, and in full reinforcement learning algorithms <ref type="bibr" target="#b13">(Clarke et al. 2017;</ref><ref type="bibr" target="#b61">Zhou et al. 2018</ref>).</s><s xml:id="_k6FeZhg">This second approach can potentially learn quickly but may result in poor performance if there is large heterogeneity between users.</s><s xml:id="_vcfWyT9">We compare to these two approaches empirically as they not only represent state-of-the-art in practice, they also represent two intuitive theoretical extremes.</s></p><p xml:id="_575S2hu"><s xml:id="_BaqR4hQ">In IntellIgentPoolIng we strike a balance between these two extremes, adjusting the degree of pooling to the degree that users are similarly responsive.</s><s xml:id="_EgvA4Sr">When users are heterogeneous, IntellIgentPoolIng achieves lower regret than the second approach while learning more quickly than the first approach.</s><s xml:id="_VTcxcAJ">When users are homogeneous our method performs as well as the second approach.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_zxMaShZ">Connections to multi-task learning and meta-learning</head><p xml:id="_zQj9U2j"><s xml:id="_J35yX8w">Following original work on non-pooled linear contextual bandits <ref type="bibr" target="#b2">(Agrawal and Goyal 2013)</ref>, researchers have proposed pooling data in a variety of ways.</s><s xml:id="_FsUgHsJ">For example, <ref type="bibr" target="#b16">Deshmukh et al. (2017)</ref> proposed pooling data from different arms of a single bandit problem.</s><s xml:id="_PwwFkpk">Li and Kar 2015 used context-sensitive clustering to produce aggregate reward estimates for the bandit algorithm.</s><s xml:id="_8VJzDpQ">More relevant to this work is multi-task Gaussian Process (GP), e.g., <ref type="bibr" target="#b32">Lawrence and Platt 2004;</ref><ref type="bibr" target="#b5">Bonilla et al. 2008</ref>; Wang and Khardon 2012, however these have been proposed in the prediction as opposed to the reinforcement learning setting.</s><s xml:id="_avEQ9py">The Gang of Bandits approach <ref type="bibr" target="#b10">(Cesa-Bianchi et al. 2013)</ref>, which is a generalization from the original LinUCB algorithm for a single task <ref type="bibr" target="#b33">(Li et al. 2010)</ref>, has been shown to be successful when there is prior knowledge on the similarities between users.</s><s xml:id="_n3ZHm2d">For example, a known social network graph might provide a mechanism for pooling.</s><s xml:id="_6Qac3az">It was later extended to the Horde of Bandits in <ref type="bibr" target="#b54">(Vaswani et al. 2017</ref>) which used Thompson Sampling, allowing the algorithm to deal with a large number of tasks.</s></p><p xml:id="_QEbGEVs"><s xml:id="_qkHM7hE">Each of the multi-task approaches introduces some concept of similarity between users.</s><s xml:id="_6y8ra4j">The extent to which a given user's data contributes to another user's policy is some function of this similarity measure.</s><s xml:id="_zD2RmY7">This is fundamentally different from the approach taken in IntellIgentPoolIng.</s><s xml:id="_nk5K2cR">Rather than determining the extent to which any two users are similar, IntellIgentPoolIng determines the extent to which a given user's reward function parameters differ from parameters in a population (average over all users) reward function.</s><s xml:id="_E5Gfg38">This approach has the advantage of requiring fewer hyper-parameters, as we do not need to learn a similarity function between users.</s><s xml:id="_HmCUmNt">Instead of a pairwise similarity function it is as if we are learning a similarity between each user and the population average.</s><s xml:id="_WRcMduC">In the limited data setting, we expect this simpler model to be advantageous.</s></p><p xml:id="_2mWJSFW"><s xml:id="_ArnsWMa">In meta-learning, one exploits shared structure across tasks to improve performance on new tasks.</s><s xml:id="_bAE8F2s">IntellIgentPoolIng thus shares similarities with meta-learning for reinforcement learning <ref type="bibr" target="#b39">(Nagabandi et al. 2018;</ref><ref type="bibr" target="#b19">Finn et al. 2019;</ref><ref type="bibr" target="#b18">Finn et al. 2018;</ref><ref type="bibr" target="#b62">Zintgraf et al. 2019;</ref><ref type="bibr" target="#b23">Gupta et al. 2018;</ref><ref type="bibr" target="#b50">Saemundsson et al. 2018)</ref>.</s><s xml:id="_wMQu5Uu">At a high level, one can view our method as a form of meta-learning where the population-level parameters are learned from all available data and each user's parameters represent deviations from the shared parameters.</s><s xml:id="_VruSqtT">However, while meta-learning might require a large collection of source tasks, we demonstrate the efficacy of our approach on data on the small scale found in clinical mHealth studies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_DcgB4q6">Connections to Gaussian process models for Thompson sampling contextual bandits</head><p xml:id="_PySATXM"><s xml:id="_WFegN8Y">IntellIgentPoolIng is based on Bayesian mixed effects model of the reward, which is similar to using a Gaussian Process (GP) model with a simple form of the kernel.</s><s xml:id="_cQ6CCUv">GP models have been used for multi-armed bandits <ref type="bibr" target="#b12">(Chowdhury and Gopalan 2017;</ref><ref type="bibr" target="#b7">Brochu et al. 2010;</ref><ref type="bibr" target="#b52">Srinivas et al. 2009;</ref><ref type="bibr" target="#b15">Desautels et al. 2014;</ref><ref type="bibr" target="#b56">Wang et al. 2016;</ref><ref type="bibr" target="#b17">Djolonga et al. 2013;</ref><ref type="bibr" target="#b4">Bogunovic et al. 2016)</ref> , and for contextual bandits <ref type="bibr" target="#b33">(Li et al. 2010;</ref><ref type="bibr" target="#b30">Krause and Ong 2011)</ref>.</s><s xml:id="_HkpvPC5">However the above approaches do not structure the way in which the pooling of data across users occurs.</s><s xml:id="_tX95vMV">IntellIgentPoolIng uses a mixed effects GP model to pool across users in structured manner.</s><s xml:id="_xyfMBVa">Although mixed effects GP models have been previously used for offline data analysis <ref type="bibr" target="#b51">(Shi et al. 2012;</ref><ref type="bibr" target="#b37">Luo et al. 2018)</ref>, to the best of our knowledge they have not been previously used in the online decision making setting considered in this work.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4" xml:id="_85kjKqf">Connection to non-stationary linear bandits</head><p xml:id="_PSErmah"><s xml:id="_yzCfZd6">There is a growing literature investigating how to adapt linear bandit algorithms to changing environments.</s><s xml:id="_R9yK3XX">A common approach is for the learning algorithm to differentially weight data across time.</s></p><p xml:id="_m3R5PJ2"><s xml:id="_4ArKWmP">Differential weighting is used by both Russac et al. 2019 (using a LinUCB algorithm) and Kim and Tewari 2019 (using perturbation-based algorithms).</s><s xml:id="_CsnVgrq">Cheung et al. 2018 to estimate the parameters in the reward function and (Zhao et al. 2020) restart the algorithm at regular intervals discarding the prior data.</s><s xml:id="_Prfz9Qc">Similarly (Bogunovic et al. 2016), using GP-based UCB algorithms, accommodate non-stationarity by both restarting and using an autoregressive model for the rewards function.</s><s xml:id="_mbJT6MB">Kim and Tewari 2020 analyze the non-stationary setting with randomized exploration.</s><s xml:id="_bWVK3DA">Wu et al. introduce a model which detects abrupt time changes cite (<ref type="url" target="https://dl.acm.org/doi/pdf/10.1145/3209978.3210051">https:// dl. acm. org/ doi/ pdf/ 10. 1145/ 32099  78. 32100 51</ref>).</s></p><p xml:id="_CQtuMFD"><s xml:id="_hp3hC6K">IntellIgentPoolIng allows for non-stationary reward functions by the use of time-varying random effects.</s><s xml:id="_WDhCZgk">The correlation between the time-varying random effects induces a weighted estimator whereby more weight is put on the recently collected samples, similar to the discounted estimators in <ref type="bibr" target="#b47">Russac et al. 2019 and</ref><ref type="bibr">Kim and</ref><ref type="bibr" target="#b26">Tewari 2019.</ref></s><s xml:id="_suRVXsn">In contrast to existing approaches, IntellIgentPoolIng considers both individual and time-specific variation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_QhpHb5h">Intelligent Pooling</head><p xml:id="_DpSnMvq"><s xml:id="_yGQ5TNA">IntellIgentPoolIng is a generalization of a Thompson sampling contextual bandit for learning personalized treatment policies.</s><s xml:id="_DCXT7K9">We first outline the components of IntellIgent-PoolIng and then introduce the problem definition in Sect.</s><s xml:id="_FbXNmrw">3.2.</s><s xml:id="_BWAp6ZT">As our approach offers a natural alternative to two commonly used approaches, we begin by describing these simpler methods in Sect.</s><s xml:id="_3vVAG5F">3.3.</s><s xml:id="_YSQARxE">We introduce our method in Sect.</s><s xml:id="_v6fXtHd">3.4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_X9BtPHh">Overview</head><p xml:id="_pFRPAAW"><s xml:id="_x9tkT2N">The central component of IntellIgentPoolIng is a Bayesian model for the reward function.</s><s xml:id="_fkdVW6n">In particular, IntellIgentPoolIng uses a Gaussian mixed effects linear model for the reward function.</s><s xml:id="_U59dVv5">Mixed effects models are widely used across the health and behavioral sciences to model the variation in the linear model parameters across users <ref type="bibr" target="#b46">(Raudenbush and Bryk 2002;</ref><ref type="bibr" target="#b31">Laird and Ware 1982)</ref> and within a user across time.</s><s xml:id="_2MuU5fy">Use of these models enhances the ability of domain scientists to inform and critique the model used in Intel-lIgentPoolIng.</s><s xml:id="_kYgGGgA">The properties and pitfalls of these models are well understood; see <ref type="bibr" target="#b43">(Qian et al. 2019)</ref> for an application of a mixed effects model in mHealth.</s><s xml:id="_Sp2HvA2">IntellIgentPoolIng uses Bayesian inference for the mixed effects model.</s><s xml:id="_qF8CDUN">As discussed in Sect.</s><s xml:id="_tWzHj56">2.3, a Bayesian mixed effects linear model is a GP model with a simple kernel.</s><s xml:id="_5zAK36V">This facilitates increasing the flexibility of the model for the reward function, given sufficient data.</s></p><p xml:id="_A7EN2RN"><s xml:id="_TwtGfyV">Furthermore, IntellIgentPoolIng uses Thompson sampling <ref type="bibr" target="#b53">(Thompson 1933)</ref>, also known as posterior sampling <ref type="bibr" target="#b48">(Russo and Van Roy 2014)</ref>, to select actions.</s><s xml:id="_VDMdwcM">At each decision point, the parameters in the model for the reward function are sampled from their posterior distribution, thus inducing exploration over the action space <ref type="bibr" target="#b49">(Russo et al. 2018)</ref>.</s><s xml:id="_QUrueJk">These sampled parameters are then used to form an estimated reward function and the action with the highest estimated reward is selected.</s></p><p xml:id="_McZBAQR"><s xml:id="_7gFsC84">The hyper-parameters (e.g., the variance of the random effects) control the extent of pooling across users and across decision times.</s><s xml:id="_Zuf9RB6">The right amount of pooling depends on the heterogeneity among users and the non-stationarity, which is often difficult to pre-specify.</s><s xml:id="_K9MkDpz">Unlike other bandit algorithms in which the hyper-parameters are set at the beginning <ref type="bibr" target="#b16">(Deshmukh et al. 2017;</ref><ref type="bibr" target="#b10">Cesa-Bianchi et al. 2013;</ref><ref type="bibr" target="#b54">Vaswani et al. 2017)</ref>, IntellIgentPool-Ing includes a procedure for updating the hyper-parameters online.</s><s xml:id="_Bfjm3Hm">In particular, empirical (Bayes Carlin and Louis 2010) is used to update the hyper-parameters in the online setting, as more data becomes available.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_qvhUHFm">Problem formulation</head><p xml:id="_AZQn6fG"><s xml:id="_UcvMFU9">Consider an mHealth study which will recruit a total of N users. 1 Let i â [N] = {1, â¦ , N} be a user index.</s><s xml:id="_aZGnjas">For each user, we use k â {1, 2, â¦ } to index decision times, i.e., times at which a treatment could be provided.</s><s xml:id="_euQrpnZ">Denote by S i,k the states/contexts at the k th deci- sion time of user i.</s><s xml:id="_Z5fAk5T">For simplicity, we focus on the case where the action is binary, i.e., A i,k â {0, 1} .</s><s xml:id="_essUJy3">The algorithm can be easily generalized to cases with more than two actions.</s><s xml:id="_FMZ425n">After the action A i,k is chosen, the reward R i,k is observed.</s><s xml:id="_XCWUbBY">Throughout the remainder of the paper, S, A and R are random variables and we use lower-case (s, a and r) to refer to a realization of these random variables.</s></p><p xml:id="_wNGnTpF"><s xml:id="_UkTbfwJ">Below we consider a simpler setting where the parameters in the reward are assumed time-stationary.</s><s xml:id="_epPqg5p">We discuss how to generalize the algorithm to the non-stationary setting in Sect.</s><s xml:id="_9KszSGz">6.</s><s xml:id="_3HhWSM4">The goal is to learn personalized treatment policies for each of the N users.</s><s xml:id="_Q6J3JyY">We treat this as N contextual bandit problems as the reward function may differ between users.</s><s xml:id="_hkRY5At">In mHealth settings this might occur due to the inability of sensors to record users' entire contexts.</s><s xml:id="_MFBcN2S">Section 3.3 reviews two approaches for using Thompson Sampling <ref type="bibr" target="#b1">(Agrawal and Goyal 2012)</ref> and Sect.</s><s xml:id="_4qwTHHA">3.4 presents IntellIgentPoolIng, our approach for learning the treatment policy for any specific user.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_8YjSnKW">Two Thompson sampling instantiations</head><p xml:id="_paWUMxE"><s xml:id="_HwmykCG">First, consider learning the treatment policy separately per person.</s><s xml:id="_29jSNpk">We refer to this approach as Person-sPecIfIc.</s><s xml:id="_TJPTkF7">At each decision time k, we would like to select a treatment A i,k â {0, 1} based on the context S i,k .</s><s xml:id="_9t2VV2Q">We model the reward R i,k by a Bayesian linear regression model: for user i and time k where (s, a) is a pre-specified mapping from a context s and treatment a (e.g., those described in Sect.</s><s xml:id="_NyqdSpG">4.2), w i is a vector of weights which we will learn, and i,k â¼ (0, 2 ) is the error term.</s><s xml:id="_GAF65Ty">The weight vectors {w i } are assumed independent across users and to follow a common prior distribution w i â¼ ( w , w ) .</s><s xml:id="_yGNmcS3">See Fig. <ref type="figure">1</ref> for a graphical representation of this approach.</s><s xml:id="_Hazgyaq">Now at the k th decision time with the context S i,k = s , Person-sPecIfIc selects the treat- ment A i,k = 1 with probability where wi,k follows the posterior distribution of the parameters w i in the model (1) given the user's history up to the current decision time k.</s><s xml:id="_ftHRQUZ">We emphasize that in this formulation the posterior distribution of w i is formed based each user's own data.</s></p><p xml:id="_KwX5Gav"><s xml:id="_CA76nwA">The opposite approach is to learn a common bandit model for all users.</s><s xml:id="_v3sAsU2">In this approach, the reward model is a single Bayesian regression model with no individual-level parameters:</s></p><p xml:id="_yYFt5h8"><s xml:id="_U5syQTT">(1)</s></p><formula xml:id="formula_0">R i,k = ð(S i,k , A i,k ) â¤ w i + ð i,k , (2) ð i,k = Pr{ð(s, 1) â¤ wi,k &gt; ð(s, 0) â¤ wi,k } R 1,1 R 1,2 R 1,3 ... R 1,T 1,1 1,2 1,3 ... 1,T w 1 R 2,1 R 2,2 R 2,3 2,1 2,2 2,3 ... ... R 2,T 2,T w 2</formula><p xml:id="_RU8jfAP"><s xml:id="_8f9eCEJ">Fig. <ref type="figure">1</ref> Consider a setting with two users, here we show the relationship between select random variables in our model: R i,k the reward for user i at decision time k, 2 i,k the noise for user i at time k and w i the latent weight vector for user i.</s><s xml:id="_sBNF6FT">In Person-sPecIfIc we see that each user's parameters are independent.</s><s xml:id="_xFjjqns">Only the prior parameter values are shared, all else is updated independently where the common parameters, w , follows the prior distribution w â¼ ( w , w ) .</s><s xml:id="_VcefPCn">See Fig. <ref type="figure">2</ref> for the graphical representation of this approach.</s><s xml:id="_aDbNuYH">We then use the posterior distribution of the weight vector w to sample treatments for each user.</s><s xml:id="_quXmxga">Here the posterior is calcu- lated based on the available data from all users observed up to and including time k.</s><s xml:id="_AkwmmUK">This approach, which we refer to as comPlete, may suffer from high bias when there is significant heterogeneity among users.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" xml:id="_fg8FT4Y">Intelligent pooling across bandit problems</head><p xml:id="_Ek8YsW7"><s xml:id="_7UQF5nU">IntellIgentPoolIng is an alternative to the two approaches mentioned above.</s><s xml:id="_8755q8Z">Specifically, in IntellIgentPoolIng data is pooled across users in an adaptive way, i.e., when there is strong homogeneity observed in the current data, the algorithm will pool more from others than when there is strong heterogeneity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1" xml:id="_AwnZES2">Model specification</head><p xml:id="_67XKQr2"><s xml:id="_GvqCVpW">We model the reward associated with taking action A i,k for user i at decision time k by the linear model (1).</s><s xml:id="_CbXY3KM">Unlike Person-sPecIfIc where the person-specific weight vectors {w i , i â [N]} are assumed to be independent to each other, IntellIgentPoolIng imposes structure on the w i 's, in particular, a random-effects structure (Raudenbush and Bryk 2002;  Laird and Ware 1982):</s></p><p xml:id="_pBw6GkG"><s xml:id="_6nadjhg">where w pop is a population-level parameter and u i is a random effect that represents the person-specific deviation from w pop for user i.</s><s xml:id="_jwDKEHm">The extent to which the posterior means for w pop and u i are based on user i's data relative to the population depends on the variances of the random effects (for a stylized example of this see Sect.</s><s xml:id="_f9M2QPX">3.5).</s><s xml:id="_8eskcDC">In Sect.</s><s xml:id="_9nXR2zC">6 we show how we can modify this structure to include time-specific parameters, or a time-specific random effect.</s><s xml:id="_xgahfgk">A graphical representation for IntellIgentPoolIng is shown in Fig. <ref type="figure">3</ref>.</s></p><p xml:id="_NntQvk3"><s xml:id="_VGStQKs">We assume the prior on w pop is Gaussian with prior mean w and variance w .</s><s xml:id="_wYwPVGn">u i is also assumed to be Gaussian with mean and covariance u .</s><s xml:id="_kH8Qgru">Furthermore, we assume</s></p><formula xml:id="formula_1">(3) R i,k = ð(S i,k , A i,k ) â¤ w + ð i,k . (4) w i = w pop + u i , R 1,1 R 1,2 R 1,3 ... R 1,T 1,1 1,2 1,3 ... 1,T w pop R 2,1 R 2,2 R 2,3 ... ... R 2,T 2,1 2,2 2,3</formula><p xml:id="_2r6MdeG"><s xml:id="_5SU22v7">...</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MC6WaNg">2,T</head><p xml:id="_uA2Ztrq"><s xml:id="_DvEeQzf">Fig. <ref type="figure">2</ref> Consider a setting with two users, here we show the relationship between select random variables in our model: R i,k the reward for user i at decision time k, k the noise at time k and w pop the latent weight vector.</s><s xml:id="_WEWXmmN">In comPlete we see that each user's parameters are the same.</s><s xml:id="_BPdy8nf">With each parameter update the weight vector for every user is also updated for i â  j and .</s><s xml:id="_8RcNsAZ">The prior parameters w , w as well as the variance of the random effect u , and the residual variance 2 are hyper-parameters.</s><s xml:id="_6pVPf3f">In (4), there is a the random effect, u i on each element of w i .</s><s xml:id="_QYar5nG">In practice, one can use domain knowledge to specify which of the parameters should include random effects; this will be the case in the feasibility study described in Sect.</s><s xml:id="_NjKzxkk">6. Conditioned on the latent variables (w pop , u i ) , as well as the current context and action, the expected reward is</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2" xml:id="_tr2RxGr">Model connections to Gaussian processes</head><p xml:id="_T7wv9ep"><s xml:id="_PbBtkKG">Under the Gaussian assumption on the distribution of the reward and prior, the Bayesian linear model of the reward (1) together with the random effect model (4) can be viewed as an example of Gaussian Process with a special kernel (see Eq. 5).</s><s xml:id="_GSchMmP">We use this connection to derive the posterior distribution and facilitate the hyper-parameter selection.</s><s xml:id="_FpUq4qW">An additional advantage of viewing the Bayesian mixed effects model as a Gaussian Process model is that we can now flexibly redesign our reward model simply by introducing new kernel functions.</s><s xml:id="_PrsGnR6">Here, we assume linear model with a person-specific random effects.</s><s xml:id="_7yGkNcs">In Sect.</s><s xml:id="_nxAf639">6 we discuss a generalization to time-specific random effects.</s><s xml:id="_GWyBsjS">Additionally, one could adopt non-linear kernels and incorporate more complex structures on the reward function.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3" xml:id="_WjjD6vt">Posterior distribution of the weights on the feature vector</head><p xml:id="_9Nt7SV3"><s xml:id="_x7Ywy4V">In the setting where both the prior and the linear model for the reward follow a Gaussian distribution, the posterior distribution of w i follows a Gaussian distribution and there are analytic expressions for these updates, as shown in <ref type="bibr" target="#b57">(Williams and Rasmussen 2006)</ref>.</s><s xml:id="_hE35qGM">Below we provide the explicit formula of the posterior distribution based on the connection to a Gaussian Process regression.</s><s xml:id="_kRCPZdW">Suppose at the time of updating the posterior distribution, the available data collected from all current users is D , where D consists of n tuples of state, action, reward and user index x = (s, a, r, i) .</s><s xml:id="_rMAZ2df">The mixed effects model (Eqs. 1 and 4) induces a kernel function K.</s><s xml:id="_3T8zQMH">For any two tuples in D , e.g., x l = (s l , a l , r l , i l ), l = 1, 2 Note that the above kernel depends on w and u (one of the hyper-parameters that will be updated using empirical Bayes approach; see below).</s><s xml:id="_e8vHg4s">The kernel matrix is of size n Ã n</s></p><formula xml:id="formula_2">E[R i,k |w pop , u i , S i,k = s, A i,k = a] = (s, a) T (w pop + u i ). (5) K(x 1 , x 2 ) = ð(s 1 , a 1 ) â¤ (ð´ w + 1 {i 1 =i 2 } ð´ u )ð(s 2 , a 2 ). R 1,1 R 1,2 R 1,3 ... R 1,T 1,1 1,2 1,3 ... 1,T u 1 w pop R 2,1 R 2,2 R 2,3 2,1 2,2 2,3 ... ... R 2,T 2,T u 2</formula><p xml:id="_g9ytUGk"><s xml:id="_vw2TPpD">Fig. <ref type="figure">3</ref> Consider a setting with two users, here we show the relationship between select random variables in our model: R i,k the reward for user i at decision time k, i,k the noise for user i at time k, w pop the latent weight vector and u i the random effect for user i.</s><s xml:id="_9CygHF3">In IntellIgentPoolIng we see that some parameters ( w pop ) are shared across the population which others ( u i ) are user specific and each element is the kernel value between two tuples in D .</s><s xml:id="_5SJHq8V">The posterior mean and vari- ance of w i given the currently available data D can be calculated by where Rn is the vector of the rewards centered by the prior means, i.e., each element cor- responds to a tuple (s, a, r, j) in D given by r -(s, a) â¤  w , and M i is a matrix of size n by p (recall p is the length of w i ), with each row corresponding to a tuple (s, a, r, j) in D given by (s, a) â¤ ( w + 1 {j=i}  u ).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4" xml:id="_HFpx8dE">Treatment selection</head><p xml:id="_Ard6MWk"><s xml:id="_F3mmrt5">To select a treatment for user i at the k th decision time, we use the posterior distribution of w i formed at the most recent update time T.</s><s xml:id="_zvWvwXj">That is, for the context S i,k of user i at the k th decision time, IntellIgentPoolIng selects the treatment A i,k = 1 with the probability calculated in the same formula as in ( <ref type="formula">2</ref>) but with a different posterior distribution as discussed above.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5" xml:id="_M9KHWXG">Setting hyper-parameter values</head><p xml:id="_7gAcmpf"><s xml:id="_vxmxWzk">Recall that the algorithm requires the hyper-parameters w , w , u , and 2 .</s><s xml:id="_rrKUAUU">The prior mean w and variance w of the population parameter w pop can be set according to previous data or domain knowledge (see Sect. 5 for a discussion on how the prior distribution is set in the feasibility study).</s><s xml:id="_gujkRVy">As we mention in Sect.</s><s xml:id="_ENfpeUx">3.1, the variance components in the mixed effects model impact how the users pool the data from others (see Sect. 3.5 for a discussion) and might be difficult to pre-specify.</s><s xml:id="_x4w3UZ6">IntellIgentPoolIng uses, at the update times, the empirical (Bayes Carlin and Louis 2010) approach to choose/update = ( u , 2 ) based on the currently available data.</s><s xml:id="_JVuzBeW">To be more specific, suppose at the time of updating the hyper-parameters, the available data is D .</s><s xml:id="_YxRQKRJ">We choose to maximize l( |D) , the marginal log-likelihood of the observed reward, marginalized over the population parameters w pop and the random effects u i .</s><s xml:id="_uefFK8C">The marginal log-likelihood l( |D) can be expressed as where ( ) is the kernel matrix as a function of parameters = ( u , 2 ) .</s><s xml:id="_XaRcm6t">The above opti- mization can be efficiently solved using existing Gaussian Process regression packages; see Sect.</s><s xml:id="_2nXghFe">4.2 for more details.</s></p><formula xml:id="formula_3">(6) Åµi = ð w + M â¤ i ( + ð 2 ð I n ) -1 Rn ð´ i = ð´ w + ð´ u -M â¤ i ( + ð 2 ð I n ) -1 M i (7) l(ð|D) = - 1 2 Râ¤ n [ (ð) + ð 2 ð I n ] -1 Rn + log det[ (ð) + ð 2 ð I n ] + n log(2ð)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5" xml:id="_BJnpdVT">Intuition for the use of random effects</head><p xml:id="_AaU58Ys"><s xml:id="_t63sdHv">IntellIgentPoolIng uses random effects to adaptively pool users' data based on the degree to which users exhibit heterogeneous rewards.</s><s xml:id="_Y8jNMCY">That is, the person-specific random effect should outweigh the population term if users are highly heterogeneous.</s><s xml:id="_rg63NVx">If users are highly homogeneous, the person-specific random effect should be outweighed by the population term.</s><s xml:id="_7CA6WZ2">The amount of pooling is controlled by the hyper-parameters, e.g., the variance components of the random effects.</s></p><p xml:id="_Rj3VtYT"><s xml:id="_TT8rVnP">To gain intuition, we consider a simple setting where the feature vector in the reward model (Eq. 1) is one-dimensional (i.e., p = 1 ) and there are only two users (i.e., i = 1, 2 ).</s><s xml:id="_gRZAVKJ">Denote the prior distributions of population parameter w pop by (0, 2 w ) and the random effect u i by (0, 2 u ) .</s><s xml:id="_nhX7ntV">Below we investigate how the hyper-parameter (e.g., 2 u in this simple case) impacts the posterior distribution.</s></p><p xml:id="_zVJMsVw"><s xml:id="_xVENMXZ">Let k i be the number of decision time of user i at an updating time.</s><s xml:id="_gjMVMQJ">In this simple setting, the posterior mean of Åµ1 can be calculated explicitly:</s></p><formula xml:id="formula_4">where for i = 1, 2 , C i = â k i k=1 (A i,k , S i,k ) 2 , Y i = â k i k=1 (A i,k , S i,k )R i,k , = 2 w â( 2 w + 2 u ) and = 2 â 2 w .</formula><p xml:id="_95qBRrG"><s xml:id="_8fESFVK">Similarly, the posterior mean of w 2 is given by When 2 u â 0 (i.e., the variance of random effect goes to 0), we have â 1 and both pos- terior means ( Åµ1 , Åµ2 ) approach the posterior mean under comPlete (Eqn 3) using prior (0, 2 w )</s></p><formula xml:id="formula_5">Åµ1 = [ð¿ð¾ + (1 -ð¾ 2 )C 2 ]Y 1 + ð¿ð¾ 2 Y 2 (1 -ð¾ 2 )C 1 C 2 + ð¿ð¾(C 1 + C 2 ) + (ð¿ð¾) 2 Åµ2 = [ð¿ð¾ + (1 -ð¾ 2 )C 1 ]Y 2 + ð¿ð¾ 2 Y 1 (1 -ð¾ 2 )C 1 C 2 + ð¿ð¾(C 1 + C 2 ) + (ð¿ð¾) 2 1 3</formula><p xml:id="_aTUpFxP"><s xml:id="_Vu3Hvjp">Alternatively, when 2 u â â , we have â 0 and the posterior means ( Åµ1 , Åµ2 ) each approach their respective posterior means under Person-sPecIfIc (Eqn 1) using a noninformative prior Figure <ref type="figure">4</ref> illustrates that when goes from 0 to 1, the posterior mean Åµi smoothly transitions from the population estimates to the person-specific estimates.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6" xml:id="_XHFV7nm">Regret</head><p xml:id="_RNFbudh"><s xml:id="_gzJxyr9">We prove a regret bound for a modification of IntellIgentPoolIng similar to that in <ref type="bibr" target="#b1">Agrawal and Goyal 2012;</ref><ref type="bibr" target="#b54">Vaswani et al. 2017</ref> in a simplified setting.</s><s xml:id="_ruYPMac">Further details are provided in Appendix 1.</s><s xml:id="_SNChja7">Let d be the length of the weight vector w i in the Bayesian mixed effects model of the reward in Eq. 1. Recall that w is the prior covariance of the weight vector w pop , u is the covariance of the random effect u i and 2 is the variance of the error term.</s><s xml:id="_VfNva4T">Let K i be the number of decision times for user i up to a given calendar time and T = â N i=1 K i be the total number of decision times encountered by all N users in the study up to the calendar time.</s><s xml:id="_2Zwtc74">We define the regret of the algorithm after T decision times by</s></p><formula xml:id="formula_6">R(T) = â N i=1 â K i k=1 max a (S i,k , a) T w i -(S i,k , A i,k ) T w i .</formula><p xml:id="_dm8nxWg"><s xml:id="_HN4geg3">Theorem 1 With probability 1 -, where â (0, 1) the total regret of the modified Thomp- son Sampling with IntellIgentPoolIng after T total number of decision times is:</s></p><p xml:id="_bBnvSDV"><s xml:id="_vVn4Mbh">Remark Observe that, up to logarithmic terms, this regret bound is Ã(dN â T) .</s><s xml:id="_ba2KAuC">Recall that <ref type="bibr" target="#b54">(Vaswani et al. 2017</ref>) introduces a similar regret bound for a Thompson Sampling algorithm which utilizes user-similarity information.</s><s xml:id="_GJ6qxGC">The bound from <ref type="bibr" target="#b54">(Vaswani et al. 2017)</ref>,</s></p><formula xml:id="formula_7">Åµ1 , Åµ2 â Y 1 + Y 2 C 1 + C 2 + ð¿ . Åµ1 â Y 1 C 1 , Åµ2 â Y 2 C 2 . R(T) = Ãï¿½ dN â T ï¿½ log ï¿½ (Tr(ð´ w ) + Tr(ð´ u ) + Tr(ð´ -1 u )) d + T ð 2 ð dN ï¿½ log 1 ð¿ ï¿½ Fig. 4</formula><p xml:id="_XKSfaWq"><s xml:id="_rRnfDhG">The posterior mean of w i , Åµ1 .</s><s xml:id="_pAGsgPu">As the variance of random effect 2 u decreases, increases and the posterior mean approaches the populationinformed estimation (comPlete) and departs from the personspecific estimation (Person-sPecIfIc).</s></p><p xml:id="_kXBebQp"><s xml:id="_P7vSUrM">1 3 Ã(dN â Tâ) , additionally depends on a hyper-parameter that is not included in our model.</s><s xml:id="_amgXHpj">In <ref type="bibr" target="#b54">(Vaswani et al. 2017)</ref>, controls the strength of prior user-similarity information.</s><s xml:id="_vDxrWde">Instead of introducing a hyper-parameter our model follows a mixed effects Bayesian structure which allows user similarities (as expressed in the extent to which users' data is pooled) to be updated with new data.</s><s xml:id="_T7wj6FN">Thus, in certain regimes of hyper-parameter , Intel-lIgentPoolIng will incur much smaller regret, as demonstrated empirically in Sect.</s><s xml:id="_kK5K4cB">4.3.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_CGYFAuA">Experiments</head><p xml:id="_qSdRpMt"><s xml:id="_vbvGARj">This work was conducted to prepare for deployment of IntellIgentPoolIng in a live trial.</s><s xml:id="_PNAzGQv">Thus, to evaluate IntellIgentPoolIng we construct a simulation environment from a precursor trial, HeartstePsV1 <ref type="bibr" target="#b28">(Klasnja et al. 2015)</ref>.</s><s xml:id="_vE3Nn97">This simulation allows us to evaluate the proposed algorithm under various settings that may arise in implementation.</s><s xml:id="_tXX7y2K">For example, heterogeneity in the observed rewards may be due to unknown subgroups across which users' reward functions differ.</s><s xml:id="_BATkjEd">Alternatively, this heterogeneity may vary across users in a more continuous manner.</s><s xml:id="_BgtDNm3">We consider both scenarios in simulated trials.</s><s xml:id="_sN7Ks7s">In Sects.</s><s xml:id="_836aH6E">4.1-4.3</s><s xml:id="_VsmWZyw">we evaluate the performance of IntellIgentPoolIng against baselines and a state-of-the-art algorithm.</s><s xml:id="_XqbEE4h">In Sect. 5 we assess feasibility of IntellIgentPoolIng in a pilot deployment in a clinical trial.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_bM2dATR">Simulation environment</head><p xml:id="_NsVQz7U"><s xml:id="_N835un8">HeartstePsV1 was a 6-week micro-randomized trial of an Android-based physical activity intervention with 41 sedentary adults.</s><s xml:id="_3X52WGk">The intervention consisted of two push interventions: planning and contextually-tailored activity suggestions.</s><s xml:id="_9ucwZap">Activity suggestions acted as action cues and were designed to provide users with actionable options for engaging in short bouts of activity in their current situation.</s><s xml:id="_4WJnxJR">The content of the suggestions was tailored based on the users' location, weather, time of day, and day of the week.</s><s xml:id="_F77aDqM">For each individual, on each day of the study, the HeartSteps system randomized whether or not to send an activity suggestion five times a day.</s><s xml:id="_nEUWXut">The intended outcome of the suggestions-the proximal outcome used to evaluate their efficacy-was the step count in the 30 minutes following suggestion randomization.</s></p><p xml:id="_b6hdxbV"><s xml:id="_q3thQQ8">HeartstePsV1 data was used to construct all features within the environment, and to guide choices such as how often to update the feature values.</s><s xml:id="_ADWxFCT">Recall that S i,k and R i,k denote the context features and reward of user i at the k th decision time.</s><s xml:id="_7w3NjP7">The reward is the log step counts in the thirty minutes immediately following a decision time.</s><s xml:id="_ED6hTkt">In HeartstePsV1 three treatment actions were considered: A i,k = 1 corresponded to a smartphone notification containing an activity suggestion designed to take 3 minutes to perform, A i,k = 0 corresponded to a smartphone notification containing an anti-sedentary message designed to take approximately 30 seconds to perform and A i,k = -1 corresponded to not sending a message.</s><s xml:id="_xgV6bQy">However, in the simulation only the actions 1, 0 are considered.</s></p><p xml:id="_hpexCUA"><s xml:id="_4JY5xQ4">Figure <ref type="figure" target="#fig_0">5</ref> describes the simulation while Table <ref type="table" target="#tab_1">1</ref> describes context features and rewards.</s><s xml:id="_cyEb5sU">Each context feature in Table <ref type="table" target="#tab_1">1</ref> was constructed from HeartstePsV1 data.</s><s xml:id="_CJTJakN">For example, we found that in HeartstePsV1 data splitting participants' prior 30 minute step count into the two categories of high or low best explained the reward.</s><s xml:id="_UBXxHfJ">Additional details about this process are included in Appendix 4.</s></p><p xml:id="_NhYtbAK"><s xml:id="_E5mY3Yz">The temperature and location are updated throughout a simulated day according to probabilistic transition functions constructed from HeartstePsV1.</s><s xml:id="_Qqheuw9">The step counts for a simulated user are generated from participants in HeartstePsV1 as follows.</s><s xml:id="_vrgBdkf">We construct a one-hot feature vector containing the group-ID of a participant, the time of day, the day of the week, the temperature, the preceding activity level, and the location.</s><s xml:id="_qBMhnQK">Then for each possible realization of the one-hot encoding we calculate the empirical mean and empirical standard deviation of all step counts observed in HeartstePsV1.</s><s xml:id="_cNSVUWd">The corresponding empirical mean and empirical standard deviation from HeartstePsV1 form S i,k S i,k respectively.</s><s xml:id="_6WkB3cu">At each 30 minute window, if a treatment is not delivered step counts are generated according to  Heterogeneity This model, which we denote HeterogeneIty, allows us to compare the performance of the approaches under different levels of population heterogeneity.</s><s xml:id="_5cCvrnj">The step count after a decision time is a modification of Eq. 8 to reflect the interaction between context and treatment on the reward and heterogeneity in treatment effect.</s><s xml:id="_d4YsTnK">Let be a vector of coefficients of S i,k which weigh the relative contributions of the entries of S i,k that inter- act with treatment on the reward.</s><s xml:id="_DgdnW9M">The magnitude of the entries of are set using Heart-stePsV1.</s></p><p xml:id="_ATsZFFt"><s xml:id="_GQuaBmx">Step counts ( R i,k ) are generated as</s></p><p xml:id="_tAHTC5y"><s xml:id="_HfBgaMZ">The inclusion of Z i will allow us to evaluate the relative performance of each approach under different levels of population heterogeneity.</s><s xml:id="_vZY4k4U">Let l i be the entry in i corresponding to the location term for the i th user.</s><s xml:id="_84VssfP">We consider three scenarios (shown in Table <ref type="table">6</ref>) to generate Z i , the person-specific effect, and l i the location-dependent effect.</s><s xml:id="_XeayE5T">The performance of each algorithm under each scenario will be analyzed in Sect.</s><s xml:id="_GUJKmSu">4.3.</s><s xml:id="_hS49HNs">In the smooth scenario, is equal to the standard deviation of the observed treatment effects [f (S i,k ) â¤  â¶ S i,k â HEARTSTEPSV1] .</s><s xml:id="_mvuqXbf">The settings for all Z i and l i terms are discussed in Sect.</s><s xml:id="_avKJube">D.</s></p><p xml:id="_j8jP9GE"><s xml:id="_bERKQry">In the bi-modal scenario each simulated user is assigned a base-activity level: low-activity users (group 1) or high-activity users (group 2).</s><s xml:id="_8NjuNYQ">When a simulated user joins the trial they are placed into either group one or two with equal probability.</s><s xml:id="_UeUH9Pm">Whether or not it is optimal to send a treatment (an activity suggestion) for user i at their k th decision time depends both on their context, and on the values of z 1 , l 1 and z 2 , l 2 .</s><s xml:id="_SHbemDE">The values of z 1 , l 1 and z 2 , l 2 are set so that for all users in group 1, it is optimal to send a treatment under 75% of the contexts they will experience.</s><s xml:id="_2Sw5ugp">Yet for all users in group 2, it is only optimal to send a treatment under 25% of the contexts they will experience.</s><s xml:id="_vepcDFP">Group membership is not known to any of the algorithms</s></p><p xml:id="_ewRMaE8"><s xml:id="_TFvBtRz">Table 2.</s><s xml:id="_zZNb9nC">The settings for all values in Table 6 are included in Sect.</s><s xml:id="_b6PQPch">D.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_NuUy9Zw">Model for the reward function in IntellIgentPoolIng</head><p xml:id="_dmG45WD"><s xml:id="_bXUWYDw">In Sect. 3 we introduced the feature vector (S i,k , A i,k ) â â p .</s><s xml:id="_gtkmwXU">This vector is used in the model for the reward and transforms a user's contextual state variables S i,k and the action A i,k as follows:</s></p><p xml:id="_aMM59tC"><s xml:id="_QKSSvb5">where S i,k = {1, time of day, day of the week, preceding activity level, location} .</s><s xml:id="_Rmkw8Kr">Recall that the bandit algorithms produce i,k which is the probability that A i,k = 1 .</s><s xml:id="_M74cScA">The inclusion of (8)</s></p><formula xml:id="formula_8">R i,k = ( S i,k , 2 S i,k ). (9) R i,k = ( S i,k , 2 S i,k ) + A i,k (S T i,k i + Z i ). (<label>10</label></formula><formula xml:id="formula_9">) (S i,k , A i,k ) T = S T i,k , i,k S T i,k , (A i,k -i,k )S i,k ,</formula><formula xml:id="formula_10">Z i = 0 l i =0 Z i , l i = z 1 , l 1 if i â group one z 2 , l 2 if i â group two Z i â¼ N(0, 2 ) l i â¼ N(0, 2 l )</formula><p xml:id="_zwFfkfb"><s xml:id="_qccYzYh">the term (A i,ki,k )S i,k is motivated by <ref type="bibr" target="#b35">Liao et al. 2016;</ref><ref type="bibr" target="#b6">Boruvka et al. 2018;</ref><ref type="bibr" target="#b22">Greenewald et al. 2017</ref>, who demonstrated that action-centering can protect against mis-specification in the baseline effect (e.g., the expected reward under the action 0).</s><s xml:id="_vXqn2j4">In HEARTSTEPSV1 we observed that users varied in their overall responsivity and that a user's location was related to their responsivity.</s><s xml:id="_8br7TwZ">In the simulation, we assume the person-specific random effect on four parameters in the reward model (i.e., the coefficients of terms in S involving the intercept and location).</s><s xml:id="_MpXYtvF">Finally, we constrain the randomization probability to be within [0.1, 0.8] to ensure continual learning.</s><s xml:id="_GVXmQN8">The update time for the hyper-parameters is set to be every 7 days.</s><s xml:id="_zZVS87k">All approaches are implemented in Python and we implement GP regression with the software package (GPytorch <ref type="bibr" target="#b21">Gardner et al. 2018)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_NKkKuwu">Simulation results</head><p xml:id="_7y8jZfT"><s xml:id="_4tBp4YH">In this section, we compare the use of mixed effects model for the reward function in INTELLIGENTPOOLING to two standard methods used in mHealth, COMPLETE and PERSON -SPECIFIC from Sect.</s><s xml:id="_DJGxxVg">3.3.</s><s xml:id="_uevTMcg">Recall that IntellIgentPoolIng includes person-spe- cific random effects, as described in Eq. 14.</s><s xml:id="_YCDr3cg">In PERSON -SPECIFIC , all users are assumed to be different and there is no pooling of data and in COMPLETE , we treat all users the same and learn one set of parameters across the entire population.</s></p><p xml:id="_E3zwYJs"><s xml:id="_XHqWwK7">Additionally, to assess IntellIgentPoolIng's ability to pool across users we compare our approach to Gang of Bandits <ref type="bibr" target="#b10">(Cesa-Bianchi et al. 2013</ref>), which we refer to as gan-goB.</s><s xml:id="_2aARcnC">As this model requires a relational graph between users, we construct a graph using the generative model ( <ref type="formula">9</ref>) and Table <ref type="table">6</ref> connecting users according to each of the three settings: homogeneous, bi-modal and smooth.</s><s xml:id="_4uAdcmU">For example, with knowledge of the generative model users can be connected to other users as a function of their Z i terms.</s><s xml:id="_zjPRquu">As we will not have true access to the underlying generative model in a real-life setting we distort the true graph to reflect this incomplete knowledge.</s><s xml:id="_8shvKAj">That is we add ties to dissimilar users at 50% of the strength of the ties between similar users.</s></p><p xml:id="_q9ZZskz"><s xml:id="_A4krHbk">From the generative model ( <ref type="formula">9</ref>), the optimal action for user i at the k th decision time is</s></p><formula xml:id="formula_11">a * i,k = 1 {S T i,k * i +Z i â¥0} . The regret is</formula><p xml:id="_AQn2jpw"><s xml:id="_Z3hKv9Q">where * i is the optimal for the ith user.</s><s xml:id="_KxPUrXD">In these simulations each trial has 32 users.</s><s xml:id="_Y7SAM79">Each user remains in the trial for 10 weeks and the entire length of the trial is 15 weeks, where the last cohort joins in week six.</s><s xml:id="_FuF9hUp">The number of users who join each week is a function of the recruitment rate observed in HEARTSTEPSV1 .</s><s xml:id="_HJUzbfr">In all settings we run 50 simulated trials.</s></p><p xml:id="_Kg8hnjt"><s xml:id="_hGUmwxM">First, Fig. <ref type="figure" target="#fig_1">6</ref> provides the regret averaged across all users across 50 simulated trials where the reward distribution follows (9) for each of the Table <ref type="table">6</ref> categories.</s><s xml:id="_TuBZfFH">The horizontal axis in Fig. <ref type="figure" target="#fig_1">6</ref> is the average regret over all users in their nth week in the trial, e.g. in their first week, their second week, etc. In the bi-modal setting there are two groups, where all users in group one have a positive response to treatment when experiencing their typical context, while the users in group two have a negative response to treatment under their typical context.</s><s xml:id="_hNgNcNx">An optimal policy would learn to not typically send treatments to users in the first group, and to typically send them to users in the second.</s><s xml:id="_VCVmS5Q">To evaluate each algorithm's (11)</s></p><formula xml:id="formula_12">regret i,k = |S T i,k * i + Z i |1 {a * i,k â A i,k }</formula><p xml:id="_HU7YFNm"><s xml:id="_vyrM5q8">ability to learn this distinction we show the percentage of time each group received a message in Table <ref type="table">3</ref>.</s></p><p xml:id="_ZejaMRQ"><s xml:id="_ZM6MWEq">The relative performance of the approaches depends on the heterogeneity of the population.</s><s xml:id="_ZKHzAN2">When the population is very homogenous comPlete excels, while its performance suffers as heterogeneity increases.</s><s xml:id="_e9YbBea">Person-sPecIfIc is able to personalize; as shown by Table <ref type="table">3</ref>, it can differentiate between individuals.</s><s xml:id="_fcn5MNe">However, it learns slowly and can only approach the performance of comPlete in the smooth setting of Table <ref type="table">6</ref> where users differ the most in their response to treatment.</s><s xml:id="_6Fnqxxr">Both IntellIgentPoolIng and gangoB are more adaptive than either comPlete or Person-sPecIfIc.</s><s xml:id="_C83hdA4">gangoB consistently outperforms Person-sPe-cIfIc and achieves lower regret than comPlete in some settings.</s><s xml:id="_APEpumk">In the homeogenous setting we see that gangoB can utilize social information more effectively than Person-sPecIfIc does while in the smooth setting it can adapt to individual differences more effectively than comPlete.</s><s xml:id="_T29SeFn">Yet, IntellIgentPoolIng demonstrates stronger and swifter adaptability than does gangoB, consistently achieving lower regret at quicker rates.</s><s xml:id="_bjzCm9p">Finally, the algorithms differ in their suitability for real-world applications, especially when data is limited.</s><s xml:id="_U58Hb2T">Table <ref type="table">3</ref> The fraction of time that messages were sent to users in each group Recall at each decision time either an activity suggestion or anti-sedentary message is sent.</s><s xml:id="_5BYg4Aw">For group one it is typically optimal to send an activity suggestion, while for group two it is typically optimal to send an anti-sedentary message.</s><s xml:id="_49An5tG">Here, IntellIgentPoolIng is best able to learn this dynamic Group one optimal policy = send activity suggestion Group two optimal policy = send anti-sedentary message</s></p><p xml:id="_6b5AJBp"><s xml:id="_CSW54QB">Complete 0.49 0.46 Person-specific 0.65 0.49 GangOB 0.57 0.35 IntellIgent-PoolIng 0.59 0.36 1 3 gangoB requires reliable values for hyper-parameters and can depend on fixed knowledge about relationships between users.</s><s xml:id="_jn5NtAR">IntellIgentPoolIng can learn how to pool between individuals over time and without prior knowledge.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_wrBrsBn">IntellIgentPoolIng feasibility study</head><p xml:id="_FeRd9Sv"><s xml:id="_J9NuDkU">The simulated experiments provide insights into the potential of this approach for a live deployment.</s><s xml:id="_ebYgKfR">As we see reasonable performance in the simulated setting, we now discuss an initial pilot deployment of IntellIgentPoolIng in a real-life physical activity clinical trial.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_5BVVz9h">Feasibility study design</head><p xml:id="_pp8baXy"><s xml:id="_nAhMktx">The feasibility study of IntellIgentPoolIng involves 10 participants added to a larger 90-day clinical trial of HeartSteps v2, an mHealth physical activity intervention.</s><s xml:id="_XS96Tap">The purpose of the larger clinical trial is to optimize the intervention for individuals with Stage 1 hypertension.</s><s xml:id="_V35hadz">Study participants with Stage 1 hypertension were recruited from Kaiser Permanente Washington in Seattle, Washington.</s><s xml:id="_MXSfjMu">The study was approved by the institutional review board of the Kaiser Permanente Washington Health Research Institute (under number 1257484-14).</s><s xml:id="_YsF5Zqz">HeartSteps v2 is a cross-platform mHealth application that incorporates several intervention components, including weekly activity goals, feedback on goal progress, planning, motivational messages, prompts to interrupt sedentary behavior, and-most relevant to this paper-actionable, contextually-tailored suggestions for individuals to perform a short physical activity (suggesting, roughly, a 3 to 5 minute walk).</s><s xml:id="_VBadm8C">In this study physical activity is tracked with a commercial wristband tracker, the Fitbit Versa smart watch.</s></p><p xml:id="_8FegTRR"><s xml:id="_qhmzr45">In this version of the intervention, activity suggestions are randomized five times per day for each participant on each day of the 90-day trial.</s><s xml:id="_haM8Mh9">These decision times are specified by each user at the start of the study, and they roughly correspond to the participant's typical morning commute, lunch time, mid-afternoon, evening commute, and after dinner periods.</s><s xml:id="_cm5nE6j">The treatment options for activity suggestions are binary: at a decision time, the system can either send or not send a notification with an activity suggestion.</s><s xml:id="_EcKjuWu">When provided, the content of the suggestion is tailored to current sensor data (location, weather, time of day, and day of the week).</s><s xml:id="_hkAJ4Gn">Examples of these suggestions are provided in <ref type="bibr" target="#b29">Klasnja et al. 2018</ref>.</s><s xml:id="_GhfDsfh">At a decision time, activity suggestions are randomized only if the system considers that the user is available for the intervention-i.e., that it is appropriate to intervene at that time (see Fig. <ref type="figure">8</ref> for criteria used to determine if it is appropriate to send an activity suggestion at a decision time).</s><s xml:id="_4TbqqHs">Subject to these availability criteria, IntellIgentPoolIng determines whether to send a suggestion at each decision time.</s><s xml:id="_4ZTH8dh">The posterior distribution was updated once per day, prior to the beginning of each day.</s><s xml:id="_jy2TdMb">Figure <ref type="figure" target="#fig_2">7</ref> provides a schematic of the feasibility study.</s></p><p xml:id="_bt93dSH"><s xml:id="_rFgS8BH">The feasibility study included the second set of 10 participants in the trial of Heart-Steps v2, following the initial 10 enrolled participants.</s><s xml:id="_ukhv2c8">IntellIgentPoolIng (Algorithm 1) is deployed for each of the second set of 10 participants.</s><s xml:id="_rFWC6Vk">At each decision time for these 10 participants, IntellIgentPoolIng uses all data up to that decision time (i.e. from the initial ten participants as well as from the subsequent ten participants).</s><s xml:id="_PGvhMgn">Thus the feasibility study allows us to assess performance of IntellIgentPoolIng after the beginning of a study instead of the performance at the beginning of the study (when there is little data) or the performance at the end of the study (when there is a large amount of data and the algorithm can be expected to perform well).</s></p><p xml:id="_946nhMV"><s xml:id="_2CsBZmw">In the feasibility study, the features used in the reward model were selected to be predictive of the baseline reward and/or the treatment effect, based on the data analysis of Heart-stePsV1; see Sect.</s><s xml:id="_nmDpDnR">6.2 in <ref type="bibr" target="#b36">(Liao et al. 2020)</ref> for details.</s><s xml:id="_qMAS4Ta">All features used in the reward model are shown in Table <ref type="table" target="#tab_5">4</ref>.</s><s xml:id="_8QMTYSV">The feature engagement represents the extent to which a user engages with the mHealth application measured as a function of how many screen views are made within the application within a day.</s><s xml:id="_DQ9BdHz">The feature dosage represents the extent to which a user has received treatments (activity suggestions).</s><s xml:id="_e2CJRCc">This feature increases and decreases depending on the number of activity suggestions recently received.</s><s xml:id="_fZyAd3B">The feature location refers to whether a user is at home or work (encoded as a 1) or somewhere else (encoded as a 0).</s><s xml:id="_e9p5rzz">The temperature feature value is set according to the temperature at a user's current location (based off of phone GPS).</s><s xml:id="_JxmpYpK">The variation feature value is set according to the variation in step count in the hour around that decision point over the prior seven-day period.</s><s xml:id="_BZxVfgy">As before we construct a feature vector , however here we only use select terms to estimate the treatment effect.</s><s xml:id="_NPbBfmH">Here, (12)</s></p><formula xml:id="formula_13">(S i,k , A i,k ) T = S T i,k , i,k S ï¿½ T i,k , (A i,k -i,k )S ï¿½ i,k ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pRsBkT2">Users enter the trial asynchronously</head><p xml:id="_exn6yAX"><s xml:id="_zFCQDF6">A user is available to receive an activity suggestion under the following conditions:</s></p><p xml:id="_h3HPJ2J"><s xml:id="_8GqCwMW">-She is not currently active and has not had a large amount of activity in the last two hours.</s><s xml:id="_gRA98cw">-She has not recently received a notification with a HeartSteps intervention.</s></p><p xml:id="_Z67Tt9D"><s xml:id="_GeqKcXg">-Her phone has an internet connection and can communicate with the HeartSteps server.</s></p><p xml:id="_NWdwJ4u"><s xml:id="_Yb79Nkb">-Her smart watch has been able to communicate with the HeartSteps server in the last ten minutes to provide the current location and step count data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_emteUAZ">Fig. 8 Availability criteria</head><p xml:id="_BxqHV9V"><s xml:id="_fE6aUW3">where S i,k = {1, temperature, yesterday's step count, preceding activity level, step variation, step variation, engagement, dosage, location} and S ï¿½ i,k = {1, step variation, engagement, dosage, location} is a subset of S i,k .</s></p><p xml:id="_8zVscWT"><s xml:id="_5weWUHK">We provide a full description of these features in Sect.</s><s xml:id="_AKch7Td">E. The prior distribution was also constructed based on HeartstePsV1; see Sect.</s><s xml:id="_CfBAqYC">6.3 in <ref type="bibr" target="#b36">(Liao et al. 2020</ref>) for more details.</s><s xml:id="_gXmTRS7">As this feasibility study only includes a small number of users, a simple model with only two person-specific random effects, each on the intercept term in S and S â² (Eq.</s><s xml:id="_unB2vGv">12) was deployed.</s></p><p xml:id="_f9Gx4ew"><s xml:id="_apAQke3">Here we discuss how much data we have to personalize the policy to each user.</s><s xml:id="_X8Mat86">Recall the 10 users only receive interventions when they meet the availability criteria outlined in Fig. <ref type="figure">8</ref>, thus we find that in practice we have a limited number of decision points to learn a personalized policy from.</s><s xml:id="_8h5ttB2">In the case of perfect availability, we would have at most 450 decision points per person.</s><s xml:id="_HQMZ7AF">However due to the criteria in Fig. <ref type="figure">8</ref>, the algorithm is used with only approximately 23% of each user's decision points.</s><s xml:id="_C4PeESn">Pooling users' data allows us to learn more rapidly.</s><s xml:id="_RBgn77s">On the day that the first pooled user joined the feasibility study there were 107 data points from the first set of 10 users.</s></p><p xml:id="_BKz2jAz"><s xml:id="_tMwn3Yv">The 10 users received an average number of .20 ( Â±0.015 ) messages a day.</s><s xml:id="_Jxgtfcb">The aver- age log step count in the 30-minute window after a suggestion was sent was 4.47, while it was 3.65 in the 30-minute windows after suggestions were not sent.</s><s xml:id="_VNXdJs7">Figure <ref type="figure">9</ref> shows the entire history of treatment selection probabilities for all of the users who received treatment according to IntellIgentPoolIng.</s><s xml:id="_Xkt8SJ5">We see that the treatment probabilities tended to be low, though they covered the whole range of possible values.</s></p><p xml:id="_RMCf9yc"><s xml:id="_qca8XSm">We would like to assess the ability of IntellIgentPoolIng to personalize and learn quickly.</s><s xml:id="_4R6pTeX">To do so we perform an analysis of the learning algorithms of IntellIgentPoolIng, comPlete and Person-sPecIfIc on batch data containing tuples of (S, A, R).</s><s xml:id="_EKXdAvn">Note that the actions in this batch data were selected by IntellIgentPoolIng, however, here we are not interested in the action selection components of each algorithm but instead on their ability to learn the posterior distribution of the weights on the feature vector.</s></p><p xml:id="_ZgBXTup"><s xml:id="_NAGvzc8">Personalization By comparing how the decisions to treat under IntellIgentPoolIng differ from those under comPlete, we gather preliminary evidence concerning whether IntellIgentPoolIng personalizes to users.</s><s xml:id="_3QsKMqh">Figure <ref type="figure">10</ref> shows the posterior mean of the coefficient of the A i,k term in the estimation of the treatment effect, for all users in the feasibility study on the 90th day after the last user joined the study.</s><s xml:id="_faH6hNy">We show this term not only for IntellIgentPoolIng but also for comPlete and Person-sPecIfIc.</s><s xml:id="_wxJ7fAW">We see that for some users this coefficient is below zero while for others it is above.</s><s xml:id="_X5DHmKM">While the terms under IntellI-gentPoolIng differ from comPlete they do not vary as much as those learned by Person-sPecIfIc.</s><s xml:id="_muH758p">Yet, crucially, the variance is much lower for these terms.</s></p><p xml:id="_tFw2bKG"><s xml:id="_JNbs4Jb">Figure <ref type="figure" target="#fig_3">11</ref> displays the posterior mean of the coefficient of the A i,k term in the estimation of the treatment effect.</s><s xml:id="_uxcv5XQ">This coefficient represents the overall effect of treatment on one of the users, User A. During the prior 7 days User A had not experienced much variation in activity at this time and the user's engagement is low.</s><s xml:id="_wtTNyJa">Note that the treatment appears to have a positive effect on a different user, User B, in this context whereas on User A there is little evidence of a positive effect.</s><s xml:id="_YdTCVKg">If comPlete had been used to determine treatment, User A might have been over-treated.</s></p><p xml:id="_JJXTqkb"><s xml:id="_jJgF2DQ">Speed of policy learning We consider the speed at which IntellIgentPoolIng diverges from the prior, relative to the speed of divergence for Person-sPecIfIc.</s><s xml:id="_6gcvduw">Figure <ref type="figure">12</ref> provides the Euclidean distance between the learned posterior and prior parameter vectors (averaged across the data from the 10 users at each time).</s><s xml:id="_4r9J3JR">From Fig. <ref type="figure">12</ref> we see that Person-sPecIfIc Fig. <ref type="figure">9</ref> We see that IntellIgent-PoolIng covers the full range of treatment selection probabilities.</s><s xml:id="_V8V46QN">The tendency seems to be to send with a lower rather than higher probability Fig. <ref type="figure">10</ref> Posterior mean and standard deviation of the coefficient of A i,k in Eq. 12 for all users in the feasibility study hardly varies over time in contrast to IntellIgentPoolIng and comPlete, which suggests that Person-sPecIfIc learns more slowly.</s></p><p xml:id="_ytbUUEs"><s xml:id="_XSk8E5P">In conclusion IntellIgentPoolIng was found to be feasible in this study.</s><s xml:id="_VbbmcnP">In particular the algorithm was operationally stable within the computational environment of the study, produced decision probabilities in a timely manner, and did not adversely impact the functioning of the overall mHealth intervention application.</s><s xml:id="_ERsC9db">Overall, IntellIgentPoolIng produced treatment selection probabilities which covered the full range of available probabilities, though treatments tended to be sent with a low probability.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_NJmFNA9">Non-stationary environments</head><p xml:id="_uNFVM5E"><s xml:id="_jAp2fCb">An additional challenge in mHealth settings is that users' response to treatment can vary over time.</s><s xml:id="_VPt9ZTk">To address this challenge we show that our underlying model can be extended to include time-varying random effects.</s><s xml:id="_WZy59v4">This allows each policy to be aware of how a user's response to treatment might vary over time.</s><s xml:id="_8nyM8gJ">We propose a new simulation to evaluate this approach and show that IntellIgentPoolIng achieves state-of-the-art regret, adjusting to non-stationarity even as user populations vary from heterogenous to homogenous.</s><s xml:id="_wrM7yA3">Fig. 12 Mean squared distance of the posterior mean from prior mean of the coefficients of A i,k</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" xml:id="_wPGGrp6">Time-varying random effect</head><p xml:id="_rQxNUNc"><s xml:id="_eb5yD6U">In addition to user-specific random effects we extend our model to include time-specific random effects.</s><s xml:id="_buBDbxS">Consider the Bayesian mixed effects model with person-specific and timevarying effects: for user i at the k th decision time,</s></p><p xml:id="_KyY7tXu"><s xml:id="_JGCE9Dd">In addition, we impose the following additive structure on the parameters w i,k :</s></p><p xml:id="_znFjNPZ"><s xml:id="_vKfBc3e">where w pop is the population-level parameter, u i represents the person-specific deviation from w pop for user i and v k is the time-varying random effects allowing w i,k to vary with time in the study.</s></p><p xml:id="_5hb7bM7"><s xml:id="_TY8CJnm">The prior terms for this model are as introduced in Sect.</s><s xml:id="_dKSUNz6">3.4.</s><s xml:id="_Kfvz7gq">Additionally, v k has mean and covariance D v .</s><s xml:id="_2KGHKfM">The covariance between two relative decision times in the trial is</s></p><formula xml:id="formula_14">Cov(v k , v k ï¿½ ) = (k, k ï¿½ )D v , where (k, k ï¿½ ) = exp(-dist(k, k ï¿½ ) 2 â ) for a distance function, dist and</formula><p xml:id="_SjGG78T"><s xml:id="_HYtVjS3">. There is no change to Algorithm 1 except that now the algorithm would select the action based on the posterior distribution of w i,k , which depends on both the user and time in the study.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_Um3u9TT">Experiments</head><p xml:id="_Nx647X7"><s xml:id="_3HTHrS6">We now modify our original simulation environment so that users' responses will vary over time.</s><s xml:id="_M8sWepr">To do so we introduce the generative model Disengagement.</s><s xml:id="_GMRcxp8">This generative model captures the phenomenon of disengagement.</s><s xml:id="_8yaJJ3X">That is as users are increasingly exposed to treatment over time they can become less responsive.</s><s xml:id="_2Ja8SBc">This model adds a further term to (9), A i,k X T w w where X w is defined as follows.</s><s xml:id="_MZdAVC5">Let w i,k be the highest number of weeks user i has completed at time k; X w encodes a user's current week in a trial,</s></p><formula xml:id="formula_15">X w = [1 {w i,k =0} , â¦ , 1 {w i,k =11} ] .</formula><p xml:id="_MF6McFZ"><s xml:id="_N2tcXmS">We set w such that the longer a user has been in treatment, the less they respond to a treatment message.</s><s xml:id="_yWxQ4MT">When a simulated user is at a decision time the user will receive a treatment message according to whichever RL policy is being run through the simulation.</s></p><p xml:id="_egDA8hk"><s xml:id="_MAvTWKx">In order to evaluate the effectiveness of our time-varying model we compare to Time-Varying Gaussian Process Thompson Sampling (tV-gP) <ref type="bibr" target="#b4">(Bogunovic et al. 2016)</ref>.</s><s xml:id="_S382adF">This approach incorporates temporal information for non-stationary environments and was shown to be competitive to stationary models.</s><s xml:id="_hufAkgX">To compare this method to IntellIgent-PoolIng we use a linear kernel for the spatial component.</s><s xml:id="_qHa42QK">We then modify Eq. 6 to compute the posterior distribution by removing the random-effects and modifying the kernel (Eq.</s><s xml:id="_QqsztBz">5) to include the temporal terms introduced in <ref type="bibr" target="#b4">(Bogunovic et al. 2016)</ref>.</s></p><p xml:id="_6hxh3MZ"><s xml:id="_KhaRn9X">Figure <ref type="figure" target="#fig_4">13</ref> provides the regret averaged across all users across 50 simulated trials where the reward distribution follows generative model DISENGAGEMENT .</s><s xml:id="_5gj3zKJ">As before the hori- zontal axis in Fig. <ref type="figure" target="#fig_4">13</ref> is the average regret over all users in their n th week in the trial, e.g. in their first week, their second week, etc. In dIsengagement, the time-specific response to treatment is set so that a negative response to treatment is introduced in the seventh week of the trial.</s></p><p xml:id="_Nd3Mj4A"><s xml:id="_NYajZdb">In the dIsengagement condition as users become increasingly less responsive to treatment good policies should learn to treat less.</s><s xml:id="_WPQzK8R">Thus, Table <ref type="table">5</ref> provides the average number</s></p><formula xml:id="formula_16">(13) R i,k = ð(S i,k , A i,k ) â¤ w i,k + ð i,k . (14) w i,k = w pop + u i + v k ,</formula><p xml:id="_JAMrGMf"><s xml:id="_CBQwraC">of times a treatment is sent in the last week of the trial for both the first and last cohort.</s><s xml:id="_pppvMHh">We expect that a policy which learns not to treat will treat less often in the last week of the last cohort than in the last week of the first cohort.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_bhhXqgH">Limitations</head><p xml:id="_QX48rBY"><s xml:id="_BNBbzDy">A significant limitation with this work is that our pilot study involved a small number of participants.</s><s xml:id="_RESx5FH">Our results from this work must be considered with caution as preliminary evidence towards the feasibility of deploying IntellIgentPoolIng, and bandit algorithms in general, in mHealth settings.</s><s xml:id="_SmTBgfz">Moreover, we cannot claim to provide generalizable evidence that this algorithm can improve health outcomes; for this larger studies with more participants must be run.</s><s xml:id="_ymGqBwH">We offer our findings as motivation for such future work.</s></p><p xml:id="_WDGSWgH"><s xml:id="_7fEMW7X">Our proposed model is designed to overcome the challenges faced when learning personalized policies in limited data settings.</s><s xml:id="_sJXr9XV">As such, if data was abundant our model would likely have limited effectiveness compared to more complex models.</s><s xml:id="_5EJJ3da">For example, a more complex model could allow us to pool between users as a function of their similarity.</s><s xml:id="_vGdeRuX">Our Table 5 Average fraction of times treatment was sent (action=1), over 50 simulations (generative model HeterogeneIty with homogenous Z h setting) Cohort one week 10 Cohort six week 10 Complete 0.62 0.44 Person-specific 0.76 0.59 HordeOB-0.50</s><s xml:id="_QwUpz7T">0.57 TV-GP 0.64 0.31 Intelligent-Pooling 0.30 0.06</s></p><p xml:id="_jFtgWxg"><s xml:id="_z3Q57X2">current model instead determines the extent to which a given user deviates from the population and does not consider between-user similarities.</s><s xml:id="_8peRyVx">A limitation with our current understanding of mHealth is that it is unclear what a good similarity measure would be.</s><s xml:id="_RukJ5zq">We leave the question of designing a data-efficient algorithm for learning such a measure as future work.</s></p><p xml:id="_fSDDBgX"><s xml:id="_f9SExaK">A component of IntellIgentPoolIng is the use of empirical Bayes to update the model hyper-parameters.</s><s xml:id="_mXuAGP8">Here, we used an approximate procedure.</s><s xml:id="_gSVFwZV">However, with our model it is possible to produce exact updates in a streaming fashion and we are currently developing such an approach.</s></p><p xml:id="_GBywCAh"><s xml:id="_t4jMqfk">Ideally, we would evaluate IntellIgentPoolIng against all other approaches in a clinical trial setting.</s><s xml:id="_HpsVZUK">However, here we only demonstrated the feasibility of our approach on a limited number of users and did not have the resources to similarly test the other approaches.</s></p><p xml:id="_FY2n4xt"><s xml:id="_2YeNUrw">To overcome this limitation we constructed a realistic simulation environment so that we could evaluate on different populations without the costly investment of designing multiple arms of a real-life trial.</s><s xml:id="_XCysjJg">While the simulated experiments and the feasibility study together demonstrate the practicality of our approach, in future work one might deploy all potential approaches in simultaneous live trials.</s></p><p xml:id="_zk37TAg"><s xml:id="_4FyjxhN">Finally, IntellIgentPoolIng can incorporate a time-specific random effect to capture the phenomenon of responsivity changing over the course of a study.</s><s xml:id="_DnYuaba">There is much to be improved with this model.</s><s xml:id="_chFczhy">For example, the first cohort in a study will not have prior cohorts to learn from, and the final cohort will have the greatest amount of data to benefit from.</s><s xml:id="_wgB6G3p">Other models might treat different cohorts with greater equality.</s><s xml:id="_YRQYQVG">Furthermore, this representation does not incorporate alternative temporal information, such as continually shifting weather patterns, where temperatures might change slowly and gradually alter one's desire to exercise outside.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" xml:id="_6WUDqp8">Conclusion</head><p xml:id="_wJTXJEe"><s xml:id="_Vhxsafy">When data on individuals is limited a natural tension exists between personalizing (a choice which can introduce variance) and pooling (a choice which can introduce bias).</s><s xml:id="_KtMpHsR">In this work we have introduced a novel algorithm for personalized reinforcement learning, IntellIgentPoolIng that presents a principled mechanism for balancing this tension.</s><s xml:id="_Gyb3Xfq">We demonstrate the practicality of our approach in the setting of mHealth.</s><s xml:id="_8MQUtTn">In simulation we achieve improvements of 26% over a state-of-the-art-method, while in a live clinical trial we show that our approach shows promise of personalization on even a limited number of users.</s><s xml:id="_66vsgwe">We view adaptive pooling as a first step in addressing the trade-offs between personalization and pooling.</s><s xml:id="_qbBZxjE">The question of how to quantify the benefits and risks for individual users is an open direction for future work.</s><s xml:id="_mv5BACA">Specifically, we assume that the posterior distribution of all users is updated after every decision time and the hyper-parameters are fixed throughout the study.</s><s xml:id="_ynW46hV"><ref type="bibr" target="#b54">Vaswani et al. (2017)</ref> also provided a regret bound for the Thompson Sampling Horde of Bandits algorithm where the data is pooled using a known, prespecified, social graph.</s><s xml:id="_9rW89DE"><ref type="bibr" target="#b54">Vaswani et al. (2017)</ref> employ the conceptual framework of <ref type="bibr" target="#b1">Agrawal and Goyal (2012)</ref> which uses the concept of saturated and unsaturated arms to bound the regret.</s><s xml:id="_G34uHYC">They show that the regret for playing an arm from the unsaturated set (which includes the optimal arm) can be bounded by a factor of the standard deviation which decreases over time.</s><s xml:id="_adGspr9">Additionaly, they show that the probability of playing a saturated arm is small, so that an unsaturated arm will be played at each time with some constant probability.</s><s xml:id="_XD7cV62"><ref type="bibr" target="#b54">Vaswani et al. (2017)</ref> follow this argument, but adapt their proof to include the prior covariance of the social graph in the bound of the variance.</s><s xml:id="_6MVYg5c">Our proof follows along similar lines with the primary difference being how the prior covariance of all parameters is formulated.</s><s xml:id="_9Eve4FA">Specifically, the prior variance in <ref type="bibr" target="#b54">Vaswani et al. (2017)</ref> is constructed by the Laplacian matrix of the social graph, whereas ours is constructed based on the Bayesian mixed effects model (4).</s><s xml:id="_sbz9pcs">As a result, while in <ref type="bibr" target="#b54">Vaswani et al. (2017)</ref> the regret bound is stated in terms of properties of the social graph, our bound depends on properties of our mixed effects model (i.e., the covariance matrix of the random effects).</s></p><p xml:id="_4HYyjJD"><s xml:id="_zpmXbDM">Recall that w is the prior covariance of the weight vector w pop , u is the covariance of the random effect u i and 2 is the variance of the error term.</s><s xml:id="_k5PwcA4">We assume that both w pop and u i have the same dimensions and that u is invertible.</s><s xml:id="_85sQtz6">Additionally, for simplicity of presentation we assume that the largest eigenvalue in w is at most d and the largest eigenvalue of u is at most dN.</s></p><p xml:id="_wmkTmbh"><s xml:id="_Y6MTjUH">Recall that Theorem 1 bounds the regret of IntellIgentPoolIng at time T by: with probability 1 -.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YJdFUbf">Proof sketch of Theorem 1</head><p xml:id="_6HKzhfj"><s xml:id="_ndARBJC">We align the decision times from all users by the calendar time.</s><s xml:id="_Q7Z486Q">Specifically, for a given time t, we retrieve the user index encountered at time t by i(t) and retrieve this user's decision time index by k(t).</s><s xml:id="_CcZSasU">IntellIgentPoolIng selects an action A i(t),k(t) â A for time t â [1, â¦ , T] .</s><s xml:id="_HJD4RbQ">We denote the selected action at time t by A t .</s></p><p xml:id="_y6wb24Q"><s xml:id="_ZnF4M3q">In this setting, we combine each user specific variable into a global shared variable.</s><s xml:id="_rcsqstd">Recall that a feature vector (A i,k , S i,k ) encodes contextual variables for the action and state of user i at their k th decision time.</s><s xml:id="_6egmNZr">For simplicity, we denote by A t the action A i(t),k(t) at time t and denote the vector (A i(t),k(t) , S i(t),k(t) ) at time t by A t ,t .</s><s xml:id="_Yhqr4gH">Additionally, we let a,t refer to (a, S i(t),k(t) ) for any a â A .</s><s xml:id="_e2stvYS">We introduce a sparse vector A t ,t â â dN , which contains A t ,t vector among N d-dimensional vectors, the rest of which are zeros .</s></p><p xml:id="_Q5GSwT6"><s xml:id="_NeyAHyN">In proving the regret we consider the equivalent way of selecting the action.</s><s xml:id="_BtYnt4Q">Instead of randomizing the action by the probability, here to select an action we assume the algorithm draws a sample wt = wi(t),k(t) and then selects the action A t = A i(t),k(t) = argmax aâA  T a,t wt that maximizes the sampled reward.</s><s xml:id="_d9Xsete">Analogously to a,t , we define Ì t and Ì t as the sparse vec- tors which contain Åµi(t),k(t) and wi(t),k(t) respectively as the i(t)-th vector among Nd-dimen- sional vector, the rest of which are zeros.</s></p><p xml:id="_mMhkA5h"><s xml:id="_47VsPqu">We concatenate the person-specific parameters w i into â â dN .</s><s xml:id="_3DFBZP5">Let the prior covari- ance of be  0 = NÃN â  w + N â  u .</s><s xml:id="_wewzUyQ">At time t, all contexts observed thus far, for all</s></p><formula xml:id="formula_17">R(T) = Ãï¿½ dN â T ï¿½ log ï¿½ (Tr(ð´ w ) + Tr(ð´ u ) + Tr(ð´ -1 u )) d + T ð 2 ð dN ï¿½ log 1 ð¿</formula><p xml:id="_FEHSkJX"><s xml:id="_8vB4Dst">ï¿½ users, can be combined into one matrix t â â tÃdN where a single row s corresponds to a s ,s , the sparse context vector associated with the action A s taken for user i(s) at their k(s)th decision time.</s><s xml:id="_PHBVthz">Let,  t = 1  2   â¤ t  t +  0 .</s><s xml:id="_hkzPqSK">At each decision time t we draw a feature vector Ì t â¼ N( Ì t , v 2 t  -1 t ).</s><s xml:id="_Bk35jd8">Now, within this framework, we rewrite the instantaneous regret as  t = â¤ a * t ,t t -â¤ A t ,t t .</s><s xml:id="_HGVDWtj">We prove that with high probability both â¤ a,t Ì t and â¤ a,t Ì t are con- centrated around their respective means.</s><s xml:id="_PJHCJcV">The standard deviation around the reward at decision time t for action a is thus s a,t = â â¤ a,t  -1 t-1 a,t .</s><s xml:id="_VBGCXrJ">We proceed as in <ref type="bibr" target="#b1">Agrawal and Goyal (2012)</ref>, <ref type="bibr" target="#b54">Vaswani et al. (2017)</ref> by bounding three terms, the event E t , the event E t and â T t=1 s 2</s></p><formula xml:id="formula_18">A t ,t</formula><p xml:id="_s3QdPFx"><s xml:id="_jswYxDD">Definition 1 Let -1 umin be the inverse of the smallest eigenvalue of u , umax be the largest eigenvalue of u , pmax be the largest eigenvalue of w and let max = umax + pmax .</s><s xml:id="_Ggveham">We assume that umax â¤ dN and pmax â¤ d.</s></p><p xml:id="_8cWrVhM"><s xml:id="_N9jq9sT">Definition 2 For all a , define  a,t = â¤ a,t Ì t .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8Gcn7SQ">Definition 3</head><p xml:id="_u2kKjf9"><s xml:id="_NKWc7r7">Definition 4 Define E t and E t as the events that â¤ t Ì t and A t ,t are concentrated around their respective means.</s><s xml:id="_dhMWA6d">Recall that |A| is the total number of actions.</s><s xml:id="_87Xt8sF">Formally, define E t as the event that Define E t as the event that Let = 1 4e â .</s><s xml:id="_VnnuZPn">Given that the events E t and E t hold with high probability, we follow an argument similar to Lemma 4 of <ref type="bibr" target="#b1">Agrawal and Goyal (2012)</ref> and obtain the following bound:</s></p><p xml:id="_PYjkkew"><s xml:id="_FpMTfzm">To bound the variance of the selected actions, â T t=1 s A t ,t , we follow an argument similar to <ref type="bibr" target="#b54">Vaswani et al. (2017)</ref>, and include the prior covariance terms of our model.</s><s xml:id="_reac3Rh">We prove the following inequality:</s></p><formula xml:id="formula_19">l t = ï¿½ dNlog ï¿½ 1 + max -1 umin + t -1 umin dN ï¿½ + ï¿½ N pmax + umax v t = 2 ï¿½ dNlog ï¿½ 1 + max -1 umin + t -1 umin dN ï¿½ g t = min{ â 4dNln(t), â 4ln(ï¿½Aï¿½t)}v t + l T . âa â¶ | â¤ a,t Ì t -â¤ a,t | â¤ l t s a,t . âa â¶ |ð A t ,t -â¤ A t ,t Ì t | â¤ min{4dNlog(t), 4log(|A|t)}v t s a,t . (15) R(T) â¤ 3g T T ï¿½ t=1 s A t ,t + 2g T T ï¿½ t=1 1 t 2 + 6g T â ï¿½Aï¿½Tlog(2â ). (<label>16</label></formula><formula xml:id="formula_20">) T ï¿½ t=1 s A t ,t â¤ â dNT ï¿½ C ï¿½ log ï¿½ (Tr( w ) + Tr( u ) + Tr( -1 u )) d + T 2 dN ï¿½ï¿½ ,</formula><p xml:id="_XsKvpcj"><s xml:id="_zkTpSYP">where C is a constant equal to</s></p><p xml:id="_RwbQpyQ"><s xml:id="_gruG2r4">-1 umin log(1+ -1 umin 2 )</s></p><p xml:id="_dHZ7NhP"><s xml:id="_ZYd5Ftr">. By combining Eqs. 15 and 16 we obtain the bound given in Theorem 1. â» Appendix 2: Supporting Lemmas Definition 5 Recall that at time t we define as D t as the history of all observed states, actions, and rewards up to time t.</s><s xml:id="_Pf3CFcf">Define filtration F t-1 as the union of history until time t -1 , and the contexts at time t, i.e., F t-1 = {D t-1 , a,t , a â A}.</s><s xml:id="_UNgYE8Z">By definition,</s></p><formula xml:id="formula_21">F 1 â F 2 â¯ â F t-1 .</formula><p xml:id="_razNWeX"><s xml:id="_HkfXwNK">The following quantities are also determined by the history D t-1 and the contexts, a,t and are included in F t-1 .</s></p><p xml:id="_RzvTN57"><s xml:id="_yVsUbnd">â¢ Ì t ,  t-1</s></p><p xml:id="_BKteaNe"><s xml:id="_NXB2ZMR">â¢ s a,t âa â¢ the identity of the optimal action a * t</s></p><p xml:id="_jY8Wv8c"><s xml:id="_NPCe4j8">â¢ whether E t is true or not</s></p><formula xml:id="formula_22">â¢ the distribution of N( Ì t , v 2 t ðº -1 t-1 )</formula><p xml:id="_BXp2ahf"><s xml:id="_ZQUpSYS">Note that the actual action A t which is selected at decision point t is not included in F t-1 .</s></p><p xml:id="_YS4wt53"><s xml:id="_KsA8XY2">We now address the lemmas used in the proof which differ from <ref type="bibr" target="#b1">Agrawal and Goyal (2012)</ref>, <ref type="bibr" target="#b54">Vaswani et al. (2017)</ref>.</s></p><p xml:id="_zd8szj7"><s xml:id="_aFrNphX">Lemma 1 For â (0, 1) :</s></p><formula xml:id="formula_23">Proof The true reward at time t, R t = â¤ A t ,t + ð t . Let, ðº t Ì t = ð 2 ð . Define t-1 â t-1 l=1 l a l ,l</formula><p xml:id="_QbfVKZN"><s xml:id="_J4mDB9Q">.</s></p><p xml:id="_2AsKSpW"><s xml:id="_SKQdRFZ">The following holds for all a:</s></p><formula xml:id="formula_24">Pr(E t ) â¥ 1 - 2 t-1 = t-1 â l=1 (R l -â¤ a l ,l ) a l ,l = t-1 â l=1 (R l a l ,l -a l ,l â¤ a l ,l ) t-1 = b t-1 - t-1 â l=1 ( a l ,l â¤ a l ,l ) = b t-1 -ð 2 ð (ðº t-1 Ì t -ðº t-1 + ð´ 0 ) Ì t -= ðº -1 t-1 t-1 ð 2 ð -ð´ 0 . ï¿½ â¤ a,t Ì t -â¤ a,t ï¿½ = ï¿½ â¤ a,t ( Ì t -)ï¿½ â¤ ï¿½ ï¿½ a,t ðº -1 t-1 ï¿½ t-1 ð 2 ð -ð´ 0 ï¿½ ï¿½ ï¿½ â¤ â a,t â ðº -1 t-1 ï¿½ ï¿½ ï¿½ ï¿½ t-1 ð 2 ð -ð´ 0 ï¿½ ï¿½ ï¿½ðº -1 t-1 ï¿½ .</formula><p xml:id="_JJrYU2y"><s xml:id="_96t3b4m">1 3</s></p><p xml:id="_6gWKyvR"><s xml:id="_rVzsn7b">By the triangle inequality,</s></p><p xml:id="_3CxcdSC"><s xml:id="_HxXyhby">We now bound the term â 0 â -1 t-1</s></p><p xml:id="_WcnztED"><s xml:id="_nr9xrXG">. Recall that the prior covariance of</s></p><formula xml:id="formula_25">, ð´ 0 = NÃN â ð´ w + N â ð´ u . For bounding â a,t â -1 t-1 , note that</formula><p xml:id="_ZsPNEGY"><s xml:id="_t99dBX7">We can thus write Eq. 17</s></p><p xml:id="_b4PSG9R"><s xml:id="_RgSFtWr">We now bound</s></p><formula xml:id="formula_26">â â â t-1 â â â -1 t-1</formula><p xml:id="_eDat74H"><s xml:id="_jbUhC6A">.</s></p><p xml:id="_j3KYRnd"><s xml:id="_2WDtSBS">Theorem 2 For any d &gt; 0, t â¥ 1 , with probability at least 1 -, For any n Ã n matrix A, det(A) â¤ Tr(A)</s></p><p xml:id="_ak2u6rm"><s xml:id="_Q2StCJc">n n .</s><s xml:id="_YAH4Mxa">This implies, log(det(A)) â¤ nlog Tr(A)   n .</s><s xml:id="_dNwbBU9">Applying this inequality for both t and -1 0 , we obtain:</s></p><formula xml:id="formula_27">(17) ï¿½ â¤ a,t Ì t -â¤ a,t ï¿½ â¤ ï¿½ ï¿½ ï¿½ t-1 ð 2 ð ï¿½ ï¿½ ï¿½ðº -1 t-1 + âð´ 0 â ðº -1 t-1 ï¿½ ð max (ð´ 0 ) = ð max ( NÃN â ð´ w + N â ð´ u ) = ð max ( NÃN ) â ð max (ð´ w ) + ð max ( N ) â ð max (ð´ u ) = Nð max (ð´ w ) + ð max (ð´ u ) = Nð pmax + ð umax âð´ 0 â ðº -1 t-1 â¤ âð´ 0 â ð´ -1 0 = ï¿½ ð´ â¤ 0 ð´ -1 0 ð´ 0 = ï¿½ â¤ ð´ 0 â¤ â ð max (ð´ 0 )â â 2 â¤ â ð max (ð´ 0 ) â¤ ï¿½ Nð pmax + ð umax â a,t â ðº -1 t-1 = ï¿½ â¤ a,t ðº -1 t-1 a,t = s a,t . (18) ï¿½ â¤ a,t Ì t -â¤ a,t ï¿½ â¤ s a,t ï¿½ 1 ð ð ï¿½ ï¿½ ï¿½ t-1 ï¿½ ï¿½ ï¿½ðº -1 t-1 + â nð pmax + ð umax ï¿½ â â â t-1 â â â 2 -1 t-1 â¤ 2 2 log det t 1 2 det 0 -1 2 â¤ 2 2 log(det t 1 2 ) + log(det 0 -1 2 ) -log( ) â¤ 2 log(det t ) + log(det 0 -1 ) -2log( ) . (19) â â â t-1 â â â -1 t-1 â¤ dN 2 log Tr( t ) dN + log Tr( -1 0 ) dN - 2 dN log( )</formula><p xml:id="_wJVua7u"><s xml:id="_X25BXj5">Next, we use the fact that</s></p><p xml:id="_NfbFvCy"><s xml:id="_3bN29uf">We now return to Eq. 19</s></p><formula xml:id="formula_28">â» Lemma 2 With probability 1 - 2 ,</formula><p xml:id="_vkN8d2b"><s xml:id="_pUEMMkf">Proof Let Z l and Y t be defined as follows:</s></p><p xml:id="_HjSuAGm"><s xml:id="_xWET4v7">Hence, Y t is a super-martingale process:</s></p><formula xml:id="formula_29">ðº t = ð´ 0 + ð´ t l=1 a l ,l â¤ a l ,l â Tr(ðº t ) â¤ Tr(ð´ 0 ) + t Tr(ð´ 0 ) = Tr( NÃN â ð´ w + N â ð´ u ) = Tr( NÃN ) â Tr(ð´ w ) + Tr( N ) â Tr(ð´ u ) = NTr(ð´ w ) + NTr(ð´ u ) = N(Tr(ð´ w ) + Tr(ð´ u )) â â â t-1 â â â 2 ðº -1 t-1 â¤ dNð 2 ð log Tr(ð´ 0 ) + t dN + log Tr(ð´ -1 0 ) dN - 2 dN log(ð¿) â¤ dNð 2 ð log Tr(ð´ )Tr(ð´ -1 0 ) + tTr(ð´ -1 0 ) d 2 N 2 -log(ð¿ 2 dN ) = dNð 2 ð log Tr(ð´ 0 )Tr(ð´ -1 0 ) + tTr(ð´ -1 0 ) d 2 N 2 ð¿ â¤ dNð 2 ð log d 2 N 2 ð max ð -1 umin + tdNð -1 umin d 2 N 2 ð¿ = dNð 2 ð log ð max ð -1 umin ð¿ + tð -1 umin dNð¿ â â â t-1 â â âðº -1 t-1 â¤ ð ð â dNlog ð max ð -1 umin ð¿ + tð -1 umin dNð¿ â â â t-1 â â âðº -1 t-1 â¤ ð ð â dNlog 1 + ð max ð -1 umin ð¿ + tð -1 umin dNð¿ | â¤ a,t Ì t -â¤ a,t | â¤ s a,t â dNlog 1 + ð max ð -1 umin ð¿ + tð -1 umin dNð¿ + â Nð pmax + ð umax â¤ s a,t l t (20) T â t=1 regret(t) â¤ T â t=1 3g t s t + T â t=1 2g t t 2 s t + â â â â 2 T â t=1 36g 2 t 2 ln( 2 ) Z l = regret(l) - 3g l s l - 2g l l 2 s l Y l = t â l=1 Z l</formula><p xml:id="_rD4tRtJ"><s xml:id="_fHPg4H5">We now apply Azuma-Hoeffding inequality.</s><s xml:id="_cbqntEv">We define</s></p><formula xml:id="formula_30">Y 0 = 0 . Note that |Y t -Y t-1 | = |Z l | is bounded by 1 + 3g l -2g l . Hence, c = 6g t . Setting a = ï¿½ 2 ln( 2 )</formula><p xml:id="_dEjnDgA"><s xml:id="_eqHem6n">â T t=1 c 2 t in the above inequality, we obtain that with probability 1 -2 , â» Lemma 3 (Azuma-Hoeffding).</s><s xml:id="_9sbFbyc">If a super-martingale Y t (with t â¥ 0) and its the correspond- ing filtration F t-1 , satisfies |Y t -Y t-1 | â¤ ct for some constant c for all t = 1, â¦ , T then for any x â¥ 0:</s></p><formula xml:id="formula_31">Lemma 4 â T t=1 s A t ,t â¤ â dNT ï¿½ C ï¿½ log ï¿½ (Tr( w )+Tr( u )+Tr( -1 u )) d + T 2 dN ï¿½ï¿½ For simplicity, we let s A t ,t = s t below. [Y t -Y t-1 |F t-1 ] = [Z t ] = [regret(t)||F t-1 ] - 3g l s l - 2g l l 2 s l [regret(t)|F t-1 ] â¤ [ t |F t-1 ] â¤ 3g l s l + 2g l l 2 s l [Y t -Y t-1 |F t-1 ] â¤ 0 (21) Y t â¤ â â â â 2 ln( 2 ) T â t=1 36g 2 t (<label>22</label></formula><formula xml:id="formula_32">) T â t=1 regret(t) - 3g t s t - 2g t t 2 s t â¤ â â â â 2 ln( 2 ) T â t=1 36g 2 t (23) T â t=1 regret(t) â¤ T â t=1 3g t s t + T â t=1 2g t t 2 s t + â â â â 2 ln( 2 ) T â t=1 36g 2 t (24) Pr(Y t -Y 0 â¥ x) â¤ exp ï¿½ -x 2 2 â T t=1 c 2 t ï¿½ 1 3</formula><p xml:id="_StKzbrt"><s xml:id="_mEgY3X5">Using the determinant-trace inequality, we have the following relation:</s></p><formula xml:id="formula_33">det| NÃN â ð´ w | + det| N â ð´ u |) = det| NÃN | d det|ð´ w | N + det| N | d det|ð´ u | N = det|ð´ u | N log(det|ðº t |) â¥ log(det|ð´ 0 |) + T â t=1 log(1 + s 2 t ð 2 ð ) â¥ log(det| NÃN â ð´ w | + det| N â ð´ u |) + T â t=1 log(1 + s 2 t ð 2 ð ) = nlog(det|ð´ u |) + T â t=1 log(1 + s 2 t ð 2 ð ) Tr(ðº t ) â¤ Tr(ð´ 0 ) + T ð 2 ð = Tr( NÃN â ð´ w ) + Tr( N â ð´ u ) + T ð 2 ð = Tr( NÃN )Tr(ð´ w ) + Tr( N )Tr(ð´ u ) + T ð 2 ð = NTr(ð´ w ) + NTr(ð´ u ) + T ð 2 ð 1 3 Let, s 2 t â¤ -1 umin . For all y â [0, -1 umin ] log(1 + y 2 ) â¥ 1 -1 umin log(1 + -1<label>umin</label></formula><p xml:id="_jST5kuu"><s xml:id="_79yvYuF">2 )y (See argument in <ref type="bibr" target="#b54">Vaswani et al. (2017)</ref>).</s></p><formula xml:id="formula_34">1 dN Tr( t ) dN â¥ det| t | dNlog( 1 dN Tr( t )) â¥ log(det| t |) dNlog( 1 dN Tr( t )) â¥ log(det| t |) dNlog( 1 dN (Tr( 0 ) + T 2 )) â¥ log(det| t |) â¥ Nlog(det| u |) + T â t=1 log(1 + s 2 t 2 ) dNlog( 1 dN (Tr( 0 ) + T 2 )) â¥ Nlog(det| u |) + T â t=1 log(1 + s 2 t 2 ) dNlog( 1 dN (Tr( 0 ) + T 2 )) -Nlog(det| u |) â¥ T â t=1 log(1 + s 2 t 2 ) dNlog( 1 dN (Tr( 0 ) + T 2 )) + Nlog(det| -1 u )| â¥ T â t=1 log(1 + s 2 t 2 ) dNlog( 1 dN (Tr( 0 ) + T 2 )) + dNlog( 1 d Tr( -1 u )) â¥ T â t=1 log(1 + s 2 t 2 ) dN log( 1 dN (Tr( 0 ) + T 2 )) + log( 1 d Tr( -1 u )) â¥ T â t=1 log(1 + s 2 t 2 ) dN log(( Tr( 0 ) 2 + T 2 dN )) + log( 1 d Tr( -1 u )) â¥ T â t=1 log(1 + s 2 t 2 ) dN log( Tr( 0 ) 2 + T + NTr( -1 u ) 2 2 dN ) â¥ T â t=1 log(1 + s 2 t 2 ) dN log( (nTr( w ) + NTr( u )) 2 + T + NTr( -1 u ) 2 2 dN ) â¥ T â t=1 log(1 + s 2 t 2 ) dN log( (Tr( w ) + Tr( u ) + Tr( -1 u )) d + T 2 dN ) â¥ T â t=1 log(1 + s 2 t 2 ) log(1 + s 2 t 2 ) â¥ 1 -1 umin log(1 + -1 umin 2 )s 2 t ) 1 umin log(1 + 1 umin 2 ) log(1 + s 2 t 2 ) â¤ s 2 t T â t=1 s 2 t â¤ C T â t=1 log(1 + s 2 t 2 )</formula><p xml:id="_tax48xs"><s xml:id="_UrTBTg7">Where, C = umin log(1 + 1 umin 2 ) By Cauchy Schwartz</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TBjs2Dj">â»</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_EJ8ZSMJ">Appendix 3: Simulation</head><p xml:id="_mm6wMVF"><s xml:id="_ThrcPUB">We include additional information about the simulation environment.</s><s xml:id="_m5NbKWp">We first explain general information about the simulation environment.</s><s xml:id="_yE7P7pC">We then provide the procedures for generating state variables (features) in the simulation.</s><s xml:id="_Nzpu2Yq">Finally, we discuss how we used HeartstePsV1 to arrive at the feature representations used in the simulation.</s></p><p xml:id="_fzcQhrr"><s xml:id="_vnUMMdQ">Simulation dynamics Within the simulation states are updated every thirty minutes.</s><s xml:id="_gZFm62V">Each thirty minutes is associated with a date-time, thus we can acquire the month from the current time which is useful in updating the temperature.</s><s xml:id="_GXps4Hb">The decision times are set roughly two hours apart from 9:00 to 19:00.</s></p><p xml:id="_Yq9C2Je"><s xml:id="_h2k26Cm">Availability In the real-study users are not always available to receive treatment for a suite of reasons.</s><s xml:id="_tk6pGD6">For example, they may be driving a vehicle or they might have recently received treatment.</s><s xml:id="_TT9EfUX">Thus, at each decision time we update the context feature Available i â¼ Bernoulli(.8) .</s><s xml:id="_fYG5n5p">for the i th user where Available i is drawn from a Bernoulli.</s><s xml:id="_nydyzNK">This condition reduces the distance between the settings in the environment and those in a realworld study.</s><s xml:id="_hBPRzNe">At each decision time interventions are only sent to users who are available; i.e. user i cannot receive an intervention when Available i = 0.</s></p><p xml:id="_YrSkuBP"><s xml:id="_fauzwqs">Recruitment We follow the recruitment rate observed in HEARTSTEPSV1 .</s><s xml:id="_RWY2W9Y">For example, if 20% of the total number of participants were recruited in the third week of HEARTSTEPSV1 we recruit 20% of the total number of participants who will be recruited in the third week of the simulation.</s><s xml:id="_mXhfnuj">To explore the effect of running the study for varying lengths we scale the recruitment rates.</s><s xml:id="_sqngAst">For example, if the true study ran for 8 weeks, and we want to run a simulation for three weeks, we proportionally scale the recruitment in each of the three weeks so that the relative recruitment in each week remains the same.</s><s xml:id="_sa6PBhn">In these experiments we would like to recruit the entire population within 6 weeks.</s><s xml:id="_QNm8feE">Thus about 10% of participants are recruited each week, except for the second week of the study where about 30% of all participants are recruited.</s><s xml:id="_usBsfHN">This reflects the recruitment rates seen in the study, which were more of less consistent throughout besides one increase in the second week.</s></p><formula xml:id="formula_35">ï¿½ t=1 s t â¤ â T ï¿½ ï¿½ ï¿½ ï¿½ T ï¿½ t=1 s 2 t T ï¿½ t=1 s t â¤ â T ï¿½ ï¿½ ï¿½ ï¿½ C T ï¿½ t=1 log(1 + s 2 t 2 ) T ï¿½ t=1 s t â¤ â T ï¿½ CdN ï¿½ log ï¿½ (Tr( w ) + Tr( u ) + Tr( -1 u )) d + T 2 dN ï¿½ï¿½ T ï¿½ t=1 s t â¤ â dNT ï¿½ C ï¿½ log ï¿½ (Tr( w ) + Tr( u ) + Tr( -1 u )) d + T 2 dN ï¿½ï¿½ 1 3</formula><p xml:id="_82s4qDp"><s xml:id="_FANqbQS">We generate states from historical data.</s><s xml:id="_MGjHR9m">Given relevant context we search historical data for states which match this given context.</s><s xml:id="_qAJ5DxA">This subset of matching states can be used to generate new states.</s><s xml:id="_tdd5mYR">We discuss this in more detail in Sect.</s><s xml:id="_DKq7zDN">C.1.</s><s xml:id="_ZW3Jyt5">Then, we describe in more detail how we generate temperature, location and step counts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_34xQscM">Querying history</head><p xml:id="_jnMbKGS"><s xml:id="_3eRF8w8">Algorithm 2 is used to obtain relevant historical data in order to form a probability distribution over some target feature value.</s><s xml:id="_kZaf3yM">For example, if we would like a probability distribution over discretized temperature IDs under a given context, we would search over the historical data for all temperature IDs present under this context.</s><s xml:id="_4zEVXsT">This set of context-specific temperature IDs can then be used to form a distribution to simulate a new ID.</s><s xml:id="_CAnDrtN">This process of querying historical data is used throughout the simulation and is outlined in Algorithm 2. For example, it is used in generating new step counts, new locations and new temperatures.</s></p><p xml:id="_NxbFStF"><s xml:id="_bD722wX">As the simulation environment simulates draws stochastically from a variety of probability distributions, it is possible it draws a state which was not present in the historical dataset.</s><s xml:id="_275Rak2">In this case there is a process for finding a matching state.</s><s xml:id="_PsH5Ag9">Similarly we might have a state in the historical dataset with insufficient samples to form an informative (not overlynoisy) distribution.</s><s xml:id="_cMFHyKK">In this case we also find a surrogate state with which to generate future step counts.</s><s xml:id="_3DKWHPG">The idea of the process is to find the closest state to the current state, such that this close state has sufficient data to generate a good distribution.</s><s xml:id="_nhcCpx2">Again, given a state, we want to be able to generate a step count from a distribution with sufficient data to inform its parameters.</s><s xml:id="_wYt4eZR">The pseudocode for how we do so is shown in Algorithm 3</s></p><p xml:id="_hmCKrJJ"><s xml:id="_F6WbGFb">This algorithm takes as input a target state, s * .</s><s xml:id="_x8ETttJ">We also have a dictionary(hasmap) formed from the historical dataset.</s><s xml:id="_fkzkAJr">The keys to this dictionary are the states which existed in the dataset.</s><s xml:id="_As5QuyY">A value is an array of step counts for this state.</s><s xml:id="_SeHdhpj">This procedure gives the closest state with the most data points to our current state.</s><s xml:id="_5g2Cqbg">To be more explicit about lines 8-11.</s><s xml:id="_Nz9nASC">A state is a vector of some length, for example [1, 0, 1].</s><s xml:id="_dFH9nXs">When we consider all subsets of size 2, we are considering the subsets [1, 0],[1, 1], and [0, 1].</s><s xml:id="_dkzMSas">For each of these we can look in the historical data set and find all points where this state was true.</s><s xml:id="_vnwEuVD">Thus for each subset we'll get a new list of points,</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_axtkW27">Generating temperature</head><p xml:id="_FutCrnj"><s xml:id="_R9VwbMz">We mimic a trial where everyone resides in the same general area, such as a city.</s><s xml:id="_aRgzbFU">In this setting everyone experiences the same global temperature.</s><s xml:id="_bXjgvCt">We describe how to obtain temperature at any point in time in Algorithm 4. The temperature is updated exactly five times a day.</s></p><p xml:id="_mNhuTZK"><s xml:id="_AdH4cQJ">In the following algorithms t, refers to a timestamp, D refers to a historical dataset, K t refers to a set of temperature IDs, and w t-1 refers to the temperature at the previous time stamp.</s><s xml:id="_Fr8Rj2r">Here, D = HEARTSTEPSV1 and K t = {hot, cold} .</s><s xml:id="_VDhb94S">The contextual features which influence temperature are time of day, day of the week and the month tod, dow and month respectively.</s><s xml:id="_7hQjYrc">Furthermore, at all times besides the first moment in the trial, the next temperature depends on the current temperature w t-1 .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pPjkEBy">Generating location</head><p xml:id="_U8zSW3j"><s xml:id="_WdwdsmQ">In the following algorithms t, refers to a timestamp, g u refers to the group id of user i,D refers to a historical dataset, K t refers to a set of location IDs, and l t-1 refers to the location at the previous time stamp.</s><s xml:id="_uVBzdEr">Here, D = HEARTSTEPSV1 and K t = {other, home or work}.</s></p><p xml:id="_du77m5u"><s xml:id="_v6vWwGF">As in generating temperature, the contextual features which influence location are time of day, day of the week and the month tod, dow and month respectively.</s><s xml:id="_XBVVHTz">Generating location is different from generating temperature in that each user moves from location to location independently.</s><s xml:id="_fzbcQCU">Whereas we model users to share one common temperature, they move from one location to another independently of other users.</s><s xml:id="_XHuMQDB">Thus we also include group id in determining the next location for a given user.</s></p><p xml:id="_6xkBeAC"><s xml:id="_pXuzsjN">1. User is at a decision time (a) User is available (b) User is not available 2. User is not at a decision time</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CMDTyQy">Generating step-counts</head><p xml:id="_uryFPsQ"><s xml:id="_BEr6KJH">A new step-count is generated for each User active in the study, every thirty-minutes according to one of the following scenarios: Scenarios 1b and 2 are equivalent with respect to how step-counts are generated; a User's step count either depends on whether or not they received an intervention (when they are at a decision time and available) or it does not (because they were either not at a decision time or not available).</s><s xml:id="_pp7QDRs">Recall, that if a user is available the final step count is generated according to Eq. 25.</s><s xml:id="_nx5yUxU">This equation requires sufficient statistics from HeartstePsV1.</s><s xml:id="_DnZwEbP">The procedure for obtaining these statistics is shown explicitly in Algorithm 6.</s></p><p xml:id="_aY7Dh7Z"><s xml:id="_cRJtB7w">Here, t, g u , w t , l u , D refer to the current time in the trial, the group id of the i th user, the tem- perature at time t, the location of the i th user, and a historical dataset, respectively.</s><s xml:id="_WeThUwR">To find sufficient statistics of step counts, we also employ the time of day and day of the week, tod and dow respectively.</s><s xml:id="_fTHuDrC">Finally, yst(t, u) describes the previous step count as high or low.</s></p><p xml:id="_BmddgvC"><s xml:id="_NWfj9jb">for HeterogeneIty (25) R i,k = ( h(S i,k ) , 2 h(S i,k ) ) + A i,k (f (S i,k ) T i + Z i ).</s></p><p xml:id="_Akw6zfa"><s xml:id="_qCqcUsP">For example, consider determining a representation for time of day.</s><s xml:id="_WgfF9ZD">We choose a partition to be morning, afternoon, evening.</s><s xml:id="_PBs7x6K">For each thirty-minute step count, if it occurred in the morning we assign it to the morning cluster, if it occurred in the afternoon we assign it to the afternoon cluster, etc.</s><s xml:id="_aj24MWk">Now we have three clusters of step counts and we can compute the C score of this clustering.</s><s xml:id="_5aYKEBC">We repeat the process for different partitions of the day.</s></p><p xml:id="_9SNgvNE"><s xml:id="_2RJm7PQ">Time of day To discover the representation for time of day which best explained the observed step counts, we considered all sequential partitions from length 2-8.</s><s xml:id="_TMfGmTu">We found that early-day, late-day, and night best explained the data.</s></p><p xml:id="_U3SRcCR"><s xml:id="_ANZwB9a">Day of the week To discover the representation for day of the week which best explained the observed step counts, we considered two partitions: every day, or weekday/ weekend.</s><s xml:id="_TRpvGQb">We found weekday/weekend to be a better fit to the data.</s></p><p xml:id="_DRzwbzK"><s xml:id="_FMsGjhH">Temperature Here we choose different percentiles to partition the data.</s><s xml:id="_hpjKsak">We consider between 2 and 5 partitions <ref type="bibr">(percentiles at 50, to 20,40,60,80</ref>).</s><s xml:id="_ZhyHHUW">Here we found two partitions to best fit the step counts.</s><s xml:id="_sZ3wssT">We also tried more complicated representations of weather combined with temperature, however for the purpose of this paper we found a simple representation to best allow us to explore the relevant questions in this problem setting.</s></p><p xml:id="_b8NhXkh"><s xml:id="_yXvH3kg">Location In representing location we relied on domain knowledge.</s><s xml:id="_ZbgHyRJ">We found that participants tend to be more responsive when they are either at home or work, than in other places.</s><s xml:id="_B8qXXbR">Thus, we decided to represent location as belonging to one of two categories: home/ work or other.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 5</head><label>5</label><figDesc><div><p xml:id="_m7Es7Ns"><s xml:id="_bPdVfBP">Fig. 5 Contextual features for a simulated User are composed of both general environmental features (such as time of day) and individual features (such as location).</s><s xml:id="_rTk3HpZ">At decision times a simulated user receives a message determined by the current treatment policy.</s><s xml:id="_bWtR4YP">Periodically this policy is updated according to a learning algorithm which outputs a new posterior distribution for each User</s></p></div></figDesc><graphic coords="14,49.63,57.72,340.10,172.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 6</head><label>6</label><figDesc><div><p xml:id="_BmcJpq9"><s xml:id="_DtmafYT">Fig. 6 Heterogeneity generative model Regret averaged across all users for each week in the trial, i.e. average regret of all users in their first week of the trial</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7</head><label>7</label><figDesc><div><p xml:id="_RmqZwTa"><s xml:id="_XEtpkdh">Fig. 7 Setup of feasIBIlItystUdy.</s><s xml:id="_D4EWFzP">Users can receive treatments up to five times a day during the 90 days.</s><s xml:id="_smup5BB">Users enter the trial asynchronously</s></p></div></figDesc><graphic coords="19,49.57,57.72,340.22,172.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 11</head><label>11</label><figDesc><div><p xml:id="_phJtWCD"><s xml:id="_WrqT5Fs">Fig. 11Posterior mean of the coefficient of A i,k in Eq. 12 for users A and B in the feasibility study</s></p></div></figDesc><graphic coords="22,165.79,57.72,226.82,145.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 13</head><label>13</label><figDesc><div><p xml:id="_DSxWgwJ"><s xml:id="_R8BqPWz">Fig. 13 Disengagement generative model Regret averaged across all users for each week in the trial, i.e. average regret of all users in their first week of the trial</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>T</head></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc><div><p xml:id="_b3M6mNj"><s xml:id="_cEybVgJ">[1, 0] = [c 1 , â¦ , c N1 ] [1, 1] = [c 1 , â¦ , c N2 ] , [0, 1] = [c 1 , â¦ , c N3 ] .We now look at N1, N2, N3 and choose the state with the highest value.</s><s xml:id="_X2BpTHR">For example, if the lists were:[1, 0] = [c 1 , â¦ , c 100 ] [1, 1] = [c 1 , â¦ , c 2 ] , [0, 1] = [c 1 , â¦ , c 300 ], we would choose s = [0, 1] .</s><s xml:id="_R8Gr3u3">Now if we encounter the state [1, 0, 1] and there is insufficient data to form a distribution from this state, we will instead form it from the values found under the state [0, 1], [c 1 , â¦ , c 300 ].</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc><div><p xml:id="_xukjm7a"><s xml:id="_8YPxmHA">The value used in encoding each feature is shown in parentheses For example cold (0) indicates that cold is coded as a 0 wherever this feature is used.</s><s xml:id="_ptVRkwv">A user's state is described as S i,k = {1, time of day, day of the week, preceding activity level, location}</s></p></div></figDesc><table><row><cell>Name</cell><cell>Value</cell><cell>User Specific</cell></row><row><cell>Time of day</cell><cell>Morning 9:00 and 15:00 (0)</cell><cell>No</cell></row><row><cell></cell><cell>Afternoon 15:00 and 21:00 (1)</cell><cell></cell></row><row><cell>Day of the week</cell><cell>Weekday (0) or Weekend (1)</cell><cell>No</cell></row><row><cell>Temperature</cell><cell>Cold (0) or Hot (1)</cell><cell>No</cell></row><row><cell>Preceding activity level</cell><cell>Low (0) or High (1)</cell><cell>Yes</cell></row><row><cell>Location</cell><cell>Other (0) or Home/work (1)</cell><cell>Yes</cell></row><row><cell>Intercept</cell><cell>1</cell><cell>Yes</cell></row><row><cell>Reward</cell><cell></cell><cell></cell></row><row><cell>Step count</cell><cell>Continuous on log scale</cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc><div><p xml:id="_T6vWs8J"><s xml:id="_Fyg9qu5">Settings for Z in three cases of homogeneous, bimodal and smoothly varying populations</s></p></div></figDesc><table><row><cell>Homogeneous</cell><cell>Bi-modal</cell><cell>Smooth</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc><div><p xml:id="_BnVgpUp"><s xml:id="_NzCFX6f">State feature descriptions for feasIBIlItystUdy</s></p></div></figDesc><table><row><cell>State Features</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Name</cell><cell>Value</cell><cell>User specific</cell><cell>Included in</cell></row><row><cell></cell><cell></cell><cell></cell><cell>treatment</cell></row><row><cell></cell><cell></cell><cell></cell><cell>effect</cell></row><row><cell>Temperature</cell><cell>Continuous</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Yesterday's step count</cell><cell>Continuous</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Prior 30-minute step count</cell><cell>Continuous</cell><cell>Yes</cell><cell>No</cell></row><row><cell>Step variation level</cell><cell>Discrete</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Engagement with mobile application</cell><cell>Discrete</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Dosage</cell><cell>Continuous</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Location</cell><cell>Discrete</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Intercept</cell><cell>1</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>Reward</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Step count</cell><cell>Continuous on log scale</cell><cell>Yes</cell><cell>NA</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p xml:id="_NvW3P9T"><s xml:id="_HJKdh6s">More generally, one can consider the setting where users become known to an algorithm over time.</s><s xml:id="_zkvRerk">For example, users may open or delete accounts on an online shopping platform.</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xml:id="_2zJFYDm"><p xml:id="_KZgJYtT"><s xml:id="_9rcvhP9">this article are those of the authors and do not necessarily reflect the official position of the <rs type="institution">National Institutes of Health</rs>, or any other part of the <rs type="institution">U.S. Department of Health and Human Services</rs>.</s></p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zAfVbQ2">Appendix 1: Regret bound</head><p xml:id="_qhQdzRj"><s xml:id="_6uNMkdf">In this section we prove a high probability regret bound for a modification of IntellIgent-PoolIng in a simplified setting.</s><s xml:id="_uPejVUU">We modify the Thompson sampling algorithm in Intel-lIgentPoolIng by multiplying the posterior covariance by a tuning parameter, following <ref type="bibr" target="#b1">Agrawal and Goyal (2012)</ref>.</s><s xml:id="_Sg5x2mZ">This is mainly due to the technical reasons; see <ref type="bibr" target="#b0">Abeille and Lazaric (2017)</ref> for a discussion.</s><s xml:id="_UFMHaE5">We also simplify the setting in this regret analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JU7QjHe">Appendix 4: Feature construction</head><p xml:id="_PAtfHmr"><s xml:id="_WuAQxtS">We provide more details on the processes used for feature construction.</s><s xml:id="_syNz6w8">As stated in the paper we rely heavily on the dataset HeartstePsV1 to make all feature construction decisions.</s><s xml:id="_uTdV6Ce">The one exception is in the design of the location feature, for which we had domain knowledge to rely on (more detail below)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gZV798w">Baseline activity</head><p xml:id="_3B7X9Us"><s xml:id="_tZzf7bu">Each user is assigned to one of two groups: a low-activity group or a high-activity group.</s><s xml:id="_CTpfAGq">These groups are found from the historical data.</s><s xml:id="_qH7pQ2V">We perform hierarchical clustering using the method hcluster in scikit-learn <ref type="bibr" target="#b3">Pedregosa (2011)</ref>.</s><s xml:id="_P8jfueH">We used a euclidean distance metric to cluster the data and found that two groups naturally arose.</s><s xml:id="_HNySfS2">These groups were consistent with the population of HeartstePsV1, which consisted of participants who were generally either office administrators or students.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8ybZj7f">State features</head><p xml:id="_3puaReV"><s xml:id="_nCNbagR">We now briefly outline the decisions for the remaining features: time of day, day of the week, and temperature.</s><s xml:id="_kEjSdKa">For each feature we explored various categorical representations.</s><s xml:id="_FSXFXPU">For each, the question was how many categories to use to represent the data.</s><s xml:id="_6P2N3uA">For each feature we followed the same procedure.</s></p><p xml:id="_SQPbwPr"><s xml:id="_ncztbrz">1. We chose a number of categories (k) to threshold the data into 2. We partitioned the data into k categories 3. We clustered the step counts according to these k categories 4. We computed the Calinski-Harabasz score of this clustering 5. We chose the final k to be that which provided the highest score For example, consider the task of representing temperature.</s><s xml:id="_VYX2Yyw">Let l be a temperature, x be a step count and x l b be a thirty-minute step count occurring when the temperature l was assigned to bucket b.</s><s xml:id="_2rHf3Xx">Given a historical dataset, we have a vector where each entry x i,t refers to the thirty-minute step count of user i at time t.</s></p><p xml:id="_5egDWEs"><s xml:id="_K8SdXSH">â¢ Let p be a number of buckets.</s><s xml:id="_KXvm44a">We create p buckets by finding quantiles of l.</s><s xml:id="_QmDFeAK">For example, if p=2, we find the 50 th quantile of l.</s><s xml:id="_gqm2kuq">A bucket is defined by a tuple of thresholds (th 1 , th 2 ) , such that for a data point d to belong to bucket i, d must be in the range of the tuple ( th 1 â¤ d &lt; th 2 ).</s><s xml:id="_k7zzryz">â¢ For each temperature, we determine the bucket label which best describes this temperature.</s><s xml:id="_EMu6AWQ">That is the label y of l, is the bucket for which th y 1 â¤ sl &lt; th y 2 .</s><s xml:id="_7F55qM3">â¢ We now create a vector of labels y, of the same length as .</s><s xml:id="_QYdreMh">Each y l i,t is the bucket assigned to l i,t .</s><s xml:id="_C7qbKms">For example, if the temperature for user i at time t falls into the lowest bucket, 0 would be the label assigned to l i,t .</s><s xml:id="_nyHPjzS">This induces a clustering of step-counts where the label is a temperature bucket.</s></p><p xml:id="_cBBNKXr"><s xml:id="_H74S6Ce">â¢ We determine the Calabrinski-Harabasz score of this clustering.</s></p><p xml:id="_PVT5T2M"><s xml:id="_ceUjWD9">We test this procedure from p equal to 1, through 4.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9h9EwKv">Appendix 5: Feasibility study</head><p xml:id="_SZMfv4E"><s xml:id="_n3FauCh">In the clinical trial we describe users' states with the features described in Table <ref type="table">4</ref>.</s><s xml:id="_45M2tV6">The two features which differ from the simulation environment are engagement and exposure to treatment.</s><s xml:id="_GDGj6a7">We clarify these features below (Table <ref type="table">6</ref>).</s></p><p xml:id="_XCqdeSR"><s xml:id="_uXpPfjv">Engagement The engagement variable measures the extent to which a user engages with the mHealth application deployed in the trial.</s><s xml:id="_b8BzfQx">There are several screens within the application that a user can view.</s><s xml:id="_UX7sGVr">Across all users we measure the 40 th percentile of number of screens viewed on day d.</s><s xml:id="_kHA2yQY">If user i views more than this percentile, we set their engagement level to 1, otherwise it is 0.</s></p><p xml:id="_PxcC6yH"><s xml:id="_nxHGs87">Exposure to treatment This variable captures the extent to which a user is treated, or the treatment dosage experienced by this user.</s><s xml:id="_Sc9fnh2">Let D i denote the exposure to treatment for user i. Whenever a message is delivered to a user's phone D i i s updated.</s><s xml:id="_NDapqT7">That is, if a mes- sage is delivered between time t and t + 1 , D t+1 = D t + 1 .</s><s xml:id="_Wf2yn5k">If a message is not delivered, D t+1 = D t .</s><s xml:id="_XKEFTHf">Here, we se according to data from HeartstePsV1 and initialize D to 0. Acknowledgements This material is based upon work supported by: NIH/NIAAA R01AA23187 ,NIH/ NIDA P50DA039838,NIH/NIBIB U54EB020404 and NIH/NCI U01CA229437.</s><s xml:id="_6ZVZNgb">The views expressed in</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_fTBNABy">Linear thompson sampling revisited</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abeille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<idno type="DOI">10.1214/17-ejs1341si</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hwxeUWj">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="5165" to="5197" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abeille, M., Lazaric, A., et al. (2017). Linear thompson sampling revisited. Electronic Journal of Statistics, 11(2), 5165-5197.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_Wat99Qh">Analysis of thompson sampling for the multi-armed bandit problem</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wBbfnuk">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="39" to="40" />
		</imprint>
	</monogr>
	<note type="raw_reference">Agrawal, S., &amp; Goyal, N. (2012). Analysis of thompson sampling for the multi-armed bandit problem. In: Conference on Learning Theory, pp 39-1.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_utramxU">Thompson sampling for contextual bandits with linear payoffs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AjPeVHY">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
	<note type="raw_reference">Agrawal, S., &amp; Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs. In: Inter- national Conference on Machine Learning, pp 127-135.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_cDDDh2e">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_T9afhnT">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825-2830.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_8nt4j4k">Time-varying Gaussian process bandit optimization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bogunovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AnXNHex">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
	<note type="raw_reference">Bogunovic, I., Scarlett, J., &amp; Cevher, V. (2016). Time-varying Gaussian process bandit optimization. In: Artificial Intelligence and Statistics, pp 314-323.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_TuQcafV">Multi-task Gaussian process prediction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_dNmJHVn">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
	<note type="raw_reference">Bonilla, E.V., Chai, K.M., &amp; Williams, C. (2008). Multi-task Gaussian process prediction. In: Advances in neural information processing systems, pp 153-160.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_BfbwB6g">Assessing time-varying causal effect moderation in mobile health</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boruvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2017.1305274</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7SjVSTR">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">523</biblScope>
			<biblScope unit="page" from="1112" to="1121" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Boruvka, A., Almirall, D., Witkiewitz, K., &amp; Murphy, S. A. (2018). Assessing time-varying causal effect moderation in mobile health. Journal of the American Statistical Association, 113(523), 1112-1121.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_Av8EQXC">Portfolio allocation for Bayesian optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>arXiv: 10095 419</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Brochu, E., Hoffman, M.W., &amp; de Freitas, N. (2010). Portfolio allocation for Bayesian optimization. arXiv preprint arXiv: 10095 419.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_wUHCg6Q">Bayes and empirical Bayes methods for data analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Louis</surname></persName>
		</author>
		<idno type="DOI">10.1201/9781420057669</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Carlin, B.P., &amp; Louis, T.A. (2010). Bayes and empirical Bayes methods for data analysis. Chapman and Hall/CRC .</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_BAy68je">An introduction to empirical Bayes data analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<idno type="DOI">10.1080/00031305.1985.10479400</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yXWDJmJ">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="87" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Casella, G. (1985). An introduction to empirical Bayes data analysis. The American Statistician, 39(2), 83-87.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_UV2MTCw">A gang of bandits</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zappella</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/1120.003.0051</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4SfsThM">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="737" to="745" />
		</imprint>
	</monogr>
	<note type="raw_reference">Cesa-Bianchi, N., Gentile, C., &amp; Zappella, G. (2013). A gang of bandits. In: Advances in Neural Informa- tion Processing Systems, pp 737-745.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simchi-Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3261050</idno>
		<idno>arXiv: 18100 3024</idno>
		<title level="m" xml:id="_zDcukuA">Learning to optimize under non-stationarity</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Cheung, W.C., Simchi-Levi, D., &amp; Zhu, R. (2018). Learning to optimize under non-stationarity. arXiv pre- print arXiv: 18100 3024.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_RZ8spPq">On kernelized multi-armed bandits</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NGdp5J2">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="844" to="853" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chowdhury, S. R., &amp; Gopalan, A. (2017). On kernelized multi-armed bandits. International Conference on Machine Learning, 70, 844-853.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_S8Zrqzk">mstress: A mobile recommender system for just-in-time interventions for stress</title>
		<author>
			<persName><forename type="first">S</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Labrador</surname></persName>
		</author>
		<idno type="DOI">10.1109/ccnc.2017.8015367</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_JXtTKDt">Consumer Communications &amp; Networking Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="raw_reference">Clarke, S., Jaimes, L.G., &amp; Labrador, M.A. (2017). mstress: A mobile recommender system for just-in-time interventions for stress. In: Consumer Communications &amp; Networking Conference, pp 1-5.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_4qXAfMP">Activity sensing in the wild: a field trial of ubifit garden</title>
		<author>
			<persName><forename type="first">S</forename><surname>Consolvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toscos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Froehlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamarca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Legrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Libby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_HHaXgVD">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1797" to="1806" />
		</imprint>
	</monogr>
	<note type="raw_reference">Consolvo, S., McDonald, D.W., Toscos, T., Chen, M.Y., Froehlich, J., Harrison, B., Klasnja, P., LaMarca, A., LeGrand, L., &amp; Libby, R., et al. (2008). Activity sensing in the wild: a field trial of ubifit garden. In: Proceedings of the SIGCHI conference on human factors in computing systems, pp 1797-1806.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_QDsn5DK">Parallelizing exploration-exploitation tradeoffs in Gaussian process bandit optimization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Desautels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SDmQkDN">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3873" to="3923" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Desautels, T., Krause, A., &amp; Burdick, J. W. (2014). Parallelizing exploration-exploitation tradeoffs in Gauss- ian process bandit optimization. The Journal of Machine Learning Research, 15(1), 3873-3923.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_3uvXDTD">Multi-task learning for contextual bandits</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8DT88z2">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4848" to="4856" />
		</imprint>
	</monogr>
	<note type="raw_reference">Deshmukh, A.A., Dogan, U., &amp; Scott, C. (2017). Multi-task learning for contextual bandits. In: Advances in Neural Information Processing Systems, pp 4848-4856.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_pbsrFr3">High-dimensional gaussian process bandits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bGHfvVJ">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1025" to="1033" />
		</imprint>
	</monogr>
	<note type="raw_reference">Djolonga, J., Krause, A., &amp; Cevher, V. (2013). High-dimensional gaussian process bandits. In: Advances in Neural Information Processing Systems, pp 1025-1033.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_kVznxkY">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SxSsTvp">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9516" to="9527" />
		</imprint>
	</monogr>
	<note type="raw_reference">Finn, C., Xu, K., &amp; Levine, S. (2018). Probabilistic model-agnostic meta-learning. In: Advances in Neural Information Processing Systems, pp 9516-9527.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_wE3fN22">Online meta-learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>arXiv: 19020 8438</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Finn, C., Rajeswaran, A., Kakade, S., &amp; Levine, S. (2019). Online meta-learning. arXiv preprint arXiv: 19020 8438.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_pBAXHsz">Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Forman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kerrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Butryn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Juarascio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>OntaÃ±Ã³n</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10865-018-9964-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TbW3Rh8">Journal of behavioral medicine</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="276" to="290" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Forman, E. M., Kerrigan, S. G., Butryn, M. L., Juarascio, A. S., Manasse, S. M., OntaÃ±Ã³n, S., et al. (2018). Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss? Journal of behavioral medicine, 42(2), 276-290.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_TcQ6Zjb">Gpytorch: Blackbox matrixmatrix gaussian process inference with gpu acceleration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GckS3Vs">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7576" to="7586" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gardner, J., Pleiss, G., Weinberger, K.Q., Bindel, D., &amp; Wilson, A.G. (2018). Gpytorch: Blackbox matrix- matrix gaussian process inference with gpu acceleration. In: Advances in Neural Information Process- ing Systems, pp 7576-7586.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_3ZT9AUz">Action centered contextual bandits</title>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4nwZWEX">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5977" to="5985" />
		</imprint>
	</monogr>
	<note type="raw_reference">Greenewald, K., Tewari, A., Murphy, S., &amp; Klasnja, P. (2017). Action centered contextual bandits. In: Advances in neural information processing systems, pp 5977-5985.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_DvZNMAD">Meta-reinforcement learning of structured exploration strategies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_a6K6rqS">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5302" to="5311" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gupta, A., Mendonca, R., Liu, Y., Abbeel, P., &amp; Levine, S. (2018). Meta-reinforcement learning of struc- tured exploration strategies. In: Advances in Neural Information Processing Systems, pp 5302-5311.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_sgd3bFd">Impact of mhealth chronic disease management on treatment adherence and patient outcomes: a systematic review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hamine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gerth-Guyette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Faulx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="DOI">10.2196/jmir.3951</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SaHPykj">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hamine, S., Gerth-Guyette, E., Faulx, D., Green, B. B., &amp; Ginsburg, A. S. (2015). Impact of mhealth chronic disease management on treatment adherence and patient outcomes: a systematic review. Jour- nal of medical Internet research, 17(2), e52.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_5kJgDWY">Preventer, a selection mechanism for just-in-time preventive interventions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Jaimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Llofriu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raij</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2015.2490062</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2TheFAa">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="257" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jaimes, L. G., Llofriu, M., &amp; Raij, A. (2016). Preventer, a selection mechanism for just-in-time preventive interventions. IEEE Transactions on Affective Computing, 7(3), 243-257.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_ECXrQDK">Near-optimal oracle-efficient algorithms for stationary and non-stationary stochastic linear bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108571401.039</idno>
		<idno>arXiv: 19120 5695</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Kim, B., Tewari, A. (2019). Near-optimal oracle-efficient algorithms for stationary and non-stationary sto- chastic linear bandits. arXiv preprint arXiv: 19120 5695.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_8aXz6bC">Randomized exploration for non-stationary stochastic linear bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_r9yrwca">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kim, B., &amp; Tewari, A. (2020). Randomized exploration for non-stationary stochastic linear bandits. In: Con- ference on Uncertainty in Artificial Intelligence, pp 71-80.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_p6326G4">Microrandomized trials: An experimental design for developing just-in-time adaptive interventions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Hekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boruvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VzyXvf6">Health Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">S</biblScope>
			<biblScope unit="page">1220</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Klasnja, P., Hekler, E.B., Shiffman, S., Boruvka, A., Almirall, D., Tewari, A., &amp; Murphy, S.A. (2015). Microrandomized trials: An experimental design for developing just-in-time adaptive interventions. Health Psychology 34(S):1220.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_FWKCMSA">Efficacy of contextually tailored suggestions for physical activity: A micro-randomized optimization trial of heartsteps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Seewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AvMXXe5">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="573" to="582" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Klasnja, P., Smith, S., Seewald, N. J., Lee, A., Hall, K., Luers, B., et al. (2018). Efficacy of contextually tai- lored suggestions for physical activity: A micro-randomized optimization trial of heartsteps. Annals of Behavioral Medicine, 53(6), 573-582.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_9w6hCgN">Contextual gaussian process bandit optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rQf9yYT">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2447" to="2455" />
		</imprint>
	</monogr>
	<note type="raw_reference">Krause, A., &amp; Ong, C.S. (2011). Contextual gaussian process bandit optimization. In: Advances in Neural Information Processing Systems, pp 2447-2455.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_dzdVAvT">Random-effects models for longitudinal data</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3V7EEJC">Biometrics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="963" to="974" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Laird, N. M., Ware, J. H., et al. (1982). Random-effects models for longitudinal data. Biometrics, 38(4), 963-974.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_vYE7TXk">Learning to learn with the informative vector machine</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_c7PM3sb">International conference on Machine learning</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">65</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Lawrence, N.D., &amp; Platt, J.C. (2004). Learning to learn with the informative vector machine. In: Interna- tional conference on Machine learning, p 65.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_w5rrHRk">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772758</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xsyJ3aG">Proceedings of the Conference on World wide web</title>
		<meeting>the Conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">Li, L., Chu, W., Langford, J., &amp; Schapire, R.E. (2010). A contextual-bandit approach to personalized news article recommendation. In: Proceedings of the Conference on World wide web, pp 661-670.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<idno>arXiv: 15100 3164</idno>
		<title level="m" xml:id="_rWtk6SD">Context-aware bandits</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Li, S., &amp; Kar, P. (2015). Context-aware bandits. arXiv preprint arXiv: 15100 3164.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_J8ctF9n">Sample size calculations for micro-randomized trials in mhealth</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zxgV68F">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1944" to="1971" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liao, P., Klasnja, P., Tewari, A., &amp; Murphy, S. A. (2016). Sample size calculations for micro-randomized trials in mhealth. Statistics in medicine, 35(12), 1944-1971.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_zEgHc4w">Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3381007</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YjeuM3J">Proceedings of the Conference on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<meeting>the Conference on Interactive, Mobile, Wearable and Ubiquitous Technologies</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
	<note type="raw_reference">Liao, P., Greenewald, K., Klasnja, P., &amp; Murphy, S. (2020). Personalized heartsteps: A reinforcement learn- ing algorithm for optimizing physical activity. Proceedings of the Conference on Interactive, Mobile, Wearable and Ubiquitous Technologies, 4(1), 1-22.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_yYhC6um">Mixed-effects Gaussian process modeling approach with application in injection molding processes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jprocont.2017.12.003</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2C9Z9F5">Journal of Process Control</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="37" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luo, L., Yao, Y., Gao, F., &amp; Zhao, C. (2018). Mixed-effects Gaussian process modeling approach with application in injection molding processes. Journal of Process Control, 62, 37-43.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_SKR7tST">Parametric empirical Bayes inference: theory and applications</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Morris</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1983.10477920</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YwrqBQk">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">381</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Morris, C. N. (1983). Parametric empirical Bayes inference: theory and applications. Journal of the Ameri- can statistical Association, 78(381), 47-55.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main" xml:id="_aNWF7X4">Deep online learning via meta-learning: Continual adaptation for model-based rl</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagabandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>arXiv: 18120 7671</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Nagabandi, A., Finn, C., &amp; Levine, S. (2018). Deep online learning via meta-learning: Continual adaptation for model-based rl. arXiv preprint arXiv: 18120 7671.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_NugDfe6">Just-in-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_835S36E">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nahum-Shani, I., Smith, S. N., Spring, B. J., Collins, L. M., Witkiewitz, K., Tewari, A., &amp; Murphy, S. A. (2017). Just-in-time adaptive interventions (JITAIs) in mobile health: key components and design prin- ciples for ongoing health behavior support. Annals of Behavioral Medicine, 52(6).</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_zeKaYX2">Poptherapy: Coping with stress through pop-culture</title>
		<author>
			<persName><forename type="first">P</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roseway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sdFxXAA">Conference on Pervasive Computing Technologies for Healthcare</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
	<note type="raw_reference">Paredes, P., Gilad-Bachrach, R., Czerwinski, M., Roseway, A., Rowan, K., &amp; Hernandez, J. (2014). Pop- therapy: Coping with stress through pop-culture. In: Conference on Pervasive Computing Technologies for Healthcare, pp 109-117.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_b343Ga2">Bandit learning with implicit feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_esme9t9">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7276" to="7286" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qi, Y., Wu, Q., Wang, H., Tang, J., &amp; Sun, M. (2018). Bandit learning with implicit feedback. Advances in Neural Information Processing Systems, 31, 7276-7286.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main" xml:id="_MfN9h5e">Linear mixed models under endogeneity: modeling sequential treatment effects with application to a mobile health study</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno>arXiv: 19021 0861</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Qian, T., Klasnja, P., &amp; Murphy, S.A. (2019). Linear mixed models under endogeneity: modeling sequential treatment effects with application to a mobile health study. arXiv preprint arXiv: 19021 0861.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_MNX7FqX">Mybehavior: automatic personalized health feedback from user behaviors and preferences using smartphones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_huXQwWS">Proceedings of the Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="707" to="718" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi, M., Aung, M.H., Zhang, M., &amp; Choudhury, T. (2015). Mybehavior: automatic personalized health feedback from user behaviors and preferences using smartphones. In: Proceedings of the Conference on Pervasive and Ubiquitous Computing, pp 707-718.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_Dp6VbWd">SARA: a mobile app to engage users in health data collection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Philyaw-Kotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bonar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123024.3125611</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QwfqAsc">Joint Conference on Pervasive and Ubiquitous Computing and the International Symposium on Wearable Computers</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi, M., Philyaw-Kotov, M., Lee, J., Mansour, A., Dent, L., Wang, X., Cunningham, R., Bonar, E., Nahum-Shani, I., &amp; Klasnja, P., et al. (2017). SARA: a mobile app to engage users in health data col- lection. In: Joint Conference on Pervasive and Ubiquitous Computing and the International Sympo- sium on Wearable Computers, pp 781-789.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main" xml:id="_z33vuuP">Hierarchical linear models: Applications and data analysis methods</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Raudenbush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bryk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Raudenbush, S.W., &amp; Bryk, A.S. (2002). Hierarchical linear models: Applications and data analysis meth- ods, vol 1.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_HxN4BQv">Weighted linear bandits for non-stationary environments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Russac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vernade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>CappÃ©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zxTmabk">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12017" to="12026" />
		</imprint>
	</monogr>
	<note type="raw_reference">Russac, Y., Vernade, C., &amp; CappÃ©, O. (2019). Weighted linear bandits for non-stationary environments. In: Advances in Neural Information Processing Systems, pp 12017-12026.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_8GwscWk">Learning to optimize via posterior sampling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WdTwNT6">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1221" to="1243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Russo, D., &amp; Van Roy, B. (2014). Learning to optimize via posterior sampling. Mathematics of Operations Research, 39(4), 1221-1243.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_dUR8pz4">A tutorial on thompson sampling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazerouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000070</idno>
		<ptr target="https://doi.org/10.1561/2200000070" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_EcquUFe">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="96" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Russo, D.J., Roy, B.V., Kazerouni, A., Osband, I., &amp; Wen, Z. (2018). A tutorial on thompson sampling. Foundations and Trends in Machine Learning 11(1):1-96, https:// doi. org/ 10. 1561/ 22000 00070.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main" xml:id="_3b9W3Gd">Meta reinforcement learning with latent variable gaussian processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saemundsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno>arXiv: 18030 7551</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Saemundsson, S., Hofmann, K., &amp; Deisenroth, M.P. (2018). Meta reinforcement learning with latent vari- able gaussian processes. arXiv preprint arXiv: 18030 7551.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_43uDHqa">Mixed-effects Gaussian process functional regression models with application to dose-response curve prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Will</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zyaS2QZ">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="3165" to="3177" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shi, J., Wang, B., Will, E., &amp; West, R. (2012). Mixed-effects Gaussian process functional regression models with application to dose-response curve prediction. Statistics in medicine, 31(26), 3165-3177.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_SaVKDgF">Gaussian process optimization in the bandit setting: No regret and experimental design</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PyKJkTK">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
	<note type="raw_reference">Srinivas, N., Krause, A., Kakade, S.M., &amp; Seeger, M. (2009). Gaussian process optimization in the ban- dit setting: No regret and experimental design. International Conference on Machine Learning p 1015-1022.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_yhZNSfV">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EdQTNfk">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4), 285-294.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_3QDGBNB">Horde of bandits using Gaussian Markov random fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lakshmanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ktSdQcA">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="690" to="699" />
		</imprint>
	</monogr>
	<note type="raw_reference">Vaswani, S., Schmidt, M., &amp; Lakshmanan, L. (2017). Horde of bandits using Gaussian Markov random fields. In: Artificial Intelligence and Statistics, pp 690-699.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main" xml:id="_XftQxdr">Nonparametric Bayesian mixed-effect model: A sparse Gaussian process approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khardon</surname></persName>
		</author>
		<idno>arXiv: 12116 653</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Wang, Y., &amp; Khardon, R. (2012). Nonparametric Bayesian mixed-effect model: A sparse Gaussian process approach. arXiv preprint arXiv: 12116 653.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_FYxkRYD">Optimization as estimation with Gaussian processes in bandit settings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9QQrjHP">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1022" to="1031" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, Z., Zhou, B., &amp; Jegelka, S. (2016). Optimization as estimation with Gaussian processes in bandit set- tings. In: Artificial Intelligence and Statistics, pp 1022-1031.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<title level="m" xml:id="_aZd8gyv">Gaussian processes for machine learning</title>
		<meeting><address><addrLine>MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Williams, C. K., &amp; Rasmussen, C. E. (2006). Gaussian processes for machine learning (Vol. 2). MA: MIT press Cambridge.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main" xml:id="_uCr9Ea8">The price of personalization: An application of contextual bandits to mobile health</title>
		<author>
			<persName><forename type="first">I</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Senior thesis</note>
	<note type="raw_reference">Xia, I. (2018). The price of personalization: An application of contextual bandits to mobile health. Senior thesis.</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_E8a8fKb">Encouraging physical activity in patients with diabetes: intervention using a reinforcement learning system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozdoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XenpHbK">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">338</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yom-Tov, E., Feraru, G., Kozdoba, M., Mannor, S., Tennenholtz, M., &amp; Hochberg, I. (2017). Encouraging physical activity in patients with diabetes: intervention using a reinforcement learning system. Journal of medical Internet research, 19(10), e338.</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_R9ppeej">A simple approach for non-stationary linear bandits</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sCFypQE">Proceedings of the Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="746" to="755" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhao, P., Zhang, L., Jiang, Y., &amp; Zhou, Z.H. (2020). A simple approach for non-stationary linear bandits. In: Proceedings of the Conference on Artificial Intelligence and Statistics, pp 746-755.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_swatQ7t">Personalizing mobile fitness apps using reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Flowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Castillejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s" xml:id="_BKGzmgg">CEUR workshop proceedings</title>
		<imprint>
			<date type="published" when="2018">2018. 2068</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou, M., Mintz, Y., Fukuoka, Y., Goldberg, K., Flowers, E., Kaminsky, P., Castillejo, A., &amp; Aswani, A. (2018). Personalizing mobile fitness apps using reinforcement learning. In: CEUR workshop proceed- ings, vol 2068.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_JW3J55F">CAML: Fast context adaptation via meta-learning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_v946tkv">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7693" to="7702" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zintgraf, L.M., Shiarlis, K., Kurin, V., Hofmann, K., &amp; Whiteson, S. (2019). CAML: Fast context adapta- tion via meta-learning. In: International Conference on Machine Learning, pp 7693-7702.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
