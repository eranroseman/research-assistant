<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_jFz9tNp">Storyfier: Exploring Vocabulary Learning Support with Text Generation Models</title>
				<funder ref="#_cKEzwhg">
					<orgName type="full">Young Scientists Fund of the National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_hGZ5cwh">
					<orgName type="full">Research Grants Council of the Hong Kong Special Administrative Region under General Research Fund</orgName>
					<orgName type="abbreviated">GRF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">In The 36th Annual ACM Symposium on User Interface * Both authors contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM</p>
				</availability>
				<date type="published" when="2023-08-07">7 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenhui</forename><surname>Peng</surname></persName>
							<idno type="ORCID">0000-0002-5700-3136</idno>
						</author>
						<author>
							<persName><forename type="first">Xingbo</forename><surname>Wang</surname></persName>
							<idno type="ORCID">0000-0001-5693-1128</idno>
						</author>
						<author>
							<persName><forename type="first">Qiushi</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junkai</forename><surname>Zhu</surname></persName>
							<email>zhujunkai@hotmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
							<email>huamin@cse.ust.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">Sun Yat-sen University Zhuhai , China</note>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Zhuhai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">The Hong Kong University of Science and Technology Hong Kong , China</note>
								<orgName type="institution">The Hong Kong University of Science and Technology Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">Sun Yat-sen University Zhuhai , China</note>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Zhuhai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">Guangdong Polytechnic of Industry &amp; Commerce Guangzhou , China</note>
								<orgName type="institution">Guangdong Polytechnic of Industry &amp; Commerce</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<note type="raw_affiliation">The Hong Kong University of Science and Technology Hong Kong , China</note>
								<orgName type="institution">The Hong Kong University of Science and Technology Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<note type="raw_affiliation">The Hong Kong University of Science and Technology Hong Kong , China</note>
								<orgName type="institution">The Hong Kong University of Science and Technology Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_auCHs9a">Storyfier: Exploring Vocabulary Learning Support with Text Generation Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-07">7 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">479D6885C21257F0354D464BA6A5803A</idno>
					<idno type="DOI">10.1145/3586183.3606786</idno>
					<idno type="arXiv">arXiv:2308.03864v1[cs.HC]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T07:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_BmSQHr2">Human-centered computing → User interface design</term>
					<term xml:id="_x48xWY6">• Computing methodologies → Natural language generation</term>
					<term xml:id="_UMqFUPm">• Applied computing → Computer-assisted instruction vocabulary learning, story generation, language models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_P7vfXDt"><p xml:id="_jGdsazp"><s xml:id="_NjdwPYk">Vocabulary learning support tools have widely exploited existing materials, e.g., stories or video clips, as contexts to help users memorize each target word.</s><s xml:id="_VxvaeAZ">However, these tools could not provide a coherent context for any target words of learners' interests, and they seldom help practice word usage.</s><s xml:id="_YDumJEV">In this paper, we work with teachers and students to iteratively develop Storyfier, which leverages text generation models to enable learners to read a generated story that covers any target words, conduct a story cloze test, and use these words to write a new story with adaptive AI assistance.</s><s xml:id="_KFHy5tM">Our within-subjects study (N=28) shows that learners generally favor the generated stories for connecting target words and writing assistance for easing their learning workload.</s><s xml:id="_CnU6a6x">However, in the read-cloze-write learning sessions, participants using Storyfier perform worse in recalling and using target words than learning with a baseline tool without our AI features.</s><s xml:id="_tEf4mQ5">We discuss insights into supporting learning tasks with generative models.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_8CXVAsp">INTRODUCTION</head><p xml:id="_CUq7AbH"><s xml:id="_ednFMXM">Learning vocabulary in meaningful contexts, such as stories and images in language learning textbooks, and video clips from movies, is a common and effective practice as it enables deep and active processing of vocabulary (e.g., word associations, logic) <ref type="bibr" target="#b53">[54]</ref>.</s><s xml:id="_WgYQnVK">Many existing vocabulary learning systems like VocabEncounter <ref type="bibr" target="#b0">[1]</ref> and Smart Subtitles <ref type="bibr" target="#b35">[36]</ref> have exploited a variety of materials to establish the contexts for words.</s><s xml:id="_Ywu969y">These systems have demonstrated that the provided contexts can enhance vocabulary memorization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36]</ref>.</s></p><p xml:id="_q7gR7Aw"><s xml:id="_DRrB29U">However, these systems may fall short in two aspects.</s><s xml:id="_Rjgq5ge">First, they largely leverage existing materials and could not provide a meaningful context for any set of target words that users wish to learn.</s><s xml:id="_ne3zyub">In other words, previous systems lack the flexibility to offer a story, an article, or a video clip that covers the target words that teachers or learners specify.</s><s xml:id="_43dhqw8">These coherent contexts that connect target words may make a difference to vocabulary learners.</s><s xml:id="_7UYx5zs">As suggested by Gu et al. <ref type="bibr" target="#b24">[25]</ref>, learning vocabulary in batches under coherent contexts could facilitate recalls of a larger amount of words compared to learning vocabulary in isolation.</s><s xml:id="_3n6HcqM">Second, previous systems primarily focus on helping users to understand and memorize the meanings of target words via meaning-focused input learning activities, e.g., reading and listening, that use language receptively <ref type="bibr" target="#b47">[48]</ref>.</s><s xml:id="_J6aWjVZ">Few systems facilitate learners to master the usage of learned words via productive and fluency development tasks (e.g., writing and speaking) -typical activities that could help master the meanings and usage of target words in traditional courses <ref type="bibr" target="#b47">[48]</ref>.</s><s xml:id="_HhfVS4H">In offline courses, teachers can provide in-situ adaptive support like hints on word usage during these learning activities; however, this is often unavailable to individual learners outside classrooms.</s></p><p xml:id="_g3FphWu"><s xml:id="_st8tfR2">In this work, we utilize stories generated by large language models (LLMs) as meaningful contexts that cover any target words and provide adaptive assistance in word usage practice.</s><s xml:id="_ZXSNJqA">Our focus is motivated, on one hand, by the prevalent use of stories in language learning textbooks, and the proven efficacy of story-based learning in various scenarios such as programming <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b72">73]</ref>, parent-child storytelling <ref type="bibr" target="#b91">[92]</ref>, and children's visual storytelling <ref type="bibr" target="#b90">[91]</ref>.</s><s xml:id="_667c2td">On the other hand, LLMs can generate fluent and relevant texts given user specifications such as keywords, which have been used to support the writings of emails <ref type="bibr" target="#b23">[24]</ref>, articles or fiction <ref type="bibr" target="#b9">[10]</ref>, and poems <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b77">78]</ref>.</s><s xml:id="_K8GHpUH">However, little work, if any, has explored LLMs for story-based vocabulary learning where users should spend effort in mastering target words' meanings and usage.</s><s xml:id="_CsmRTb7">Questions arise such as 1) whether and how LLMs can generate the meaningful context of any target word set for vocabulary learning, 2) if so, what vocabulary learning activities can these generative models support, and 3) how would the support from generative models impact the users' vocabulary learning outcome and experience.</s></p><p xml:id="_dBrGAVR"><s xml:id="_tQVx3aB">To this end, we seek to provide insights into these questions by designing, developing, and evaluating an AI-generated story-based vocabulary learning system, Storyfier, that can provide meaningful story contexts and adaptive assistance for learning any set of target words.</s><s xml:id="_FzautRH">Here, we choose English as the target language to learn and target ESL (English-as-the-Second-Language) Chinese learners, e.g., high-school or university students in China.</s><s xml:id="_HSxqE4T">We take an iterative design approach with insights from educational literature and the involvement of teachers, learners, and HCI researchers in this process.</s><s xml:id="_ECr2w2X">We first fine-tune a text-generation model on a shortstory corpus and validate its capability in producing meaningful story context given a set of target CET-4 English words <ref type="foot" target="#foot_0">1</ref> .</s><s xml:id="_572ncZx">We then present this model to three English teachers and five experienced ESL learners in an interview study to explore possible learning activities that Storyfier can support.</s><s xml:id="_TYMYzfg">Based on the insights from the interviews and educational literature, we develop a Storyfier prototype that supports three types of vocabulary learning activities: 1) reading an AI-generated story with target words, 2) solving story cloze tests on target words (i.e., fill blanks of the generated story by using target words), and 3) writing a story using target words with the AI models by turns.</s><s xml:id="_ABuFSAT">We seek feedback on Storyfier's design and refine it via a user study with twelve ESL learners and two co-design workshops with the three English teachers mentioned above and four HCI researchers.</s></p><p xml:id="_3rENRNw"><s xml:id="_9HAeGWd">We conduct a 2 × 2 within-subjects study with 28 university students to evaluate the impact of Storyfier's AI functions (with vs. without generative models) and learning activity (read-only vs. read-cloze-write) on the learning outcome and experience.</s><s xml:id="_y7qctc3">The results show that in the read-only learning sessions, the generative stories do not help to improve learning gains in recalling target words' meanings and mastering their usage.</s><s xml:id="_H4Gm3Eg">In read-cloze-write learning sessions, participants with generated stories and AI assistance perform even worse compared to the condition without the generative models.</s><s xml:id="_K3PRTme">However, most participants still indicate their preferences on Storyfier's generated stories for connecting target words and its writing assistance for reducing learning workload.</s><s xml:id="_Q5nyjUh">Based on our findings, we highlight the value of generative models in offering meaningful materials and enjoyable experience for learning tasks.</s><s xml:id="_3wpeRan">We also urge future AI-supported learning tools to ensure users to spend the necessary effort in their learning tasks.</s></p><p xml:id="_Ty8Ha5r"><s xml:id="_JgEbNgm">Our work makes three contributions.</s><s xml:id="_m4ynftu">First, we present a vocabulary learning system Storyfier that facilitates users to master the meanings and usage of any target English words via AI-generated stories and writing assistance.</s><s xml:id="_yRcNvPB">Second, our design and evaluation of Storyfier provide first-hand findings on the feasibility, effectiveness, and user experience of applying generative models to vocabulary learning.</s><s xml:id="_qXJwJHC">Third, we offer insights and design considerations of leveraging generative models to support learning tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_3ufPNFk">RELATED WORK</head><p xml:id="_FPMV798"><s xml:id="_BgJPvfR">To situate our work, we start by reviewing the pedagogical strategies and activities for vocabulary learning.</s><s xml:id="_dNFwVsX">We then discuss previous vocabulary learning support systems.</s><s xml:id="_ZSY7kft">Lastly, we introduce related textual story-generation techniques that enable us to achieve the envisioned Storyfier.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_qYN6MX4">Pedagogical Strategies and Activities For Vocabulary Learning</head><p xml:id="_C3vpVUF"><s xml:id="_5PgAcxf">According to the amount of context information used, vocabulary learning strategies can be categorized as decontextualized, partiallycontextualized, and fully-contextualized <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b51">52]</ref>.</s><s xml:id="_ZPrQ7gH">Decontexutualized techniques, including using word lists in alphabetical order or by part of speech, flashcards, dictionary, focus on learning isolated words without meaningful contexts.</s><s xml:id="_nqjT283">For example, dictionary provides detailed instructions on grammar, pronunciation, and brief usage examples.</s><s xml:id="_2jY2pp7">However, improper use of dictionary, e.g., checking every word's meaning during reading and failing to associate it with the current context, would result in poor learning outcomes <ref type="bibr" target="#b73">[74]</ref>.</s></p><p xml:id="_75nKbFa"><s xml:id="_MXjRpta">In other words, decontextualized techniques may not aid long-term vocabulary retention and practical word usage <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b74">75]</ref>.</s></p><p xml:id="_KyrFF5A"><s xml:id="_Zd9wqy2">Educators have argued that vocabulary is better learned through contextualized learning activities <ref type="bibr" target="#b25">[26]</ref>.</s><s xml:id="_KRJQeYU">Partially-contextualized techniques provide a certain amount of context information (e.g., word association).</s><s xml:id="_qCc437Y">For instance, Word grouping organizes words according to different criteria, such as (dis)similarity and topic.</s><s xml:id="_vYXEEQS">Concept association (or "elaboration") constructs connections between new words and some familiar contexts, such as previously learned words, personal experience, or knowledge in learners' memory <ref type="bibr" target="#b6">[7]</ref>.</s><s xml:id="_BKn93q5">Besides, keyword techniques <ref type="bibr" target="#b58">[59]</ref> link words with visual <ref type="bibr" target="#b1">[2]</ref> or aural <ref type="bibr" target="#b15">[16]</ref> objects to improve vocabulary memorization.</s><s xml:id="_cjHKTUh">Fully contextualized techniques associate words in fully authentic communication contexts and connect them with a meaningful flow (e.g., logic), which are considered the peak of L2 vocabulary learning techniques <ref type="bibr" target="#b51">[52]</ref>.</s><s xml:id="_8Mp6DBd">They use existing newspapers, articles, magazines, and novels as learning material.</s><s xml:id="_qfnHtnn">The most common activities are reading or listening to the stories in contextual inference tasks (e.g., cloze test) <ref type="bibr" target="#b25">[26]</ref>.</s><s xml:id="_Hk7rn5D">Speaking and writing practices are regarded as the more effective but also challenging activities, which require turning receptive vocabulary knowledge into productive use in communication contexts <ref type="bibr" target="#b52">[53]</ref>.</s><s xml:id="_JkR4ZxX">Nevertheless, contextualized methods are demanding and complex for individual learners and are usually adopted by teachers in classroom activities <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b68">69]</ref>.</s></p><p xml:id="_6jHnTwy"><s xml:id="_E3VYFm3">Regarding the learning activities in a traditional language course, Paul Nation, suggested that there should be roughly equal amounts of time given to each of the following four strands <ref type="bibr" target="#b46">[47]</ref>.</s><s xml:id="_WutNf6x">The meaningfocused input strand involves learning through listening and reading -using language receptively.</s><s xml:id="_7v7eqSH">This strand mainly focuses on understanding what they listen to and read, e..g, stories, TVs, films, conversations, and so on.</s><s xml:id="_njn6bds">The meaning-focused output strand involves learning through speaking and writing -using language productively.</s><s xml:id="_Sp4jHCZ">Typical activities in this strand include talking in conversations, writing a letter or a note, keeping a diary, telling a story, etc.</s><s xml:id="_WWcDdmb">The language-focused learning strand involves the deliberate learning of language features such as pronunciation, spelling, vocabulary, grammar, and discourse.</s><s xml:id="_A6kHjAK">Lastly, the fluency development strand should involve four skills of listening, speaking, reading, and writing.</s><s xml:id="_5ZHUWhn">In this strand, learners are helped to make the best use of what they have already known in typical activities like ten-minutes writing and listening to easy stories.</s><s xml:id="_cJfZgFf">These four strands can fit together in many different ways <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref>.</s><s xml:id="_GS8axs9">For example, a group collaborative writing activity in the high-school can combine the meaning-focused output and language-focused learning strands, if the output written work deliberately focuses on the vocabulary and grammar <ref type="bibr" target="#b46">[47]</ref>.</s></p><p xml:id="_bWgmdhQ"><s xml:id="_86vxyNr">Our work is motivated by the benefits of fully contextualized strategies and gets inspired by the four strands of activities for vocabulary learning.</s><s xml:id="_fWfa5YX">We use textual short stories as contextualized vocabulary learning materials.</s><s xml:id="_YTxz5jA">We support individual vocabulary learners with a proper integration of the four strands of learning activities based on the story contexts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_pFuhwVF">Vocabulary Learning Support Systems</head><p xml:id="_q5jNzya"><s xml:id="_Pvdmmg6">Researchers have proposed various approaches and systems to support vocabulary learning.</s><s xml:id="_EYbmKTs">For rote learning, a bunch of work manage to model users' memory cycles and plan the target words with proper difficulty level and repetition frequency <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b89">90]</ref>.</s><s xml:id="_heKQ7js">As for our focused contextual learning, previous vocabulary learning systems have exploited materials in different mediums, such as images <ref type="bibr" target="#b75">[76]</ref>, physical locations <ref type="bibr" target="#b87">[88]</ref>, textual articles in webpage <ref type="bibr" target="#b0">[1]</ref>, subtitles of videos <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b66">67]</ref>, and augmented/virtual reality <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b67">68]</ref>.</s><s xml:id="_h3tZWDY">For instance, FinDo <ref type="bibr" target="#b87">[88]</ref> is a mobile application that helps users understand the vocabulary about the surrounding objects with the contexts of users' current locations.</s><s xml:id="_KSKVB57">Tangworakitthaworn et al. <ref type="bibr" target="#b75">[76]</ref> used image processing techniques to extract visual objects in photos and matched them with the target vocabulary.</s><s xml:id="_zXqunaz">VocabEncounter <ref type="bibr" target="#b0">[1]</ref> encloses target vocabulary into reading materials to facilitate micro learning in daily life.</s><s xml:id="_Ufb3dpG">Smart Subtitles <ref type="bibr" target="#b35">[36]</ref> equips video subtitles with features like vocabulary definitions on hover and dialog-based video navigation.</s></p><p xml:id="_GNq3w6r"><s xml:id="_J3jGXRu">However, these systems largely make use of existing materials as contexts, which may not be able to provide a meaningful flow that covers any set of target words -a requirement of fully contextualized learning <ref type="bibr" target="#b51">[52]</ref>.</s><s xml:id="_KTPF6Sm">We seek to mitigate this constraint by generating a short story for any target word set.</s><s xml:id="_9MaJJTk">Our decision to use stories as the context for words is inspired by their common usage in language learning textbooks and the proven efficacy of story-based learning in other scenarios <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b91">92]</ref>.</s><s xml:id="_P4r5Pbt">Further, previous vocabulary learning support systems mainly focus on supporting meaning-focused input activities that aim at understanding the words' meanings.</s><s xml:id="_y8hcXqW">Our work further supports other types of learning activities that help to master the usage of target words.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_QFvuFRJ">Textual Story Generation Techniques</head><p xml:id="_DdJZBmU"><s xml:id="_xGu5WtH">Recent advances in textual story generation offer potentials to support vocabulary learners with meaningful contexts that cover any word set and offer in-situ learning support.</s><s xml:id="_6A4T3FS">The textual story generation techniques aim at generating coherent and fluent narratives or ideas based on simple user inputs, such as a title <ref type="bibr" target="#b45">[46]</ref> and prompts <ref type="bibr" target="#b16">[17]</ref>.</s><s xml:id="_C2EqzFQ">Early computational work adopts symbolic approaches <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b83">84]</ref> that first select a sequence of characters and actions according to aesthetic, narrative conflicts, and logic, and then create a story with pre-defined templates.</s><s xml:id="_K345bdy">Another approach is case-based reasoning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b76">77]</ref>, which extracts the story plots of existing stories and adapts them to new contexts.</s><s xml:id="_dpsjsXx">Yet, these methods are restricted by predefined story domains and styles.</s><s xml:id="_JM3MNXg">Recent story generation methods mainly adopt sequence-to-sequence language models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b88">89]</ref>, which can learn complex and implicit relationships among story plots.</s><s xml:id="_GNqyC6U">Particularly, transformer-based models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b69">70]</ref> are able to produce incredibly fluent texts after training on a large language corpus.</s><s xml:id="_JWNShaB">These models can be finetuned to support downstream applications like writing assistants <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and health consultation <ref type="bibr" target="#b81">[82]</ref>.</s></p><p xml:id="_TA7jVyZ"><s xml:id="_khduEpZ">To generate stories with desired properties (e.g., keywords, topic, styles), researchers apply techniques like decoding strategies, prompt controls, and finetuning to build controllable language models.</s><s xml:id="_eXHC6p8">Decoding strategies aim to restrict and influence the sampling process of generation to change the features of output texts.</s><s xml:id="_TG796aa">These features can describe the user preferences and are modeled by heuristics <ref type="bibr" target="#b19">[20]</ref>, supervised signals <ref type="bibr" target="#b29">[30]</ref>, and reinforcement learning <ref type="bibr" target="#b39">[40]</ref>.</s><s xml:id="_PBknAxB">Prompt controls use natural language (e.g., "translate to English") to elicit desired contents <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b71">72]</ref>.</s><s xml:id="_vasZUDE">Finetuning methods investigate effective conditional training based on key words <ref type="bibr" target="#b16">[17]</ref>, story valence <ref type="bibr" target="#b54">[55]</ref>, character fortune <ref type="bibr" target="#b8">[9]</ref>, control codes <ref type="bibr" target="#b34">[35]</ref> (e.g., , topic, sentiment), and simpler attribute models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b36">37]</ref>.</s></p><p xml:id="_qbEuYMZ"><s xml:id="_Bqu6jvT">Recent intelligent systems have explored the usage of text generation techniques in a variety of scenarios, such as creative writing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b56">57]</ref>, AI-mediated communication <ref type="bibr" target="#b17">[18]</ref>, and health intervention <ref type="bibr" target="#b33">[34]</ref>.</s><s xml:id="_Db42kqM">In the story-based learning scenario, StoryBuddy <ref type="bibr" target="#b91">[92]</ref> assists parents-children storytelling via a question-answer generation model, which consists of a rule-based answer generation module, a BART-based question generation module, and a ranking module.</s><s xml:id="_pes9HRD">It can help parents create a storytelling bot that can tell stories, ask children questions, and provide feedback <ref type="bibr" target="#b91">[92]</ref>.</s><s xml:id="_PPEjdhu">However, these studies present a different focus compared to ours.</s><s xml:id="_84kQqHF">We specifically investigate the use of and interaction with text generation models for story-based vocabulary learning.</s></p><p xml:id="_fGrvZJV"><s xml:id="_432UywH">In this paper, we first customize and evaluate a controllable language model for generating meaningful stories that cover given target word set.</s><s xml:id="_6qrWMtX">We then explore what vocabulary learning activities that this model can support with teachers and students.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_C8QWKhp">PHASE 1: FEASIBILITY AND SUPPORTED ACTIVITIES OF STORY GENERATION MODELS FOR VOCABULARY LEARNING</head><p xml:id="_EyrpMpJ"><s xml:id="_Mg8fD2r">To help individual learners to master the meaning and usage of any target word sets, we design and develop Storyfier via a two-phase process (Figure <ref type="figure" target="#fig_0">1</ref>).</s><s xml:id="_4DSrgUY">In this section, we present the first phase in which we 1) develop a story generation model, 2) validate its feasibility for providing meaningful contexts for vocabulary learning, and 3) explore what vocabulary learning activities this model could support.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_KJwXn7P">Developing Story Generation Models</head><p xml:id="_rpN7Ua5"><s xml:id="_ZdYE4n6">Given the potential benefits of a meaningful story for learning a batch of words <ref type="bibr" target="#b24">[25]</ref>, we first seek to develop a controllable language model that can generate stories with given target word sets.</s><s xml:id="_jVZskTb">Here, we target the vocabulary pool (4,827 in total) required by the College English Test Band 4 (CET-4), a mandatory national test for Chinese university students to obtain bachelor degrees.</s></p><p xml:id="_thr24Ce"><s xml:id="_4MJdt4s">3.1.1</s><s xml:id="_2VZ9yw4">Dataset.</s><s xml:id="_NJZS6pK">We choose ROCStory corpus <ref type="bibr" target="#b45">[46]</ref> to contextualize CET-4 words and build story generation models.</s><s xml:id="_9RhKmMk">ROCStory collects over 100,000 five-sentence commonsense human-written stories (Table <ref type="table" target="#tab_0">1</ref> <ref type="foot" target="#foot_1">foot_1</ref> ).</s><s xml:id="_9mcguuR">The simple and short story form could help learners easily understand the story flow and mitigate diversion from vocabulary learning to story comprehension.</s><s xml:id="_eDKt6tD">The simplicity of the story structures and logic is also appreciated by English teachers who participate in the later studies (subsubsection 3.3.2).</s><s xml:id="_faeahPS">Though these stories are short, they are created by various human workers and have passed qualification tests to ensure story quality and creativity.</s><s xml:id="_qfGsYyK">In addition, these stories have causal and temporal commonsense relationships between story sentences and cover a wide range of everyday topics, such as movie, school, birthday, and music.</s><s xml:id="_x9epajs">Therefore, if there is a story that covers a set of target words, learners can easily associate a group of words with a common topic following a meaningful logic flow.</s><s xml:id="_fYMNa6p">With this dataset, we aim to develop a model that can generate meaningful stories like those in ROCStory given any set of target words in the CET-4 pool.</s></p><p xml:id="_Kprrftb"><s xml:id="_egvJjhf">3.1.2</s><s xml:id="_zwP6q3D">Data Preprocessing and Model Building.</s><s xml:id="_9yV3Q73">Figure <ref type="figure" target="#fig_2">2</ref> summarizes our data preprocessing and model building procedure.</s><s xml:id="_nFTt4zZ">Specifically, we follow recent story generation techniques <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b54">55]</ref> and formulate the problem as a sequence-to-sequence translation task.</s><s xml:id="_Q5XKszN">We first segment the stories into titles 3 and sentences.</s><s xml:id="_tYQzMwr">Then, using the CET-4 word list, we identify the occurrences of these words in each sentence of every story and sort them chronologically.</s><s xml:id="_9CuXvzV">This leads to the creation of a set of {story title, target words, story sentences}  <ref type="figure" target="#fig_2">2A</ref>).</s><s xml:id="_MebGYe3">For story generation, we leverage a state-of-theart open-source language model T5 <ref type="bibr" target="#b61">[62]</ref> as the base model.</s><s xml:id="_jnFNkkV">Our decision is made based on two reasons.</s><s xml:id="_sVzWNBf">First, T5 exhibits impressive performance across various NLP tasks (e.g., text generation and classification), which can be attributed to its unified text-to-text framework and its pretraining on a large language corpus.</s><s xml:id="_YVhWtTZ">Moreover, it is freely available and adaptable to our application scenario compared to other impressive but closed-source language models (e.g., GPT-3 <ref type="bibr" target="#b3">[4]</ref> and GPT-4 <ref type="bibr" target="#b50">[51]</ref>).</s></p><p xml:id="_V72XuPd"><s xml:id="_xUXF866">Then, we adopt a prompt-based approach to finetune and steer the model generation process to learn the mappings between target English words and a story (Figure <ref type="figure" target="#fig_2">2B1</ref>).</s><s xml:id="_6UuJ96c">We formulate the input prompt as the concatenation of the story title and target words derived from story tuples of the processed dataset.</s><s xml:id="_4FcYgzB">According to our experiments, the title imposes a high-level control of story relevance and leads to faster and better convergence compared to training without the title signal.</s><s xml:id="_vGkjvwF">We finetune the pretrained T5large model offered in the HuggingFace on our dataset using Adam optimization algorithm with a 0.0001 learning rate.</s><s xml:id="_hhk3vX2">The training process lasts for five epochs and has a 0.9857 cross-entropy loss.</s></p><p xml:id="_uNQXEdK"><s xml:id="_UTHudqP">After training, our model can generate complete stories rather than isolated sentences, thus creating meaningful contexts for the target words across multiple sentences.</s><s xml:id="_JppsPNu">For instance, when given the words "athlete", "avid", and "frequently" (as shown in Figure <ref type="figure">3</ref>), the model begins a narrative about an avid athlete who frequently participates in marathons.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_jjkv9mC">Evaluating the Quality of Generative Stories</head><p xml:id="_q5uu5r5"><s xml:id="_cDcnTZS">While our model can generate a story given any word set with or without a title, at this stage, we would like to compare the quality of machine-generated and human-written stories in the corpus which cover the same target word set.</s><s xml:id="_cSV5Qkk">This evaluation aims at validating if the generated stories were competent for vocabulary learning support.</s><s xml:id="_5k9MUfg">We will assess the perceived quality and helpfulness of generated stories given any target words in our later interviews with teachers (subsubsection 3.3.2) and experiments with learners (subsubsection 6.3.2).</s><s xml:id="_wg3xGVE">Following prior work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b54">55]</ref>, we conduct technical and human evaluations.</s><s xml:id="_6VH6ynm">We sample 20 stories from the ROCStory dataset with varied difficulty levels (i.e., word frequency) of contained CET-4 words.</s><s xml:id="_3mv2s9G">For each human-written story, we use our trained language models to generate a machine version based on the story title and contained CET-4 words.</s><s xml:id="_UR4EbnJ">Thereafter, we create 20 human-machine story pairs (40 stories in total).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1" xml:id="_npCPVAZ">Technical Evaluation.</head><p xml:id="_D9TTwbk"><s xml:id="_qghk5Ez">We assess the story content from grammatical accuracy, lexical diversity (i.e., number of unique words, and</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YEd5nft">Data Preparation</head><p xml:id="_qjMSMXW"><s xml:id="_RbA5CDe">Story Title: Sammy Gets A Cramp CET-4 Words: Story Content: Sammy was doing sit-ups.</s><s xml:id="_9DfWPzS">He felt a sharp pain in his stomach.</s><s xml:id="_8PTXuep">He was having a cramp on his sides.</s><s xml:id="_JVGvByV">He grasped his stomach while rolling on the ground.</s><s xml:id="_4jJkTNW">The cramp went away after 5 minutes.</s><s xml:id="_KrFQdty">We finetune T5 language models to 1) generate a story given a CET-4 word set with or without a title (presented in section 3.2) and 2) infill a sentence or n-grams given preceding and following sentences, unused target words, and story title (if any) (subsubsection 4.2.1).</s><s xml:id="_THBTsZd">(C) We apply the models to support three kinds of story-based learning activities.</s><s xml:id="_R8dFFtS">Table 3: Average human ratings of machine-generated and human-written stories.</s><s xml:id="_kSVDPdq">(*: p &lt; .05</s><s xml:id="_u8G4ATf">using Wilcoxon Signedrank test) Coherence * Relevance * Interestingness Overall Human 4.53 4.58 4.08 4.26 Machine 3.92 4.33 3.97 4.01</s></p><p xml:id="_8fSvmBa"><s xml:id="_WtUX5t9">type-token ratio: number of unique words/total number of words), and lexical coherence (i.e., trigram repetition, and sentence coherence 4 : average semantic similarities between sentences) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b64">65]</ref>.</s></p><p xml:id="_BQP6MVn"><s xml:id="_XTgcWcz">As shown in Table <ref type="table" target="#tab_1">2</ref>, both the machine-generated and humanwritten stories have no grammar issues.</s><s xml:id="_QSRSzsN">Moreover, the machine performance is commensurate with the human in terms of lexical diversity and coherence, as indicated by close scores of type-token ratio, trigram repetition, and sentence coherence.</s><s xml:id="_Rzzzjhb">The results provide quantitative support that our model can generate grammartically correct and lexically coherent and diverse story texts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2" xml:id="_SVarpZ2">Human Evaluation.</head><p xml:id="_27HnwFc"><s xml:id="_bY43VFp">We invite eight PhD students (four females and four males, mean age: 25.50 (SD = 2.07)) with English paper publications to rate their perceived quality of these 40 stories in random order.</s><s xml:id="_sqXXQkp">According to previous work <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b88">89]</ref>, we consider: 4 Cosine similarities (range 0-1) between sentence embeddings using sbert.</s></p><p xml:id="_7C2R5SD"><s xml:id="_sKF9Jj6">coherence (The story is logically consistent and coherent), relevance (The story is relevant to the title), interestingness (The story is interesting), and overall quality (Overall, it is a good story).</s><s xml:id="_UyQ4wjJ">Each aspect is rated on a standard five-point Likert Scale (1 for "Strongly disagree" and 5 for "Strongly agree").</s><s xml:id="_7EMQ8pX">As shown in Table <ref type="table">3</ref>, the machine-generated stories achieve comparable performance with the human version regarding overall quality and interestingness.</s><s xml:id="_Yg9p9ZF">The human-written stories are considered significantly more coherent and relevant using the Wilcoxon Signed-rank test.</s><s xml:id="_qruJtjs">Nevertheless, the machine-generated stories have average scores of around four points in terms of coherence and relevance.</s><s xml:id="_s6aueP6">Therefore, we consider that our system could produce adequate story context given a set of target words for vocabulary learning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_BBHDU6m">Exploring Vocabulary Learning Activities with Story Generation Models</head><p xml:id="_Pe6SrRc"><s xml:id="_asXCR6B">After validating the feasibility of our model for generating meaningful context that covers a set of CET-4 English words, we explore possible vocabulary learning activities that the model can support.</s><s xml:id="_2AGVFgE">We conduct semi-structured interviews with three English teachers (E1-3, age: 27 -28) and five university students (S1-5, age: 21 -29) in China.</s><s xml:id="_bAJN5hj">E1 has two years of experience in teaching IELTS and half-ayear experience in teaching English in a higher vocational college.</s><s xml:id="_eXAAeew">E2 has spent five years in high-school English teaching, and E3 has taught high-school students mainly about TOEFL writings for three years in an educational institution.</s><s xml:id="_qebXyfU">S1-5 are well-experienced in using different English vocabulary learning software for Chinese (e.g., Liulishuo, Baicizhan, Shanbay).</s></p><p xml:id="_j2gKvAw"><s xml:id="_AAVDbws">3.3.1 Procedure.</s><s xml:id="_ADHQUGP">Each interview starts with participants' practices (whether, why, and how) of story-based activities for teaching or learning vocabulary.</s><s xml:id="_wQRvbPN">Then, we show participants a web interface that allows users to input target English words and generate stories with those words based on our model.</s><s xml:id="_YZPujfz">We prepare example CET-4 word sets, each with the top-five topic-relevant words (e.g., cable, complain, library, instruction, unfortunate) <ref type="foot" target="#foot_3">5</ref> under our specified titles (e.g., the internet) and the generated stories in the interface.</s><s xml:id="_E4mBYsV">We invite our participants to check the generated stories and have a trial using their specified words.</s><s xml:id="_WdFcZmg">During this process, we encourage them to brainstorm the vocabulary learning activities that our story generation model can support.</s><s xml:id="_EuJfMzu">Each interview lasts for about 30 minutes with about USD $3.5 for compensation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2" xml:id="_5V4Yvem">Results</head><p xml:id="_ADYUb58"><s xml:id="_dpf5juB">. We transcribe the audio data into texts and group them into themes following the interview structure.</s><s xml:id="_ZJgQFfU">Both groups of interviewees confirmed that learning English vocabulary via stories is a common and effective practice.</s><s xml:id="_rWUSmxc">For example, E1 mentioned that he usually asks students to first write sentences and then create a short story with newly learned words, which helps them master the usage of words.</s><s xml:id="_d5jb5hd">Three student participants regularly read English books and articles, which expands their vocabulary.</s><s xml:id="_nKCCxc2">In general, all participants agreed that our generative stories are suitable materials for learning target words.</s><s xml:id="_fnYFX3E">For instance, E3 tried the story generation model using "health" as title and "tobacco, alcohol, abuse, dominate, harmful" as target words.</s><s xml:id="_UFV57uf">These words come from an article of her high-school text book.</s><s xml:id="_N6NZSvM">"I like the generated story.</s><s xml:id="_NgbjZCu">It is generally coherent, and it is simpler than the one in the text book.</s><s xml:id="_ymas7ts">My students would like it for vocabulary learning as they do not need to pay too much attentions on the long sentences" (E3).</s><s xml:id="_w4EPuFK">Nevertheless, our three teachers pointed out that the generated stories lack explicit logic transition words like "nevertheless" and "for example", which can further improve the stories' coherence.</s><s xml:id="_YZUSekX">This is probably due to the lack of these words in our training dataset ROCStory corpus.</s><s xml:id="_gKgEr3u">Our interviewees actively provide ideas for leveraging our generative model to support vocabulary learning.</s><s xml:id="_MZRUPKQ">Together with the insights from pedagogical literature (e.g., those in subsection 2.1), we summarize three supported vocabulary learning activities.</s></p><p xml:id="_vYcdahF"><s xml:id="_Yz8Cxz8">Story reading: learners can read the generated story to understand the meanings of the target words (E1-3, S1-5).</s><s xml:id="_xBaYjnq">This is also a typical meaning-focused input activity suggested by language educators <ref type="bibr" target="#b46">[47]</ref>.</s></p><p xml:id="_wkb99ss"><s xml:id="_AYaWuu2">Cloze test: learners can do a cloze test that fills blanks of the generated story using target words to strengthen their understandings (E2, S2, S3).</s><s xml:id="_DVt3r7Z">"Cloze test is a common vocabulary learning strategy in the textbook.</s><s xml:id="_mDArq2p">I feel that it would be helpful to customize the generated stories into cloze tests for students" (E2).</s><s xml:id="_Gzx6qQc">Cloze test can be viewed as a language-focused learning activity with a focus on the usage of target words <ref type="bibr" target="#b46">[47]</ref>.</s></p><p xml:id="_HCpEQMb"><s xml:id="_rv3P6BN">Turn-taking writing: learners can take turns with the generative models to co-write a story using target words (E1, E3).</s><s xml:id="_M4qRbkQ">This practice combines the meaning-focused output and fluency development learning activities, as it requires learners to use the learned words productively and fluently <ref type="bibr" target="#b46">[47]</ref>.</s><s xml:id="_m4MMx8H">"It can generate a sentence using a target word as a start, and users write down the second sentence using another word.</s><s xml:id="_QaxY23N">With such interaction, students can learn how to use the words in a successive manner" (E1).</s><s xml:id="_srFTyDw">"The system can act as one student to do a turn-by-turn, co-writing practice" (E3).</s><s xml:id="_69KND8p">The system can provide in-situ guidance and feedback on users' input in the writing process <ref type="bibr" target="#b46">[47]</ref>, e.g., "are target words used correctly" (E3) and "is the story coherent and correct in grammar?" (E1).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_4wJBvqQ">PHASE 2: STORYFIER SYSTEM IMPLEMENTATION AND REFINEMENT</head><p xml:id="_WKbWktR"><s xml:id="_qZRbaR5">After validating the feasibility of our story generation model and identifying promising ways to apply it, we present our secondphase design process about how we implement Storyfier and refine it with feedback from learners, teachers, and HCI researchers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_EGbj4jC">First Storyfier Prototype with Three Modes</head><p xml:id="_8f24WXm"><s xml:id="_rFJzp6t">Based on the interview findings, we design and implement three modes of user interfaces to facilitate vocabulary learning via story reading (Figure <ref type="figure">3B</ref>), cloze test (C), and turn-taking writing (D).</s></p><p xml:id="_P8GfyEp"><s xml:id="_YSHDdgC">4.1.1</s><s xml:id="_8BN2HFr">Interface Designs and User Workflow.</s><s xml:id="_jh5WAHk">All three modes share the following two features (Figure <ref type="figure">3A</ref>).</s><s xml:id="_swXD9Tm">[Target words setting] Users can manually add new words ("+") or delete them ("x") as they wish.</s><s xml:id="_eKKwUUz">They can also click the button to get a randomly sampled target word set.</s><s xml:id="_fu4rejq">[Dictionary lookup] Users can click each target word to inspect its definition, part of speech, phonetic symbol, and usage example.</s><s xml:id="_pQy2Tgk">The click on will lead to three vocabulary learning activities supported in the following interface variants.</s><s xml:id="_PYpdhtr">Story reading mode.</s><s xml:id="_6wpFMHf">This interface (Figure <ref type="figure">3B</ref>) presents the AIgenerated story with the target words highlighted in blue, which could help users quickly inspect their contextual use.</s><s xml:id="_eV8hWzh">In addition, users can conduct minor edits (e.g., , revise words) of the sentences to refine the story if they wish.</s></p><p xml:id="_YAyCUMw"><s xml:id="_gTumdzq">Cloze test mode.</s><s xml:id="_h8d2HBx">This interface (Figure <ref type="figure">3C</ref>) replaces the target words in the generated stories with blanks.</s><s xml:id="_ENyCEes">Users are required to make contextual inferences about the missing words and choose the proper ones to fill the blanks.</s><s xml:id="_UmURFwK">After they submit the results, Storyfier will check the correctness and highlight the misused words (if any) in red.</s><s xml:id="_KcwBpjp">Users can iteratively fix the errors if they wish.</s></p><p xml:id="_GVCTHzh"><s xml:id="_r5ggj8C">Turn-taking writing mode.</s><s xml:id="_Y4tyArG">This interface (Figure <ref type="figure">3D</ref>) encourages users to write a story with AI using the target words sentence by sentence.</s><s xml:id="_qEYraQT">During the writing process, users can gain an overview of the used (gray) and unused (blue) target words at the top in Fig- <ref type="figure">ure 3D</ref>.</s><s xml:id="_TGkm76U">The used target words are highlighted in the corresponding sentences.</s><s xml:id="_4rJ2af5">To provide adaptive feedback to learners, in each turn, the system will check and alert the grammar issues of the written text using LanguageTool API <ref type="foot" target="#foot_4">6</ref> .</s><s xml:id="_bJ8kpzz">Meanwhile, Storyfier provides writing feedback on the story sentences at the bottom regarding grammar errors, lexical diversity, and lexical coherence (in Figure <ref type="figure">3</ref><ref type="figure" target="#fig_5">4</ref><ref type="figure" target="#fig_6">5</ref>).</s><s xml:id="_RgtmeYD">Red and green triangles indicate a decrease or increase in scores of all current story sentences compared to the one in the previous turn.</s><s xml:id="_NM32sT9">Users can write and refine the story with Storyfier until all the target words are used.</s><s xml:id="_muQyDzF">and story sentences as prompts (described in subsubsection 3.1.2).</s><s xml:id="_EENVdG9">Meanwhile, for each story sentence, we randomly mask variedlength of text spans of this sentence, following prior work <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_SDVjat6">Then, we train the model to predict the masked spans of the current sentence based on the prompts.</s><s xml:id="_SZ4K54H">We use a cross-entropy loss and finetune the pretrained T5-large model provided by Huggingface on our mask prediction task using adam optimization algorithm.</s><s xml:id="_mUvNDe6">We train 10 epochs, and the training loss is 1.0701.</s></p><p xml:id="_RHurGN4"><s xml:id="_tnjZMYv">With this finetuned model, Storyfier can write the next sentence using target words following the users' written ones.</s><s xml:id="_2sHFYj9">Furthermore, in our refined Storyfier presented below (subsection 4.3), it can also help users revise an existing sentence or complete the unfinished one via text infilling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_Zt3KBxh">Testing Storyfier Prototype</head><p xml:id="_qeR7P5z"><s xml:id="_PxYdB4t">To seek feedback on the 1st Storyfier prototype, we conduct a usability test with ESL learners and two workshops with English teachers and HCI researchers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1" xml:id="_W3zmzju">Usability Test with 12 ESL Learners.</head><p xml:id="_qEmdeSN"><s xml:id="_2WfhafS">To probe the user experience and perceived usefulness of the three activities supported by Storyfier, we conduct a within-subjects usability test with 12 junior undergraduate students (6 females, 6 males, mean age: 19.5 (SD = 0.52)) in a university in China.</s><s xml:id="_RPG32e6">The baseline condition does not have the generated story contexts but provides a dictionary function that shows the meanings, synonyms/antonyms, and usage examples for each target word (Figure <ref type="figure" target="#fig_0">3A1</ref>).</s><s xml:id="_xBrsAJP">All participants have passed the national English exam CET-4, with an average score 560.50 (SD = 37.75) 7 .</s><s xml:id="_X5tBW9X">We do not aim to evaluate Storyfier's effectiveness but seek to improve it with quick user feedback at this stage.</s><s xml:id="_VAfXQ9h">These participants can provide us with valuable feedback as they have fresh CET-4 vocabulary learning experience.</s></p><p xml:id="_Ayg5VyY"><s xml:id="_tM4faAm">[Procedure] Participants use their own computers to remotely conduct the study following the instructions.</s><s xml:id="_mfQFptG">They experience the four experiment conditions (i.e., Dictionary, Read, Cloze, Turn-taking Write) one by one in a Latin-Scale counterbalanced order.</s><s xml:id="_dcc3vVw">In each condition, they learn two prepared word sets, each with five CET-4 words sampled based on topic relevance.</s><s xml:id="_UB6mhXq">After each condition, participants rate their perceived usefulness, easiness to use, and intention to use <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b80">81]</ref> of each interface in a 7-points Likert scale; 7 for a strong agreement.</s><s xml:id="_2EBpYTr">In the end, we ask for their comments and suggestions on Storyfier.</s><s xml:id="_YZTmFXa">They receive about USD $9.5 for around 50 minutes spent in the study. 7</s><s xml:id="_EtCG3uy">425/710 points are considered passed for CET-4.</s></p><p xml:id="_DHDPmvM"><s xml:id="_3kjrXgQ">[Results] We use repeated measured ANOVA test (Dictionary vs. Read vs. Cloze vs. Turn-taking Write) to evaluate the user experience of Storyfier's three modes.</s><s xml:id="_S2DdymU">There is a significant difference in perceived usefulness of the four activities;  (3, 33) = 3.83,  &lt; 0.05,  2 = 0.26.</s><s xml:id="_UdCVjW4">Specifically, they feel that the Read ( = 4.94,  = 1.15,  &lt; 0.05) system is significantly more useful than the Dictionary one ( = 3.56,  = 1.45).</s><s xml:id="_pVrMhRJ">Participants feel that the Read ( = 4.83,  = 1.12,  &lt; 0.01) system is significantly easier to use than the Turn-taking one ( = 3.15,  = 0.85);  (3, 33) = 9.54,  &lt; 0.001,  2 = 0.46; Bonferroni post-hoc test.</s><s xml:id="_v7STqnc">Besides, the Cloze ( = 4.27,  = 1.13,  &lt; 0.05) system is deemed significantly easier to use than the Turn-taking one.</s><s xml:id="_pyvq95M">Lastly, participants have significantly higher intentions to use the Read ( = 4.71,  = 0.33) system for their vocabulary learning in the future, compared to the Dictionary ( = 2.96,  = 1.63,  &lt; 0.01) and Turn-taking systems ( = 3.13,  = 1.40,  &lt; 0.05);  (3, 33) = 5.80,  &lt; 0.01,  2 = 0.35.</s><s xml:id="_7fmkqgv">In summary, ESL learners found that the three learning activities supported by Storyfier are more useful than the baseline without story context.</s><s xml:id="_F9jHE8Y">However, the Storyfier's turn-taking writing mode should be further improved.</s><s xml:id="_q7MenEK">For example, two students indicated that sometimes they found it difficult to use words to write the next story sentence in this activity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2" xml:id="_kWMxjRq">Co-Design</head><p xml:id="_MMMPvks"><s xml:id="_dgMxyFG">Workshops with English Teachers and HCI Researchers.</s><s xml:id="_AQ4EnWw">Apart from the feedback from ESL learners, we conduct two co-design workshops to seek experts' feedback.</s><s xml:id="_QsWaeUY">The two workshops share a similar procedure but have a different focus.</s><s xml:id="_CQaCWUp">One is with the same three English teachers (E1-3) in Phase 1 and focuses on refining the vocabulary learning activities in Storyfier.</s><s xml:id="_HdXRfbd">The other is with four HCI researchers (H1-4, all males, age: 25-27) and mainly works on the interface and interaction design of Storyfier.</s><s xml:id="_ksTTkkB">All HCI researchers have experience in developing intelligent systems and have papers published in top venues like CHI and VIS.</s><s xml:id="_kmqNMKk">Each workshop starts with a warm-up activity in which participants share their experience of story-based vocabulary teaching or learning.</s><s xml:id="_FCpfqsg">Then, we show our Storyfier prototype to them, invite them to have a trial, and ask them to give comments on the system.</s><s xml:id="_x78sSEd">Next, we organize a brainstorming session to discuss how to leverage the three learning activities of Storyfier for effective vocabulary learning support and how to improve the interaction and interface design.</s><s xml:id="_SwcUfMp">Each workshop lasts about one hour, and participants receive about USD $17 as compensation.</s><s xml:id="_nVzxWmt">We present their suggestions on Storyfier together with its refinement in the next subsection.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_znqf2ZZ">Refined Storyfier System</head><p xml:id="_FDaJE3b"><s xml:id="_h994gnA">Based on the collected feedback on the first Storyfier prototype, we refine its workflow and features (Figure <ref type="figure">3</ref>).</s></p><p xml:id="_SpbR2PC"><s xml:id="_PuGGMDG">Workflow.</s><s xml:id="_XnKXeFZ">We unify the three separate learning activities into one workflow using step bars and next-step buttons (in Figure <ref type="figure" target="#fig_2">3-2</ref>) to guide learners to read the story, do a cloze test, and write a new story.</s><s xml:id="_gaxQC23">Our English teachers agree that all three learning activities would be generally helpful, but there could be a flow that chains these activities to maximize their values.</s><s xml:id="_hNMRZh6">They suggest that reading should be the first activity to help comprehend the target words.</s><s xml:id="_yx9RDJD">The cloze test should come next to strengthen their understanding, and the co-writing practice should be the last activity.</s><s xml:id="_uMnX3vU">"Cloze test is a controlled practice, and co-writing is a free one" (E3).</s><s xml:id="_zTFeGHQ">To chain the three learning activities into a flow, S3 proposes to use a chatbot to guide users through the learning process, which could be engaging.</s><s xml:id="_Tnv4k6A">This is similar to the chatbot interaction in StoryBuddy <ref type="bibr" target="#b91">[92]</ref>.</s><s xml:id="_5Xp92PK">However, the other three HCI researchers are concerned that it might distract users' attention from vocabulary learning to interaction with the chatbot.</s><s xml:id="_9T9U6bJ">S1 suggests that we can use clear widgets (e.g., the right arrow and "Next Turn" buttons) to order the flow of the three activities.</s></p><p xml:id="_n53yFfV"><s xml:id="_cYThfZf">Features.</s><s xml:id="_bCWzDDu">First, we add the main Chinese meaning of each target word in the dictionary (Figure <ref type="figure">3</ref>-1) as suggested by E2.</s><s xml:id="_d4vZbHJ">Second, we modify the turn-taking order by encouraging users write the first sentence of the story (Figure <ref type="figure">3</ref>-3), as suggested by E3 that we should encourage learners to spend effort first.</s><s xml:id="_ygMpuk5">Third, we add an inline sentence suggestion function that can infill a generated next sentence using target words (Figure <ref type="figure">3</ref>-4.1 and -4.2), to address learners' difficulties in story writing activity found in the usability test.</s><s xml:id="_AzqS5Sp">This function can be triggered in real-time by the "tab" key on users' demands, as suggested by the HCI researcher H4.</s><s xml:id="_vH5dB5s">Third, we remove the technical metrics about sentence quality (Figure <ref type="figure">3</ref><ref type="figure" target="#fig_5">4</ref><ref type="figure" target="#fig_6">5</ref>), as suggested by E1-3 that they are complicated for learners and not focused on the target words' usage.</s><s xml:id="_EDMFC3R">Fourth, we add the number of used target words and the number of words written by human/machine as writing feedback because it could encourage learners to write more.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_BB7ZDsQ">EXPERIMENT</head><p xml:id="_R6HpmfY"><s xml:id="_5CAF4uk">To explore how would Storyfier impact the users' vocabulary learning outcome and experience, we conduct an experiment with 28 ESL (English-as-the-Second-Language) Chinese students.</s><s xml:id="_BurrJhE">We adopt a 2 (with vs. without AI features) x 2 (read-only vs. read-cloze-write activities) within-subjects design.</s><s xml:id="_4KEsTnb">The first one -AI factor -aims to study the impacts of Storyfier's AI-generated story and adaptive writing support.</s><s xml:id="_mah8Fa9">We note the conditions with AI features with "-AI" and those without AI features with "-sen".</s><s xml:id="_Axwjx64">The second oneactivity factor -identifies the value of additional cloze test and writing activities to the reading activity that previous vocabulary learning support systems focus on.</s><s xml:id="_s2q7FuW">We note the conditions with the read-only activity with "Read-" and those with read-cloze-write activities with "Storyfier".</s><s xml:id="_nPjENeY">The four conditions are:</s></p><p xml:id="_3NgVeJx"><s xml:id="_3F6Bz2r">Read-only</s></p><p xml:id="_Y3fDAgy"><s xml:id="_9Wfw9by">• Read-sen interface only provides dictionary features with an example existing sentence for each target word (  without adaptive support, which can help us evaluate the impact of Storyfier's story generation model.</s><s xml:id="_ahJVfUB">Our research questions are: RQ1.</s><s xml:id="_q3DDvc9">How would Storyfier affect vocabulary learning outcome?</s><s xml:id="_35EcXbN">RQ2.</s><s xml:id="_UUPbmDp">How would Storyfier affect the learning experience?</s><s xml:id="_mfrYXX3">RQ3.</s><s xml:id="_eQTwMEX">What are user perceptions towards Storyfier?</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_q7aCJDw">Participants</head><p xml:id="_cvrtp4K"><s xml:id="_EeQDRKj">We recruit 28 second-year undergraduate students (P1-28, 24 females, 3 males, 1 prefer not to tell, mean age: 20.04 (SD = 0.69)) from a course in a college in mainland China.</s><s xml:id="_mp9SExJ">They are typical ESL learners who major in Business English.</s><s xml:id="_RrHuGea">The course nature leads to the gender and major unbalance of our participants, which we will discuss in the Limitations subsection.</s><s xml:id="_EAurHER">Twelve of them have not passed the national English exam CET-4 in China, and the rest have passed it with an average score 493.8/710 ( = 35.8) <ref type="foot" target="#foot_5">8</ref></s><s xml:id="_MqfV4NA">.</s><s xml:id="_2uAcCR3">Their self-assessed English vocabulary proficiency score is 4.39 (SD = 0.63; 1 -not proficient at all, 7 -very proficient).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_UfjwBeq">Procedure and Tasks</head><p xml:id="_BTYPbnC"><s xml:id="_kpf8YUM">We conduct the experiment remotely.</s><s xml:id="_VBuJmVW">In a similar manner as <ref type="bibr" target="#b0">[1]</ref>, the procedure of our experiment consists of three stages (Figure <ref type="figure" target="#fig_5">4</ref>).</s><s xml:id="_Yvs6KV4">First, after collecting the background information with consent, we ask each participant to take a pretest to identify the CET-4 words that they did not know.</s><s xml:id="_Dy2KGgB">In the pretest, the participant needs to choose one of the five options, including four meanings written in Chinese and one indicating "I do not know this word", for each CET-4 word.</s><s xml:id="_Mj34saK">We invite a postgraduate to prepare 170 CET-4 words that are not easy (e.g., excluding words like "easy" and "feel") from the English learning app Baicizhan and only include the intended participants who answer incorrectly or indicate lack of knowledge on at least 40 words.</s><s xml:id="_mJrQ3yn">For each participant, we randomly select 40 of the identified unknown words and divide them into eight sets, each with five target words.</s><s xml:id="_XTSs8Qd">After the pretest, we also have participants read the instructions of the learning tasks and the four interfaces.</s><s xml:id="_edDT8Xp">We inform them not to learn the words that appear in the pretest prior to learning sessions.</s></p><p xml:id="_eFvfrsv"><s xml:id="_Yb34j9y">Then, on the experiment day, participants log in to their learning sessions via their unique IDs.</s><s xml:id="_8mwWHQD">Participants are asked to learn two word sets with each Storyfier interface.</s><s xml:id="_tV8GwNK">We counterbalance the order of the four interfaces using Latine Square.</s><s xml:id="_fmQmjSA">After learning two word sets with an interface, participants rate their engagement and enjoyment in the learning process, perceived learning performance, and perceptions of the system in a questionnaire.</s><s xml:id="_fyxVhz5">Upon completion of four tasks, we further ask for their preferences on the interfaces, comments on the generated stories and AI's writing assistance, and suggestions for improving Storyfier.</s></p><p xml:id="_EmMRVwu"><s xml:id="_8xtw5ew">Next, two days after the experiment day, we ask the participants to take a posttest, which has a similar format as the pretest but only presents the 40 words they met in the learning sessions.</s><s xml:id="_tCxvyqF">In the posttest, participants also need to write a sentence for each target word if they do not choose "I do not know this word".</s><s xml:id="_fQpeJju">They can write "nothing" if they feel hard to write the sentence.</s><s xml:id="_STqmaDU">Each participant spends about 1.5 hours in total on the full procedure and gets around USD $12 as compensation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_e4RsqJ3">Measurements</head><p xml:id="_RpbKCg4"><s xml:id="_wyskRzK">RQ1. Learning Outcome.</s><s xml:id="_y8QzGxc">We measure participants' retention of target words' meanings via the number of correct answers to the multiple-choice questions in the posttest.</s><s xml:id="_xRkA5gp">To capture how well they learn the usage of target words, we invite one English teacher (E1 in our workshop) to rate the grammar correctness (e.g., tense and part of speech) and context appropriateness of the target word in each written sentence in the posttest using a three-point scale; 0not correct, 1 -partially correct, 2 -correct.</s><s xml:id="_5bvuvMB">For each participant in each system interface, we calculate i) the numbers (range: 0 -10) of sentences that use target words correctly in terms of both grammar and context and ii) the total score (0 -40) of sentences <ref type="foot" target="#foot_6">9</ref> .</s></p><p xml:id="_4RtseDa"><s xml:id="_dR9MMJt">RQ2. Experience.</s><s xml:id="_Bz3azD4">We measure users' engagement and enjoyment during the learning process with each system interface ("I was absorbed in using this interface to learn vocabulary" and "It is enjoyable to learn vocabulary with this interface" <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b84">85]</ref>).</s><s xml:id="_AgueEUc">Besides, we measure the perceived task workload of learning sessions using items adapted from NASA Task Load Index <ref type="bibr" target="#b27">[28]</ref> (e.g., "I have to work hard to accomplish the writing activity.</s><s xml:id="_hyhjje7">").</s><s xml:id="_ZqDhvSD">Apart from the questionnaire data, we also log the i) task completion time of learning two word sets with each interface, as well as ii) the amount of time spent in reading, cloze-test, and writing activities and iii) written stories in Storyfier-sen and Storyfier-AI interfaces.</s></p><p xml:id="_sKXC9uq"><s xml:id="_3CdnYT9">RQ3. Perceptions towards Storyfier.</s><s xml:id="_CAjMFk8">We adapt the technology acceptance model <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b80">81]</ref> to the perceived usefulness (four items, e.g., "The use of this interface enables me to learn the vocabulary more efficiently"; Cronbach's  = 0.944), easiness to use (four items, e.g., "I would find this interface to be flexible to use";  = 0.786), and intention to use (two items, e.g., "If this interface is available there to help me learn vocabulary, I would use it";  = 0.966) of each system.</s><s xml:id="_Tz2VYmN">We average the ratings of multiple questions as the final score for each aspect.</s><s xml:id="_AnT5zfM">All statements in the questionnaire are rated on a standard 7-point Likert Scale, with 7 for a strong agreement.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_Pkdcwuh">ANALYSES AND RESULTS</head><p xml:id="_BEkcRnG"><s xml:id="_FXcx2NY">For the rated items, we first check whether the order of the four experienced interfaces affects our results via a set of mixed ANOVA tests (order as between-subjects, interfaces as within-subjects) on each rating.</s><s xml:id="_W6HJWPh">Neither the main effect of the order nor its interaction effect with the system interface is significant.</s><s xml:id="_RE9UZNq">Hence, except those with additional notations (e.g., one-way ANOVA), the statistic tests in this section are two-way repeated measured ANOVAs.</s><s xml:id="_JuZ6ENB">For each ANOVA, the assumption of equal variance holds according to Macuchly's test of sphericity <ref type="bibr" target="#b20">[21]</ref>.</s><s xml:id="_rgtF8BV">For the participants' comments on Storyfier, two authors conduct an inductive thematic analysis <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_45szrjv">They first independently assign codes to the text data and then discuss the codes for several rounds.</s><s xml:id="_y7Q3S96">After that, they group the codes into categories, which are incorporated into the results below.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" xml:id="_uQHvmMg">RQ1: Impact on Learning Outcome</head><p xml:id="_pgWsZB2"><s xml:id="_JYpwme3">Figure <ref type="figure" target="#fig_6">5</ref> shows the results regarding learning outcomes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1" xml:id="_gEgyjE6">Retention of target words' meanings.</head><p xml:id="_SDQbX6B"><s xml:id="_Bsg6scp">Our results indicate that neither the AI factor nor its interaction with the activity factor significantly affects the retention of target words in four conditions.</s><s xml:id="_C6tbYEP">However, the Storyfier-sen interface results in a better retention performance ( = 7.00,  = 2.16) than the Storyfier-AI interface ( = 6.07,  = 2.09);  = 0.049, one-way repeated-measures ANOVA.</s><s xml:id="_8TGPeRW">Besides, participants perform significantly better in target words' retention in the read-cloze-test (i.e., Storyfier-sen and Storyfier-AI) conditions ( = 6.54,  = 2.19) than that in the readonly (i.e., Read-sen and Read-AI) conditions ( = 5.46,  = 2.16);  = 9.605,  = 0.004.</s><s xml:id="_U3pk7s3">6.1.2</s><s xml:id="_RYXDDX9">Target words' usage in the sentences.</s><s xml:id="_UVrYV34">Neither the AI factor nor its interaction with the activity factor has a significant impact on i) the number of sentences that correctly use target words and ii) the total scores of written sentences in four conditions.</s><s xml:id="_9y2RUYg">However, when comparing the means between the Storyfier-sen and Storyfier-AI interfaces, we observe that in the read-cloze-write learning sessions, Storyfier's AI features could reduce learning gains on target words' usage.</s><s xml:id="_a9fwXTf">As for the activity factor, our results show that participants with the read-cloze-write interfaces ( = 24.20,</s><s xml:id="_r9nacMt"> = 8.66) perform significantly better in word usage in their written sentences than the cases with the read-only interfaces ( = 20.02,</s><s xml:id="_jFGGW7C"> = 7.92); e.g., for ii) total scores,  = 12.721,  = 0.001.</s></p><p xml:id="_HehaZVA"><s xml:id="_gGbfhWv">In all, we find that Storyfier's AI features reduce learning gains on the retention of target words' meanings in the read-cloze-write vocabulary learning sessions.</s><s xml:id="_CrpNMVE">Its supported additional cloze-test and writing practices improve learning gains on target words' meanings and usage compared to learning via reading-only activities.</s></p><p xml:id="_6xXkJhX"><s xml:id="_Js9QYGb">6.2 RQ2: Impact on Learning Experience 6.2.1 Engagement, enjoyment, and workload.</s><s xml:id="_QaxPnkU">As shown in Figure <ref type="figure" target="#fig_8">6</ref> <ref type="foot" target="#foot_7">foot_7</ref> , neither the AI factor nor its interaction with activity factor significantly affects users' perceptions on their learning experience.</s><s xml:id="_5BF5epP">When digging into each measured item in each condition, we have several interesting observations: a) in read-only sessions, participants with AI-generated stories could feel more engaged and enjoyed than the cases without these stories; b) Storyfier's AI features could increase mental demand and perceived performance in read-only learning sessions but decrease the ratings on these measures in read-cloze-write sessions; c) Storyfier's AI features could reduce temporal demand, i.e., how rushed is the pace of the task, and perceived spent effort in the vocabulary learning tasks.</s><s xml:id="_gpcF9H5">As for the activity factor, we found significant differences regarding the perceived mental demand ( = 0.002), physical demand ( = 0.029), and perceived performance ( = 0.025).</s><s xml:id="_56QanUy">Specifically, Storyfier's supported additional cloze-test and writing practices increase mental and physical demand and perceived performance compared to learning via reading-only activities.</s><s xml:id="_w76kumn">We also observe that these additional activities could increase engagement and spent effort in the vocabulary learning tasks.</s></p><p xml:id="_mFfRHmF"><s xml:id="_dhXEKW2">6.2.2 Task completion time and written stories.</s><s xml:id="_hHMbJgr">i) On average, participants spent 89.22( = 105.25)</s><s xml:id="_AXunmCE">/ 226.52(150.66)</s><s xml:id="_6n98VUs">/ 805.00(490.55)</s><s xml:id="_tHgjE5z">/ 806.</s><s xml:id="_4aaauXB"><ref type="bibr">61(368.19)</ref></s><s xml:id="_P7chSYW">seconds in the learning session with Read-sen, Read-AI, Storyfier-sen, or Storyfier-AI interface.</s><s xml:id="_WCGNnXD">This indicates that in the read-only vocabulary learning sessions, participants spent significantly more time when they were presented with AI-generated stories than when they were not ( &lt; 0.001).</s><s xml:id="_mpjQww2">However, as shown in Figure <ref type="figure" target="#fig_6">5</ref>, the learning gains do not increase accordingly.</s><s xml:id="_BbBvQdq">ii) When digging into the average amount of time spent in each learning activity for each word set in Storyfier-sen and -AI interfaces, we have 49.48 vs. 81.56</s><s xml:id="_zJTjbJ5">(read,  = 0.004), 30.76 vs. 43.84</s><s xml:id="_944xXSn">(cloze,  = 0.004), and 263.46 vs. 208.71</s><s xml:id="_9rwqZxN">(write,  = 0.09) seconds <ref type="foot" target="#foot_8">11</ref> .</s><s xml:id="_wh2xdRW">This shows that compared to the sessions with Storyfier-sen, participants with Storyfier-AI spent significantly more time in reading and cloze-test activities but less time in writing activities.</s><s xml:id="_pmMgx7a">This implies that in read-only learning sessions, participants would find Storyfier more useful and have a higher intention to use it if it provides AI-generated stories.</s><s xml:id="_uNfF2wV">As for the activity factor, participants feel that Storyfier is significantly more useful ( = 0.035) if it supports cloze test and writing practices in addition to the reading activities.</s><s xml:id="_3aMPz8C">There are no significant differences in the perceived easiness to use across the four interfaces.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" xml:id="_AuzH4bU">RQ3: Perceptions on Storyfier</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2" xml:id="_PZ3DtJt">Qualitative responses. Preference.</head><p xml:id="_ynjURfF"><s xml:id="_jEQg6Ha">In the open-response questions after four learning sessions, fifteen participants indicate their preferences for the Story-AI interface for vocabulary learning.</s><s xml:id="_jQ95pGD">They especially favor adaptive writing assistance ( = 9), generative stories <ref type="bibr" target="#b4">(5)</ref>, and useful practices <ref type="bibr" target="#b3">(4)</ref>.</s><s xml:id="_p25XwnD">"It (Storyfier-AI) not only provides the meaning, pronunciation, and example sentences of words, but more importantly, it has AI-generated short stories that can help me better understand the meanings of words and how to use them.</s><s xml:id="_8smx4wB">In addition, the following cloze test and story writing practice can further consolidate my understanding.</s><s xml:id="_BFuYZ49">When I do not know how to write, AI will also provide prompts to help me find my weak points and mistakes so that I can pay more attention on them later on" (P22).</s></p><p xml:id="_ymG7qHM"><s xml:id="_mFYcsrT">Six participants prefer the Story-sen interface, and three of them credit the writing practice without AI assistance.</s><s xml:id="_3SJtSCX">"I prefer Storyfiersen as I need to rely on myself to think and write down the story, which would be more impressive for vocabulary learning" (P8).</s><s xml:id="_AaSUzDG">Five participants prefer the Read-AI interface for its low task workload ( = 3), meaningful contexts for learning ( = 3), and enjoying the experience (2), while the rest two participants favor the Read-sen one as they are more used to the rote learning practices.</s></p><p xml:id="_bkbSzMA"><s xml:id="_7SqjGkP">Generative stories.</s><s xml:id="_m7uNS2A">Regarding participants' comments on the generative stories, we found positive opinions that they are coherent <ref type="bibr" target="#b7">(8)</ref> and novel/interesting/impressive <ref type="bibr" target="#b8">(9)</ref>.</s><s xml:id="_TngR6T2">P26 gives us an example.</s><s xml:id="_H2mqpjD">"At first, I could not remember the word 'veil'.</s><s xml:id="_MYUzhka">Then, I checked the generated story, which tells that a veil blocked my vision when I was driving in traffic.</s><s xml:id="_gXjms27">This story is close to real life, and I felt terrified.</s><s xml:id="_dwZzDgv">It is impressive.</s><s xml:id="_6mNqeGW">I remembered the word 'veil' now.</s><s xml:id="_6sYH4A4">" Nevertheless, there are six comments suggesting that the stories were not coherent, which may be due to the lack of semantic connections among the target words.</s><s xml:id="_x8MMP3M">"When the five target words, e.g., 'hasten, infinity, jet, basin, and trolley' are not naturally relevant to each other, it would be hard to have a reasonable story that covers them, making it hard for memorizing the words in a batch" (P27).</s><s xml:id="_Wpepmne">Besides, three users comment that some target words in the generated stories have different meanings from the dictionary ones, and another three users mention that the stories contain some words that are unknown, which disturbs story comprehension.</s></p><p xml:id="_Ec3baxz"><s xml:id="_FD38TTK">Cloze test.</s><s xml:id="_SUBEh9d">Twenty users indicate their preferences on using generative stories for the cloze test, which can "make it easy to connect the words in context" ( = 10), "enhance memory of target words" (7), and "train reading comprehension skills" (2).</s><s xml:id="_J9jU8su">The other eight participants, however, prefer the existing sentences provided by Storyfier-sen for cloze test materials, with four comments mentioning that "separate sentences are easier to understand".</s></p><p xml:id="_tcAu8kV"><s xml:id="_WpMnmAf">Writing practice.</s><s xml:id="_z4PuUC3">There are seventeen positive responses on the adaptive writing assistance from the generative AI models, suggesting that it can encourage writing <ref type="bibr" target="#b7">(8)</ref>, reduce writing workload <ref type="bibr" target="#b4">(5)</ref>, and provide example usage of target words for reference <ref type="bibr" target="#b3">(4)</ref>.</s><s xml:id="_KXjqWRU">"I didn't feel confused when writing with Storyfier-AI.</s><s xml:id="_SdV4UJd">I can write a sentence first and then let the AI write the next one, and so on.</s><s xml:id="_nwjAagF">It's like having a buddy to memorize words together, which is more interesting and not boring" (P2).</s><s xml:id="_qUDc4sx">"The AI's prompts inspire my writing exercises" (P23).</s><s xml:id="_82t8gTf">However, these prompts in the turn-taking writing process may not match the learners' idea flow (2) and language styles (1) and cause their reliance on AI for using target words <ref type="bibr" target="#b0">(1)</ref>.</s></p><p xml:id="_AdGQAgP"><s xml:id="_vj4neqd">In all, these qualitative responses reveal that participants generally favor Storyfier with AI generative models in Read-AI and Storyfier-AI learning sessions compared to the Read-sen and Storyfiersen interfaces.</s><s xml:id="_JStASx3">However, these models still need to be improved and customized regarding the coherence, complexity, and style of the generative content.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_B6zGrbG">DISCUSSION</head><p xml:id="_GMJfxgh"><s xml:id="_USmJXBj">7.1 Insights from our findings 7.1.1</s><s xml:id="_uR8yYRJ">Text generation models for vocabulary learning support.</s><s xml:id="_sJN3GfV">In Phase 1, we develop a story generation model and verify that it can generate comparably good stories with the human-written ones given target word sets and titles.</s><s xml:id="_zzvnA3P">We receive divided opinions on the generative stories regarding their coherence and interestingness in the experiment with 28 ESL learners.</s><s xml:id="_DzHx88D">The main reason could be that if the target five words are not naturally relevant to each other, it would be difficult for the generative model to connect them to form a meaningful story.</s><s xml:id="_G4aBaNk">Besides, we get feedback from English teachers in Phase 1 that our generative stories have generally acceptable complexity for vocabulary learners.</s><s xml:id="_EBkwNEa">Yet, there are still cases that our stories contain unknown words in addition to the target ones.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2" xml:id="_spAsKQ3">Vocabulary learning activities.</head><p xml:id="_3mS2uYa"><s xml:id="_v7s4Xpu">In Phase 1, we propose that our generative models can empower the cloze test and writing practices in addition to the traditional reading activities that previous vocabulary learning tools support.</s><s xml:id="_mHYmBvR">Our evaluation study with ESL learners reveals that they have significantly more learning gains in read-cloze-write (i.e., Storyfier-sen and Storyfier-AI) learning sessions compared to that in read-only (i.e., Read-sen and Read-AI) sessions.</s><s xml:id="_PWF53Ne">This is non-surprising as learners spent more effort in understanding target words and practicing their usage in the readcloze-write sessions.</s><s xml:id="_359VFcy">Participants' responses in the open-ended questions suggest that generative models can facilitate vocabulary learners by providing meaningful reading and cloze-test materials and adaptive prompts in writing practices.</s></p><p xml:id="_jC2Vmm7"><s xml:id="_7GnwzR5">7.1.3</s><s xml:id="_R6ceAee">Impact of generative models on vocabulary learning.</s><s xml:id="_WffgApg">In the read-only sessions, we observe that the additional AI-generated stories improve learning engagement but do not improve learning gains.</s><s xml:id="_kkJpC2J">This does not support the Gu et al. 's implication that learning vocabulary in a batch under a coherent context could facilitate recalls of target words <ref type="bibr" target="#b24">[25]</ref>.</s><s xml:id="_TE92rkd">One key reason could be the low semantic relevance among some target words in our experiment.</s><s xml:id="_AWBsDk4">Gu et al. 's implication could still be valid if the selected target words are topically relevant to each other, e.g., as organized in a typical language course book.</s><s xml:id="_ED6cu4c">Generative stories can explicitly reveal the words' relationship.</s></p><p xml:id="_vwFYwuT"><s xml:id="_QGzWzuX">In the read-cloze-write sessions, we found that the AI support leads to reduced vocabulary learning gains.</s><s xml:id="_TExjkwE">We attribute these results to the amount of effort spent in the writing practices.</s><s xml:id="_eAyAPHS">While participants mostly favor the assistance from our generative model, they spent less time in writing and wrote significantly fewer words in the story.</s><s xml:id="_NuJDV6N">This provides a lesson that the AI's assistance should encourage necessary effort in vocabulary learning instead of aiming to reduce learners' workload.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2" xml:id="_YMnacWm">Design Considerations</head><p xml:id="_Hghvhn9"><s xml:id="_4gUVfAr">Based on our findings, we outline three directions for supporting vocabulary learning with generative models.</s></p><p xml:id="_DBC6djj"><s xml:id="_nz5cDry">Support four strands of vocabulary learning activities with generative models.</s><s xml:id="_ryKuTZf">Our results (subsection 6.1) with 28 ESL learners show that Storyfier improves learning gains compared to the read-only baselines.</s><s xml:id="_3mCrBNS">This improvement can be due to the Storyfier's cloze test and writing activities that integrate the recommended four strands of learning activities <ref type="bibr" target="#b46">[47]</ref> (subsection 2.1).</s><s xml:id="_69nePKG">We thus recommend that vocabulary learning tools should integrate multiple strands of activities.</s><s xml:id="_aneje6k">While prior language learning systems, e.g., Smart Titles <ref type="bibr" target="#b35">[36]</ref> and EnglishBot <ref type="bibr" target="#b65">[66]</ref>, have explored vocabulary learning activities like watching videos and speaking to others, they largely leveraged existing learning materials.</s><s xml:id="_DuVj3H7">We suggest that generative techniques, such as the story generation model we developed, the text-to-image <ref type="bibr" target="#b86">[87]</ref>, the text-to-video <ref type="bibr" target="#b41">[42]</ref>, and the music generation <ref type="bibr" target="#b28">[29]</ref> approaches, can enrich the learning materials and offer in-situ assistance in these vocabulary learning activities.</s><s xml:id="_fy2uY4m">For example, the learning support tool can generate an image based on the example sentence of each target word to help them understand the word's meaning.</s><s xml:id="_tNPTtn3">It can further offer generated music clips that use this sentence as listening resources.</s></p><p xml:id="_hCEv5EC"><s xml:id="_exDFQKT">Provide adaptive learning feedback.</s><s xml:id="_WUMK5e6">Storyfier offers feedback on the correctness of cloze test results and grammar of written sentences.</s><s xml:id="_C3Jt354">However, two participants comment that it could offer more adaptive learning feedback.</s><s xml:id="_DFPffce">For example, it can "first let the learner draft a story and then assess its quality and usage of target words" (P27).</s><s xml:id="_PqvgbyX">It can further "recommend how to improve the written story" (P25).</s><s xml:id="_ZHyEuyu">Previous skills learning tools, such as ArgueTutor <ref type="bibr" target="#b79">[80]</ref>, Persua <ref type="bibr" target="#b85">[86]</ref> and VoiceCoach <ref type="bibr" target="#b82">[83]</ref>, and writing support tools like MepsBot <ref type="bibr" target="#b55">[56]</ref> also adopt a similar feedback flow, which can mitigate disturbance on the practicing process.</s><s xml:id="_yWCPSFe">Generative models can offer such learning feedback by generating polished versions of the written story for reference.</s><s xml:id="_KyEP4sk">However, based on the teachers' suggestions on our first Storyfier prototype (Figure <ref type="figure" target="#fig_6">3D-5</ref>), the provided feedback should emphasize the vocabulary learning goal; otherwise, learners may chase for other objectives, e.g., trigram repetition and sentence coherence.</s></p><p xml:id="_4thNGAQ"><s xml:id="_NCqwacb">Balance machine and human effort in learning tasks.</s><s xml:id="_yrGUQ4f">Our results show that Storyfier's AI features reduce learning gains on the retention of target words' meanings in the read-cloze-write sessions (subsection 6.1).</s><s xml:id="_ArC95Y4">Participants report the need for increased workload and autonomy in the writing task for effective vocabulary learning (subsubsection 6.3.2).</s><s xml:id="_EnH7SNc">As such, we recommend that Storyfier should further motivate necessary user effort in learning.</s><s xml:id="_cuJJgUH">In the refinement of Storyfier (subsection 4.3), we have experienced a few features to encourage more user effort, e.g., users need to write the first sentence of the story.</s><s xml:id="_XDESpQJ">Future work could explore the inclusion of gamification features like badges, timers, and leader boards for promoting learners' efforts in educational scenarios <ref type="bibr" target="#b11">[12]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3" xml:id="_2m6e4cu">Generality of Storyfier</head><p xml:id="_QdvqR4f"><s xml:id="_vmG7gAy">While Storyfier presets the target words that participants are unknown in the experiment, we have equipped it with features like adding or deleting any words, suggesting words semantically related to the title, filtering words based on difficulty level, and editing the generated stories.</s><s xml:id="_fMVHacC">In other words, Storyfier can support customized individual vocabulary learning beyond the controlled lab sessions.</s><s xml:id="_wXFJSM7">Besides, our English teachers in the design workshop express their interest in applying Storyfier in language teaching.</s><s xml:id="_pnEqY7y">One teacher, E2, had a trial on her offline course by inputting five words she just taught and asking students to have a cloze test on the generated story.</s><s xml:id="_HZPknjC">Therefore, our Storyfier is promising for supporting customized vocabulary learning and teaching in the wild.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4" xml:id="_5jsJMr2">Limitations and Future Work</head><p xml:id="_WWBmT6c"><s xml:id="_WwpxYkz">Handle language ambiguity.</s><s xml:id="_bqgUWAM">Currently, our system does not consider language ambiguity when generating stories for target vocabulary.</s><s xml:id="_s6gkcNc">For example, one word can carry multiple meanings (i.e., polysemy) in different contexts, while our system only considers its most common meaning in practical usage.</s><s xml:id="_645tvhM">Comparative studies of the same word in different contexts can help disambiguate words and deepen the understanding of vocabulary.</s></p><p xml:id="_VTp83hw"><s xml:id="_mKujsQU">Improve story generation quality.</s><s xml:id="_rVFkUvm">To further improve the quality of story generation for vocabulary learning, we can consider two aspects: dataset and model.</s><s xml:id="_6RqGh9T">The simplicity of the ROCStory dataset, while appreciated by English teachers for vocabulary learning, has certain limitations.</s><s xml:id="_3SNVAgz">It lacks transition words, which makes it challenging for models to learn sentence transition logic.</s><s xml:id="_Bw2FKfe">Additionally, the simplicity of the story structures can potentially compromise the richness of intra-sentence contexts.</s><s xml:id="_Vkr9dtJ">In the future, we can incorporate more complex stories with a wide range of narrative structures into our dataset for model training.</s></p><p xml:id="_xUvVMed"><s xml:id="_8PcQTkq">Besides, we can investigate the use of more powerful language models (e.g., ChatGPT) to enhance the quality of the generated stories.</s><s xml:id="_tT2GA5y">For example, we have experimented with prompts (e.g., with "simple", "CET-4", "within 50 words", and/or "no more complex words") to steer ChatGPT towards generating stories that fulfill our specifications <ref type="foot" target="#foot_9">12</ref> .</s><s xml:id="_2f6j9Wd">Notably, however, there is a trade-off between the simplicity and coherence of the generated stories.</s><s xml:id="_aQqTfrx">Future research can focus on refining prompting strategies to optimize the balance between these two elements for enhancing the efficacy of vocabulary learning.</s></p><p xml:id="_tjttjxu"><s xml:id="_9zVdUUb">Generate diverse and harmless stories.</s><s xml:id="_w32HxuF">Storyfier presents one story at a time based on the generation models trained on ROCStory dataset, which contains simple and short stories.</s><s xml:id="_x6BJjf9">In the future, we can consider generating more diverse stories (e.g., in the form of newspapers, novels, poems, and humor) that have varied styles and lengths for vocabulary learning.</s><s xml:id="_TzpMMVq">Besides, while our participants did not report harmful content in the generated stories, Storyfier should include features like "report" and automatic detection of unwanted content to offer a healthy learning environment.</s></p><p xml:id="_YSCTxEZ"><s xml:id="_2C5nkV3">Identify helpful characteristics of stories for vocabulary learning.</s><s xml:id="_b3rC8Bf">We evaluate the quality of our generated stories via coherence, relevance, and interestingness (Table <ref type="table">3</ref>) and collect qualitative feedback from participants on the stories' helpfulness.</s><s xml:id="_RSaU7w5">We call for future work to complement our evaluation studies by identifying what are the helpful characteristics of stories for vocabulary learning.</s><s xml:id="_bEDqbHh">For example, we can talk to English-learning textbook authors or conduct a content analysis on textbook stories.</s><s xml:id="_YVnZRDK">The identified characteristics can further guide us to customize story generation models and enhance our evaluation metrics of the stories.</s></p><p xml:id="_PwEGjUY"><s xml:id="_cPKYWFn">Invite diverse language learners.</s><s xml:id="_dJXP7AU">We conduct the experiment with Chinese students in an English learning course to evaluate Storyfier.</s><s xml:id="_pq3QdBk">Their CET-4 test scores and self-reported proficiency indicate that they are intermediate-level English learners.</s><s xml:id="_MDfcmH4">Further studies can explore whether and how Storyfier with generative models can support novices with no or little prior experience in learning English.</s><s xml:id="_Ppdr4ze">Moreover, we can extend Storyfier to support users from different cultures to learn their foreign languages (e.g., English learners study Chinese).</s></p><p xml:id="_VeJF2pE"><s xml:id="_qPbP9JD">Evaluate cloze test and writing practices separately.</s><s xml:id="_VN58HFZ">In the experiment, we evaluate the impact of the cloze test and writing practices by comparing the participants' performance and experience in read-only and read-cloze-write sessions.</s><s xml:id="_fufpvHT">This study design is to identify the value of the vocabulary learning activities beyond the meaning-focused input activities that previous systems support.</s><s xml:id="_tcTbtfQ">However, we can not quantitatively tell how much the cloze test or writing practice contributes to the impact, which requires a future study that separately evaluates these two activities.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" xml:id="_FhfmEMw">CONCLUSION</head><p xml:id="_bfe2WVD"><s xml:id="_prCu5t2">In this paper, we designed and developed an interactive system, Storyfier, to support reading, cloze test, and writing activities for vocabulary learning.</s><s xml:id="_gRPT8N6">We power the system with controllable language models that can generate stories given any target words and provide adaptive assistance when using these words in the writing practices.</s><s xml:id="_CWuRgjr">We explore its supported vocabulary learning activities and interface design with teachers, learners, and Human-Computer Interaction researchers.</s><s xml:id="_r4MNqX5">Our two-by-two within-subjects experiment with 28 English-as-Second-Language Chinese students shows that participants generally favor the generated stories and writing assistance.</s><s xml:id="_HdeFH34">However, their learning gains with Storyfier in the read-cloze-test sessions decrease compared to the cases they are with a baseline system without generative models.</s><s xml:id="_TTXMDN5">We discuss insights from our findings for leveraging generative models to support learning tasks.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc><div><p xml:id="_4wxNgwW"><s xml:id="_ZnhbMFN">Figure 1: Our two-phases design and development process of Storyfier with teachers, learners, and HCI researchers.</s></p></div></figDesc><graphic coords="4,60.43,83.68,226.98,175.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc><div><p xml:id="_gnsbSrp"><s xml:id="_AJ4J75Q">Gets A Cramp &lt;SEP&gt; cet-4 words: sharp pain cramp grasp stomach roll Mask He felt a sharp pain in his stomach [n-grams]: He felt [BLANK] in his stomach [sentence]: [BLANK]</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc><div><p xml:id="_2AFh6nb"><s xml:id="_TFYS3yW">Figure2: The technical framework of Storyfier.</s><s xml:id="_BP4tVUD">We mainly adopt prompt-based fine-tuning strategies to build story generation models.</s><s xml:id="_UzdWHgb">(A) We derive CET-4 words from the stories in ROCStory dataset.</s><s xml:id="_2hNhkGm">(B) We finetune T5 language models to 1) generate a story given a CET-4 word set with or without a title (presented in section 3.2) and 2) infill a sentence or n-grams given preceding and following sentences, unused target words, and story title (if any) (subsubsection 4.2.1).</s><s xml:id="_QVxTYfp">(C) We apply the models to support three kinds of story-based learning activities.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 1 . 2 Figure 3 :</head><label>123</label><figDesc><div><p xml:id="_hpm4erc"><s xml:id="_aMquPS2">Figure 3: The interface designs of Storyfier.</s><s xml:id="_mPA6JWP">(A) Users can specify target words and check their meanings.</s><s xml:id="_4VRK4pT">(B) Story reading: users can read a machined-generated story that contains target words.</s><s xml:id="_vAfvrEq">(C) Cloze test: users can conduct a cloze test by using target words on the generated story.</s><s xml:id="_swQQJAP">(D) Story writing: users can take turns with Storyfier to write a new story using target words.</s><s xml:id="_mgkdREe">(E) The interface for story writing without adaptive support in the Storyfier-sen baseline (section 5).</s><s xml:id="_mDN2A23">Note that in the first Storyfier prototype, the three modes (B-D) are separated.</s><s xml:id="_NPEev3f">In the refined Storyfier, we unify them into one flow and improve the system designs (1-5) based on feedback from learners and experts.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc><div><p xml:id="_AXWyYNf"><s xml:id="_Px8Jg46">Figure 3A); • Read-AI interface additionally provides a generated story that covers target words (Figure 3A + B); Read-cloze-write • Storyfier-sen interface offers example sentences for target words, a cloze test on these sentences, and a writing exercise without AI's intervention (Figure 3A + B + D, but the stories in B and C are replaced by the example sentences of target words); • Storyfier-AI interface contains all features of Storyfier (Figure 3A + B + C).The Read-sen and Storyfier-sen interfaces simulate how individuals traditionally use existing materials to learn any target word set</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc><div><p xml:id="_Ur4Sxsv"><s xml:id="_89TJgKK">Figure 4: Procedure of the experiment.</s><s xml:id="_ygVkKxd">(A) Participants first took a pretest, and the words they did not know were target words.</s><s xml:id="_KNNVPMw">(B) On the experiment day, they used the four interfaces of Storyfier for vocabulary learning.</s><s xml:id="_MkW6eP9">(C) Two days later, they took the posttest on words' meanings and usage.</s></p></div></figDesc><graphic coords="9,60.43,83.69,226.97,99.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc><div><p xml:id="_CbsAHBf"><s xml:id="_xMBQTym">Figure 5: RQ1 results regarding numbers of correct choices on target words' meanings, numbers of sentences that correctly use target words, and total scores of the written sentences in each condition.</s><s xml:id="_MMsRaeV">***:  &lt; 0.001, **:  &lt; 0.01, *:  &lt; 0.05.</s></p></div></figDesc><graphic coords="10,53.80,83.68,242.11,138.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7</head><label>7</label><figDesc><div><p xml:id="_chBWXmj"><s xml:id="_ckHUHxr">Figure 7 depicts users perceptions of each Storyfier interface.6.3.1 Quantitative items.In read-only sessions, there is a trend on the improved perceived usefulness and intention to use of the interfaces with AI-generative stories over those without the stories.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc><div><p xml:id="_aUY8AAr"><s xml:id="_K5XDJGM">Figure 6: RQ2 results regarding perceived engagement, enjoyment, and workload in vocabulary learning sessions with Read-sen, Read-AI, Storyfier-sen, and Storyfier-AI interfaces.</s><s xml:id="_wwbNHmT">**:  &lt; 0.01, *:  &lt; 0.05, +:  &lt; 0.1.</s></p></div></figDesc><graphic coords="11,53.80,83.69,504.37,182.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc><div><p xml:id="_8vMjXDB"><s xml:id="_zQnpPay">Figure 7: RQ3 results regarding user perceptions with each interface.</s><s xml:id="_Tzk8eQa">*:  &lt; 0.05, +:  &lt; 0.1.</s></p></div></figDesc><graphic coords="11,53.80,314.68,242.11,125.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc><div><p xml:id="_8u8UvvS"><s xml:id="_6nU539u">The statistics of ROCStory dataset.</s></p></div></figDesc><table><row><cell>Attributes</cell><cell>Values</cell></row><row><cell># of stories</cell><cell>101,661</cell></row><row><cell># of words</cell><cell>4,640,319</cell></row><row><cell>Average story length</cell><cell>45.65</cell></row><row><cell>Average sentence length</cell><cell>7.80</cell></row><row><cell>Average readability</cell><cell>57.14</cell></row><row><cell cols="2">Coverage of CET-4 words 89.52%</cell></row><row><cell>tuples (Figure</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc><div><p xml:id="_YfX9xzE"><s xml:id="_hBdfvVa">Automated evaluation of human-written and machine-generated stories using lexical metrics.</s></p></div></figDesc><table><row><cell></cell><cell>Grammar</cell><cell>Type-token ratio</cell><cell>Trigram repetition</cell><cell>Sentence coherence</cell></row><row><cell>Human</cell><cell>1.00</cell><cell>0.75</cell><cell>0.01</cell><cell>0.42</cell></row><row><cell cols="2">Machine 1.00</cell><cell>0.77</cell><cell>0.01</cell><cell>0.43</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p xml:id="_CvCkmDW"><s xml:id="_dXKCHAN">Short for College English Test Band 4, a mandatory test for acquiring bachelor degrees in China.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p xml:id="_vG6dCdz"><s xml:id="_KgyWDWq">Readability is measured by Flesch Reading-Ease, and CET-4 is</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_NCzFaku"><s xml:id="_M9fnpfH">34.23 on average.<ref type="bibr" target="#b2">3</ref></s><s xml:id="_SYsraCV">For the stories without titles, we represent their title features as "no title".</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p xml:id="_w6TTwTV"><s xml:id="_ZPGXaW2">We use sentence-bert<ref type="bibr" target="#b62">[63]</ref> to encode the words into vectors and rank them based on their cosine similarities with the vector of the encoded title.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p xml:id="_w89Gsj3"><s xml:id="_3pMMsfw">https://languagetool.org/http-api/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p xml:id="_UHKQEPE"><s xml:id="_TvX2e2u">425/710 points are considered passed for CET-4.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p xml:id="_9Zx56wZ"><s xml:id="_tEdbrBQ">In each interface, participants learn ten words and write at most ten sentences in posttest.</s><s xml:id="_RrX9ufR">The maximum score for each sentence is 2 + 2 = 4.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p xml:id="_8zZ8zFw"><s xml:id="_TEUzh7c">The full statistics are attached in the supplementary materials.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8"><p xml:id="_5KPTwKA"><s xml:id="_RwszkGm">The total time does not match task completion time as it does not include time spent on checking each word's meaning.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_9"><p xml:id="_RkEGpna"><s xml:id="_neFCBUS">For example, we queried ChatGPT using "write a five-sentences simple story using words: hasten, infinity, jet, basin, and trolley".</s><s xml:id="_Ek5Zxyr">This results in a 71-word coherent story but contains more complex sentence structure and words like "marvel", "exhilarated", and "adventure".</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_g7uB4xZ">ACKNOWLEDGMENTS</head><p xml:id="_zxcMUeT"><s xml:id="_MmZAbfc">This work is supported by the <rs type="funder">Young Scientists Fund of the National Natural Science Foundation of China</rs> with Grant No. <rs type="grantNumber">62202509</rs> and partially supported by the <rs type="funder">Research Grants Council of the Hong Kong Special Administrative Region under General Research Fund (GRF)</rs> with Grant No. <rs type="grantNumber">16203421</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cKEzwhg">
					<idno type="grant-number">62202509</idno>
				</org>
				<org type="funding" xml:id="_hGZ5cwh">
					<idno type="grant-number">16203421</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_AjGspnq">VocabEncounter: NMT-Powered Vocabulary Learning by Presenting Computer-Generated Usages of Foreign Words into Users&apos; Daily Lives</title>
		<author>
			<persName><forename type="first">Riku</forename><surname>Arakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiromu</forename><surname>Yakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3501839</idno>
		<ptr target="https://doi.org/10.1145/3491102.3501839" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_YuNMzZB">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems<address><addrLine>New Orleans, LA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note>) (CHI &apos;22) Article 6</note>
	<note type="raw_reference">Riku Arakawa, Hiromu Yakura, and Sosuke Kobayashi. 2022. VocabEncounter: NMT-Powered Vocabulary Learning by Presenting Computer-Generated Usages of Foreign Words into Users&apos; Daily Lives. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI &apos;22). Association for Computing Machinery, New York, NY, USA, Article 6, 21 pages. https://doi.org/10.1145/3491102.3501839</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_QhBhnhx">Analysis of a mnemonic device: Modern psychology uncovers the powerful components of an ancient system for improving memory</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><surname>Bower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6cZBbva">American Scientist</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="496" to="510" />
			<date type="published" when="1970">1970. 1970</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gordon H Bower. 1970. Analysis of a mnemonic device: Modern psychology uncovers the powerful components of an ancient system for improving memory. American Scientist 58, 5 (1970), 496-510.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_KXgbDtQ">Using thematic analysis in psychology</title>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Clarke</surname></persName>
		</author>
		<idno type="DOI">10.1191/1478088706qp063oa</idno>
		<ptr target="https://doi.org/10.1191/1478088706qp063oa" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_ms76GXu">Qualitative Research in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="101" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative Research in Psychology 3, 2 (2006), 77-101. https://doi.org/10.1191/ 1478088706qp063oa</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_UFeMGxB">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4jqCSfs">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_Zd2M4Gg">The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non-native english writers</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Buschek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zürn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malin</forename><surname>Eiband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ek7PweG">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel Buschek, Martin Zürn, and Malin Eiband. 2021. The impact of multiple parallel phrase suggestions on email input and composition behaviour of native and non-native english writers. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1-13.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_edkgy8w">How Novelists Use Generative Language Models: An Exploratory User Study</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Calderwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivian</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katy</forename><forename type="middle">Ilonka</forename><surname>Gero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><forename type="middle">B</forename><surname>Chilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9CFNMR3">HAI-GEN+ user2agent@ IUI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alex Calderwood, Vivian Qiu, Katy Ilonka Gero, and Lydia B Chilton. 2020. How Novelists Use Generative Language Models: An Exploratory User Study.. In HAI-GEN+ user2agent@ IUI.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_f3rU4Es">Schema theory and ESL reading: Classroom implications and applications</title>
		<author>
			<persName><forename type="first">Patricia</forename><forename type="middle">L</forename><surname>Carrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_u2v8ZNw">The modern language journal</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="332" to="343" />
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Patricia L Carrell. 1984. Schema theory and ESL reading: Classroom implications and applications. The modern language journal 68, 4 (1984), 332-343.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_wzMmDwj">Personalized mobile English vocabulary learning system based on item response theory and learning memory cycle</title>
		<author>
			<persName><forename type="first">Chih-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Ju</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_47jrSwB">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="624" to="645" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chih-Ming Chen and Ching-Ju Chung. 2008. Personalized mobile English vo- cabulary learning system based on item response theory and learning memory cycle. Computers &amp; Education 51, 2 (2008), 624-645.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_qg37kWj">TaleBrush: Sketching Stories with Generative Pretrained Language Models</title>
		<author>
			<persName><forename type="first">John</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wooseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwaran</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsuk</forename><surname>Adar</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_M7y9HUw">CHI Conference on Human Factors in Computing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
	<note type="raw_reference">John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, and Minsuk Chang. 2022. TaleBrush: Sketching Stories with Generative Pretrained Language Models. In CHI Conference on Human Factors in Computing Systems. 1-19.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_canPJFA">Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karim</forename><surname>Benharrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Buschek</surname></persName>
		</author>
		<idno type="DOI">10.1145/3526113.3545672</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wXTGPvG">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 35th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hai Dang, Karim Benharrak, Florian Lehmann, and Daniel Buschek. 2022. Beyond Text Generation: Supporting Writers with Continuous Automatic Text Summaries. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. 1-13.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main" xml:id="_2fT2YqX">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02164</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: A simple approach to controlled text generation. arXiv preprint arXiv:1912.02164 (2019).</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_8Pvz39k">Empirical support for a causal relationship between gamification and learning outcomes</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Denny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Empson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cjxyxyp">Proceedings of the 2018 CHI conference on human factors in computing systems</title>
		<meeting>the 2018 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Denny, Fiona McDonald, Ruth Empson, Philip Kelly, and Andrew Petersen. 2018. Empirical support for a causal relationship between gamification and learning outcomes. In Proceedings of the 2018 CHI conference on human factors in computing systems. 1-13.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_z2XvrQG">Storycoder: Teaching computational thinking concepts through storytelling in a voice-guided app for children</title>
		<author>
			<persName><forename type="first">Griffin</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadin</forename><surname>Tamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyowon</forename><surname>Gweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">L</forename><surname>Murnane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445039</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ENZANDc">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note type="raw_reference">Griffin Dietz, Jimmy K Le, Nadin Tamer, Jenny Han, Hyowon Gweon, Eliza- beth L Murnane, and James A Landay. 2021. Storycoder: Teaching computational thinking concepts through storytelling in a voice-guided app for children. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 1-15.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_g9YBD7g">Visual StoryCoder: A Multimodal Programming Environment for Children&apos;s Creation of Stories</title>
		<author>
			<persName><forename type="first">Griffin</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadin</forename><surname>Tamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carina</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Landay</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3580981</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QDztFg2">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note type="raw_reference">Griffin Dietz, Nadin Tamer, Carina Ly, Jimmy K Le, and James A Landay. 2023. Visual StoryCoder: A Multimodal Programming Environment for Children&apos;s Creation of Stories. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 1-16.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main" xml:id="_mVTZznf">Enabling Language Models to Fill in the Blanks</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mina</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>ArXiv abs/2005.05339</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling Language Models to Fill in the Blanks. ArXiv abs/2005.05339 (2020).</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Rita</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dunn</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">J</forename><surname>Dunn</surname></persName>
		</author>
		<title level="m" xml:id="_6wN3Mfw">Practical approaches to individualizing instructions: contracts and other effective teaching strategies</title>
		<imprint>
			<publisher>Parker Publishing</publisher>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rita Stafford Dunn and Kenneth J Dunn. 1972. Practical approaches to indi- vidualizing instructions: contracts and other effective teaching strategies. Parker Publishing.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_jjDaRMg">Hierarchical neural story generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p18-1082</idno>
		<idno type="arXiv">arXiv:1805.04833</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833 (2018).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_kJ35MBv">Comparing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication</title>
		<author>
			<persName><forename type="first">Liye</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Jakesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Kreps</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3581351</idno>
		<ptr target="https://doi.org/10.1145/3544548.3581351" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_gKK4J2k">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems<address><addrLine>Hamburg, Germany; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Liye Fu, Benjamin Newman, Maurice Jakesch, and Sarah Kreps. 2023. Compar- ing Sentence-Level Suggestions to Message-Level Suggestions in AI-Mediated Communication. In Proceedings of the 2023 CHI Conference on Human Fac- tors in Computing Systems (Hamburg, Germany) (CHI &apos;23). Association for Computing Machinery, New York, NY, USA, Article 103, 13 pages. https: //doi.org/10.1145/3544548.3581351</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_ATxnTea">Story plot generation based on CBR</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gervás</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belén</forename><surname>Díaz-Agudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Peinado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Hervás</surname></persName>
		</author>
		<idno type="DOI">10.1007/1-84628-103-2_3</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QmKeEa3">International Conference on Innovative Techniques and Applications of Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="46" />
		</imprint>
	</monogr>
	<note type="raw_reference">Pablo Gervás, Belén Díaz-Agudo, Federico Peinado, and Raquel Hervás. 2004. Story plot generation based on CBR. In International Conference on Innovative Techniques and Applications of Artificial Intelligence. Springer, 33-46.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_SZzE9nU">Hafez: an interactive poetry generation system</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Priyadarshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p17-4008</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mXSTJCh">Proceedings of ACL 2017, System Demonstrations</title>
		<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
	<note type="raw_reference">Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. 2017. Hafez: an interactive poetry generation system. In Proceedings of ACL 2017, System Demonstrations. 43-48.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Girden</surname></persName>
		</author>
		<title level="m" xml:id="_3f7PmeW">ANOVA: Repeated measures</title>
		<meeting><address><addrLine>Thousand Oaks, CA, US</addrLine></address></meeting>
		<imprint>
			<publisher>Sage Publications, Inc</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E.R. Girden. 1992. ANOVA: Repeated measures. Sage Publications, Inc., Thousand Oaks, CA, US.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main" xml:id="_xqn2yRe">Content planning for neural story generation with aristotelian rescoring</title>
		<author>
			<persName><forename type="first">Seraphina</forename><surname>Goldfarb-Tarrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.351</idno>
		<idno type="arXiv">arXiv:2009.09870</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, and Nanyun Peng. 2020. Content planning for neural story generation with aristotelian rescoring. arXiv preprint arXiv:2009.09870 (2020).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_JbvHXKe">Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation</title>
		<author>
			<persName><forename type="first">Seraphina</forename><surname>Goldfarb-Tarrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haining</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-4016</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_W4nBk23">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Seraphina Goldfarb-Tarrant, Haining Feng, and Nanyun Peng. 2019. Plan, Write, and Revise: an Interactive System for Open-Domain Story Generation. In NAACL.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_ePRcjkf">LaMPost: Design and Evaluation of an AI-assisted Email Writing Prototype for Adults with Dyslexia</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Steven M Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Clary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiffanie</forename><forename type="middle">N</forename><surname>Donsbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Horne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rain</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajit</forename><surname>Breaw Michaels</surname></persName>
		</author>
		<author>
			<persName><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_u3dkkJ2">Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility</title>
		<meeting>the 24th International ACM SIGACCESS Conference on Computers and Accessibility</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
	<note type="raw_reference">Steven M Goodman, Erin Buehler, Patrick Clary, Andy Coenen, Aaron Donsbach, Tiffanie N Horne, Michal Lahav, Robert MacDonald, Rain Breaw Michaels, Ajit Narayanan, et al. 2022. LaMPost: Design and Evaluation of an AI-assisted Email Writing Prototype for Adults with Dyslexia. In Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility. 1-18.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_CT56mW3">Vocabulary Learning Strategies and Language Learning Outcomes</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-1770.1996.tb01355.x</idno>
		<ptr target="https://doi.org/10.1111/j.1467-1770.1996.tb01355.x" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_MNfAhh3">Language Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="679" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yongqi Gu and Robert Keith Johnson. 1996. Vocabulary Learning Strategies and Language Learning Outcomes. Language Learning 46, 4 (1996), 643-679. https://doi.org/10.1111/j.1467-1770.1996.tb01355.x</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_VmD7eFM">Vocabulary learning strategies and language learning outcomes</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-1770.1996.tb01355.x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YHuzv4j">Language learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="643" to="679" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yongqi Gu and Robert Keith Johnson. 1996. Vocabulary learning strategies and language learning outcomes. Language learning 46, 4 (1996), 643-679.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_rCVgcz9">Toward automated story generation with markov chain monte carlo methods and deep neural networks</title>
		<author>
			<persName><forename type="first">Brent</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Purdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Riedl</surname></persName>
		</author>
		<idno type="DOI">10.1609/aiide.v13i2.13003</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_89RSky2">Thirteenth Artificial Intelligence and Interactive Digital Entertainment Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brent Harrison, Christopher Purdy, and Mark O Riedl. 2017. Toward automated story generation with markov chain monte carlo methods and deep neural net- works. In Thirteenth Artificial Intelligence and Interactive Digital Entertainment Conference.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_Dv5URnb">NASA-task load index (NASA-TLX); 20 years later</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandra</surname></persName>
		</author>
		<author>
			<persName><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RgNpFPj">Proceedings of the human factors and ergonomics society annual meeting</title>
		<meeting>the human factors and ergonomics society annual meeting<address><addrLine>Sage CA; Los Angeles, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Sage</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="904" to="908" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sandra G Hart. 2006. NASA-task load index (NASA-TLX); 20 years later. In Proceedings of the human factors and ergonomics society annual meeting, Vol. 50. Sage publications Sage CA: Los Angeles, CA, 904-908.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_ZgRqEew">A Functional Taxonomy of Music Generation Systems</title>
		<author>
			<persName><forename type="first">Dorien</forename><surname>Herremans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ching-Hua</forename><surname>Chuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elaine</forename><surname>Chew</surname></persName>
		</author>
		<idno type="DOI">10.1145/3108242</idno>
		<ptr target="https://doi.org/10.1145/3108242" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Fe86zSu">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2017-09">2017. sep 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dorien Herremans, Ching-Hua Chuan, and Elaine Chew. 2017. A Functional Taxonomy of Music Generation Systems. ACM Comput. Surv. 50, 5, Article 69 (sep 2017), 30 pages. https://doi.org/10.1145/3108242</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06087</idno>
		<title level="m" xml:id="_XfPSu3j">Learning to write with cooperative discriminators</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. arXiv preprint arXiv:1805.06087 (2018).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_SXVJyzr">Interaction design based on augmented reality technologies for English vocabulary learning</title>
		<author>
			<persName><forename type="first">Min-Chai</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Chiang Koong</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fyE2AQS">Proceedings of the 18th International Conference on Computers in Education</title>
		<meeting>the 18th International Conference on Computers in Education</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="663" to="666" />
		</imprint>
	</monogr>
	<note type="raw_reference">Min-Chai Hsieh and Hao-Chiang Koong Lin. 2006. Interaction design based on augmented reality technologies for English vocabulary learning. In Proceedings of the 18th International Conference on Computers in Education, Vol. 1. 663-666.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_GPb6RKy">A ubiquitous English vocabulary learning system: Evidence of active/passive attitudes vs. usefulness/ease-of-use</title>
		<author>
			<persName><forename type="first">Yueh-Min</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Ming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Hsien</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Ting</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4he3fUK">Computers &amp; Education</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yueh-Min Huang, Yong-Ming Huang, Shu-Hsien Huang, and Yen-Ting Lin. 2012. A ubiquitous English vocabulary learning system: Evidence of active/passive attitudes vs. usefulness/ease-of-use. Computers &amp; Education 58, 1 (2012), 273-282.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_umb5F6C">How can we know what language models know</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8aUa9pr">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computa- tional Linguistics 8 (2020), 423-438.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_DCDAFxg">Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention</title>
		<author>
			<persName><forename type="first">Eunkyung</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunhoon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Ho</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3581503</idno>
		<ptr target="https://doi.org/10.1145/3544548.3581503" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_QykSzRA">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems<address><addrLine>Hamburg, Germany; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Article 18</note>
	<note type="raw_reference">Eunkyung Jo, Daniel A. Epstein, Hyunhoon Jung, and Young-Ho Kim. 2023. Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI &apos;23). Association for Computing Machinery, New York, NY, USA, Article 18, 16 pages. https://doi.org/10.1145/3544548.3581503</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main" xml:id="_yktah9C">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for con- trollable generation. arXiv preprint arXiv:1909.05858 (2019).</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_FK7VTd8">Smart subtitles for vocabulary learning</title>
		<author>
			<persName><forename type="first">Geza</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_d5GSRsR">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="853" to="862" />
		</imprint>
	</monogr>
	<note type="raw_reference">Geza Kovacs and Robert C Miller. 2014. Smart subtitles for vocabulary learning. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 853-862.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main" xml:id="_QBN5TVc">Gedi: Generative discriminator guided sequence generation</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06367</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2020. Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367 (2020).</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_NUEvMkB">Automatic evaluation of text coherence: Models and representations</title>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UwXXs2A">IJCAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1085" to="1090" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mirella Lapata, Regina Barzilay, et al. 2005. Automatic evaluation of text coher- ence: Models and representations. In IJCAI, Vol. 5. Citeseer, 1085-1090.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<title level="m" xml:id="_zgbZP4K">The power of scale for parameter-efficient prompt tuning</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021).</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Monroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06549</idno>
		<title level="m" xml:id="_n38QdjM">Learning to decode for future success</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jiwei Li, Will Monroe, and Dan Jurafsky. 2017. Learning to decode for future success. arXiv preprint arXiv:1701.06549 (2017).</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main" xml:id="_vfngwfE">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00190</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_qffMfWZ">Video generation from text</title>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YQCAJH9">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. 2018. Video generation from text. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m" xml:id="_3q2tFRU">GPT understands, too</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT understands, too. arXiv preprint arXiv:2103.10385 (2021).</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_ckwW2P2">Event representations for automated story generation with deep neural nets</title>
		<author>
			<persName><forename type="first">Lara</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZTxPhTn">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Lara Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock, Shruti Singh, Brent Harrison, and Mark Riedl. 2018. Event representations for automated story generation with deep neural nets. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_vT24x4K">Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kory</forename><forename type="middle">W</forename><surname>Mathewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaylen</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3581225</idno>
		<ptr target="https://doi.org/10.1145/3544548.3581225" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Gh3renM">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems<address><addrLine>Hamburg, Germany; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and Richard Evans. 2023. Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI &apos;23). Association for Computing Machinery, New York, NY, USA, Article 355, 34 pages. https://doi. org/10.1145/3544548.3581225</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_uRV9Rht">A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1098</idno>
		<ptr target="https://doi.org/10.18653/v1/N16-1098" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_SETmCtk">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, San Diego, California, 839-849. https://doi.org/10. 18653/v1/N16-1098</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_zQEbndp">The four strands</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Nation</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hgHXjaD">International Journal of Innovation in Language Learning and Teaching</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Nation. 2007. The four strands. International Journal of Innovation in Language Learning and Teaching 1, 1 (2007), 2-13.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_5Apeg6s">Teaching and testing vocabulary</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Nation</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AWN7ES9">The handbook of language teaching</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Nation and Teresa Chung. 2009. Teaching and testing vocabulary. The handbook of language teaching (2009), 543-559.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_WbBmaTf">Improving artificial teachers by considering how people learn and forget</title>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Nioche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alexandre</forename><surname>Murena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>De La Torre-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Oulasvirta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hraYXqJ">26th International Conference on Intelligent User Interfaces</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="445" to="453" />
		</imprint>
	</monogr>
	<note type="raw_reference">Aurélien Nioche, Pierre-Alexandre Murena, Carlos de la Torre-Ortiz, and Antti Oulasvirta. 2021. Improving artificial teachers by considering how people learn and forget. In 26th International Conference on Intelligent User Interfaces. 445-453.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_tEKdHpy">Improving Artificial Teachers by Considering How People Learn and Forget</title>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Nioche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alexandre</forename><surname>Murena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>De La Torre-Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Oulasvirta</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397481.3450696</idno>
		<ptr target="https://doi.org/10.1145/3397481.3450696" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_P3K67YH">26th International Conference on Intelligent User Interfaces</title>
		<meeting><address><addrLine>College Station, TX, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="445" to="453" />
		</imprint>
	</monogr>
	<note>) (IUI &apos;21)</note>
	<note type="raw_reference">Aurelien Nioche, Pierre-Alexandre Murena, Carlos de la Torre-Ortiz, and Antti Oulasvirta. 2021. Improving Artificial Teachers by Considering How People Learn and Forget. In 26th International Conference on Intelligent User Interfaces (College Station, TX, USA) (IUI &apos;21). Association for Computing Machinery, New York, NY, USA, 445-453. https://doi.org/10.1145/3397481.3450696</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report</note>
	<note>cs.CL</note>
	<note type="raw_reference">OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_EVeZNAN">Vocabulary learning: A critical analysis of techniques</title>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Oxford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Crookall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_s6xYHSv">TESL Canada Journal</title>
		<imprint>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rebecca Oxford and David Crookall. 1990. Vocabulary learning: A critical analysis of techniques. TESL Canada Journal (1990), 09-30.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_qecNgTd">Language learning strategies, the communicative approach, and their classroom implications</title>
		<author>
			<persName><forename type="first">Rebecca</forename><forename type="middle">L</forename><surname>Oxford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberta</forename><forename type="middle">Z</forename><surname>Lavine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Crookall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zCzu6FH">Foreign Language Annals</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="1989">1989. 1989</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rebecca L Oxford, Roberta Z Lavine, and David Crookall. 1989. Language learning strategies, the communicative approach, and their classroom implications. Foreign Language Annals 22, 1 (1989), 29-39.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_dcHcU99">Second language vocabulary learning among adults: State of the art in vocabulary instruction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">C</forename><surname>Oxford</surname></persName>
		</author>
		<author>
			<persName><surname>Scarcella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TBcHRUg">System</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="231" to="243" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rebecca L Oxford and Robin C Scarcella. 1994. Second language vocabulary learning among adults: State of the art in vocabulary instruction. System 22, 2 (1994), 231-243.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_rk8X6Sw">Towards controllable story generation</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-1505</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wzaQPuG">Proceedings of the First Workshop on Storytelling</title>
		<meeting>the First Workshop on Storytelling</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="43" to="49" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. 2018. Towards controllable story generation. In Proceedings of the First Workshop on Storytelling. 43-49.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_fzFHNhU">Exploring the Effects of Technological Writing Assistance for Support Providers in Online Mental Health Community</title>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka</forename><forename type="middle">Wing</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376695</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376695" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_uQYK8jP">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems<address><addrLine>Honolulu, HI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>) (CHI &apos;20)</note>
	<note type="raw_reference">Zhenhui Peng, Qingyu Guo, Ka Wing Tsang, and Xiaojuan Ma. 2020. Exploring the Effects of Technological Writing Assistance for Support Providers in Online Mental Health Community. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI &apos;20). Association for Computing Machinery, New York, NY, USA, 1-15. https://doi.org/10.1145/ 3313831.3376695</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_6sqpfVu">AngleKindling: Supporting Journalistic Angle Ideation with Large Language Models</title>
		<author>
			<persName><forename type="first">Savvas</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Diakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Crowston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keren</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">V</forename><surname>Nickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lydia</forename><forename type="middle">B</forename><surname>Chilton</surname></persName>
		</author>
		<idno type="DOI">10.1145/3544548.3580907</idno>
		<ptr target="https://doi.org/10.1145/3544548.3580907" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_eKnjR26">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2023 CHI Conference on Human Factors in Computing Systems<address><addrLine>Hamburg, Germany; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Article 225</note>
	<note type="raw_reference">Savvas Petridis, Nicholas Diakopoulos, Kevin Crowston, Mark Hansen, Keren Henderson, Stan Jastrzebski, Jeffrey V Nickerson, and Lydia B Chilton. 2023. AngleKindling: Supporting Journalistic Angle Ideation with Large Language Models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI &apos;23). Association for Computing Machinery, New York, NY, USA, Article 225, 16 pages. https://doi.org/10.1145/3544548. 3580907</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_tDbGBpy">Controlling narrative generation with planning trajectories: the role of constraints</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Porteous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Cavazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mqxTuJV">Joint International Conference on Interactive Digital Storytelling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="234" to="245" />
		</imprint>
	</monogr>
	<note type="raw_reference">Julie Porteous and Marc Cavazza. 2009. Controlling narrative generation with planning trajectories: the role of constraints. In Joint International Conference on Interactive Digital Storytelling. Springer, 234-245.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_MhQwqup">The mnemonic keyword method</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pressley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><forename type="middle">D</forename><surname>Delaney</surname></persName>
		</author>
		<idno type="DOI">10.3102/00346543052001061</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZN3XpqP">Review of Educational Research</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="91" />
			<date type="published" when="1982">1982. 1982</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Pressley, Joel R Levin, and Harold D Delaney. 1982. The mnemonic keyword method. Review of Educational Research 52, 1 (1982), 61-91.</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_2mNRW8J">Development of a situational interaction game for improving preschool children&apos;performance in English-vocabulary learning</title>
		<author>
			<persName><forename type="first">Mei</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3231848.3231851</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QCEAxvR">Proceedings of the 2018 international conference on distance education and learning</title>
		<meeting>the 2018 international conference on distance education and learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="88" to="92" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mei Pu and Zheng Zhong. 2018. Development of a situational interaction game for improving preschool children&apos;performance in English-vocabulary learning. In Proceedings of the 2018 international conference on distance education and learning. 88-92.</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_f4eUQcJ">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_epFUcWR">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<title level="m" xml:id="_kmQpmBt">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the lim- its of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683 (2019).</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m" xml:id="_6Zsv8sy">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084 (2019).</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_8DF4y2t">Narrative planning: Balancing plot and character</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Riedl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KCaubmf">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="217" to="268" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mark O Riedl and Robert Michael Young. 2010. Narrative planning: Balancing plot and character. Journal of Artificial Intelligence Research 39 (2010), 217-268.</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_QdjX7vu">Evaluating story generation systems using automated linguistic analyses</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xXrqrhh">SIGKDD 2017 Workshop on Machine Learning for Creativity</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="13" to="17" />
		</imprint>
	</monogr>
	<note type="raw_reference">Melissa Roemmele, Andrew S Gordon, and Reid Swanson. 2017. Evaluating story generation systems using automated linguistic analyses. In SIGKDD 2017 Workshop on Machine Learning for Creativity. 13-17.</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_SrbPTV6">Englishbot: An ai-powered conversational system for second language learning</title>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Sherry Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><forename type="middle">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><surname>Landay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8u2bUuH">26th international conference on intelligent user interfaces</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="434" to="444" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sherry Ruan, Liwei Jiang, Qianyao Xu, Zhiyuan Liu, Glenn M Davis, Emma Brunskill, and James A Landay. 2021. Englishbot: An ai-powered conversational system for second language learning. In 26th international conference on intelligent user interfaces. 434-444.</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_ebGdkpf">Gliflix: Using movie subtitles for language learning</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Sakunkoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pattie</forename><surname>Sakunkoo</surname></persName>
		</author>
		<idno type="DOI">10.1145/2501988</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_DN7hHS4">Proceedings of the 26th Symposium on User Interface Software and Technology</title>
		<meeting>the 26th Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nathan Sakunkoo and Pattie Sakunkoo. 2013. Gliflix: Using movie subtitles for language learning. In Proceedings of the 26th Symposium on User Interface Software and Technology. ACM.</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_a6MRN4t">Augmented reality as multimedia: the case for situated vocabulary learning</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Ericson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takafumi</forename><surname>Taketomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goshiro</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mercedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Sandor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokazu</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jFJB2Cm">Research and Practice in Technology Enhanced Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marc Ericson C Santos, Takafumi Taketomi, Goshiro Yamamoto, Ma Rodrigo, T Mercedes, Christian Sandor, Hirokazu Kato, et al. 2016. Augmented reality as multimedia: the case for situated vocabulary learning. Research and Practice in Technology Enhanced Learning 11, 1 (2016), 1-23.</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_psQSNuX">Instructed second language vocabulary learning</title>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Schmitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Cvx8uEt">Language teaching research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="329" to="363" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Norbert Schmitt. 2008. Instructed second language vocabulary learning. Language teaching research 12, 3 (2008), 329-363.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohun</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhila</forename><surname>Yerukola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k19-1079</idno>
		<idno type="arXiv">arXiv:1909.10705</idno>
		<title level="m" xml:id="_3twMxpW">Do massively pretrained language models make better storytellers?</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher D Manning. 2019. Do massively pretrained language models make better story- tellers? arXiv preprint arXiv:1909.10705 (2019).</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_mvBdqm7">A sentiment and style controllable approach for chinese poetry generation</title>
		<author>
			<persName><forename type="first">Yizhan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_A5mnQ7J">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4784" to="4788" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yizhan Shao, Tong Shao, Minghao Wang, Peng Wang, and Jie Gao. 2021. A sentiment and style controllable approach for chinese poetry generation. In Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management. 4784-4788.</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main" xml:id="_X7JQtHB">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980 (2020).</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_7Ge2aWR">Codetoon: Story ideation, auto comic generation, and structure mapping for code-driven storytelling</title>
		<author>
			<persName><forename type="first">Sangho</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edith</forename><surname>Law</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_S6wVevy">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 35th Annual ACM Symposium on User Interface Software and Technology</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sangho Suh, Jian Zhao, and Edith Law. 2022. Codetoon: Story ideation, auto comic generation, and structure mapping for code-driven storytelling. In Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. 1-16.</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_AbSaYZJ">Readers, texts, and second languages: The interactive processes</title>
		<author>
			<persName><forename type="first">Janet</forename><forename type="middle">K</forename><surname>Swaffar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6xrdNmb">The Modern Language Journal</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="149" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Janet K Swaffar. 1988. Readers, texts, and second languages: The interactive processes. The Modern Language Journal 72, 2 (1988), 123-149.</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main" xml:id="_8aFbDXg">Vocabulary learning strategies and foreign language acquisition</title>
		<author>
			<persName><forename type="first">Pavii</forename><surname>Višnja</surname></persName>
		</author>
		<author>
			<persName><surname>Taka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nA5mUpq">Multilingual Matters</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Višnja Pavii Taka. 2008. Vocabulary learning strategies and foreign language acquisition. Multilingual Matters.</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_3DqWtfZ">An Image-Based Vocabulary Learning System Based on Multi-Agent System</title>
		<author>
			<persName><forename type="first">Preecha</forename><surname>Tangworakitthaworn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preeyapol</forename><surname>Owatsuwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nutsima</forename><surname>Nongyai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nongnapas</forename><surname>Arayapong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QERH2FQ">2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="324" to="329" />
		</imprint>
	</monogr>
	<note type="raw_reference">Preecha Tangworakitthaworn, Preeyapol Owatsuwan, Nutsima Nongyai, and Nongnapas Arayapong. 2019. An Image-Based Vocabulary Learning System Based on Multi-Agent System. In 2019 16th International Joint Conference on Computer Science and Software Engineering (JCSSE). IEEE, 324-329.</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main" xml:id="_6h2VErs">The creative process: A computer model of storytelling and creativity</title>
		<author>
			<persName><surname>Scott R Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Scott R Turner. 2014. The creative process: A computer model of storytelling and creativity. Psychology Press.</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_Y3DvW4B">Automatic poetry generation from prosaic text</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Van De Cruys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EJp7usF">Proceedings of the 58th annual meeting of the association for computational linguistics</title>
		<meeting>the 58th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2471" to="2480" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tim Van de Cruys. 2020. Automatic poetry generation from prosaic text. In Pro- ceedings of the 58th annual meeting of the association for computational linguistics. 2471-2480.</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_uvhttDm">Technology Acceptance Model 3 and a Research Agenda on Interventions</title>
		<author>
			<persName><forename type="first">Viswanath</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hillol</forename><surname>Bala</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1540-5915.2008.00192.x</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5915.2008.00192.x" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_yUYfmAg">Decision Sciences</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="315" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Viswanath Venkatesh and Hillol Bala. 2008. Technology Acceptance Model 3 and a Research Agenda on Interventions. Decision Sciences 39, 2 (2008), 273-315. https://doi.org/10.1111/j.1540-5915.2008.00192.x arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5915.2008.00192.x</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_n7T2cGk">ArgueTutor: An Adaptive Dialog-Based Learning System for Argumentation Skills</title>
		<author>
			<persName><forename type="first">Thiemo</forename><surname>Wambsganss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Kueng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Soellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Marco</forename><surname>Leimeister</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445781</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445781" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_rE68G2z">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems<address><addrLine>Yokohama, Japan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">683</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Thiemo Wambsganss, Tobias Kueng, Matthias Soellner, and Jan Marco Leimeister. 2021. ArgueTutor: An Adaptive Dialog-Based Learning System for Argumen- tation Skills. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI &apos;21). Association for Computing Machinery, New York, NY, USA, Article 683, 13 pages. https://doi.org/10.1145/ 3411764.3445781</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main" xml:id="_d7j7aAA">AL: An Adaptive Learning Support System for Argumentation Skills</title>
		<author>
			<persName><forename type="first">Thiemo</forename><surname>Wambsganss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Cetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Söllner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Marco</forename><surname>Leimeister</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376732</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376732" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_rTXKbYh">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems<address><addrLine>Honolulu, HI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>) (CHI &apos;20)</note>
	<note type="raw_reference">Thiemo Wambsganss, Christina Niklaus, Matthias Cetto, Matthias Söllner, Siegfried Handschuh, and Jan Marco Leimeister. 2020. AL: An Adaptive Learn- ing Support System for Argumentation Skills. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI &apos;20). Association for Computing Machinery, New York, NY, USA, 1-14. https: //doi.org/10.1145/3313831.3376732</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main" xml:id="_t3xkm3J">An Evaluation of Generative Pre-Training Model-based Therapy Chatbot for Caregivers</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munif</forename><surname>Ishad Mujib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Demiris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jina</forename><surname>Huh-Yoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13115</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Lu Wang, Munif Ishad Mujib, Jake Williams, George Demiris, and Jina Huh-Yoo. 2021. An Evaluation of Generative Pre-Training Model-based Therapy Chatbot for Caregivers. arXiv preprint arXiv:2107.13115 (2021).</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_tTg4k6q">Voicecoach: Interactive evidence-based training for voice modulation skills in public speaking</title>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Xingbo ; Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhida</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huamin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9vn8fXE">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2020 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Xingbo Wang, Haipeng Zeng, Yong Wang, Aoyu Wu, Zhida Sun, Xiaojuan Ma, and Huamin Qu. 2020. Voicecoach: Interactive evidence-based training for voice modulation skills in public speaking. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. 1-12.</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_PbdMYuW">A computational model of plan-based narrative conflict at the fabula level</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stephen G Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Michael Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FHET4g3">IEEE Transactions on Computational Intelligence and AI in Games</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="271" to="288" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Stephen G Ware, R Michael Young, Brent Harrison, and David L Roberts. 2013. A computational model of plan-based narrative conflict at the fabula level. IEEE Transactions on Computational Intelligence and AI in Games 6, 3 (2013), 271-288.</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main" xml:id="_6mT5vc6">Predicting and Diagnosing User Engagement with Mobile UI Animation via a Data-Driven Approach</title>
		<author>
			<persName><forename type="first">Ziming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3313831.3376324</idno>
		<ptr target="https://doi.org/10.1145/3313831.3376324" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1" to="13" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Ziming Wu, Yulun Jiang, Yiding Liu, and Xiaojuan Ma. 2020. Predicting and Diagnosing User Engagement with Mobile UI Animation via a Data-Driven Ap- proach. Association for Computing Machinery, New York, NY, USA, 1-13. https://doi.org/10.1145/3313831.3376324</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main" xml:id="_cSSUmnC">Persua: A visual interactive system to enhance the persuasiveness of arguments in online discussion</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huamin</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_85gYXc5">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
	<note type="raw_reference">Meng Xia, Qian Zhu, Xingbo Wang, Fei Nie, Huamin Qu, and Xiaojuan Ma. 2022. Persua: A visual interactive system to enhance the persuasiveness of arguments in online discussion. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2 (2022), 1-30.</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main" xml:id="_pvpySHY">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00143</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ndJdPby">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1316-1324.</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main" xml:id="_Z5FnDM6">FinDo: A Foreign Language Vocabulary Learning System Based on Location-Context</title>
		<author>
			<persName><forename type="first">Keiko</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshihiro</forename><surname>Tsujino</surname></persName>
		</author>
		<idno type="DOI">10.1109/snpd.2019.8935794</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_JEyqwsV">20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="302" to="307" />
		</imprint>
	</monogr>
	<note type="raw_reference">Keiko Yamamoto, Jesus Rodriguez, and Yoshihiro Tsujino. 2019. FinDo: A Foreign Language Vocabulary Learning System Based on Location-Context. In 2019 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD). IEEE, 302-307.</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main" xml:id="_rZCJTdM">Plan-and-write: Towards better automatic storytelling</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UA7Bz3B">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7378" to="7385" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-and-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 7378-7385.</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main" xml:id="_g4EDm4V">An interactive vocabulary learning system based on word frequency lists and Ebbinghaus&apos; curve of forgetting</title>
		<author>
			<persName><forename type="first">Liren</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/dmdcm.2011.71</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3W3msmp">Workshop on Digital Media and Digital Content Management</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
	<note type="raw_reference">Liren Zeng and Ling Lin. 2011. An interactive vocabulary learning system based on word frequency lists and Ebbinghaus&apos; curve of forgetting. In 2011 Workshop on Digital Media and Digital Content Management. IEEE, 313-317.</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main" xml:id="_G94wAgK">StoryDrawer: A Child-AI Collaborative Drawing System to Support Children&apos;s Creative Visual Storytelling</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangtian</forename><surname>Ying</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3501914</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fWF7sNx">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chao Zhang, Cheng Yao, Jiayi Wu, Weijia Lin, Lijuan Liu, Ge Yan, and Fangtian Ying. 2022. StoryDrawer: A Child-AI Collaborative Drawing System to Support Children&apos;s Creative Visual Storytelling. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 1-15.</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main" xml:id="_P6KUUUM">Storybuddy: A human-ai collaborative chatbot for parent-child interactive storytelling with flexible parental involvement</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dakuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby Jia-Jun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3491102.3517479</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PA7rnCv">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2022 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zheng Zhang, Ying Xu, Yanhao Wang, Bingsheng Yao, Daniel Ritchie, Tong- shuang Wu, Mo Yu, Dakuo Wang, and Toby Jia-Jun Li. 2022. Storybuddy: A human-ai collaborative chatbot for parent-child interactive storytelling with flex- ible parental involvement. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. 1-21.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
