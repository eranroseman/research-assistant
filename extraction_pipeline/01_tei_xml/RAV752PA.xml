<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_fUTqrv3">Cohesion-based Online Actor-Critic Reinforcement Learning for mHealth Intervention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-08-23">23 Aug 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>‡</label> Department of CSE in the University of Texas at Arlington . Feiyun</note>
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Feiyun</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>‡</label> Department of CSE in the University of Texas at Arlington . Feiyun</note>
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<settlement>Feiyun</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinliang</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yaowen</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">Department of Statistics in the University of Michigan.</note>
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_QmWJkD5">Cohesion-based Online Actor-Critic Reinforcement Learning for mHealth Intervention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-23">23 Aug 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">B61FC28A0D7789CC32B2C36BB193E12B</idno>
					<idno type="arXiv">arXiv:1703.10039v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_MCKBevT">Actor-Critic</term>
					<term xml:id="_ftu6RhR">Reinforcement Learning</term>
					<term xml:id="_dsYNjw8">Mobile Health (mHealth) Intervention</term>
					<term xml:id="_CAg6qDM">Cohesion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_zWnHhND"><p xml:id="_sV5NHs7"><s xml:id="_SwQtsE2">In the wake of the vast population of smart device users worldwide, mobile health (mHealth) technologies are hopeful to generate positive and wide influence on people's health.</s><s xml:id="_RjdJkb6">They are able to provide flexible, affordable and portable health guides to device users.</s><s xml:id="_H7qc38x">Current online decision-making methods for mHealth assume that the users are completely heterogeneous.</s><s xml:id="_YeaR4Rh">They share no information among users and learn a separate policy for each user.</s><s xml:id="_qbK5xWY">However, data for each user is very limited in size to support the separate online learning, leading to unstable policies that contain lots of variances.</s><s xml:id="_ntDE42T">Besides, we find the truth that a user may be similar with some, but not all, users, and connected users tend to have similar behaviors.</s><s xml:id="_3Ee8XwP">In this paper, we propose a network cohesion constrained (actor-critic) Reinforcement Learning (RL) method for mHealth.</s><s xml:id="_qDRg6P3">The goal is to explore how to share information among similar users to better convert the limited user information into sharper learned policies.</s><s xml:id="_JUCj9WQ">To the best of our knowledge, this is the first online actor-critic RL for mHealth and first network cohesion constrained (actor-critic) RL method in all applications.</s><s xml:id="_Wh7Na3N">The network cohesion is important to derive effective policies.</s><s xml:id="_PReHCfG">We come up with a novel method to learn the network by using the warm start trajectory, which directly reflects the users' property.</s><s xml:id="_nJcDFTb">The optimization of our model is difficult and very different from the general supervised learning due to the indirect observation of values.</s><s xml:id="_tFU8zy5">As a contribution, we propose two algorithms for the proposed online RLs.</s><s xml:id="_WZsCbN2">Apart from mHealth, the proposed methods can be easily applied or adapted to other health-related tasks.</s><s xml:id="_Pe6aBEm">Extensive experiment results on the HeartSteps dataset demonstrates that in a variety of parameter settings, the proposed two methods obtain obvious improvements over the state-of-the-art methods.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_yfHFd6S">INTRODUCTION</head><p xml:id="_6wexdJs"><s xml:id="_A38rWeN">With billions of smart device 1 users globally, it is increasingly popular among the scientist community to make use of the state-of-the-art articial intelligence and mobile health technologies to leverage supercomputers and big data to facilicate the prediction of healthcare tasks <ref type="bibr" target="#b5">[1]</ref><ref type="bibr" target="#b6">[2]</ref><ref type="bibr" target="#b7">[3]</ref><ref type="bibr" target="#b8">[4]</ref><ref type="bibr" target="#b9">[5]</ref><ref type="bibr" target="#b10">[6]</ref><ref type="bibr" target="#b11">[7]</ref>.</s><s xml:id="_ckpuJGg">In this paper, the goal of mobile health (mHealth) is to make use of various smart devices as great platforms to collect and analyze raw data (weather, location, social activity, stress, etc.).</s><s xml:id="_YJ6GBwk">Based on that, the aim is to provide effective intervention that helps users to change to or adapt to healthy behaviors, such as reducing the alcohol abuse <ref type="bibr" target="#b12">[8,</ref><ref type="bibr" target="#b13">9]</ref> and promoting physical activities <ref type="bibr" target="#b14">[10]</ref>.</s><s xml:id="_mdsYF8Q">The traditional adaptive treatment has restrictions on the time, location and frequency-patients have to visit the doctor's office for treatments.</s><s xml:id="_t5asrXA">Compared with them, mHealth is more affordable, portable and much more flexible in the sense that smart devices allow for the real-time collection and analysis of data as well as in-time delivery of interventions.</s><s xml:id="_b5Xcbxb">Thus, mHealth technologies are widely used in lots of healthrelated tasks, such as physical activity <ref type="bibr" target="#b14">[10]</ref>, eating disorders <ref type="bibr" target="#b15">[11]</ref>, alcohol use <ref type="bibr" target="#b12">[8,</ref><ref type="bibr" target="#b13">9]</ref>, mental illness <ref type="bibr" target="#b16">[12,</ref><ref type="bibr" target="#b17">13]</ref>, obesity/weight management <ref type="bibr" target="#b18">[14]</ref>.</s></p><p xml:id="_tenKfDG"><s xml:id="_D6RXxrt">Formally, the mHealth intervention is modeled as a sequential decision making (SDM) problem.</s><s xml:id="_Csa4utf">It aims to learn the optimal policy to determine when, where and how to deliver the intervention <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b7">3,</ref><ref type="bibr" target="#b8">4]</ref> to best serve users.</s><s xml:id="_ZGr7qEV">This is a new research topic that lacks of methodological guidance.</s><s xml:id="_PfFUFxz">In 2014, Lei <ref type="bibr" target="#b5">[1]</ref> made a first attempt to formulate the mHealth intervention as an online actor-critic contextual bandit problem.</s><s xml:id="_AJbcyAx">Lei's method served a good starting point for the mHealth study.</s><s xml:id="_EmMy4ce">However, this method did not consider the important delayed effect in the SDMthe current action may influence not only the immediate reward but also the next states and, through that, all the subsequent rewards <ref type="bibr" target="#b19">[15,</ref><ref type="bibr" target="#b20">16]</ref>.</s><s xml:id="_E3aE2kC">Dr. Murphy <ref type="bibr" target="#b7">[3]</ref> proposed an average reward based RL to consider the delayed effect in the mHealth.</s><s xml:id="_XmeFCYV">However, those two methods rely on some ideal assumptions.</s><s xml:id="_y93aB6p">They either assume that all the users are completely homogenous or completely heterogeneous.</s><s xml:id="_vfNn5fv">We find the truth lying between those extremes: a user might be similar with some, but not all, users.</s><s xml:id="_D9cu8XJ">Their methods are easy to bring in too much bias or too much variance.</s><s xml:id="_5gq4yBU">Besides, <ref type="bibr" target="#b7">[3]</ref> is in the batch learning setting, which is different from this paper's focuses.</s></p><p xml:id="_6Py9XeN"><s xml:id="_Tf2kR6e">Recently, Dr. Cesa-Bianchi <ref type="bibr" target="#b21">[17]</ref> proposed a contextual bandit algorithm that considers the network information.</s><s xml:id="_StEhN8M">It is for the recommendation system, which is very different from the mHealth task.</s><s xml:id="_dAcSDx6">Besides, there are three drawbacks making the method in <ref type="bibr" target="#b21">[17]</ref> impractical for the mHealth: (1) Cesa-Bianchi's method focues on the bandit algorithm.</s><s xml:id="_AgN9VYW">It doesn't consider the important delayed effect in mHealth.</s></p><p xml:id="_eDCWcMf"><s xml:id="_txHkvDn">(2) They assume the network information is given beforehand from the social information.</s><s xml:id="_9Gk7zkF">The given network may not be targeted for the mHealth study.</s><s xml:id="_4cZx3vM">There is lots of misleading network information for the mHealth study <ref type="bibr" target="#b21">[17]</ref><ref type="bibr" target="#b22">[18]</ref><ref type="bibr" target="#b23">[19]</ref>.</s><s xml:id="_v3zmMnQ"><ref type="bibr" target="#b7">(3)</ref> In their work, however, it is unable to control the amount of information shared among linked users, which is not flexible for the mHealth study <ref type="bibr" target="#b24">[20,</ref><ref type="bibr" target="#b25">21]</ref>.</s></p><p xml:id="_DKzMw5H"><s xml:id="_T3GZkJM">In this paper, we propose a cohesion-based reinforcement learning for the mHealth and derive two algorithms.</s><s xml:id="_YSMYMD8">It is in an online, actor-critic setting.</s><s xml:id="_fUs2JQD">The aim is to explore how to share information across similar users in order to improve the performance.</s><s xml:id="_a9EbhxF">The main contributions of this paper are summarized as follows: <ref type="bibr" target="#b5">(1)</ref> to the best of our knowledge, this is the first online (actor-critic) RL method for the mHealth.</s><s xml:id="_m9aQJpK">(2) Current evidence verifies the wide existence of networks among users <ref type="bibr" target="#b24">[20]</ref><ref type="bibr" target="#b25">[21]</ref><ref type="bibr" target="#b26">[22]</ref>.</s><s xml:id="_nM4RU79">We improve the online RL by considering the network cohesion among users.</s><s xml:id="_EWYnaMb">Such improvement makes it the first network constrained (actorcritic) RL method to the best of our knowledge.</s><s xml:id="_DwKVxU9">It is able to relieve the tough problem of current online decision-making methods for the mHealth by reducing variance at the cost of inducing bias.</s><s xml:id="_NX2Qchs">Current online RL learns a separate policy for each user.</s><s xml:id="_CduJ4Kd">However, there are too few of samples to support the separate online learning, which leads to unsatisfactory interventions (policies) for the users.</s><s xml:id="_UD3hNVd">(3) Our method doesn't require the given network cohesion.</s><s xml:id="_m8TeHgN">We propose a method to learn the network intentionally for the mHealth study.</s><s xml:id="_ktTTMKz">It makes use of the warm start trajectories in the online learning, which are expected to represent the users' properties.</s><s xml:id="_KgBcumr">(4) Compared with <ref type="bibr" target="#b21">[17]</ref>, the proposed method has a tuning parameter, which allows us to control how much information we should share with similar users.</s><s xml:id="_vR4ANEh">It is worth mentioning that our method may not be limited to mHealth.</s><s xml:id="_9Dh8H6Z">It can be applied to other health-related tasks.</s><s xml:id="_uFPZUAt">Extensive experiment results on the HeartSteps dataset verifies that our method can achieve clear improvement over the Separate-RL.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_hZNSsyf">PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_DNKkVgr">Markov Decision Process (MDP)</head><p xml:id="_pKuY6NG"><s xml:id="_zbjR69c">We assume the mHealth intervention is a Markov Decision Process (MDP) <ref type="bibr" target="#b27">[23]</ref><ref type="bibr" target="#b28">[24]</ref><ref type="bibr" target="#b29">[25]</ref><ref type="bibr" target="#b30">[26]</ref> that consists of a 5-tuple {S, A, P, R, γ}, where S is the state space and A is the action space.</s><s xml:id="_8fbtsEn">P : S × A × S → [0, 1] is the state transition model in which P (s, a, s ) indicates the probability of transiting from one state s to another s after taking action a; R (s, a, s ) is the corresponding immediate reward for such transition where R : S × A × S → R. For simplicity, the expected immediate reward R (s, a) = E s ∼P [R (s, a, s )] is assumed to be bounded over the state and action spaces.</s><s xml:id="_pWjgGCj">γ ∈ [0, 1) is the discount factor that reduces the influence of future rewards.</s><s xml:id="_TmPKdRJ">To allow for the matrix operators, the state space S and action space A are assumed to be finite, though very large in mHealth.</s></p><p xml:id="_Ac9fbQy"><s xml:id="_3fABsgF">The policy of an MDP is to choose actions for any state s ∈ S in the system <ref type="bibr" target="#b19">[15,</ref><ref type="bibr" target="#b28">24]</ref>.</s><s xml:id="_e6AFgAY">There are two types of policies: (1) the deterministic policy π : S → A selects an action directly for the state, and (2) the stochastic policy π : s ∈ S → π (• | s) ∈ P (A) chooses the action for any state s by providing s with a probability distribution over all the possible actions <ref type="bibr" target="#b27">[23]</ref>.</s><s xml:id="_TS6E2Kb">In mHealth, the stochastic policy is preferred due to two reasons: (a) current evidence shows that some randomness in the action is likely to draw users' interest, thus helpful to reduce the intervention burden/habituation <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b7">3,</ref><ref type="bibr" target="#b31">27]</ref>; (b) though some deterministic policy is theoretically optimal for the MDP, however, we do not know where it is for the large state space on the one hand and the MDP is a simplification for the complex behavioral process on the other; some variation may be helpful to explore the system and search for a desirable policy <ref type="bibr" target="#b7">[3]</ref>.</s><s xml:id="_ntbBQ2x">We consider the parameterized stochastic policy, π θ (a | s), where θ ∈ R m is the unknown parameter.</s><s xml:id="_gA6T9Ck">Such policy is interpretable in the sense that we could know the key features that contribute most to the policy by analyzing the estimated θ, which is important to behavior scientists for the state (feature) design <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b7">3]</ref>.</s></p><p xml:id="_NS4q5kA"><s xml:id="_8Y5c6Yb">In RL, value is a core concept that quantifies the quality of a policy π <ref type="bibr" target="#b19">[15]</ref>.</s><s xml:id="_bWJrzRc">There are two definitions of values: the state value and the state-action (Q-) value <ref type="bibr" target="#b32">[28]</ref>.</s><s xml:id="_eCr5X2W">In mHealth, the Q-value is considered because the model (i.e.</s><s xml:id="_7nrS7Rh">state transition and immediate reward) is assumed to be unknown, and Q-value allows for action selection without knowing the model while the state value requires the model for the action selection <ref type="bibr" target="#b30">[26]</ref>.</s><s xml:id="_DpJanft">Formally, the Q-value Q π (s, a) ∈ R |S|×|A| measures the total amount of rewards an agent can obtain when starting from state s, first choosing action a and then following the policy π.</s><s xml:id="_vh97FAU">Specially, the discounted reward is one of the most commonly used value measures</s></p><formula xml:id="formula_0">Q π (s, a) = E ai∼π,si∼P ∞ i=0 γ i r i | s 0 = s, a 0 = a . (1)</formula><p xml:id="_2PNJvHp"><s xml:id="_bxczzha">The goal of RL is to learn an optimal policy π * that maximizes the Q-value for all the state-action pairs via interactions with the dynamic system <ref type="bibr" target="#b27">[23]</ref>.</s><s xml:id="_Vp7wkk3">The objective is θ * = arg max θ J (θ), where</s></p><formula xml:id="formula_1">J (θ) = s∈S d ref (s) a∈A π θ (a | s) Q π θ (s, a)<label>(2)</label></formula><p xml:id="_4Eyjwz5"><s xml:id="_SeXqHXY">and d ref (s) is the reference distribution of states (e.g. the distribution of initial states); Q π θ is the value for the policy π θ .</s><s xml:id="_psQAH5f">According to (2), we have to learn the Q π θ for all the state-action pairs to determine the objective (2) and, after then, to improve the policy.</s><s xml:id="_tGhzQJ4">Thus in this paper, we employ the actor-critic algorithm.</s><s xml:id="_F5jg6FZ">It is an alternating updating algorithm between two steps untill convergence.</s><s xml:id="_FbhCF4a">At each iteration, the critic updating estimates the Q-value function (i.e. policy evaluation, cf.</s><s xml:id="_9ufHRgf">Section 2.2 and 2.3) for the lastest policy; the actor updating (i.e. policy improvement, cf.</s><s xml:id="_gkWWV3u">Section 2.4) learns a better policy based on the newly estimated Q-value.</s><s xml:id="_UE7pSHw">Moreover, the actor-critic algorithm has great properties of quick convergence with low variance and learning continuous policies <ref type="bibr" target="#b28">[24]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_UvfhCV2">Bellman Equation and Q-value Estimation</head><p xml:id="_CmXq7sv"><s xml:id="_7rshAFq">It is well known that due to the Markovian property, the Q-value satisfies the linear Bellman equation <ref type="bibr" target="#b33">[29]</ref> for any policy π:</s></p><formula xml:id="formula_2">Q π (s, a) = R (s, a) + γ s ∈S P (s, a, s ) a ∈A π (a | s ) Q π (s , a ) .</formula><p xml:id="_cY85nCu"><s xml:id="_n8Q3Gtp">It has the matrix form as</s></p><formula xml:id="formula_3">q π = r + γPΠ π q π ,<label>(3)</label></formula><p xml:id="_4K4upPA"><s xml:id="_y5MrJTd">where q π and r are vectors both with |S| |A| elements; P ∈ R |S||A|×|S| is the stochastic state transition matrix, in which P ((s, a) , s ) = P (s, a, s ); Π π ∈ R |S|×|S||A| is the stochastic policy matrix, where Π π (s, (s, a)) = π (a | s) <ref type="bibr" target="#b30">[26]</ref>.</s><s xml:id="_zHpSeBe">Once both the reward and the state transition models are given <ref type="bibr" target="#b34">[30]</ref>, it is easy to obtain the analytical solution as q π = (I -γPΠ π ) -1 r.</s></p><p xml:id="_ps5nsDU"><s xml:id="_j6cSB56">However, there are two factors making it impossible to have the analytical solution for the Q-value estimation: (a) in mHealth, both reward r and state transition P (i.e.</s><s xml:id="_gRhM4Eb">P) models are unknown.</s><s xml:id="_aWApcXE">(b) the state space in mHealth is usually very large or even infinite, which makes it impossible to directly learn the Q-value due to lack of observations for sharper learning and too high storage requirements, i.e.</s><s xml:id="_6aKREeC">O (|S| |A|) to only store the Q-value table.</s><s xml:id="_utUnAHk">We resolve these problems via the parameterized function approximation, which assumes that Q π is in a low dimensional space:</s></p><formula xml:id="formula_4">Q π ≈ Q w = w x (s, a)</formula><p xml:id="_Es3ccSv"><s xml:id="_EuBUTJp">, where w ∈ R u is the unknown variable and x (s, a) is a feature processing step that combines information in the state and action.</s><s xml:id="_NSrvhS3">We then learn the value Q w from observations via a supervised learning paramdigm, which, however, is much more challenging than the general supervised learning since the Q-value is not directly observed <ref type="bibr" target="#b27">[23]</ref>.</s><s xml:id="_cNDeCfw">As a direct solution, the Monte Carlo (MC) method draws very deep trajectories to obtain the observation of actual Q value.</s><s xml:id="_wZJPkPh">Although MC can provide an unbiased estimation of Q w , it is not suitable for mHealth since MC can't learn from the incomplete trajectory <ref type="bibr" target="#b19">[15]</ref>.</s><s xml:id="_rxSJWfD">Such case requires massive sampling from users, which, however, is very labor-intensive and expensive in time.</s><s xml:id="_ZXEUK2Q">As a central idea of RL <ref type="bibr" target="#b19">[15]</ref>, the temporal-difference (TD) learning is able to make use of the Bellman equation ( <ref type="formula" target="#formula_3">3</ref>) and to learn the value from the incomplete trajectories.</s><s xml:id="_XhcsjZ9">The learned result of TD has the property of low variance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3" xml:id="_TRVGJjS">The critic updating: Least-Squares TD for Q-value (LSTDQ) Estimation</head><p xml:id="_pbQR7rh"><s xml:id="_74UUuHP">In mHealth, though the data for all users is abundant, the data for each user is limited in size.</s><s xml:id="_e4DNVQp">We employ the least-square TD for the Q-value (LSTDQ) estimation, due to its advantage of efficient use of samples over the pure temporal-difference algorithms <ref type="bibr" target="#b30">[26,</ref><ref type="bibr" target="#b35">31]</ref>.</s><s xml:id="_nwMn8s7">The goal of LSTDQ is to learn a Q w to approximately satisfy the Bellman equation ( <ref type="formula" target="#formula_3">3</ref>), by minimizing the TD error <ref type="bibr" target="#b34">[30]</ref> as</s></p><formula xml:id="formula_5">w = h = min h∈R K X h -r + γPΠ π X w 2 D ,<label>(4)</label></formula><p xml:id="_D3Sy5JC"><s xml:id="_aE6GRMc">where w = h is a fixed point problem and h is a function of w; X is a designed matrix consisting of all the state and action pairs in the MDP; D describes the distributions over the state and action pairs.</s><s xml:id="_tgZUXMK">Since the state transition P is unknown and X is too large to form in mHealth, we can not directly solve <ref type="bibr" target="#b8">(4)</ref>.</s><s xml:id="_nbVHyer">Instead, we have to make use of the trajectories collected from N users, i.e.</s></p><formula xml:id="formula_6">D = {D n } N n=1 , where D n = {U i = (s i , a i, r i , s i ) | i = 0, • • • ,</formula><p xml:id="_JDvpMCM"><s xml:id="_Ev8apmF">t} summarizes all the t + 1 tuples for the n-th user and U i is the i-th tuple in D n .</s></p><p xml:id="_96eqGXz"><s xml:id="_ZRz8PY9">Current online contextual bandit (i.e. a special RL with γ = 0) methods for mHealth assume that all users are completely heterogeneous.</s><s xml:id="_FHGs8xc">They share no information and run a separate algorithm for each user <ref type="bibr" target="#b5">[1]</ref>.</s><s xml:id="_Eut6y3z">Following this idea, we extend <ref type="bibr" target="#b5">[1]</ref> to the separate RL setting.</s><s xml:id="_TfAG6Wj">The objective for the n-th user is defined as</s></p><formula xml:id="formula_7">w n = h n = arg min hn Ui∈Dn x i h n -(r i + γy i w n ) 2 2</formula><p xml:id="_aMutUyR"><s xml:id="_g2q96hf">(5)</s></p><formula xml:id="formula_8">for n ∈ {1, • • • , N }, where x i = x (s i , a i ) is the value feature at time i and y i = a ∈A x (s i , a ) π θ (a | s i )</formula><p xml:id="_cqBnewP"><s xml:id="_gXRAccv">is the value feature at the next time point.</s><s xml:id="_8KqREUQ">For the sake of easy derivation, we define the following matrices to store the actual observations</s></p><formula xml:id="formula_9">X n = [x (s 1 , a 1 ) , x (s 2 , a 2 ) • • • , x (s t , a t )] ∈ R u×t Y n = [y (s 1 ; θ n ) , • • • , y (s t ; θ n )] ∈ R u×t<label>(6)</label></formula><formula xml:id="formula_10">r n = [r 1 , r 2 , • • • , r t ] ∈ R t ,</formula><p xml:id="_dbeQh4q"><s xml:id="_TuPgdYm">where u is the length of the sample feature for the Q-value approximation, t is the current time point in the online RL learning procedure (i.e. the current trajectory length),</s></p><formula xml:id="formula_11">y (s i ; θ n ) = a ∈A x (s i , a ) π θn (a | s i ), and π θn (a | s) is the policy for the n-th user. Let R = [r 1 , • • • , r N ] ∈ R t×N</formula><p xml:id="_brpqP44"><s xml:id="_fFNc3bb">store the reward of all N users at all the t time points.</s><s xml:id="_AehY7c3">To prevent the overfitting when t is small at the beginning of online RL learning, the ℓ 2 norm based constraint is considered in the objective as follows</s></p><formula xml:id="formula_12">w n = h n = arg min hn X n h n -(r n + γY n w n ) 2 2 + ζ c h n 2 2 (7) for n ∈ {1, • • • , N }.</formula><p xml:id="_kFZSgG5"><s xml:id="_6A2NFBK">The LSTDQ provides a closed-form solution</s></p><formula xml:id="formula_13">w θn = [X n (X n -γY n ) + ζI] -1 X n r n ,<label>(8)</label></formula><p xml:id="_rDgXJPA"><s xml:id="_zJkUTbG">for {n} N n=1 , where w θn is a function of the policy parameter θ n .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4" xml:id="_G4wWHSs">The actor updating for policy improvement</head><p xml:id="_xZwyg8g"><s xml:id="_MQbC7Zb">In mHealth, the reference distribution of states d ref (s) is unknown and hard to estimate due to the lack of samples.</s><s xml:id="_Zyf88r7">We set d ref (s) as the empirical distribution of states.</s><s xml:id="_MCjsBpb">Accordingly, the observations in the trajectory, i.e.</s><s xml:id="_WbPYMyA">D n , are used to form the objective for the actor updating θ n = arg max θn J (θ n ), where</s></p><formula xml:id="formula_14">J (θ n ) = 1 |D n | si∈Dn a∈A Q (s i , a; w θn ) π θn (a|s i ) - ζ a 2 θ n 2 2 (9) for n ∈ {1, • • • , N }. Here θ n 2 2</formula><p xml:id="_BTbQ6C3"><s xml:id="_mPK3qSz">is the constraint to make (9) a well-posed problem and ζ a is the tuning parameter that controls the strength of the smooth penalization <ref type="bibr" target="#b5">[1]</ref>.</s><s xml:id="_cSJrwK5">We use J (θ n ) rather than J (θ n ) in <ref type="bibr" target="#b13">(9)</ref> to indicate that the objective function for the actor updating is defined based on the Qvalue estimation.</s></p><p xml:id="_rhXDDYM"><s xml:id="_jMV3W24">Since the critic updating results in a closed-form solution <ref type="bibr" target="#b12">(8)</ref>, we could substitute the expression (8) into the objective for the actor updating <ref type="bibr" target="#b13">(9)</ref>.</s><s xml:id="_EPpCpGV">Such case, however, leads to a very complex optimization problem.</s><s xml:id="_CvgjExP">In the case of large feature space, one can recursively update w θ and θ n to reduce the computational cost.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_paZg9kJ">NETWORK COHESION BASED ONLINE ACTOR-CRITIC RL</head><p xml:id="_JQZKAAm"><s xml:id="_5rMzYz9">It is a famous phenomenon observed in lots of social behavior studies <ref type="bibr" target="#b36">[32,</ref><ref type="bibr" target="#b37">33]</ref> that people are widely connected in a network and linked users tend to have similar behaviors.</s><s xml:id="_fYqQVcG">Advances in social media help a lot to record the relational information among users, which ensures the availability of network information for health-related studies.</s><s xml:id="_a4mXM3C">Besides, individuals are widely connected due to the similar features, such as age, gender, race, religion, education level, work, income, other socioeconomic status, medical records and genetics features etc <ref type="bibr" target="#b26">[22]</ref>.</s><s xml:id="_KfCfAky">However, for simple study, current online methods for the mHealth simply assume that users are completely different; they share no information among users and learn a separate RL for each user by only using his or her data.</s><s xml:id="_X6QXaS8">Such assumption works well in the ideal condition where the sample drawn from each user is large in size to support the separate online learning.</s><s xml:id="_642DeUA">However, though the data for all users is abundant, the data for each user is limited in size.</s><s xml:id="_N744yGb">For example at the beginning of online learning, there are t = 5 tuples, which is hardly enough to support a separate learning and likely to result in unstable policies.</s><s xml:id="_zxwpytc">From the perspective of optimization, the problem of lack of samples badly affects the actor-critic updating not only at the beginning of online learning but also along the whole learning process.</s><s xml:id="_x85Ddc8">This is because the actor-critic objective functions are non-convex and nonlinear; the bad solution at the beginning of online learning would bias the optimization to sub-optimal directions.</s><s xml:id="_uXpGFze">Besides, the policy achieved at the early stage of online learning is of bad user experience, which is likely for the users to be inactive with or even to abandon the mHealth.</s></p><p xml:id="_JFJmCzZ"><s xml:id="_7fk5CCn">Different from current methods, we consider the phenomenon that a user is similar to some (but not all) users, and similar users behave similar but not completely identical to each other.</s><s xml:id="_75P2mX3">To this end, we propose a cohesionbased online RL method for the mHealth study.</s><s xml:id="_4bMwXGz">We aim to understand how to share information across similar users in order to improve the performance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_46PTTFr">Construct the network cohesion by using the warm start trajectory (WST)</head><p xml:id="_YHjA2YC"><s xml:id="_EdsPSnq">We assume there is an undirected network cohesion connecting similar users, i.e.</s><s xml:id="_5WWbZ7d">G = (V, E), where V = {1, 2, • • • , N } is the set of nodes (representing users) and E ⊂ V × V is the edge set.</s><s xml:id="_hGxEJEc">Altough advanced social medias, like Facebook, Twitter and Linkedin, could provide us ith various network information, they are not designed for the mHealth.</s><s xml:id="_Zjqe3V7">There is noisy and misleading relational information in the network for mHealth WS <ref type="bibr" target="#b21">[17]</ref><ref type="bibr" target="#b22">[18]</ref><ref type="bibr" target="#b23">[19]</ref><ref type="bibr" target="#b38">[34]</ref><ref type="bibr" target="#b39">[35]</ref><ref type="bibr" target="#b40">[36]</ref><ref type="bibr" target="#b41">[37]</ref><ref type="bibr" target="#b42">[38]</ref><ref type="bibr" target="#b43">[39]</ref><ref type="bibr" target="#b44">[40]</ref><ref type="bibr" target="#b45">[41]</ref><ref type="bibr" target="#b46">[42]</ref><ref type="bibr" target="#b47">[43]</ref>.</s><s xml:id="_bHac8zW">Thus, we want to learn the network cohesion intentionally for the mHealth by measuring the similarities between the related behaviors of users.</s></p><p xml:id="_KumEyyr"><s xml:id="_ShhdQ89">In RL, the MDP provides a mathematical tool to describe the property of users in a specific mHealth study 2 .</s><s xml:id="_Nk8CDut">By measuring the similarities among the users' MDPs , we could 2. The MDPs of one user on two diverse mHealth studies should be very different; for example, the MDP in the HeartSteps study <ref type="bibr" target="#b48">[44]</ref> for one user should be different from that in the alcohol control <ref type="bibr" target="#b12">[8,</ref><ref type="bibr" target="#b13">9]</ref> study.</s></p><p xml:id="_ePAq7dd"><s xml:id="_ehr3vvY">learn the network cohesion targeted to that mHealth study.</s><s xml:id="_FQjWVDU">However, the MDP models are unknown to the RL problem.</s><s xml:id="_v3dbRRR">Instead, the warm start trajectories (WSTs) of all the N users are available, which provide the observation of users.</s><s xml:id="_KPdY2GX">Thus, we use the WSTs for the graph learning, i.e.</s></p><formula xml:id="formula_15">D (0) = D (0) n | n = 1, • • • , N , where D (0) n = {(s i,n , a i,n, r i,n )} T0 i=1</formula><p xml:id="_3WV6B8S"><s xml:id="_JT6d6ed">is the WST for the n-th user.</s><s xml:id="_3zt8yNK">Since an MDP consists of the state transistion and immediate reward model, the feature for the cohesion network learning is constructed by stacking the states and rewards in the WST as follows</s></p><formula xml:id="formula_16">v n = s 1,n , r 1,n , • • • , s T0,n , r T0,n ∈ R pT0+T0 ,<label>(10)</label></formula><p xml:id="_mf2DwZk"><s xml:id="_RZzcM3s">for n ∈ {1, • • • , N }.</s><s xml:id="_39rQt3F">Note that the action or policy is not part of an MDP.</s><s xml:id="_ZD6czrs">To reduce the influence of random actions in the WST, we get rid of the temporal order by sorting all the elements in v n <ref type="bibr" target="#b14">(10)</ref>.</s><s xml:id="_VhVSTYQ">Then the benchmark method, i.e.</s><s xml:id="_W2nUJpS">Knearest neighbor (KNN), is used to learn the neighboring information among users</s></p><formula xml:id="formula_17">c ij = 1, if v i ∈ N (v j ) or v j ∈ N (v i ) 0, otherwise ,<label>(11)</label></formula><p xml:id="_thMRErN"><s xml:id="_KxcnFUg">where v i ∈ N (v j ) indicates that i-th node is the KNN of the j-th node <ref type="bibr" target="#b49">[45]</ref>; <ref type="bibr" target="#b15">(11)</ref> is an undirected Graph.</s><s xml:id="_bZbhdAZ">The value of K controls how widely the users are connected.</s><s xml:id="_Akkj27z">A large K indicates a wide connection among users and vice versa.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_abmJrun">Model of cohesion based Actor-Critic RL</head><p xml:id="_hyjQSpf"><s xml:id="_qXbHtWe">The underlying assumption throughout this paper is that if two users are connected, their values and policies are constrained to be similar, e.g.</s><s xml:id="_fszcgqx">w i -w j and θ i -θ j are small if i ↔ j <ref type="bibr" target="#b47">[43,</ref><ref type="bibr" target="#b50">46]</ref>.</s><s xml:id="_VV77cVY">With the network cohesion C = (c ij ) N ×N , the objective function for the critic updating is formed as follows</s></p><formula xml:id="formula_18">W = H = arg min H N n=1 U i ∈Dn</formula><p xml:id="_un4MKSF"><s xml:id="_gMGskkA">x i hn -(ri + γy i wn)</s></p><p xml:id="_9gVpF8F"><s xml:id="_gJQnMHr">2 2 (12) s.t.</s><s xml:id="_GY4TmyZ">N i,j=1 cijd (hi, hj) ≤ δ1 and N i,j=1 cijd (wi, wj) ≤ δ2,</s></p><p xml:id="_h9F3vMb"><s xml:id="_eydY8C6">where</s></p><formula xml:id="formula_19">H = [h 1 , • • • , h N ] ∈ R u×N and W = [w 1 , • • • , w N ] ∈ R u×N are</formula><p xml:id="_6SzynuK"><s xml:id="_KkmexjJ">designed matrices that consist of all the N users' variables (each column summarizes the unknown varibile of one user); d (h i , h j ) is a distance measure between two vectors; usually we set d (•, •) as the Euler distance.</s><s xml:id="_M8a74vj">With the matrix notations in Section 2.3, we turn (12) into the following two-level nested optimization problems</s></p><formula xml:id="formula_20">H = arg min H N n=1 X n h n -(r n + γY n w n ) 2 2 + (13) µ 1 N i,j=1 c ij h i -h j 2 2 + ζ 1 N n=1 h n 2 2 , W = arg min W N n=1 Φ n w n -Φ n h n 2 2 + (<label>14</label></formula><formula xml:id="formula_21">)</formula><formula xml:id="formula_22">µ 2 N i,j=1 c ij w i -w j 2 2 + ζ 2 N n=1 w n 2 2 ,</formula><p xml:id="_ZkVJESE"><s xml:id="_teVbMEA">where Φ n is a designed matrix to facilitate the optimization of ( <ref type="formula" target="#formula_20">14</ref>).</s><s xml:id="_k95Uz72">The 1st level <ref type="bibr" target="#b17">(13)</ref> projects the Bellman image onto a linear space (we refer (13) as the projection step); the 2nd level ( <ref type="formula" target="#formula_20">14</ref>) deals with the fixed point problem (i.e. the fixedpoint step) <ref type="bibr" target="#b51">[47]</ref>.</s></p><p xml:id="_X3UHHBb"><s xml:id="_54Q7rNY">The objective for the actor updating is defined as follows</s></p><formula xml:id="formula_23">θ 1 , • • • , θ n , • • • , θ N = arg max {θn} N n=1 J (θ 1 , • • • , θ N ) ,<label>(15)</label></formula><p xml:id="_wU4zmwU"><s xml:id="_pqMsUrw">where</s></p><formula xml:id="formula_24">Θ = [θ 1 , • • • , θ N ], Q (s i , a; w θn ) = x (s i , a)</formula><p xml:id="_nAgzyB8"><s xml:id="_qNCYnGW">T w θn is the estimated value for the n-th policy π θn and</s></p><formula xml:id="formula_25">J (Θ) = N n=1   1 |D n | Ui∈Dn a∈A Q (s i , a; w θn ) π θn (a|s i )   - µ 3 2 N i,j=1 c ij θ i -θ j 2 2 - ζ 3 2 N n=1 θ n 2 2 . (<label>16</label></formula><formula xml:id="formula_26">)</formula><p xml:id="_z4g8AWt"><s xml:id="_2F95JMK">Although we are able to obtain a closed-form solution for the critic updating <ref type="bibr" target="#b16">(12)</ref>, to reduce the computational costs, we substitute the solution in value for { w n } N n=1</s></p><p xml:id="_2mynbch"><s xml:id="_Bgw5VRT">rather than the closed-form expression of { w θn } N n=1 into the actor updating.</s><s xml:id="_UtjVRYT">The actor updating algorithm performs the maximization of ( <ref type="formula" target="#formula_25">16</ref>) over Θ, which is computed via the Sequential Quadratic Programming (SQP) algorithm.</s><s xml:id="_BgNMSr2">We use the implementation of SQP with finite-difference approximation to the gradient in the FMINCON function of Matlab.</s></p><p xml:id="_uGc28Bc"><s xml:id="_hUC37sZ">In the objectives ( <ref type="formula">13</ref>), ( <ref type="formula" target="#formula_20">14</ref>) and ( <ref type="formula" target="#formula_25">16</ref>), µ 1 , µ 2 and µ 3 are the tuning parmaters to control the strength of the network cohesion constraints.</s><s xml:id="_sEV2FBF">It is an advantage of our methods over the network based bandit <ref type="bibr" target="#b21">[17]</ref>.</s><s xml:id="_953bSZ4">When µ 1 , µ 2 , µ 3 → ∞, the connected users are enforced to have identical values and policies.</s><s xml:id="_q9vPsfC">When µ 1 , µ 2 , µ 3 = 0, there is no network cohesion constraint.</s><s xml:id="_RTm8EDC">In such case, our method is equivalent to the separate online RL method.</s><s xml:id="_FsE4PkM">Compared with the Separate-RL, the model complexity of our methods is reduced since their parameter domain is constrained via the network cohesion regularization.</s><s xml:id="_HBxYY7y">Such case ensures our methods to work well when the sample size is small.</s><s xml:id="_drSK75A">However, the optimization of our method is much more complex than that of the separate-RL.</s><s xml:id="_awm8pqr">The updating rules of all the users are independent with each other in the separate-RL; while in our method, the optimization of all the users is all coupled together.</s><s xml:id="_JH8Y8cv">In the following section, two actor-critic RL algorithms are proposed to deal the objectives ( <ref type="formula">13</ref>) and ( <ref type="formula" target="#formula_20">14</ref>).</s><s xml:id="_hkkRTTe"><ref type="bibr" target="#b17">(13)</ref> We first discuss how to minimize the objective for the projection step.</s><s xml:id="_nkxThn3">The objective is</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_RRFRvhq">ALGORITHM#1 FOR THE CRITIC UPDATE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_ZWecg5n">Updating Rules for the Projection Step</head><formula xml:id="formula_27">J = N n=1 X n h n -(r n +γY n w n ) 2 2 +µ 1 Tr (HLH )+ζ 1 H 2 F , (17) where • 2 F is the Frobenius norm of a matrix, Tr (HLH ) = N i=1 N j=1 c ij h i -h j 2 2 (18)</formula><p xml:id="_rZeUjSb"><s xml:id="_fMWtTtp">Algorithm 1 Two online actor-critic algorithms for the Cohesion-RL Input: T, T 0 , µ {1,2,3} , ζ {1,2,3} , nAlg (i.e. the algorithm index).</s></p><p xml:id="_TH4XNVc"><s xml:id="_BJUygKc">1: Initialize the states (s t,n ) p×N , where t = 0, and the policy parameters</s></p><formula xml:id="formula_28">Θ = [θ 1 , • • • , θ N ] ∈ R m×N for N users. 2: for n = 1, • • • , N do 3: for t = 1, • • • , T do 4:</formula><p xml:id="_RzAvWe5"><s xml:id="_yVfbRJ9">At time point t, observe context s t,n for the n-th user.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KW2X6DU">5:</head><p xml:id="_aZEJWPY"><s xml:id="_rnAVd53">Draw an action a t,n according to the policy π θn (a|s t,n ).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZrY2Ema">6:</head><p xml:id="_AftkWv8"><s xml:id="_XpdG8hx">Observe an immediate reward r t,n .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_y8aYShx">7:</head><p xml:id="_PKWrsH3"><s xml:id="_hVsM42e">end for 8:</s></p><formula xml:id="formula_29">if t = T 0 then 9:</formula><p xml:id="_UF5tBUs"><s xml:id="_d2P5HCp">Construct the network cohesion via <ref type="bibr" target="#b14">(10)</ref>, <ref type="bibr" target="#b15">(11)</ref> Data preparation and feature construction (6) for the critic update and the actor update.</s></p><p xml:id="_8YBbhXa"><s xml:id="_qjnrJ29">12:</s></p><formula xml:id="formula_30">if nAlg = 1 then 13:</formula><p xml:id="_waWEn4a"><s xml:id="_EaCnr6t">Critic update to learn W t (value parameter) via <ref type="bibr" target="#b28">(24)</ref>.</s><s xml:id="_Y23TvXp">Critic update to learn W t (value parameter) via <ref type="bibr" target="#b30">(26)</ref>.</s></p><p xml:id="_5erxSq8"><s xml:id="_5MF46w7">16: end if 17:</s></p><p xml:id="_mmwGMuS"><s xml:id="_marhwDW">Actor update to learn Θ t (policy paramter) via <ref type="bibr" target="#b19">(15)</ref>.</s><s xml:id="_PghSCXp">and L = D -C ∈ R N ×N is a graph laplacian; D is a diagonal matrix whose elements are column (or row, as C is a symmetric matrix) sums of C, i.e. d ii = i c ij .</s><s xml:id="_8ue3VxF">The partial derivative of J 1 , i.e. the 1st term in <ref type="bibr" target="#b21">(17)</ref>, with respect to h n is</s></p><formula xml:id="formula_31">∂J 1 ∂h n = 2X n X n h n -2X n r n -2γX n Y n w n .<label>(19)</label></formula><p xml:id="_BDcx7hc"><s xml:id="_VxUHe4b">Summarizing the partial derivatives with respect to all the variables in H</s></p><formula xml:id="formula_32">= (h 1 , • • • , h N ) ∈ R K×N , we have ∂J 1 ∂vec (H) = 2 N n=1 E n ⊗ (X n X n ) vec (H) -2 N n=1 E n ⊗ X n vec (R) -2γ N n=1 E n ⊗ (X n Y n ) vec (W) (20)</formula><p xml:id="_SbXwsXr"><s xml:id="_VzBb7tH">where vec</s></p><formula xml:id="formula_33">(H) = [h 1 , • • • , h N ] ∈ R uN is the vectorization process for a matrix; E n = diag (0, • • • , 1, • • • , 0) ∈ R N ×N</formula><p xml:id="_haTKhav"><s xml:id="_vhYfgxC">is a diagonal matrix with the n-th diagonal element equal to 1, all the other equal to zero; ⊗ indicates the Kronecker product between two matrices resulting in a block matrix.</s></p><p xml:id="_pGAKPC8"><s xml:id="_28ZCpvk">Let</s></p><formula xml:id="formula_34">F 1 = n E n ⊗ (X n X n ), F 2 = n E n ⊗ X n and F 3 = n E n ⊗ (X n Y n ).</formula><p xml:id="_YqzQ7Ef"><s xml:id="_rr6eWK8">We have a simpler formulation for the ∂J 1 /∂vec (H) as follows</s></p><formula xml:id="formula_35">∂J 1 ∂vec (H) = 2F 1 vec (H) -2 [F 2 vec (R) + γF 3 vec (W)] .</formula><p xml:id="_wV2bkDv"><s xml:id="_87peRUa">(21) The partial derivatives of the 2nd term in <ref type="bibr" target="#b21">(17)</ref></s></p><formula xml:id="formula_36">, i.e. J 2 = µ 1 Tr (HLH ) + ζ 1 H 2 F , with respect to H is ∂J 2 ∂H = 2µ 1 HL + 2ζ 1 H.</formula><p xml:id="_3CYCVrC"><s xml:id="_GHvgqMr">According to the Encapsulating Sum <ref type="bibr" target="#b52">[48]</ref>, we have</s></p><formula xml:id="formula_37">∂J 2 ∂vec (H) = 2 [(µ 1 L + ζ 1 I N ) ⊗ I u ] vec (H)<label>(22)</label></formula><p xml:id="_Vm4aYnz"><s xml:id="_pwXAVfD">where I N ∈ R N ×N and I u ∈ R u×u are identical matrices.</s></p><p xml:id="_cbNddsZ"><s xml:id="_f5u6Yzk">Setting the gradient of J in <ref type="bibr" target="#b21">(17)</ref> with respect to vec (H) to zero gives the closed-form solution for the projection step as follows <ref type="bibr" target="#b18">(14)</ref> Considering the 1st term in the fixed point step <ref type="bibr" target="#b18">(14)</ref> gives</s></p><formula xml:id="formula_38">vec H = [F 1 + L ⊗ (µ 1 , ζ 1 )] -1 [F 2 vec (R) + γF 3 vec (W)] , (<label>23</label></formula><formula xml:id="formula_39">) where L ⊗ (µ 1 , ζ 1 ) = (µ 1 L + ζ 1 I N ) ⊗ I u ∈ R uN ×uN .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_vKc7uXV">Updating Rules for the Fixed Point step</head><formula xml:id="formula_40">O 1 = n Φ n w n -Φ n h n 2 2 = Φ ⊗ vec (W) -Φ ⊗ vec H 2 2 where Φ ⊗ = N n=1 E n ⊗ Φ n . To facilitate the optimiza- tion, we design {Φ n } N n=1 to let Φ ⊗ = F 1 +L ⊗ (µ 1 , ζ 1 ), which leads to O 1 = Φ ⊗ vec (W) -Φ ⊗ vec H 2 2 = Φ ⊗ vec (W) -[F 2 vec (R) + γF 3 vec (W)] = (Φ ⊗ -γF 3 ) vec (W) -F 2 vec (R) 2 2 ,</formula><p xml:id="_ubJjyKt"><s xml:id="_wcg9pHJ">and finally results in an easy solution for the critic updating (24) (cf.</s><s xml:id="_24pgHm8">Theorem 1).</s><s xml:id="_RTkSHEH">Letting P = Φ ⊗ -γF 3 , we have</s></p><formula xml:id="formula_41">O 1 = Pvec (W) -F 2 vec (R) 2 2 .</formula><p xml:id="_hHTfxfZ"><s xml:id="_CyDsGng">The partial derivative of O 1 with respect to vec (W) is</s></p><formula xml:id="formula_42">∂O 1 ∂vec (W) = 2P Pvec (W) -2P F 2 vec (R) .</formula><p xml:id="_qCeZkFB"><s xml:id="_yB3f8E3">Considering the partial derivative of the cohesion constraint and the Frobenius norm based smooth constraint with respect to vec (H), and setting the overll partial derivative to zero, i.e. ∂O/∂vec (W) = 0, we can obtain the following closed-form solution</s></p><formula xml:id="formula_43">vec (W * ) = [P P + L ⊗ (µ 2 , ζ 2 )] -1 P F 2 vec (R) , (<label>24</label></formula><formula xml:id="formula_44">)</formula><p xml:id="_gWn8XT7"><s xml:id="_nJEJK37">where</s></p><formula xml:id="formula_45">L ⊗ (µ 2 , ζ 2 ) = (µ 2 L + ζ 2 I N ) ⊗ I u . Theorem 1. B = P P + L ⊗ (µ 2 , ζ 2</formula><p xml:id="_H2QAyg2"><s xml:id="_ezRp2BJ">) is a symmetric and positive definite matrix, which leads to an easy critic updating rule in <ref type="bibr" target="#b28">(24)</ref>.</s></p><formula xml:id="formula_46">Lemma 2. Suppose that A ∈ R n×n and B ∈ R m×m are square matrices. Let λ 1 , • • • , λ n be the eigenvalues of A and ν 1 , • • • , ν m be those of B.</formula><p xml:id="_gnYN6es"><s xml:id="_C6WfQwy">Then the eigenvalues of A ⊗ B <ref type="bibr" target="#b53">[49]</ref>, where ⊗ is the Kronecker Product, are</s></p><formula xml:id="formula_47">λ i ν j , for i ∈ {1, • • • , n} , j ∈ {1, • • • , m} .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_s9K927s">ALGORITHM#2 FOR THE CRITIC UPDATE</head><p xml:id="_q37BqH2"><s xml:id="_x5QUWHG">In this section, we provide another updating rule for the critic update (i.e. policy improvement).</s><s xml:id="_S8kTdQq">Note that to prevent the overfitting when the sample size is very small, the conventional LSTDQ usually employs the ℓ 2 constraint on the variable H in the projection step.</s><s xml:id="_KqgbuKS">They do not put the ℓ 2 constraint on the fixed-point variable W <ref type="bibr" target="#b34">[30,</ref><ref type="bibr" target="#b51">47]</ref>.</s><s xml:id="_DvqhVT7">Following this idea, we have a simpler objective function for the critic update as</s></p><formula xml:id="formula_48">w n = h n = arg min hn Ui∈Dn x i h n -(r i + γy i w n ) 2 2 , (<label>25</label></formula><formula xml:id="formula_49">)</formula><formula xml:id="formula_50">for n ∈ {1, • • • , N } and s.t. N i,j=1 c ij d (h i , h j ) ≤ δ 1 .</formula><p xml:id="_z9M8W5G"><s xml:id="_u7zaVbA">According to the derivation in Section 4.1 that considers the Frobenius norm based smooth constraint, the updating rule for the projection step is <ref type="bibr" target="#b27">(23)</ref>.</s><s xml:id="_pKWBxNk">In the fixed-point step, the objective is simply w n = h n (i.e. a fixed-point problem), which leads to vec (W) = vec H .</s></p><p xml:id="_6yETQta"><s xml:id="_VPmZ3Bt">Thus, we have the closed-form solution for vec (W) as follows</s></p><formula xml:id="formula_51">vec W = [F 1 -γF 3 + L ⊗ (µ 1 , ζ 1 )] -1 F 2 vec (R) . (<label>26</label></formula><formula xml:id="formula_52">)</formula><p xml:id="_BHfXdKW"><s xml:id="_rEsKkZz">It is simpler than the 1st updating rule for the critic update <ref type="bibr" target="#b28">(24)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_H8e2AGS">EXPERIMENT RESULTS</head><p xml:id="_dZkFAZK"><s xml:id="_fTywR9r">We verify the proposed methods on the HeartSteps dataset.</s><s xml:id="_B86BzeC">It has two choices for an action, i.e. {0, 1} , where a = 1 means sending the positive intervention, while a = 0 indicates no intervention <ref type="bibr" target="#b7">[3]</ref>.</s><s xml:id="_mkDDjzt">Specifically, the stochastic policy is assumed to be in the form</s></p><formula xml:id="formula_53">π θ (a | s) = exp[-θ φ(s,a)] a exp[-θ φ(s,a )]</formula><p xml:id="_gJxk7ZJ"><s xml:id="_W8Zn5xE">, where θ ∈ R m is the unknown parameter and φ (•, •) is a feature process that combines the information in actions and states, i.e. φ (s, a) = [as , a] ∈ R m .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" xml:id="_guEQFTg">The HeartSteps Dataset</head><p xml:id="_ACK4Fvp"><s xml:id="_ppW9j2s">To verify the performance of our method, we use a dataset from a mobile health study, called HeartSteps <ref type="bibr" target="#b48">[44]</ref>, to approximate the generative model.</s><s xml:id="_K8g8Dxv">This is a 42-day mHealth intervention that aims to increase the users' steps they take each day by providing positive treatments (i.e.</s><s xml:id="_Pc4wwHK">interventions), which are adapted to users' ongoing status, such as suggesting users to take a walk after long sitting <ref type="bibr" target="#b48">[44]</ref>, or to do some exercises after work.</s></p><p xml:id="_NetMKzm"><s xml:id="_kv9BQxg">A trajectory of T tuples D = {(s i , a i, r i ) | i = 1, • • • , T } are generated for each user <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b7">3]</ref>.</s><s xml:id="_UEp2Yhz">The initial state is drawn from the Gaussian distribution S 0 ∼ N p {0, Σ}, where Σ is a p × p covariance matrix with pre-defined elements.</s><s xml:id="_sHc3sKw">The action a t for 0 ≤ t ≤ T 0 is drawn from the random TABLE <ref type="table">1</ref> The performance of 3 online RLs vs. the rising trajectory length T ∈ {50, 150}, for the experiment setting (S1).</s></p><p xml:id="_ddaTzwm"><s xml:id="_ddvjmc8">policy, with a probability of 0.5 to provide interventions, i.e. µ (1 | s) = 0.5 for all states s.</s><s xml:id="_6GDhTwS">Such process is called drawing warm start trajectory (WST) via the micro-randomized trials <ref type="bibr" target="#b8">[4,</ref><ref type="bibr" target="#b48">44]</ref>, and T 0 is the length of the WST.</s><s xml:id="_GWXmtcg">When t ≥ T 0 , we start the actor-critic updating, and the action is drawn from the learned policy, i.e. a t ∼ π θt (• | s t ).</s><s xml:id="_pX2fnjK">When t ≥ 1, the state and immediate reward are generated as follows</s></p><formula xml:id="formula_54">S t,1 = β 1 S t-1,1 + ξ t,1 , S t,2 = β 2 S t-1,2 + β 3 A t-1 + ξ t,2 ,<label>(27)</label></formula><formula xml:id="formula_55">S t,3 = β 4 S t-1,3 + β 5 S t-1,3 A t-1 + β 6 A t-1 + ξ t,3 , S t,j = β 7 S t-1,j + ξ t,j , for j = 4, . . . , p R t = β 14 × [β 8 + A t × (β 9 + β 10 S t,1 + β 11 S t,2 ) (28) + β 12 S t,1 -β 13 S t,3 + t ],</formula><p xml:id="_Gh2vg5q"><s xml:id="_xvRupee">where β = {β i } 14 i=1 is the main parameter for the MDP and -β 13 S t,3 is the treatment fatigue; {ξ t,i } p i=1 ∼ N 0, σ 2 s is the noise in the state <ref type="bibr" target="#b31">(27)</ref> and t ∼ N 0, σ 2 r is the noise in the reward model <ref type="bibr" target="#b32">(28)</ref>.</s></p><p xml:id="_bTPcguJ"><s xml:id="_eyWHWKG">As it is known to us, the individuals are generally more or less different from each other, and each individual is similar to a part, but not all, of the individuals.</s><s xml:id="_uKjwKsf">In the mHealth and RL study, an individual is abstracted as an MDP, which is determined by the value of β, cf. ( <ref type="formula" target="#formula_54">27</ref>) and <ref type="bibr" target="#b32">(28)</ref>.</s><s xml:id="_tT3ErHE">To achieve a more practical dataset compared with <ref type="bibr" target="#b5">[1,</ref><ref type="bibr" target="#b7">3,</ref><ref type="bibr" target="#b8">4]</ref>, we come up with a method to generate N users (i.e.</s><s xml:id="_5d8B9sn">βs) that satisfy the above requirements in two steps: (a) manually design V basic βs, i.e.</s></p><formula xml:id="formula_56">β basic v | v = 1, • • • , V ,</formula><formula xml:id="formula_57">β i = β basic v + δ i , for i ∈ {1, 2, • • • , N v },</formula><p xml:id="_bApghxF"><s xml:id="_Q6qgAve">where δ i ∼ N (0, σ b I 14 ) is the noise in the MDPs and I 14 ∈ R 14×14 is an identity matrix.</s><s xml:id="_6EwCnWy">After such processing, the individuals are all different from the others.</s><s xml:id="_7qe3uXe">The value of σ b specifies how different the individuals are.</s><s xml:id="_U45tR6T">In the experiments, the number of groups is set as V = 3 (each group has N v = 15 people, leading to N = 45 users involved in the experiment).</s><s xml:id="_9eCBYre">The β basic 's for the V groups are set as follows</s></p><formula xml:id="formula_58">β basic 1 =[0.</formula><p xml:id="_KPDa4Wk"><s xml:id="_caDdrWn">40, 0.25, 0.35, 0.65, 0.10, 0.50, 0.22, 2.00, 0.15, 0.20, 0.32, 0.10, 0.45, 800] β basic 2 =[0.35, 0.30, 0.30, 0.60, 0.05, 0.65, 0.28, 2.60, 0.35, 0.45, 0.45, 0.15, 0.50, 650] β basic 3 =[0.20,</s><s xml:id="_DMxHzmg">0.50, 0.20, 0.62, 0.06, 0.52, 0.27, 3.00, 0.15, 0.15, 0.50, 0.16, 0.70, 450].</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_9Vxwch3">Compared Methods and Parameter Settings</head><p xml:id="_Nn4ymJp"><s xml:id="_jTmxQDd">There are three online actor-critic RL methods for the comparison: (a) Separate-RL, which is an extension of the online actor-critic contextual bandit in <ref type="bibr" target="#b5">[1]</ref> to the online actor-critic reinforcement learning.</s><s xml:id="_vUvfv6q">It learns a separate RL policy for each user by only using his or her data.</s><s xml:id="_DJ9HhhS">1,238.4</s><s xml:id="_gpb77Tc">1,239.3</s><s xml:id="_cWTNX8n">1,238.6 1,239.5 1,332.3</s><s xml:id="_adKV5ep">1,341.1 1,343.3</s><s xml:id="_pTMTDNE">1,342.1 1,394.4</s><s xml:id="_k7WfYDs">1,395.6 1,396.8</s><s xml:id="_kSSHZ9A">1,397.4</s></p><p xml:id="_UJmtaC9"><s xml:id="_XV4UGuc">(a) Discount Factor when γ = 0 Average Reward Separate-RL Cohesion-RL#1 Cohesion-RL#2 T = 50 T = 80 T = 110 T = 150 1 3 0 0 1 3 5 0 1 4 0 0 1 4 5 0 1 5 0 0 1 5 5 0 1 6 0 0</s></p><p xml:id="_3ncmD6A"><s xml:id="_kqqeTtQ">1,346.3 1,374.4</s><s xml:id="_taB5PVt">1,386 1,388.5 1,488.4</s><s xml:id="_TasbtJe">1,497.7 1,503.1 1,502.6 1,505.1 1,508.9</s><s xml:id="_xMUMpBm">1,513.3</s><s xml:id="_6jn3VYd">1,515.3</s></p><p xml:id="_tHxxvm7"><s xml:id="_ENDPKgX">(b) Discount Factor when γ = 0.6 Average Reward Separate-RL Cohesion-RL#1 Cohesion-RL#2 T = 50 T = 80 T = 110 T = 150 1 2 0 0 1 3 0 0 1 4 0 0 1 5 0 0 1 6 0 0</s></p><p xml:id="_qw2QhXT"><s xml:id="_JwTkuKa">1,265.2 1,305.5 1,312.4</s><s xml:id="_vqDcrej">1,315.2</s><s xml:id="_PVkW2nK">1,542.7 1,556.3</s><s xml:id="_ZRfMjnc">1,565.8</s><s xml:id="_VkzHwFJ">1,570 1,556.5 1,564.7 1,574.6 1,577.8</s><s xml:id="_B2vxU5w">(c) Discount Factor when γ = 0.95 Average Reward Separate-RL Cohesion-RL#1 Cohesion-RL#2 T0 = 5 T0 = 10 T0 = 15 T0 = 20 1 1 0 0 1 2 0 0 1 3 0 0 1 4 0 0 1 5 0 0</s></p><p xml:id="_wtnT2x3"><s xml:id="_tZHXKJj">1,194 1,239.3</s><s xml:id="_mBdTbwn">1,256 1,380.8</s><s xml:id="_e3ppSnK">1,324.7 1,343.2</s><s xml:id="_ZWqWSWd">1,354.8</s><s xml:id="_ju8wQXR">1,358.9</s><s xml:id="_pHSEUzj">1,396.7 1,397.2</s><s xml:id="_BC3GzMx">1,398.4</s><s xml:id="_7crdRyt">1,399.1</s></p><p xml:id="_xef4P2z"><s xml:id="_sK6PBNZ">(a) Discount Factor when γ = 0 Average Reward Separate-RL Cohesion-RL#1 Cohesion-RL#2 T0 = 5 T0 = 10 T0 = 15 T0 = 20 1 2 0 0 1 3 0 0 1 4 0 0 1 5 0 0 1 6 0 0</s></p><p xml:id="_35EdVuM"><s xml:id="_4cVy3RH">1,254.1 1,374.4</s><s xml:id="_zksYP3m">1,432.2</s><s xml:id="_3UWemAn">1,463.6 1,487.9</s><s xml:id="_mf5QMvM">1,494.4</s><s xml:id="_URbQvrg">1,488.6 1,505.7 1,515.2</s><s xml:id="_SS6hwPd">1,508.8</s><s xml:id="_EH7dGck">1,510.7 1,516.8</s><s xml:id="_SaVB89Q">1,245.1 1,305.5 1,392.9</s><s xml:id="_vCrHHjh">1,434.4</s><s xml:id="_Guwwcyy">1,554.7 1,554.1 1,559.1 1,564.1 1,568.5 1,577 1,569.8</s><s xml:id="_8dSNJtE">1,574.6</s><s xml:id="_Dj3eSjk">(c) Discount Factor when γ = 0.95 Average Reward Separate-RL Cohesion-RL#1 Cohesion-RL#2 (b) the length of warm start trajectory is set as T 0 = 10; (c) to reduce the number of parameters in the algorithm, the parameters for the cohesion constraint in our methods are set as µ 1 = 0.1, µ 3 = µ 1 and µ 2 = 0.01µ 1 .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" xml:id="_AbheyTX">Evaluation Metrics</head><p xml:id="_bvF7Uun"><s xml:id="_sywfjYb">We use the expectation of long run average reward (ElrAR) E [η π Θ ] to quantify the quality of the estimated policy π Θ on a set of N =45 individuals.</s><s xml:id="_bCkTwnQ">Here π Θ summarizes the policies for all the 45 users, in which π θn is the n-th user's policy.</s><s xml:id="_a4SGnRx">Intuitively, ElrAR measures how much average reward in the long run we could totally get by using the learned policy π Θ on the testing users (i.e.</s><s xml:id="_TGpQ9Mt">MDPs), for example measuring how much alcohol users have in a fixed time period in the alcohol use study <ref type="bibr" target="#b12">[8,</ref><ref type="bibr" target="#b13">9]</ref>.</s><s xml:id="_2dJ2Rhb">Specifically in the HeartSteps, ElrAR measures the average steps that users take per day over a long time; a larger ElrAR corresponds to a better performance.</s><s xml:id="_rPrZf9j">The average reward for the n-th user, i.e. η π θn , is calculated by averaging the rewards over the last 4, 000 elements in a trajectory of 5, 000 tuples under the policy π θn , i.e. η π θn = 1</s></p><p xml:id="_vDAFTxm"><s xml:id="_NdpUqfh">T -i T j=i R s j,n , a j,n ∼ π θn , where T = 5000 and i = 1000.</s><s xml:id="_KK3dntr">Then ElrAR E [η π Θ ] is approximated by averaging over the 45 η π θn 's, i.e.</s><s xml:id="_aSUguWa">E [η π Θ ] ≈ 1 N N n=1 η π θn .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4" xml:id="_MZEVrnA">Comparisons in three experiment settings</head><p xml:id="_zj3QMSx"><s xml:id="_BXNXZfK">The following experiments are carried out to verify different aspects of the three online actor-critic RL algorithms:</s></p><p xml:id="_yRVdYbh"><s xml:id="_thBAaen">(S1) In this part, the trajectory length of all users ranges as T ∈ {50, 80, 110, 150}.</s><s xml:id="_8GGSgFa">The experiment results are showed in Table <ref type="table">1</ref> and Fig. <ref type="figure" target="#fig_11">2</ref>.</s><s xml:id="_mVRMNJz">There are two sub-tables in Table <ref type="table">1</ref>; each sub-table displays the ElrAR of three RL methods (i.e.</s><s xml:id="_ChVKUe8">Separate-RL, Cohesion-RL#1 and Cohesion-RL#2 respectively) under six γ settings; the last row shows the average ElrAR over the results of all the six γs.</s><s xml:id="_eHsKnYt">In Fig. <ref type="figure" target="#fig_11">2</ref>, there are three sub-figures; each sub-figure illustrates the results of three methods under one γ setting.</s><s xml:id="_9Gja6GQ">As we shall see that the performance of three methods generally increases as T rises.</s><s xml:id="_eCxrvdR">The performance of our RL methods, i.e.</s><s xml:id="_dZmK5ym">Cohesion-RL#1 and Cohesion-RL#2, have an obvious advantage over the Separate-RL under all the parameters settings in (S1).</s><s xml:id="_NaZwNqn">Besides, the advantage of our methods over Separate-RL slowly decreases as T rises.</s><s xml:id="_Txhnu8u">Compared with Separate-RL, our methods averagely improve 156.0 steps and 188.3 steps when T = 50, and averagely improve 136.3 steps and 163.7 steps when T = 150.</s></p><p xml:id="_hKFmYyh"><s xml:id="_kpXpeRk">(S2) In this part, the length of warm start trajectory ranges as T 0 = {5, 10, 15, 20}, which indicates that the RL methods wait longer and longer before starting the online learning.</s><s xml:id="_xqrWyM9">The experiment results are summarized in Table <ref type="table">2</ref> and Fig. <ref type="figure" target="#fig_11">2</ref>. As we shall see that as T 0 rises across this range, the performance of Separate-RL increases dramatically and Cohesion-RL#1 rises gradually, while Cohesion-RL#2 remains stable.</s><s xml:id="_PdCScPP">Thus, the average advantage of our method over Separate-RL decreases dramatically as T 0 rises, i.e., from 224.07 steps and 261.67 steps when T 0 = 5 to 33.26 steps and 52.09 steps when T 0 = 20.</s><s xml:id="_NWsXSV2">Such case suggests that our methods work perfectly when the WST is very short.</s><s xml:id="_mjzhTw9">In this case, the mining of network cohesion is necessary for the online RL learning.</s><s xml:id="_fs6JB92">In general, however, our methods still outperform Separate-RL significantly.</s></p><p xml:id="_a9fwJRV"><s xml:id="_aDYZv8V">(S3) The parameter of the Network-Cohesion constraint µ 1 for the projection step ranges from 0.001 to 10.</s><s xml:id="_qZMuHrQ">To reduce the number of parameters in our algorithm, we simply set µ 2 = 0.01µ 1 (i.e. the cohesion constraint for the fixed-point step) and µ 3 = µ 1 (i.e. the cohesion constraint for the actor updating).</s><s xml:id="_wrNWuf9">The experiment results are illustrated in Fig. <ref type="figure">3</ref>, where there are three sub-figures.</s><s xml:id="_aetf4ds">Each sub-figure shows the results of three online RLs vs. five µ 1 settings under one γ.</s><s xml:id="_BNZGH2Y">As we shall see that as µ 1 rises across this range, our method always obtains superior performance compared with Separate-RL.</s><s xml:id="_mmYQYsK">Specially, Cohesion-RL#2 is very stable and always better than Cohesion-RL#1.</s><s xml:id="_aBJvJRJ">Such case indicates that it is reliable to follow the idea on how to introduce the ℓ 2 constraint in LSTDQ.</s><s xml:id="_4Twhj2H">In Fig. <ref type="figure">3</ref>, since Separate-RL does not have the Network-Cohesion constraint, its result keeps unchanged.</s></p><p xml:id="_gbWSY2Y"><s xml:id="_e8q58zJ">Consider (S1) and (S2) for the Separate-RL, we find: (a) the lack of samples at the beginning of the online learning may bias the optimization direction, which badly influence the performance even when the trajectory is very long; (b) Compared with T , the increase of T 0 has a more important influence on the performance.</s><s xml:id="_NtbubVN">In (S1), where T 0 = 10 is fixed and T ranges from T = 50 to T = 150, the performance of Separate-RL increases 32.74 steps.</s><s xml:id="_xcpQYxd">In (S2), where T = 80 is fixed and T 0 rises from T 0 = 5 to T 0 = 20, Separate-RL achieve an improvement of 210.51 steps, which is much significant than the rise caused by the rising T .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_NQedRVy">CONCLUSIONS AND DISCUSS</head><p xml:id="_CvweJXH"><s xml:id="_Cbh4dDj">This paper presents a first attempt to employ the online actor-critic reinforcement learning for the mHealth.</s><s xml:id="_Q99cAfg">Following the current methods that learn a separate policy for each user, the Separate-RL can not achieve satisfactory results.</s><s xml:id="_vS6z9cr">This is due to that data for each user is very limited in size to support the separate learning, leading to unstable policies that contain lots of variances.</s><s xml:id="_kUTs4V5">After considering the universal phenomenon that users are generally connected in a network and linked users tend to have similar behaviors, we propose a network cohesion constrained actorcritic reinforcement learning for mHealth.</s><s xml:id="_w3znqxP">It is able to share the information among similar users to convert the limited user information into sharper learned policies.</s><s xml:id="_Tt7UANb">Extensive experiment results demonstrate that our methods outperform the Separate-RL significantly.</s><s xml:id="_THHdWgB">We find it easy to apply the proposed methods to other health-related tasks.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>. 10 :</head><label>10</label><figDesc><div><p xml:id="_N2gWGS2"><s xml:id="_xmuQAyQ">else if t ≥ T 0 then 11:</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>14 :</head><label>14</label><figDesc><div><p xml:id="_Nse9bMd"><s xml:id="_J4Wpwrk">else if nAlg = 2 then 15:</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc><div><p xml:id="_gqvAwzg"><s xml:id="_RX8jn9B">the policy for N users, i.e. π θn (a | s), for {n} N n=1 .</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc><div><p xml:id="_QF5hef6"><s xml:id="_UeNtMGg">that are very different from each other; (b) a set of N v different individuals (i.e.</s><s xml:id="_BKY2JFG">βs) are generated for each β basic v via the following process</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><figDesc><div><p xml:id="_cNVnEar"><s xml:id="_UD9Gk8W">(b) Cohesion-RL#1 is the first version of our method.</s><s xml:id="_j5mUXNf">(c) Cohesion-RL#2 is the second version of our method (cf.</s><s xml:id="_pahzVdg">Algorithm 1 for detail).</s><s xml:id="_DrrjeDR">Specially, Cohesion-RL#1 and Cohesion-RL#2 share the same actor updating.</s><s xml:id="_XMbYtSn">The difference between them is the different critic updating rules that they employ.The noises in the MDP are set as σ s = 0.5, σ r = 1 and σ β = 0.05.</s><s xml:id="_WDC462U">The state has dimension p = 3 and the policy feature has m = 4 elements.</s><s xml:id="_D8QC5ak">We set the ℓ 2 constraint in the Separate-RL as ζ a = ζ c = 0.1.</s><s xml:id="_e6fRhsN">When the cohesion constraint in our methods are too small (10 -4 , say), we need the ℓ 2 constraint for the actor-critic updating to avoid the overfitting, with the parameters asζ 1 = ζ 2 = ζ 3 = 0.1.</s><s xml:id="_C7j7U92">Otherwise, we set ζ 1 = ζ 2 = ζ 3 → 0.The feature processing for the value estimation is x (s, a) = [1, s , a, s a] ∈ R u , where u = 2p+ 2, for all the compared methods.</s><s xml:id="_WCu6TCv">The feature for the policy is processed as φ (s, a) = [as , a] ∈ R m where m = p + 1.</s><s xml:id="_bQykFjT">We set K = 8 for the K-NN based network cohesion learning.</s><s xml:id="_wADqH7Z">If there is no special setting, the following three paremeters are set as: (a) the trajectory length in mHealth is T = 80, which indicates that the online RL learning ends at t = 80;</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 1 .</head><label>1</label><figDesc><div><p xml:id="_krKJ7JY"><s xml:id="_pcCzjBd">Fig. 1.</s><s xml:id="_PuTfwpU">Performance of three RL methods for experiment setting (S1).</s><s xml:id="_NEub7TG">Each sub-figure shows results under one γ setting.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><figDesc><div><p xml:id="_jRwUSTy"><s xml:id="_PP95Ux9">(b) Discount Factor when γ = 0.6</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 2 .</head><label>2</label><figDesc><div><p xml:id="_CN9JmDE"><s xml:id="_vQS4CRf">Fig. 2. Performance of three RL methods for experiment setting (S2).</s><s xml:id="_DZzV7d7">Each sub-figure shows results under one γ setting.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Appendix: the proof of Theorem 1 Proof:</head><label>1</label><figDesc><div><p xml:id="_mYwVdPs"><s xml:id="_D789bna">Considering L ⊗ (µ 2 , ζ 2 ) = (µ 2 L + ζ 2 I N ) ⊗ I u gives the equation B = P P + µ 2 L ⊗ I u + ζ 2 I uN .The first term B 1 = P P is obviously positive semi-definite as ∀x, we have x P Px = Px</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 2 ≥ 0 .</head><label>20</label><figDesc><div><p xml:id="_WHPNSPS"><s xml:id="_Q3HtKAN">The graph laplacian L is positive semi-definite, which indicates that its eigenvalues are non-negative, i.e.λ 1 , • • • , λ N ≥ 0. The eigenvalues of I u are µ 1 = • • • = µ u = 1.According to Lemma 2, we have the conclusion that the eigenvalues of L ⊗ I u are nonnegative, which indicates that it is a positive semi-definite matrix.</s><s xml:id="_Yp6m823">The last term in B is an identical matrix, which is surely positive definite.</s><s xml:id="_SP32kXh">The sum of two positive semidefinite matrices and a positive definite matrix results in a positive definite matrix.Since for any matrices A ∈ R l×k and D ∈ R m×n , the Kronecker product has the property (A ⊗ D) = (A ⊗ D )<ref type="bibr" target="#b53">[49]</ref>.</s><s xml:id="_vGwnxrq">Besides, the graph laplacian L is symmetric.</s><s xml:id="_wrFcRH8">We haveB = (P P + µ 2 L ⊗ I u + ζ 2 I uN ) = B.</s></p></div></figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_ZvTeEWJ" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<idno>T = 150</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZrFbb6F">reward when</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">reward when T = 150</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_RhxBJZR">Separate-RL Cohesion-RL#1 Cohesion-RL#2 Separate-RL Cohesion-RL#1 Cohesion-RL#2 0 1238</title>
		<idno type="DOI">10.33196/wbl201704022201</idno>
		<idno>4±81.6 1332.3±57.0 1394.4±68.5 1239.5±83.9 1342.1±59.7 1397.4±68.6 0.2 1272.9±83.7 1376.1±55.8 1428.1±64.1 1279.5±83.4 1386.4±56.0 1429.2±64.3 0.4 1286.7±85.3 1429.3±53.0 1472.4±58.7 1316.4±77.5 1436.2±55.6 1472.4±59.0 0.6 1346.3±75.9 1488.4±55.5 1505.1±55.4 1388.5±70.8 1502.6±52.4 1515.3±55.9 0.8 1373.9±66.2 1550.8±52.6 1556.7±54.0 1440.7±63.5 1560.4±53.1 1570.0±54.0 0.95 1265.2±78.3 1542.7±50.7 1556.5±49.6 1315.2±68.6 1570.0±50.3 1577.8±51.3 Avg. 1297.2 1453.3 1485.5 1330.0 1466.3 1493.7</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Separate-RL Cohesion-RL#1 Cohesion-RL#2 Separate-RL Cohesion-RL#1 Cohesion-RL#2 0 1238.4±81.6 1332.3±57.0 1394.4±68.5 1239.5±83.9 1342.1±59.7 1397.4±68.6 0.2 1272.9±83.7 1376.1±55.8 1428.1±64.1 1279.5±83.4 1386.4±56.0 1429.2±64.3 0.4 1286.7±85.3 1429.3±53.0 1472.4±58.7 1316.4±77.5 1436.2±55.6 1472.4±59.0 0.6 1346.3±75.9 1488.4±55.5 1505.1±55.4 1388.5±70.8 1502.6±52.4 1515.3±55.9 0.8 1373.9±66.2 1550.8±52.6 1556.7±54.0 1440.7±63.5 1560.4±53.1 1570.0±54.0 0.95 1265.2±78.3 1542.7±50.7 1556.5±49.6 1315.2±68.6 1570.0±50.3 1577.8±51.3 Avg. 1297.2 1453.3 1485.5 1330.0 1466.3 1493.7</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_V7Kzctt">TABLE 2 The performance of 3 online RLs vs. the warm start trajectory length T 0 ∈ {5, 20</title>
		<idno type="DOI">10.7717/peerj-cs.759/table-4</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4CM6Shu">). γ Average reward when T 0 = 5 Average reward when T 0 = 20</title>
		<imprint/>
	</monogr>
	<note>for the experiment setting (S2</note>
	<note type="raw_reference">TABLE 2 The performance of 3 online RLs vs. the warm start trajectory length T 0 ∈ {5, 20}, for the experiment setting (S2). γ Average reward when T 0 = 5 Average reward when T 0 = 20</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_atqChxz">Separate-RL Cohesion-RL#1 Cohesion-RL#2 Separate-RL Cohesion-RL#1 Cohesion-RL#2 0 1194</title>
		<idno type="DOI">10.1201/9781420027228-47</idno>
		<idno>0±86.8 1324.7±60.0 1396.7±68.5 1380.8±68.7 1358.9±63.6 1399.1±68.0 0.2 1183.8±87.8 1380.5±52.0 1427.4±64.5 1410.1±66.8 1408.0±57.8 1430.2±63.6 0.4 1200.4±81.1 1433.2±54.2 1469.2±58.2 1423.7±63.9 1453.9±55.4 1471.3±59.3 0.6 1254.1±75.7 1487.9±51.2 1515.2±55.0 1463.6±58.1 1505.7±53.8 1516.8±56.1 0.8 1291.8±76.8 1532.7±48.6 1562.3±53.8 1519.8±53.9 1541.3±52.4 1552.9±54.0 0.95 1245.1±81.5 1554.7±49.4 1568.5±51.0 1434.4±53.3 1564.1±49.2 1574.6±50.0 Avg. 1228.2 1452.3 1489.9 1438.7 1472.0 1490</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Separate-RL Cohesion-RL#1 Cohesion-RL#2 Separate-RL Cohesion-RL#1 Cohesion-RL#2 0 1194.0±86.8 1324.7±60.0 1396.7±68.5 1380.8±68.7 1358.9±63.6 1399.1±68.0 0.2 1183.8±87.8 1380.5±52.0 1427.4±64.5 1410.1±66.8 1408.0±57.8 1430.2±63.6 0.4 1200.4±81.1 1433.2±54.2 1469.2±58.2 1423.7±63.9 1453.9±55.4 1471.3±59.3 0.6 1254.1±75.7 1487.9±51.2 1515.2±55.0 1463.6±58.1 1505.7±53.8 1516.8±56.1 0.8 1291.8±76.8 1532.7±48.6 1562.3±53.8 1519.8±53.9 1541.3±52.4 1552.9±54.0 0.95 1245.1±81.5 1554.7±49.4 1568.5±51.0 1434.4±53.3 1564.1±49.2 1574.6±50.0 Avg. 1228.2 1452.3 1489.9 1438.7 1472.0 1490.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main" xml:id="_VxgE3XM">γ &lt; 1 is the discounted reward RL, which is first compared in the online actor-critic setting for mHealth</title>
		<imprint/>
	</monogr>
	<note>(b) 0 &lt; The value of γ specifies different RL methods: (a) γ = 0 means the contextual bandit In each comparision, the bold value is the best, and the blue itatlic value is the 2nd best. REFERENCES</note>
	<note type="raw_reference">The value of γ specifies different RL methods: (a) γ = 0 means the contextual bandit []; (b) 0 &lt; γ &lt; 1 is the discounted reward RL, which is first compared in the online actor-critic setting for mHealth. In each comparision, the bold value is the best, and the blue itatlic value is the 2nd best. REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_rPVScg5">An actor-critic contextual bandit algorithm for personalized interventions using mobile devices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_DFkYw6X">NIPS 2014 Workshop: Personalization: Methods and Applications</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Lei, A. Tewari, and S. Murphy, &quot;An actor-critic contextual bandit algorithm for personalized interven- tions using mobile devices,&quot; in NIPS 2014 Workshop: Personalization: Methods and Applications, pp. 1 -9, 2014.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main" xml:id="_DZjuMBW">An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">H. Lei, An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention. PhD thesis, University of Michigan, 2016.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_tbta3zW">A batch, off-policy, actorcritic algorithm for optimizing the average reward</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<idno>abs/1607.05047</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Esea5qk">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Murphy, Y. Deng, E. B. Laber, H. R. Maei, R. S. Sutton, and K. Witkiewitz, &quot;A batch, off-policy, actor- critic algorithm for optimizing the average reward,&quot; CoRR, vol. abs/1607.05047, 2016.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_tpTtNQs">Constructing justin-time adaptive interventions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JD2AgXs">Phd Section Proposal</title>
		<imprint>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Liao, A. Tewari, and S. Murphy, &quot;Constructing just- in-time adaptive interventions,&quot; Phd Section Proposal, pp. 1-49, 2015.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_2HKhVgY">Deep correlational learning for survival prediction from multimodality datay</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_46</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_J9vc5tf">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Yao, X. Zhu, F. Zhu, and J. Huang, &quot;Deep corre- lational learning for survival prediction from multi- modality datay,&quot; in International Conference on Medi- cal Image Computing and Computer Assisted Intervention (MICCAI), 2017.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_EPrmeM3">Wsisa: Making survival prediction from whole slide histopathological images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VnWbzAE">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7234" to="7242" />
		</imprint>
	</monogr>
	<note type="raw_reference">X. Zhu, J. Yao, F. Zhu, and J. Huang, &quot;Wsisa: Making survival prediction from whole slide histopathological images,&quot; in IEEE Conference on Computer Vision and Pattern Recognition, pp. 7234 -7242, 2017.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_u2UdB7c">Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_e2DJaVS">ACM Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<imprint>
			<publisher>ACM-BCB</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Xu, S. Wang, F. Zhu, and J. Huang, &quot;Seq2seq fin- gerprint: An unsupervised deep molecular embedding for drug discovery,&quot; in ACM Conference on Bioinformat- ics, Computational Biology, and Health Informatics (ACM- BCB), 2017.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_KPDfJXy">A smartphone application to support recovery from alcoholism: a randomized clinical trial</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mctavish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B .</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamapsychiatry.2013.4642</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_umT76J4">JAMA Psychiatry</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Gustafson, F. McTavish, M. Chih, A. Atwood, R. Johnson, M. B. ..., and D. Shah, &quot;A smartphone application to support recovery from alcoholism: a randomized clinical trial,&quot; JAMA Psychiatry, vol. 71, no. 5, 2014.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_zaC8yBK">Development and evaluation of a mobile intervention for heavy drinking and smoking among college studen</title>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirouac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larimer</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0034747</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cHrMzqd">Psychology of Addictive Behaviors</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Witkiewitz, S. Desai, S. Bowen, B. Leigh, M. Kirouac, and M. Larimer, &quot;Development and evaluation of a mobile intervention for heavy drinking and smoking among college studen,&quot; Psychology of Addictive Behav- iors, vol. 28, no. 3, 2014.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_udPX4xh">Harnessing different motivational frames via mobile phones to promote daily physical activity and reduce sedentary behavior in aging adults</title>
		<author>
			<persName><forename type="first">K</forename><surname>Abby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lauren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jylana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">˙</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jesse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fUxzEDe">Plos ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Abby, H. Eric, G. Lauren, W. Sandra, S. Jylana, B. Matthew, ˙.., and C. Jesse, &quot;Harnessing different motivational frames via mobile phones to promote daily physical activity and reduce sedentary behavior in aging adults,&quot; Plos ONE, vol. 8, 2013.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_MgrBnDj">Technology-enhanced maintenance of treatment gains in eating disorders: Efficacy of an intervention delivered via text messaging</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Okon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kordy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yJh5vZN">Journal of Consulting and Clinical Psychology</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Bauer, E. Okon, R. Meermann, and H. Kordy, &quot;Technology-enhanced maintenance of treatment gains in eating disorders: Efficacy of an intervention de- livered via text messaging,&quot; Journal of Consulting and Clinical Psychology, vol. 80, no. 4, 2012.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_bsUdZku">Mobile interventions for severe mental illness: design and preliminary data from three approaches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Depp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mausbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granholm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ben-Zeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">.</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jeste</surname></persName>
		</author>
		<idno type="DOI">10.1097/nmd.0b013e3181f49ea3</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vPrVytc">The Journal of Nervous and Mental Disease</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Depp, B. Mausbach, E. Granholm, V. Cardenas, D. Ben-Zeev, ..., and D. Jeste, &quot;Mobile interventions for severe mental illness: design and preliminary data from three approaches,&quot; The Journal of Nervous and Mental Disease, vol. 198, no. 10, 2010.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_2qGTFFY">Mobile technologies among people with serious mental illness: opportunities for future services</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ben-Zeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Krzsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Drake</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10488-012-0424-x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_k6XEdyW">Administration and Policy in Mental Health and Mental Health Services Research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ben-Zeev, K. E. Davis, S. Kaiser, I. Krzsos, and R. E. Drake, &quot;Mobile technologies among people with seri- ous mental illness: opportunities for future services,&quot; Administration and Policy in Mental Health and Mental Health Services Research, vol. 40, no. 4, 2013.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_vxs8PPw">A text message-based intervention for weight loss: randomized controlled trial</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zabin-Ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Griswold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nKQbRAm">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Patrick, F. Raab, M. Adams, L. Dillon, M. Zabin- ski, C. Rock, W. Griswold, and G. Norman, &quot;A text message-based intervention for weight loss: random- ized controlled trial,&quot; Journal of Medical Internet Re- search, vol. 11, no. 1, 2009.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_W5Ky69T">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
	<note type="raw_reference">R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. Cambridge, MA, USA: MIT Press, 2nd ed., 2012.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_yXfZ8kT">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AfGQ7xW">International Conference on World Wide Web (WWW)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">L. Li, W. Chu, J. Langford, and R. E. Schapire, &quot;A contextual-bandit approach to personalized news ar- ticle recommendation,&quot; in International Conference on World Wide Web (WWW), pp. 661-670, 2010.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_b5VNDnE">A gang of bandits</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zappella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qJjUdgR">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="737" to="745" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Cesa-Bianchi, C. Gentile, and G. Zappella, &quot;A gang of bandits,&quot; in NIPS, pp. 737-745, 2013.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_Rv5cymT">Online clustering of bandits</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zappella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_abuEaMH">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. Gentile, S. Li, and G. Zappella, &quot;Online clustering of bandits,&quot; in ICML, pp. 757-765, 2014.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_9QkqcKZ">Revealing graph bandits for maximizing local influence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZdUcMNq">AISTATS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Carpentier and M. Valko, &quot;Revealing graph bandits for maximizing local influence,&quot; in AISTATS, pp. 10-18, 2016.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_fbenWx3">Structured sparse method for hyperspectral unmixing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2013.11.014</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4A4Kydh">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, Y. Wang, S. Xiang, B. Fan, and C. Pan, &quot;Struc- tured sparse method for hyperspectral unmixing,&quot; ISPRS Journal of Photogrammetry and Remote Sensing, vol. 88, no. 0, pp. 101-118, 2014.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_gKux8Hf">A label propagation method using spatial-spectral consistency for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2015.1125547</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_z3qU9cD">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="211" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Li, Y. Wang, S. Xiang, J. Duan, F. Zhu, and C. Pan, &quot;A label propagation method using spatial-spectral consis- tency for hyperspectral image classification,&quot; Interna- tional Journal of Remote Sensing, vol. 37, no. 1, pp. 191- 211, 2016.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_FBYpDEu">Prediction models for network-linked data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1602.01192</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_p3fmtz6">CoRR</title>
		<imprint>
			<date type="published" when="2016-02">February 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Li, E. Levina, and J. Zhu, &quot;Prediction models for network-linked data,&quot; CoRR, vol. abs/1602.01192, February 2016.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_MTra4V3">Algorithmic survey of parametric value function approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2013.2247418</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_r3Yq42m">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="845" to="867" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Geist and O. Pietquin, &quot;Algorithmic survey of para- metric value function approximation,&quot; IEEE Transac- tions on Neural Networks and Learning Systems, vol. 24, no. 6, pp. 845-867, 2013.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_sZUFvmB">A survey of actor-critic reinforcement learning: Standard and natural policy gradients</title>
		<author>
			<persName><forename type="first">I</forename><surname>Grondman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A D</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsmcc.2012.2218595</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tnQ5Ne7">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1291" to="1307" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska, &quot;A survey of actor-critic reinforcement learning: Standard and natural policy gradients,&quot; IEEE Trans. Systems, Man, and Cybernetics, vol. 42, no. 6, pp. 1291-1307, 2012.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_WhP7mma">Sequential cost-sensitive decision making with reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P D</forename><surname>Pednault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TSsJugZ">ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. P. D. Pednault, N. Abe, and B. Zadrozny, &quot;Sequen- tial cost-sensitive decision making with reinforcement learning,&quot; in ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min., pp. 259-268, 2002.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_ZtbSwqf">Least-squares policy iteration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-30164-8_468</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rjKrTtK">J. of Machine Learning Research (JLMR)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. G. Lagoudakis and R. Parr, &quot;Least-squares policy iteration,&quot; J. of Machine Learning Research (JLMR), vol. 4, pp. 1107-1149, 2003.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_2bVskVz">Variety influences habituation of motivated behavior for food and energy intake in children</title>
		<author>
			<persName><forename type="first">L</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Roemmich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marusewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nadbrzuch</surname></persName>
		</author>
		<idno type="DOI">10.3945/ajcn.2008.26911</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ENRVjnt">The American Journal of Clinical Nutrition</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="746" to="754" />
			<date type="published" when="2009-03">Mar 2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Epstein, J. Robinson, J. Temple, J. Roemmich, A. Marusewski, and R. Nadbrzuch, &quot;Variety influences habituation of motivated behavior for food and energy intake in children,&quot; The American Journal of Clinical Nutrition, vol. 89, pp. 746 -754, Mar 2009.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_TFUaWFH">Cross channel optimized marketing by reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Apté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schroko</surname></persName>
		</author>
		<idno type="DOI">10.1145/1014052.1016912</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uxt5zjU">ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="767" to="772" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Abe, N. K. Verma, C. Apté, and R. Schroko, &quot;Cross channel optimized marketing by reinforcement learn- ing,&quot; in ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min., pp. 767-772, 2004.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_2Yq5Z9J">Optimizing debt collections using constrained reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pendus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kowalczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Domick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gardinier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_yA9PTP5">ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Abe, P. Melville, C. Pendus, C. K. Reddy, D. L. Jensen, V. P. Thomas, J. J. Bennett, G. F. Anderson, B. R. Cooley, M. Kowalczyk, M. Domick, and T. Gar- dinier, &quot;Optimizing debt collections using constrained reinforcement learning,&quot; in ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Min., pp. 75-84, 2010.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_fetNFjx">Regularization and feature selection in least-squares temporal difference learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xn4EAMc">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Z. Kolter and A. Y. Ng, &quot;Regularization and feature selection in least-squares temporal difference learning,&quot; in International Conference on Machine Learning (ICML), pp. 521-528, 2009.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_rnGZWFC">Privacypreserving reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sakuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Wright</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390265</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NkBAGpx">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="864" to="871" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Sakuma, S. Kobayashi, and R. N. Wright, &quot;Privacy- preserving reinforcement learning,&quot; in International Conference on Machine Learning, pp. 864-871, 2008.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_wN7BnaY">Delinquent peers revisited: Does network structure matter?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Haynie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g9uf95v">American journal of sociology</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1013" to="1057" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. L. Haynie, &quot;Delinquent peers revisited: Does net- work structure matter?,&quot; American journal of sociology, vol. 106, no. 4, pp. 1013 -1057, 2001.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_CFywHAq">Social network influences on adolescent substance use: disentangling structural equivalence from cohesion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Valente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5QPHUcn">Social Science &amp; Medicine</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1952" to="1960" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Fujimoto and T. W. Valente, &quot;Social network in- fluences on adolescent substance use: disentangling structural equivalence from cohesion,&quot; Social Science &amp; Medicine, vol. 74, no. 12, pp. 1952 -1960, 2012.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_nQ4CfGE">Semisupervised hyperspectral image classification via discriminant analysis and robust regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UNdrnUf">IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Cheng, F. Zhu, S. Xiang, Y. Wang, and C. Pan, &quot;Semisupervised hyperspectral image classification via discriminant analysis and robust regression,&quot; IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 9, no. 2, pp. 595-608, 2016.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_3eGc6AY">Road centerline extraction via semisupervised segmentation and multidirection nonmaximum suppression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ga78D3X">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="545" to="549" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Cheng, F. Zhu, S. Xiang, and C. Pan, &quot;Road center- line extraction via semisupervised segmentation and multidirection nonmaximum suppression,&quot; IEEE Geo- science and Remote Sensing Letters, vol. 13, no. 4, pp. 545- 549, 2016.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_NRzvC5G">Spectral unmixing via data-guided sparsity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2014.2363423</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_r2Dtcjs">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="5412" to="5427" />
			<date type="published" when="2014-12">Dec 2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, Y. Wang, B. Fan, S. Xiang, G. Meng, and C. Pan, &quot;Spectral unmixing via data-guided sparsity,&quot; IEEE Transactions on Image Processing (TIP), vol. 23, pp. 5412- 5427, Dec 2014.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_KTmTNMb">Urban road extraction via graph cuts based probability propagation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FA5zuNJ">Image Processing (ICIP), 2014 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="5072" to="5076" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Cheng, Y. Wang, Y. Gong, F. Zhu, and C. Pan, &quot;Urban road extraction via graph cuts based probabil- ity propagation,&quot; in Image Processing (ICIP), 2014 IEEE International Conference on, pp. 5072-5076, IEEE, 2014.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_BtmdRbH">10,000+ times accelerated robust subset selection (ARSS)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v29i1.9565</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YKMGYaE">Proc. Assoc. Adv. Artif. Intell. (AAAI)</title>
		<meeting>Assoc. Adv. Artif. Intell. (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3217" to="3224" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, B. Fan, X. Zhu, Y. Wang, S. Xiang, and C. Pan, &quot;10,000+ times accelerated robust subset selec- tion (ARSS),&quot; in Proc. Assoc. Adv. Artif. Intell. (AAAI), pp. 3217-3224, 2015.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_WDCw2zB">Road extraction via adaptive graph cuts with multiple features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/icip.2015.7351549</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xmS6FPV">Image Processing (ICIP), IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3962" to="3966" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Cheng, Y. Wang, F. Zhu, and C. Pan, &quot;Road extrac- tion via adaptive graph cuts with multiple features,&quot; in Image Processing (ICIP), IEEE International Conference on, pp. 3962-3966, IEEE, 2015.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_7BG8MPZ">Robust hyperspectral unmixing with correntropy-based metric</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2fkexcN">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4027" to="4040" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Wang, C. Pan, S. Xiang, and F. Zhu, &quot;Robust hy- perspectral unmixing with correntropy-based metric,&quot; IEEE Transactions on Image Processing, vol. 24, no. 11, pp. 4027-4040, 2015.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_Wa9NRcC">Learning-based fully 3d face reconstruction from a single image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_seD442q">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1651" to="1655" />
		</imprint>
	</monogr>
	<note type="raw_reference">X. Hu, Y. Wang, F. Zhu, and C. Pan, &quot;Learning-based fully 3d face reconstruction from a single image,&quot; in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pp. 1651-1655, IEEE, 2016.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_ew9T27h">Accurate urban road centerline extraction from vhr imagery via multiscale segmentation and tensor voting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kdDde44">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Cheng, F. Zhu, S. Xiang, Y. Wang, and C. Pan, &quot;Accurate urban road centerline extraction from vhr imagery via multiscale segmentation and tensor vot- ing,&quot; Neurocomputing, vol. 205, pp. 407-420, 2016.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_CJAGCzt">Effective spectral unmixing via robust representation and learning-based sparsity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2014.2363423</idno>
		<idno>abs/1409.0685</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kPVkmjp">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, Y. Wang, B. Fan, G. Meng, and C. Pan, &quot;Effec- tive spectral unmixing via robust representation and learning-based sparsity,&quot; CoRR, vol. abs/1409.0685, 2014.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_ujd6HhB">Randomised trials for the fitbit generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1740-9713.2015.00863.x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gKckqGW">Significance</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2016-12">Dec 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Dempsey, P. Liao, P. Klasnja, I. Nahum-Shani, and S. A. Murphy, &quot;Randomised trials for the fitbit genera- tion,&quot; Significance, vol. 12, pp. 20 -23, Dec 2016.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_S6JvNT2">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RUAHg5j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">U. von Luxburg, &quot;A tutorial on spectral clustering,&quot; Statistics and Computing, vol. 17, no. 4, pp. 395 -416, 2007.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main" xml:id="_Y8hmmMW">Unsupervised Hyperspectral Unmixing Methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">F. Zhu, Unsupervised Hyperspectral Unmixing Methods. PhD thesis, 2015.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_gRUbT2r">Regularized least squares temporal difference learning with nested ℓ 2 and ℓ 1 penalization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bDsqYBk">Recent Advances in Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="102" to="114" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. W. Hoffman, A. Lazaric, M. Ghavamzadeh, and R. Munos, &quot;Regularized least squares temporal differ- ence learning with nested ℓ 2 and ℓ 1 penalization,&quot; in Recent Advances in Reinforcement Learning, pp. 102-114, 2011.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main" xml:id="_5eFfgZX">The matrix cookbook</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.disc.2012.08.007</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. B. Petersen and M. S. Pedersen, &quot;The matrix cook- book,&quot; 2012.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_7Qj6qff">The kronecker product and stochastic automata networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Langville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Stewart</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cam.2003.10.010</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MEpe6Rf">Journal of Computational and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page" from="429" to="447" />
			<date type="published" when="2004-06">June 2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. N. Langville and W. J. Stewart, &quot;The kronecker prod- uct and stochastic automata networks,&quot; Journal of Com- putational and Applied Mathematics, vol. 167, pp. 429- 447, June 2004.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
