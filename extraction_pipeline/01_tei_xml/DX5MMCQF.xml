<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_A5XegCg">Application of substitution box of present cipher for automated detection of snoring sounds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">© 2021 Elsevier B.V. All rights reserved.</p>
				</availability>
				<date type="published" when="2021-05-06">6 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sengul</forename><surname>Dogan</surname></persName>
							<email>sdogan@firat.edu.tr</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> Department of Digital Forensics Engineering , College of Technology , Firat University , Elazig , Turkey</note>
								<orgName type="department" key="dep1">Department of Digital Forensics Engineering</orgName>
								<orgName type="department" key="dep2">College of Technology</orgName>
								<orgName type="institution">Firat University</orgName>
								<address>
									<settlement>Elazig</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erhan</forename><surname>Akbal</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> Department of Digital Forensics Engineering , College of Technology , Firat University , Elazig , Turkey</note>
								<orgName type="department" key="dep1">Department of Digital Forensics Engineering</orgName>
								<orgName type="department" key="dep2">College of Technology</orgName>
								<orgName type="institution">Firat University</orgName>
								<address>
									<settlement>Elazig</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Turker</forename><surname>Tuncer</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>a</label> Department of Digital Forensics Engineering , College of Technology , Firat University , Elazig , Turkey</note>
								<orgName type="department" key="dep1">Department of Digital Forensics Engineering</orgName>
								<orgName type="department" key="dep2">College of Technology</orgName>
								<orgName type="institution">Firat University</orgName>
								<address>
									<settlement>Elazig</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">U</forename><surname>Rajendra Acharya</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>b</label> Ngee Ann Polytechnic , Department of Electronics and Computer Engineering , 599489 , Singapore</note>
								<orgName type="department">Department of Electronics and Computer Engineering</orgName>
								<orgName type="institution">Ngee Ann Polytechnic</orgName>
								<address>
									<postCode>599489</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>c</label> Department of Biomedical Engineering , School of Science and Technology , SUSS University , Singapore</note>
								<orgName type="department" key="dep1">Department of Biomedical Engineering</orgName>
								<orgName type="department" key="dep2">School of Science and Technology</orgName>
								<orgName type="institution">SUSS University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>d</label> Department of Biomedical Informatics and Medical Engineering , Asia University , Taichung , Taiwan</note>
								<orgName type="department">Department of Biomedical Informatics and Medical Engineering</orgName>
								<orgName type="institution">Asia University</orgName>
								<address>
									<settlement>Taichung</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_zYJCGWm">Application of substitution box of present cipher for automated detection of snoring sounds</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-06">6 May 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">2D0D7C93996BA97DEFF73D42DEB66240</idno>
					<idno type="DOI">10.1016/j.artmed.2021.102085</idno>
					<note type="submission">Received 26 November 2020; Received in revised form 30 April 2021; Accepted 3 May 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords xml:id="_UWRAjuB">Present pattern Maximum absolute pooling NCAINCA Snore sound classification Artificial intelligence</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_b36BGHs"><p xml:id="_tpeXkBy"><s xml:id="_mGtafTf">Background and purpose: Snoring is one of the sleep disorders, and snoring sounds have been used to diagnose many sleep-related diseases.</s><s xml:id="_jCUfHPB">However, the snoring sound classification is done manually which is timeconsuming and prone to human errors.</s><s xml:id="_GwcxeAn">An automated snoring sound classification model is proposed to overcome these problems.</s><s xml:id="_zzeUQ8h">Material and method: This work proposes an automated snoring sound classification method using three new methods.</s><s xml:id="_4XwK7fg">These methods are maximum absolute pooling (MAP), the nonlinear present pattern, and two-layered neighborhood component analysis, and iterative neighborhood component analysis (NCAINCA) selector.</s><s xml:id="_eSn4uhr">Using these methods, a new snoring sound classification (SSC) model is presented.</s><s xml:id="_pPe4bUr">The MAP decomposition model is applied to snoring sounds to extract both low and high-level features.</s><s xml:id="_KdJxTfG">The presented model aims to attain high performance for SSC problem.</s><s xml:id="_2qYNHjQ">The developed present pattern (Present-Pat) uses substitution box (SBox) and statistical feature generator.</s><s xml:id="_75kz9HQ">By deploying these feature generators, both textural and statistical features are generated.</s><s xml:id="_fNb3tdc">NCAINCA chooses the most informative/valuable features, and these selected features are fed to knearest neighbor (kNN) classifier with leave-one-out cross-validation (LOOCV).</s><s xml:id="_McnjZrC">The Present-Pat based SSC system is developed using Munich-Passau Snore Sound Corpus (MPSSC) dataset comprising of four categories.</s><s xml:id="_a8bJBJd">Results: Our model reached an accuracy and unweighted average recall (UAR) of 97.10 % and 97.60 %, respectively, using LOOCV.</s><s xml:id="_Wx25zce">Moreover, a nocturnal sound dataset is used to show the universal success of the presented model.</s><s xml:id="_YnTkc2D">Our model attained an accuracy of 98.14 % using the used nocturnal sound dataset.</s><s xml:id="_7qz64Px">Conclusions: Our developed classification model is ready to be tested with more data and can be used by sleep specialists to diagnose the sleep disorders based on snoring sounds.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="793.701"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1." xml:id="_mdSjyjA">Introduction</head><p xml:id="_Au2J8pM"><s xml:id="_zBcvSQw">Snoring is a sleep disorder affecting 45 % of people worldwide <ref type="bibr" target="#b0">[1]</ref> and the studies show that 45 % of people have snoring problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</s><s xml:id="_Cpup4Eb">This problem is more common in men and the elderly <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_qsh887Q">Sleep problems cause many ailments such as daytime fatigue, drowsiness, sleepiness, social activity disorders, problems between spouses, problems in the family, problems in business life, and health problems <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_72mHSsE">A few studies have shown a positive association between high snoring and risk of stroke or even death <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</s><s xml:id="_XrJXKxC">It is important for people to have good quality sleep without snoring problems.</s></p><p xml:id="_wBN3bqs"><s xml:id="_vhMRrsY">Snoring is the problem with airflow in the airways during sleep.</s><s xml:id="_hPjbmeK">Soft-tissue structures in the upper airways (UA) are stimulated by inspiratory airflow and vibrate.</s><s xml:id="_U3MqKPq">Thus, the person cannot perform healthy breathing during sleep and hence snores.</s><s xml:id="_bNPdsRf">This problem may trigger different diseases such as obstructive sleep apnea (OSA), brain and heart disease <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_QsJCdEU">The source of the problem must be diagnosed to treat the snoring.</s><s xml:id="_SwTaRW6">Different methods are used to diagnose snoring, such as detailed clinical history and examination, polysomnography (PSG) <ref type="bibr" target="#b2">[3]</ref>, drug-induced sleep endoscopy (DISE) <ref type="bibr" target="#b3">[4]</ref>, camera <ref type="bibr" target="#b4">[5]</ref>, and voice recording <ref type="bibr" target="#b5">[6]</ref>.</s><s xml:id="_fcTbgTW">The purpose of all methods is to diagnose the reason for snoring and suggest appropriate treatment <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</s></p><p xml:id="_CkQ4x7r"><s xml:id="_uEPQqJg">The PSG is a reliable diagnostic method used to diagnose the type and severity of sleep-related breathing disorder <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_QQ2Nu6P">However, it has limited use in identifying the underlying mechanisms.</s><s xml:id="_KX6j7ke">Drug-induced sleep endoscopy (DISE) is a method developed to determine obstruction in the palate or tongue area during sleep and problems in the airway structures <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_StGukxw">During DISE, the patient is examined intranasal using a nasopharyngoscope (an endoscope for visually examining the nasal passages and pharynx) while the patient is unconscious during sleep <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_4HgcWyQ">During the test, video, audio recordings, and documentation are made, and then the expert evaluation is made.</s><s xml:id="_ytRq7zz">However, it is not possible to do it at home or during natural sleep because the patient takes medication, the process takes a long time, and the endoscope is attached to the patient.</s><s xml:id="_FKv2Tkv">Sound-based snoring diagnosis studies have become popular in recent years because of their easier and more convenient application <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>.</s><s xml:id="_pHsCjvd">Many studies have shown that the snoring sound occurs during the air passage through the respiratory system and has different acoustic properties in different locations <ref type="bibr" target="#b5">[6]</ref>.</s><s xml:id="_YkAUjwZ">Therefore, it is possible to classify snoring sounds by using characteristic acoustic properties.</s><s xml:id="_SyrpRqX">The sounds collected during sleep can be detected using artificial intelligence-based computer-aided diagnostic systems.</s><s xml:id="_W8PHcAe">There are many sound-based studies done using artificial intelligence in the literature.</s><s xml:id="_dSu6ST6">Several automated snoring sound detection methods have been presented <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_JpXP5V2">Studies generally focused on distinguishing snoring from OSA or snoring / not snoring <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>.</s><s xml:id="_SQ4stU6">There are also many sound-based studies linking sleep sounds to sleep stages <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>.</s><s xml:id="_dVatuQc">Also, fewer studies are presented to determine the location of sound in the respiratory system <ref type="bibr" target="#b20">[21]</ref>.</s><s xml:id="_MgBqBDU">In their study, an automated system is proposed to classify the snoring sound's location in the respiratory system using the Munich-Passau Snore Sound Corpus (MPSSC) dataset.</s></p><p xml:id="_prYRFHr"><s xml:id="_WFNMJ4d">Nowadays, many studies have been conducted on vibration source classification using snoring sound.</s><s xml:id="_nJjN7ED">Some of them are discussed below.</s><s xml:id="_sdvYxSZ">Albornoz et al. <ref type="bibr" target="#b21">[22]</ref> proposed a model using spectral properties of a sound signal, Mel frequency cepstral coefficients (MFCCs) <ref type="bibr" target="#b22">[23]</ref>, and support vector machine (SVM) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>.</s><s xml:id="_uvEtKNj">Their model has shown an unweighted average recall (UAR) of 49.38 %.</s><s xml:id="_s3bJgwV">Amiriparian et al. <ref type="bibr" target="#b9">[10]</ref> showed a method using the convolutional neural network (CNN) <ref type="bibr" target="#b25">[26]</ref> and Alexnet <ref type="bibr" target="#b25">[26]</ref> to classify the snoring sounds.</s><s xml:id="_5S427uN">They have reported the performance of 67.0 % using CNN and Alexnet with deep spectrum features.</s><s xml:id="_6GFCvcf">Demir et al. <ref type="bibr" target="#b26">[27]</ref> presented an image processing-based SSC model.</s><s xml:id="_ymxuVqR">They extract spectrogram images of the snoring sound and used a local binary pattern (LBP) <ref type="bibr" target="#b27">[28]</ref> and a histogram of oriented gradients (HOG) <ref type="bibr" target="#b28">[29]</ref> extractors-based classification model.</s><s xml:id="_cKd7vMg">MPSSC dataset was used for testing, and they reached a UAR of 72.60 %.</s><s xml:id="_tAvvB9u">Tuncer et al. <ref type="bibr" target="#b5">[6]</ref> proposed a method using iterative hybrid feature selector and local dual octal pattern (LDOP).</s><s xml:id="_BkZCbF2">They presented a multileveled feature generation model using LDOP local extractor, and a two-layered feature selector picked the most valuable features.</s><s xml:id="_AK4H22F">They attained UAR of 94.65 % and accuracy of 95.53 % using leave-one-out cross-validation (LOOCV) <ref type="bibr" target="#b29">[30]</ref> training and testing strategy.</s><s xml:id="_KdRTjjA">Vesperini et al. <ref type="bibr" target="#b30">[31]</ref> used a scattering spectrum transformation with a deep neural network, using MPSSC dataset and reported UAR of 74.19 %.</s><s xml:id="_NNPF6ak">Wang et al. <ref type="bibr" target="#b31">[32]</ref> presented a model to detect obstructive sleep apnea (OSA) by analyzing acoustic snoring sounds.</s><s xml:id="_ydKdH9M">They obtained 91.14 % recognition rate using SVM classifier.</s><s xml:id="_ySePsxd">Lim et al. <ref type="bibr" target="#b12">[13]</ref> showed that snoring sounds can be classified using recurrent neural network (RNN) <ref type="bibr" target="#b32">[33]</ref>.</s><s xml:id="_kxpHakQ">For this purpose, a dataset is created consisting of different environmental sounds and snoring sounds.</s><s xml:id="_QBN6txY">Their method is able to detect the snoring sound with an accuracy of 98.9 %.</s><s xml:id="_74zrvUf">Cavusoglu et al. <ref type="bibr" target="#b1">[2]</ref> proposed a method to determine the relationship between sleep sounds and snoring.</s><s xml:id="_uFUGH3D">They have shown that collected sounds during the polysomnography test are classified with an accuracy of 97.3 % using spectral features and robust linear regression classifier.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1." xml:id="_b8TS6cF">Main contributions novelties</head><p xml:id="_S9kAbMU"><s xml:id="_R3XDVkY">The novelties and key contributions of this work are given below.</s><s xml:id="_WD66Hu5">The novelties of this work are:</s></p><p xml:id="_KZCb995"><s xml:id="_UyqJ7Pa">• A novel nonlinear and multi-kernel feature generation function is presented by using SBox <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> of the present lightweight cipher.</s><s xml:id="_zsqU6BU">• Novel MAP decomposer is presented.</s></p><p xml:id="_CAxPC6N"><s xml:id="_xZVDEd5">• Combination of NCA <ref type="bibr" target="#b37">[38]</ref> and INCA named as NCAINCA is proposed to select the discriminative features easily.</s></p><p xml:id="_cgveWhR"><s xml:id="_SNcAewv">The main contributions of this paper are given below:</s></p><p xml:id="_jjFncfa"><s xml:id="_j8ew3u4">• Handcrafted feature generation methods have used linear patterns and binary feature extractors <ref type="bibr" target="#b37">[38]</ref>.</s><s xml:id="_X8NjQCQ">In this work, we have proposed an efficient and simple feature generation function called Present-Pat to extract the hidden subtle changes in the signal.</s><s xml:id="_9MhzhGk">This pattern (feature generation function) uses both a nonlinear function to create pattern and multiple binary feature generation functions.</s><s xml:id="_RA6qUTT">A new MAP decomposer is used to create features at various levels and statistical features are also generated.</s><s xml:id="_R6mUAb8">The presented multileveled feature generation method generates both low-level and high-level features.</s></p><p xml:id="_7DrRqdC"><s xml:id="_rZxAEtE">• Feature selection is one of the critical phases of a machine learning method.</s><s xml:id="_N6XqMUU">A two-layered NCAINCA features selection method is used to select the most discriminative features.</s><s xml:id="_WJTRakE">Also, a highly accurate SSC method is presented by using the presented feature generation method (Present-Pat and MAP-based method) and NCAINCA selector.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_YzhtrHB">Material</head><p xml:id="_7SmZEzq"><s xml:id="_z5RkACV">In this work, we have used Munich-Passau Snore Sound Corpus (MPSSC) dataset, which was presented at the INTERSPEECH 2017 Computational Paralinguistic Challenge <ref type="bibr" target="#b38">[39]</ref>.</s><s xml:id="_rFhR7GW">The dataset was collected from the endoscopy records of patients who had previously undergone polysomnography (PSG) and were diagnosed with OSA during DISE examinations.</s><s xml:id="_nwb23As">It was collected from 3 health centers: Munich Technical University, Alfried Krupp Hospital Essen, and University Hospital Halle.</s></p><p xml:id="_jkX2rMv"><s xml:id="_xKVH4VD">A total of 2174 raw records collected from 3 centers were used to create this dataset.</s><s xml:id="_p5BuyEg">Records were taken from 38 subjects from Munich Technical University from 2013 to 2014, 2090 subjects from Alfried Krupp Hospital Essen between 2006 and 2015, 46 subjects from University Hospital Halle from 2012 to 2015.</s><s xml:id="_YVcFfQ8">331 suitable snoring subjects were selected from raw DISE records <ref type="bibr" target="#b38">[39]</ref>.</s><s xml:id="_khExEau">Subsequently, 219 subjects suitable for VOTE classification were selected manually by the experts.</s><s xml:id="_NrvRf9j">The sound samples are labeled in four classes: V, O, T, and E <ref type="bibr" target="#b39">[40]</ref>.</s><s xml:id="_4zaC7gX">The 'V' is the vibration level in the velum, 'O' is the oropharyngeal vibration level in the tonsils, 'T' indicates the vibration of the tongue base and the posterior pharyngeal wall, and 'E' denotes the vibration of the epiglottis represents.</s><s xml:id="_YXgTftn">Finally, the dataset containing sound samples of 828 snoring events from 219 subjects is created.</s><s xml:id="_KpuJmz9">There are a total of 828 sound samples (484 V, 216 O, 39 T and 89 E) of 16 bits and sampled at 16 kHz frequency.</s><s xml:id="_G9vQJ6W">The average sample time is 1.46 s (range 0.73… 2.75 s).</s><s xml:id="_ug5vgwF">The average age of subjects is 49.8 years (range 24… 78), with 93.6 % of total subjects are male subjects.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_WKhPpJy">The presented SSC model</head><p xml:id="_33j399E"><s xml:id="_64RmFcb">Detection of various sleep orders using snoring sounds is complex and time-consuming.</s><s xml:id="_fYjw4RA">Hence, automated systems have been proposed for this work.</s><s xml:id="_YjN9NS5">We have used the MPSSC dataset containing 828 snoring sounds with four categories to develop the automated model.</s><s xml:id="_86SbQaq">Most of the prior methods have used the conventional feature generation methods and used widely known sound descriptors.</s><s xml:id="_2FYMHHK">In this work, we have presented an automated SSC method using nonlinear feature generation functions to obtain the highest classification performance.</s><s xml:id="_NTxddY9">Our proposed method consists of three phases: multiple kernelled Present-Pat (the prime feature generation function), MAP decomposer, and NCAINCA selector.</s><s xml:id="_tF4BWyR">The proposed multiple kernelled Present-Pat generates nonlinear features using both signum and ternary binary feature extraction functions.</s><s xml:id="_9jgNR9d">It efficiently extracts the hidden nonlinear features from the sound signals.</s><s xml:id="_MrmPdJF">The multileveled extraction method can obtain both low-level and high-level features with more detailed hidden changes.</s><s xml:id="_SuauWjy">The maximum pooling (MAP) method routes only peak values and minimum pooling routes only minimum values.</s><s xml:id="_VrfEpZv">The main idea behind the MAP is to obtain an efficient and simple decomposition/ routing function.</s><s xml:id="_fGQN2Pf">The neighborhood component analysis (NCA) is presented to select clinically significant features.</s><s xml:id="_NFQKkV5">The main problem in NCA is its inability to choose the optimal feature vector automatically.</s><s xml:id="_DR9TVBu">Hence, iterative neighborhood component analysis (INCA) <ref type="bibr" target="#b40">[41]</ref> is used to overcome this problem.</s><s xml:id="_gx8HCXh">However, the computational complexity of INCA is high.</s><s xml:id="_2rMCAQU">Hence the proposed new selector (NCAINCA) overcomes this problem.</s><s xml:id="_xBKm5db">Hence, we have developed our SSC model using these three functions coupled with a kNN classifier <ref type="bibr" target="#b41">[42]</ref>.</s><s xml:id="_Q7K7SVr">The general flow diagram of our proposed automated SSC model is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</s></p><p xml:id="_r3wzxWx"><s xml:id="_vSUkJEJ">The presented SSC model has three main sections: multileveled nonlinear multiple kernel feature extraction, NCAINCA based feature selection, and classification.</s><s xml:id="_TKdf3TR">The details of these sections are given in the following sections.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1." xml:id="_qbBcDrG">Overview of the proposed model</head><p xml:id="_qKurwjK"><s xml:id="_FDQwHhv">This work presents a novel multileveled nonlinear features-based SSC model.</s><s xml:id="_D2ck7xv">Fig. <ref type="figure" target="#fig_1">2</ref> presents the snapshot of the proposed multiple kernels Present-Pat, MAP feature generation, and NCAINCA selector-based SSC method.</s></p><p xml:id="_GESBpvF"><s xml:id="_HPVvN5U">In Fig. <ref type="figure" target="#fig_1">2</ref>, L is used to define the length of signals.</s><s xml:id="_Neep8nc">The proposed feature generation method has six levels.</s><s xml:id="_q57TbfH">We achieved the highest performance using six leveled feature generation structure.</s><s xml:id="_ukPHkCM">Hence, we have developed the model with six levels.</s><s xml:id="_Tg5exZ2">The presented multiple kernel Present-Pat and statistical generator generated 1561 features at each level, and the generated features are merged, and finally, 9366 features are obtained.</s><s xml:id="_vuzVDKj">The NCAINCA selector applied on the generated features to select the most valuable features.</s><s xml:id="_5NHnPFe">Then these selected features are fed to the kNN classifier for automated classification.</s><s xml:id="_GFmWyJz">The pseudocode of the SSC method is shown in Algorithm 1.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JhhB46U">Algorithm 1. Pseudocode of proposed SSC method.</head><p xml:id="_hk2RPnv"><s xml:id="_rwD9UeZ">Algorithm 1 demonstrates the steps involved in the presented SSC method.</s><s xml:id="_d5UdqFp">Lines 1-8 define the recommended multiple kernels Present-Pat, MAP decomposer-based feature generation, feature selection, and classification phases are indicated in lines 09-11.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2." xml:id="_vJ3yfXa">Feature extraction</head><p xml:id="_4Bvg42S"><s xml:id="_hWfDbYG">As stated in Algorithm 1, the recommended feature generation process used two prime functions: Present-Pat and MAP decomposer.</s><s xml:id="_NJ4xTQm">The feature generator extracted 9366 features from a one-dimensional signal, and the used prime functions are explained in the following subsections.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1." xml:id="_yUHtyzY">Multiple kernelled present pattern</head><p xml:id="_RRV3GDC"><s xml:id="_bMYm9fP">The used prime feature generation function in this work is the multiple kernelled present pattern.</s><s xml:id="_7nFgwrU">This function aims to generate nonlinear features by using nonlinear pattern (SBox of the Present cipher) and two binary features extraction functions.</s><s xml:id="_2yeQawB">The used kernels (binary feature generation functions) are signum and ternary.</s><s xml:id="_MsAhPKs">16 sized overlapping blocks (windows) are used to generate local features as the used SBox of the present cipher is a 4-bits SBox and its length is 2 4 = 16.</s><s xml:id="_R6YMhxv">Steps of the recommended Present-Pat are;</s></p><p xml:id="_TqaSsVz"><s xml:id="_ThybUT4">Step 1: Divide sound signal into 16 sized overlapping blocks.</s><s xml:id="_h5SUZSC">Herein, L -15 overlapping blocks are created using the signal of length = L.</s></p><p xml:id="_g8W7zAw"><s xml:id="_DwQ52qB">Step 2: Use SBox of the lightweight present cipher for constructing nonlinear pattern.</s><s xml:id="_E3EJ9Kc">The used SBox is given in Table <ref type="table" target="#tab_1">1</ref> and it shows indices of the generated pattern.</s></p><p xml:id="_VgNtWuH"><s xml:id="_e3X4JVB">This SBox (see Table <ref type="table" target="#tab_1">1</ref>) is used as the pattern of proposed feature generation function.</s><s xml:id="_WrktWXP">Therefore, this function is named present pattern (Table <ref type="table" target="#tab_2">2</ref>).</s></p><p xml:id="_ejS8xtG"><s xml:id="_Jx879sx">Step 3: Generate 48 bits using signum and ternary functions</s></p><formula xml:id="formula_0">bit(t) = { 0, blck(S(t) ) -blck(t) &lt; 0 1, blck(S(t) ) -blck(t) ≥ 0 , t = {1, 2, …, 16}<label>(1</label></formula><p xml:id="_W6bNCMm"><s xml:id="_rvaSbYM">) bit(t + 16) = { 0, blck(S(t) )blck(t) ≥thr 1, blck(S(t) )blck(t) &lt;thr (2) bit(t + 32) = { 0, blck(S(t) )blck(t) ≤ thr 1, blck(S(t) )blck(t) &gt; thr (3)</s></p><formula xml:id="formula_1">thr = 1 2 ̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅ ̅ ∑ L i=1 ( SS(i) -1 L ∑ L i=1 SS(i) ) L √ √ √ √ √ √<label>(4)</label></formula><p xml:id="_xZKeFd7"><s xml:id="_mW2KHCd">where blck represents an overlapping block with a size of 16 and thr is a threshold value.</s><s xml:id="_zqv47G6">We selected the threshold value as half of the standard deviation of the snore sound (SS).</s><s xml:id="_6gUeheS">As seen from Eqs. ( <ref type="formula" target="#formula_0">1</ref>) to (4), the binary feature generation process is defined mathematically.</s><s xml:id="_SNBjqyB">Eq. ( <ref type="formula" target="#formula_0">1</ref>) shows the signum function, and first 16 bits are generated to deploy the signum function.</s><s xml:id="_5Qdmt7Z">The Eqs. ( <ref type="formula">2</ref>) and ( <ref type="formula">3</ref>) indicate the ternary function.</s><s xml:id="_mgTaBep">Eq. ( <ref type="formula">2</ref>) shows the lower bit generation using the ternary function, and Eq. ( <ref type="formula">3</ref>) denotes the upper bit generation.</s></p><p xml:id="_34JVP39"><s xml:id="_EwnE5xu">Step 4: Create six map values by calculating the generated 48 bits.</s><s xml:id="_69Q6Q7B">We have used binary to decimal conversion in this step.</s></p><formula xml:id="formula_2">map k (i) = ∑ 8 l=1 bit((k -1) * 8 + l ) * 2 8-l , i = {1, 2, …, L -15}, k = {1, 2, …, 6}<label>(5)</label></formula><p xml:id="_3XgY6T2"><s xml:id="_9VUjd6f">As stated in Eq. ( <ref type="formula" target="#formula_2">5</ref>), the generated 48 bits are divided into six groups with 8-bits length as local binary pattern use 8-bits for coding.</s><s xml:id="_6Webnty">By employing Eq. ( <ref type="formula" target="#formula_2">5</ref>), six map signals are calculated for feature generation.</s></p><p xml:id="_AdbPyg7"><s xml:id="_cEQqGyh">Step 5: Compute histograms from the generated map signals (six map signals).</s><s xml:id="_9HAA538">The developed Present-Pat is a histogram-based feature generator, and hence histograms are generated in this step.</s></p><formula xml:id="formula_3">histo k (j) = 0, j = {1, 2, …, 2 8 } (6) histo k ( map k (i) + 1 ) = histo k ( map k (i) + 1 ) + 1<label>(7)</label></formula><p xml:id="_KBt4EnB"><s xml:id="_A6Tvwzc">where histo k defines k th histogram value.</s></p><p xml:id="_GU3yUAf"><s xml:id="_PbZwaVh">Step 6: Merge the generated six histograms and obtain the feature vector.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_dzXR76g">fvec((k</head><formula xml:id="formula_4">-1) * 256 + j ) = histo k (j), j = {1, 2, …, 256}<label>(8)</label></formula><p xml:id="_QCMqRCe"><s xml:id="_qg2s9qw">where fvec is a feature vector with a length of 1536.</s><s xml:id="_E2VEMxr">These six steps define the recommended multiple kernel Present-Pat and is defined as Present -Pat(.) in Algorithm 1.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2." xml:id="_KvbBMv7">Statistical feature generation</head><p xml:id="_CuHtfAz"><s xml:id="_N2E4fxC">The mostly preferred handcrafted feature extraction functions are divided into two primary groups: textural feature extractors and statistical feature generators.</s><s xml:id="_KyPFns6">The Present-Pat is a textural feature generator.</s><s xml:id="_7T5AAdB">Therefore, a statistical feature generation function is also implemented to generate 25 statistical moments and these moments are mean, standard deviation, variance, median, maximum, minimum, range, root mean square error, energy, mean of the absolute of the signal, standard deviation of the absolute of the signal, variance of the absolute of the signal, median of the absolute of the signal, Shannon entropy, sure entropy, log energy entropy, Shannon entropy of the absolute signal, sure entropy of the absolute signal, log energy entropy of the absolute signal, kurtosis, skewness, kurtosis of the absolute of the signal, skewness of the absolute signal, mean absolute deviation and mean absolute deviation absolute signal <ref type="bibr" target="#b42">[43]</ref>.</s><s xml:id="_pDg35nd">By applying these moments, 25 features are generated from the signal.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3." xml:id="_nTQD9NQ">Maximum absolute pooling</head><p xml:id="_r4v7Prx"><s xml:id="_BCYpdTT">We have used a novel and simple maximum absolute pooling (MAP) decomposition method in this work.</s><s xml:id="_UfVaAZu">As stated in the capsule network, the weakest atomic structure of the convolutional neural networks (CNNs) is pooling <ref type="bibr" target="#b25">[26]</ref>.</s><s xml:id="_VGg4eBy">This is because the maximum pooling method routes only the peak values.</s><s xml:id="_PCWKfCZ">By presenting the MAP, both minimum and peak values can be routed.</s><s xml:id="_Z4p3q5Q">It is a very basic decomposition function, and Eq. ( <ref type="formula">9</ref>) defines it.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_VheQKsB">MAP(x, y)</head><formula xml:id="formula_5">= { x, |x| ≥ |y| y, |y| &gt; |x| (9)</formula><p xml:id="_bKShTEK"><s xml:id="_QadY6K8">where x, y are input parameters of the MAP(., .)</s><s xml:id="_2Zx7t6E">function.</s><s xml:id="_XpjjHkg">In this work, 2 sized non-overlapping blocks are used for decomposition.</s><s xml:id="_5hk8tBs">A graphical example of MAP is shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3." xml:id="_TDF4vMd">Feature selection</head><p xml:id="_WrMQGUn"><s xml:id="_jCEuuFq">The generated feature vector is fed to NCAINCA, which uses both NCA <ref type="bibr" target="#b43">[44]</ref> and INCA <ref type="bibr" target="#b40">[41]</ref> to solve the time complexity problem of INCA.</s><s xml:id="_5QSYn7f">The NCA is a weighted feature selector.</s><s xml:id="_vW8yXcM">In the first layer, NCA is employed to generate 9366 weighted vectors.</s><s xml:id="_r49Zvnr">A threshold value of 1 × 10 -5 is chosen to eliminate the redundant ones.</s><s xml:id="_KYXDw4S">Then, INCA feature selector is applied on the selected features.</s><s xml:id="_xEJvGRH">The steps of recommended NCAINCA are given below.</s><s xml:id="_yakCUP6">NCA(., .)</s><s xml:id="_DnH6BjZ">function is used to define the NCA feature selector.</s></p><p xml:id="_jSW9TBa"><s xml:id="_Y4ChpzV">Step 1: Calculate weights (w) of each feature.</s></p><p xml:id="_Q6dnaGN"><s xml:id="_VWzqXyx">w = NCA(X, y) <ref type="bibr" target="#b9">(10)</ref> Step 2: Remove the redundant ones using the defined threshold value and calculated weights.</s><s xml:id="_BDXW4Bn">where first defines the selected feature vector.</s></p><formula xml:id="formula_6">first(i, cnt) = X(i, j), if w(j) &gt; 0.00001 and cnt = cnt + 1, i = {1, 2, …, D} (11)</formula><p xml:id="_gn7Sbzt"><s xml:id="_gbnXP4z">Step 3: Apply INCA the first.</s><s xml:id="_CWfXn3c">The subroutine of INCA is given in Algorithm 2 <ref type="bibr" target="#b40">[41]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3veJmK6">Algorithm 2. Pseudo code of INCA subroutine.</head><p xml:id="_6UE6THZ"><s xml:id="_YGZPnkV">In this work, 850 features are selected from the generated 9366 features in the first phase, and INCA selected 283 most discriminative features from 850 features.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4." xml:id="_4s7k5an">Classification</head><p xml:id="_nF7QKvZ"><s xml:id="_BNF63Sk">The last phase of the presented Present-Pat and NCAINCA based SSC method is classification using kNN <ref type="bibr" target="#b44">[45]</ref> classifier with leave-one-out (LOOCV) strategy <ref type="bibr" target="#b45">[46]</ref>.</s><s xml:id="_2chpd5K">The selected 283 features are fed as input to k-nearest neighbor (kNN) classifier.</s><s xml:id="_nqWzdxj">k value is chosen as 1 to obtain the optimum performance, and spearman distance is used.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_phbKCXm">Results</head><p xml:id="_fsPgf78"><s xml:id="_nXAqzU9">We have implemented the proposed model in MATLAB (2020a) environment and coded all six (main, Present-Pat, statistical-generator, MAP, NCAINCA, and classification) functions.</s><s xml:id="_QbYba44">The pseudocode of the main function is shown in Algorithm 1.</s><s xml:id="_JcfkMHr">The main function calls the other functions to implement the SSC model.</s><s xml:id="_aDD2wRr">In order to evaluate the performance of the model, geometric mean (GM), accuracy, F1-score, unweighted average recall (UAR), unweighted average precision (UAP), Cohen's kappa, and Matthews correlation coefficient (MCC) are used <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>.</s><s xml:id="_KMQDcuE">The computed results and confusion matrix are presented below.</s></p><p xml:id="_tazgrsE"><s xml:id="_9bNgKMz">The obtained confusion matrix of the proposed SSC model is presented in Table <ref type="table" target="#tab_3">3</ref>.   model generates 9366 features from a sound signal.</s><s xml:id="_Vp9VdyV">Our model used six leveled feature generation structure as the best performance is obtained with six levels.</s><s xml:id="_E843kvq">The plot of accuracies versus the number of levels obtained using our proposed method with LOOCV strategy is given in Fig. <ref type="figure" target="#fig_4">4</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5." xml:id="_45JFwC5">Discussion</head><p xml:id="_t2RJA3y"><s xml:id="_H2xRjcw">The NCAINCA method is used to select the most discriminative 283 features.</s><s xml:id="_kUCnQgy">The graph of loss value versus the number of features obtained during the feature selection process with NCAINCA selector is shown in Fig. <ref type="figure">5</ref>.</s></p><p xml:id="_HwSrdjy"><s xml:id="_zEu6zng">The selected 283 features are fed to the kNN classifier, which yielded an accuracy of 97.10 % and UAR of 97.60 %.</s><s xml:id="_26fPAte">The box plots of generated 283 features for (a) V, (b) O, (c) T, and (d) E are shown in Fig. <ref type="figure">6</ref>.</s></p><p xml:id="_gr8UZ4Q"><s xml:id="_RDyXg8u">The boxplot analysis is used to discriminate the generated and selected features graphically.</s><s xml:id="_peGfptq">It denotes the statistical attributes of features per class.</s><s xml:id="_VBxrFka">It shows the differences of quartile 3 and quartile 1 using blue boxes, red pluses indicating the abnormal attributes.</s><s xml:id="_CctpWUv">Moreover, minimum, maximum, and mean values of each feature is shown in the box plots.</s><s xml:id="_Sv6UxCE">Fig. <ref type="figure">5</ref> shows unique features for each class which has yielded the highest classification accuracy.</s><s xml:id="_TDN3QgE">Fig. 6.</s><s xml:id="_JB4vqEJ">Range of 283 features obtained for categories: (a) V, (b) O, (c) T, and (d) E. In the classification phase, LOOCV is considered to obtain the generalized results.</s><s xml:id="_pAeURqY">Also, the results of hold-out validation (75:25 and 50:50), three-fold cross-validation, and ten-fold cross-validation techniques are used, and their results are shown in Table <ref type="table" target="#tab_4">4</ref>.</s></p><p xml:id="_ACKDCa6"><s xml:id="_dbesjCm">The confusion matrices obtained using four validation techniques are shown in Fig. <ref type="figure" target="#fig_6">7</ref>.</s></p><p xml:id="_7QTYTzf"><s xml:id="_h7qrVNJ">The summary of comparison with other similar works developed using the same database is shown in Table <ref type="table" target="#tab_5">5</ref>.</s></p><p xml:id="_9ayffBn"><s xml:id="_AUpHRyG">In Table <ref type="table" target="#tab_5">5</ref>, UAR, AP, and Acc.</s><s xml:id="_MWyYbmB">indicate unweighted average recall, average precision, and accuracy, respectively.</s><s xml:id="_YZkv7br">It can be noted from the presented model that our proposed model reached the highest results as compared to the other state-of-the-techniques using a heterogeneous dataset.</s><s xml:id="_gbEAU7C">Therefore, the UAR metric is used to evaluate the performance of the models.</s><s xml:id="_vngAzUw">Tuncer et al. <ref type="bibr" target="#b5">[6]</ref> have used LOOCV to develop their model in their work.</s><s xml:id="_UBgpTm4">Hence, we have also used the LOOCV strategy to validate our results.</s><s xml:id="_67Xqhxz">We have obtained UAR of 2.95 % more than Tuncer et al.'s model <ref type="bibr" target="#b5">[6]</ref>.</s><s xml:id="_hntsrvz">Also, they have used a linear local feature generator to obtain the results and generated eight leveled feature generation structure.</s><s xml:id="_6pY3vSd">In this work, we have used six leveled feature generation structure and used nonlinear Present-Pat pattern.</s><s xml:id="_M3GV42Y">This nonlinear pattern successfully extracted subtle changes in the sound signal and yielded clinically  49.58 UAR Janott et al. <ref type="bibr" target="#b38">[39]</ref> 55.8 UAR Wang et al. <ref type="bibr" target="#b51">[52]</ref> 63.8 UAR Demir et al. <ref type="bibr" target="#b26">[27]</ref> 72.6 UAR Amiriparian et al. <ref type="bibr" target="#b9">[10]</ref> 67.0 UAR Zhang et al. <ref type="bibr" target="#b52">[53]</ref> 67.4 UAR Qian et al. <ref type="bibr" target="#b14">[15]</ref> 69.4 UAR Vesperini et al. <ref type="bibr" target="#b30">[31]</ref> 74.19 UAR Freitag et al. <ref type="bibr" target="#b53">[54]</ref> 72.6 UAR  Furthermore, we have also applied our proposed to a nocturnal sounds dataset <ref type="bibr" target="#b54">[55]</ref> to validate the robustness of our model.</s><s xml:id="_c4PtA5m">This dataset is available at: <ref type="url" target="http://web.firat.edu.tr/turkertuncer/nocturnal.rar">http://web.firat.edu.tr/turkertuncer/nocturnal.rar</ref> with seven classes.</s><s xml:id="_vMETt7M">The presented model has reached an accuracy of 97.49 % using this dataset.</s><s xml:id="_eamKqMQ">The work by <ref type="bibr" target="#b54">[55]</ref> has attained an accuracy of 97.22 % with the same database.</s><s xml:id="_a67wfJp">This confirms the success of the presented model.</s><s xml:id="_CCX2nFb">Accuracy (%) obtained using our method with nocturnal sound dataset is shown in Table <ref type="table" target="#tab_6">6</ref>.</s></p><p xml:id="_su7NqmJ"><s xml:id="_zx8EJpS">It can be noted from Table <ref type="table" target="#tab_6">6</ref> that the presented model achieved the highest accuracy (%) using a decision tree, Quadratic SVM, Cubic SVM, and kNN classifiers as compared to other models <ref type="bibr" target="#b54">[55]</ref>.</s></p><p xml:id="_Ugww87j"><s xml:id="_NHubbM5">Our developed automated snoring classification system can be employed in medical centers and homes to detect the type of snoring which can be used to detect the type of sleep disorder.</s><s xml:id="_6YmUSW7">The snapshot of such a cloud-based system is shown in Fig. <ref type="figure" target="#fig_8">8</ref>.</s></p><p xml:id="_TrHRUwQ"><s xml:id="_2NxfedN">It can be noted from Table <ref type="table" target="#tab_6">6</ref> that the highest performance is obtained using our present pattern-based model.</s><s xml:id="_maTqmNk">Our proposed nonlinear Present pattern is able to pick clearly the subtle changes from the features and yielded the highest classification performance.</s><s xml:id="_hAtpgVX">Other nonlinear methods like instance quantum transformations, chaos theorem, chaotic maps, entropies, and fractals can also be used.</s><s xml:id="_Mg8rqAH">Also, new deep learning architectures can be used for the automated classification of snoring sounds.</s></p><p xml:id="_b6j3GNc"><s xml:id="_4aq6WHy">It can be noted from Table <ref type="table" target="#tab_5">5</ref> that, the presented SSC model yielded 2.95 % higher UAR than the reported best methods.</s><s xml:id="_FbwvwaJ">The advantages and disadvantages of the proposed method are given below.</s></p><p xml:id="_ufkD5gx"><s xml:id="_hnb5dGC">The advantages of our model are as follows:</s></p><p xml:id="_27VzfMG"><s xml:id="_xBe8PkS">• Proposed a nonlinear textural feature generation function, a cryptographic structure (SBox of the lightweight Present cipher) based pattern is presented.</s><s xml:id="_uDs2xHs">The proposed method is able to extract subtle changes from the sound signal and yielded the highest classification performance.</s></p><p xml:id="_RxDek9k"><s xml:id="_sm8Zwy2">• A new decomposition method (MAP) is presented to perform the effective decomposition.</s><s xml:id="_ShthKnq">It routes both peak and minimum values depending on the situation.</s><s xml:id="_sQZ2fqY">• Given model has low computational complexity and hence it is fast.</s><s xml:id="_KVyRCe6">• NCAINCA selector solved the time complexity problem of NCA.</s></p><p xml:id="_st8Kq7U"><s xml:id="_j5cRfUU">• Developed model has yielded the highest performance, better than the state-of-art-technique models.</s><s xml:id="_GVhuxhP">• Proposed method is robust as it is generated using five various validation techniques.</s></p><p xml:id="_w8Rvyqs"><s xml:id="_W8eQNRp">The limitations of this work are given below:</s></p><p xml:id="_uSzUN8p"><s xml:id="_tTpt4CM">• The sound signals are fed to MAP to obtain both negative and positive values.</s><s xml:id="_SukSJ4z">• NCAINCA is an effective feature selector only if the appropriate threshold value is selected.</s><s xml:id="_pRTvneM">So, the performance depends on the selection of threshold values.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6." xml:id="_zTmVqHf">Conclusion</head><p xml:id="_aFtbcsB"><s xml:id="_b8CyFPC">Manual snoring sound detection is tedious, time-consuming, and prone to human errors.</s><s xml:id="_hMc9m7x">Hence, computer-assisted systems are proposed to aid clinicians in their diagnosis.</s><s xml:id="_BMdz7Mp">This paper presents a novel SSC system using three new functions: (i) multiple kernelled Present-Pat, ii) MAP decomposer and (iii) NCAINCA feature selector.</s><s xml:id="_8rgM23W">Our developed model has yielded an accuracy of 97.1 % and UAR of 97.6 % using kNN classifier with LOOCV strategy.</s><s xml:id="_4jTWu35">This SSC model performed better than the state-of-the-art techniques and can be used in medical centers to diagnose sleep-related diseases accurately by detecting the type of snoring sound (see Fig. <ref type="figure" target="#fig_8">8</ref>).</s></p><p xml:id="_m5kyqXc"><s xml:id="_vha3pJn">We have obtained the highest detection performance using MPSSC and nocturnal datasets.</s><s xml:id="_G6b3Sq7">In future, we intend to explore the possibility of using our developed model to detect various arrhythmias and neurological disorders using electrocardiogram (ECG) and electroencephalogram (EEG) signals, respectively.</s><s xml:id="_be3J526">Also, our developed system can be tested with big SSC and nocturnal datasets to classify sleep sound automatically.</s><s xml:id="_MpSSjCW">Our developed SSC model can be used in sleep centers to assist the sleep specialists in making accurate diagnosis.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc><div><p xml:id="_5xcB5nt"><s xml:id="_kXrteeX">Fig. 1.</s><s xml:id="_KT9QpHQ">General flow diagram of automated SSC model.</s></p></div></figDesc><graphic coords="4,117.64,55.43,360.00,351.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc><div><p xml:id="_TdzeRMn"><s xml:id="_jmn7kZP">Fig. 2. Snapshot of proposed multiple kernel Present-Pat, MAP feature generation, and NCAINCA selector based SSC method.</s></p></div></figDesc><graphic coords="5,55.16,55.44,216.00,446.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc><div><p xml:id="_bXnRYw6"><s xml:id="_W567aVa">Fig. 3. Illustration of MAP decomposition.</s></p></div></figDesc><graphic coords="5,324.11,181.40,216.00,96.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc><div><p xml:id="_WnVe8FZ"><s xml:id="_SSyrUuU">This work employed three novel functions: Present-Pat feature generation function, MAP decomposer, and NCAINCA selector to develop an accurate SSC model.</s><s xml:id="_vnHDkxc">The Present-Pat based snoring sound classification</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc><div><p xml:id="_WZrmw8C"><s xml:id="_6R4Ck9w">Fig. 4. Plot of accuracies versus the number of levels obtained using our proposed method.</s></p></div></figDesc><graphic coords="6,324.11,536.22,216.00,180.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc><div><p xml:id="_zfjKVEd"><s xml:id="_bZV8Kde">Fig. 5. Graph of loss value versus the number of features obtained during feature selection process using NCAINCA selector.</s></p></div></figDesc><graphic coords="7,55.16,55.44,216.00,180.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc><div><p xml:id="_V7aaXD5"><s xml:id="_hbg5nf2">Fig. 7. Confusion matrices obtained using four validation techniques with our proposed method: (a) hold-out (75 %-25 %), (b) hold-out (50 %-50 %), (c) 3-fold, and (d) 10-fold.</s></p></div></figDesc><graphic coords="8,81.64,55.42,432.00,329.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><figDesc><div><p xml:id="_dTnkn6S"><s xml:id="_ceGnUgF">significant features.</s><s xml:id="_mUABY9w">Freitag et al.<ref type="bibr" target="#b53">[54]</ref> have used a deep model to classify the snoring sound signals.</s><s xml:id="_BXnab5y">They used a spectrogram image of sound signals and a pre-trained deep feature generator.</s><s xml:id="_JbKn32J">A metaheuristic optimization-based feature selector is utilized to select the features and these selected features are classified using SVM classifier.</s><s xml:id="_nSGh3fY">Their model used deep learning and metaheuristic optimization-based feature selectors, which is computationally expensive (high computational complexity).</s><s xml:id="_MuMQnJB">Our presented model reached higher performance than Freitag et al.<ref type="bibr" target="#b53">[54]</ref> model with lower time burden.</s><s xml:id="_yzvkUAp">Furthermore, Vesperini et al.'s<ref type="bibr" target="#b30">[31]</ref> used a deep model to classify these snoring sound signals and obtained an accuracy of 74.19 %.</s><s xml:id="_GtBZXXy">But our model has yielded the highest accuracy of 97.58 % and lower computational complexity.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc><div><p xml:id="_7PaxYVk"><s xml:id="_fJn4dhA">Fig. 8. Snapshot of the planned web-based real-time snoring sound classification and monitoring application.</s></p></div></figDesc><graphic coords="9,81.64,55.41,432.00,243.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="3,118.09,473.72,359.14,248.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc><div><p xml:id="_mJVGsDB"><s xml:id="_bKjSdVD">SBox of the present cipher.</s></p></div></figDesc><table><row><cell>t</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>16</cell></row><row><cell>S(t)</cell><cell>6</cell><cell>1</cell><cell>2</cell><cell>5</cell><cell>8</cell><cell>13</cell><cell>4</cell><cell>9</cell><cell>14</cell><cell>7</cell><cell>10</cell><cell>3</cell><cell>16</cell><cell>11</cell><cell>12</cell><cell>15</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc><div><p xml:id="_vTfXQ74"><s xml:id="_y8AJJNT">Results obtained using the proposed method.</s></p></div></figDesc><table><row><cell>Measurement metric</cell><cell>Result (%)</cell></row><row><cell>Accuracy</cell><cell>97.10</cell></row><row><cell>GM</cell><cell>97.58</cell></row><row><cell>F1-score</cell><cell>97.55</cell></row><row><cell>UAR</cell><cell>97.60</cell></row><row><cell>AP</cell><cell>97.50</cell></row><row><cell>Cohen's kappa</cell><cell>94.97</cell></row><row><cell>MCC</cell><cell>96.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc><div><p xml:id="_G4JAfd4"><s xml:id="_C3VeJgb">Confusion matrix obtained using the proposed model.</s></p></div></figDesc><table><row><cell>Real/actual category</cell><cell cols="2">Predicted category V O</cell><cell>T</cell><cell>E</cell><cell>Recall (%)</cell></row><row><cell>V</cell><cell>473</cell><cell>9</cell><cell>0</cell><cell>2</cell><cell>97.73</cell></row><row><cell>O</cell><cell>10</cell><cell>205</cell><cell>0</cell><cell>1</cell><cell>94.91</cell></row><row><cell>T</cell><cell>0</cell><cell>0</cell><cell>39</cell><cell>0</cell><cell>100.0</cell></row><row><cell>E</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>87</cell><cell>97.75</cell></row><row><cell>Precision (%)</cell><cell>97.53</cell><cell>95.79</cell><cell>100.0</cell><cell>96.67</cell><cell>97.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc><div><p xml:id="_d3JYDKV"><s xml:id="_jD72FYp">Results (%) obtained using various validation techniques.</s></p></div></figDesc><table><row><cell>Measurement</cell><cell>Hold-out</cell><cell>Hold-out</cell><cell>3-fold</cell><cell>10-fold</cell></row><row><cell>metric</cell><cell>(75:25)</cell><cell>(50:50)</cell><cell>CV</cell><cell>CV</cell></row><row><cell>Accuracy(%)</cell><cell>97.58</cell><cell>93.96</cell><cell>94.44</cell><cell>95.77</cell></row><row><cell>GM(%)</cell><cell>97.60</cell><cell>93.80</cell><cell>93.18</cell><cell>95.82</cell></row><row><cell>F1-score(%)</cell><cell>98.28</cell><cell>94.96</cell><cell>94.18</cell><cell>95.70</cell></row><row><cell>UAR(%)</cell><cell>97.69</cell><cell>93.97</cell><cell>93.27</cell><cell>95.86</cell></row><row><cell>AP(%)</cell><cell>99.02</cell><cell>96.11</cell><cell>95.14</cell><cell>95.61</cell></row><row><cell>Cohen's kappa(%)</cell><cell>95.71</cell><cell>89.29</cell><cell>90.27</cell><cell>92.63</cell></row><row><cell>MCC(%)</cell><cell>97.21</cell><cell>92.22</cell><cell>91.64</cell><cell>93.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc><div><p xml:id="_khwjJxt"><s xml:id="_3ZmTgUx">Summary of comparison with other similar works developed using the same database.</s></p></div></figDesc><table><row><cell>Work</cell><cell>Result (%)</cell><cell>Performance metric</cell></row><row><cell>Albornoz et al. [22]</cell><cell>49.38</cell><cell>UAR</cell></row><row><cell>Rao et al. [51]</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc><div><p xml:id="_wfqw2u2"><s xml:id="_T2RfMST">Accuracy (%) obtained using our method with the nocturnal sound dataset.</s></p></div></figDesc><table><row><cell></cell><cell>94.65</cell><cell>UAR</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tuncer et al. [6]</cell><cell>95.53</cell><cell>Acc.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>95.84</cell><cell>AP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>97.10</cell><cell>Acc.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our method (LOOCV)</cell><cell>97.60</cell><cell>UAR</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>97.50</cell><cell>AP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>97.58</cell><cell>Acc.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our method hold-out (75:25)</cell><cell>97.69</cell><cell>UAR</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>99.02</cell><cell>AP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>93.96</cell><cell>Acc.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our method hold-out (50:50)</cell><cell>93.97</cell><cell>UAR</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>96.11</cell><cell>AP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>94.44</cell><cell>Acc.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our method 3-fold CV</cell><cell>93.27</cell><cell>UAR</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>95.14</cell><cell>AP</cell><cell>Classifier used</cell><cell>Accuracy(%) using our model</cell><cell>Accuracy(%) using [55]</cell></row><row><cell>Our method 10-fold CV</cell><cell>95.77 95.86 95.61</cell><cell>Acc. UAR AP</cell><cell>Decision tree Linear discriminant Quadratic SVM</cell><cell>88.43 93.86 97.14</cell><cell>86.71 94.29 96.71</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Cubic SVM</cell><cell>97.85</cell><cell>97.43</cell></row><row><cell></cell><cell></cell><cell></cell><cell>kNN</cell><cell>98.14</cell><cell>98.0</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AjzMwVQ">Declaration of Competing Interest</head><p xml:id="_CP2gPEG"><s xml:id="_hMYW5Rh">The authors report no declarations of interest.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_qs3yJKG">Ambient air pollutants aggravate association of snoring with prevalent hypertension: results from the Henan Rural Cohort</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdulai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fxmYtKW">Chemosphere</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">127108</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang H, Li S, Chen G, Abdulai T, Liu X, Wang Y, et al. Ambient air pollutants aggravate association of snoring with prevalent hypertension: results from the Henan Rural Cohort. Chemosphere 2020:127108.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_bqwunEg">An efficient method for snore/nonsnore classification of sleep sounds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavusoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Erogul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ciloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Serinagaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akcam</surname></persName>
		</author>
		<idno type="DOI">10.1088/0967-3334/28/8/007</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8dkpkSW">Physiol Meas</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">841</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cavusoglu M, Kamasak M, Erogul O, Ciloglu T, Serinagaoglu Y, Akcam T. An efficient method for snore/nonsnore classification of sleep sounds. Physiol Meas 2007;28:841.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_9hqgq6r">Snoring patterns during home polysomnography. A proposal for a new classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Chiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boccuzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XGBRuQD">Am J Otolaryngol</title>
		<imprint>
			<biblScope unit="page">102589</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cambi J, Chiri ZM, Boccuzzi S. Snoring patterns during home polysomnography. A proposal for a new classification. Am J Otolaryngol 2020:102589.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_B63tmkX">Drug-induced sleep endoscopy: the VOTE classification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Kezirian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hohenhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Vries</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00405-011-1633-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ea5mFrs">Eur Arch Oto-Rhino-laryngol</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="page" from="1233" to="1236" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kezirian EJ, Hohenhorst W, de Vries N. Drug-induced sleep endoscopy: the VOTE classification. Eur Arch Oto-Rhino-laryngol 2011;268:1233-6.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_Q6sw7hU">Video sleep nasendoscopy: the Hong Kong experience</title>
		<author>
			<persName><forename type="first">V</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="DOI">10.1016/s0030-6665(02)00176-7</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_t2PYvgU">Otolaryngol Clin North Am</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="461" to="471" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abdullah V, Wing Y, Van Hasselt C. Video sleep nasendoscopy: the Hong Kong experience. Otolaryngol Clin North Am 2003;36:461-71.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_wV7zSPp">An automated snoring sound classification method based on local dual octal pattern and iterative hybrid feature selector</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuncer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Akbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dogan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bspc.2020.102173</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_U2Eufer">Biomed Signal Process Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page">102173</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tuncer T, Akbal E, Dogan S. An automated snoring sound classification method based on local dual octal pattern and iterative hybrid feature selector. Biomed Signal Process Control 2020;63:102173.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_sy55jg3">Overview and implications of obstructive sleep apnoea</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.53347/rid-72580</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_a4fwEuH">Indian J Chest Dis Allied Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sharma H, Sharma S. Overview and implications of obstructive sleep apnoea. Indian J Chest Dis Allied Sci 2008;50:137.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_f8tpYDa">Palatal procedures for obstructive sleep apnea</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yaremchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yus2r6m">Otolaryngol Clin North Am</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1383" to="1397" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yaremchuk K. Palatal procedures for obstructive sleep apnea. Otolaryngol Clin North Am 2016;49:1383-97.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_gwG3BED">Clinical value of polysomnography</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">Ma</forename></persName>
		</author>
		<idno type="DOI">10.1016/0140-6736(92)91660-z</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nyajvRF">Lancet</title>
		<imprint>
			<biblScope unit="volume">339</biblScope>
			<biblScope unit="page" from="347" to="350" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Douglas NJ, Thomas S, Jan MA. Clinical value of polysomnography. Lancet 1992; 339:347-50.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_jBeK6RT">Snore sound classification using image-based deep spectrum features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Amiriparian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ottl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pugachevskiy</surname></persName>
		</author>
		<idno type="DOI">10.21437/interspeech.2017-434</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_W7TfvNg">INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="3512" to="3516" />
		</imprint>
	</monogr>
	<note type="raw_reference">Amiriparian S, Gerczuk M, Ottl S, Cummins N, Freitag M, Pugachevskiy S, et al. Snore sound classification using image-based deep spectrum features. INTERSPEECH 2017:3512-6.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_6ZcYahw">Automatic breath and snore sounds classification from tracheal and ambient sounds recordings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yadollahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Moussavi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.medengphy.2010.06.013</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_B4pGVA5">Med Eng Phys</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="985" to="990" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yadollahi A, Moussavi Z. Automatic breath and snore sounds classification from tracheal and ambient sounds recordings. Med Eng Phys 2010;32:985-90.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_uKvac5F">A deep learning model for snoring detection and vibration notification using a smart wearable gadget</title>
		<author>
			<persName><forename type="first">T</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HuDYVFn">Electronics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">987</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Khan T. A deep learning model for snoring detection and vibration notification using a smart wearable gadget. Electronics 2019;8:987.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_QtFsbm2">Classification of snoring sound based on a recurrent neural network</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ko</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2019.01.020</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Tj7ycJD">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="237" to="245" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lim SJ, Jang SJ, Lim JY, Ko JH. Classification of snoring sound based on a recurrent neural network. Expert Syst Appl 2019;123:237-45.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_4PRXJrP">Automatic snore sound extraction from sleep sound recordings via auditory image modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nonaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Emoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Abeyratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jinnouchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kawata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ohnishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JHNd35E">Biomed Signal Process Control</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="7" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nonaka R, Emoto T, Abeyratne UR, Jinnouchi O, Kawata I, Ohnishi H, et al. Automatic snore sound extraction from sleep sound recordings via auditory image modeling. Biomed Signal Process Control 2016;27:7-14.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_HpE8ePy">A bag of wavelet features for snore sound classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hohenhorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eauSVCU">Ann Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1000" to="1011" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qian K, Schmitt M, Janott C, Zhang Z, Heiser C, Hohenhorst W, et al. A bag of wavelet features for snore sound classification. Ann Biomed Eng 2019;47:1000-11.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_txbk25V">Acoustic information in snoring noises</title>
		<author>
			<persName><forename type="first">C</forename><surname>Janott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WaRarMD">HNO</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Janott C, Schuller B, Heiser C. Acoustic information in snoring noises. HNO 2017; 65:107-16.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_W3MZX6a">Prediction of obstructive sleep apnea based on respiratory sounds recorded between sleep onset and sleep offset</title>
		<author>
			<persName><forename type="first">J-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-S</forename><surname>Rhee</surname></persName>
		</author>
		<idno type="DOI">10.21053/ceo.2018.00388</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PQeMzNG">Clin Exp Otorhinolaryngol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kim J-W, Kim J-W, Shin J, Choe G, Lim HJ, Rhee C-S, et al. Prediction of obstructive sleep apnea based on respiratory sounds recorded between sleep onset and sleep offset. Clin Exp Otorhinolaryngol 2019;12:72.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_vMukaQR">Automatic snoring sounds detection from sleep sounds based on deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13246-020-00876-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eXpQVju">Phys Eng. Sci Med</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiang Y, Peng J, Zhang X. Automatic snoring sounds detection from sleep sounds based on deep learning. Phys Eng. Sci Med 2020.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_6jwJKkQ">Non-contact sleep stage detection using canonical correlation analysis of respiratory sound</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_syfrFEy">IEEE J Biomed Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="614" to="625" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xue B, Deng B, Hong H, Wang Z, Zhu X, Feng DD. Non-contact sleep stage detection using canonical correlation analysis of respiratory sound. IEEE J Biomed Health Inform 2019;24:614-25.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_NHUQM58">Sleep staging using nocturnal sound analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dafna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tarasiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zigel</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-018-31748-0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qWDtS48">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dafna E, Tarasiuk A, Zigel Y. Sleep staging using nocturnal sound analysis. Sci Rep 2018;8:1-14.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_bdKt3Aa">Classification of the excitation location of snore sounds in the upper airway by acoustic multifeature analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hohenhorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QFgn7ex">IEEE Trans Biomed Eng</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1731" to="1741" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qian K, Janott C, Pandit V, Zhang Z, Heiser C, Hohenhorst W, et al. Classification of the excitation location of snore sounds in the upper airway by acoustic multifeature analysis. IEEE Trans Biomed Eng 2016;64:1731-41.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_rYtwCjq">Snore recognition using a reduced set of spectral features</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Albornoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Bugnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Martínez</surname></persName>
		</author>
		<idno type="DOI">10.23919/rpic.2017.8214357</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_aZzj4F9">2017 XVII Workshop on Information Processing and Control (RPIC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="raw_reference">Albornoz EM, Bugnon LA, Martínez CE. Snore recognition using a reduced set of spectral features. In: 2017 XVII Workshop on Information Processing and Control (RPIC); 2017. p. 1-5.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_V9AKtpx">A discriminative filter bank model for speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Biem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katagiri</surname></persName>
		</author>
		<idno type="DOI">10.21437/eurospeech.1995-140</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qanQwGZ">Fourth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Biem A, McDermott E, Katagiri S. A discriminative filter bank model for speech recognition. Fourth European Conference on Speech Communication and Technology 1995.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_WDtSaSb">The support vector method of function estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2RPK64e">Nonlinear Modeling</title>
		<imprint>
			<biblScope unit="page" from="55" to="85" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Vapnik V. The support vector method of function estimation. Nonlinear Modeling: Springer; 1998. p. 55-85.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main" xml:id="_2EJGWE2">The nature of statistical learning theory</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4757-3264-1_8</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer science &amp; business media</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Vapnik V. The nature of statistical learning theory. Springer science &amp; business media; 2013.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_sCxPMgB">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5FXqng2">Commun ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Commun ACM 2017;60:84-90.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_MJwq69G">Low level texture features for snore sound discrimination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sengur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amiriparian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.1109/embc.2018.8512459</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rhvKEug">2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="413" to="416" />
		</imprint>
	</monogr>
	<note type="raw_reference">Demir F, Sengur A, Cummins N, Amiriparian S, Schuller B. Low level texture features for snore sound discrimination. In: 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC); 2018. p. 413-6.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_tcQzGzH">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BXH7J2c">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ojala T, Pietikainen M, Maenpaa T. Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. IEEE Trans Pattern Anal Mach Intell 2002;24:971-87.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_g9Y45Fh">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2005.177</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xX6J3AP">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dalal N, Triggs B. Histograms of oriented gradients for human detection. In: 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05); 2005. p. 886-93.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main" xml:id="_P5WUJG7">Model determination using predictive distributions with implementation via sampling-based methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1093/oso/9780198522669.003.0009</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>Stanford Univ CA Dept of Statistics</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Gelfand AE, Dey DK, Chang H. Model determination using predictive distributions with implementation via sampling-based methods. Stanford Univ CA Dept of Statistics; 1992.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_bGyBVnQ">Snore sounds excitation localization by using scattering transform and deep neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vesperini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Principi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Squartini</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2018.8489576</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_gcjmBGz">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note type="raw_reference">Vesperini F, Galli A, Gabrielli L, Principi E, Squartini S. Snore sounds excitation localization by using scattering transform and deep neural networks. In: 2018 International Joint Conference on Neural Networks (IJCNN); 2018. p. 1-8.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_P42Hzqh">A classification method related to respiratory disorder events based on acoustical analysis of snoring</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kVq9QGF">Arch Acoust</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="141" to="151" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang C, Peng J, Zhang X. A classification method related to respiratory disorder events based on acoustical analysis of snoring. Arch Acoust 2020;45:141-51.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_x4TBdCp">Learning representations by backpropagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1038/323533a0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_STtnDhk">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rumelhart DE, Hinton GE, Williams RJ. Learning representations by back- propagating errors. Nature 1986;323:533-6.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main" xml:id="_uksabTH">Report of the workshop on cryptography in support of computer security</title>
		<author>
			<persName><forename type="first">D</forename><surname>Branstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Katzke</surname></persName>
		</author>
		<idno type="DOI">10.6028/nbs.ir.77-1291</idno>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>National Institute of Standards and Technology</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Branstad D, Gait J, Katzke S. Report of the workshop on cryptography in support of computer security. National Institute of Standards and Technology; 1977.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_6Sc6Fde">Essential algebraic structure within the AES</title>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Robshaw</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45708-9_1</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vKWB4fy">Annual International Cryptology Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note type="raw_reference">Murphy S, Robshaw MJ. Essential algebraic structure within the AES. In: Annual International Cryptology Conference; 2002. p. 1-16.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_bjn5Ght">A simple algebraic representation of rijndael</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schroeppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whiting</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-45537-x_8</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jnM2nJX">International Workshop on Selected Areas in Cryptography</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ferguson N, Schroeppel R, Whiting D. A simple algebraic representation of rijndael. In: International Workshop on Selected Areas in Cryptography; 2001. p. 103-11.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_CGq7wEE">In: A Systematic Evaluation of Compact Hardware Implementations for the Rijndael S-Box</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mentens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Batina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Preneel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Verbauwhede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_dSnpadt">Cryptographers&apos; Track at the RSA Conference</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="323" to="333" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mentens N, Batina L, Preneel B, Verbauwhede I. In: A Systematic Evaluation of Compact Hardware Implementations for the Rijndael S-Box. Cryptographers&apos; Track at the RSA Conference; 2005. p. 323-33.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_8yagUP4">Integration of deep feature representations and handcrafted features to improve the prediction of N6methyladenosine sites</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.04.082</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_agDajgp">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="page" from="3" to="9" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wei L, Su R, Wang B, Li X, Zou Q, Gao X. Integration of deep feature representations and handcrafted features to improve the prediction of N6- methyladenosine sites. Neurocomputing 2019;324:3-9.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_hc8PdCT">Snoring classified: the Munich-Passau snore sound corpus</title>
		<author>
			<persName><forename type="first">C</forename><surname>Janott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2018.01.007</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hX72vEV">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="106" to="118" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Janott C, Schmitt M, Zhang Y, Qian K, Pandit V, Zhang Z, et al. Snoring classified: the Munich-Passau snore sound corpus. Comput Biol Med 2018;94:106-18.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_UGcYw4M">VOTE versus ACLTE: comparison of two snoring noise classifications using machine learning methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Janott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hohenhorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WAYbUNB">HNO</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="670" to="678" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Janott C, Schmitt M, Heiser C, Hohenhorst W, Herzog M, Carrasco ML, et al. VOTE versus ACLTE: comparison of two snoring noise classifications using machine learning methods. HNO 2019;67:670-8.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_aSBXPng">Novel multi center and threshold ternary pattern based method for disease detection method using voice</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuncer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özyurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Belhaouari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bensmail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EzFb7mm">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="84532" to="84540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tuncer T, Dogan S, Özyurt F, Belhaouari SB, Bensmail H. Novel multi center and threshold ternary pattern based method for disease detection method using voice. IEEE Access 2020;8:84532-40.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_NSJCpXJ">Ns-k-nn: Neutrosophic set-based knearest neighbors classifier</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Akbulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sengur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Smarandache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Xwr5DA5">Symmetry</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">179</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Akbulut Y, Sengur A, Guo Y, Smarandache F. Ns-k-nn: Neutrosophic set-based k- nearest neighbors classifier. Symmetry 2017;9:179.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_WH6PCcE">Sensör is ¸aretlerinden cinsiyet tanıma için yerel ikili örüntüler tabanlı yeni yaklas ¸ımlar</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kuncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kuncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EcFX7qB">J Faculty Eng Arch Gazi Univ</title>
		<imprint>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kuncan F, Kaya Y, Kuncan M. Sensör is ¸aretlerinden cinsiyet tanıma için yerel ikili örüntüler tabanlı yeni yaklas ¸ımlar. J Faculty Eng Arch Gazi Univ 2019:34.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_d9MpkxH">Classification of focal and non-focal EEG signals using neighborhood component analysis and machine learning algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sriraam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Mdkj44C">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="18" to="32" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Raghu S, Sriraam N. Classification of focal and non-focal EEG signals using neighborhood component analysis and machine learning algorithms. Expert Syst Appl 2018;113:18-32.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_FHVPtnv">kNN-IS: an iterative spark-based design of the k-nearest neighbors classifier for big data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Maillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ramírez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_a6ZFTsY">Knowledge Based Syst</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maillo J, Ramírez S, Triguero I, Herrera F. kNN-IS: an iterative spark-based design of the k-nearest neighbors classifier for big data. Knowledge Based Syst 2017;117: 3-15.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_UTw3aBd">Practical Bayesian model evaluation using leaveone-out cross-validation and WAIC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gabry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2XWDMDE">Stat Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1413" to="1432" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vehtari A, Gelman A, Gabry J. Practical Bayesian model evaluation using leave- one-out cross-validation and WAIC. Stat Comput 2017;27:1413-32.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_G7C4SYM">Automated arrhythmia detection using novel hexadecimal local pattern and multilevel wavelet transform with ECG signals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tuncer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pławiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">R</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3MaBXeM">Knowledge Based Syst</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page">104923</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tuncer T, Dogan S, Pławiak P, Acharya UR. Automated arrhythmia detection using novel hexadecimal local pattern and multilevel wavelet transform with ECG signals. Knowledge Based Syst 2019;186:104923.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_NC5SMCN">The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chicco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jurman</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12864-019-6413-7</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cQTAUb7">BMC Genomics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chicco D, Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation. BMC Genomics 2020;21:6.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_tDYwFAR">On the equivalence of Cohen&apos;s kappa and the Hubert-Arabie adjusted Rand index</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Warrens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_h7CNd3K">J Classif</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Warrens MJ. On the equivalence of Cohen&apos;s kappa and the Hubert-Arabie adjusted Rand index. J Classif 2008;25:177-83.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Powers</surname></persName>
		</author>
		<idno>arXiv 2020:201016061</idno>
		<title level="m" xml:id="_hbZ9wB6">Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Powers DM. Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv preprint arXiv 2020:201016061.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_d75TmG2">A dual source-filter model of snore audio for snorer group classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QktUWrG">INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="3502" to="3506" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rao MA, Yadav S, Ghosh PK. A dual source-filter model of snore audio for snorer group classification. INTERSPEECH 2017:3502-6.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_R7b97Sb">A CNN-GRU approach to capture timefrequency pattern interdependence for snore sound classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Strömfeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_u6pKvAE">2018 26th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="997" to="1001" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wang J, Strömfeli H, Schuller BW. A CNN-GRU approach to capture time- frequency pattern interdependence for snore sound classification. In: 2018 26th European Signal Processing Conference (EUSIPCO); 2018. p. 997-1001.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_UTRK5uD">Snore-GANs: Improving automatic snore sound classification with synthesized data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Janott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aMURtRm">IEEE J Biomed Health Inform</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="300" to="310" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang Z, Han J, Qian K, Janott C, Guo Y, Schuller B. Snore-GANs: Improving automatic snore sound classification with synthesized data. IEEE J Biomed Health Inform 2019;24:300-10.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_bdvdZjy">An&apos;end-toevolution&apos;hybrid approach for snore sound classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amiriparian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerczuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<idno type="DOI">10.21437/interspeech.2017-173</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pJfMwgz">INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="3507" to="3511" />
		</imprint>
	</monogr>
	<note type="raw_reference">Freitag M, Amiriparian S, Cummins N, Gerczuk M, Schuller BW. An&apos;end-to- evolution&apos;hybrid approach for snore sound classification. INTERSPEECH 2017: 3507-11.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_3atEFg5">FusedTSNet: an automated nocturnal sleep sound classification method based on a fused textural and statistical feature generation network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuncer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.apacoust.2020.107559</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Msn3myp">Appl Acoust</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page">107559</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Akbal E, Tuncer T. FusedTSNet: an automated nocturnal sleep sound classification method based on a fused textural and statistical feature generation network. Appl Acoust 2021;171:107559.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
