<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_wUW7cfy">Recommendation system for human physical activities using smartphones</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nesrine</forename><surname>Kadri</surname></persName>
							<email>kadrinesrine2@gmail.com</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">CEM Lab , ENIS , University of Sfax , ISIT&apos;COM , University of Sousse CEM Lab , Departement of Electrical Engineering , ENIS , University of Sfax CEM Lab , Departement of Electrical Engineering , ENIS , University of Sfax</note>
								<orgName type="department" key="dep1">ISIT&apos;COM</orgName>
								<orgName type="department" key="dep2">Departement of Electrical Engineering</orgName>
								<orgName type="department" key="dep3">Departement of Electrical Engineering</orgName>
								<orgName type="laboratory" key="lab1">CEM Lab</orgName>
								<orgName type="laboratory" key="lab2">CEM Lab</orgName>
								<orgName type="laboratory" key="lab3">CEM Lab</orgName>
								<orgName type="institution" key="instit1">ENIS</orgName>
								<orgName type="institution" key="instit2">University of Sfax</orgName>
								<orgName type="institution" key="instit3">University of Sousse</orgName>
								<orgName type="institution" key="instit4">ENIS</orgName>
								<orgName type="institution" key="instit5">University of Sfax</orgName>
								<orgName type="institution" key="instit6">ENIS</orgName>
								<orgName type="institution" key="instit7">University of Sfax</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ameni</forename><surname>Ellouze</surname></persName>
							<email>ameni_ellouze@yahoo.fr</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">CEM Lab , ENIS , University of Sfax , ISIT&apos;COM , University of Sousse CEM Lab , Departement of Electrical Engineering , ENIS , University of Sfax CEM Lab , Departement of Electrical Engineering , ENIS , University of Sfax</note>
								<orgName type="department" key="dep1">ISIT&apos;COM</orgName>
								<orgName type="department" key="dep2">Departement of Electrical Engineering</orgName>
								<orgName type="department" key="dep3">Departement of Electrical Engineering</orgName>
								<orgName type="laboratory" key="lab1">CEM Lab</orgName>
								<orgName type="laboratory" key="lab2">CEM Lab</orgName>
								<orgName type="laboratory" key="lab3">CEM Lab</orgName>
								<orgName type="institution" key="instit1">ENIS</orgName>
								<orgName type="institution" key="instit2">University of Sfax</orgName>
								<orgName type="institution" key="instit3">University of Sousse</orgName>
								<orgName type="institution" key="instit4">ENIS</orgName>
								<orgName type="institution" key="instit5">University of Sfax</orgName>
								<orgName type="institution" key="instit6">ENIS</orgName>
								<orgName type="institution" key="instit7">University of Sfax</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Ksantini</surname></persName>
							<email>mohamedksantini@yahoo.fr</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">CEM Lab , ENIS , University of Sfax , ISIT&apos;COM , University of Sousse CEM Lab , Departement of Electrical Engineering , ENIS , University of Sfax CEM Lab , Departement of Electrical Engineering , ENIS , University of Sfax</note>
								<orgName type="department" key="dep1">ISIT&apos;COM</orgName>
								<orgName type="department" key="dep2">Departement of Electrical Engineering</orgName>
								<orgName type="department" key="dep3">Departement of Electrical Engineering</orgName>
								<orgName type="laboratory" key="lab1">CEM Lab</orgName>
								<orgName type="laboratory" key="lab2">CEM Lab</orgName>
								<orgName type="laboratory" key="lab3">CEM Lab</orgName>
								<orgName type="institution" key="instit1">ENIS</orgName>
								<orgName type="institution" key="instit2">University of Sfax</orgName>
								<orgName type="institution" key="instit3">University of Sousse</orgName>
								<orgName type="institution" key="instit4">ENIS</orgName>
								<orgName type="institution" key="instit5">University of Sfax</orgName>
								<orgName type="institution" key="instit6">ENIS</orgName>
								<orgName type="institution" key="instit7">University of Sfax</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_jcrkRrv">Recommendation system for human physical activities using smartphones</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">133CF3C0B450FD3454F9A9BE20E5585C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T05:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_7Pay7sr">physical movements recognition</term>
					<term xml:id="_r2zQAyN">machine learning</term>
					<term xml:id="_2ZcbeWu">deep learning</term>
					<term xml:id="_pyVdtPN">healthy recommendation system</term>
					<term xml:id="_ZshgvBX">smartphone sensors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_hurtRv8"><p xml:id="_ajfeymJ"><s xml:id="_BMjaVn6">Important information can be obtained from smartphone users data such as profile modeling, behavior recognition, geolocalization, etc. Human activity recognition (HAR) from sensor smartphone data is a field which has garnered a lot of attention due to its high application in various domains such as the user health.</s><s xml:id="_sfw8ASU">In this paper, we will consider data from accelerometer to recognize the kind of user movements that we will classify to six kinds using machine and deep learning algorithms.</s><s xml:id="_nEyk2j6">Then, based on these results, we will make a recommendation system to inform the users of smartphone about their healthy behavior related to their physical activities.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.32" lry="841.92"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NrqGQ9N">I. INTRODUCTION</head><p xml:id="_k3gHhWr"><s xml:id="_5uv3x75">Beyond their use in calls, messages, mailing and connectivity, smartphones can give us many data that we can extract knowledge used in many aspects of our life.</s></p><p xml:id="_Pm5NzC4"><s xml:id="_yjM3fgE">Mobile devices especially smartphones contain big data such as the geolocalization, movement, etc.</s><s xml:id="_6ss9Zbw">This work deals with the recognition of physical activities which is obtained from the classification of person's movements.</s><s xml:id="_V5EEZbU">Considered movements are often physical activities such as jogging, walking, upstairs, downstairs, standing and sitting.</s><s xml:id="_MRMUu6V">We will collect movement data from the accelerometer smartphone sensor.</s><s xml:id="_g6NMwtR">This data will be trained in first time and features will be extracted from it in the second time.</s><s xml:id="_Z8uMRRN">These transformed data were segmented and a recognition model was created to identify the user's activities.</s><s xml:id="_h9xExxu">Many methods have been developed that can identify these activities such as machine learning algorithms or deep learning algorithms.</s><s xml:id="_CBfA73j">In the case of machine learning, we can talk about the most used methods with this kind of data, like Support Vector Machine (SVM) <ref type="bibr" target="#b0">[1]</ref>, Decision Tree algorithm <ref type="bibr" target="#b1">[2]</ref> and K-Nearest Neighbors (KNN) <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_KyZ3hCR">In the case of deep learning, we can talk about convolutional neural networks <ref type="bibr" target="#b3">[4]</ref> and recurrent neural networks <ref type="bibr" target="#b4">[5]</ref>.</s></p><p xml:id="_93VWDTB"><s xml:id="_vvfz9xR">In this work, we will use decision tree algorithm and bidirectional long short-term memory (BiLSTM) algorithm to train and classify the physical activities.</s></p><p xml:id="_daQE9Da"><s xml:id="_W4hsyCJ">We will compare results from machine learning and deep learning algorithms.</s><s xml:id="_FXyXpNn">We will compute the total time of physical activities consuming calories (jogging, walking, upstairs and downstairs) to decide about the healthy behavior of a person taking on his gender and age, based on their daily physical activities.</s></p><p xml:id="_3wjSEdG"><s xml:id="_CUVhCgg">This system can be implemented as an application on smartphone to inform users that don't move enough on a day.</s><s xml:id="_sRqTY8Z">This paper will be structured as follows: Section 2 will present machine learning and deep learning applied to mobile sensors data.</s><s xml:id="_cJVnVjX">Section 3 describes our method used to predict physical activities.</s><s xml:id="_vPQfe23">Section 4 presents experimental results and discussion.</s><s xml:id="_P7e5KDk">In conclusion, we will propose some perspectives to extend this work.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_26Pn6tq">II. MACHINE AND DEEP LEARNING</head><p xml:id="_favD4ug"><s xml:id="_tgg8C3b">Machine and Deep learning are two disciplines of Artificial Intelligence used to classify and predict phenomena in large scale of fields.</s><s xml:id="_r2TYtZE">The performance of algorithms used in machine and deep learning depends on the nature and the origin of data.</s></p><p xml:id="_cCvPt9T"><s xml:id="_wmxZMXr">In our case, we will use data from accelerometer sensors that gives signals divided into three variables X, Y and Z positions taken of different periods of time.</s></p><p xml:id="_GugraZH"><s xml:id="_3N9TdCf">The machine and deep learning algorithms will be applied to determine the best accuracy based on our date.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_EGTMZFq">A. Machine learning</head><p xml:id="_PwksZC8"><s xml:id="_7VjBrG9">Many methods can be used in machine learning like SVM and KNN.</s><s xml:id="_YNKjdzF">Hence, A. Jain et al. proposed in <ref type="bibr" target="#b5">[6]</ref> these two classifiers for human activity classification using accelerometer and gyroscope sensor signals of Smartphone.</s><s xml:id="_CrNgGmp">Moreover, by using Radial Basis Function kernel (RBF) with SVM, W. Wang et al. proposed in <ref type="bibr" target="#b6">[7]</ref> a classifier that uses confusion matrix to give information about the actual and predicted classifications of nine activities.</s><s xml:id="_k5cgFMX">This algorithm gave different recognition accuracies.</s><s xml:id="_YFdhHm2">For example, biking and gym-bike activities give higher accuracy (82% and 80% respectively), but Walking presents the lowest accuracy (51%).</s><s xml:id="_wMKJa33">For better performance methods of activity recognition than previous ones, R. Yang and B. Wang proposed in <ref type="bibr" target="#b0">[1]</ref> a new position-independent method called PACP (Parameters Adjustment Corresponding to smartphone Position).</s><s xml:id="_cZCC8MA">In this method, many performances were extracted from sensors data to recognize the position of the smartphone and the activities by the trained SVM model.</s><s xml:id="_hwC7paE">Another model proposed by A. Harasimowic et al. in <ref type="bibr" target="#b2">[3]</ref> named the KNN algorithm which used for to classify eight user activities such as running, walking, going upstairs, going downstairs, standing, lying, turning and sitting.</s></p><p xml:id="_Y4A2nEn"><s xml:id="_EEWAXky">Decision tree algorithms could be also used to recognize human physical activities.</s><s xml:id="_DRPCWjT">In this case, Shoaib et al. proposed in <ref type="bibr" target="#b1">[2]</ref> the use of multiple motion smartphone sensors to recognize complex human activity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Yzv9qES">B. Deep learning</head><p xml:id="_xsSUa3t"><s xml:id="_AsbqXgh">Convolutional Neural Network (CNN) and Recurrent Neural Network models are used for time series classification.</s><s xml:id="_EqkprRh">In fact, they perform well on human activity recognition using sensor Smartphone data.</s><s xml:id="_WmDUqhf">An important work using CNNs was proposed by Zeng et al. in <ref type="bibr" target="#b3">[4]</ref> where the authors develop a simple CNN model for accelerometer data which is fed to convolutional layers, pooling layers and fully connected layers.</s><s xml:id="_ARYaxr4">Cho et al. <ref type="bibr" target="#b7">[8]</ref> have proposed another interesting example, where a CNN model has been developed to distinguish between dynamic activities such as "walking" and static activities as such "sitting".</s><s xml:id="_hvJezTN">The human activity recognition was then done by using two 3-class classifiers instead of using a single 6-class classifier.</s><s xml:id="_77Ckj8B">Another interesting approach was proposed by W. Jiang et al. in <ref type="bibr" target="#b8">[9]</ref>, where they combined the signal data from different sensors, together, and used CNN algorithm with two dimensions instead of one.</s></p><p xml:id="_ktMxxvE"><s xml:id="_Mug3x4b">LSTMs can also be applied to the problem of human activity recognition.</s><s xml:id="_nmrUEQj">Murad and Pyun in <ref type="bibr" target="#b9">[10]</ref> explored the use of LSTMs and Bidirectional LSTMs for predicting the activities for each input time step of a subsequence of sensor data.</s><s xml:id="_Xrk6k2q">LSTM can be used with CNN on human activity recognition as in <ref type="bibr" target="#b4">[5]</ref> where the authors proposed a deep network with four convolutional layers followed by two LSTM layers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rjTArkX">III. OUR APPROCH</head><p xml:id="_C2BTR5Q"><s xml:id="_UfWe8cv">Our approach is based on a three-step method (Fig. <ref type="figure">1</ref>): the data processing, the prediction of physical activities using machine and deep learning algorithms and the computation of the calories of dynamic activities to decide about the physical behavior.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YjrPrPF">Fig.1. Steps of our approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_dR3QWtz">A. Data processing</head><p xml:id="_gHcbvzb"><s xml:id="_4zjERRm">The data used for our approach is provided from the Wireless Sensor Data Mining (WISDM) <ref type="bibr" target="#b10">[11]</ref> by collecting the sensor data from smart phones.</s><s xml:id="_hC2Uec8">The user's movements are measured by the smartphone tri-axial accelerometer, which it consists of 1,098,207 examples of various physical activities (sampled at 20Hz, 1 sample every 50 ms) obtained from 36 users with six attributes: user, activity, timestamp, xacceleration y-acceleration, z-acceleration and the activities included walking, jogging, downstairs, upstairs, sitting and standing.</s></p><p xml:id="_J2UkvYP"><s xml:id="_wJ96M8c">We have added two attributes: age and gender for every physical activity with random distribution.</s></p><p xml:id="_aYxwNMT"><s xml:id="_gBywyuN">For the data processing, we have splitted the dataset based on the user identifiers (Ids).</s><s xml:id="_Qw2abG2">We kept users with Id from 1 to 28 for training the model and users with Id greater than 28 for the test set.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_y9NFPs4">B. Machine and Deep learning algorithms</head><p xml:id="_v8uwz6Q"><s xml:id="_7Rfb87W">For the machine learning algorithm, we have used the decision tree classifier and we have used the BiLSTM with 10 epochs and 400 samples as a deep learning model with a bidirectional layer followed by a dropout layer to read, extract its own features before a final mapping to an activity was made.</s><s xml:id="_CYucRsp">The efficient Adam version of stochastic gradient descent will be used to optimize the deep neural network.</s><s xml:id="_9W4UAey">The categorical cross entropy loss function will be also used.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RzJ7QkF">C. Decision of healthy behavior</head><p xml:id="_GzAWZ8w"><s xml:id="_NawQpJu">For the decision of predicted physical activities of person per day, we have classified users by age and gender into four categories: Men with age is between 20 and 40 years, Men with age is between 41 and 60 years, Women with age is between 20 and 40 years and Women with age between is 41-60 years.</s></p><p xml:id="_jMnquU9"><s xml:id="_wZEc493">According to the World Health Organization (WHO), we had classified also the activities into 3 categories: intense, moderate and inactive.</s><s xml:id="_er6p4R3">For that, we have considered jogging and upstairs as intense, walking and downstairs as moderate and others activities as inactive (sitting and standing).</s></p><p xml:id="_DyV5f29"><s xml:id="_dvnZbAR">We compared the number of calories for each activity per day with references from the WHO.</s><s xml:id="_2nFWQtJ">Therefore the concerned person will be informed by message on its healthiness behavior.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kjzjMRe">IV. EXPERIMENTS RESULTS AND DISCUSSION</head><p xml:id="_SArRDqc"><s xml:id="_QeH4Dj4">In our experiments, we have used Python (version 3.  False Negatives (FN): The number of positive instances that were classified as negative.</s></p><p xml:id="_TYm3A9e"><s xml:id="_8kzwzdB">The metrics of the confusion matrix were:</s></p><p xml:id="_qYVDW7C"><s xml:id="_MrPF63e">Precision, often referred to as positive predictive value, is the ratio of correctly classified positive instances to the total number of instances classified as positive:</s></p><p xml:id="_pEEPK4z"><s xml:id="_uE8MwjC">(1) Recall, also called sensitivity or true positive rate, is the ratio of correctly classified positive instances to the total number of positive instances:</s></p><p xml:id="_jR8bCvD"><s xml:id="_pGYRDwG">(2) F1 combines precision and recall as single value:</s></p><p xml:id="_yDvXbWX"><s xml:id="_VDw2Dyb">(3) At the beginning, let's know where the decision tree and the BiLSTM algorithms wrongly predicted the labels using a confusion matrix.</s><s xml:id="_AsBesbS">In fact, as shown in (Fig. <ref type="figure">2</ref> and Fig. <ref type="figure" target="#fig_1">3</ref>), the precision of these two algorithms is good for predicting jogging, sitting, standing, and walking but some problems have been appeared for clearly identifying upstairs and downstairs activities.</s><s xml:id="_ECs5vqC">As shown in Table <ref type="table" target="#tab_0">1</ref>, the decision tree algorithm can predict jogging with 80%, standing with 83% and walking with 67%.</s><s xml:id="_dmkpbJJ">The results are better than the Bidirectional LSTM which had 74% for jogging, 52% for standing and 57% for walking.</s><s xml:id="_TP3xMUP">As our deep learning model can't predict upstairs and has 0% for this activity, we have decided to continue our work with the machine learning algorithm: Decision Tree.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2zgRVPq">Fig.2. Confusion matrix by Decision Tree</head><p xml:id="_Cg4qB8E"><s xml:id="_c4WMKfh">We have used these predicted labels to verify the distribution of daily physical activities (Table <ref type="table">2</ref>) and construct a recommandation system for being informed on healthy daily physical activities of persons classified by age and gender (Table <ref type="table">3</ref>).</s></p><p xml:id="_vp32k7S"><s xml:id="_rRMwxaE">Table 2. Distribution of daily physical activities per gender and age</s></p><p xml:id="_2k8US4U"><s xml:id="_QJBcbPM">As shown in (Table <ref type="table">2</ref>) and in the case of dynamic activities, walking dominate all the other activities.</s><s xml:id="_KaZTR55">Indeed, it presents the percentage of 39.82% for young men that their age is between 20 and 40 years and the percentage of 33.50% for women for the same age.</s><s xml:id="_cxv4eFp">Although, these young men have the lower distribution in jogging and adult women that their age is between 41 and 60 years have the lower distribution in walking.</s><s xml:id="_ynMUNbE">In the case of the downstairs and upstairs, adult women which their age is between 41 and 60 years have the lowest distribution with 5.65% and 10.20%.</s><s xml:id="_8YRr9T8">This distribution helps us to compute the number of calories for each physical activity per day and understand the healthy daily activities of persons (Table <ref type="table">3</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NMEgYsM">Table3. Recommendation system per gender and age</head><p xml:id="_t53AeuE"><s xml:id="_ES8M3rj">As shown in (Table3.),</s><s xml:id="_NzT4s84">all the participants are not healthy persons and they should practice sports more times.</s><s xml:id="_dAppjVj">This result is justified by the energy produced by these persons was not enough in comparison with WHO references' values per day.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4CW6VNE">V. CONCLUSION AND PERSPECTIVES</head><p xml:id="_mvU7sa8"><s xml:id="_TA6m5G7">In this work, we have used smartphone accelerometer data to make a healthy recommendation system.</s><s xml:id="_Rs5hhzX">This system gives the state of the daily physical activities for users of mobiles where are healthy or not based on references from WHO.</s></p><p xml:id="_AdepBjk"><s xml:id="_UKPMyFb">We have classified data into six categories including four active physical activities consuming calories and 2 passives.</s><s xml:id="_B8NykGS">The machine learning classification had given us better results than deep learning algorithms.</s><s xml:id="_sBfXzDt">This is can be justified by the problem of the unbalanced classes during the step of the training.</s></p><p xml:id="_8qvfE47"><s xml:id="_EMpXqA4">As perspectives, we can apply the recommendation system per week.</s><s xml:id="_mzwhsZn">Also, we can use another sensor smartphone data which is Global Positioning System (GPS) to know the kilometers number of dynamic activities as walking and jogging to improve the results.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_YC7Hbnx"><s xml:id="_6Myhxdt">6.5) with Anaconda distribution on Ubuntu 16.04.6</s><s xml:id="_UG4sEMM">LTS (Xenial Xerus), Keras (version 2.1.6)</s><s xml:id="_QuwyNUp">and Tensorflow (version 1.7.0) as backend of keras.</s><s xml:id="_3qD5kZP">The following values can be obtained from the confusion matrix in a classification problem:  True Positives (TP): The number of positive instances that were classified as positive.</s><s xml:id="_Fhk5vT3"> True Negatives (TN): The number of negative instances that were classified as negative.</s><s xml:id="_hy7e3Ew"> False Positives (FP): The number of negative instances that were classified as positive.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc><div><p xml:id="_TcBPmkW"><s xml:id="_vrSEqh5">Fig.3.</s><s xml:id="_N4ZPGK8">Confusion matrix by Bidirectional LSTM However, the two models presents different values of the predicted precision activities (Table.1)</s></p></div></figDesc><graphic coords="3,322.55,53.43,216.38,308.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc><div><p xml:id="_rQQd4Ps"><s xml:id="_gBhcck3">1) Precision of activities between machine and deep learning algorithms</s></p></div></figDesc><table><row><cell></cell><cell>Decision Tree</cell><cell>Bidirectional</cell></row><row><cell></cell><cell></cell><cell>LSTM</cell></row><row><cell>Downstairs</cell><cell>23%</cell><cell>37%</cell></row><row><cell>Jogging</cell><cell>80%</cell><cell>74%</cell></row><row><cell>Sitting</cell><cell>89%</cell><cell>99%</cell></row><row><cell>Standing</cell><cell>83%</cell><cell>52%</cell></row><row><cell>Upstairs</cell><cell>19%</cell><cell>00%</cell></row><row><cell>Walking</cell><cell>67%</cell><cell>57%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_tpCMc3y"><s xml:id="_cTDJJbM">Authorized licensed use limited to: The University of Iowa.</s><s xml:id="_Fk5ESUp">Downloaded on August 05,2025 at 04:21:00 UTC from IEEE Xplore.</s><s xml:id="_jfHWeSh">Restrictions apply.</s></p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_ajWsrXz">PACP: A Position-Independent Activity Recognition Method Using Smartphone Sensors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">210044. 2016</date>
			<pubPlace>Nanjing; Jiangsu, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>School of Computer and Software, Nanjing University of Information Science &amp; Technology</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">R. Yang and B. Wang, &quot;PACP: A Position-Independent Activity Recognition Method Using Smartphone Sensors&quot;, School of Computer and Software, Nanjing University of Information Science &amp; Technology, Nanjing 210044, Jiangsu, China, 2016.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_FRau3XR">Complex Human Activity Recognition Using Smartphone and Wrist-Worn Motion Sensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Incel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Scholten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Havinga</surname></persName>
		</author>
		<idno type="DOI">10.3390/s16040426</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_e5HPcVW">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">426</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Shoaib, S. Bosch, O. Incel, H. Scholten and P. Havinga &quot;Complex Human Activity Recognition Using Smartphone and Wrist-Worn Motion Sensors&quot;, Sensors, 16(4), 426. doi:10.3390/s16040426, 2016.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_hrMrNay">Accelerometer-based Human Activity Recognition and the Impact of the Sample Size</title>
		<author>
			<persName><forename type="first">A</forename><surname>Harasimowic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dziubich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brzeski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9SPqfW4">Proceedings of the 13th International Conference on Artificial Intelligence, Knowledge Engineering and Data Bases</title>
		<meeting>the 13th International Conference on Artificial Intelligence, Knowledge Engineering and Data Bases<address><addrLine>Gdansk, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Harasimowic, T. Dziubich and A. Brzeski, &quot;Accelerometer-based Human Activity Recognition and the Impact of the Sample Size&quot;, in Proceedings of the 13th International Conference on Artificial Intelligence, Knowledge Engineering and Data Bases, Gdansk, Poland, 2014.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_WmKyBu3">Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Mengshoel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.4108/icst.mobicase.2014.257786</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kkuaE5D">Proceedings of the 6th International Conference on Mobile Computing, Applications and Services</title>
		<meeting>the 6th International Conference on Mobile Computing, Applications and Services</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Zeng, L. T. Nguyen, B. Yu, O. J. Mengshoel, J. Zhu, P. Wu and J. Zhang, &quot;Convolutional Neural Networks for Human Activity Recognition using Mobile Sensors&quot;, Proceedings of the 6th International Conference on Mobile Computing, Applications and Services. doi: 10.4108/icst. mobicase. 2014. 257786, 2014.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_hzKZZTe">Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roggen</surname></persName>
		</author>
		<idno type="DOI">10.3390/s16010115</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HaMZKWE">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. J. Ordonez and D. Roggen, &quot;Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition&quot;, Sensors, 16 (1), 115. doi: 10. 3390/ s16010115, 2016.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_gRFHGjX">Human Activity Classification in Smartphones Using Accelerometer and Gyroscope Sensors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kanhangad</surname></persName>
		</author>
		<idno type="DOI">10.1109/jsen.2017.2782492</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mzCJPVK">IEEE Sensors Journal</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Jain and V. Kanhangad, &quot;Human Activity Classification in Smartphones Using Accelerometer and Gyroscope Sensors&quot;, IEEE Sensors Journal, 2018.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_Tk3CDyc">Human Activity Recognition using Smart Phone Embedded Sensors: A Linear Dynamical Systems Method</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ijcnn.2014.6889585</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nsWbJGj">International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Wang, H. Liu, L. Yu, F. Sun, &quot;Human Activity Recognition using Smart Phone Embedded Sensors: A Linear Dynamical Systems Method&quot;, International Joint Conference on Neural Networks (IJCNN), China, 2014.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_B7jb88b">Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18041055</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hmxBPcK">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1055</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Cho and S. M. Yoon, &quot;Divide and Conquer-Based 1D CNN Human Activity Recognition Using Test Data Sharpening&quot;, Sensors, 18(4), 1055. doi:10.3390/s18041055, 2018.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_SUjP95C">Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2733373.2806333</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_b3eWKe6">Proceedings of the 23rd ACM International Conference on Multimedia-MM&apos;15</title>
		<meeting>the 23rd ACM International Conference on Multimedia-MM&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Jiang and Z. Yin, &quot;Human Activity Recognition Using Wearable Sensors by Deep Convolutional Neural Networks&quot;, Proceedings of the 23rd ACM International Conference on Multimedia- MM&apos;15. doi:10.1145/2733373.2806333, 2015.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_QfRtrMg">Deep Recurrent Neural Networks for Human Activity Recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Murad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y</forename><surname>Pyun</surname></persName>
		</author>
		<idno type="DOI">10.3390/s17112556</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PePXGvm">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2556</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Murad and J-Y. Pyun, &quot;Deep Recurrent Neural Networks for Human Activity Recognition&quot;, Sensors, 17 (11), 2556. doi: 10.3390/ s17112556, 2017.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_EsqcM54">Activity Recognition using Cell Phone Accelerometers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kwapisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Moore</surname></persName>
		</author>
		<idno type="DOI">10.1145/1964897.1964918</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MaeqU6M">Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data</title>
		<meeting>the Fourth International Workshop on Knowledge Discovery from Sensor Data<address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. R. Kwapisz, G. M. Weiss and S. A. Moore, &quot;Activity Recognition using Cell Phone Accelerometers&quot;, Proceedings of the Fourth International Workshop on Knowledge Discovery from Sensor Data, Washington DC, 2010.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
