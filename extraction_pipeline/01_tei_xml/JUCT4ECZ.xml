<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_D5YmUDC">Multi task opinion enhanced hybrid BERT model for mental health analysis</title>
				<funder>
					<orgName type="full">Research Chair of Online Dialogue and Cultural Communication, King Saud University, Riyadh, Saudi Arabia</orgName>
				</funder>
				<funder ref="#_QUZQbS8">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Md</forename><forename type="middle">Mithun</forename><surname>Hossain</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Computer Science and Engineering , Bangladesh University of Business and Technology , Dhaka 1216 , Bangladesh.</note>
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Bangladesh University of Business and Technology</orgName>
								<address>
									<postCode>1216</postCode>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Md</forename><forename type="middle">Shakil</forename><surname>Hossain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Mridha</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Computer Science and Engineering , Bangladesh University of Business and Technology , Dhaka 1216 , Bangladesh.</note>
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Bangladesh University of Business and Technology</orgName>
								<address>
									<postCode>1216</postCode>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Computer Science , American International University-Bangladesh , Dhaka 1229 , Bangladesh.</note>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">American International University-Bangladesh</orgName>
								<address>
									<postCode>1229</postCode>
									<settlement>Dhaka</settlement>
									<country key="BD">Bangladesh</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mejdl</forename><surname>Safran</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Research Chair of Online Dialogue and Cultural Communication , Department of Computer Science , College of Computer and Information Sciences , King Saud University , 11543 Riyadh , Saudi Arabia.</note>
								<orgName type="department" key="dep1">Research Chair of Online Dialogue and Cultural Communication</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="department" key="dep3">College of Computer and Information Sciences</orgName>
								<orgName type="institution">King Saud University</orgName>
								<address>
									<postCode>11543</postCode>
									<settlement>Riyadh</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> Department of Computer Science , College of Computer and Information Sciences , King Saud University , 11543 Riyadh , Saudi Arabia.</note>
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">College of Computer and Information Sciences</orgName>
								<orgName type="institution">King Saud University</orgName>
								<address>
									<postCode>11543</postCode>
									<settlement>Riyadh</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_UPRvbQ4">Multi task opinion enhanced hybrid BERT model for mental health analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBB08A5870020893E6DB3ABC5485D7DC</idno>
					<idno type="DOI">10.1038/s41598-025-86124-6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T10:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_NyEr8CA">Opinion-BERT</term>
					<term xml:id="_W4n69yg">Opinions embedding</term>
					<term xml:id="_5P8cRuj">Hybrid BERT</term>
					<term xml:id="_adZ2Gzz">Mental health sentiment analysis</term>
					<term xml:id="_dcQAKtp">Multi-task learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_WAJutQQ"><p xml:id="_rUtK3xP"><s xml:id="_t3uKUqk">Understanding the nuanced emotions and points of view included in user-generated content remains challenging, even though text data analysis for mental health is a crucial instrument for assessing emotional well-being.</s><s xml:id="_Ts42S3a">Most current models neglect the significance of integrating viewpoints in comprehending mental health in favor of single-task learning.</s><s xml:id="_bDYCuqf">To offer a more thorough knowledge of mental health, in this study, we present an Opinion-Enhanced Hybrid BERT Model (Opinion-BERT), built to handle multi-task learning for simultaneous sentiment and status categorization.</s><s xml:id="_unF49mA">With the help of TextBlob and SciPy, we extracted opinions and dynamically constructed new opinion embeddings to complement the pre-trained BERT model.</s><s xml:id="_EsS5EQf">Using a hybrid architecture, these embeddings are integrated with the contextual embeddings of BERT, whereby the CNN and BiGRU layers collected local and sequential characteristics.</s><s xml:id="_qGUk3PP">This combination helps our model to identify and categorize user status and attitudes from the text more accurately, which leads to more accurate mental health assessments.</s><s xml:id="_s2M4aGy">When we compared the performance of Opinion-BERT to some baseline models, including BERT, RoBERTa, and DistilBERT, we found that it performed much better.</s><s xml:id="_xvCqnde">Opinion-enhanced embeddings are crucial for improving performance, as demonstrated by our multi-task learning framework's 96.77% sentiment classification accuracy of 94.22% status classification accuracy.</s><s xml:id="_XbqX3Qh">This work provides a more nuanced understanding of emotions and psychological states by demonstrating the potential of combining opinion and sentiment data for mental health analysis in a multi-task learning environment.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="782.362"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3Paws24">Literature review</head><p xml:id="_z5MuQAT"><s xml:id="_BReR2uk">The increased availability of textual data from social media, online forums, and patient records has drawn considerable interest in the use of natural language processing (NLP) in mental health studies in recent years.</s><s xml:id="_fd3eAEq">Traditional machine learning techniques have been the mainstay of early approaches, but the development of deep learning models such as BERT, RoBERTa, and GPT, has made substantial progress in the field.</s><s xml:id="_fEQQQ8N">However, these models frequently fail to consider the complex role that feelings and views play in texts on mental health.</s><s xml:id="_xB8wbc8">This overview of the literature looks at multi-task learning models, sentiment analysis, and mental health detection, emphasizing the significant developments and gaps that our Opinion-Enhanced Hybrid BERT model seeks to fill.</s></p><p xml:id="_GNXCrRs"><s xml:id="_WUJDGJf">Machine learning and NLP techniques are increasingly being applied to early mental health diagnosis through text analysis on various platforms.</s><s xml:id="_8UtCM25">Singh et al. <ref type="bibr" target="#b11">12</ref> explored SMS-based mental health assessments using classifiers such as decision trees, random forest, and logistic regression, with logistic regression achieving 93% accuracy.</s><s xml:id="_ahaWvV4">Inamdar et al. <ref type="bibr" target="#b12">13</ref> focused on detecting mental stress in Reddit posts, using ELMo and BERT embeddings, achieving an 0.76 F1 score with Logistic Regression.</s><s xml:id="_USWxvE7">Alanazi et al. <ref type="bibr" target="#b13">14</ref> investigated the influence of financial news on public mental health and found that SLCNN was the most effective with 93.9% accuracy in sentiment classification.</s><s xml:id="_qg9Snv2">A systematic review conducted by Abd Rahman et al. <ref type="bibr" target="#b14">15</ref> highlighted SVM as a commonly used model for mental health detection in Online Social Networks, emphasizing challenges such as data quality and ethical concerns.</s><s xml:id="_BqdTSHQ">Abdulsalam et al. <ref type="bibr" target="#b15">16</ref> introduced an Arabic suicidality detection dataset with 5719 tweets, demonstrating that the AraBert model outperforms traditional models such as SVM and Random Forest, achieving 91% accuracy and an F1 score of 88%.</s><s xml:id="_A9nnqrK">Almeqren et al. <ref type="bibr" target="#b16">17</ref> applied Arabic Sentiment Analysis (ASA) to predict anxiety levels during the COVID-19 pandemic in Saudi Arabia, using a Bi-GRU model and a custom Arabic Psychological Lexicon (AraPh), reaching 88% accuracy in classifying anxiety from tweets.</s><s xml:id="_Dvge7Hm">Ameer et al. <ref type="bibr" target="#b17">18</ref> focused on detecting depression and anxiety from 16,930 Reddit posts, with the RoBERTa model achieving the highest accuracy of 83%, highlighting the role of automation in supporting mental health providers.</s><s xml:id="_v7NzpNB">Su et al. <ref type="bibr" target="#b18">19</ref> conducted a scoping review on deep learning applications in mental health, categorizing studies across clinical, genetic, vocal, and social media data, underscoring DL's potential for early detection, while noting challenges in model interpretability and data diversity.</s><s xml:id="_Uy4KgjK">Together, these studies demonstrate the transformative potential of AI in enhancing early mental health diagnosis across various platforms and languages.</s></p><p xml:id="_EfsDhhK"><s xml:id="_t3eCWyc">Several studies have highlighted the effectiveness of multi-task learning (MTL) in detecting mental health conditions and improving sentiment analysis across various domains.</s><s xml:id="_A3kc7T8">Liu and Su <ref type="bibr" target="#b19">20</ref> explored a BERT-based MTL framework for mental health detection on social media, achieving superior performance on the Reddit SuicideWatch and Psychiatric-disorder Symptoms datasets compared to single-task models and generalpurpose Large Language Models (LLMs).</s><s xml:id="_Es4uj3D">Buddhitha and Inkpen <ref type="bibr" target="#b20">21</ref> focused on suicide ideation detection using CNN-based MTL models applied to Reddit and Twitter, outperforming baselines and emphasizing the role of comorbid conditions such as PTSD in improving prediction accuracy.</s><s xml:id="_pPyQmUc">Sarkar et al. <ref type="bibr" target="#b21">22</ref> introduced AMMNet, an MTL model that integrates word embeddings and topic modeling to predict depression and anxiety in Reddit posts, demonstrating strong performance through active learning despite data limitations.</s><s xml:id="_f2JaPTg">Li et al. <ref type="bibr" target="#b22">23</ref> applied MTL to detect depression in dialogues, combining tasks such as emotion, dialogue act, and topic classification, and achieved a 70.6% F1 score on dialogue datasets.</s><s xml:id="_Drc8XSv">Similarly, Plaza-Del-Arco et al. <ref type="bibr" target="#b23">24</ref> proposed an MTL model for hate speech detection in Spanish tweets, combining sentiment analysis and emotion classification, where their MTLsent+emo configuration outperformed the single-task learning models.</s><s xml:id="_dGjJz9j">Finally, Jin et al. <ref type="bibr" target="#b24">25</ref> developed a sentiment classification MTL model (MTL-MSCNN-LSTM), that integrates multi-scale CNN and LSTM to capture global and local text features in commodity reviews, achieving higher accuracy and F1 scores across six datasets.</s><s xml:id="_qwaRnNF">These studies collectively demonstrate the potential of MTL in improving the prediction accuracy, handling multiple tasks, and addressing challenges such as data sparsity and model efficiency across different application areas.</s></p><p xml:id="_7mQxcdu"><s xml:id="_f4eNgPD">Multiple studies have focused on accelerating opinion-embedding approaches, primarily using attention mechanisms and state-of-the-art architectures in sentiment analysis.</s><s xml:id="_Fwhn5QZ">Malik et al. <ref type="bibr" target="#b25">26</ref> introduced a hybrid model combining multi-head attention with Bi-LSTM and Bi-GRU classifiers, achieving a remarkable 95% accuracy, 97% recall, and 96% F1 score on student feedback sentiment analysis using advanced embeddings such as FastText and RoBERTa.</s><s xml:id="_urMsTDT">In a similar manner, Chen et al. <ref type="bibr" target="#b26">27</ref> proposed a BERT-based dual-channel hybrid neural network that integrates a CNN and BiLSTM with attention mechanisms, which significantly improves sentiment analysis on hotel review datasets, achieving 92.35% accuracy.</s><s xml:id="_2APEyn6">On the other hand, Dimple Tiwari and Nagpal <ref type="bibr" target="#b27">28</ref> introduced KEAHT, a knowledge-enriched hybrid transformer for analyzing sentiment related to COVID-19 and farmer protests, incorporating LDA topic modeling and BERT, demonstrating its ability to handle complex social media sentiment analysis tasks.</s><s xml:id="_UWBWN3j">The AEC-LSTM model introduced by Huang et al. <ref type="bibr" target="#b28">29</ref> uses a novel approach that integrates emotional intelligence and attention mechanisms, resulting in superior performance on real-world datasets for sentiment classification.</s><s xml:id="_U5ZUYGg">In contrast, Han and Kando 30 focused on improving opinion expression detection using a BiLSTM-CRF model with deep contextualized embeddings such as BERT, showing superior results for detecting subjective expressions.</s><s xml:id="_qyzwMXw">Jebbara and Cimiano 31 explored character-level embeddings to improve opinion target expression (OTE) extraction, addressing issues such as misspellings and domain-specific terminology, whereas Liu et al. <ref type="bibr" target="#b31">32</ref> highlight the effectiveness of RNNs, particularly LSTMs, over traditional CRF models in fine-grained opinion mining, achieving enhanced performance by fine-tuning word embeddings.</s><s xml:id="_YgM4tV7">These studies highlight the evolution of sentiment analysis models, emphasizing the need for advanced embeddings, attention mechanisms, and hybrid architectures to improve sentiment classification and opinion mining across diverse applications.</s></p><p xml:id="_NEeBJCV"><s xml:id="_cjnPWht">In conclusion, the progress made in sentiment analysis, especially concerning mental health, highlights the revolutionary potential of deep learning and machine learning methods.</s><s xml:id="_XvMtV8Q">Sophisticated models that use RNNs, CNNs, and transformer architectures such as BERT have replaced traditional lexicon-based methods, improving the capacity to extract subtle emotions from textual input.</s><s xml:id="_c59D9EX">Multi-task learning (MTL) and attention processes have helped handle complicated tasks and improve prediction accuracy in recent experiments conducted on a variety of platforms.</s><s xml:id="_h7XThbU">Sentiment analysis addresses the issues of data variety and requirement for interpretability in AI models.</s><s xml:id="_9UKXB4x">As it continues to be incorporated, it presents potential paths for early mental health diagnoses and interventions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_x27TNYU">Data description</head><p xml:id="_7EFnz4N"><s xml:id="_EBjeqRG">The dataset employed in this study is an extensive and painstakingly assembled set of utterances from several platforms annotated with mental health statuses.</s><s xml:id="_ZxTfJj4">It combines information from other Kaggle datasets, such as the 3k Conversations dataset for chatbots and depressions.</s><s xml:id="_v2BbBrq">The Reddit Cleaned, Human Stress Prediction, Bipolar Mental Health Dataset, Reddit Mental Health Data, Students Anxiety and Depression Dataset, Suicidal Mental Health Dataset, and Suicidal Tweet Detection Dataset are resources related to mental health and anxiety prediction.</s><s xml:id="_zDvgdXt">One of the following seven mental health statuses is associated with the statements in this dataset: Normal, Depression, Suicidal, Anxiety, Stress, Bipolar, or Personality Disorder.</s><s xml:id="_Uxd2nUD">Every entry in the data was marked with a particular mental health condition, and it was gathered from a variety of sites, such as Reddit and social media.</s><s xml:id="_xhJfG3K">The dataset, obtained from Kaggle, is already labeled and does not include any user-level information, such as the number of posts per user.</s></p><p xml:id="_phg895x"><s xml:id="_KvAdYHd">The dataset, which is organized with variables such as unique_id, statements, and mental health status, is a priceless tool for researching mental health trends, creating sophisticated chatbots for mental health, and conducting in-depth sentiment analyses.</s></p><p xml:id="_h99SfVJ"><s xml:id="_mm2kuNT">Important insights into the attitudes and mental health conditions of the sample are shown in Fig. <ref type="figure" target="#fig_1">1</ref>.</s><s xml:id="_CtA9eNe">The distribution of mental health statuses is shown in Fig. <ref type="figure" target="#fig_1">1a</ref>, where "Normal" accounts for the biggest percentage (31%), followed by "Depression" (20.2%) and "Suicidal" instances (20.2%).</s><s xml:id="_VnhwzpV">Smaller percentages are explained by other categories including "Anxiety" (7.3%), "Bipolar" (5.3%), "Stress" (4.9%), and "Personality Disorder" (2.0%).</s><s xml:id="_MssZdX5">The frequency of mental health conditions such as depression and suicidal thoughts in the data was demonstrated by this distribution.</s></p><p xml:id="_4JymXFD"><s xml:id="_XnNMvPR">The sentiment distribution is shown in Fig. <ref type="figure" target="#fig_1">1b</ref>, where "Positive" sentiment is most prevalent at 40.3%, followed by "Negative" emotion at 37.6% and "Neutral" sentiment at 22.1%.</s><s xml:id="_BzR4DY8">This sentiment breakdown provides crucial context for sentiment analysis in the study of mental health trends by reflecting the emotional tones connected to mental health statuses.</s></p><p xml:id="_6YTrNhz"><s xml:id="_XcHMsnA">Figure <ref type="figure" target="#fig_0">2</ref> shows that the majority of posts in the dataset are relatively short, with most containing fewer than 500 words and a significant concentration below 100 words.</s><s xml:id="_hdUGNXs">The sharp decline in the number of posts as the word count increases highlights the dominance of shorter posts in user-generated content.</s><s xml:id="_YykuE7p">This pattern suggests that users tend to communicate their thoughts in brief formats, which is typical for social media or digital platforms where brevity is often encouraged.</s><s xml:id="_heSkFYq">Posts exceeding 1,000 words are rare, further reinforcing the preference for concise communication.</s><s xml:id="_ueQjsNk">Understanding this distribution is critical for natural language processing tasks, as it helps us set appropriate tokenization limits, ensuring that most posts are fully captured while minimizing the need for truncation or excessive padding.</s><s xml:id="_pxWQYDH">Additionally, this insight allows us to design models optimized for the typical length of user content, improving efficiency and accuracy in downstream tasks like classification, sentiment analysis, or topic modeling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_k49V27c">Data pre-processing</head><p xml:id="_WSWVV4q"><s xml:id="_vXkgnz6">Data cleaning Data cleaning requires several important procedures to ensure the quality and integrity <ref type="bibr" target="#b32">33</ref> .</s><s xml:id="_HCfPdb5">Duplicate entries were identified and eliminated to avoid redundancy.</s><s xml:id="_rqpfh8Y">Special characters, URLs, and other non-alphanumeric components were removed to streamline text data.</s><s xml:id="_sGDXfEq">Furthermore, the text was transformed to lowercase to preserve uniformity and eliminate problems due to case sensitivity.</s><s xml:id="_tPDPF8v">Missing values were carefully handled by imputing suitable values depending on the context or eliminating impacted rows.</s><s xml:id="_JjfVhup">Finally, to improve the analysis's emphasis on the main ideas, stopwords-common words that had minimal bearing on the statements' meanings-were eliminated.</s><s xml:id="_PqhpAbt">All of these procedures worked together to provide a clean and trustworthy dataset for further processing and examination.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_7PF8pUf">Lemmatization</head><p xml:id="_3wJVG4K"><s xml:id="_SR6tc7X">Lemmatization, which focuses on breaking words down to their most basic or root forms, is an essential stage in text preparation.</s><s xml:id="_CPcvd6W">In contrast to stemming, which frequently truncates words, lemmatization seeks to yield legitimate words by considering meaning and context.</s><s xml:id="_nS674fE">First, to maintain uniformity, all punctuation was removed and the text was converted to lowercase.</s><s xml:id="_N2ZYU67">Following the tokenization of the text into individual words, frequent stop words were eliminated.</s><s xml:id="_GAkKqVR">Next, each surviving word is lemmatized with the help of WordNetLemmatizer from NLTK <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35</ref> , which guarantees that terms like "running" and "runs" are reduced to their root, "run.</s><s xml:id="_2xcwbhw">" This method aids in text normalization, which increases the accuracy of the ensuing analytical jobs.</s><s xml:id="_tdBwk9d">The lemmatized tokens were subsequently combined into a processed statement, prepared for the subsequent stages of feature extraction and modeling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_tGwfSra">Data augmentation with synonym replacement</head><p xml:id="_q634QeV"><s xml:id="_eMFsHVt">As part of our data augmentation process, we used a synonym replacement technique to improve the variety and resilience of the dataset <ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37</ref> .</s><s xml:id="_m9wCJq4">This method finds words and replaces them with synonyms by using the WordNet lexical database <ref type="bibr" target="#b37">38</ref> .</s><s xml:id="_mCfgmC7">Using WordNet synsets, the procedure begins by extracting synonyms for every word in a given text.</s><s xml:id="_ZfjWbDR">A synonym is randomly selected from the list of potential synonyms for every word in the text.</s><s xml:id="_AkPN2cK">This replacement introduces modifications in wording while maintaining the textual meaning up to a predetermined number of times (n=3, in this example).</s><s xml:id="_RSTnHu9">Subsequently, the modified texts were combined and added to the dataset, increasing both the quantity and variety.</s><s xml:id="_rQtWS9S">To provide a more complete dataset, several copies of each original text item were created and appended using synonyms.</s><s xml:id="_7h5E9wS">Using the synonym_replacement function, this method significantly increases the diversity of the dataset and can help machine learning models that have been trained on it perform better and become more broadly applicable.</s><s xml:id="_KM3R8f6">For consistency in further analysis or training, the original labels were retained in the augmented dataset created using both original and freshly generated texts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MqtEfZH">Sentiment analysis with TextBlob</head><p xml:id="_8JK5kCE"><s xml:id="_5mUQ6QQ">During the sentiment analysis stage, TextBlob is used to assess each statement in the supplemented dataset to ascertain its sentiment polarity <ref type="bibr" target="#b38">39</ref> .</s><s xml:id="_HkUN2mD">Based on the polarity score, p, where p ∈ [-1, 1], TextBlob divides attitudes into three categories: "Positive, " "Neutral, " or "Negative.</s><s xml:id="_fXncq5E">" The following section describes how sentiment categorization was performed.</s></p><formula xml:id="formula_0">• Positive sentiment: p &gt; 0 • Neutral sentiment: p = 0 • Negative sentiment: p &lt; 0</formula><p xml:id="_hPsEKe9"><s xml:id="_hjzKSgn">Function analysis_sentiment_by_status was used to perform this categorization.</s><s xml:id="_bUct7aN">It processes each statement s and the status that accompanies it status, evaluates the sentiment, and yields a tuple (s, status, sentiment).</s><s xml:id="_DBUhF5B">After the sentiments are combined, we can determine the number of sentiment types for each status by examining a DataFrame called sentiment_df.</s><s xml:id="_kQzq3jx">In particular, the counts were acquired by employing</s></p><formula xml:id="formula_1">sentiment_countsij = Count (sentiment_df[status = i and sentiment = j])</formula><p xml:id="_SGY92er"><s xml:id="_9Z57Mdw">where the number of statements with status i and sentiment j is represented by sentiment_countsij.</s><s xml:id="_3cA7hRx">This distribution sheds light on the distribution of feelings across various mental health states and is described in sentiment_counts.</s><s xml:id="_EMdrcZz">Analyzing the emotional tone of utterances in connection with reported mental health issues requires an understanding of this distribution.</s></p><p xml:id="_huqKDhA"><s xml:id="_Y36848g">To ensure the robustness of our findings, we also experimented with other sentiment analysis tools, including VADER and Afinn.</s><s xml:id="_hzUUWXy">These tools offer varying approaches to sentiment scoring and were used to reassess the dataset's sentiment labels.</s><s xml:id="_4GdCU7j">The comparative analysis revealed that while TextBlob provided a balanced distribution of sentiment categories, VADER detected more nuanced polarity shifts, especially in "Neutral" and "Negative" sentiments.</s><s xml:id="_SJQ5VEA">A summary of these results and their impact on the final model is presented in the Results section.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DCzegvr">Opinions</head><p xml:id="_QusEwED"><s xml:id="_FPs4hka">We identified important opinion-related terms using spaCy's English NLP model to extract and sanitize subjective expressions from textual input <ref type="bibr" target="#b39">40</ref> .</s><s xml:id="_UjuCcwF">We refined the extraction procedure by removing auxiliary verbs and concentrating on adjectives, adverbs, and verbs likely to communicate subjective feelings by running each text through the model.</s><s xml:id="_ph2FN7f">For convenience, the detected opinions were combined into a string format.</s><s xml:id="_dVsXt5b">In conclusion, we have used a cleaning mechanism after extraction.</s><s xml:id="_kaasmP2">This function eliminated non-alphabetic letters and phrases that were too repetitive or insignificant, leaving only the significant and original expressions.</s><s xml:id="_fB8E3Jp">The end product is a dataset enhanced with pertinent and clean opinion phrases that, offers a strong basis for additional research or model training.</s></p><p xml:id="_GwP4hep"><s xml:id="_SavM77e">The average number of opinions for each sentiment type is presented in Table <ref type="table">1</ref>.</s><s xml:id="_mK2dauY">With 36.31 opinions on average, the "Positive" category has the most opinions, closely followed by the "Negative" category with 33.80 opinions.</s><s xml:id="_77GVZNF">The "Neutral" category, on the other hand, has a far lower average-just 3.64 opinions per feeling.</s><s xml:id="_ezv8H8f">This shows that users record fewer neutral comments and prefer to express stronger sentiments more frequently in terms of positivity or negativity.</s><s xml:id="_9J5bZjX">There might be a general trend toward polarized sentiment in the dataset, as seen by the imbalance in the number of neutral thoughts.</s></p><p xml:id="_vsqVZSc"><s xml:id="_Pz7Jpg4">The opinions most frequently used in the dataset are listed in Table <ref type="table" target="#tab_1">2</ref>.</s><s xml:id="_BYdSpwf">With 81,114 occurrences, the verb "feel" is the most common, suggesting that emotional expressiveness is important to the dataset.</s><s xml:id="_z3jyvR8">Verbs like "know" and "want, " which appear 52,940 and 50,827 times, respectively, are closely followed, indicating that knowledge and want are strongly associated with user attitudes.</s><s xml:id="_2wxaYb5">Additional frequently used words such as "get, " "even, " and "really" denote typical declarations of emphasis or intensity.</s><s xml:id="_vKBzEtZ">These frequently expressed opinions draw attention to important phrases that users frequently use to communicate their feelings, ideas, and behaviors, thereby providing insights into the recurrent themes of the dataset.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qK2eE6H">Data tokenization and label encoding</head><p xml:id="_4dZ4hzy"><s xml:id="_w7XxyGZ">We used a two-step procedure of tokenization and label encoding to obtain textual input for model training.</s><s xml:id="_nMVhF8t">First, the raw text data were transformed into token IDs and attention masks appropriate for BERT-based models using BertTokenizer <ref type="bibr" target="#b8">9</ref> .</s><s xml:id="_RqhNZ4Y">After processing the phrases in the DataFrame's statement column, the tokenize_data function generates input_ids and attention_masks using the given parameters of padding and truncating to a maximum length of 100 tokens, among other things.</s><s xml:id="_sgnrqk3">The text data were structured correctly for the model input owing to this tokenization.</s></p><p xml:id="_YBgFUna"><s xml:id="_GBqvtUG">We set the input sequence length to 100 tokens after conducting an exploratory analysis of our dataset.</s><s xml:id="_jqjT7VX">Initially, we examined the distribution of tokenized sentences and observed that the majority of instances contained fewer than 100 tokens.</s><s xml:id="_e8uHrss">Specifically, our preliminary data analysis showed that approximately 90% of sentences in the dataset had a token count below this threshold, minimizing the amount of unnecessary padding for most samples.</s><s xml:id="_rWqYPtU">Selecting a slightly longer length would have increased computational overhead, while a shorter length risked truncating relevant context.</s><s xml:id="_Xtp884f">Thus, a padding length of 100 tokens represented a balanced compromise between computational efficiency and preserving important textual information.</s></p><p xml:id="_MAg2UD5"><s xml:id="_8vNnGEH">The next step involved label encoding, which converts category labels into numerical values required for model training <ref type="bibr" target="#b40">41</ref> .</s><s xml:id="_ChtDfHZ">For the status, emotion, and opinions_str columns, we initialized label encoders.</s><s xml:id="_Hz9Fa2F">These encoders enable the model to comprehend and handle categorical data efficiently by mapping distinct class labels to integers.</s><s xml:id="_ZqB6TE6">The opinions labels were kept as integer encodings, whereas the status and emotion labels were transformed into one-hot encoded vectors by using the to_categorical function.</s><s xml:id="_VjxbD2B">Thorough preparation guarantees that both categorical and textual data are correctly prepared and incorporated into the training pipeline.</s><s xml:id="_s55hN3a">These encoded labels are now included in the new DataFrame, which makes managing the data and training the models easier.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_j9XtEz6">Data splitting</head><p xml:id="_bnWuSsW"><s xml:id="_TgXywQc">The dataset was systematically divided into training, validation, and test sets to simplify the training and evaluation of the model.</s><s xml:id="_8RzJqZC">For compatibility with the scikit-learn's train_test_split function, the input tensors (input_ids and attention_masks) and encoded labels (status_labels, sentiment_labels, and opinions_labels) were first transformed from TensorFlow tensors to NumPy arrays.</s></p><p xml:id="_ZDZFKJS"><s xml:id="_dg7AwBc">Opinion Frequency Opinion Frequency Feel 81,114 Go 32,254 Know 52,940 Think 31,005 Want 50,827 Make 30,437 Get 46,721 Even 46,529 Really 43,638 Table 1.</s><s xml:id="_yGqBmYq">Average number of opinions per sentiment category.</s></p><p xml:id="_QByQ3HZ"><s xml:id="_Y6uHJ7Y">Initially, the dataset was split into training and temporary sets, with 30% set aside for testing and validation and 70% going toward training.</s><s xml:id="_T2UTpf6">The temporary set was then divided evenly into the test and validation sets, each comprising 15% of the initial data.</s><s xml:id="_6U4Z3PG">This method guarantees solid model training and objective assessment by evenly distributing data.</s><s xml:id="_ePeGAHN">The corresponding dataset sizes were 1,10,365 samples for the training, 23,650 samples for validation, and 23,650 samples for testing.</s><s xml:id="_MKXrnbv">These splits are essential for evaluating model performance, finetuning hyperparameters, and guaranteeing the applicability of the model to new data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2yF76fZ">Problem statement</head><p xml:id="_CZtfyyf"><s xml:id="_dAJ9Quy">Mental health analysis through text requires a deep understanding of both sentiments and subjective opinions expressed in user-generated content.</s><s xml:id="_fPjkCkc">This challenge can be effectively addressed using multi-task learning, in wich a model is trained to handle multiple related tasks simultanously.</s><s xml:id="_rbBkgdP">In this study, we aim to classify both sentiment and status using a BERT-based hybrid model that integrates external opinion embeddings to enhance the model's interpretative capabilities.</s></p><p xml:id="_DTRjtMM"><s xml:id="_hqcd3U3">Input: Let Xi represent the input sequence for the i-th instance, where</s></p><formula xml:id="formula_2">Xi = {xi1, xi2, . . . , xim}</formula><p xml:id="_tJQEQG6"><s xml:id="_uYNbTsT">represents a sequence of m tokens.</s><s xml:id="_C6X5nBe">In addition, let Oi denote the external opinion information associated with the input sequence, represented as an embedding.</s></p><p xml:id="_Xd9dwBW"><s xml:id="_qUpTD2B">• Text input: Xi ∈ R m×d (BERT embeddings of the input text sequence)</s></p><p xml:id="_eRkfTCh"><s xml:id="_NuhPYyj">• Opinion Input: Oi ∈ R d (dense opinion embedding)</s></p><p xml:id="_mMk4aE6"><s xml:id="_Zrzpktv">Output: The model is designed for multi-task learning, providing predictions for both sentiment and status classification:</s></p><p xml:id="_ZFVwMnG"><s xml:id="_SgUdhrh">• Sentiment output:</s></p><formula xml:id="formula_3">Y sentiment i ∈ R C , where C is the number of sentiment classes. • Status output: Y status i ∈ R S ,</formula><p xml:id="_mUCnhkt"><s xml:id="_bNXeYta">where S is the number of status classes.</s></p><p xml:id="_XQ3WgBH"><s xml:id="_TMFV9Ya">• Final output: The model predicts:</s></p><formula xml:id="formula_4">Yi = (Y sentiment i , Y status i )</formula><p xml:id="_XtmF8t5"><s xml:id="_acgz7KU">where Yi is a pair consisting of the predicted sentiment and status labels for input sequence <ref type="bibr">Xi.</ref></s><s xml:id="_fjkv3Dj">This formulation encapsulates the multi-task nature of the problem, aiming to accurately classify both sentiment and status from text data enriched with opinion-based insights.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_v5VxbRv">Proposed methodology</head><p xml:id="_MwKA3ft"><s xml:id="_5BgWEbM">Figure <ref type="figure" target="#fig_2">3</ref> illustrates the architecture of our Opinion-Enhanced Hybrid BERT Model, which is designed to handle two tasks simultaneously: sentiment classification and status classification in mental health-related text data.</s><s xml:id="_bHhhEsx">Our model enhances standard BERT embeddings with custom opinion embeddings, capturing subjective opinions expressed in the text.</s><s xml:id="_4z4XZC6">These opinions are critical for understanding mental health conditions.</s><s xml:id="_bSfPdFB">By combining contextual information from BERT with subjective insights through opinion embeddings, we provide a deeper understanding of emotional states.</s></p><p xml:id="_fqFWbWw"><s xml:id="_FwpsVrA">We employed BERT as the foundation of our model to generate contextual embeddings.</s><s xml:id="_JWENAM8">We extended this using a custom OpinionsEmbedding Layer that integrates opinion-based information.</s><s xml:id="_eVVbVXK">A hybrid feature extraction mechanism, utilizing both CNN and BiGRU layers, captures the local patterns and long-range dependencies in the text.</s><s xml:id="_xBMXjNV">Our model is structured within a multi-task learning framework, enabling us to perform sentiment and status classification in parallel, learning shared representations that improve performance on both tasks.</s></p><p xml:id="_3TvgEpz"><s xml:id="_r4GDUcU">1. BERT encoder: Our model is based on BERT and produces embeddings represented by HBERT.</s><s xml:id="_a2kC7bE">These BERT embeddings capture syntactic and semantic relationships between words, offering a comprehensive, context-aware representation of the input text.</s><s xml:id="_dnxDgCY">By leveraging BERT, we ensure the model identifies subtle nuances in meaning, which is essential for correctly understanding information linked to mental health.</s><s xml:id="_wFapAam">Each word is represented in relation to its surrounding context, which is particularly helpful when handling complex language patterns often found in mental health-related text, such as metaphorical language and emotive expressions.</s></p><p xml:id="_tncVFDu"><s xml:id="_rcyCFHK">To enhance sentiment and status classification accuracy, we complement the contextual understanding provided by HBERT with additional components, such as CNN, BiGRU, and attention-based opinion embeddings.</s><s xml:id="_D86m6VP">This layered approach enables the model to capture nuanced changes in tone, inferred emotions, and subtle viewpoints.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2." xml:id="_BJ7gQ26">Opinion embeddings:</head><p xml:id="_cZbmsXe"><s xml:id="_7TurdKS">We incorporate a specific layer called the OpinionsEmbedding Layer, which extracts subjective information like opinions and emotions.</s><s xml:id="_p5MsKQx">This layer complements the contextual information captured by BERT embeddings.</s><s xml:id="_MQJuRa7">To extract sentiment-related subtleties, we employ an attention mechanism that highlights the most relevant textual elements.</s><s xml:id="_Nb5TZTC">Specifically, we calculate queries Qi, keys Ki, and values Vi for each word in the input sequence:</s></p><formula xml:id="formula_5">Qi = Eo • WQ i , Ki = Eo • WK i , Vi = Eo • WV i</formula><p xml:id="_nXRjgwE"><s xml:id="_rHqdgdH">Here, Eo is the base opinion embedding derived from external sentiment annotations, while WQ i , WK i , and WV i are learned weight matrices that transform Eo into queries, keys, and values.</s></p><p xml:id="_JEgQVYG"><s xml:id="_eCuaUWp">Next, we compute the attention score Si, which determines the significance of each word in expressing opinions or emotions:</s></p><formula xml:id="formula_6">Si = tanh(Qi • Wa + Ki • Ua + Va)</formula><p xml:id="_YHETAGt"><s xml:id="_TPMFSE8">This score leverages learned weight matrices Wa, Ua, and a vector Va to highlight opinion-relevant words.</s><s xml:id="_vwytaMd">Applying these scores to the values generates the final opinion embeddings Oopinions, which encapsulate the subjective dimensions of the text.</s></p><p xml:id="_uJvawsS"><s xml:id="_gMgBf7q">We then concatenate these opinion embeddings with the contextual embeddings from BERT to create a unified representation:</s></p><formula xml:id="formula_7">H combined = Concat(HBERT, Oopinions)</formula><p xml:id="_rVcHNne"><s xml:id="_EnjBkrX">This combined embedding H combined enriches the model's understanding by integrating both factual and emotional insights, which is crucial for mental health studies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3." xml:id="_fMEBebm">Hybrid architecture (CNN and BiGRU):</head><p xml:id="_yr5HadE"><s xml:id="_CCp4k2c">We process the combined embeddings H combined using a hybrid architecture that includes convolutional neural networks (CNNs) and bidirectional GRUs (BiGRUs).</s><s xml:id="_sMsYMPR">The CNN branch captures local patterns, such as emotionally charged phrases, using 1D convolution: HCNN = Conv1D(H combined , k)</s></p><p xml:id="_K5KR5PP"><s xml:id="_2c9fc9D">A max-pooling operation reduces the dimensionality while retaining significant features:</s></p><formula xml:id="formula_8">HCNN = MaxPool1D(HCNN)</formula><p xml:id="_2hvtzq3"><s xml:id="_9KKNU5R">Simultaneously, the BiGRU captures long-range dependencies and sequential relationships in the text.</s><s xml:id="_3DwZMBx">We generate the BiGRU output HBiGRU as:</s></p><formula xml:id="formula_9">HBiGRU = BiGRU(H combined )</formula><p xml:id="_zJAHtyz"><s xml:id="_W3keBnk">A global max-pooling operation further condenses the BiGRU output:</s></p><formula xml:id="formula_10">HBiGRU = GlobalMaxPooling1D(HBiGRU)</formula><p xml:id="_2DXp9yK"><s xml:id="_CZnN8Pp">By combining the CNN and BiGRU outputs, our architecture captures both short-term and long-term dependencies, making it well-suited for analyzing nuanced emotional expressions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4." xml:id="_FfEv4hH">Multi-task learning:</head><p xml:id="_dwSh9AN"><s xml:id="_P3wDn4y">We adopt a multi-task learning framework to perform sentiment classification and status classification simultaneously.</s><s xml:id="_2tgfkEa">This approach enables the model to generalize better by leveraging shared patterns between the two tasks.</s><s xml:id="_t2fpGy8">We apply dropout and layer normalization to the concatenated outputs from the CNN and BiGRU layers:</s></p><formula xml:id="formula_11">H final = LayerNormalization(Dropout(Concat(HCNN, HBiGRU)))</formula><p xml:id="_jEESESq"><s xml:id="_STJdXMF">Finally, two separate softmax layers generate predictions for sentiment and status classifications:</s></p><formula xml:id="formula_12">ŷsentiment =softmax(WsentimentH final ) ŷstatus =softmax(WstatusH final )</formula><p xml:id="_dMAWeGt"><s xml:id="_xvAPnfr">This framework effectively integrates data from both tasks, improving overall accuracy and efficiency.</s></p><p xml:id="_gvSfDCm"><s xml:id="_6yPuuGR">Our CNN-BiGRU hybrid architecture, combined with opinion embeddings and BERT, captures both subjective viewpoints and factual information in mental health-related texts.</s><s xml:id="_DASChWB">Consequently, we achieve a comprehensive understanding of emotional tone (sentiment) and broader emotional contexts (status).</s><s xml:id="_5s7qRqs">This approach makes our model a robust tool for analyzing text in the domain of mental health.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3kcYp9Q">Loss functions</head><p xml:id="_tfRFpw7"><s xml:id="_7qtTFD3">For sentiment and status categorization, the described model uses categorical cross-entropy loss functions, that are subsequently applied to the respective outputs.</s><s xml:id="_2kWBBdj">Each task has an output layer, however, the two loss functions are calculated separately and combined to provide the overall loss.</s></p><p xml:id="_gXEZp2y"><s xml:id="_y7TQUvm">Lstatus represents the categorical cross-entropy loss for the status classification problem, whereas Lsentiment represents the sentiment classification task.</s><s xml:id="_YdDRKqw">The overall loss function is the weighted sum of these individual losses and is represented by the model as L.</s></p><formula xml:id="formula_13">L = α • Lstatus + β • Lsentiment (<label>1</label></formula><formula xml:id="formula_14">)</formula><p xml:id="_4ndt2x4"><s xml:id="_rHuCkAP">where:</s></p><formula xml:id="formula_15">Lstatus = - C status ∑ i=1 ystatus,i log(pstatus,i)<label>(2)</label></formula><formula xml:id="formula_16">Lsentiment = - C sentiment ∑ j=1 ysentiment,j log(psentiment,j)<label>(3)</label></formula><p xml:id="_GnWCS5Y"><s xml:id="_EqV4Ytm">Here:</s></p><p xml:id="_PzZnZQX"><s xml:id="_BSCPVAg">• ystatus,i is the true label for the status classification, where Cstatus is the number of status classes.</s></p><p xml:id="_MsXxQen"><s xml:id="_v2BwmSv">• pstatus,i is the predicted probability for the status classification.</s></p><p xml:id="_4UxXdyD"><s xml:id="_peESjen">• ysentiment,j is the true label for sentiment classification, where Csentiment denotes the number of sentiment classes.</s><s xml:id="_vCgJfbP">• psentiment,j is the predicted probability for the sentiment classification.</s></p><p xml:id="_GHzskFm"><s xml:id="_PURkemk">Weights α and β balance the contributions of each loss term to the overall loss function, allowing for tailored optimization based on the importance of each task.</s><s xml:id="_xWCF5PU">These weights can be adjusted to consider task-specific needs or performance.</s><s xml:id="_VHezdfS">For equal weighting, these weights were typically set to one.</s></p><p xml:id="_tJaexdz"><s xml:id="_nAGaKqM">The model minimizes the combined loss L, which guides the optimization process by balancing errors across both classification tasks to guarantee that it learns to perform well in both status and sentiment predictions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_yGhtwat">Experimental setup</head><p xml:id="_CNwBfe6"><s xml:id="_ZUVqcFH">The trials in this study were conducted using a machine with 16GB of RAM and an NVIDIA GeForce RTX 2060 GPU, which provided sufficient processing capability for deep learning model training.</s><s xml:id="_4PR6wbA">The implementation was completed using TensorFlow 2.10.1, a popular deep learning framework that facilitates GPU acceleration for effective training and inference.</s><s xml:id="_4M84amC">Python 3.9.19 was used to construct the codebase because it provides a stable environment integrating different libraries and performing machine learning operations.</s><s xml:id="_WymfWvk">Fast training iterations and efficient use of computing resources were made possible by this configuration, which guaranteed a stable and consistent platform for testing the proposed Opinion-Enhanced Hybrid BERT Model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZwWPDYQ">Baseline models</head><p xml:id="_pz4XBeQ"><s xml:id="_BaAXVYd">The baseline models used in this study were DistilBERT 11 , RoBERTa 10 , and BERT <ref type="bibr" target="#b8">9</ref> .</s><s xml:id="_R9yC77E">These sophisticated models are quite effective at comprehending text.</s><s xml:id="_6ZRKejr">Strong language comprehension was provided by the primary model, BERT.</s><s xml:id="_JEKWKWf">To outperform BERT, RoBERTa requires longer training times and more data.</s><s xml:id="_xEhDbGz">DistilBERT is a useful alternative to BERT, as it is a scaled-down version of the algorithm that operates more quickly without sacrificing much of its accuracy.</s></p><p xml:id="_CXTGNEb"><s xml:id="_PAYzqfW">• BERT-base-uncased: A transformer-based model that uses a masked language-modeling objective for pre-training, making it effective for various NLP tasks.</s><s xml:id="_deNXpVV">• RoBERTa: An optimized version of BERT that modifies key hyperparameters, removes the next sentence prediction objective, and trains on larger mini-batches.</s><s xml:id="_dEkpujN">• DistilBERT: A smaller, faster, and lighter version of BERT trained using knowledge distillation, providing competitive performance with reduced resource consumption.</s></p><p xml:id="_c6PM4HY"><s xml:id="_9zkNgXT">Because these models have demonstrated efficacy in natural language processing tasks and consistently deliver high performance across a range of benchmarks, we chose them as baselines.</s><s xml:id="_2HbsQCn">We can evaluate efficiency and accuracy by comparing their different sizes and training approaches, which provide information about the tradeoffs associated with model selection.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_VqWRw5n">Ablation study</head><p xml:id="_X7pVnVz"><s xml:id="_AcJPsFa">The goal of this ablation study was to evaluate the efficacy of several elements incorporated into the suggested model.</s><s xml:id="_HB5cJjE">We performed this by experimenting with different configurations and evaluating how well they performed in comparison to the entire BERT-CNN-BiGRU model with attention-based opinion embeddings.</s></p><p xml:id="_npgMTTJ"><s xml:id="_vqTxwgP">In the original version, sentiment and status predictions were made using only the BERT encoder, thereby eliminating opinion embeddings.</s><s xml:id="_Kdqgjjh">Subsequently, we simply used BERT and opinion embeddings to evaluate the model, not the hybrid CNN-BiGRU architecture, to see how opinion information contributed.</s><s xml:id="_PG2kjbW">We also experimented with the more straightforward hybrid models, BERT-BiGRU and BERT-CNN, to separate the effects of each element.</s><s xml:id="_KRZkeMB">To determine how adding or removing CNN, BiGRU, and opinion embeddings impacted the overall performance, each configuration was examined.</s><s xml:id="_CtDTsKv">The effects of adding or removing the CNN, BiGRU, and opinion embeddings on the overall performance were examined for each configuration.</s><s xml:id="_hGeVXsM">The full BERT-CNN-BiGRU model with attention-based opinion embeddings consistently outperformed all other configurations, according to the results, proving that attention-enhanced opinion integration and local and sequential feature extraction are essential for precise and nuanced mental health analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_nXHutqT">Hyperparameter settings</head><p xml:id="_DfCPs6g"><s xml:id="_fXkJTsw">Certain hyperparameters were used to fine-tune the suggested Opinion-BERT model.</s><s xml:id="_MdfrW6R">The Adam optimizer was employed with a learning rate of 2e-5 and a batch size of 32.</s><s xml:id="_4F8V6ub">With an embedding dimension of 768, the length of the input sequence is restricted to 100 tokens.</s><s xml:id="_uMdPcvb">The CNN component employed 64 filters with a kernel size of 3, and MaxPooling1D came next.</s><s xml:id="_YPD6G7m">The attention mechanism features four heads.</s><s xml:id="_4FPXDQx">The 64 units of the BiGRU layer were used to record sequential data.</s><s xml:id="_CPd9P5r">The model was trained for 15 epochs with categorical cross-entropy as the loss function, and a dropout rate of 0.3 was used to avoid overfitting.</s><s xml:id="_5Wggx4K">Early halting with four-epoch patience was used to maximize training, and when the validation loss plateaued, the learning rate scheduler modified the learning rate.</s><s xml:id="_BuT6BHX">This setup allowed the model to accurately capture complex sentiments and status patterns in mental health texts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JuV23VW">Evaluation metrics</head><p xml:id="_vSqYEym"><s xml:id="_5zgvu6w">Several assessment measures were used to evaluate how well the suggested Opinion-BERT model classified sentiment and status:</s></p><p xml:id="_RgRe6hs"><s xml:id="_VeP6vyX">• Accuracy: This metric measures the proportion of correct predictions to the total predictions.</s><s xml:id="_RjBJNpR">It is a straightforward indicator of how well the model performs in both sentiment and status classification tasks.</s></p><p xml:id="_CCrKMEy"><s xml:id="_2WQbaqk">Accuracy =</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6JgMV8z">Number of Correct Predictions Total Number of Predictions</head><p xml:id="_eGMZrey"><s xml:id="_jXR7BrS">• Precision: Precision evaluates the accuracy of positive predictions.</s><s xml:id="_vCzBUKY">This is the ratio of true positive predictions to the total predicted positives, providing insight into the ability of the model to avoid false positives.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fr9Vu2U">Precision = True Positives True Positives + False Positives</head><p xml:id="_5avcRPY"><s xml:id="_FbEEJY3">• Recall (sensitivity): Recall assesses the model's capability to correctly identify positive instances.</s><s xml:id="_xwK7T3n">This is the ratio of true positive predictions to the total actual positives, indicating the effectiveness of the model in detecting relevant cases.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_A2xGcgk">Recall = True Positives True Positives + False Negatives</head><p xml:id="_c5qjQ7k"><s xml:id="_DEtppjH">• F1-score: The F1-Score is the harmonic mean of the Precision and Recall, providing a balanced measure that accounts for both false positives and false negatives.</s><s xml:id="_NTG8Fff">This is particularly useful when dealing with unbalanced datasets.</s></p><formula xml:id="formula_17">F1-Score = 2 × Precision × Recall Precision + Recall</formula><p xml:id="_Rzeb8d9"><s xml:id="_yfnejeP">• AUC-ROC (area under the receiver operating characteristic curve): This metric evaluates the model's ability to distinguish between classes.</s><s xml:id="_SjfQ2kq">A higher AUC indicates better performance, reflecting the trade-off between the true positive rate (recall) and the false positive rate.</s></p><p xml:id="_uJ4S36Z"><s xml:id="_sfahxaz">A thorough assessment of the model's efficacy in sentiment and status categorization is provided by these metrics combined, enabling a thorough examination of its prediction abilities and dependability while processing intricate text data on mental health.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qh6GUy4">Results analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Zg8kJaa">Baseline comparison</head><p xml:id="_6GXKtMR"><s xml:id="_3fne2AN">The suggested Opinion BERT model and the baseline models (BERT, RoBERTa, and DistilBERT) are thoroughly compared in Table <ref type="table" target="#tab_3">3</ref> for the Status and Sentiment Classification tasks.</s><s xml:id="_9vzFSw8">As can be seen, the proposed model achieved the maximum accuracy, macro precision, recall, and F1-score metrics while continuously outperforming the baselines in both tasks.</s><s xml:id="_DPeGhkv">Opinion BERT outperformed the next best BERT, with an accuracy of 93.74% for Status Classification, compared to 90.98% for BERT.</s><s xml:id="_yWx98Y8">Opinion BERT outperformed BERT in Sentiment Classification, with 96.25% accuracy.</s><s xml:id="_VPushRr">Incorporation of attention-based opinion embeddings, which improve the model's comprehension of complex contextual information, is responsible for the model's higher performance.</s><s xml:id="_2hutmvA">Furthermore, a higher F1-score in both tests indicates that Opinion BERT is competent at properly identifying both positive and negative examples, as seen by its more balanced accuracy and recall values.</s><s xml:id="_KkrsUSz">This demonstrates the effect of combining the BERT-CNN-GRU architecture with opinion-aware embeddings.</s><s xml:id="_MzJnTd6">The confusion matrices for status and sentiment classification tasks across different models are shown in Figs. <ref type="figure" target="#fig_3">4</ref> and <ref type="figure" target="#fig_4">5</ref>, respectively.</s><s xml:id="_7PTBaPc">The effectiveness of BERT, RoBERTa, DistilBERT, and the suggested model in categorizing mental health status into groups including stress, anxiety, depression, and personality disorder are contrasted in Fig. <ref type="figure" target="#fig_3">4</ref>. Improved prediction accuracy across several categories is demonstrated by the suggested model's decreased misclassification rates, particularly in more complicated categories like bipolar and suicidal.</s><s xml:id="_pE4G9FP">With a greater count in the diagonal element for the true class, for instance, it more accurately identifies bipolar instances.</s><s xml:id="_2k6ZEZz">The confusion matrices for sentiment classification show how well various models distinguish between positive, neutral, and negative attitudes, as shown in Fig. <ref type="figure" target="#fig_4">5</ref>. Compared with the conventional BERT and RoBERTa models, the suggested model shows improved accuracy in predicting Positive and Neutral classes, lowering the number of cases that are incorrectly categorized.</s><s xml:id="_QjuSaDt">The proposed model exhibits fewer mistakes in differentiating between Positive and Neutral sentiments, which results in improved overall performance metrics.</s><s xml:id="_v3TzvfR">This is especially evident in the accurate classification of positive sentiment.</s><s xml:id="_qVbYGKX">The outcomes demonstrate the effectiveness of the proposed model design, which combines CNN, BiGRU, and sophisticated attention mechanisms to capture subtle linguistic signals essential for mood and status predictions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NzMuvTj">Model</head><p xml:id="_e2p3d38"><s xml:id="_uYyUG3X">The performance of several models across status and sentiment classification tasks are depicted by the ROC curves in Figs. <ref type="figure" target="#fig_5">6</ref> and <ref type="figure" target="#fig_6">7</ref>, respectively.</s><s xml:id="_j99GVg9">These graphs, which plot the True Positive Rate (sensitivity) versus the False Positive Rate at different threshold values, demonstrate the capacity of the models to discriminate between the classes.</s></p><p xml:id="_BbNJteA"><s xml:id="_bkrWhH3">A strong discriminative capacity is indicated by the proposed model's high AUC values across all mental health status categories, as shown in Fig. <ref type="figure" target="#fig_5">6</ref>.</s><s xml:id="_pBfzT3z">With an AUC of 1.00, the model attained perfect or almost perfect AUC values for categories, such as stress, anxiety, and suicidal thoughts.</s><s xml:id="_sYFr5Ke">Interestingly, the suggested model achieves AUC values of 0.98, exceeding RoBERTa and DistilBERT, especially in more complicated categories such as depression and bipolar disorder, which sometimes offer difficulties because of overlapping symptoms.</s><s xml:id="_eZhcqFw">This demonstrates how well the proposed approach can identify minute patterns connected to various mental health issues.</s></p><p xml:id="_jxtfZjB"><s xml:id="_bSZDkM6">A strong performance in the emotion categorization challenge is also shown in Fig. <ref type="figure" target="#fig_6">7</ref>.</s><s xml:id="_7yHvJw2">The suggested model regularly outperformed the conventional BERT and RoBERTa models, producing AUC values of 0.99 or higher, across Positive, Neutral, and Negative attitudes.</s><s xml:id="_EhxhGKz">According to this, the architecture that combines CNN and BiGRU layers with sophisticated attention mechanisms is very good at capturing the subtleties of sentiments, which reduces misclassifications and improves overall precision, particularly when separating positive and neutral sentiments.</s><s xml:id="_5eADX4G">These ROC curves highlight the accuracy and dependability of the proposed model in sentiment and status classification tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Fta6EQC">Comparison of sentiment lexicons</head><p xml:id="_sjxJnRb"><s xml:id="_yJkuAxd">Table <ref type="table">4</ref> highlights the performance comparison of sentiment lexicons-Afinn, VADER, and TextBlob-across status and sentiment classification tasks.</s><s xml:id="_8vAsdZe">TextBlob consistently outperforms the others, achieving the highest test accuracy of 93.74% for status classification and 96.25% for sentiment classification.</s><s xml:id="_7H2d6UD">It also demonstrates superior macro precision, recall, and F1-scores, reaching 94.17%, 94.48%, and 94.32% for status classification, and 96.50%, 96.68%, and 96.58% for sentiment classification.</s><s xml:id="_gU4mv47">VADER shows strong performance, particularly in sentiment classification, with a test accuracy of 94.12% and an F1-score of 93.68%.</s><s xml:id="_YurnDb4">Afinn performs reliably, achieving a sentiment classification F1-score of 91.97%.</s><s xml:id="_Bqh6NaU">As shown in Table <ref type="table">4</ref>, TextBlob proves to be the most effective lexicon for sentiment analysis, with VADER and Afinn providing solid alternatives for specific use cases.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JAqWKGe">Ablation study</head><p xml:id="_W89rw8t"><s xml:id="_bFgkW6P">Table <ref type="table">5</ref> displays the findings of the ablation research, which was conducted to assess how well different models performed on tasks involving the categorization of emotion and status.</s><s xml:id="_4DHBRkZ">BERT with opinion embedding, BERT-CNN-BiGRU with attention-based opinion embedding, BERT-BiGRU with opinion embedding, BERT-CNN with opinion embedding, BERT-CNN-BiGRU without opinion embedding, and suggested Opinion BERT with attention-based opinion embedding are among the models that were evaluated.</s><s xml:id="_FnHYGE5">For both classification tasks, each model's test accuracy, F1-score, recall, and macro precision are displayed.</s></p><p xml:id="_n6PaRRK"><s xml:id="_DkgtVYt">Notably, the suggested Opinion BERT demonstrated its efficacy in capturing subtle semantic links through attention-based opinion embedding, with the highest test accuracy of 93.74% for status classification and 96.25% for sentiment classification.</s><s xml:id="_Hr7pTYp">By concentrating on the important elements of the input text, this novel embedding technique improves the model's comprehension and contextualization of the attitudes conveyed.</s><s xml:id="_T44gQdB">More accurate predictions are produced by the model's ability to evaluate the importance of various words or phrases according to their applicability to the attitude under study, owing to the inclusion of attention processes.</s></p><p xml:id="_QdQFx4M"><s xml:id="_5VfeWQM">Additionally, BERT-CNN-BiGRU with conventional opinion embedding performed competitively, particularly in status categorization, where it achieved 91.94% accuracy.</s><s xml:id="_Ft5tvvb">The CNN and BiGRU layers work together to effectively extract features and comprehend the context, both of which are essential for precise categorization.</s></p><p xml:id="_N5yWteC"><s xml:id="_mNHKaqy">Furthermore, the findings show that opinion embedding is important for enhancing performance.</s><s xml:id="_HTup5xH">Illustration of the significance of integrating domain-specific information, BERT-BiGRU with opinion embedding fared better than versions utilizing conventional embeddings alone, although it had lower accuracy than the suggested model.</s><s xml:id="_r3C4VF5">The BERT-CNN-BiGRU model without opinion embedding, on the other hand, performed the worst, highlighting the need for embedding techniques to increase model efficacy.</s></p><p xml:id="_EfGnw5U"><s xml:id="_wSAmc3z">Overall, the findings demonstrate how various model architectures and embedding tactics affect performance, highlighting the significance of customized methods in opinion analysis, especially with the inclusion of attention mechanisms in the suggested Opinion BERT.</s><s xml:id="_ZjUqEzD">Lexicon Task Test accuracy (%) Macro precision (%) Recall (%) F1-score (%) Afinn Status classification 91.35 90.53 92.09 91.23 Sentiment classification 92.99 92.01 92.28 91.97 VADER Status classification 92.27 91.84 92.94 92.36 Sentiment classification 94.12 94.39 93.07 93.68 TextBlob Status classification 93.74 94.17 94.48 94.32 Sentiment classification 96.25 96.50 96.68 96.58 Table 4. Comparison of different sentiment lexicons.</s><s xml:id="_nABH7xV">Table 5. Ablation study results for status and sentiment classification.</s><s xml:id="_pNknNaB">OE opinion embedding, TOE traditional opinion embedding, w/ with, w/o without.</s><s xml:id="_EVjZZGb">a 92.35% accuracy rate for hotel reviews by combining BERT with CNN, BiLSTM, and Attention processes to overcome the conventional drawbacks of CNN and RNN.</s><s xml:id="_UbEar2e">By using a Bi-LSTM architecture for social media data from sites such as Facebook, Instagram, and Twitter, Selva Mary et al. <ref type="bibr" target="#b43">44</ref> were able diagnosed with depression, with an astounding 98.5% accuracy rate.</s><s xml:id="_6BJzHz2">Sowbarnigaa et al. <ref type="bibr" target="#b44">45</ref> classified depression with a 93% accuracy rate using a CNN-LSTM combination.</s><s xml:id="_4QZD4Xw">By using the EmoMent corpus and RoBERTa, Atapattu et al. <ref type="bibr" target="#b45">46</ref> concentrated on South Asian settings and obtained F1 values of 0.76 and 0.77 for post-categorization and mental health prediction, respectively.</s><s xml:id="_pPxs2eZ">The intricacy of the MAMS dataset was addressed by Wu et al. <ref type="bibr" target="#b46">47</ref> work on multi-aspect sentiment analysis using a RoBERTa-TMM ensemble, which demonstrated good F1 scores in both ATSA (85.24%) and ACSA (79.41%).</s><s xml:id="_JUgPspS">In contrast, our study uses a multi-input neural network called Opinion-BERT, which integrates CNN, BiGRU, Transformer blocks, token embeddings, and attention mechanisms.</s><s xml:id="_REFUuXU">It achieved an accuracy of 93.74% for classifying mental health status and 96.25% for sentiment analysis.</s><s xml:id="_ZD7fNFj">This demonstrates how well our method handles the complex and multidimensional characteristics of mental health literature.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_bDrDwjq">Discussion</head><p xml:id="_aFdSKG8"><s xml:id="_wm3Gk4q">The proposed Opinion-BERT model has shown great promise for improving mental health analysis through multi-task learning by integrating attention-based opinion embeddings.</s><s xml:id="_h9cjKqV">Understanding both explicit and subtle material in texts pertaining to mental health has been demonstrated to be greatly aided by the combination of opinion-specific data with contextual embeddings of BERT.</s><s xml:id="_WwQ2hJy">This integrated method reflects the intricacies of human language, which frequently consists of a combination of ideas, feelings, and factual information that is difficult for standard models to comprehend.</s><s xml:id="_k2JZKbc">The effect of opinion embeddings on overall model performance is one noteworthy finding.</s><s xml:id="_VYPggXU">Incorporating subjective signals is crucial because the ablation study makes it evident that adding opinion embeddings improves emotion and status classification tasks.</s><s xml:id="_r5cqYrZ">According to these results, opinions are important in mental health analysis since they frequently give emotional states and behavioral inclinations context.</s><s xml:id="_2FnG3Np">To extract sentimentrelated information from the text, the Opinions Embedding Layer's attention mechanism effectively finds the most pertinent passages, which enhances the accuracy and dependability of the model.</s></p><p xml:id="_8gpmWCY"><s xml:id="_EwMAbWW">Additionally, the hybrid design that included CNN and BiGRU layers worked well.</s><s xml:id="_K3ySnMT">Local patterns and relationships, including phrases and word n-grams, are well captured by the CNN component, but the BiGRU component represents the long-term context and sequential dependencies.</s><s xml:id="_sH3m8nR">In texts on mental health, which may contain subtle tone changes, sarcasm, or complicated emotional states that call for an awareness of both local and global text elements, this combination is particularly helpful.</s></p><p xml:id="_z9xkY79"><s xml:id="_rd6UJcu">Furthermore, by utilizing shared information between various tasks, the multi-task learning architecture enabled the model to predict sentiment and status at the same time.</s><s xml:id="_Q3U7xHz">This shared learning was beneficial because of its improved generalization and decreased the possibility of overfitting to a single task.</s><s xml:id="_a2eUCDg">The findings support the idea that multi-task learning works well in complicated fields like mental health, where overlapping variables affect different emotional states.</s></p><p xml:id="_6XQZ8v2"><s xml:id="_qGzWZ9e">All things considered, the Opinion-BERT model shows promise for enhancing mental health analysis through the use of cutting-edge NLP approaches.</s><s xml:id="_ZzfyKtU">A step toward developing more precise and thorough language models for this delicate and important topic has been made with the effective integration of context and subjective perspectives.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pqgB2My">Limitation and future work</head><p xml:id="_nXFUsXb"><s xml:id="_czATMQ6">Despite the encouraging outcomes of the Opinion-BERT model, it is important to recognize several limitations.</s><s xml:id="_rFzv82y">First, both the quality and representativeness of the training data are critical to the success of the model.</s><s xml:id="_3jqBNcy">Real-</s></p><p xml:id="_fuGra9X"><s xml:id="_Dh6qvp2">Citation Dataset Problem statement Proposed methodology Accuracy Kokane et al. 42 Twitter dataset, Reddit dataset Detecting mental illness using NLP Transformers on social media.</s><s xml:id="_DFa8hPv">Analyzing mental health status through text analysis on Twitter and Reddit.</s><s xml:id="_95br7EW">DistilBERT 91% (Twitter), 84% (Reddit) Chen et al. 43 Hotel review datasets Traditional CNN ignores contextual semantic information.</s><s xml:id="_jbG8Gmb">Traditional RNN has information memory loss and vanishing gradient.</s><s xml:id="_Npesg4w">BERT + CNN + BiLSTM + Attention 92.35% Selva Mary et al. 44 User-generated content from Twitter, Facebook, and Instagram Detecting depression signs in social media content.</s><s xml:id="_ud3GNYU">Enhancing early intervention and support for mental health challenges.</s><s xml:id="_C2nbhdm">Bi-LSTM 98.5% Sowbarnigaa et al. 45 English language social media postings.</s><s xml:id="_gw4ZKM5">Shared task introduced by ACL 2022.</s><s xml:id="_Fkkt25k">Detecting signs of depression from social media postings.</s><s xml:id="_rSZ8Hqy">Utilizing sentiment analysis to categorize depression indicators.</s><s xml:id="_RU7cyJZ">CNN-LSTM Precision: 93% Atapattu et al. 46 EmoMent corpus (2802 Facebook posts from Sri Lanka and India) Detect mental health issues from text using NLP techniques.</s><s xml:id="_ZB22eHP">Develop emotionannotated mental health corpus from South Asian countries.</s><s xml:id="_xNp2bN3">RoBERTa F1: 0.76, Macro F1: 0.77 Wu et al. 47 NLPCC 2020 Shared Task 2 MAMS dataset Re-formalize ABSA as a multi-aspect sentiment analysis task.</s><s xml:id="_UPJWtYS">Address the complexity of the MAMS dataset with Transformer-based Multi-aspect Modeling.</s><s xml:id="_MqSE88s">RoBERTa-TMM ensemble F1: 85.24% (ATSA), F1: 79.41% (ACSA) Our work Mental health The model aims to classify mental health-related text into status categories and sentiment labels using a multi-input neural network combining token embeddings, CNN, BiGRU, Transformer blocks, and attention mechanisms.</s><s xml:id="_phDct5Y">Opinion-BERT Sentiment 96.25%, Status 93.74%</s></p><p xml:id="_2duMMKb"><s xml:id="_vunx2UM">Table 7.</s><s xml:id="_6hVnZex">Comparison of various approaches for mental health detection and sentiment analysis.</s></p><p xml:id="_Xp64MsJ"><s xml:id="_BeG5a7F">world applications may perform less well if the dataset is undiversified or skewed toward particular emotions or mental health issues.</s><s xml:id="_qReq94W">While the model's dependence on attention-based embeddings is advantageous for capturing context, it can also present problems such as overfitting, especially in situations where there is a lack of data.</s><s xml:id="_xfdeUpX">Moreover, there is still uncertainty over the interpretability of the model's choices.</s><s xml:id="_rQXEsAG">Although the attention mechanism emphasizes significant terms, it might be challenging to comprehend the underlying logic of certain forecasts.</s></p><p xml:id="_3TvngEg"><s xml:id="_9QD4jrv">To strengthen the generalizability of the Opinion-BERT model across a range of demographics and circumstances, future research should concentrate on expanding and diversifying its datasets.</s><s xml:id="_Ceaucje">Further improvements classification accuracy and resilience may include investigating different embedding structures and methodologies, such as adding knowledge graphs or strengthening multi-task learning techniques.</s><s xml:id="_qWbyM5Z">Future research could also focus on including explainability tools to provide more precise information on how the model makes decisions.</s><s xml:id="_fdkjQkv">Finally, expanding the Opinion-BERT framework's scope and fostering a more thorough comprehension of mental health feelings across various settings may be achieved by applying it to other fields, including social media and healthcare.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GqGQkXy">Conclusion</head><p xml:id="_9btvKGg"><s xml:id="_uM56U6J">We introduced the Opinion-BERT model in this study, which effectively integrates attention-based opinion embeddings to classify emotional states and mental health conditions.</s><s xml:id="_9ZekP7N">Through extensive experimentation, including an ablation study, we demonstrated that incorporating opinion-related features significantly enhances the model's performance.</s><s xml:id="_76vRAMt">Our results show that Opinion-BERT outperforms baseline models across key evaluation metrics, such as accuracy, precision, recall, and F1-score.</s></p><p xml:id="_Q72qHQd"><s xml:id="_nSxGeab">Our findings emphasize the critical role of both emotional and contextual cues in improving the accuracy of machine learning models for mental health analysis.</s><s xml:id="_jjcrhCq">By combining sentiment-specific embeddings with advanced contextual representations from BERT, we lay a strong foundation for future advancements in sentiment analysis and multi-task learning frameworks.</s></p><p xml:id="_D6fdHJq"><s xml:id="_DsYMmAR">This research opens new avenues for more effective and nuanced mental health evaluations, offering the potential for better understanding and intervention in this important field.</s><s xml:id="_ecJUpWW">Integrating sentiment analysis with mental health assessments not only improves classification accuracy but also contributes to more insightful and adaptable mental health monitoring tools.</s></p><p xml:id="_wks6ryJ"><s xml:id="_VCJvGje">In conclusion, the Opinion-BERT model takes a significant step forward in leveraging machine learning for mental health applications, offering a robust framework for future research and practical implementation in the field.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc><div><p xml:id="_bb2QKJs"><s xml:id="_36CCGXT">Fig. 2. Distribution of post lengths in the dataset.</s><s xml:id="_sayqanW">The histogram displays the frequency of posts according to their word count, highlighting the most common post lengths and providing insight into the dataset's typical content size.</s></p></div></figDesc><graphic coords="4,155.91,323.59,283.20,184.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc><div><p xml:id="_xggwqzK"><s xml:id="_6N6G2pz">Fig. 1.</s><s xml:id="_DHBj98H">Mental health status and sentiment distributions in the dataset.</s></p></div></figDesc><graphic coords="4,96.01,50.50,458.16,221.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc><div><p xml:id="_4Kks6fE"><s xml:id="_XJZeAGJ">Fig.3.</s><s xml:id="_fkmah2H">Our proposed architecture combines dynamically produced opinion embeddings with BERT embeddings.</s><s xml:id="_eSuhCmt">We integrate these opinion embeddings with BERT's contextual representations, which are obtained via sentiment annotations.</s><s xml:id="_HTEdFQU">Our design uses a hybrid feature extraction technique to capture sequential relationships as well as local patterns.</s><s xml:id="_fUAyEK2">This mechanism consists of layers of CNN and BiGRU.</s><s xml:id="_JhZuNP3">We apply concatenation, dropout, and normalizing layers to the final representations, yielding outputs for status prediction and sentiment categorization.</s></p></div></figDesc><graphic coords="8,103.93,50.50,450.24,295.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc><div><p xml:id="_twMmGNf"><s xml:id="_XAGaeJ8">Fig. 4. Confusion matrices for status classification using different models.</s></p></div></figDesc><graphic coords="12,106.09,50.50,448.08,413.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc><div><p xml:id="_znvprct"><s xml:id="_d35auPQ">Fig. 5. Confusion matrices for sentiment classification using different models.</s></p></div></figDesc><graphic coords="13,103.69,50.50,450.48,392.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc><div><p xml:id="_U5ZDfDU"><s xml:id="_brJ2TAm">Fig. 6.</s><s xml:id="_rddDg6V">AUC-ROC (Area under the receiver operating characteristic curve) for status classification using different models.</s></p></div></figDesc><graphic coords="14,102.97,50.50,451.20,481.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc><div><p xml:id="_xGySCtn"><s xml:id="_ncc8ery">Fig. 7. AUC-ROC (Area under the receiver operating characteristic curve) for sentiment classification using different models.</s></p></div></figDesc><graphic coords="15,104.89,50.50,449.28,479.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc><div><p xml:id="_j7kwUaC"><s xml:id="_6jVuKcN">Most frequent opinions.</s></p></div></figDesc><table><row><cell cols="2">Sentiment category Average number of opinions</cell></row><row><cell>Negative</cell><cell>33.80</cell></row><row><cell>Neutral</cell><cell>3.64</cell></row><row><cell>Positive</cell><cell>36.31</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc><div><p xml:id="_Mm5A9Pk"><s xml:id="_DRRe3GD">Baseline comparison for status and sentiment classification.</s></p></div></figDesc><table><row><cell></cell><cell>Task</cell><cell cols="4">Test accuracy (%) Precision (%) Recall (%) F1-score (%)</cell></row><row><cell>BERT</cell><cell cols="2">Status Sentiment 95.78 90.98</cell><cell>91.63 96.31</cell><cell>91.02 96.01</cell><cell>91.24 96.16</cell></row><row><cell>RoBERTa</cell><cell cols="2">Status Sentiment 89.52 82.28</cell><cell>82.30 89.56</cell><cell>80.44 90.32</cell><cell>80.91 89.91</cell></row><row><cell>DistilBERT</cell><cell cols="2">Status Sentiment 95.05 90.14</cell><cell>91.99 95.72</cell><cell>89.88 95.28</cell><cell>90.82 95.46</cell></row><row><cell>Proposed model (Opinion BERT)</cell><cell cols="2">Status Sentiment 96.25 93.74</cell><cell>94.48 96.68</cell><cell>94.17 96.50</cell><cell>94.32 96.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc><div><p xml:id="_wkP6rqn"><s xml:id="_bHwAd7G">Comparison of predicted and actual mental health status and sentiment for various models.</s></p></div></figDesc><table><row><cell>Model</cell><cell>Task</cell><cell cols="4">Test accuracy (%) Macro precision (%) Recall (%) F1-score (%)</cell></row><row><cell>BERT w/ OE</cell><cell cols="2">Status Sentiment 96.38 90.69</cell><cell>90.51 96.68</cell><cell>92.33 96.62</cell><cell>91.28 96.65</cell></row><row><cell>BERT-CNN-BiGRU w/ T.OE</cell><cell cols="2">Status Sentiment 96.03 91.94</cell><cell>93.00 96.45</cell><cell>92.80 96.29</cell><cell>92.79 96.36</cell></row><row><cell>BERT-BiGRU w/ OE</cell><cell cols="2">Status Sentiment 95.18 87.55</cell><cell>88.02 95.70</cell><cell>89.19 95.55</cell><cell>88.23 95.62</cell></row><row><cell>BERT-CNN w/ OE</cell><cell cols="2">Status Sentiment 95.32 92.13</cell><cell>93.04 95.97</cell><cell>92.38 95.20</cell><cell>92.68 95.55</cell></row><row><cell>BERT-CNN-BiGRU w/o OE</cell><cell cols="2">Status Sentiment 96.27 92.15</cell><cell>92.24 96.70</cell><cell>93.03 96.48</cell><cell>92.61 96.59</cell></row><row><cell>Proposed opinion BERT</cell><cell cols="2">Status Sentiment 96.25 93.74</cell><cell>94.48 96.68</cell><cell>94.17 96.50</cell><cell>94.32 96.58</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_9MaNsZX"><s xml:id="_p5jHdH9">Scientific Reports |(2025) 15:3332</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_gSvzfsH"><s xml:id="_a9QdVHt">| https://doi.org/10.1038/s41598-025-86124-6</s><s xml:id="_36s7E6f">www.nature.com/scientificreports/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_Nwbpfd8"><s xml:id="_VUzSqwF">Scientific Reports | (2025) 15:3332 4 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p xml:id="_JtFxfgf"><s xml:id="_jtRC9FQ">Scientific Reports | (2025) 15:3332 5 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p xml:id="_4NQbpcB"><s xml:id="_PVPZ2uH">Scientific Reports | (2025) 15:3332 6 | https://doi.org/10.1038/s41598-025-86124-6</s><s xml:id="_yaHWE9k">www.nature.com/scientificreports/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p xml:id="_PYVuBEC"><s xml:id="_2n7rdDu">Scientific Reports | (2025) 15:3332 7 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p xml:id="_NHcZXU4"><s xml:id="_RAHBuuz">Scientific Reports | (2025) 15:3332 8 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p xml:id="_eGPhzUC"><s xml:id="_j3QENKs">Scientific Reports | (2025) 15:3332 10 | https://doi.org/10.1038/s41598-025-86124-6</s><s xml:id="_2bTuyBA">www.nature.com/scientificreports/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p xml:id="_4FdHQbZ"><s xml:id="_ehzZ2UG">Scientific Reports | (2025) 15:3332 12 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p xml:id="_BnrDRFk"><s xml:id="_Dx5dD7q">Scientific Reports | (2025) 15:3332 13 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p xml:id="_3uYw2fV"><s xml:id="_FfUuD8C">Scientific Reports | (2025) 15:3332 14 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_11"><p xml:id="_tdQsWJw"><s xml:id="_5EhvRKt">Scientific Reports | (2025) 15:3332 15 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_12"><p xml:id="_WZkuS3D"><s xml:id="_rPSpcyM">Scientific Reports | (2025) 15:3332 16 | https://doi.org/10.1038/s41598-025-86124-6</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_13"><p xml:id="_7nWESKb"><s xml:id="_FZGxw7n">Scientific Reports | (2025) 15:3332 20 | https://doi.org/10.1038/s41598-025-86124-6</s><s xml:id="_nNscndH">www.nature.com/scientificreports/</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xml:id="_Xu6ZnRe"><p xml:id="_S7NCapN"><s xml:id="_sddafrG">| <ref type="url" target="https://doi.org/10.1038/s41598-025-86124-6">https://doi.org/10.1038/s41598-025-86124-6</ref></s><s xml:id="_U7p6BJu"><ref type="url" target="http://www.nature.com/scientificreports">www.nature.com/scientificreports/</ref></s></p></div>
<div><head xml:id="_pJwFvy9">Acknowledgements</head><p xml:id="_3z6Z42u"><s xml:id="_KegttWU">The authors extend their appreciation to the <rs type="funder">Research Chair of Online Dialogue and Cultural Communication, King Saud University, Riyadh, Saudi Arabia</rs> for funding this study.</s></p></div>
			</div>
			<div type="funding">
<div><head xml:id="_Pj7Zzvk">Funding</head><p xml:id="_ZZaYU9g"><s xml:id="_6yRUBZr">This research is funded by the <rs type="funder">Research Chair of Online Dialogue and Cultural Communication, King Saud University, Riyadh, Saudi Arabia</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_QUZQbS8">
					<idno type="grant-number">s41598-025-86124-</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AyknR5W">Data availability</head><p xml:id="_9fQkBcA"><s xml:id="_PQr62P7">Dataset available at <ref type="url" target="https://shorturl.at/LzeMH">https://shorturl.at/LzeMH</ref>.</s><s xml:id="_AvH8r2a">Received: 25 October 2024; Accepted: 8 January 2025</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_n8dN5cJ">Prediction analysis</head><p xml:id="_ftU9dNZ"><s xml:id="_SCr4VQa">The anticipated and actual sentiment and mental health conditions for many models, including BERT, RoBERTa, DistilBERT, and the suggested Opinion-BERT, are compared in Table <ref type="table">6</ref>.</s><s xml:id="_mFu85Kv">The sentiment and actual mental health state of the input text were compared with each model's predictions.</s><s xml:id="_TfqeqZX">When the actual state was "Depression, " as in the first example, all models expected the status to be "Suicidal, " but the predicted attitude stayed consistently "Positive.</s><s xml:id="_JTyv5Xk">" This shows that certain algorithms have a propensity to correctly detect emotions but misclassify mental health conditions.</s><s xml:id="_RRcxX8S">However, in the same scenario, Opinion-BERT demonstrated an improved capacity to comprehend subtle manifestations of mental health by effectively matching the projected condition with the actual status.</s></p><p xml:id="_Wd67RxW"><s xml:id="_9QAfnmN">Opinion-BERT continuously performed well in a variety of scenarios according to additional examinations of other entrants.</s><s xml:id="_yG7ftvg">While other models showed differences, particularly in sentiment predictions, the second entry accurately classified the status and sentiment as "Bipolar" and "Positive, " respectively.</s><s xml:id="_HYz3QtM">Opinion-BERT further proved its resilience by maintaining accurate predictions under both neutral and negative circumstances.</s><s xml:id="_4TARF9a">These findings imply that, in comparison to its competitors, Opinion-BERT's attention-based opinion embedding may provide a deeper understanding of mental health manifestations and improve classification performance.</s><s xml:id="_mqYkR3s">This table demonstrates how well the suggested model interprets complicated sentiments and mental health states in a variety of textual inputs.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3VKKXXf">Comparison among and existing work</head><p xml:id="_6mVBTPm"><s xml:id="_X9rtfhF">A comparison of several methods for sentiment analysis and mental health identification across the datasets is presented in Table <ref type="table">7</ref>.</s><s xml:id="_bDY9X5Q">When Kokane et al. <ref type="bibr" target="#b41">42</ref> applied NLP transformers such as DistilBERT to social media data, they were able to obtain noteworthy accuracy for Reddit (84%) and Twitter (91%).</s><s xml:id="_DUQhFGk">Chen et al. <ref type="bibr" target="#b42">43</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fNU6ttG">Declarations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WaNJx6Z">Competing interests</head><p xml:id="_Y8YWtGK"><s xml:id="_R6mf7rh">The authors assert that there are no conflicts of interest to disclose.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_s4kUSYY">Additional information</head><p xml:id="_aVmhJZ2"><s xml:id="_zVxD53t">Correspondence and requests for materials should be addressed to M.F.M. or M.S.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZweZBp4">Reprints and permissions information</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_jtYNR3M">Estimating the true global burden of mental illness</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Thornicroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Atun</surname></persName>
		</author>
		<idno type="DOI">10.1016/s2215-0366(15)00505-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZfzBxRQ">Lancet Psychiatr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="171" to="178" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vigo, D., Thornicroft, G. &amp; Atun, R. Estimating the true global burden of mental illness. Lancet Psychiatr. 3, 171-178 (2016).</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_cT3hAj8">Social media big data analytics: A survey</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A T</forename><surname>Hashem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sF7hw6x">Comput. Hum. Behav</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="417" to="428" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ghani, N. A., Hamid, S., Hashem, I. A. T. &amp; Ahmed, E. Social media big data analytics: A survey. Comput. Hum. Behav. 101, 417-428 (2019).</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_awA396z">A systematic review of applications of natural language processing and future challenges with special emphasis in text-based emotion detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kusal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qWvbHMw">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="15129" to="15215" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kusal, S. et al. A systematic review of applications of natural language processing and future challenges with special emphasis in text-based emotion detection. Artif. Intell. Rev. 56, 15129-15215 (2023).</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_Uc69ZG5">Mental health prediction model on social media data using CNN-BILSTM</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Fudholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7Zx8PPn">Kinetik: Game technology, information system, computer network, computing, electronics, and control</title>
		<imprint>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fudholi, D. H. Mental health prediction model on social media data using CNN-BILSTM. In Kinetik: Game technology, information system, computer network, computing, electronics, and control 29-44 (2024).</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_xYZnENK">A domain-independent framework for modeling emotion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cogsys.2004.02.002</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qRhhm4t">Cogn. Syst. Res</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="269" to="306" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gratch, J. &amp; Marsella, S. A domain-independent framework for modeling emotion. Cogn. Syst. Res. 5, 269-306 (2004).</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" xml:id="_DEZqVeA">The reader in the text: Essays on audience and interpretation</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Suleiman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Crosman</surname></persName>
		</editor>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">617</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Suleiman, S. R. &amp; Crosman, I. (eds.) The reader in the text: Essays on audience and interpretation, Vol. 617 (Princeton University Press, 2014).</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_Ak4wEet">A complete process of text classification system using state-of-the-art NLP models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Dogra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RdcfrFJ">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page">1883698</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dogra, V. et al. A complete process of text classification system using state-of-the-art NLP models. Comput. Intell. Neurosci. 2022, 1883698 (2022).</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_fJnQdht">Transformer models for text-based emotion detection: A review of BERTbased approaches</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Acheampong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nunoo-Mensah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dpYKPY7">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="5789" to="5829" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Acheampong, F. A., Nunoo-Mensah, H. &amp; Chen, W. Transformer models for text-based emotion detection: A review of BERT- based approaches. Artif. Intell. Rev. 54, 5789-5829 (2021).</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_EaKA7Ae">pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Devlin, J., Chang, M., Lee, K. &amp; Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. CoRRabs/1810.04805 (2018). arXiv:1810.04805.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main" xml:id="_kCb8xZ2">A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, Y. et al. Roberta: A robustly optimized BERT pretraining approach. CoRRabs/1907.11692 (2019). arXiv:1907.11692.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.5860/choice.169427</idno>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<title level="m" xml:id="_5hNvTBy">Distilbert, a distilled version of BERT: Smaller, faster, cheaper and lighter</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sanh, V., Debut, L., Chaumond, J. &amp; Wolf, T. Distilbert, a distilled version of BERT: Smaller, faster, cheaper and lighter (2020). arXiv:1910.01108.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_vUcZdCe">The early detection and diagnosis of mental health status employing NLP-based methods with ml classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.37648/ijrmst.v17i01.009</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PXnHxJz">J. Mental Health Technol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="45" to="60" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Singh, A. The early detection and diagnosis of mental health status employing NLP-based methods with ml classifiers. J. Mental Health Technol. 12, 45-60 (2024).</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_ZZ8rN7m">Machine learning driven mental stress detection on reddit posts using natural language processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Inamdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chapekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_d9BWrse">Hum.-Centric Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="80" to="91" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Inamdar, S., Chapekar, R., Gite, S. &amp; Pradhan, B. Machine learning driven mental stress detection on reddit posts using natural language processing. Hum.-Centric Intell. Syst. 3, 80-91 (2023).</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_6aNwyZf">Public&apos;s mental health monitoring via sentimental analysis of financial text using machine learning techniques</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Alanazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kgWVmJc">Int. J. Environ. Res. Public Health</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">9695</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alanazi, S. A. et al. Public&apos;s mental health monitoring via sentimental analysis of financial text using machine learning techniques. Int. J. Environ. Res. Public Health 19, 9695 (2022).</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_f78yxWy">Application of machine learning methods in mental health detection: A systematic review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abd Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Omar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A M</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S N M</forename><surname>Danuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Garadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mSXVSWz">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="183952" to="183964" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abd Rahman, R., Omar, K., Noah, S. A. M., Danuri, M. S. N. M. &amp; Al-Garadi, M. A. Application of machine learning methods in mental health detection: A systematic review. IEEE Access 8, 183952-183964 (2020).</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_pYdBucT">Detecting suicidality in Arabic tweets using machine learning and deep learning techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alhothali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al-Ghamdi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13369-024-08767-3</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NYAeRrb">Arabian J. Sci. Eng</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abdulsalam, A., Alhothali, A. &amp; Al-Ghamdi, S. Detecting suicidality in Arabic tweets using machine learning and deep learning techniques. Arabian J. Sci. Eng. 1-14 (2024).</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_VFES3ks">Using deep learning to analyze the psychological effects of COVID-19</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Almeqren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Almuqren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alhayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Cristea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2023.962854</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EXCpc3v">Front. Psychol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">962854</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Almeqren, M. A., Almuqren, L., Alhayan, F., Cristea, A. I. &amp; Pennington, D. Using deep learning to analyze the psychological effects of COVID-19. Front. Psychol. 14, 962854 (2023).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_zj9UbdZ">Mental illness classification on social media texts using deep learning and transfer learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ameer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gómez-Adorno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.01012</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Ameer, I., Arif, M., Sidorov, G., Gómez-Adorno, H. &amp; Gelbukh, A. Mental illness classification on social media texts using deep learning and transfer learning. arXiv preprint arXiv:2207.01012 (2022).</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_f3MjgqE">Deep learning in mental health outcome research: A scoping review</title>
		<author>
			<persName><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41398-020-0780-3</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aemx97W">Transl. Psychiatr</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Su, C., Xu, Z., Pathak, J. &amp; Wang, F. Deep learning in mental health outcome research: A scoping review. Transl. Psychiatr 10, 116 (2020).</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_8qgecHz">Enhancing mental health condition detection on social media through multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cAeghR8">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2024" to="2026" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, J. &amp; Su, M. Enhancing mental health condition detection on social media through multi-task learning. medRxiv 2024-02 (2024).</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_gEb4s76">Multi-task learning to detect suicide ideation and mental disorders among social media users</title>
		<author>
			<persName><forename type="first">P</forename><surname>Buddhitha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Inkpen</surname></persName>
		</author>
		<idno type="DOI">10.3389/frma.2023.1152535</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GEWbpZq">Front. Res. Metrics Anal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1152535</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Buddhitha, P. &amp; Inkpen, D. Multi-task learning to detect suicide ideation and mental disorders among social media users. Front. Res. Metrics Anal. 8, 1152535 (2023).</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_sxmVRKw">Predicting depression and anxiety on reddit: a multi-task learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alhamadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alkulaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/asonam55673.2022.10068655</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4Wdhnc8">IEEE/ACM international conference on advances in social networks analysis and mining (ASONAM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sarkar, S., Alhamadani, A., Alkulaib, L. &amp; Lu, C. T. Predicting depression and anxiety on reddit: a multi-task learning approach. In 2022 IEEE/ACM international conference on advances in social networks analysis and mining (ASONAM), 427-435 (IEEE, 2022).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main" xml:id="_DKTwG43">Multi-task learning for depression detection in dialogs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amblard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.10250</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Li, C., Braud, C. &amp; Amblard, M. Multi-task learning for depression detection in dialogs. arXiv preprint arXiv:2208.10250 (2022).</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_Q4tznWq">A multi-task learning approach to hate speech detection leveraging sentiment analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Plaza-Del-Arco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Molina-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ureña-López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Martín-Valdivia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MQStGJ3">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="112478" to="112489" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Plaza-Del-Arco, F. M., Molina-González, M. D., Ureña-López, L. A. &amp; Martín-Valdivia, M. T. A multi-task learning approach to hate speech detection leveraging sentiment analysis. IEEE Access 9, 112478-112489 (2021).</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_cSBm8cK">Multi-task learning model based on multi-scale CNN and LSTM for sentiment classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yawfzYH">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="77060" to="77072" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jin, N., Wu, J., Ma, X., Yan, K. &amp; Mo, Y. Multi-task learning model based on multi-scale CNN and LSTM for sentiment classification. IEEE Access 8, 77060-77072 (2020).</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_tQnC4Gt">Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.2283</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KpDSXCW">PeerJ Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">2283</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Malik, S. et al. Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques. PeerJ Comput. Sci. 10, e2283 (2024).</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_FNSUkee">Sentiment analysis and research based on two-channel parallel hybrid neural network model with attention mechanism</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1049/cth2.12463</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Er7zR9j">IET Control Theory Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2259" to="2267" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen, N., Sun, Y. &amp; Yan, Y. Sentiment analysis and research based on two-channel parallel hybrid neural network model with attention mechanism. IET Control Theory Appl. 17, 2259-2267 (2023).</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_ZKuuJec">A knowledge-enriched attention-based hybrid transformer model for social sentiment analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName><surname>Keaht</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00354-022-00182-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_twgs9hT">N. Gener. Comput</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1165" to="1202" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tiwari, D. &amp; Nagpal, B. Keaht: A knowledge-enriched attention-based hybrid transformer model for social sentiment analysis. N. Gener. Comput. 40, 1165-1202 (2022).</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_zjv8Zt4">Attention-emotion-enhanced convolutional lstm for sentiment analysis</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2021.3056664</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gdsYGRe">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4332" to="4345" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Huang, F. et al. Attention-emotion-enhanced convolutional lstm for sentiment analysis. IEEE Trans. Neural Netw. Learn. Syst. 33, 4332-4345 (2021).</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_CwF72ZP">Opinion mining with deep contextualized embeddings</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kando</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-3006</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RtzTUHc">Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: student research workshop</title>
		<meeting>the 2019 conference of the north american chapter of the association for computational linguistics: student research workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
	<note type="raw_reference">Han, W. &amp; Kando, N. Opinion mining with deep contextualized embeddings. In Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: student research workshop, 35-42 (2019).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Jebbara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06317</idno>
		<title level="m" xml:id="_awXyfTa">Improving opinion-target extraction with character-level word embeddings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jebbara, S. &amp; Cimiano, P. Improving opinion-target extraction with character-level word embeddings. arXiv preprint arXiv:1709.06317 (2017).</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_eQYR4Rf">Fine-grained opinion mining with recurrent neural networks and word embeddings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d15-1168</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YGRyEFC">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1433" to="1443" />
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, P., Joty, S. &amp; Meng, H. Fine-grained opinion mining with recurrent neural networks and word embeddings. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1433-1443 (2015).</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_AaN6zNX">Data quality considerations for big data and machine learning: Going beyond data cleaning and transformations</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gudivada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Apon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cpJZ8zg">Int. J. Adv. Softw</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gudivada, V., Apon, A. &amp; Ding, J. Data quality considerations for big data and machine learning: Going beyond data cleaning and transformations. Int. J. Adv. Softw. 10, 1-20 (2017).</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_pUhxxxe">Lemmatization for ancient greek: An experimental assessment of the state of the art</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vatri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgillivray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DEUBmG5">J. Greek Linguistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="179" to="196" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vatri, A. &amp; McGillivray, B. Lemmatization for ancient greek: An experimental assessment of the state of the art. J. Greek Linguistics 20, 179-196 (2020).</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_Mz5UZu3">Intelligent analysis of multimedia healthcare data using natural language processing and deep-learning techniques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bondugula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Udgata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sivangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fRH8qNQ">Edge-of-things in personalized healthcare support systems</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="335" to="358" />
		</imprint>
	</monogr>
	<note type="raw_reference">Bondugula, R., Udgata, S., Rahman, N. &amp; Sivangi, K. Intelligent analysis of multimedia healthcare data using natural language processing and deep-learning techniques. In Edge-of-things in personalized healthcare support systems, 335-358 (Academic Press, 2022).</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_wXcZFDQ">Reversible natural language watermarking using synonym substitution and arithmetic coding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.3970/cmc.2018.03510</idno>
		<ptr target="https://doi.org/10.3970/cmc.2018.03510" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_smxHsX5">Comput. Mater. Continua</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="541" to="559" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiang, L., Li, Y., Hao, W., Yang, P. &amp; Shen, X. Reversible natural language watermarking using synonym substitution and arithmetic coding. Comput. Mater. Continua 55, 541-559. https://doi.org/10.3970/cmc.2018.03510 (2018).</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main" xml:id="_6KdGT9z">Lexsubcon: Integrating knowledge from lexical resources into contextual embeddings for lexical substitution</title>
		<author>
			<persName><forename type="first">G</forename><surname>Michalopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mckillop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.87</idno>
		<idno type="arXiv">arXiv:2107.05132</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Michalopoulos, G., McKillop, I., Wong, A. &amp; Chen, H. Lexsubcon: Integrating knowledge from lexical resources into contextual embeddings for lexical substitution. arXiv preprint arXiv:2107.05132 (2021).</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_wgEShNW">Using wordnet synonym substitution to enhance UMLS source integration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Perl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2008.11.008</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rbVR3ja">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Huang, K., Geller, J., Halper, M., Perl, Y. &amp; Xu, J. Using wordnet synonym substitution to enhance UMLS source integration. Artif. Intell. Med. 46, 97-109 (2009).</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_GdQs43w">Sentiment analysis about product and service evaluation of pt telekomunikasi indonesia tbk from tweets using textblob, naive bayes &amp; k-nn method</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hermansyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarno</surname></persName>
		</author>
		<idno type="DOI">10.1109/isemantic50169.2020.9234238</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EWFfeXP">international seminar on application for technology of information and communication (iSemantic)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="511" to="516" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hermansyah, R. &amp; Sarno, R. Sentiment analysis about product and service evaluation of pt telekomunikasi indonesia tbk from tweets using textblob, naive bayes &amp; k-nn method. In 2020 international seminar on application for technology of information and communication (iSemantic), 511-516 (IEEE, 2020).</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_dJRSXdD">Aspect based opinion mining of online reviews</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Chachoo</surname></persName>
		</author>
		<idno type="DOI">10.37896/gor33.03/500</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9V338mu">Gedrag Organisatie Rev</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1185" to="1199" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hilal, A. &amp; Chachoo, M. A. Aspect based opinion mining of online reviews. Gedrag Organisatie Rev. 33, 1185-1199 (2020).</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main" xml:id="_AgdSCsz">Text classification using label names only: A language model self-training approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07245</idno>
		<idno>arXiv:2010.07245</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Meng, Y. et al. Text classification using label names only: A language model self-training approach. arXiv preprint arXiv:2010.07245 (2020). arXiv:2010.07245.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_cMqcbMh">Predicting mental illness (depression) with the help of nlp transformers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kokane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abhyankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shrirao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khadkikar</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdsis61070.2024.10594036</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jUtztKR">2024 second international conference on data science and information system (ICDSIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kokane, V., Abhyankar, A., Shrirao, N. &amp; Khadkikar, P. Predicting mental illness (depression) with the help of nlp transformers. In 2024 second international conference on data science and information system (ICDSIS), 1-5 (IEEE, 2024).</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_Y7KpnG5">Sentiment analysis and research based on two-channel parallel hybrid neural network model with attention mechanism</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1049/cth2.12463</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fuTBzvs">IET Control Theory Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2259" to="2267" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen, N., Sun, Y. &amp; Yan, Y. Sentiment analysis and research based on two-channel parallel hybrid neural network model with attention mechanism. IET Control Theory Appl. 17, 2259-2267 (2023).</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_jDGANyf">Enhancing conversational sentimental analysis for psychological depression prediction with BI-LSTM</title>
		<author>
			<persName><forename type="first">G</forename><surname>Selva Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_T7UTeDC">J. Autonomous Intell</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Selva Mary, G. et al. Enhancing conversational sentimental analysis for psychological depression prediction with BI-LSTM. J. Autonomous Intell., 7 (2023).</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_3ZWTART">Leveraging multi-class sentiment analysis on social media text for detecting signs of depression</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sowbarnigaa</surname></persName>
		</author>
		<idno type="DOI">10.54254/2755-2721/2/20220660</idno>
		<ptr target="https://doi.org/10.54254/2755-2721/2/20220660" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_2GMzPxf">Appl. Comput. Eng</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1020" to="1029" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sowbarnigaa, K. S. et al. Leveraging multi-class sentiment analysis on social media text for detecting signs of depression. Appl. Comput. Eng. 2, 1020-1029. https://doi.org/10.54254/2755-2721/2/20220660 (2023).</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main" xml:id="_44653WT">Emoment: An emotion annotated mental health corpus from two south asian countries</title>
		<author>
			<persName><forename type="first">T</forename><surname>Atapattu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.08486</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Atapattu, T. et al. Emoment: An emotion annotated mental health corpus from two south asian countries. arXiv preprint arXiv:2208.08486 (2022).</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_xaAGhqq">Transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-60457-8_45</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cRrGjQB">Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC 2020</title>
		<title level="s" xml:id="_ru35JBy">Proceedings, Part II</title>
		<meeting><address><addrLine>Zhengzhou, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">October 14-18, 2020. 2020</date>
			<biblScope unit="page" from="546" to="557" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, Z., Ying, C., Dai, X., Huang, S. &amp; Chen, J. Transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis. In Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14-18, 2020, Proceedings, Part II, 546-557 (Springer International Publishing, 2020).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
