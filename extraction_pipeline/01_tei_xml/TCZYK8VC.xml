<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_dKV8wuA">Personalized HeartSteps: A Reinforcement Learning Algorithm for Optimizing Physical Activity</title>
				<funder ref="#_f2gysZC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-09-14">2021 September 14.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peng</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan IBM Research University of Michigan Harvard University</note>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kristjan</forename><surname>Greenewald</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan IBM Research University of Michigan Harvard University</note>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Predrag</forename><surname>Klasnja</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan IBM Research University of Michigan Harvard University</note>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><surname>Murphy</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan IBM Research University of Michigan Harvard University</note>
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
								<orgName type="institution" key="instit4">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_gZmJFDC">Personalized HeartSteps: A Reinforcement Learning Algorithm for Optimizing Physical Activity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-14">2021 September 14.</date>
						</imprint>
					</monogr>
					<idno type="MD5">009544770724626A14837E7456CBBDBE</idno>
					<idno type="DOI">10.1145/3381007</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_gZSrnNm">Computing methodologies → Machine learning algorithms</term>
					<term xml:id="_pgRXNRA">Applied computing → Health care information systems</term>
					<term xml:id="_TZQeXEU">Mobile Health</term>
					<term xml:id="_2USxVSh">Just-in-Time Adaptive Intervention</term>
					<term xml:id="_jsFX3HN">Reinforcement Learning HHS Public Access</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_zQsMQNW"><p xml:id="_dTUDSgf"><s xml:id="_rUUSAFn">With the recent proliferation of mobile health technologies, health scientists are increasingly interested in developing just-in-time adaptive interventions (JITAIs), typically delivered via notifications on mobile devices and designed to help users prevent negative health outcomes and to promote the adoption and maintenance of healthy behaviors.</s><s xml:id="_HyGdHVf">A JITAI involves a sequence of decision rules (i.e., treatment policies) that take the user's current context as input and specify whether and what type of intervention should be provided at the moment.</s><s xml:id="_HrHPqP4">In this work, we describe a reinforcement learning (RL) algorithm that continuously learns and improves the treatment policy embedded in the JITAI as data is being collected from the user.</s><s xml:id="_fRz3DqU">This work is motivated by our collaboration on designing an RL algorithm for HeartSteps V2 based on data collected HeartSteps V1.</s><s xml:id="_EDrCF7t">HeartSteps is a physical activity mobile health application.</s><s xml:id="_YdGKm8Z">The RL algorithm developed in this work is being used in HeartSteps V2 to decide, five times per day, whether to deliver a context-tailored activity suggestion.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_Ca4CxuV">INTRODUCTION</head><p xml:id="_QpcggnR"><s xml:id="_RPMsW34">With the recent proliferation of mobile health technologies, health scientists are increasingly interested in delivering interventions via notifications on mobile devices at the moments when they are most effective in helping the user prevent negative health outcomes and adopt pengliao@umich.edu .</s><s xml:id="_bqVWxAa">Authors' addresses: Peng Liao, University of Michigan, Ann Arbor, MI; Kristjan Greenewald, IBM Research, Cambridge, MA; Predrag Klasnja, University of Michigan, Ann Arbor, MI; Susan Murphy, Harvard University, Cambridge, MA.</s></p><p xml:id="_bacm9ZK"><s xml:id="_Ba4hdBp">and maintain healthy behaviors.</s><s xml:id="_QwygSUJ">The type and timing of the mobile health interventions should ideally adapt to the real-time information collected about the user's context, e.g., the time of the day, location, current activity, and stress level.</s><s xml:id="_gE4yD2E">This type of intervention is called just-in-time adaptive intervention (JITAI) <ref type="bibr" target="#b27">[28]</ref>.</s><s xml:id="_aEPxfqH">Operationally, a JITAI includes a sequence of decision rules (i.e., treatment policies) that take the user's current context as input and specify whether and/or what type of intervention should be provided at the moment.</s><s xml:id="_4bd5kmU">Behavioral theory supplemented with expert opinion and analyses of existing data is often used to design these decision rules.</s><s xml:id="_YkZXFPT">However, these behavioral theories are often insufficiently mature to precisely specify which particular intervention should be delivered and when in order to ensure the interventions have the intended effects and optimize the long-term efficacy of the interventions.</s><s xml:id="_wjuWysv">As a result, there is much interest in how to best use data to inform the design of JITAIs <ref type="bibr">[3, 10, 12, 26, 33-35, 39, 41, 42]</ref>.</s></p><p xml:id="_gKFEcVs"><s xml:id="_77a6fNm">In this work, we describe a reinforcement learning (RL) algorithm to continuously learn and optimize the treatment policy in the JITAI as the user experiences the interventions.</s><s xml:id="_Jcqgux6">This work is motivated by our collaboration on the design of the HeartSteps V2 clinical trial for individuals who have stage 1 hypertension.</s><s xml:id="_dqYM8Tc">As the clinical trial progresses, the RL algorithm learns whether to deliver a context-tailored physical activity suggestion at each decision time.</s></p><p xml:id="_ZFjE3Qy"><s xml:id="_xXKxWGX">The remainder of the article is organized as follows.</s><s xml:id="_WeHrmkV">We first describe HeartSteps, including HeartSteps V1 and HeartSteps V2, which is in progress at the time of writing.</s><s xml:id="_MSSHCa7">We then briefly introduce RL and identify key challenges in applying RL to optimize JITAI treatment policies in mobile health.</s><s xml:id="_JSP2rtf">Existing mobile health studies that have used RL are reviewed, as well as related RL algorithms.</s><s xml:id="_2HyxU2H">We then describe the HeartSteps V2 RL algorithm and the implementation and evaluation of this algorithm using a generative model built from HeartSteps V1 data.</s><s xml:id="_yKspgZJ">We discuss the performance of our algorithm based on the initial pilot data from HeartSteps V2.</s><s xml:id="_duufmp9">We close with a discussion of future work.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_A9YdqzH">HEARTSTEPS V1 AND V2: PHYSICAL ACTIVITY MOBILE HEALTH STUDY</head><p xml:id="_NGUWTtw"><s xml:id="_5EFPFAN">HeartSteps V2 is a 90-day physical activity clinical trial for improving the physical activity of individuals with blood pressure in the stage 1 hypertension range (120-130 systolic).</s><s xml:id="_hQABgGB">In this trial, participants are provided a Fitbit tracker and a mobile phone application designed to help them improve their physical activity.</s><s xml:id="_Zc5MNuS">The participant first wears the Fitbit tracker for one week and then installs the mobile app at the beginning of the second week.</s><s xml:id="_7GepRSm">One of the intervention components is a contextually-tailored physical activity suggestion that may be delivered at any of the five user-specified times during each day.</s><s xml:id="_dAtcDMg">These five times are roughly separated by 2.5 hours, corresponding to the user's morning commute, mid-day, mid-afternoon, evening commute, and post-dinner times.</s><s xml:id="_uaAtZyJ">The content of the suggestion is designed to encourage activity in the current context and thus the suggestions are intended to impact near-time physical activity.</s><s xml:id="_AFmKDM8">The RL algorithm in this work is being used to decide at each time whether or not to send the activity suggestion as well as to optimize these decisions.</s><s xml:id="_x4g2sAv">An illustration of the study design and how the RL algorithm is used is described in Figure <ref type="figure" target="#fig_3">1</ref>.</s><s xml:id="_ZKyXxTc">Currently, HeartSteps V2 is being deployed in the field.</s><s xml:id="_NXnzEgn">We will provide an initial assessment of the proposed algorithm in Section 8.</s></p><p xml:id="_tugkeUw"><s xml:id="_mdHaWgU">In order to design HeartSteps V2, our team conducted HeartSteps V1, which was a 42-day physical activity mobile health study <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>.</s><s xml:id="_PTxCxYX">HeartSteps V1 has many similarities to HeartSteps V2 in terms of the intervention components but differs in the study population and the method of delivering interventions.</s><s xml:id="_NpAaFaa">The population in HeartSteps V1 is healthy sedentary adults.</s><s xml:id="_DYC4Bx4">For the walking suggestion intervention component in HeartSteps V1, whether to provide a contextually-tailored walking suggestion message was randomized at each of the five user-specified times per day with a constant probability of 0.30 whenever the participant is available.</s><s xml:id="_EQjMspU">The HeartSteps V1 walking suggestions have the same content as those of HeartSteps V2.</s><s xml:id="_c7stsGc">While the HeartSteps V1 anti-sedentary message was randomized with probability 0.3 at the same five decision times, the anti-sedentary messages in HeartSteps V2 are delivered only when the participant has been sedentary during the past 40 minutes, with the randomization probability being adjusted on the fly to meet the average constraint on the number of anti-sedentary messages sent per day <ref type="bibr" target="#b23">[24]</ref>.</s><s xml:id="_QTyM83R">We used the data collected from HeartSteps V1 to inform the design of the RL algorithm for HeartSteps V2 (e.g., selecting the variables that are predictive of step counts and the efficacy of walking suggestion messages as well as forming a prior distribution) and to create a simulation environment (i.e., the generative model) in order to choose certain tuning parameters and evaluate the proposed RL algorithm (see Section 6 and 7 for details).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_ESK9kaB">CHALLENGES TO APPLYING RL IN MHEALTH</head><p xml:id="_pUnDGuJ"><s xml:id="_eMUXd3u">Reinforcement learning (RL) is an area of machine learning in which an algorithm learns how to act optimally by continuously interacting with an unknown environment <ref type="bibr" target="#b37">[38]</ref>.</s><s xml:id="_x4rUJMy">The algorithm inputs the current state, selects the next action, and receives the reward, with the goal of learning the best sequence of actions (i.e., the policy) to maximize the total reward.</s><s xml:id="_guSB3S2">For example, in the case of HeartSteps, the state is a set of features of the user's current and past context, the actions are whether to deliver an activity suggestion or not, and the reward is a function of near-time physical activity.</s><s xml:id="_Rrd4NYE">A fundamental challenge in RL is the tradeoff between exploitation (e.g., selecting the action that seems the best given data observed so far) and exploration (to gather information to learn the best action, for example).</s><s xml:id="_gmqWT5D">RL has seen rapid development in recent years and shown remarkable success across many fields, such as video games, chess-playing, and robotic control.</s><s xml:id="_3TWrd8k">However, many challenges remain that need to be carefully addressed before RL can be usefully deployed to adapt and optimize mobile health interventions.</s><s xml:id="_4KRP3vB">Below we discuss some of these challenges.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cDS6nVU">(C1)</head><p xml:id="_DTAC4DC"><s xml:id="_2WFrhy4">The RL algorithm must adjust for the longer-term effects of current action.</s></p><p xml:id="_KvwrCeY"><s xml:id="_YfXXNrw">In mobile health, interventions often tend to have a positive effect on the immediate reward, but can have a negative impact on future rewards due to a user's habituation and/or burden <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17]</ref>.</s><s xml:id="_kj3CEtk">Thus, the optimal treatment can only be identified by taking into account the impact of current action on rewards farther into the future.</s><s xml:id="_wsB2FKq">This is akin to using a large discount rate (i.e., a long planning horizon) in RL.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gfZahB7">(C2)</head><p xml:id="_Wwh7Gmw"><s xml:id="_2XdYmKB">The RL algorithm should learn quickly and accommodate noisy data.</s><s xml:id="_GqWYMdd">Most online RL algorithms require the agent to interact many times with the environment prior to performing well.</s><s xml:id="_AcJpSc7">This is impractical in mobile health applications, as users can lose interest and disengage quickly.</s><s xml:id="_HHydNxW">Furthermore, because mobile health interventions are provided in uncontrolled environments both context information and rewards can be very noisy.</s><s xml:id="_cm3g8PB">For example, step count data collected from a wrist band is noisy due to a variety of confounders, such as incidental hand movements.</s><s xml:id="_kDfxJWz">Additionally, the sensors do not detect the user's entire context; non-sensed aspects of the current context act as sources of variance.</s><s xml:id="_HqJgwMg">Such a high noise setting typically requires even more interactions with the environment to select the optimal action.</s><s xml:id="_4VCBCaW">Additionally, while consideration of challenge (C1) motivates a long planning horizon, it has been shown that, in both practice and theory, a discount rate close to 1 often leads to high variance and slow learning rates <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>.</s><s xml:id="_DWNYArR">We need, then, to trade off carefully between bias and variance when designing the RL algorithm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BtaD43P">(C3)</head><p xml:id="_ua7Bjsb"><s xml:id="_x7rnwsj">The RL algorithm should accommodate some model mis-specification and non stationarity.</s><s xml:id="_NwWZtS8">Since the context space is complex and some aspects of the contexts are not observed (e.g., engagement and burden), the mapping from context to reward is likely to exhibit non-stationarity over a longer period of time.</s><s xml:id="_AKBHKCW">Indeed, the analysis of HeartSteps V1 provides evidence of non-stationarity: there is strong evidence that the treatment effect of sending an activity suggestion on subsequent activity decreases over the time the user is in the study <ref type="bibr" target="#b18">[19]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rwHyCA7">(C4)</head><p xml:id="_h86rHwN"><s xml:id="_GbN5RJb">The RL algorithm should select actions in a way such that after the study is over, secondary data analyses are feasible.</s><s xml:id="_WuKJG28">This is particularly the case for experimental trials involving clinical populations.</s><s xml:id="_JxcgKux">In these settings, an interdisciplinary team is required to design the intervention and to conduct the clinical trial.</s><s xml:id="_JhzmF5A">As a result, multiple stakeholders will want to analyze the resulting data in a large variety of ways.</s><s xml:id="_rQTfXfs">Thus, for example, off-policy learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref> and causal inference <ref type="bibr" target="#b3">[4]</ref> as well as other more standard statistical analyses must be feasible after the study ends.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_KGKr9Gx">EXISTING RL-BASED MOBILE HEALTH STUDIES</head><p xml:id="_t2VQ6Gb"><s xml:id="_R4Z4abA">There are few existing mobile health studies in which RL methods are applied to adapt the individual's intervention in real time.</s><s xml:id="_myEwFsn">Here we focus on the setting where the treatment policy is not pre-specified, but instead continuously learned and improved as more data is collected.</s></p><p xml:id="_vKCmq5B"><s xml:id="_wmtqxuv">In <ref type="bibr" target="#b40">[41]</ref>, an RL system was deployed to choose the different types of daily suggestions to encourage physical activity in patients with diabetes in a 26-week study.</s><s xml:id="_gmE3qqH">The authors use a contextual bandit learning algorithm combined with a Softmax approach to select the actions (daily suggestions) to maximize increased minutes of activity.</s><s xml:id="_nBREm5z">Paredes et al. <ref type="bibr" target="#b32">[33]</ref> employed a contextual bandit learning algorithm combined with an Upper Confidence Bound approach to select the best among 10 types of stress management strategies when the participant requests an intervention in the mobile app.</s><s xml:id="_XQK4Q3M">In <ref type="bibr" target="#b9">[10]</ref>, the authors reported a recent weight loss study in which one of three types of interventions is chosen twice a week over 12 weeks.</s><s xml:id="_mNeYna7">Their RL system featured an explicit separation of exploration and exploitation: 10 decision times are predetermined for exploration (i.e., randomly selecting the intervention at each decision time) and the remaining 14 decision times are predetermined for exploitation (i.e., choosing the best intervention to maximize the expected reward based on the history).</s></p><p xml:id="_dmQfQJJ"><s xml:id="_VPV9s9X">MyBehavior <ref type="bibr" target="#b33">[34]</ref>, a smartphone app that delivered personalized interventions to promote physical activity and dietary health, used EXP3, a multi-arm, context-free bandit algorithm to select the interventions.</s><s xml:id="_SZQUDfq">While the RL methods in the aforementioned studies aim to select actions so as to optimize the immediate reward, in a recent physical study reported in <ref type="bibr" target="#b41">[42]</ref>,</s></p><p xml:id="_gDzSJmm"><s xml:id="_CjbhM5V">the RL system at the end of every week used the participant's historical daily step count data to estimate a dynamical model for the daily step count and used that model to infer the optimal daily step goals for the next 7 days, with the goal of maximizing the minimal step counts taken in the next week.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_WeyySZK">Existing RL Algorithms' Insufficiency to Address Challenges</head><p xml:id="_qG5KNyw"><s xml:id="_hDPAdxD">We argue that the above-mentioned RL algorithms are insufficient to address the challenges listed in Section 3 and thus we must generalize these algorithms in several directions.</s><s xml:id="_AUyvyzq">First, these studies use only a pure data collection phase to initialize the RL algorithms.</s><s xml:id="_EHMWHYq">But often there are additional data from other sources, such as a pilot study or prior expert knowledge.</s><s xml:id="_EEb5xgb">Challenge (C2) means that it is critical to incorporate such available information to speed up the learning in the early phase of the study.</s><s xml:id="_FJr98UW">Second, the RL algorithms in these studies require knowledge of the correct model for the reward function, a requirement that is likely unrealistic due to the dimensionality and complexity of the context space and the potential non-stationarity noted in challenge (C3).</s><s xml:id="_jusxKN7">It has been empirically shown that the performance of standard RL algorithms is quite sensitive to the model for the reward function <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>.</s><s xml:id="_7YCUntK">Third, among the above-mentioned studies, only the algorithm used in <ref type="bibr" target="#b41">[42]</ref> attempts to optimize rewards over a time period longer than the immediate time step.</s><s xml:id="_5k37FDC">It turns out that there is a bias-variance trade-off when designing how long into the future the RL should attempt to optimize rewards.</s><s xml:id="_2YkwrF9">That is, only focusing on maximizing the immediate rewards speeds the learning rate (e.g., due to lower estimation variance) compared with a full RL algorithm that attempts to maximize over a longer time horizon.</s><s xml:id="_5Un7vV9">However, an RL algorithm focused on optimizing the immediate reward might end up sending too many treatments due to challenge (C1), i.e., the treatment tends to have a positive effect on immediate reward and negative effects on future rewards.</s><s xml:id="_mb2BfUK">Such an algorithm is likely to have a poorer overall performance than an algorithm that attempts to optimize over a longer time horizon to account for treatment burden and disengagement.</s><s xml:id="_nJ6HhHz">Fourth, both <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b41">[42]</ref> use algorithms that select the action deterministically based on the history, and <ref type="bibr" target="#b9">[10]</ref> incorporates a pure exploitation phase.</s><s xml:id="_gjDZcSW">It is known that action selection probabilities close to 0 or 1 cause instability (i.e., high variance) in batch data analysis in challenge (C4) that uses importance weights, e.g., in off-policy evaluation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_kqXA2SW">PERSONALIZED HEARTSTEPS: ONLINE RL ALGORITHM</head><p xml:id="_mvhTyAU"><s xml:id="_et7u47C">In this section, we discuss the design of the RL algorithm in HeartSteps V2.</s><s xml:id="_bUPmkQp">Recall that this algorithm determines whether to send the activity suggestion at each decision time (see Figure <ref type="figure" target="#fig_3">1</ref>).</s><s xml:id="_eGuKpyA">We first give an overview of how the proposed algorithm operates.</s><s xml:id="_Bm9nenx">We then describe each component in our setting, i.e., the decision times, action, states, and reward, and formally introduce our proposed RL algorithm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_pVAvW2X">Algorithm Overview</head><p xml:id="_R7xR6Gu"><s xml:id="_E5y6XnD">Below we provide an overview of how our proposed RL algorithm addresses the challenges listed in Section 3 that were not sufficiently addressed by existing RL algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1" xml:id="_Hybmym8">Addressing Challenge (C1</head><p xml:id="_uwrFJcx"><s xml:id="_ZsanDHz">).-We introduce a "dosage" variable based on the history of past treatments.</s><s xml:id="_fhJNYHs">This is motivated by the analyses of HeartSteps V1 in which contexts with a larger recent dosage appears to result in the smaller immediate effect of treatment and lower future rewards.</s><s xml:id="_jVAe4hz">A similar "dosage" variable was explored in a recent unpublished manuscript <ref type="bibr" target="#b26">[27]</ref> whose authors developed a bandit algorithm, called ROGUE (Reducing or Gaining Unknown Efficacy) Bandits.</s><s xml:id="_CYxr2qV">They use the "dosage" idea to accommodate settings in which an (unknown) dosage variable causes non-stationarity in the reward function.</s><s xml:id="_8pNEk8b">Our use of dosage, on the other hand, is to form a proxy of the future rewards, mimicking a full RL setting (as opposed to the bandit setting) while still managing variance in consideration of challenge (C2).</s><s xml:id="_TdCWD9J">We construct a proxy of the future rewards (proxy value) under a low dimensional proxy MDP model.</s><s xml:id="_BVvGJen">Model-based RL is well studied in the RL literature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>.</s><s xml:id="_7s5ZJsV">In these papers, the algorithm uses a model for the transition function from current state and action to the next state.</s><s xml:id="_s4DTdPw">Instead, our algorithm only uses the MDP model to provide a low variance proxy to adjust for the longer-term impact of actions on future rewards.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2" xml:id="_r8R5rUE">Addressing Challenge (C2</head><p xml:id="_7eKev2h"><s xml:id="_2MTQc3R">).-We propose using a low-dimensional linear model to model the differences in the reward function under alternate actions and using Thompson Sampling (TS), a general algorithmic idea that uses a Bayesian paradigm to trade off between exploration and exploitation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>.</s><s xml:id="_HXv4T6h">A relatively low-dimensional model is chosen to trade off the bias and variance to accelerate learning.</s><s xml:id="_EypVYc3">The use of TS allows us to incorporate prior knowledge in the algorithm through the use of a prior distribution on the parameters in the reward model.</s><s xml:id="_e2FktJN">We propose using an informative prior distribution to speed up the learning in the early phase of the study as well as to reduce the variance and diminish the impact of noisy observations.</s><s xml:id="_36KdwFc">Note that TS-based algorithms have been shown to enjoy not only strong theoretical performance guarantees but also strong empirical performance in many problems in comparison to other state-of-the-art methods, such as Upper Confidence Bound <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3" xml:id="_KwkBgwZ">Addressing Challenge (C3</head><p xml:id="_arJAVuU"><s xml:id="_D3KJCRv">).-To deal with challenge (C3), we use the concept of action centering in modeling the reward.</s><s xml:id="_kskN7kU">The motivation is to protect the RL algorithm from a mis-specified model for the "baseline" reward function (e.g., in HeartSteps example with binary actions, the baseline reward function is the expected number of steps taken in the next 30 minutes given the current state and no activity suggestion).</s><s xml:id="_wqUjC8p">The idea of action centering in RL was first explored in <ref type="bibr" target="#b12">[13]</ref> and recently improved in <ref type="bibr" target="#b19">[20]</ref>.</s><s xml:id="_c2MYfTn">In both works, the RL algorithm is theoretically guaranteed to learn the optimal action without any assumption about the baseline reward generating process (e.g., the baseline reward function can be non-stationary).</s><s xml:id="_wy5drZK">However, neither of these methods attempts to reduce the noise in the reward.</s><s xml:id="_bxhzGu4">We generalize action centering for use in higher variance, non-stationary reward settings.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4" xml:id="_DWzaMZZ">Addressing Challenge (C4</head><p xml:id="_4qTPYfN"><s xml:id="_SPPcyaU">).-Lastly, in consideration of challenge (C4), the actions in our proposed RL algorithm are selected stochastically via TS (i.e., each action is randomized with known probability) and furthermore we restrict the randomization probabilities away from 0 and 1 to ensure that secondary analyses can be conducted when the study is over.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_pcqfNAb">RL Framework</head><p xml:id="_s4BDRRU"><s xml:id="_dtuB5fC">Let the participant's longitudinal data recorded via mobile device be the sequence</s></p><formula xml:id="formula_0">{S 1 , A 1 , R 1 , S 2 , A 2 , R 2 , …, S t , A t , R t , ⋯}</formula><p xml:id="_GX6MXVg"><s xml:id="_3twvt2S">Decision time.-We</s><s xml:id="_nRrKGXb">use t to index decision time.</s><s xml:id="_D6VzJzN">In HeartSteps V2, there are five decision times each day over the 90 days of the study (i.e., t = 1, . .</s><s xml:id="_vbRsEMX">., 450).</s><s xml:id="_Aa4dxPM">Where convenient we also use (l, d) to refer to the l-th decision time on study day d.</s><s xml:id="_YWtuQ7q">For example, (l, d) = (5, 3) refers to the fifth time in day 3, which corresponds to the decision time t = 5(d -1) + l = 15.</s></p><p xml:id="_ZTuWA8C"><s xml:id="_SeFChNU">Action.-A t ∈ A is the action or treatment at time t.</s><s xml:id="_6fmB3AJ">In this work, we assume binary treatment, i.e., the action space A = {0, 1}), where A t = 1 if an activity suggestion is delivered and A t = 0 otherwise.</s></p><p xml:id="_4cyH6Vb"><s xml:id="_egpenYn">Reward.-R t is the (immediate) reward collected after the action A t is selected.</s><s xml:id="_qQGxDHd">Typically, the reward is defined to capture the proximal impact of the actions.</s><s xml:id="_ZAN9Cum">Recall that mobile health interventions are often designed to have a near-term impact on health outcomes.</s><s xml:id="_cZr7FtE">In HeartSteps, the reward is based on the step count collected 30 minutes after the decision time.</s><s xml:id="_3FD9WXv">Note that the raw step counts can be highly noisy and positively skewed <ref type="bibr" target="#b18">[19]</ref>.</s></p><p xml:id="_EJJBDwS"><s xml:id="_83XFd8u">The reward used in the RL algorithm is the log-transformed step count where the log transformation is to make the reward distribution more symmetric and less heavy-tailed; see how this log transformation is related to the modeling assumption in 5.4.</s></p><p xml:id="_7AJq7af"><s xml:id="_BgMs4Pj">States.-S t is the state vector at decision time t, which is decomposed into S t = {I t , Z t , X t }.</s><s xml:id="_7PuUnr5">I t is used to indicate times at which only A t = 0 is feasible and/or ethical.</s><s xml:id="_v4kp5tu">For example, if sensors indicate that the participant may be driving a car, then the suggestion should not be sent; that is, the participant is unavailable for treatment (I t = 0).</s><s xml:id="_jSXr46k">Z t denotes features used to represent the current context at time t.</s><s xml:id="_uYuvUpu">In HeartSteps, these features include current location, the prior 30-minute step count, yesterday's daily step count, the current temperature, and measures of how active the participant has been around the current decision time over the last week.</s><s xml:id="_tfQMxFT">Lastly, X t ∈ X is the "dosage" variable that captures our proxy for the treatment burden, which is a function of the participant's treatment history.</s><s xml:id="_2ETwJH2">In contrast to HeartSteps V1, in HeartSteps V2, an additional intervention component, i.e., an anti-sedentary suggestion, will sometimes be delivered when the participant is sedentary.</s><s xml:id="_EdrUsND">As the anti-sedentary suggestion can also cause burden, it is included in defining the dosage variable.</s><s xml:id="_xYRVf78">Specifically, denote by E t the event that an walking suggestion is sent at decision time t -1 (e.g., A t-1 = 0) and any anti-sedentary suggestion is sent between time t -1 and t.</s><s xml:id="_vfedtSG">The dosage at the moment is constructed by first multiplying the previous dosage variable by a discount rate λ ∈ (0, 1) and incrementing it by 1 if any suggestions were sent to the user since the last decision time.</s><s xml:id="_5UGdxbb">Specifically, starting with the initial value X 1 = 0, the dosage at time t + 1 is defined as</s></p><formula xml:id="formula_1">X t + 1 = λX t + 1 E t + 1 .</formula><p xml:id="_CcEKRv9"><s xml:id="_K72Uvjh">Based on the data analysis result from HeartSteps V1, we choose λ = 0.95; see Section 6 for how this value is selected.</s><s xml:id="_sGG98e6">As we will see in the next two sections, this simple form of dosage variable is used to capture the treatment burden and forecast the delayed impact of sending the walking suggestion.</s><s xml:id="_TgFu59n">See Section 9 for a discussion of other choices.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_GzEKUxu">Action Selection</head><p xml:id="_Da787JU"><s xml:id="_SyfED7C">At each decision time t = (l, d), the RL algorithm selects the action based on each participant's current history (past states, actions and rewards), with the goal of optimizing the total rewards during the process.</s><s xml:id="_NPtBBtk">The proposed algorithm is stochastic, that is, the algorithm will output a probability π l, d for sending the walking suggestion message (A l, d is sampled from a Bernoulli distribution with probability π l, d ).</s><s xml:id="_JEd4uKs">Note that, at the beginning of study (d = 1), both the distribution (μ 1 , Σ 1 ) and the proxy of delayed effect η 1 are set based on HeartSteps V1; see details in Section 6.</s><s xml:id="_DM6kuQ9">Without loss of generality, we implicitly assume throughout that the probability π l, d is part of the state S l, d .</s><s xml:id="_mEDB8JP">The pseudo code of the proposed HeartSteps V2 RL algorithm is provided in Algorithm 1.</s></p><p xml:id="_pZJVxwF"><s xml:id="_QbDbMEg">The reward function is denoted as</s></p><formula xml:id="formula_2">r t (s, a) = E R t | S t = s, A t = a, I t = 1 .</formula><p xml:id="_mmMqR8z"><s xml:id="_JW3PrP5">The action selection is formed on the basis of a low dimensional linear model (to address challenge (C2)) for the treatment effect:</s></p><formula xml:id="formula_3">r t (s, 1) -r t (s, 0) = f(s) ⊤ β (1)</formula><p xml:id="_mADWFTc"><s xml:id="_vCHumwy">where the feature vector, f(s), is selected based on the domain science as well as on analyses of HeartSteps V1 data; see Section 6 for a discussion of how the features are selected.</s><s xml:id="_JdQsava">At the l-th decision time on day d, availability is ascertained (I l, d = 1) .</s><s xml:id="_GrSGQPB">Then for S l, d = s with the dosage variable X l, d = x, the action, A l, d = 1 is selected based on</s></p><formula xml:id="formula_4">Pr{f(s) ⊤ β &gt; η d (x); β N(μ d , Σ d )}</formula><p xml:id="_hAy5duw"><s xml:id="_kDpZSAt">where the random variable β follows the Gaussian distribution N(μ d , Σ d ), which is the posterior distribution of the parameters obtained at the end of the previous day.</s><s xml:id="_uwCWHnW">The term η d (x) proxies the negative long-term effect of delivering the activity suggestion at the moment given the current dosage level X l, d = x (see the detailed formulation of ηd in Section 5.4.2).</s><s xml:id="_ax4r9Gn">Note that when η d (x) = 0, we recover the bandit formulation, i.e., the action is selected to maximize the immediate rewards ignoring any impact on future rewards.</s><s xml:id="_hPXsy6z">The probability π l, d of sending an activity suggestion given</s></p><formula xml:id="formula_5">I l, d = 1, S l, d = s, X l, d = x is clipped, i.e., π l, d = ϕ(Pr{f(s) ⊤ β &gt; η d (x); β N(μ d , Σ d )}) .<label>(2)</label></formula><p xml:id="_UErcxQa"><s xml:id="_cWnmJxB">The clipping function is ϕ(π) = min(1 -ϵ 0 , max(π, ϵ 1 )) ∈ [ϵ 1 , 1 -ϵ 0 ] .</s><s xml:id="_KPDXAmB">This restricts the randomization probability of sending nothing and of sending an activity suggestion to be at least ϵ 0 and ϵ 1 , respectively.</s><s xml:id="_MjuEQrX">The probability clipping enables off-policy data analyses after the study is over (challenge (C4)).</s><s xml:id="_nhj82e9">This clipping also ensures that</s></p><p xml:id="_FBCD5yU"><s xml:id="_VGKjbXE">ALGORITHM 1:</s></p><p xml:id="_M5TJRY2"><s xml:id="_G4ZPtka">HeartSteps V2 RL Algorithm the RL algorithm will continue to explore and learn, instead of locking itself into a particular policy (challenge (C3)); see the discussion of ϵ 0 , ϵ 1 in Section 6.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" xml:id="_HEX9DSq">Nightly Updates</head><p xml:id="_rYPDD2b"><s xml:id="_5yB4ZpW">The posterior distribution of β for the immediate treatment effect and the proxy for the delayed effect are updated at the end of each day.</s><s xml:id="_rUXHnZt">Operationally, the nightly update is a mapping:</s></p><formula xml:id="formula_6">{S l, k , A l, k , R l, k } 1 ≤ l ≤ 5, 1 ≤ k ≤ d = ℋ d {(μ d + 1 , Σ d + 1</formula><p xml:id="_avxHsgQ"><s xml:id="_bX46Bdg">), η d + 1 } that takes the current history up to day d as the input and outputs the posterior distribution and proxy of delayed effect, which are used in the action selection in the following day (d + 1).</s><s xml:id="_TCyDKM8">We discuss each of these in turn.</s><s xml:id="_YQE5MNZ">A pseudo code of the proposed HeartSteps V2 RL algorithm is provided in Algorithm 1.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1" xml:id="_hBAys47">Posterior Update of Immediate Treatment</head><p xml:id="_yPhKaXF"><s xml:id="_FjEEdgd">Effect.-We use the following linear Bayesian regression "working model" for the reward to derive the posterior distribution of the treatment effect:</s></p><formula xml:id="formula_7">R t = g(S t ) ⊤ α 0 + π t f(S t ) ⊤ α 1 + (A t -π t )f(S t ) ⊤ β + ϵ t , if I t = 1<label>(3)</label></formula><p xml:id="_AqYKwFH"><s xml:id="_6qWEgEa">where we assume the error term {ϵ t } is independent and identically distributed (i.i.d.)</s></p><p xml:id="_fNwexZz"><s xml:id="_VDuK4nh">Gaussian noise with mean 0 and variance σ 2 .</s><s xml:id="_nDfuBrB">Recall that in HeartSteps V2, the reward is the log-transformed 30-minute step count following the decision time, in which the log-transformation brings the distribution close to a Gaussian distribution.</s><s xml:id="_2c5ZyFS">We also note that the Gaussian assumption of the error term is merely used to derive the randomization probability (i.e., the posterior distribution of β).</s><s xml:id="_9du7ShQ">In fact, the theoretical result of the TS sampling algorithm does not rely on the Gaussian assumption <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_tkx6mTQ">The variance of the error term is estimated using HeartSteps V1 data and fixed throughout the study; see the discussion in Section 6.</s></p><p xml:id="_FeUgYGJ"><s xml:id="_nfd55JN">From(3,) the working model for the mean reward function is r t (s, a) = g(s) ⊤ α 0 + π t f(s) ⊤ α 1 + a -π t f(s) ⊤ β .</s><s xml:id="_j6smcfB">Recall that f(s) is the feature vector that predicts the immediate treatment effect (1).</s><s xml:id="_Mhx7Vad">Similarly, here the baseline feature vector g(s) is chosen to approximate the baseline reward function:</s></p><formula xml:id="formula_8">r t (s, 0) ≈ g(s) ⊤ α .<label>(4)</label></formula><p xml:id="_YEmEqCf"><s xml:id="_4bBGaZk">The baseline feature vector g(s) is selected based on the domain science and analyses of HeartSteps V1 data; see Section 6 for a discussion.</s><s xml:id="_vMaGY45">The working models for both treatment effect and baseline reward are assumed to be linear in the feature vector and to have time-invariant parameters.</s><s xml:id="_gfwZpFt">Although these are rather strong assumptions, below we argue that action centering, (i.e. the use of π t in ( <ref type="formula" target="#formula_7">3</ref>)) provides the robustness to the violation of these assumptions.</s></p><p xml:id="_QXWWh86"><s xml:id="_G72JDYt">First, consider the action-centered term (A t -π t ) in the working model <ref type="bibr" target="#b2">(3)</ref>.</s><s xml:id="_PQEab7k">As long as the treatment effect model ( <ref type="formula" target="#formula_17">1</ref>) is correctly specified, the estimator of β based on the model ( <ref type="formula" target="#formula_7">3</ref>) is guaranteed to be unbiased even when the baseline reward model ( <ref type="formula" target="#formula_8">4</ref>) is incorrect <ref type="bibr" target="#b3">[4]</ref>, for example, due to the non-linearity in g(s) or non-stationarity (changes in α over time).</s><s xml:id="_3Cz368Y">That is, through the use of action centering, we achieve robustness against mis-specification of the approximate baseline model, (4), addressing the challenge (C3).</s><s xml:id="_wt2aBUN">The rationale of including the term π t f(S t ) in the Bayesian regression working model <ref type="bibr" target="#b2">(3)</ref> is to capture the time-varying aspect of the main effect due to the action-centered term (since π t is continuously changing/updated during the study).</s><s xml:id="_zhabf5c">Omitting this term would reduce the number of parameters in the model, but we have found in experiments that the inclusion of π t f(S t ) reduces the variance of the treatment effect estimates and thus speeds up learning.</s><s xml:id="_ka7g5hV">Second, in the case where the treatment effect model ( <ref type="formula" target="#formula_17">1</ref>) is incorrect, for example, when the treatment effect is non-linear in f(S t ) or is non-stationary (e.g., with time-varying parameters), it can be shown <ref type="bibr" target="#b3">[4]</ref> that the Bayesian regression provides a linear approximation to the treatment effect.</s><s xml:id="_muyTe9x">When the action is not centered, the treatment effect estimates may not converge to any useful approximation at all, which could lead to poor performance in selecting the action.</s></p><p xml:id="_p4rc2gr"><s xml:id="_tXz3q6S">The Bayesian model (3) requires the specification of prior distributions on α 0 , α 1 and β.</s></p><p xml:id="_W7wh8VG"><s xml:id="_enurQY8">Here the priors are independent and given by</s></p><formula xml:id="formula_9">α 0 N(μ α 0 , Σ α 0 ), α 1 N(μ β , Σ β ), β N(μ β , Σ β )<label>(5)</label></formula><p xml:id="_KG4m6nN"><s xml:id="_S6GCwTR">See Section 6 for a discussion of how the informative priors (challenge (C2)) are constructed using HeartSteps V1 data.</s><s xml:id="_7uaZ32Z">Because the priors are Gaussian and the error in ( <ref type="formula" target="#formula_7">3</ref>) is Gaussian, the posterior distribution of β given the current history ℋ d is also Gaussian, denoted by N(μ d + 1 , Σ d + 1 ) .</s><s xml:id="_NwmwXvz">Below we provide the details about the calculation of (μ d + 1 , Σ d + 1 ) .</s></p><p xml:id="_bzfk8Jg"><s xml:id="_jrAHQPC">We first calculate the posterior distribution of all parameters, θ ⊤ = (α 0 ⊤ , α 1 ⊤ , β ⊤ ) and the posterior distribution of β can then be identified.</s><s xml:id="_q6e4Q4S">The posterior distribution of θ, denoted by N(μ ¯d + 1 , Σ ¯d + 1 ), given the current history</s></p><formula xml:id="formula_10">ℋ d = {S l, k , A l, k , R l, k } 1 ≤ l ≤ 5, 1 ≤ k ≤ d , can be found by Σ ¯d + 1 = 1 σ 2 ∑ k = 1 d ∑ l = 1 5 I l, k ϕ(S l, k , A l, k )ϕ(S l, k , A l, k ) ⊤ + Σ ¯-1 -1<label>(6)</label></formula><p xml:id="_ygD5yeQ"><s xml:id="_SKDCsAx">μ ¯d + 1 = Σ ¯d + 1 1</s></p><formula xml:id="formula_11">σ 2 ∑ k = 1 d ∑ l = 1 5 I l, k ϕ(S l, k , A l, k )R l, k + Σ ¯-1 μ ¯(7)</formula><p xml:id="_sYnDnWa"><s xml:id="_S9gqFEg">where ϕ(S l, k , A l, k ) ⊤ = (g(S l, k ) ⊤ , π t f(S l, k ) ⊤ , (A l, k -π l, k )f(S l, k ) ⊤ ) denotes the joint feature vector and (μ ¯, Σ ¯) is the prior mean and variance of θ, e.g., μ ¯= (μ α 0 , μ β , μ β ) and Σ ¯= diag(Σ α 0 , Σ β , Σ β ) .</s><s xml:id="_BPhCnJh">Suppose the size of f(s) is p.</s><s xml:id="_VDcRRvE">Then the posterior mean of β, μ d+1 is the last p elements of the above μ ¯d + 1 and the posterior variance of β, Σ d+1 is the bottom-right corner matrix of size p by p in Σ ¯d + 1 .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2" xml:id="_6epQG8F">Proxy Delayed</head><p xml:id="_bqKPew2"><s xml:id="_MRgS7fD">Effect on Future Rewards.-The</s><s xml:id="_jVqpdaY">proxy is formed based on a simple Markov Decision Process (MDP) for the states S t = (Z t , I t , X t ), in which we make the following working assumptions about the transition of states: (S1) the context {Z t } is i.i.d. with distribution F, (S2) the availability {I t } is i.i.d. with probability p avail (S3) the dosage variable {X t } makes transitions according to τ(x ′ |x,a)</s></p><p xml:id="_CWKMu2v"><s xml:id="_AbdJJRn">We use this simple MDP to capture the delayed effect of delivering the intervention on the future rewards.</s><s xml:id="_kYxSZ9w">The key assumption in this model is that the action impacts the future rewards only through the dosage since the context is assumed to be independent of the past actions.</s><s xml:id="_neu9v6u">This assumption allows us to form a low-variance estimate of the delayed effect of treatment based only on the current dosage.</s><s xml:id="_GNk9QFV">Recall that the decision times in both HeartSteps V1 and V2 are roughly separated by 2-2.5 hours during the day.</s><s xml:id="_Z9BaFQt">The impact of the current action on the next context is likely weak.</s><s xml:id="_Ky26Dcd">We use HeartSteps V1 data to perform the Generalized Estimating Equations (GEE, <ref type="bibr" target="#b22">[23]</ref>) analysis to confirm that the effect is in fact not significant for all of the selected context variables (see the list in Section 6).</s></p><p xml:id="_BqbDQkD"><s xml:id="_TskERAz">To reduce the model complexity, we assume that the context and availability are both i.i.d.</s><s xml:id="_wzwMZx4">across times.</s><s xml:id="_jqGV4vd">This i.i.d.</s><s xml:id="_pT88W9H">assumption is likely unrealistic (for example, the next temperature might depend on the current temperature), however it leads to a reduced variance of the estimator of the delayed effect as we do not need to learn a transition model for the context and availability.</s><s xml:id="_vHxeTuz">We believe that relaxing the i.i.d.</s><s xml:id="_yuY8Qn6">assumption of context and availability in modeling the delayed effect could be an interesting future direction.</s><s xml:id="_qRKZxQX">Recall the definition of dosage variable at the beginning of Section 5.2: given the previous dosage x and whether the participant receives the previous walking suggestion message a, the next dosage x ′ would be fully determined by knowing whether the participant has received any anti-sedentary messages since the last decision time.</s><s xml:id="_pDA8myt">In (S3), the transition model τ(x ′ |x,a) essentially models the probability of receiving anti-sedentary messages between two decision times; see details below.</s></p><p xml:id="_f3st3Mf"><s xml:id="_AJ9uPh2">We now discuss how each component in the simple MDP is constructed.</s></p><p xml:id="_eudHT5e"><s xml:id="_KJCXzXv">Given the history up to the end of day d, ℋ d , we set (1) the average prior availability to be p avail = The mean reward at unavailable decision times has the same form but with posterior means from a similar linear Bayesian regression using the unavailable time points in ℋ d .</s></p><p xml:id="_cwWVJvt"><s xml:id="_GaUgb53">We formulate the proxy of delayed effect based on the above constructed MDP as follows.</s></p><p xml:id="_d6XcxUb"><s xml:id="_EtXrCGC">Consider an arbitrary policy π that chooses the action π(S) at the state S = (Z,I,X) if the user is available (i.e., I = 1) and chooses action 0 otherwise (i.e., π(S) = 0 if I = 0).</s><s xml:id="_7aDzCSX">Recall the state-action value function for policy π under discount rate γ:</s></p><formula xml:id="formula_12">Q π (s, a) = E π [R t + γR t + 1 + γ 2 R t + 2 + … | S t = s, A t = a]</formula><p xml:id="_TVWMkwb"><s xml:id="_BvnEAnn">where the subscript π means the actions (A 2 , A 3 , …) are selected according to the policy π.</s></p><p xml:id="_Pm9rNC6"><s xml:id="_wxK2DAe">Also recall the state value function V π (s) = Q π (s, π(s)) .</s><s xml:id="_hcgpjh7">The value function Q π is divided into two parts: Q π (s, a) = r(s, a) + γH π (x, a) where r(s, a) is the estimated reward function and</s></p><formula xml:id="formula_13">H π (x, a) = E V π S t + 1 | S t = s, A t = a = E π [R t + 1 + γR t + 2 + γ 2 R t + 3 + … | S t = s, A t = a]</formula><p xml:id="_6VYr8Gh"><s xml:id="_BG6yKeE">is the sum of future discounted rewards (future value, in short).</s><s xml:id="_sxNhU34">H π (x, a) excludes the first, immediate reward (R t ) and is only a function of (x,a) under the working assumptions (S1) and (S2).</s><s xml:id="_RC58Krp">Note that the difference H π (x, 1) -H π (x, 0) measures the impact of sending treatment at dosage x on the future rewards in the setting in which future actions are selected by policy π.</s><s xml:id="_x8KuPB8">We select the policy π to maximize the future value under the constraint that π depends only on the dosage and availability.</s><s xml:id="_4MFYdg6">Specifically, let H*(x, a) = max H π (x, a): π: X × 0, 1 A, π(x, 0) = 0, ∀x ∈ X .</s><s xml:id="_6agGzDJ">It can be shown that H* is given by H*(x, a) = ∑ x′, i′ τ x′ | x, a p avail i′ (1 -p avail 1 -i′ V * x′, i′ , where the bivariate function</s></p><formula xml:id="formula_14">V *: X × 0, 1</formula><p xml:id="_RyktfdN"><s xml:id="_kZd2Z4A">ℝ solves the following equations:</s></p><formula xml:id="formula_15">V (x, i) = max a ∈ A(i) {r 1 (x, a) + γ ∑ x′, i′ τ x′ | x, a p avail i′ 1 -p avail 1 -i′ V x′, i′ }</formula><p xml:id="_EBjcvBb"><s xml:id="_BJkUSmC">for all x ∈ X and i ∈ {0, 1}, where A(i) is the constrained action space based on availability, i.e., A(1) = {0, 1} and A(0) = 0, r 0 and r 1 (x,a) are the marginal reward function (marginal in the sense that it only depends on the dosage variable) given by r 0 (x) = ∫r((z, 0, x), 0)dF(z),r 1 (x, a) = ∫r((z, 1, x), a)dF(z).</s><s xml:id="_N7Qe3GD">Finally, the proxy for the delayed effect is calculated by</s></p><formula xml:id="formula_16">η d + 1 (x) = γH d + 1 (x, 0) -γH d + 1 (x, 1)<label>(8)</label></formula><p xml:id="_MS5B6PU"><s xml:id="_4AHrvRg">where H d + 1 = (1 -w)H 1 + wH* is the weighted average of the estimate H* and the initial function H 1 calculated based only on data from HeartSteps V1.</s><s xml:id="_DwpwbNa">The selection of the discount rate γ and the weight w will be discussed in Section 6.</s><s xml:id="_F3wsRBC">This delayed effect is the mean difference of the discounted future rewards between sending nothing and sending an activity suggestion.</s><s xml:id="_pWYuHYV">From here we see that in (2) A t , the action at decision time t is essentially selected to maximize the sum of discounted rewards, i.e., A t ≈ argmax a {r(S t , a) + γH d (X t , a)}.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_s9xn5sf">CHOOSING INPUTS TO THE RL ALGORITHM</head><p xml:id="_RkqNs6H"><s xml:id="_F5Vtj4Q">We review the inputs required by the HeartSteps V2 RL algorithm and discuss how each is selected by the scientific team and on the basis of HeartSteps V1 data analysis.</s><s xml:id="_V9m7CFp">The list of required inputs is summarized in Table <ref type="table">2</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" xml:id="_V52Qjbb">Probability Clipping</head><p xml:id="_KAD8YUq"><s xml:id="_Q3xNKwp">The scientific team decided ϵ 0 = 0.2 and ϵ 1 = 0.1 for the probability clipping to ensure enough exploration (in order to, for example, force the RL algorithm to continuously explore without converging to a deterministic policy).</s><s xml:id="_kg6j8rA">In addition, clipping at ϵ 0 = 0.2 also introduces a soft constraint on the number of walking suggestion messages delivered per day.</s><s xml:id="_Yt4xB9Q">In particular, at most on average 5 × (1 -ϵ 0 ) = 4 walking suggestion messages can be sent in a day assuming the participant is always available.</s><s xml:id="_TgbxHCJ">Finally, the probability of selecting each action is greater than 0.1 and ensures the stability of causal inference after the study is over.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_HtWzPHf">Feature Vector</head><p xml:id="_5Gxrfgt"><s xml:id="_XcsSqkQ">Recall that the working model ( <ref type="formula" target="#formula_7">3</ref>) requires the specification of the feature vectors f(s) and g(s) (transformed into [0, 1] in the algorithm) in ( <ref type="formula" target="#formula_17">1</ref>) and ( <ref type="formula" target="#formula_8">4</ref>).</s><s xml:id="_TVss7nd">The feature vectors f(s), g(s) are chosen based on the GEE analysis using HeartSteps V1 data.</s></p><p xml:id="_v8ZVh4y"><s xml:id="_apKmQkU">Specifically, each feature is included in a marginal GEE model with the prior 30-minute step count in the main effect model to reduce the variance.</s><s xml:id="_dkMMkre">The candidate feature is included in both the main effect and treatment effect models.</s><s xml:id="_hKvSy5V">The procedure is done for each feature separately and a p-value is obtained.</s><s xml:id="_6TBZnT9">The feature is then selected into g(s) and f(s) at the significance level of 0.05.</s><s xml:id="_EYPdA7z">Although we found that the 30-minute step count before the decision time is highly predictive of the rewards (e.g., 30-minute step count after the decision), it is not significant in terms of predicting the treatment effect.</s><s xml:id="_S8Dbneh">Therefore, the prior 30-minute step count is included in the baseline features g(s), but not in the feature vector f(s) for treatment effect.</s></p><p xml:id="_Yse6UnS"><s xml:id="_xrmm8ws">As mentioned in Section 5.2, we define the dosage in the form of X t + 1 = λX t + 1 E t + 1 .</s></p><p xml:id="_Dexau7Y"><s xml:id="_es55tFh">We conducted GEE analysis for a variety of values of λ.</s><s xml:id="_Jr5HA3p">When λ is relatively large, the dosage significantly impacts the effectiveness of the activity suggestions on the subsequent 30-minute step count and we selected λ = 0.95 (p-value 0.085).</s></p><p xml:id="_zyFs7KS"><s xml:id="_SVXCrnX">A measure of how well the participant engages with the mobile app (e.g., the daily number of screens that the participant encounters) is planned to be included in both g(s) and f(s).</s></p><p xml:id="_cRDNPBg"><s xml:id="_WX3XNhs">This variable was not collected in HeartSteps V1.</s><s xml:id="_VSnNp6u">The scientific team believes this variable likely interacts with the treatment and thus decided to include it in the features.</s><s xml:id="_Y6MBd8M">In both f(s) and g(s), the intercept term is also included.</s><s xml:id="_sMKzrpJ">See Table <ref type="table">3</ref> for the list of the selected features.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" xml:id="_FYm5pNs">Noise Variance and Prior Distribution in Reward Model</head><p xml:id="_TJqghCP"><s xml:id="_7VGWHxB">Recall the variance of the noise σ 2 in the model <ref type="bibr" target="#b2">(3)</ref>.</s><s xml:id="_epk2jnw">The variance σ 2 can be learned on the fly, e.g., estimated by the residual variance in the model fitted by the current data.</s><s xml:id="_uAAuf2N">However to ensure the stability of the algorithm (since the step count can be highly noisy), we set the variance parameter using the data from HeartSteps V1, that is, σ 2 is not updated during the study.</s><s xml:id="_KhnfxMj">We calculate the residual variance in the regression model using the above-selected feature and get σ 2 = 2.65 2 .</s></p><p xml:id="_vYY9W9V"><s xml:id="_5tCRNb6">The prior distribution ( <ref type="formula" target="#formula_9">5</ref>) is constructed on the basis of the analysis of HeartSteps V1 data.</s><s xml:id="_PMcTF23">Specifically, we first conduct GEE regression analyses <ref type="bibr" target="#b22">[23]</ref>, using all participants' data in HeartSteps V1 and assess the significance of each feature.</s><s xml:id="_KbVXxr2">To form the prior variance, on each participant we fit a separate GEE linear regression model and calculated the standard deviations of the point estimates across the 37 participant models.</s></p><p xml:id="_3D7Zbfs"><s xml:id="_DXfQYVj">We formed the prior mean and prior standard deviation as follows.</s></p><p xml:id="_cSTTX5q"><s xml:id="_Ddeguxr">(1) For the features that are significant in the GEE analysis using all participants' data, we set the prior mean to be the point estimate from this analysis; we set the prior standard deviation to be the standard deviation across participant models from the participant-specific GEE analyses.</s><s xml:id="_uwQCckV">(2) For the features that are not significant, we set the corresponding prior mean to be zero and shrink the standard deviation by half.</s><s xml:id="_krBQMxZ">(3) For the app engagement variable, we set the prior mean to be 0 and the standard deviation to be the average prior standard deviation of other features.</s><s xml:id="_vMjnTEG">Σ α 0 , Σ β are diagonal matrices with the above prior variances on the diagonals; see Table <ref type="table" target="#tab_3">4</ref> and 5 for the prior distributions.</s><s xml:id="_hHPnXGa">The same procedure is applied to form the prior mean and variance for the reward model at the unavailable times.</s><s xml:id="_srnwz6M">This mean and variance will be used in the proxy value updates.</s><s xml:id="_4dgjFXH">The rationale of setting the mean to zero and shrinking the standard deviation for the non-significant features is to ensure the stability of the algorithm: unless there is strong evidence or signal detected from the participant during the HeartSteps V2 study, these features only have minimal impact on the selection of actions.</s><s xml:id="_CuTVbNF">In Section 7.1, we also apply the above procedure to construct the prior in the simulation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4" xml:id="_FPGAgKQ">Parameters in Proxy Delayed Effect</head><p xml:id="_Y9y7rcv"><s xml:id="_wCBS7TE">The initial proxy delayed effect, η 1 , and the estimation of proxy delayed effect, η d , both require the initial proxy value estimates H 1 .</s><s xml:id="_UanmtKZ">To calculate H 1 we use the same procedure as described in Section 5.4.2 to calculate H*, except that the empirical probability of being available, the empirical distribution of contexts, and the reward function are constructed using only HeartSteps V1 data.</s></p><p xml:id="_jPEduZR"><s xml:id="_7RmUjce">Two remaining parameters need to be specified in estimating the proxy delayed effect: the discount rate γ and the updating weight parameter w (both part of the proxy MDP in Section 5.4.2) For simplicity, we refer to them as "tuning parameters" in the rest of the article.</s><s xml:id="_qzgPKyE">These tuning parameters are difficult to specify directly as the optimal choice likely depends on the noise level of rewards, how the context varies over time, and the length of the study.</s><s xml:id="_qxZDPwR">We propose to choose the tuning parameters, (w,γ), based on a simulation-based procedure.</s><s xml:id="_fttJdRc">Specifically, we first build a simulation environment (i.e., a data generating model) using HeartSteps V1 data (see Section 7.1 for details).</s><s xml:id="_w6d5NwZ">We then apply the algorithm as shown in Figure <ref type="figure" target="#fig_3">1</ref> with each candidate pair of tuning parameters.</s><s xml:id="_C7Bf9cn">Finally, the tuning parameters are chosen to maximize the total simulated rewards.</s><s xml:id="_azPh7MG">In Section 7, we demonstrate the validity of this simulation-based procedure to select the tuning parameters by three-fold cross-validation, showing that the selected tuning parameters in the training phase generalize well to the testing phase.</s></p><p xml:id="_WV6sWDb"><s xml:id="_GufVcGA">In this section, we use HeartSteps V1 data to conduct a simulation study to demonstrate the validity of the procedure for choosing the inputs, including the tuning parameters, described in Section 6, the validity of using proxy values in the proposed algorithm addressing the challenge (C1) about the negative delayed effect of treatments, and the validity of using action centering to protect against model mis-specification (C3).</s><s xml:id="_JPYq37u">Here the use of a previous dataset to build a simulation environment for evaluating an online algorithm is similar to <ref type="bibr" target="#b23">[24]</ref>.</s><s xml:id="_GeuzpmK">In Section 8, we also provide the assessment of the proposed algorithm using pilot data from HeartSteps V2.</s></p><p xml:id="_fpM8Yzq"><s xml:id="_FWPjb22">We carry out a three-fold cross validation (CV) procedure.</s><s xml:id="_TK6XtqJ">Specifically, we first partition the HeartSteps V1 dataset into three folds.</s><s xml:id="_b8mqw6P">In each of the three iterations, two folds are marked as a training batch and the third fold is marked as a testing batch.</s><s xml:id="_t8GkTEQ">The training batch is used to (1) construct the prior distribution, (2) form an estimate of noise variance, and</s></p><p xml:id="_G6rsEDF"><s xml:id="_ypGb958">(3) select the tuning parameters.</s><s xml:id="_6wqFeZb">We call this process the "training phase".</s><s xml:id="_NpG5hmf">Note that the training batch serves the same purpose as HeartSteps V1.</s><s xml:id="_nJNWcjX">Next, the testing batch is used to construct a simulation environment to test the algorithm with the estimated noise variance, prior, and tuning parameters.</s><s xml:id="_DNEj99s">The use of a testing batch is akin to applying the RL algorithm in HeartSteps V2.</s><s xml:id="_FsAvZSv">In Section 7.1 and 7.2 below, we will describe in greater detail how the training batch and the testing batch are used in each iteration of cross validation.</s><s xml:id="_bRYfYug">Note that we will apply the same procedure three times.</s></p><p xml:id="_gMXtqAe"><s xml:id="_J8ghgx8">We compare the performance to that of the Thompson Sampling Bandit algorithm, a version similar to <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_ynzxWee">The TS Bandit algorithm is a widely used RL algorithm showing good performance in many real-world settings <ref type="bibr" target="#b4">[5]</ref>.</s><s xml:id="_5u34233">At each decision time, it selects the action probabilistically according to the posterior distribution of reward with the goal of maximizing the immediate reward.</s><s xml:id="_v3SevHu">We choose the TS Bandit as the comparator over other standard contextual bandit algorithms (e.g., LinUCB in <ref type="bibr" target="#b21">[22]</ref>) because the TS Bandit is a stochastic algorithm that better suits our setting due to challenge (C4).</s><s xml:id="_AfVq7ts">In the TS Bandit, the expected reward is modeled by E R t | S t = s, A t = a = r(s, a; θ) for some parameter θ.</s></p><p xml:id="_EKmqCB9"><s xml:id="_8g4hmDj">At each decision time t with context S t = s and availability I t = 1, the action A t = a is selected with probability Pr{r(s, a; θ) = max a ˜∈ A r(s, a ˜; θ); θ N(μ, Σ)}, where N(μ, Σ) is the posterior distribution of the parameters θ given the current history under the Bayesian model R t = r S t , A t ; θ + ϵ t of rewards with Gaussian prior and error.</s><s xml:id="_vwqEpYE">The main difference to our algorithm is that TS Bandit attempts to choose the action that maximizes the immediate reward, whereas our proposed algorithm takes into account the longer term impact of the current action per challenge (C1).</s><s xml:id="_AQYMv9v">In addition, the TS Bandit algorithm requires the correct modeling of each arm, while our method uses action centering (3) to protect against mis-specifying the baseline reward per challenge (C3) and only requires correctly modeling the difference between two arms, i.e., the treatment effect model in (1).</s></p><p xml:id="_s9Cea5r"><s xml:id="_3Eseymf">In the implementation of TS Bandit, we parametrize the reward model by r(s, a; θ) = g(s) ⊤ α + af(s) ⊤ β where f(s) and g(s) are the same feature vectors used in our proposed algorithm.</s><s xml:id="_26znxZU">Furthermore, to allow for a fair comparison, the prior distribution of θ = (α, β) and the variance of error term σ 2 are both constructed by the training batch using the same procedure that will be discussed in Section 7.1 and the probability of selecting each arm is clipped with the same constraints.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1" xml:id="_q3E7hJn">Training Phase</head><p xml:id="_cQNmyHu"><s xml:id="_ukezs4U">Prior distribution.-The</s><s xml:id="_peUzwja">algorithm requires three prior distributions: the prior of the parameters in the main effect when available, the prior of parameters in the treatment effect, and the prior of parameters in the mean reward when not available.</s><s xml:id="_AaMvgrc">The last one is used in calculating the proxy value.</s><s xml:id="_z9eb3Eb">The prior distributions are calculated using the training batch as described in Section 6.</s><s xml:id="_gGn3uqr">We refer to the estimated GEE model using all participants' data in the training batch as population GEE in what follows.</s></p><p xml:id="_NCktV7W"><s xml:id="_KWESYFH">Noise variance.-We</s><s xml:id="_Y3Yq7N7">set the noise variance to be the variance of residuals obtained from the above population GEE.</s><s xml:id="_AesAf7q">{Z t i , I t i , ϵ t i } t = 1 450 by first creating the 42-day sequence of the context, availability, residual, {Z t i , I t i , ϵ t i } t = 1 210 where residual {ϵ t i } t = 1 210 is obtained from the person-specific regression model fit.</s><s xml:id="_nrEvgH8">Then we extend this 42-day sequence to a 90-day sequence, {Z t i , I t i , ϵ t i } t = 1 450 , by concatenating (90 -42) days' data, randomly selected from the 42-days' data.</s><s xml:id="_cW7NHpz">Specifically, we randomly choose d from {1, . .</s><s xml:id="_ue5hQ62">., 42} and append all data from day d onto the 42-day data and repeat until we have a 90-day data set.</s><s xml:id="_jv7P3tB">The sampling is done only once and the sequence is fixed throughout the simulation.</s><s xml:id="_zCNpRPa">The generative model for participants i in the training batch is given as follows.</s><s xml:id="_4esGERz">At time t = 1, 2, 3 . .</s><s xml:id="_DjYnYT6">., 450,</s></p><p xml:id="_4tXhSPU"><s xml:id="_8KPFwPe">Randomly generate a binary variable B t with probability 0.2 (on average 1 per day).</s><s xml:id="_qcvYRmB">Here B t is the indicator of whether there is any anti-sedentary suggestion sent between (t -1) and t.</s></p><p xml:id="_J3tGj9Y"><s xml:id="_t3Ke9XB">(2) Obtain the current dosage X t = λX t -1 + 1 E t , where λ = 0.95, the event E t = {A t -1 = 1} ∪ {B t = 1} .</s></p><p xml:id="_kAXCxWx"><s xml:id="_vS7xzWd">(3)</s></p><formula xml:id="formula_18">Set (Z t , I t ) = (Z t i , I t i )<label>(4</label></formula><p xml:id="_MGWWnez"><s xml:id="_zSR9zHU">) Select the action A t according to (2) (5) Receive the reward R t defined as R t + 1 = g S t ⊤ α 1 train + A t ⋅ f S t ⊤ β train + ϵ t i , I t = 1 g S t ⊤ α 0 train + ϵ t i , I t = 0</s></p><p xml:id="_MMtxpn7"><s xml:id="_KNvhptf">where the coefficients (α 0 train , α 1 train , β train ) are set based on population GEE using the data of all participants in the training batch.</s></p><p xml:id="_mSp9YQF"><s xml:id="_8frJrpK">For a given candidate value of tuning parameters, together with the above-constructed noise variance and prior, the algorithm is run 96 times under each training participant's generative model.</s><s xml:id="_2N97Zup">The average total reward (over all training participants and re-runs) is calculated and we select the tuning parameters that maximize the average total reward.</s><s xml:id="_2tBCafy">We use the grid search over γ ∈ {0, 0.25, 0.5, 0.75, 0.9, 0.95} and w ∈ {0, 0.1, 0.25, 0.5, 0.75, 1}.</s><s xml:id="_qUhHjza">Recall that the training is done three times and each time uses two folds as the training batch.</s><s xml:id="_2F4YkjB">The selected tuning parameters for the three iterations in CV are given by (γ,w) = (0.9, 0.5), (0.9, 0.75), (0.9, 0.1).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2" xml:id="_K2mNycW">Testing Phase</head><p xml:id="_D3rHVRW"><s xml:id="_QH8XAtb">We build the generative model using the testing batch following the procedure described in Section 7.1 with the only difference being that in the testing phase the coefficients in generating the reward <ref type="bibr" target="#b8">(9)</ref> are replaced by (α 0 train , α 1 train , β train ), which are the least squared estimates calculated using the testing dataset.</s><s xml:id="_ZE8JXZ9">We run the algorithm under each test participant's generative model with the noise variance estimates, the prior distribution, and the tuning parameters selected from the training data.</s><s xml:id="_nD2aT6P">The algorithm is run 96 times per testing participant, and the average total reward over the runs is calculated.</s></p><p xml:id="_h77MjV9"><s xml:id="_qdzjV8e">Recall that we conduct a three-fold cross validation.</s><s xml:id="_GPABAMc">Every participant in HeartSteps V1 data is assigned to exactly one testing batch in the cross validation.</s><s xml:id="_raQRtQV">The performance of our algorithm and that of the comparator, the TS Bandit algorithm, on each participant when assigned to the testing batch is provided in Figure <ref type="figure" target="#fig_4">2</ref>. We see that for 29 out of 37 of the participants, the total rewards are higher for our approach than for the approach using the TS Bandit algorithm.</s><s xml:id="_5PRVSzj">The average improvement of the total rewards over TS Bandit is 29.753, which gives an improvement of 29.753/450 = 0.066 per decision time.</s><s xml:id="_NN7nvEH">Recall that the reward is the log-transformed 30-minute step count following each decision time.</s><s xml:id="_Q2kNJWk">Translating back into the raw step count, we see the improvement is about (exp(0.066)</s><s xml:id="_SkPrrdT">-1) × 100% = 6.8% increase in the 30-minute step count.</s><s xml:id="_qazQPxW">Recall that the TS Bandit algorithm is sensitive to model mis-specification/non-stationarity and greedily maximizes the immediate reward.</s><s xml:id="_AqMaS4z">The simulation results demonstrate that the use of action centering and the proxy delayed effect effectively addresses the challenges (C1) and (C3).</s></p><p xml:id="_77V66MQ"><s xml:id="_3R9nVcJ">HeartSteps V2 has been deployed in the field since June 2019.</s><s xml:id="_H9ugRtF">Our team has conducted a pilot study to test the software and multiple intervention components.</s><s xml:id="_4xjmqBT">The RL algorithm developed above was used to decide whether to trigger the context-tailored activity suggestion at each of the five decision times per day.</s><s xml:id="_WhBGje8">The inputs to the algorithm (e.g., the choice of feature vectors, the prior distribution, and the tuning parameters) were determined according to Section 6.</s><s xml:id="_mRNmmfM">In other words, we used all the HeartSteps V1 data to choose the inputs following the procedure described in Section 7.1.</s><s xml:id="_zpamNUy">Below we provide an initial assessment of the algorithm and discuss the lessons learned from the pilot participants' data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1" xml:id="_Fas3aaY">Initial Assessment</head><p xml:id="_sNDmdwu"><s xml:id="_dGQUWue">Recall that each participant in HeartSteps V2 wears the Fitbit tracker for one week before starting to use the mobile app; no activity suggestion is delivered during this initial week.</s><s xml:id="_bG8UkRa">Currently, there are eight participants in the field who have been in the study for over one week and are experiencing the RL algorithm.</s><s xml:id="_BcbngjC">For each participant, we calculated the average 30-minute step count after each user-specified decision time during the first week and compared this to the average 30-minute step count in the subsequent weeks during which activity suggestions are delivered.</s><s xml:id="_jpkyHgz">This comparison is provided in Table <ref type="table">6</ref>.</s><s xml:id="_7R7pFfy">All except one participant (ID = 4) experienced positive increases in step count.</s><s xml:id="_C3z3bv2">We see that on average each participant takes 125 more steps in the 30-minute window following the decision time than in the first week.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2" xml:id="_hUNCPJE">Lessons</head><p xml:id="_PrfVEqp"><s xml:id="_yY5ndFa">In this section, we discuss two lessons learned from the examination of the pilot participants' data.</s><s xml:id="_DvUz2C5">We illustrate these lessons using data from participants ID=4 and ID=7.</s><s xml:id="_ueWFYMj">First, consider participant ID=4, who is not responsive to the activity suggestions (i.e., sending a suggestion does not significantly improve the step count).</s><s xml:id="_FuXkucS">That is, as seen in Table <ref type="table">6</ref> participant ID = 4 has step counts that decrease after the first week.</s><s xml:id="_y6yCWPE">Figure <ref type="figure" target="#fig_5">3</ref> shows the randomization probability and the posterior mean estimates for participant ID = 4.</s><s xml:id="_V35NXMn">We see that for this participant the posterior mean estimates start with a positive value and drop below 0, i.e., no sign of the effectiveness of the suggestions is seen, however the randomization probability still ranges between 0.2 and 0.4.</s><s xml:id="_Rz4MBHf">Given that HeartSteps is intended for long-term use (recall HeartSteps V2 is a 3-month study) and there are other intervention components (the weekly reflection and planning and the anti-sedentary suggestion message), randomizing with these probabilities is likely too much.</s><s xml:id="_VXJPBQM">In consideration of the user's engagement and burden, it makes sense to reduce the chance of receiving intervention when the algorithm does not have enough evidence of the effectiveness of the intervention.</s></p><p xml:id="_cPefK9h"><s xml:id="_v9k6UzT">Next, consider participant ID =7, who appears highly responsive to the activity suggestions (see Table <ref type="table">6</ref> and the right-hand graph in Figure <ref type="figure" target="#fig_6">4</ref> of the posterior mean of the treatment effect).</s><s xml:id="_977QMUQ">First, we note that the probability clipping takes effect multiple times during this time period.</s><s xml:id="_Apy2ZA9">That is, the randomization probability calculated in (2) exceeds the limit 1 -ϵ 0 = 0.8 and thus reaching the average constraint on the number of suggestions per day (i.e., 0.8 × 5 = 4).</s><s xml:id="_FZjDYqa">The probability clipping or the induced average constraint is important to manage the user's burden as we can see from the right-hand graph in Figure <ref type="figure" target="#fig_6">4</ref> that this participant's responsiveness begins to decrease around time July 10.</s><s xml:id="_wCNvzcf">Next, the left-hand graph in this same figure shows that the randomization probabilities from our RL algorithm do not really start to decrease until 07-16.</s><s xml:id="_Y6j7Nk4">Ideally, the proxy value should be responding quickly to the excessive dose and signaling that the probability should decrease more.</s><s xml:id="_Fc66HJu">Note that the proxy is in fact reducing the probability of sending the walking suggestion when the delayed effect is present; see the left-hand graph in Figure <ref type="figure" target="#fig_6">4</ref> and compare the black points (which correspond to the actual randomization probability) to the red points (which correspond to the randomization probability without the proxy value adjustment).</s><s xml:id="_ZWzFnsd">Ideally, we would like to see a bigger gap between the black and red points in the period from 07-16 to 07-15.</s><s xml:id="_TvfVqny">We are currently revising the algorithm in response to these two lessons as discussed in Section 9.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9" xml:id="_srf74bA">DISCUSSIONS</head><p xml:id="_f2ztxMU"><s xml:id="_DUefA5H">Most of the parameters used in the algorithm for HeartSteps V2 are constructed based on a pilot study, HeartSteps V1.</s><s xml:id="_VyDq5jW">One natural concern is whether the parameters chosen from HeartSteps V1 can generalize well to HeartSteps V2.</s><s xml:id="_SWUZ4CH">First, we note that while the populations in these two studies are different, we expect the sedentary behavior of participants to be similar.</s><s xml:id="_hHtTMT2">Also, recall that this work aims to develop an online RL algorithm, as opposed to simply applying a pre-specified treatment policy learned from a previous study in another new study.</s><s xml:id="_eRgqVFP">This is very different in that the former allows the underlying treatment policy to be continuously updated throughout the study.</s><s xml:id="_DeXfaue">For example, we can see that the impact of the prior distribution would eventually get washed out as more data is collected from the participant.</s><s xml:id="_MzJaHAD">The parameters selected from HeartSteps V1 can be viewed as a "warm start" and do not prevent generalization too much in this online setting.</s></p><p xml:id="_TaQQarD"><s xml:id="_gfbb3vR">We recommend that scientists develop just-in-time adaptive interventions in an iterative, sequential manner.</s><s xml:id="_G8A4n2c">Specifically in the case of HeartSteps, our team first conducted HeartSteps V1 to gain some evidence of the effectiveness of interventions and to build the RL algorithm for use in HeartSteps V2.</s><s xml:id="_DeAXS2h">We then conducted another pilot study for HeartSteps V2 to evaluate the algorithm, and this pilot data will be used to further improve the design of the algorithm for the clinical trial.</s></p><p xml:id="_QzXSGkz"><s xml:id="_99GZDXa">This work is largely motivated by the design of the RL algorithm for use in a physical activity study.</s><s xml:id="_8W6UvX2">We believe the challenges mentioned in Section 3 arise in many mobile health applications and our solutions to address these challenges for HeartSteps can be applied in other settings.</s><s xml:id="_JUZa7Xb">While this RL algorithm cannot be directly applied in other studies (since, for example, the inputs to the algorithm might be completely different depending on the application), the design considerations and the procedure used to select inputs described in Section 6 may be useful in other studies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1" xml:id="_HFDpp4m">Limitations and Future Work</head><p xml:id="_BXWJcGV"><s xml:id="_GDSkk4H">Our RL algorithm has several important limitations and we foresee several opportunities to improve it.</s></p><p xml:id="_ZgCGDnN"><s xml:id="_554vAjK">Building a more sophisticated model.-In</s><s xml:id="_5jnNbKQ">designing the algorithm for HeartSteps V2, we used a relatively simple model and made several strong working assumptions; see Table <ref type="table">1</ref>.</s><s xml:id="_rm2wpxf">For example, in modeling the reward we assumed a low-dimensional linear model for both treatment effect and baseline reward.</s><s xml:id="_dVAw6Wn">The modeling of the delayed effect was built on a simple MDP model in which the action does not impact the states except for the dosage variable.</s><s xml:id="_q7rwQSm">See the list of assumptions in Table <ref type="table">1</ref>.</s><s xml:id="_ey9RVBP">This is mainly because we have a limited amount of pilot data collected from HeartSteps V1 (37 participants) to train and validate a more complex model.</s><s xml:id="_RfD4VaH">With more pilot data, one could consider relaxing some of the assumptions.</s><s xml:id="_QzsbV2j">For example, modeling the dependence structure among the error terms (i.e., within-subject correlation) could potentially reduce the estimation error.</s><s xml:id="_DHEVSRN">Also, the current algorithm takes into account the delayed effect of treatment by using a pre-defined "dosage variable" capturing the burden.</s><s xml:id="_vawRZkj">It would be interesting to develop a version in which more sophisticated measures of the burden and engagement (for example, a latent variable approach) are used to approximate the delayed effect and respond quickly to prevent disengagement.</s></p><p xml:id="_gD2jCJh"><s xml:id="_KPDseXF">Adjusting the tuning parameters online.-In the current algorithm, the tuning parameters (i.e., the discount factor γ and the updating weight w in the proxy value) are selected by a simulation-based procedure based on HeartSteps V1 data and fixed during the study.</s><s xml:id="_dBBs4Bj">We did this mainly to ensure the stability of the algorithm.</s><s xml:id="_ZMVfhgv">However, as we discussed in Section 6, the optimal choice of these tuning parameters is likely person-specific.</s><s xml:id="_TY2GrD8">It would be interesting to design a method that evaluates and/or adjusts these tuning parameters for each participant as more data is collected, especially for a long study.</s></p><p xml:id="_SAF3V2P"><s xml:id="_KcwnYMY">Online monitoring.-The</s><s xml:id="_pauKtfJ">current algorithm has no mechanism to detect any sudden change of the participant's environment or any unusual user behavior (e.g., the participant becomes sick).</s><s xml:id="_BthmsC3">When these changes last for a long period of time, the algorithm needs a sufficient amount of data to adapt to the changes and may respond slowly.</s><s xml:id="_nvgWxRr">But the algorithm may not detect temporary changes at all and thus may provide too many treatments.</s><s xml:id="_vApFuqW">It would be interesting to draw on techniques developed in the change-point detection literature so that the RL algorithm can pick up these changes more quickly.</s></p><p xml:id="_8BJXgRJ"><s xml:id="_U2WZ38v">Pooling across participants.-The</s><s xml:id="_2Ant4ZY">algorithm described in this work learns the treatment policy separately for each participant (i.e., it is fully personalized).</s><s xml:id="_M2TpA7M">If the participants in the study are similar enough, pooling information from other participants (either those still in the study or those who have already finished) can speed up learning and achieve better performance, especially for those entering the study later.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10" xml:id="_gQt2qYd">CONCLUSION</head><p xml:id="_hrPXumk"><s xml:id="_PNKBQ9H">In this paper, we developed a reinforcement learning algorithm for use in HeartSteps V2.</s><s xml:id="_QzYJ34n">Preliminary validation of the algorithm demonstrates that it performs better than the Thompson Sampling Bandit algorithm in synthetic experiments constructed based on a previous study, HeartSteps V1.</s><s xml:id="_x4nAm8T">We also assessed the performance of the algorithm using pilot data from HeartSteps V2.</s><s xml:id="_wJSbxuU">After HeartSteps V2 is completed, the data gathered will be used to further assess the algorithm's performance and utility.</s><s xml:id="_nZ78GEV">An illustration of the study design and RL algorithm in HeartSteps V2 study.</s><s xml:id="_q9gW8PT">During each of the 90 days in the study, there are five user-specified decision times for potentially receiving a contextually-tailored walking suggestion message.</s><s xml:id="_u6aMehx">At each decision time, the availability is first assessed.</s><s xml:id="_8pZdkDm">If the user is not currently available for treatment (e.g., the user is already walking or driving a vehicle), no message is sent.</s><s xml:id="_MHvFd6g">Otherwise, the RL algorithm uses the current context (e.g., location) and a summary of past history (e.g., yesterday's app usage) to determine the randomization probability (i.e., π t ) for sending the message; see Section 5.3 for details.</s><s xml:id="_g56P995">After the five decision times in the day, the RL algorithm updates the treatment policy using the information collected during the day (e.g., the number of 30-minute step counts following each decision time); see Section 5.4 for details.</s><s xml:id="_4mfSazc">Testing performance for all three iterations in the cross validation.</s><s xml:id="_zsSVqvg">Each bar corresponds to the improvement of the total reward of the proposed algorithm with the selected inputs and tuning parameters in the training phase over the total reward achieved by Thompson Sampling Bandit algorithm for a single participant.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc><div><p xml:id="_uWVzhA3"><s xml:id="_kKptBP7">, k ( ⋅ ) where δ z( ⋅ ) is the Dirac measure.</s><s xml:id="_pqFg99s">For the transition model of the dosage variable, τ x′ | x, a , let p sed be the probability of delivering any anti-sedentary suggestions between decision times given no activity suggestion was sent at the previous decision time.</s><s xml:id="_bcgtsRJ">We set p sed = 0.2 based on the planned scheduling of anti-sedentary suggestions (an average of 1 anti-sedentary suggestion uniformly distributed in a 12-hour time window during the day implies approximately 0.2 probability of sending an anti-sedentary message between two decision times).</s><s xml:id="_33pZnAM">Then τ(x′ | x, a) is given by τ(x′ | x, 1) = 1 {x′ = λx + 1} , τ(x′ | x, 0) = p sed 1 {x′ = λx + 1} + (1 -p sed )1 {x′ = λx}.</s><s xml:id="_4VpF83y">Recall from Section 5.2 that λ = 0.95.</s><s xml:id="_RAPuBQ7">Lastly, we specify the reward function at available decision times by r(s, a) = g(s) ⊤ α 0 + af(s) ⊤ β where α 0 , β are the posterior means based on the model (3).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc><div><p xml:id="_XyHj47R"><s xml:id="_KtjARvg">Initial proxy value function.-Recall</s><s xml:id="_KY7e5yk">that the proxy value function requires the specification of the (1) context distribution, (2) availability probability, (3) the transition model of dosage, and (4) reward function (for available and unavailable times), as well as the discount factor γ; see Section 5.4.2.</s><s xml:id="_ZeWD836">We form the initial proxy using the training batch by setting (1) the empirical distribution in the training batch, (2) the empirical availability probability in the training batch, (3) the average probability of receiving anti-sedentary message between decision time and (4) the reward estimates from population GEE.Generative model to select tuning parameters.-Recall that the tuning parameters are (γ,w), corresponding to the discount rate in defining the proxy value and the updating weight in forming the estimated proxy value.</s><s xml:id="_pPDNf8k">The tuning parameters are chosen to optimize the total simulated rewards using the generative model of participants in the training batch.</s><s xml:id="_cDJ7kMR">Below we describe how we form the generative model.</s><s xml:id="_qmfsMKX">For participant i, we first construct a 90-day sequence of context, availability, residuals</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc><div><p xml:id="_fRqep24"><s xml:id="_tvYA8DK">Fig. 1.</s></p></div></figDesc><graphic coords="25,84.24,62.00,503.51,247.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc><div><p xml:id="_3GATJEr"><s xml:id="_2JPKX4E">Fig. 2.</s></p></div></figDesc><graphic coords="26,192.00,62.00,288.00,174.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 .</head><label>3</label><figDesc><div><p xml:id="_9sHqGNC"><s xml:id="_Ur4bDez">Fig. 3. Participant ID = 4. Left: the randomization probability at the available decision times.</s><s xml:id="_WfbUz3e">The x-axis is the time stamp.</s><s xml:id="_e8MB2Kn">The y-axis is the randomization probability.</s><s xml:id="_RjFYw6f">Right: the posterior mean estimates of treatment effect at the available times.</s><s xml:id="_UeKhQyw">The x-axis is the time stamp.</s><s xml:id="_5nCJKGN">The y-axis is the posterior mean (i.e., f(s) ⊤ μ d ) .</s></p></div></figDesc><graphic coords="27,84.49,62.00,503.03,123.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 .</head><label>4</label><figDesc><div><p xml:id="_mfha2ST"><s xml:id="_ptEEWGp">Fig. 4.Participant ID = 7. Left: the randomization probability at the available decision times.</s><s xml:id="_ZVAmWKN">The x-axis is the time stamp.</s><s xml:id="_Qc4qdyc">The y-axis is the randomization probability.</s><s xml:id="_Krkmjwk">The black points corresponds to the actual randomization probability and the red points corresponds to the randomization probability without the proxy adjustment (i.e., η d = 0).</s><s xml:id="_GgnqQa8">Right: the posterior mean estimates of treatment effect at the available times.</s><s xml:id="_K4eDgrr">The x-axis is the time stamp.</s><s xml:id="_NGGe3pu">The y-axis is the posterior mean estimates (i.e., f(s) ⊤ μ d ).</s></p></div></figDesc><graphic coords="28,84.49,62.00,503.03,123.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc><div><p xml:id="_qqs9BH5"><s xml:id="_FcG9tnH">Prior distribution of α 0</s></p></div></figDesc><table><row><cell>Variable</cell><cell cols="2">Mean Std.</cell></row><row><cell>Intercept</cell><cell>0</cell><cell>1.43</cell></row><row><cell>Yesterday's step count</cell><cell>1.67</cell><cell>2.67</cell></row><row><cell cols="2">Prior 30-minute step count 3.79</cell><cell>1.55</cell></row><row><cell>Other Location</cell><cell>0</cell><cell>0.43</cell></row><row><cell>Temperature</cell><cell>0</cell><cell>1.63</cell></row><row><cell>Work Location</cell><cell>0</cell><cell>0.84</cell></row><row><cell>Step variation level</cell><cell>0</cell><cell>0.45</cell></row><row><cell>Dosage</cell><cell>0</cell><cell>1.67</cell></row><row><cell>App Engagement</cell><cell>0</cell><cell>1.33</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_DWqcgpB"><s xml:id="_EM2R646">Proc ACM Interact Mob Wearable Ubiquitous Technol.</s><s xml:id="_T6WWDmf">Author manuscript; available in PMC 2021 September 14.</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_hw4QS9Z">Acknowledgments</head><p xml:id="_AsTaNVD"><s xml:id="_X5x2Myc">Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.</s><s xml:id="_2TjVYQ7">Copyrights for components of this work owned by others than ACM must be honored.</s><s xml:id="_Y66CHFd">Abstracting with credit is permitted.</s><s xml:id="_hRqFh6V">To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.</s><s xml:id="_7gtJGhh">Request permissions from permissions@acm.org.</s></p></div>
<div><head xml:id="_rHd8yRx">Model Assumption</head></div>
<div><head xml:id="_6hEukyZ">Immediate treatment effect</head><p xml:id="_TGqmghS"><s xml:id="_KN9jpe9">Time-invariant linear baseline reward model Immediate <rs type="projectName">treatment effect Time-invariant linear treatment effect model Immediate treatment effect i</rs>.i.d.</s><s xml:id="_MnQCxc2">Gaussian error Proxy delayed effect The states follows a Markov Decision Process with the i.i.d.</s><s xml:id="_gsGRn9a">context and availability and dosage transition τ(x′ | x, a)</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_f2gysZC">
					<orgName type="project" subtype="full">treatment effect Time-invariant linear treatment effect model Immediate treatment effect i</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZbKvexd">Author Manuscript</head><p xml:id="_Wjx5kcT"><s xml:id="_rHFnBnW">LIAO et al.</s></p><p xml:id="_C8CS5HS"><s xml:id="_Z8uHWNx">Page 30</s></p><p xml:id="_ZTgGmzu"><s xml:id="_8AuZHH7">Table <ref type="table">2</ref>.</s></p><p xml:id="_ZRDX5V4"><s xml:id="_4BBN4qt">The list of parameters used in pilot HeartSteps V2.</s></p><p xml:id="_FUMu6mv"><s xml:id="_W9JccDn">Feature vector in modeling reward Table <ref type="table">3</ref> HeartSteps V1 data analysis (Sec.</s><s xml:id="_6jfQ4ad">6.2)  The list of selected features in HeartSteps V2 study.</s></p><p xml:id="_qaWzy87"><s xml:id="_swRWvXm">Step count variation is the standard deviation of the 60-min step count centered around the current decision time over the past seven days and then thresholded by the median of the past standard deviation.</s><s xml:id="_F2nj8AT">The app engagement is a binary indicator of whether the number of screens encountered in the app over the prior day is greater than the 40% quantile of the screens collected over the last seven days.</s><s xml:id="_mT2DGuu">All of the variables are in the baseline feature vector g(s).</s><s xml:id="_EvuZJuD">Location takes three possible values: home, work, and other.</s><s xml:id="_DfRkuyS">The third column indicates whether the variable is in the treatment effect feature vector f(s).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NqqYpxD">Variable Type</head><p xml:id="_bKynZdE"><s xml:id="_RGUUYVy">In treatment effect model?</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_daWFA4N">Thompson sampling for contextual bandits with linear payoffs</title>
		<author>
			<persName><forename type="first">Agrawal</forename><surname>Shipra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goyal</forename><surname>Navin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VXzsCY7">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
	<note type="raw_reference">Agrawal Shipra and Goyal Navin. 2013. Thompson sampling for contextual bandits with linear payoffs. In International Conference on Machine Learning. 127-135.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_8BTVbh9">Mitigating Planner Overfitting in Model-Based Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Arumugam</forename><surname>Dilip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asadi</forename><surname>Kavosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopalan</forename><surname>Nakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grimm</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Jun Ki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lehnert</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Littman</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01129</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Arumugam Dilip, Abel David, Asadi Kavosh, Gopalan Nakul, Grimm Christopher, Lee Jun Ki, Lehnert Lucas, and Littman Michael L. 2018. Mitigating Planner Overfitting in Model-Based Reinforcement Learning. arXiv preprint arXiv:1812.01129 (2018).</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_Xfmnqta">Control engineering methods for the design of robust behavioral treatments</title>
		<author>
			<persName><forename type="first">Bekiroglu</forename><surname>Korkut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lagoa</forename><surname>Constantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Suzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lanza</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<idno type="PMID">28344431</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5R7AYfp">IEEE Transactions on Control Systems Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="979" to="990" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bekiroglu Korkut, Lagoa Constantino, Murphy Suzan A, and Lanza Stephanie T. 2016. Control engineering methods for the design of robust behavioral treatments. IEEE Transactions on Control Systems Technology25, 3 (2016), 979-990. [PubMed: 28344431]</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_t7gqGtK">Assessing time-varying causal effect moderation in mobile health</title>
		<author>
			<persName><forename type="first">Boruvka</forename><surname>Audrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Almirall</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Witkiewitz</forename><surname>Katie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3qUes4a">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="1112" to="1121" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Boruvka Audrey, Almirall Daniel, Witkiewitz Katie, and Murphy Susan A. 2018. Assessing time-varying causal effect moderation in mobile health. J. Amer. Statist. Assoc113, 523 (2018), 1112-1121.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_zk7PmzW">An empirical evaluation of thompson sampling</title>
		<author>
			<persName><forename type="first">Chapelle</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lihong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NQH5u3k">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chapelle Olivier and Li Lihong. 2011. An empirical evaluation of thompson sampling. In Advances in Neural Information Processing Systems. 2249-2257.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_F9jxeAC">Randomised trials for the Fitbit generation</title>
		<author>
			<persName><forename type="first">Dempsey</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klasnja</forename><surname>Pedja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nahum-Shani</forename><surname>Inbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="PMID">26807137</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZTEH3pd">Significance</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dempsey Walter, Liao Peng, Klasnja Pedja, Nahum-Shani Inbal, and Murphy Susan A. 2015. Randomised trials for the Fitbit generation. Significance12, 6 (2015), 20-23. [PubMed: 26807137]</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main" xml:id="_w2Kp4kt">Estimation considerations in contextual bandits</title>
		<author>
			<persName><forename type="first">Dimakopoulou</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athey</forename><surname>Zhou Zhengyuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imbens</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><surname>Guido</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07077</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Dimakopoulou Maria, Zhou Zhengyuan, Athey Susan, and Imbens Guido. 2017. Estimation considerations in contextual bandits. arXiv preprint arXiv:1711.07077 (2017).</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_D99rsgw">Habituation: effects of regular and stochastic stimulation</title>
		<author>
			<persName><forename type="first">Dimitrijević</forename><surname>Milan Radovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faganel</forename><surname>Janez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregorić</forename><surname>Matej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Trontelj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KFSwGnv">Journal of Neurology, Neurosurgery &amp; Psychiatry</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="1972">1972. 1972</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dimitrijević Milan Radovan, Faganel Janez, Gregorić Matej, Nathan PW, and Trontelj JK. 1972. Habituation: effects of regular and stochastic stimulation. Journal of Neurology, Neurosurgery &amp; Psychiatry35, 2 (1972), 234-242.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_hCZKeq9">An optimistic posterior sampling strategy for bayesian reinforcement learning</title>
		<author>
			<persName><forename type="first">Fonteneau</forename><surname>Raphaël</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korda</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munos</forename><surname>Rémi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ujgHpae">Neural Information Processing Systems 2013 Workshop on Bayesian Optimization</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fonteneau Raphaël, Korda Nathan, and Munos Rémi. 2013. An optimistic posterior sampling strategy for bayesian reinforcement learning. Neural Information Processing Systems 2013 Workshop on Bayesian Optimization (2013).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_cQKEFAp">Can the artificial intelligence technique of reinforcement learning use continuously monitored digital data to optimize treatment for weight loss</title>
		<author>
			<persName><forename type="first">M</forename><surname>Forman Evan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerrigan</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Butryn</forename><surname>Meghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Juarascio</forename><surname>Adrienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Manasse</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ontañón</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallal</forename><surname>Diane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Crochiere</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Moskow</forename><surname>Danielle</surname></persName>
		</author>
		<idno type="PMID">28712010</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xZZQBRk">Journal of behavioral medicine</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Forman Evan M, Kerrigan Stephanie G, Butryn Meghan L, Juarascio Adrienne S, Manasse Stephanie M, Ontañón Santiago, Dallal Diane H, Crochiere Rebecca J, and Moskow Danielle. 2018. Can the artificial intelligence technique of reinforcement learning use continuously monitored digital data to optimize treatment for weight loss?Journal of behavioral medicine (2018), 1-15. [PubMed: 28712010]</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_ftuCkhS">How to discount deep reinforcement learning: Towards new dynamic strategies</title>
		<author>
			<persName><forename type="first">François-Lavet</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fonteneau</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernst</forename><surname>Damien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mnst9a9">Neural Information Processing Systems 2015 Workshop on Deep Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">François-Lavet Vincent, Fonteneau Raphael, and Ernst Damien. 2015. How to discount deep reinforcement learning: Towards new dynamic strategies. Neural Information Processing Systems 2015 Workshop on Deep Reinforcement Learning (2015).</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_pQGSccW">Misspecified linear bandits</title>
		<author>
			<persName><forename type="first">Ghosh</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chowdhury</forename><surname>Sayak Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gopalan</forename><surname>Aditya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_K83yR46">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ghosh Avishek, Chowdhury Sayak Ray, and Gopalan Aditya. 2017. Misspecified linear bandits. In Thirty-First AAAI Conference on Artificial Intelligence.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_UmFadAA">Action centered contextual bandits</title>
		<author>
			<persName><forename type="first">Greenewald</forename><surname>Kristjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tewari</forename><surname>Ambuj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klasnja</forename><surname>Predag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_q8SaDXG">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5977" to="5985" />
		</imprint>
	</monogr>
	<note type="raw_reference">Greenewald Kristjan, Tewari Ambuj, Murphy Susan, and Klasnja Predag. 2017. Action centered contextual bandits. In Advances in Neural Information Processing Systems. 5977-5985.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_3tjNWq5">The dependence of effective planning horizon on model accuracy</title>
		<author>
			<persName><forename type="first">Kulesza</forename><surname>Jiang Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Singh</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><surname>Richard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kUepChx">Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems</title>
		<meeting>the 2015 International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1181" to="1189" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jiang Nan, Kulesza Alex, Singh Satinder, and Lewis Richard. 2015. The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 1181-1189.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_sqR28Jb">Doubly Robust Off-policy Value Evaluation for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lihong</surname></persName>
		</author>
		<idno type="DOI">10.2514/6.2023-0967.vid</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rBkq3js">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="652" to="661" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jiang Nan and Li Lihong. 2016. Doubly Robust Off-policy Value Evaluation for Reinforcement Learning. In International Conference on Machine Learning. 652-661.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_nqSf25A">Thompson sampling: An asymptotically optimal finite-time analysis</title>
		<author>
			<persName><forename type="first">Korda</forename><surname>Kaufmann Emilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munos</forename><surname>Nathaniel</surname></persName>
		</author>
		<author>
			<persName><surname>Rémi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VGKq8Pt">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="199" to="213" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kaufmann Emilie, Korda Nathaniel, and Munos Rémi. 2012. Thompson sampling: An asymptotically optimal finite-time analysis. In International Conference on Algorithmic Learning Theory. Springer, 199-213.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_HZbc6Rv">Using wearable sensors and real time inference to understand human recall of routine activities</title>
		<author>
			<persName><forename type="first">Klasnja</forename><surname>Predrag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrison</forename><surname>Beverly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Legrand</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lamarca</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Froehlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hudson</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fRkbvdU">Proceedings of the 10th International Conference on Ubiquitous Computing</title>
		<meeting>the 10th International Conference on Ubiquitous Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="154" to="163" />
		</imprint>
	</monogr>
	<note type="raw_reference">Klasnja Predrag, Harrison Beverly L, LeGrand Louis, LaMarca Anthony, Froehlich Jon, and Hudson Scott E. 2008. Using wearable sensors and real time inference to understand human recall of routine activities. In Proceedings of the 10th International Conference on Ubiquitous Computing. ACM, 154-163.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_PemJjZa">Micro-randomized trials: An experimental design for developing just-in-time adaptive interventions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Hekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boruvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Sa</surname></persName>
		</author>
		<idno type="DOI">10.1037/hea0000305</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BTgHnbx">Health Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">1220</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Klasnja P, Hekler EB, Shiffman S, Boruvka A, Almirall D, Tewari A, and Murphy SA2015. Micro-randomized trials: An experimental design for developing just-in-time adaptive interventions. Health Psychology34, S (2015), 1220.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_8wf35Ww">Efficacy of contextually tailored suggestions for physical activity: A micro-randomized optimization trial of HeartSteps</title>
		<author>
			<persName><forename type="first">Klasnja</forename><surname>Predrag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename><surname>Shawna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seewald</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Andy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hall</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luers</forename><surname>Brook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hekler</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EAHVyT3">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="573" to="582" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Klasnja Predrag, Smith Shawna, Seewald Nicholas J, Lee Andy, Hall Kelly, Luers Brook, Hekler Eric B, and Murphy Susan A. 2018. Efficacy of contextually tailored suggestions for physical activity: A micro-randomized optimization trial of HeartSteps. Annals of Behavioral Medicine53, 6 (2018), 573-582.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_HjktSNp">Semiparametric contextual bandits</title>
		<author>
			<persName><forename type="first">Krishnamurthy</forename><surname>Akshay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Zhiwei Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syrgkanis</forename><surname>Vasilis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04204</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXivpreprint</note>
	<note type="raw_reference">Krishnamurthy Akshay, Wu Zhiwei Steven, and Syrgkanis Vasilis. 2018. Semiparametric contextual bandits. arXivpreprint arXiv:1803.04204 (2018).</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_nscvJjY">On value function representation of long horizon problems</title>
		<author>
			<persName><forename type="first">Lehnert</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Laroche</surname></persName>
		</author>
		<author>
			<persName><surname>Van Seijen</surname></persName>
		</author>
		<author>
			<persName><surname>Harm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Rw4rFfh">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lehnert Lucas, Laroche Romain, and van Seijen Harm. 2018. On value function representation of long horizon problems. In Thirty-Second AAAI Conference on Artificial Intelligence.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_4efFGjW">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Lihong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Langford</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schapire</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EUvd6WJ">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">Li Lihong, Chu Wei, Langford John, and Schapire Robert E. 2010. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web. ACM, 661-670.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_RdNgRYk">Longitudinal data analysis using generalized linear models</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Kung-Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeger</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EJUq24b">Biometrika</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="22" />
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liang Kung-Yee and Zeger Scott L. 1986. Longitudinal data analysis using generalized linear models. Biometrika 73, 1 (1986), 13-22.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_Y76NCQJ">Just-in-time but not too much: Determining treatment timing in mobile health</title>
		<author>
			<persName><forename type="first">Liao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dempsey</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarker</forename><surname>Hillol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossain</forename><surname>Syed Monowar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al'absi</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klasnja</forename><surname>Predrag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jhZUykQ">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">179</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liao Peng, Dempsey Walter, Sarker Hillol, Hossain Syed Monowar, Al&apos;Absi Mustafa, Klasnja Predrag, and Murphy Susan. 2018. Just-in-time but not too much: Determining treatment timing in mobile health. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies2, 4 (2018), 179.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_rTjhThB">Sample size calculations for micro-randomized trials in mHealth</title>
		<author>
			<persName><forename type="first">Liao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klasnja</forename><surname>Predrag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tewari</forename><surname>Ambuj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<idno type="PMID">26707831</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5qx8myS">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1944" to="1971" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liao Peng, Klasnja Predrag, Tewari Ambuj, and Murphy Susan A. 2016. Sample size calculations for micro-randomized trials in mHealth. Statistics in Medicine35, 12 (2016), 1944-1971. [PubMed: 26707831]</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_p9Rf4Vp">Development of a control-oriented model of social cognitive theory for optimized mHealth behavioral interventions</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Cesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rivera</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hekler</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Riley</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Buman</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Magann</forename><surname>Alicia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_t9f4NcF">IEEE Transactions on Control Systems Technology</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Martin Cesar A, Rivera Daniel E, Hekler Eric B, Riley William T, Buman Matthew P, Adams Marc A, and Magann Alicia B. 2018. Development of a control-oriented model of social cognitive theory for optimized mHealth behavioral interventions. IEEE Transactions on Control Systems Technology (2018).</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_wyEGv2T">Non-stationary bandits with habituation and recovery dynamics</title>
		<author>
			<persName><forename type="first">Aswani</forename><surname>Mintz Yonatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaminsky</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flowers</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fukuoka</forename><surname>Elena</surname></persName>
		</author>
		<author>
			<persName><surname>Yoshimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ruH4nhT">Operations Research</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>to appear</note>
	<note type="raw_reference">Mintz Yonatan, Aswani Anil, Kaminsky Philip, Flowers Elena, and Fukuoka Yoshimi. 2019. Non-stationary bandits with habituation and recovery dynamics. Operations Research (2019). to appear.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_DK5Pw2S">Just-in-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support</title>
		<author>
			<persName><forename type="first">Nahum-Shani</forename><surname>Inbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename><surname>Shawna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Spring</forename><surname>Bonnie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Collins</forename><surname>Linda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Witkiewitz</forename><surname>Katie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tewari</forename><surname>Ambuj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5EGVBMY">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="446" to="462" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nahum-Shani Inbal, Smith Shawna N, Spring Bonnie J, Collins Linda M, Witkiewitz Katie, Tewari Ambuj, and Murphy Susan A. 2017. Just-in-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support. Annals of Behavioral Medicine52, 6 (2017), 446-462.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_JrWpcUG">More) efficient reinforcement learning via posterior sampling</title>
		<author>
			<persName><forename type="first">Osband</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russo</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TSpXeNs">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3003" to="3011" />
		</imprint>
	</monogr>
	<note type="raw_reference">Osband Ian, Russo Daniel, and Van Roy Benjamin. 2013. (More) efficient reinforcement learning via posterior sampling. In Advances in Neural Information Processing Systems. 3003-3011.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main" xml:id="_7uEkCR4">On optimistic versus randomized exploration in reinforcement learning</title>
		<author>
			<persName><forename type="first">Osband</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName><surname>Van Roy Benjamin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04241</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Osband Ian and Van Roy Benjamin. 2017. On optimistic versus randomized exploration in reinforcement learning. arXiv preprint arXiv:1706.04241 (2017).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_wrtSH7y">Why is Posterior Sampling Better than Optimism for Reinforcement Learning?</title>
		<author>
			<persName><forename type="first">Osband</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName><surname>Van Roy Benjamin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tjT8WQa">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
	<note type="raw_reference">Osband Ian and Van Roy Benjamin. 2017. Why is Posterior Sampling Better than Optimism for Reinforcement Learning?. In International Conference on Machine Learning. 2701-2710.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_4P479TD">Learning unknown markov decision processes: A thompson sampling approach</title>
		<author>
			<persName><forename type="first">Ouyang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gagrani</forename><surname>Mukul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayyar</forename><surname>Ashutosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jain</forename><surname>Rahul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BRERnQq">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1333" to="1342" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ouyang Yi, Gagrani Mukul, Nayyar Ashutosh, and Jain Rahul. 2017. Learning unknown markov decision processes: A thompson sampling approach. In Advances in Neural Information Processing Systems. 1333-1342.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_ffJsVBm">PopTherapy: coping with stress through pop-culture</title>
		<author>
			<persName><forename type="first">Gilad-Bachrach</forename><surname>Paredes Pablo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Czerwinski</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roseway</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Asta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hernandez</forename><surname>Kael</surname></persName>
		</author>
		<author>
			<persName><surname>Javier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CUuZMj2">Proceedings of the 8th International Conference on Pervasive Computing Technologies for Healthcare. ICST</title>
		<meeting>the 8th International Conference on Pervasive Computing Technologies for Healthcare. ICST</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
		<respStmt>
			<orgName>Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Paredes Pablo, Gilad-Bachrach Ran, Czerwinski Mary, Roseway Asta, Rowan Kael, and Hernandez Javier. 2014. PopTherapy: coping with stress through pop-culture. In Proceedings of the 8th International Conference on Pervasive Computing Technologies for Healthcare. ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering), 109-117.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_yprYaxR">MyBehavior: automatic personalized health feedback from user behaviors and preferences using smartphones</title>
		<author>
			<persName><forename type="first">Rabbi</forename><surname>Mashfiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Hane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choudhury</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><surname>Tanzeem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bjhbFT9">Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="707" to="718" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi Mashfiqui, Aung Min Hane, Zhang Mi, and Choudhury Tanzeem. 2015. MyBehavior: automatic personalized health feedback from user behaviors and preferences using smartphones. In Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing. ACM, 707-718.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_KUkU93N">Intensively adaptive interventions using control systems engineering: Two illustrative examples</title>
		<author>
			<persName><forename type="first">Rivera</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hekler</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Savage</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Symons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VFAZKYJ">Optimization of Behavioral, Biobehavioral, and Biomedical Interventions</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="121" to="173" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rivera Daniel E, Hekler Eric B, Savage Jennifer S, and Downs Danielle Symons. 2018. Intensively adaptive interventions using control systems engineering: Two illustrative examples. In Optimization of Behavioral, Biobehavioral, and Biomedical Interventions. Springer, 121-173.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_cPSSTbv">Learning to optimize via posterior sampling</title>
		<author>
			<persName><forename type="first">Russo</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Van</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6F9nXaR">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1221" to="1243" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Russo Daniel and Van Roy Benjamin. 2014. Learning to optimize via posterior sampling. Mathematics of Operations Research 39, 4 (2014), 1221-1243.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_8ac6Axv">A tutorial on thompson sampling</title>
		<author>
			<persName><forename type="first">Russo</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Roy Benjamin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazerouni</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osband</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Sg7VSnJ">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="96" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Russo Daniel J, Van Roy Benjamin, Kazerouni Abbas, Osband Ian, Wen Zheng, et al.2018. A tutorial on thompson sampling. Foundations and Trends® in Machine Learning11, 1 (2018), 1-96.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main" xml:id="_2uYfjBR">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">Sutton</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Barto</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton Richard S and Barto Andrew G. 2018. Reinforcement learning: An introduction. MIT press.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_NhKQCwX">From ads to interventions: Contextual bandits in mobile health</title>
		<author>
			<persName><forename type="first">Tewari</forename><surname>Ambuj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Susan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Rk75Crb">Mobile Health</title>
		<imprint>
			<biblScope unit="page" from="495" to="517" />
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Tewari Ambuj and Murphy Susan A. 2017. From ads to interventions: Contextual bandits in mobile health. In Mobile Health. Springer, 495-517.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_JPwx4BJ">Data-efficient off-policy policy evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brunskill</forename><surname>Emma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3STNdfG">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2139" to="2148" />
		</imprint>
	</monogr>
	<note type="raw_reference">Thomas Philip and Brunskill Emma. 2016. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning. 2139-2148.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_jkyyuZY">Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system</title>
		<author>
			<persName><forename type="first">Yom-Tov</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feraru</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kozdoba</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannor</forename><surname>Shie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tennenholtz</forename><surname>Moshe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hochberg</forename><surname>Irit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tcjqtrH">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yom-Tov Elad, Feraru Guy, Kozdoba Mark, Mannor Shie, Tennenholtz Moshe, and Hochberg Irit. 2017. Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system. Journal of medical Internet research19, 10 (2017).</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_7HcHYmf">Personalizing Mobile Fitness Apps using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mintz</forename><surname>Yonatan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fukuoka</forename><surname>Yoshimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goldberg</forename><surname>Ken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flowers</forename><surname>Elena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaminsky</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Castillejo</forename><surname>Alejandro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aswani</forename><surname>Anil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QRMN3PE">Companion Proceedings of the 23rd International on Intelligent User Interfaces: 2nd Workshop on Theory-Informed User Modeling for Tailoring and Personalizing Interfaces (HUMANIZE)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou Mo, Mintz Yonatan, Fukuoka Yoshimi, Goldberg Ken, Flowers Elena, Kaminsky Philip, Castillejo Alejandro, and Aswani Anil. 2018. Personalizing Mobile Fitness Apps using Reinforcement Learning. In Companion Proceedings of the 23rd International on Intelligent User Interfaces: 2nd Workshop on Theory-Informed User Modeling for Tailoring and Personalizing Interfaces (HUMANIZE).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
