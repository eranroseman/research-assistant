<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_aDuB32H">Adaptive Multi-Agent Deep Reinforcement Learning for Timely Healthcare Interventions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thanveer</forename><surname>Shaik</surname></persName>
							<email>thanveer.shaik@unisq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Xiaohui</forename><surname>Tao</surname></persName>
							<email>xiaohui.tao@unisq.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Xie</surname></persName>
							<email>hrxie@ln.edu.hk</email>
						</author>
						<author>
							<persName><forename type="first">Hong-Ning</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
							<email>zhaof@hust.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jianming</forename><surname>Yong</surname></persName>
							<email>jianming.yong@unisq.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">the School of Mathematics, Physics &amp; Computing , University of Southern Queensland , Queensland , Australia</note>
								<orgName type="department" key="dep1">School of Mathematics</orgName>
								<orgName type="department" key="dep2">Physics &amp; Computing</orgName>
								<orgName type="institution">University of Southern Queensland</orgName>
								<address>
									<settlement>Queensland</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">School of Computer and Artificial Intelligence , Wuhan University of Technology , China</note>
								<orgName type="department">School of Computer and Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">Department of Computing and Decision Sciences , Lingnan University , Tuen Mun , Hong Kong</note>
								<orgName type="department">Department of Computing and Decision Sciences</orgName>
								<orgName type="institution">Lingnan University</orgName>
								<address>
									<addrLine>Tuen Mun</addrLine>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">Department of Computer Science , Hong Kong Baptist University , Hong Kong</note>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<note type="raw_affiliation">Huazhong University of Science and Technology , Wuhan , China School of Business ,</note>
								<orgName type="department">School of Business</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<note type="raw_affiliation">University of Southern Queensland ,</note>
								<orgName type="institution">University of Southern Queensland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_FndpSH8">Adaptive Multi-Agent Deep Reinforcement Learning for Timely Healthcare Interventions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">16F0A9D4214A7E5AC3927C8FDEDC1D51</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T05:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_e78E7Ef">Behavior Patterns</term>
					<term xml:id="_4bE2r2Y">Decision Making</term>
					<term xml:id="_m8HM7G2">Patient Monitoring</term>
					<term xml:id="_PExZPqJ">Reinforcement Learning</term>
					<term xml:id="_AQMpGhc">Vital Signs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_9aHeUe3"><p xml:id="_batFY6K"><s xml:id="_ZDD99qM">Effective patient monitoring is vital for timely interventions and improved healthcare outcomes.</s><s xml:id="_jmXea8n">Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions.</s><s xml:id="_rGgZDHp">To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL).</s><s xml:id="_M3tg3G7">Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature.</s><s xml:id="_ydDmGpY">These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated.</s><s xml:id="_cvPNGrz">In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD.</s><s xml:id="_xqy292j">We compare the results with several baseline models, including Q-Learning, PPO, Actor-Critic, Double DQN, and DDPG, as well as monitoring frameworks like WISEML and CA-MAQL.</s><s xml:id="_WcWf9n3">Our experiments demonstrate that the proposed DRL approach outperforms all other baseline models, achieving more accurate monitoring of patient's vital signs.</s><s xml:id="_5V83jAz">Furthermore, we conduct hyperparameter optimization to fine-tune the learning process of each agent.</s><s xml:id="_JPGm2zF">By optimizing hyperparameters, we enhance the learning rate and discount factor, thereby improving the agents' overall performance in monitoring patient health status.</s><s xml:id="_UKspf6Z">Our AIdriven patient monitoring system offers several advantages over traditional methods, including the ability to handle complex and uncertain environments, adapt to varying patient conditions, and make real-time decisions without external supervision.</s><s xml:id="_CgNTzcs">However, we identify limitations related to data scale and prediction of future vital signs, paving the way for future research directions.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YBXUxTM">I. INTRODUCTION</head><p xml:id="_kKXhKhh"><s xml:id="_pBJQfrx">In the dynamic domain of healthcare, the significance of informed decision-making cannot be overstated.</s><s xml:id="_v972286">With the advent of continuous patient monitoring systems, it has become possible to remotely track vital signs and physical movements, thereby enhancing the decision-making capabilities of clinicians <ref type="bibr" target="#b1">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>.</s><s xml:id="_Gvk6hQ7">The application of machine learning models to analyze transmitted vital sign data has seen a significant uptick in various healthcare applications, ranging from pre-clinical data processing and diagnosis assistance to early warning detection of health deterioration, treatment decision-making, and drug prescription <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>.</s><s xml:id="_p3HhNu4">In this context, the monitoring of human behavior patterns plays a crucial role, especially for remote patient monitoring in hospitals or through Internet of Things (IoT)-enabled home monitoring systems.</s></p><p xml:id="_8gAst2J"><s xml:id="_fhDjJ4j">Traditionally, methodologies in this field have predominantly relied on unsupervised and supervised learning techniques to identify patterns and classify patients' activities and vital signs <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>.</s><s xml:id="_uMaTctY">However, these techniques are limited in their capacity to only observe data and suggest potential decisions without the ability to act upon these observations.</s><s xml:id="_3JbmxXA">In contrast, Reinforcement Learning (RL) introduces a novel paradigm by deploying learning agents within complex and uncertain environments.</s><s xml:id="_Kkdcbj2">These agents are empowered to explore and exploit the environment through actions, learning from the outcomes of their actions <ref type="bibr" target="#b7">[7]</ref>.</s><s xml:id="_xbKTz5j">A cornerstone of RL is its reward mechanism, which provides the agent with feedback in the form of rewards for its actions.</s><s xml:id="_sWQBekz">These rewards serve as crucial signals that guide the learning process of the agent, encouraging actions that lead to favorable outcomes and discouraging detrimental ones.</s><s xml:id="_vuUjyEX">This reward system is instrumental in enabling the agent to iteratively refine its strategy based on the consequences of its actions, thereby enhancing its performance over time.</s><s xml:id="_gGXVMNw">The versatility of RL has been demonstrated in various dynamic domains, such as stock market trading <ref type="bibr" target="#b8">[8]</ref>, and is increasingly being adapted for healthcare applications, including diagnostic decisions and dynamic treatment regimes that arXiv:2309.10980v4</s><s xml:id="_W99krDE">[cs.LG] 28 Oct 2024 require the consideration of delayed feedback <ref type="bibr" target="#b9">[9]</ref>.</s><s xml:id="_rdmBWkK">Specifically, RL-based patient monitoring applications have focused on optimizing the timing and dosage of medications to ensure their correct administration <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>.</s><s xml:id="_bEqdakA">The analogy of probabilistic machine learning models, such as RL, to an ICU clinician monitoring a patient's state and making subsequent decisions based on observed changes, underscores the potential of RL in healthcare <ref type="bibr" target="#b12">[12]</ref>.</s></p><p xml:id="_4NqSESZ"><s xml:id="_xY7UWkQ">This study addresses the challenge of monitoring multiple vital signs of the human body, tracking health status, and enabling timely interventions during emergencies by proposing an innovative approach that employs multiple deep learning agents within a healthcare monitoring environment.</s><s xml:id="_2dau3q4">Each agent is responsible for monitoring specific vital signs, progressively learning threshold levels based on Modified Early Warning Scores (MEWS) and rewards accumulated from previous iterations.</s><s xml:id="_jNYCPMd">These well-trained deep reinforcement learning (DRL) agents are capable of monitoring parameters such as heart rate, respiration rate, and temperature, alerting clinical teams in case of deviations from predefined thresholds.</s><s xml:id="_MjmeFUE">While reinforcement learning (RL) has been applied in gaming environments like Deep Q-Networks (DQN) and sensor-based systems, this study offers novel contributions by integrating multi-agent reinforcement learning (MARL) within healthcare, creating a real-time, autonomous patient monitoring system.</s><s xml:id="_X3D69vY">Unlike conventional RL models that focus on singular tasks, this approach uses multiple agents to monitor different physiological parameters simultaneously, enabling concurrent learning and more dynamic, comprehensive system responses.</s><s xml:id="_jjAZeVh">Additionally, the study introduces a novel reward mechanism that optimizes agent behavior based on MEWS, allowing agents to learn more efficiently and facilitate timely medical interventions, thus enhancing healthcare decision-making through a multiagent, clinically informed AI framework.</s><s xml:id="_e2UVpGk">This represents a significant advancement in AI-driven patient monitoring.</s></p><p xml:id="_mGbrRaN"><s xml:id="_5rTsecx">The primary aim of this study is to learn human behavior patterns in the context of clinical health by deploying a DRL agent for each physiological feature.</s><s xml:id="_mNAJMxS">These agents are designed to monitor, learn, and alert the respective clinical teams if any vital signs deviate from the norms established by MEWS.</s><s xml:id="_ah3DhYy">We introduce a novel approach for rewarding the actions of RL agents to facilitate the learning of behavior patterns.</s><s xml:id="_uETghRu">The generic monitoring environment developed in this study supports multi-agent functionality to monitor various vital signs of a patient, thereby introducing a new paradigm for remotely monitoring patients' health status using a multi-agent DRL environment as shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</s></p><p xml:id="_ZpFCnYN"><s xml:id="_Q8AB37Z">The contributions of this study are as follows:</s></p><p xml:id="_KdnaFWq"><s xml:id="_hcn5WNR">• Introduction of a novel approach for rewarding RL agents' actions to foster the learning of behavior patterns.</s><s xml:id="_7jJJ4fc">• Development of a generic monitoring environment that accommodates multi-agents for monitoring various vital signs of a patient.</s><s xml:id="_TPw6Bun">• Establishment of a new paradigm for remotely monitoring patients' health status utilizing the multi-agent DRL environment.</s><s xml:id="_H7tJGmc">The paper is organized as follows: Section II presents related works in the RL community, specifically focusing on learning human behavior patterns and applications in the healthcare domain.</s><s xml:id="_8ys744P">The research problem formulation and the proposed multi-agent DRL methodology are detailed in Section III.</s><s xml:id="_utuyd9t">Section IV evaluates the proposed methodology on 10 different subject vital signs, and baseline models are discussed.</s><s xml:id="_AF2FksM">In Section V, the results of the proposed approach are compared with baseline models, and hyper-parameter optimization of the learning rate and discount factor is discussed.</s><s xml:id="_tj7sRrP">Based on the results, applications of the proposed framework are discussed in Section VI.</s><s xml:id="_EenAfU2">Section VII concludes the paper, including limitations and future work.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZZnVNY7">II. RELATED WORKS A. Machine Learning in Healthcare</head><p xml:id="_rr6FyVF"><s xml:id="_WVffBvs">Machine learning has transformed healthcare with its ability to predict, detect, and monitor, as noted in <ref type="bibr" target="#b13">[13]</ref>.</s><s xml:id="_r2DHPgD">Supervised learning algorithms can learn from labeled data and make predictions or classify based on the input features <ref type="bibr" target="#b14">[14]</ref>.</s><s xml:id="_yKVauGA">For example, machine learning or deep learning techniques can predict human vital signs like heart rate or classify physical activities <ref type="bibr" target="#b15">[15]</ref>.</s><s xml:id="_AQMZ2nE">In a study by Oyeleye et al. <ref type="bibr" target="#b16">[16]</ref>, machine learning and deep learning models were used to estimate heart rate using data from wearable devices.</s><s xml:id="_7t5eXQc">The authors tested different regression algorithms including linear regression, knearest neighbor, decision tree, random forest, autoregressive integrated moving average, support vector regressor, and long short-term memory recurrent neural networks.</s><s xml:id="_UwsukPY">Similarly, Luo et al. <ref type="bibr" target="#b17">[17]</ref> utilized the LSTM model to predict heart rate based on five factors: heart rate signal, gender, age, physical activities <ref type="bibr" target="#b18">[18]</ref>, and mental state.</s><s xml:id="_QEDhx6h">Unsupervised learning algorithms learn from unlabeled data and find patterns using association and clustering techniques <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>.</s><s xml:id="_PzxEd3S">Sheng and Huber <ref type="bibr" target="#b19">[19]</ref> developed an unsupervised method with an encoder and decoder network to identify similar physical activities using clustering, which achieved a clustering accuracy of 85% based on learning embeddings and behavior clusters.</s><s xml:id="_kDbcRCD">RL, on the other hand, does not require prior knowledge or information and works on an environment-driven approach <ref type="bibr" target="#b21">[21]</ref>.</s><s xml:id="_EYgCtmD">The agents learn through receiving rewards or penalties based on their actions which is called as experience.</s><s xml:id="_9vp2RNJ">Unlike supervised learning, RL can learn a sequence of actions through exploration and exploitation and does not require extensive labeled data for data-driven models <ref type="bibr" target="#b22">[22]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MWt8VKV">B. Mimic Human Behavior Patterns</head><p xml:id="_qqvPksB"><s xml:id="_S6JmVcG">Tirumala et al. <ref type="bibr" target="#b23">[23]</ref> studied how to understand human behavior patterns and identify common movements and interactions in a set of related tasks and situations.</s><s xml:id="_puuPXux">They used probabilistic trajectory models to develop a framework for hierarchical reinforcement learning (HRL).</s><s xml:id="_KEQWUvw">Janssen et al. <ref type="bibr" target="#b24">[24]</ref> suggested breaking down a complex task such as biological behavior into smaller parts, with HRL able to organize sequential actions into a temporary option.</s><s xml:id="_WGKuzEk">They compared biological behavior to options in HRL.</s><s xml:id="_hUyFbD5">Tsiakas et al. <ref type="bibr" target="#b25">[25]</ref> proposed a human-centered cyber-physical systems framework for personalized human-robot collaboration and training, focusing on monitoring and evaluating human behavior.</s><s xml:id="_T85D433">The authors aimed to effectively predict human attention with the minimum and least intrusive sensors.</s><s xml:id="_kVyQgVY">Kubota et al. <ref type="bibr" target="#b26">[26]</ref> investigated how robots can adapt to the behavior of people with cognitive impairments for cognitive neuro-rehabilitation.</s><s xml:id="_4ba7zbd">They explored different types of robots for therapeutic, companion, and assistive applications.</s><s xml:id="_feSu6dC">For health applications, robots must be able to perceive and understand human behavior, which includes high-level behaviors like cognitive abilities and engagement, as well as low-level behaviors like speech, gesture, and physiological signs <ref type="bibr" target="#b27">[27]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sQXyjHn">C. RL in Healthcare</head><p xml:id="_HSUyD9E"><s xml:id="_MpngCzV">Lisowska et al. <ref type="bibr" target="#b28">[28]</ref> developed a digital intervention for cancer patients to promote positive health habits and lifestyle changes.</s><s xml:id="_MuWYtEW">They used RL to determine the best time to send intervention prompts to the patients and employed three RL approaches (Deep Q-Learning, Advance Actor Critic, and proximal policy optimization) to create a virtual coach for sending prompts.</s><s xml:id="_YKNhmXn">Other studies have also shown that personalized messages can increase physical activity in type 2 diabetes patients <ref type="bibr" target="#b29">[29]</ref>.</s><s xml:id="_ZTRkrGp">Li et al. <ref type="bibr" target="#b30">[30]</ref> proposed a RL approach based on electronic health records for sequential decision-making tasks.</s><s xml:id="_t6wBe3F">They used a model-free Deep Q Networks (DQN) algorithm to make clinical decisions based on patient data and achieved better results with cooperative multi-agent RL.</s><s xml:id="_FjgE9My">R decisionmaking can also be used for human activity recognition, as shown in a study that proposed a dynamic weight assignment network architecture and used a twin delayed deep deterministic algorithm inspired by various other RL algorithms <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b32">[32]</ref>.</s></p><p xml:id="_5ArEXzZ"><s xml:id="_cKP5rQg">Reinforcement learning (RL) has been extensively researched across various domains, including gaming, human behavior modeling, and socially assistive robotics.</s><s xml:id="_sXj6zDB">While these applications have demonstrated the effectiveness of RL in controlled environments, the deployment of physical robots in sensitive settings like hospitals, elderly care, and mental health facilities poses significant safety risks to patients and staff <ref type="bibr" target="#b33">[33]</ref>.</s><s xml:id="_c8VWNZx">Moreover, traditional supervised and unsupervised learning methods used in health monitoring systems often struggle to handle the uncertain and dynamic nature of hospital environments.</s><s xml:id="_sS4jjUq">In contrast, our study introduces a novel application of multi-agent reinforcement learning (MARL) for healthcare, where virtual agents, rather than physical robots, autonomously monitor and learn from patient vital signs in real-time.</s><s xml:id="_uFBPxkr">This approach goes beyond the typical uses of RL by integrating multi-agent systems in a healthcare setting, enabling concurrent learning across multiple physiological parameters.</s><s xml:id="_WQ4AgSU">Each agent adapts its behavior based on changes in vital signs and takes actions to alert clinical teams during emergencies.</s><s xml:id="_caFSY8m">Our framework not only addresses the challenges of safety and uncertainty in dynamic healthcare environments but also introduces a clinically-informed reward mechanism that enhances the agents' decision-making abilities, making this application of AI a novel contribution to healthcare monitoring.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_e2uHpyQ">III. DRL MONITORING FRAMEWORK</head><p xml:id="_zVc9hJH"><s xml:id="_qNbpR73">In this section, the design of a human behavior monitoring system, DRL monitoring framework, that uses R is presented in detail.</s><s xml:id="_GNGmxva">The aim of the system is to monitor vital signs to learn human behavior patterns and ensure clinical safety in an uncertain environment.</s><s xml:id="_A4bnqDJ">The proposed framework involves a multi-agent system where each vital sign state is observed by an individual agent, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. A DRL algorithm, DQN, is used to learn effective strategies in the sequential decision-making process without prior knowledge through trial-and-error interactions with the environment <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4RYJzMg">A. Technical Background</head><p xml:id="_VqkCK46"><s xml:id="_WGxpeUe">The challenge addressed in this research is the development of a multi-agent framework for real-time health status monitoring by learning and interpreting patterns in vital signs through wearable sensors.</s><s xml:id="_nJZN3JY">The agents must detect deviations from normal vital sign patterns that exceed Modified Early Warning Scores (MEWS) thresholds and alert the emergency team accordingly.</s></p><p xml:id="_M9DRDjg"><s xml:id="_hXq4Wkh">To formulate this problem, we leverage the framework of Markov Decision Processes (MDP), expressed as a 5-tuple M = (S, A, P, R, γ).</s><s xml:id="_JmU37Wu">Here, S represents the finite state space, where each state s t ∈ S corresponds to a distinct combination of vital sign readings at time t.</s><s xml:id="_AZK6Xdp">The action set A comprises potential alerts the agents can issue based on the observed vital signs.</s><s xml:id="_9Q5gbPA">The transition function P (s, a, s ′ ) models the probability of moving from state s to state s ′ upon taking action a, reflecting the dynamic nature of human vital signs.</s></p><p xml:id="_SDes2SK"><s xml:id="_dXEyauV">Central to our approach is the reward function R(s, a), which is defined to prioritize actions that lead to the early detection of potential health risks, thereby enabling timely intervention.</s><s xml:id="_W2Z5jjz">This is mathematically represented as:</s></p><formula xml:id="formula_0">R(s t , a t ) = ∞ t=0 γ t r t ,<label>(1)</label></formula><p xml:id="_NasJyM2"><s xml:id="_fbqmCcW">where γ is the discount factor that balances the importance of immediate versus future rewards, ensuring the agents' actions are aligned with long-term health monitoring objectives.</s></p><p xml:id="_WUDcktS"><s xml:id="_jfAFqqp">The goal is to discover an optimal policy π(s t ) that maximizes the expected reward by selecting the most appropriate action a t in any given state s t .</s><s xml:id="_gkfHbNw">This optimization is achieved through the iterative update of the Q-function, as outlined in the Bellman equation:</s></p><formula xml:id="formula_1">Q new (s t , a t ) ← (1-α)Q(s t , a t )+α r t + γ max a Q(s t+1 , a) ,<label>(2)</label></formula><p xml:id="_azJ8tpS"><s xml:id="_KAYwrKy">where α represents the learning rate, influencing the integration of new information into the Q-function.</s><s xml:id="_eh6mRj6">Through this process, the agents continually refine their decision-making strategy, enhancing the system's capability to monitor and respond to emerging health risks effectively.</s><s xml:id="_pHXdatz">TABLE I: Modified Early Warning Scores [36] MEWS 4/MET 3 2 1 0 1 2 3 4/MET Respiratory Rate ≤4 5-8 9-20 21-24 25-30 31-35 ≥36 Oxygen Saturation ≤84 85-89 90-92 93-94 ≥95 Temperature ≤34.0 34.1-35.0</s><s xml:id="_7cHVRQf">35.1-36.0</s><s xml:id="_zfKQuDG">36.1-37.9</s><s xml:id="_K9sZeJ3">38.0-38.5 ≥38.6 Heart Rate ≤39 40-49 50-99 100-109 110-129 130-139 ≥140 Sedation Score Awake Mild Moderate Severe</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jgk4cSy">B. Monitoring Environment</head><p xml:id="_sDFKsrV"><s xml:id="_he5zmqM">A custom RL monitoring system based on MDP has been created to have human vital signs data serve as the observation space S, action space A for learning agents to make decisions, and rewards R for the agents' actions as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>.</s><s xml:id="_bXamAX5">This study introduces a novel isolated multi-agent MDP framework that allows multi-agents to share the same environment and make decisions based on the health parameters they are monitoring, receiving rewards without being influenced by the decisions of other agents.</s><s xml:id="_54SZYcs">The goal of all agents in this environment is to monitor the health of patients using the predefined MEWS, as shown in Tab.</s><s xml:id="_PZ9drz6">I.</s><s xml:id="_nRYu56S">In healthcare, each vital sign plays a critical role in determining a person's clinical safety.</s></p><p xml:id="_nH7jbXn"><s xml:id="_6SFM3Vw">In the current framework, we have implemented three RL agents to monitor heart rate, respiration, and temperature.</s><s xml:id="_SsTxGug">These agents operate primarily in cooperative mode, sharing information about the patient's health status and working together to ensure timely interventions.</s><s xml:id="_XhPpgRv">Cooperation allows the agents to pool rewards from collective actions, improving overall system learning.</s><s xml:id="_ASA9crh">However, when multiple patients are being monitored or resource constraints arise (e.g., limited access to medical personnel), the agents may enter competitive mode.</s><s xml:id="_MyFFwTY">In this mode, agents prioritize the most critical health states and may compete for resources by adjusting the urgency of alerts based on the patient's condition.</s></p><p xml:id="_A96u9Sk"><s xml:id="_ASGgdUy">As the number of agents increases, the framework is designed to scale effectively.</s><s xml:id="_sxQqbyK">Each additional agent monitors new physiological parameters or additional patients, with the system adjusting the reward mechanism and communication strategy to maintain efficient performance.</s><s xml:id="_nZpWfzh">The system remains modular, enabling easy expansion without significantly impacting computational load or decision-making speed.</s><s xml:id="_2zFqGDT">Importantly, the system's ability to operate in both cooperative and competitive modes ensures flexibility, allowing it to adapt to various healthcare scenarios, including large-scale monitoring in hospitals.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FXsH7C4">1) Observation Space:</head><p xml:id="_DS82zCn"><s xml:id="_eSDNfsg">The environment in Fig. <ref type="figure" target="#fig_1">2</ref> has a state, represented by s i t ϵS, where i = 0, 1, 2, ...n, refers to observations at time t.</s><s xml:id="_5gb7rTB">The aim is to divide the state into observations and allocate them to multi-agents.</s><s xml:id="_fyX2YWa">Suppose S represents the state of the human body, and there are three observations, s 0 t , s 1 t , s 2 t ϵS, that represent different internal vital signs of the human body at time t.</s><s xml:id="_vJaCwgR">The human health status is controlled by multiple internal vital signs, each with a different threshold as shown in MEWS Tab.I.</s><s xml:id="_fFRWcMs">Using a single agent to monitor all the vital signs can result in a sparse rewards challenge <ref type="bibr" target="#b37">[37]</ref>, where the environment might produce few useful rewards and hinders the learning of an agent.</s><s xml:id="_hSPxbAv">Therefore, multi-agents need to be deployed for each human to monitor the critical vital signs.</s><s xml:id="_kUaVtY6">The expected return E π of a policy π in a state s can be defined by state-value Eq. 3 in the multiagent setting, where i = 0, 1, 2, 3, ...n is a finite number of observations n in the state.</s></p><formula xml:id="formula_2">V π (s i ) = E π ∞,n t=0,i=0 γ t R(s t , π(s t ))|s 0 0 = s (3)</formula><p xml:id="_yq7ezUB"><s xml:id="_3bNwVyZ">2) Action Space: The action space of the monitoring environment is defined based on the MEWS <ref type="bibr" target="#b36">[36]</ref> as shown in Tab.</s><s xml:id="_vXryStn">I.</s><s xml:id="_SyGb4hd">The table presents early warning scores of adults' vital signs with the appropriate Medical Emergency Team (MET) to contact if any escalations in the health parameters.</s><s xml:id="_twrMPkd">Based on the MEWS as threshold values, the action space has been segmented to have five discrete actions to communicate the vital signs to MET-0, MET-1, MET-2, MET-3, and MET-4.</s><s xml:id="_4U5N6va">Each of these actions will be taken by agents based on the current state of the vital signs they are monitoring.</s><s xml:id="_9YcGy9X">The expected return E π for taking an action a in a state s under a policy π can be measured using the action-value function Q π (s, a) defined in Eq. 4.</s></p><formula xml:id="formula_3">Q π (s, a) = E π ∞ t=0 γ t R(s t , a t , π(s t ))|s 0 = s, a 0 = a (4) TABLE II: Rewards Policy MEWS 4 3 2 1 0 Action 0 -4 -3 -2 -1<label>10</label></formula><formula xml:id="formula_4">Action 1 -4 -3 -2 10 -1 Action 2 -4 -3 10 -1 -2 Action 3 -4 10 -1 -2 -3 Action 4 10 -3 -2 -1 -<label>4</label></formula><p xml:id="_p4F5cZH"><s xml:id="_dTcYXCv">3) Rewards: The goal of RL is to maximize cumulative rewards obtained through the actions of learning agents in an environment.</s><s xml:id="_mc9X6yY">In traditional RL, an agent is rewarded based on its action that leads to a transition from state s t to s t+1 .</s><s xml:id="_UJDatHX">In this study, the objective of the learning agent is to learn patterns in human vital signs.</s><s xml:id="_G7TQbZR">This is achieved through the design of an effective reward policy.</s><s xml:id="_ayXMXvK">The reward policy, as defined in this study, is calculated using Eq. 5.</s><s xml:id="_6QKU3fp">The agents are positively rewarded if they monitor vital signs in a state and take the correct action from the action space to communicate with the correct MET as defined in MEWS Tab.I.</s><s xml:id="_TE26UcZ">On the other hand, if the agent takes the wrong action, it is negatively rewarded.</s><s xml:id="_K7qBZQD">The rewards are split into five categories for the five actions in the action space based on the MET from MEWS Tab.I.</s><s xml:id="_t2hqDEw">The full rewards for each action selected by the agents are presented in Tab.II.</s><s xml:id="_QSsKV6y">The reward policy utilizes the DRL agents' desire to maximize rewards in each learning iteration, making them learn the behavior patterns.</s><s xml:id="_XV5jpEZ">Under each category, different levels of rewards were configured.</s><s xml:id="_kh8DDGu">For example, an observation s 1 t ϵS at the time t is related to heart rate falling under MET-4, the rewards are shown in Eq. 6.</s></p><formula xml:id="formula_5">R(s t , a t ) = +reward if action = M ET -reward if action ̸ = M ET (5) R(s 1 t , a t ) =                10 if M ET = 4&amp;action = 4 -1 if M ET = 4&amp;action = 3 -2 if M ET = 4&amp;action = 2 -3 if M ET = 4&amp;action = 1 -4 if M ET = 4&amp;action = 0<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TGdBj5C">C. Learning Agent</head><p xml:id="_4j4x2sS"><s xml:id="_49aaGWd">In this study, a game learning agent DQN algorithm is employed.</s><s xml:id="_G75zPqE">The DQN algorithm was first introduced by Deep-Mind, a subsidiary of Google, for playing Atari games.</s><s xml:id="_tkzSgQD">It allows the agent to play games by simply observing the screen, without any prior training or knowledge about the games.</s><s xml:id="_zER3QV3">The DQN algorithm approximates the Q-Learning function using neural networks, and the learning agent is rewarded based on the neural network's prediction of the best action for the current state.</s><s xml:id="_jgwEjrP">For this research, the reward policy is described in more detail in Section III-B3.</s></p><p xml:id="_3ut49yS"><s xml:id="_QJBDrex">1) Function Approximation: The neural network used in this study to estimate the Q-values for each action has three layers: an input layer, a hidden layer, and an output layer.</s><s xml:id="_jGRkvUQ">The input layer has a node for each vital sign in a state and the output layer has a node for each action in the action space.</s><s xml:id="_X9jCfhg">The model is configured with a relu activation function, mean square error as the loss function, and an Adam optimizer.</s><s xml:id="_VVZjqy3">The model is trained on the states and their corresponding rewards and, once trained, it can predict the accumulated reward.</s></p><p xml:id="_vyqmQCB"><s xml:id="_UMuWUXd">The learning agent takes an action a t ∈ A in a transition from state s t to s ′ t and receives a reward R. In this transition, the maximum Q-function value is calculated according to Eq. 4, and the calculated value is discounted by a discount factor γ to prioritize immediate rewards over future rewards.</s><s xml:id="_GaDJsGp">The discounted future reward is combined with the current reward to obtain the target value.</s><s xml:id="_6Zy4yKX">The difference between the prediction from the neural network and the target value forms the loss function, which is a measure of the deviation of the predicted value from the target value and can be estimated using Eq. 7. The square of the loss function penalizes the agent for large loss values.</s></p><formula xml:id="formula_6">loss = (R + γ • max(Q π * (s, a)) target value -Q π (s, a) predicted value ) 2<label>(7)</label></formula><p xml:id="_ztwDWQd"><s xml:id="_ujs9H3t">2) Memorize and Replay: The basic neural network model has a limitation in its memory capacity and can forget previous observations as they are overwritten by new observations.</s><s xml:id="_qGDsjQ4">To mitigate this issue, a memory array that stores the previous observations including the current state s t , action a t , reward R, and next state s ′ t is used.</s><s xml:id="_3JqGenW">This memory array enables the neural network to be retrained using the replay method, where a random sample of previous observations from the memory is selected for training.</s><s xml:id="_ZkHXf4v">In this study, the neural network model was retained by using a batch size of 32 previous observations.</s></p><p xml:id="_27YVjXF"><s xml:id="_BSHkEUW">3) Exploration and Exploitation: The explorationexploitation trade-off in RL refers to the balancing act between trying out new actions to gather information and exploiting the actions that lead to the highest rewards.</s><s xml:id="_8VChGWB">This balance can be modeled mathematically using the ϵ-greedy algorithm, which defines a probability ϵ of choosing a random action and a probability 1 -ϵ of choosing the action believed to lead to the highest reward based on the current knowledge of the action-value function Q(s t , a).</s><s xml:id="_kckRMtB">The equation to determine the action taken at time t is as follows:</s></p><formula xml:id="formula_7">a t =</formula><p xml:id="_zBt6aQJ"><s xml:id="_XqHYktx">random(a t ) with probability ϵ greedy(a t ) with probability 1 -ϵ</s></p><p xml:id="_nBPBNKT"><s xml:id="_RKQq2CK">where the greedy action is defined as:</s></p><formula xml:id="formula_9">a t = arg max a Q(s t , a)<label>(9)</label></formula><p xml:id="_XTkjqJ7"><s xml:id="_7ZZJfmt">The value of ϵ determines the level of exploration versus exploitation, with smaller values leading to more exploitation and larger values leading to more exploration.</s><s xml:id="_zbCsr8d">Over time, as the action-value function becomes more accurate, ϵ can be decreased to allow for more exploitation and convergence to the optimal policy.</s></p><p xml:id="_zKA32bC"><s xml:id="_GEPBrxG">In this study, we emphasize the importance of balancing exploration and exploitation for effective patient monitoring.</s><s xml:id="_DQqHxFs">Exploration allows agents to discover better monitoring strategies, while exploitation ensures timely alerts by acting on learned knowledge.</s><s xml:id="_yj4fKsH">Through empirical testing, we found that an exploration rate ϵ between 0.1 and 0.2 provided the optimal balance in our healthcare environment.</s><s xml:id="_8mfHC9V">This range ensured that agents could adapt to changing patient conditions while still providing timely and accurate interventions.</s><s xml:id="_YUpJnkg">In critical situations with frequent health deviations, a higher exploitation rate proved beneficial, whereas environments with fewer critical events required more exploration to discover new monitoring patterns.</s></p><p xml:id="_UcWuttA"><s xml:id="_VNhhwYn">4) Hyper Parameters: Other than the parameters defined for the neural networks, a set of hyperparameters has to supply for the RL process.</s><s xml:id="_JqARXj5">They are as follows:</s></p><p xml:id="_KukhTPk"><s xml:id="_32QNHsz">• episodes (M): This is a gaming term that means the number of times an agent has to execute the learning process.</s></p><p xml:id="_gSgrdMT"><s xml:id="_s8qqgvM">• learning rate(α): Learning rate is to determine much information neural networks learn in an iteration.</s><s xml:id="_wKXUK4E">• discount factor(γ): Discount factor ranges from 0 to 1 to limit future rewards and focus on immediate rewards.</s></p><p xml:id="_35tVVed"><s xml:id="_q4nJxcw">Algorithm 1 implements the proposed multi-agent human monitoring framework.</s><s xml:id="_mk4pcvX">It takes as input a set of subjects C = 1, 2, . . .</s><s xml:id="_VraGa2w">, C and a set of vital signs V = 1, 2, . . .</s><s xml:id="_4NzBU29">, V , along with the number of episodes M = 1, 2, . . .</s><s xml:id="_ySpDNMG">, M .</s><s xml:id="_V55HTnB">The algorithm outputs the rewards achieved by agents in each episode.</s><s xml:id="_FnQAKQR">Lines 1-2 initializes all the parameters needed for monitoring the environment and learning agent.</s><s xml:id="_5kGdEDA">Lines 3-7 present the reward policy.</s><s xml:id="_9YrPemH"><ref type="bibr">Lines</ref> 8-14 present the function approximation using the neural networks model, memorize &amp; replay, exploration &amp; exploitation of the DRL agent.</s><s xml:id="_ZnGPbDH">Lines 15-28 are nested for loops with conditional statements to check if the episode is completed or not.</s><s xml:id="_UfChdun">The outer loop is to iterate each episode while resetting the environment to initial values and score to Algorithm 1 multi-agents Monitoring Require: Input: a set of subjects C = {1, 2, . . .</s><s xml:id="_Q6rZ8MU">, C};a set of vital signs V = {1, 2, . . .</s><s xml:id="_F8tbhAM">, V }; Episodes M = {1, 2, . . .</s><s xml:id="_K4A9Asy">, M }; Ensure: Output: Rewards achieved by agentss in each episode.</s><s xml:id="_cBaBC4F">1: Initialization : observation space = s i t ϵS, action space = atϵA, reward R, γ, ϵ, ϵ decay , ϵmin, memory = ∅, batch size 2: Set monitor length = N 3: if action is appropriate then 4: R ← +reward 5: else 6: R ← -reward 7: end if 8: Define model ← N euralN etworkM odel 9: memory ← memory ∪ (st, at, R, st+1) 10: if np.random.rand</s><s xml:id="_2G4dRes">&lt; ϵ then ▷ Exploration 11: action value ← random(at) 12: else ▷ Exploitation 13: action value ← greedy(at) 14: end if 15: for episode m ∈ M do 16: score = 0 17: for time in range(monitor length) do 18: at ← action(st) 19: st+1, R, done ← step(at) 20: memory ← memory ∪ (st, at, R, st+1) 21: st ← st+1 22: if done then 23: displaym, score 24: break 25: end if 26: end for 27:</s></p><p xml:id="_bDJGXAU"><s xml:id="_SpKWXhn">replay ← batch size 28: end for zero.</s><s xml:id="_rPe8ag2">The inner loop is to iterate timesteps which denote the time of the current state and calls the methods.</s></p><p xml:id="_uaDyMJb"><s xml:id="_yKsDp3j">The patient monitoring system operates with multiple agents, each tasked with monitoring specific vital signs such as heart rate, respiration rate, and temperature.</s><s xml:id="_KpZXmhS">The agents are initialized with a basic action set, which includes triggering alerts, adjusting monitoring intervals, and taking no action based on the patient's condition.</s><s xml:id="_5Kb4rxm">At each time step, agents receive vital sign data as input and evaluate the patient's state.</s><s xml:id="_PkVeWfw">Based on the current state and the agent's policy, an action is selected.</s><s xml:id="_ZnzSVSh">The reward function provides feedback based on the timeliness and accuracy of the action: positive rewards are given for correct, timely interventions, while penalties are applied for false alarms or missed emergencies.</s><s xml:id="_G7dWxgy">Over time, the agents improve their performance through continuous learning and collaboration, ensuring that vital signs are monitored comprehensively and interventions are timely.</s><s xml:id="_9ZhjdCA">IV.</s><s xml:id="_9hjAQB9">EXPERIMENT</s></p><p xml:id="_92Zt7X4"><s xml:id="_ZjVVYPU">In this study, the proposed multi-agent framework was evaluated by deploying an agent for each physiological feature of a different set of subjects.</s><s xml:id="_nbpwGjg">The aim of the learning agents was to monitor their respective vital signs, communicate with the corresponding MET based on the estimated level of emergency, and learn the subjects' behavior patterns.</s><s xml:id="_2wnsDnS">All the experiments were conducted using Python programming language version 3.7.6 and related libraries such as TensorFlow, Keras, Open Gym AI, and stable baselines3.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NbE3Hf4">A. Dataset</head><p xml:id="_AgGbnks"><s xml:id="_GcNVwF6">• PPG-DaLiA <ref type="bibr" target="#b38">[38]</ref>: The dataset contains physiological and motion data of 15 subjects, recorded from both a wrist-worn device and a chest-worn device while the subjects were performing a wide range of activities under conditions close to real life.</s></p><p xml:id="_6N8Fuxu"><s xml:id="_QMVvYKa">• WESAD [39]: The WESAD (Wearable Stress and Affect Detection) dataset is a collection of physiological signals recorded from participants while they perform various activities.</s><s xml:id="_HjUdkTJ">It includes multi-modal signals such as ECG, PPG, GSR, respiration, and body temperature.</s><s xml:id="_ZCjAdkw">B. Baseline Models • WISEML [40]: Mallozzi et al. proposed an RL framework for runtime monitoring to prevent dangerous and safety-critical actions in safety-critical applications.</s><s xml:id="_72ajDKy">In this framework, runtime monitoring is used to enforce properties to the agent and shape its reward during learning.</s><s xml:id="_Enknmy7">• CA-MQL [41]: Chen et al. proposed constrained actionbased MQL (CA-MQL) for UAVs to autonomously make flight decisions that consider the uncertainty of the reference point location.</s><s xml:id="_DdVhfPS">• MADDPG [42]: Lowe et al. introduced a deep reinforcement learning framework for multi-agent environments.</s><s xml:id="_HqmSnmS">This framework uses an adaptation of actor-critic methods to coordinate agents in both cooperative and competitive settings by accounting for other agents' policies.</s><s xml:id="_kwmXHhJ">It highlights the difficulty of traditional algorithms in multiagent scenarios and introduces policy ensembles for more robust learning.</s><s xml:id="_UYqrEMX">• QMIX [43]: Rashid et al. developed QMIX, a value-based multi-agent RL algorithm that factors joint action-values into per-agent values, allowing for decentralised policies while training in a centralised manner.</s><s xml:id="_6qBvtQf">QMIX demonstrated superior performance on challenging StarCraft II tasks by ensuring consistency between centralised and decentralised learning.</s></p><p xml:id="_CXA8ZZ6"><s xml:id="_q9SX7sY">• Existing RL baseline models by Li et al. <ref type="bibr" target="#b30">[30]</ref> were deployed to optimize sequential treatment strategies based on Electronic Health Records (EHRs) for chronic diseases using DQN.</s><s xml:id="_C9feW7E">The multi-agent framework results were compared with Q-Learning and Double DQN.</s><s xml:id="_9bmxR9s">• Similarly, RL was deployed to recognize human activity using a dynamic weight assignment network architecture with TD3 (a combination of Deep Deterministic Policy Gradient (DDPG), Actor-Critic, and DQN) by Guo et al. [31].</s><s xml:id="_FD7agSJ">• Yom et al. [29] used Advantage Actor-Critic (A2C) and</s></p><p xml:id="_sqSwr8s"><s xml:id="_y8TpUrP">Proximal Policy Optimization (PPO) algorithms to act as virtual coaches in decision-making and send personalized messages.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Xv22dHD">C. Performance Measures</head><p xml:id="_22qvCUz"><s xml:id="_3MyuwwY">In the initial phase, Cumulative Rewards were selected as the primary performance metric because they offer a direct reflection of the RL agents' success in achieving healthcare objectives.</s><s xml:id="_8tSwzrF">These cumulative rewards quantify the agents' ability to make correct decisions based on real-time physiological data, which is essential for ensuring timely medical interventions.</s><s xml:id="_WHcaD9B">Given the critical nature of healthcare systems, focusing on cumulative rewards allowed for the evaluation of how well the agents were trained to detect early signs of health deterioration.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_bKauv43">TABLE III: Comparison of DRL and MARL Frameworks on Cumulative Rewards</head><p xml:id="_GXAd4eM"><s xml:id="_tbEYJGb">To provide a more holistic evaluation, we introduced additional performance metrics:</s></p><p xml:id="_Z3XC8GX"><s xml:id="_ywD3gKa">• Learning Rate: This metric evaluates how quickly the agents converge to an optimal policy, which is vital in healthcare applications where rapid adaptation to changing patient conditions is crucial.</s><s xml:id="_GuXv3QE">Faster learning ensures that the agents can respond to emergencies in real time, improving the effectiveness of the system.</s><s xml:id="_NnmeVRR">• Computational Complexity: This metric assesses the system's processing demands, particularly in terms of CPU/GPU time.</s><s xml:id="_yw6pTEV">Minimizing computational complexity is essential in healthcare settings with resource constraints, such as hospitals or wearable monitoring devices.</s><s xml:id="_TaQGvgZ">Lower complexity ensures that the system can run efficiently without causing delays in decision-making.</s><s xml:id="_HnGwc8D">• Memory Usage: As the system scales to monitor multiple physiological parameters across various agents, memory usage becomes a key factor.</s><s xml:id="_6Am9QvC">Efficient memory utilization is critical for deploying the framework on resourceconstrained devices like wearables, ensuring scalability and adaptability without compromising performance.</s><s xml:id="_Keh53ER">Incorporating these metrics provides a more comprehensive evaluation of the proposed framework, ensuring not only its effectiveness in terms of rewards but also its efficiency, scalability, and real-world deployment potential in healthcare environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AQTvHRG">V. EXPERIMENT RESULTS AND ANALYSIS</head><p xml:id="_cRe6azW"><s xml:id="_MVuxUqN">The advantage of RL for monitoring systems is that it can learn to handle complex, dynamic environments.</s><s xml:id="_3V4QBD7">Many monitoring tasks involve making decisions based on incomplete, uncertain information, and the optimal decision may depend on the context of the situation <ref type="bibr" target="#b44">[44]</ref>.</s><s xml:id="_9uzyanj">RL can learn to make decisions in these types of problems by considering the current state of the system and past experience.</s><s xml:id="_37bDQYR">In this study, the aim is to leverage the RL capability to optimize the decision-making process in patient monitoring.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_654Qm7R">A. DRL Agents Performance</head><p xml:id="_XXR2X6e"><s xml:id="_Cvz6yUE">The performance of the proposed DRL framework was evaluated using two datasets, with a focus on cumulative rewards, learning rate, computational complexity, and memory usage.</s><s xml:id="_zRWNXK8">Additionally, we expanded our comparison to include multi-agent RL frameworks, MADDPG and QMIX, to assess how well these frameworks handle the complexities of realtime health monitoring tasks.</s></p><p xml:id="_exbZuSQ"><s xml:id="_CjPFmFh">The results are summarized in Tab.</s><s xml:id="_ZTNRYsY">III, which includes the performance of single-agent RL methods (Q-Learning, PPO, A2C, and DDPG) and multi-agent RL models (MADDPG and QMIX).</s><s xml:id="_nXMhngH">The proposed DRL framework consistently outperforms all other models in terms of cumulative rewards, with significant improvements over the baseline methods.</s></p><p xml:id="_wSrc5Fs"><s xml:id="_y8RcAk7">As shown in Tab.</s><s xml:id="_dGrXqjS">III, the proposed DRL framework surpasses both MADDPG and QMIX in cumulative rewards for both datasets, particularly excelling in agent 1's performance on the PPG-DaLia dataset.</s><s xml:id="_rTzPHvG">This indicates that our framework's design, which includes a tailored reward mechanism based on Modified Early Warning Scores (MEWS), enables more efficient learning in healthcare environments.</s><s xml:id="_cbzuugv">Additionally, the exploration-exploitation trade-off in our system is better optimized for the variability of physiological data.</s></p><p xml:id="_DHSAuEe"><s xml:id="_7DmKssV">Beyond cumulative rewards, we evaluated the proposed DRL framework against baseline models using additional performance metrics, including learning rate, computational complexity, and memory usage, as shown in Tab.</s><s xml:id="_fRebpWe">IV.</s><s xml:id="_ujNAqrC">The proposed DRL framework showed superior performance across all these metrics, indicating its suitability for real-time applications in resource-constrained healthcare environments.</s></p><p xml:id="_RVtHAsv"><s xml:id="_5TkeGw6">In terms of learning rate, the proposed DRL framework converged after 850 epochs, outperforming all baseline models, including Q-Learning (1200 epochs) and Double DQN (1100 epochs).</s><s xml:id="_rqsJeDs">This faster convergence demonstrates the DRL framework's enhanced efficiency in learning complex healthcare scenarios.</s><s xml:id="_zJXSRed">Faster learning is especially critical in healthcare, where timely interventions directly impact patient outcomes.</s><s xml:id="_WhdjfpC">The use of multiple agents, each dedicated to a specific physiological metric, accelerates policy optimization and enhances responsiveness in dynamic, real-world environments.</s></p><p xml:id="_PexnHHx"><s xml:id="_z4y8nMt">For computational complexity, the proposed DRL framework exhibited a significantly lower iteration time of 0.70 seconds, outperforming more complex multi-agent models like CA-MQL (1.30 seconds) and PPO (1.10 seconds).</s><s xml:id="_YV4wqVW">This indicates that the framework is computationally efficient, making it ideal for real-time healthcare monitoring where decision delays could compromise patient safety.</s><s xml:id="_mS9qxx3">This improved efficiency is due to an optimized reward structure and action space, which reduces the time required for decision-making without compromising accuracy.</s></p><p xml:id="_hnGBdpK"><s xml:id="_gYHZUAU">In terms of memory usage, the DRL framework consumed 110MB, which is lower than all other baseline models, such as DDPG (160MB) and CA-MQL (175MB).</s><s xml:id="_JmW644z">This low memory footprint is crucial for deploying the framework on resourceconstrained hardware like wearable devices or low-power hospital systems.</s><s xml:id="_hBcFxXc">The efficient memory usage ensures the system can scale with additional agents without overloading system resources, making the framework suitable for largescale healthcare applications.</s></p><p xml:id="_Cctax8u"><s xml:id="_NQbRHGJ">TABLE IV: Evaluation of DRL Framework and Baseline Models on Additional Metrics RL Method Learning Rate (Epochs to Converge) Computational Complexity (Time in Seconds) Memory Usage (MB) Q-Learning 1200 0.85s per iteration 120MB PPO [29] 900 1.10s per iteration 150MB A2C [29] 1000 1.05s per iteration 140MB Double DQN [30] 1100 0.95s per iteration 135MB DDPG [31] 950 1.20s per iteration 160MB WISEML [40] 900 1.15s per iteration 145MB CA-MQL [41] 1000 1.30s per iteration 175MB MADDPG [42] 950 1.25s per iteration 155MB QMIX [43] 1100 1.20s per iteration 165MB Proposed DRL 850 0.70s per iteration 110MB</s></p><p xml:id="_yABNU9X"><s xml:id="_9BYYJX6">All three learning agents were fed with physiological features such as heart rate, respiration, and temperature, respectively, from the PPG-DaLiA dataset.</s><s xml:id="_kmPfmUp">Based on the observation space, action space, and reward policy defined for a customized gym environment for human behavior monitoring, the learning agents were run for 10 episodes, as shown in Fig. <ref type="figure">4</ref>. In the results, agent 1 refers to the heart rate monitoring agent, which showed a constant increase in scores for each episode for most of the subjects except subjects 5 and 6.</s><s xml:id="_jar8UfN">The intermittent low scores in agent 1 performance are due to the exploration rate in DQN learning, where the algorithm tries exploring all the actions randomly instead of relying on neural networks' predictions.</s><s xml:id="_JjSnShS">Similarly, agent 2 and agent 3 monitor two other physiological features, respiration and temperature, respectively.</s><s xml:id="_FqJ2uQN">agent 2 performed better than the other two agents and achieved consistent scores for all subjects.</s><s xml:id="_jN3DChh">Out of all agents, agent 3, temperature monitoring performance, was poor.</s><s xml:id="_a8cWc2Y">This issue was traced back to the data level, where the units of the temperature thresholds in the MEWS table and the input body temperature data from the dataset were different.</s><s xml:id="_QP3nhQY">Still, agent 3 achieved high scores in monitoring subjects 9, 8, 4, and 10.</s></p><p xml:id="_chZVQW6"><s xml:id="_3MYQwgH">The reward policy designed in the proposed multi-agent framework enables agents to learn the human physiological feature patterns.</s><s xml:id="_UWwMUHC">For example, if a subject's heart rate is 139 beats per minute, agent 1 takes Action 3 to communicate the message to MET-3.</s><s xml:id="_fq8chw7">The agent will get rewarded with +10 points only if Action 3 is taken; otherwise, the agent gets negatively rewarded according to the reward policy (Table <ref type="table">II</ref>).</s><s xml:id="_zFszK4n">With this example, the results in Fig. <ref type="figure">4</ref> can be interpreted better.</s><s xml:id="_2p43vAr">An increase in scores episode by episode, with the exception of the exploration rate, actually infers an increase in the learning curve of the agents in terms of human physiological patterns.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zRrtMd7">B. Hyper-Parameters Optimization</head><p xml:id="_ee86FUt"><s xml:id="_tB3dBRR">The DRL agents were further evaluated by hyperparameters optimization.</s><s xml:id="_tzZ7cHE">Out of all the hyperparameters discussed in this study, two hyperparameters, learning rate (α) and discount factor (γ), were optimized for all three agents, and the results are shown in Figs. <ref type="figure">5 and6</ref>.</s><s xml:id="_FVehzYE">The learning rate determines how much information neural networks learn in an iteration to predict action and approximate the rewards.</s><s xml:id="_d4v4qUB">The discount factor measures how much RL agents focus on future rewards relative to those in the immediate rewards.</s><s xml:id="_ckej5H3">In Fig. <ref type="figure">5</ref>, Figs.</s><s xml:id="_h9XRyeq"><ref type="figure">5a</ref>,<ref type="figure">5b</ref>, and5c show the agents' performance while optimizing α of neural networks.</s><s xml:id="_gFctsqK">The x-axis of the plots represents scores (cumulative rewards) achieved by an agent in each episode shown on the y-axis.</s><s xml:id="_ZQnExR3">The bar plots show that the learning rate α = 0.01 is a more optimized value in all the monitoring agents.</s><s xml:id="_vtY5fEV">Similarly, Figs.</s><s xml:id="_NNTjkH6"><ref type="figure">6a</ref>,<ref type="figure">6b</ref>, and 6c present the γ optimization of agent 1, agent 2, and agent 3, respectively.</s><s xml:id="_cRJekqF">The discount factors γ = 0.9 and γ = 0.75 are the more optimized values for agents 2 and 3, respectively, after 10 episodes of training.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WnbTCat">VI. DISCUSSION</head><p xml:id="_BCW6VgT"><s xml:id="_jc3xCHc">This study introduces an innovative approach to patient monitoring within the unpredictable environment of healthcare settings, employing adaptive multi-agent deep reinforcement learning (DRL) to ensure timely healthcare interventions.</s><s xml:id="_yHmqGKt">The fluctuating nature of vital signs, crucial indicators of patient health, necessitates a robust system capable of realtime analysis and decision-making.</s><s xml:id="_ryWe7U8">By leveraging the sequential decision-making prowess of RL algorithms, we have established a framework where each vital sign is monitored by a dedicated DRL agent.</s><s xml:id="_xHWvegV">These agents operate within a cohesive monitoring environment, guided by meticulously defined reward policies to identify and respond to potential health emergencies based on MEWS and MET standards.</s></p><p xml:id="_M5zeRvw"><s xml:id="_EMWxeT2">A notable aspect of our research is the emphasis on the design of the observation space for each DRL agent.</s><s xml:id="_KFWzVCy">This design is pivotal in ensuring the accuracy and effectiveness of the learning process, as it directly impacts the agent's ability to interpret vital sign data and make informed decisions.</s><s xml:id="_CQPJckf">The challenge encountered with DRL agent 3, responsible for monitoring body temperature, underscores the importance of data consistency and the need for a harmonized observation space.</s><s xml:id="_3VKHNMu">The discrepancy between the temperature units in the MEWS table and the dataset highlighted a critical area for improvement, emphasizing the need for standardized data inputs to enhance agent performance.</s></p><p xml:id="_4VBvqFs"><s xml:id="_4WuD5dw">The autonomous decision-making capability inherent in RL represents a significant advancement in supporting clinicians.</s><s xml:id="_SjnzMWv">By providing real-time updates on patient health, the DRL framework facilitates a proactive approach to patient care, extending its applicability beyond hospital settings to include home and specialized care environments.</s><s xml:id="_dUg9qES">This adaptability is further enhanced by the strategic optimization of hyperparameters, which fine-tunes the learning process of DRL agents to achieve optimal performance.</s><s xml:id="_ePawb4Y">Our investigation into hyperparameters such as the learning rate and discount rate reveals the critical balance between immediate and future rewards, a balance that is essential for the effective monitoring of patient health.</s></p><p xml:id="_RahZ6UD"><s xml:id="_vtNWBjV">Comparatively, traditional supervised learning algorithms, while accurate in predicting vital signs, fall short in dynamic healthcare environments due to their reliance on extensive labeled datasets and external supervision.</s><s xml:id="_bVzaMkH">The DRL approach, free from the constraints of labeled data, offers a more flexible and efficient solution for patient monitoring.</s><s xml:id="_FpWmJCf">However, it is essential to acknowledge the considerable effort required in data preparation and model tuning within supervised learning frameworks, which, despite their limitations, contribute significantly to the development of informed clinical decisions.</s></p><p xml:id="_M4wmXhT"><s xml:id="_yeET2SB">The adaptive multi-agent DRL framework proposed in this study represents a paradigm shift in patient monitoring, offering a dynamic, efficient, and scalable solution for timely healthcare interventions <ref type="bibr" target="#b45">[45]</ref>, [?].</s><s xml:id="_TRg9XZ6">The challenges and insights gleaned from this research pave the way for future advancements in the field, promising to enhance the quality of patient care through innovative technological solutions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DU8hjbA">VII. CONCLUSION</head><p xml:id="_ut2qYAv"><s xml:id="_vHHCJjt">This study has pioneered an adaptive framework for healthcare interventions using multi-agent DRL to dynamically monitor vital signs, establishing a novel approach in patient care.</s><s xml:id="_SB3pU9h">Through the development of a generic monitoring environment coupled with a strategic reward policy, the DRL agents were empowered to learn from and adapt to vital sign fluctuations, enabling timely interventions by healthcare professionals.</s><s xml:id="_NpTVcVa">Despite its innovative contributions, the research faced challenges, such as discrepancies in body temperature data scales and the absence of predictive capabilities for future vital sign trends, which limited the effectiveness of one DRL agent and the overall predictive potential of the system.</s><s xml:id="_6xNUzV6">Addressing these limitations, future research will focus on enhancing the framework with predictive analytics, allowing DRL agents to forecast vital sign trends and thus revolutionize patient monitoring.</s><s xml:id="_v2kHsV9">This advancement aims to facilitate proactive healthcare measures, significantly reducing the risk of critical health episodes and heralding a new era in adaptive patient monitoring and healthcare management.</s><s xml:id="_WmVpdbP">Having said that, the future direction of our research will be focused on extending the scope of the research to predict future vital signs using multi-agent DRL.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc><div><p xml:id="_4ZMbJNp"><s xml:id="_A7CJt5r">Fig. 1: Human monitoring framework to monitor vital signs of the client during regular activities and alert medical emergency teams accordingly.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc><div><p xml:id="_ZQPQRjs"><s xml:id="_QNA7fEE">Fig. 2: Multi-agent monitoring framework</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 Fig. 3 :</head><label>33</label><figDesc><div><p xml:id="_kzN5Xkx"><s xml:id="_UX4EeHc">Fig. 3: Experiemental Design</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Fig. 4 :</head><label>34</label><figDesc><div><p xml:id="_UF2rgA2"><s xml:id="_h2HBSUx">Fig. 4: DQN Agents Performance</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 Fig. 5 :</head><label>35</label><figDesc><div><p xml:id="_spFCyGa"><s xml:id="_FFXkWCu">Fig. 5: Hyper Parameters -α optimization</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3 Fig. 6 :</head><label>36</label><figDesc><div><p xml:id="_4SCDAh4"><s xml:id="_EW2ZfYj">Fig. 6: Hyper Parameters -γ optimization</s></p></div></figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_M4XBVhP">WESAD Dataset Method Agent 1 Agent 2 Agent 3 Agent 1 Agent 2 Agent 3 Q-Learning</title>
		<idno>42] 42500 29870 36015 41200 28560 35345 QMIX [43] 44800 30520 37600 43200 29230 36980 Proposed DRL 48354 30019 38651 47794 29056 37786</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VDdA3BQ">PPG-DaLia Dataset</title>
		<imprint/>
	</monogr>
	<note>DDPG WISEML MADDPG</note>
	<note type="raw_reference">PPG-DaLia Dataset WESAD Dataset Method Agent 1 Agent 2 Agent 3 Agent 1 Agent 2 Agent 3 Q-Learning 25878 17304 23688 25318 16341 22823 PPO [29] 23688 20367 17688 23128 19404 16823 A2C [29] 24717 13707 24369 24157 12744 23504 Double DQN [30] 25569 15360 20367 25009 14397 19502 DDPG [31] 26760 20754 23967 26200 19791 23102 WISEML [40] 28654 25789 33669 28094 24826 32804 CA-MQL [41] 32985 27856 34685 32425 26893 33820 MADDPG [42] 42500 29870 36015 41200 28560 35345 QMIX [43] 44800 30520 37600 43200 29230 36980 Proposed DRL 48354 30019 38651 47794 29056 37786 REFERENCES</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_bPkvjCx">Mobile health in remote patient monitoring for chronic diseases: principles, trends, and challenges</title>
		<author>
			<persName><forename type="first">N</forename><surname>El-Rashidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>El-Sappagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>El-Bakry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdelrazek</surname></persName>
		</author>
		<idno type="DOI">10.3390/diagnostics11040607</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XM2kh9e">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">607</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. El-Rashidy, S. El-Sappagh, S. R. Islam, H. M. El-Bakry, and S. Abdelrazek, &quot;Mobile health in remote patient monitoring for chronic diseases: principles, trends, and challenges,&quot; Diagnostics, vol. 11, no. 4, p. 607, 2021.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_nc9AUyk">Ai enabled rpm for mental health facility</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gururajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6zPYb3E">Proceedings of the 1st ACM Workshop on Mobile and Wireless Sensing for Smart Healthcare</title>
		<meeting>the 1st ACM Workshop on Mobile and Wireless Sensing for Smart Healthcare</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Shaik, X. Tao, N. Higgins, H. Xie, R. Gururajan, and X. Zhou, &quot;Ai enabled rpm for mental health facility,&quot; in Proceedings of the 1st ACM Workshop on Mobile and Wireless Sensing for Smart Healthcare, 2022, pp. 26-32.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_yUrNFby">A computerized analysis with machine learning techniques for the diagnosis of parkinson&apos;s disease: Past studies and future perspectives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dumka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Priyadarshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WEuTTh8">Diagnostics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">2708</biblScope>
			<date type="published" when="2022-11">Nov. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Rana, A. Dumka, R. Singh, M. K. Panda, and N. Priyadarshi, &quot;A computerized analysis with machine learning techniques for the diagnosis of parkinson&apos;s disease: Past studies and future perspectives,&quot; Diagnostics, vol. 12, no. 11, p. 2708, Nov. 2022.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_VmmCZsB">The rise of artificial intelligence in healthcare applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bohr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Memarzadeh</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-0-12-818438-7.00002-2</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nRqF9BX">Artificial Intelligence in healthcare</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="25" to="60" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Bohr and K. Memarzadeh, &quot;The rise of artificial intelligence in healthcare applications,&quot; in Artificial Intelligence in healthcare. El- sevier, 2020, pp. 25-60.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_tdCauWd">E-commerce application with analytics for pharmaceutical industry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Pattanayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Pooja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_quvWwnu">Advances in Intelligent Systems and Computing</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Nature</publisher>
			<date type="published" when="2022-09">Sep. 2022</date>
			<biblScope unit="page" from="291" to="298" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. K. Pattanayak, V. S. Kumar, K. Raman, M. M. Surya, and M. R. Pooja, &quot;E-commerce application with analytics for pharmaceutical in- dustry,&quot; in Advances in Intelligent Systems and Computing. Springer Nature Singapore, Sep. 2022, pp. 291-298.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_5RvgeZH">Towards computational solutions for precision medicine based big data healthcare system using deep learning models: A review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thirunavukarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gopikrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Palanisamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ME5txMS">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page">106020</biblScope>
			<date type="published" when="2022-10">Oct. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Thirunavukarasu, G. P. D. C, G. R, M. Gopikrishnan, and V. Palanisamy, &quot;Towards computational solutions for precision medicine based big data healthcare system using deep learning models: A review,&quot; Computers in Biology and Medicine, vol. 149, p. 106020, Oct. 2022.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_qfTGXcX">Deep reinforcement learning for autonomous driving: A survey</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sobh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Talpaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Sallab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yogamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5T7r3Nr">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4909" to="4926" />
			<date type="published" when="2022-06">Jun. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yo- gamani, and P. Perez, &quot;Deep reinforcement learning for autonomous driving: A survey,&quot; IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 6, pp. 4909-4926, Jun. 2022.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_kCMQsyq">Auto uning of price prediction models for high-frequency trading via reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2022.108543</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FpwsrxS">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">108543</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Zhang, N. Zhang, J. Yan, G. Li, and X. Yang, &quot;Auto uning of price prediction models for high-frequency trading via reinforcement learning,&quot; Pattern Recognition, vol. 125, p. 108543, 2022.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_A5qZ72v">State of the art of machine learning-enabled clinical decision support in intensive care units: Literature review</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.2196/28781</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bcejyFX">JMIR Medical Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">28781</biblScope>
			<date type="published" when="2022-03">Mar. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Hong, C. Liu, J. Gao, L. Han, F. Chang, M. Gong, and L. Su, &quot;State of the art of machine learning-enabled clinical decision support in intensive care units: Literature review,&quot; JMIR Medical Informatics, vol. 10, no. 3, p. e28781, Mar. 2022.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_QAag8a2">Optimizing individualized treatment planning for parkinson&apos;s disease using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khojandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramdhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_JztGvXz">2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-07">Jul. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Watts, A. Khojandi, R. Vasudevan, and R. Ramdhani, &quot;Optimizing individualized treatment planning for parkinson&apos;s disease using deep reinforcement learning,&quot; in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC). IEEE, Jul. 2020.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_YH94QS4">A reinforcement learning and deep learning based intelligent system for the support of impaired patients in home treatment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paragliola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coronato</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2020.114285</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EnXxQYC">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page">114285</biblScope>
			<date type="published" when="2021-04">Apr. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Naeem, G. Paragliola, and A. Coronato, &quot;A reinforcement learning and deep learning based intelligent system for the support of impaired patients in home treatment,&quot; Expert Systems with Applications, vol. 168, p. 114285, Apr. 2021.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_bfmwg43">Probabilistic machine learning for healthcare</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-biodatasci-092820-033938</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UwNxTku">Annual Review of Biomedical Data Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="393" to="415" />
			<date type="published" when="2021-07">Jul. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Y. Chen, S. Joshi, M. Ghassemi, and R. Ranganath, &quot;Probabilistic machine learning for healthcare,&quot; Annual Review of Biomedical Data Science, vol. 4, no. 1, pp. 393-415, Jul. 2021.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_c8AzBB6">Role of machine learning in healthcare sector</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Vijarania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Goel</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.4195384</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ajk68dt">SSRN Electronic Journal</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Rastogi, D. M. Vijarania, and D. N. Goel, &quot;Role of machine learning in healthcare sector,&quot; SSRN Electronic Journal, 2022.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_fr6cQeG">Machine learning algorithms-a review</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mahesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_S7GCD8U">International Journal of Science and Research (IJSR)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="381" to="386" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Internet</note>
	<note type="raw_reference">B. Mahesh, &quot;Machine learning algorithms-a review,&quot; International Jour- nal of Science and Research (IJSR).[Internet], vol. 9, pp. 381-386, 2020.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_MKGCTzT">Physical activity monitoring and classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Alsareii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alamri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Alasmari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irfan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qqMcgHc">Life</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1103</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Alsareii, M. Awais, A. M. Alamri, M. Y. AlAsmari, M. Irfan, N. Aslam, and M. Raza, &quot;Physical activity monitoring and classification using machine learning techniques,&quot; Life, vol. 12, no. 8, p. 1103, 2022.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_3wSCW2e">A predictive analysis of heart rates using machine learning techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oyeleye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Titarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniou</surname></persName>
		</author>
		<idno type="DOI">10.3390/ijerph19042417</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CBTXD47">International Journal of Environmental Research and Public Health</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">2417</biblScope>
			<date type="published" when="2022-02">Feb. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Oyeleye, T. Chen, S. Titarenko, and G. Antoniou, &quot;A predictive analysis of heart rates using machine learning techniques,&quot; International Journal of Environmental Research and Public Health, vol. 19, no. 4, p. 2417, Feb. 2022.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_WAHTgbv">Heart rate prediction model based on neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1088/1757-899x/715/1/012060</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_M26dRFr">IOP Conference Series: Materials Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">715</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12060</biblScope>
			<date type="published" when="2020-01">Jan. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Luo and K. Wu, &quot;Heart rate prediction model based on neural network,&quot; IOP Conference Series: Materials Science and Engineering, vol. 715, no. 1, p. 012060, Jan. 2020.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_c3prCSV">Sensor-based and vision-based human activity recognition: A comprehensive survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Piran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XRHPRyg">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">107561</biblScope>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. M. Dang, K. Min, H. Wang, M. J. Piran, C. H. Lee, and H. Moon, &quot;Sensor-based and vision-based human activity recognition: A compre- hensive survey,&quot; Pattern Recognition, vol. 108, p. 107561, Dec. 2020.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_Phnumuw">Unsupervised embedding learning for human activity recognition using wearable sensor data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_WJcyP6b">The Thirty-Third International Flairs Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Sheng and M. Huber, &quot;Unsupervised embedding learning for human activity recognition using wearable sensor data,&quot; in The Thirty-Third International Flairs Conference, 2020.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_jPzVtQG">Synthetic sensor data generation for health applications: A supervised deep learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Norgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Gebremedhin</surname></persName>
		</author>
		<idno type="DOI">10.1109/embc.2018.8512470</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QVTHwmY">2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1164" to="1167" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Norgaard, R. Saeedi, K. Sasani, and A. H. Gebremedhin, &quot;Synthetic sensor data generation for health applications: A supervised deep learn- ing approach,&quot; in 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEE, 2018, pp. 1164-1167.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_zgaA746">Machine learning: Algorithms, real-world applications and research directions</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Sarker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JcSpJrP">SN computer science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">160</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. H. Sarker, &quot;Machine learning: Algorithms, real-world applications and research directions,&quot; SN computer science, vol. 2, no. 3, p. 160, 2021.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_HE4zZBS">Application of machine learning in ocean data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00530-020-00733-x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zBr5x7f">Multimedia Systems</title>
		<imprint>
			<date type="published" when="2021-02">Feb. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Lou, Z. Lv, S. Dang, T. Su, and X. Li, &quot;Application of machine learning in ocean data,&quot; Multimedia Systems, Feb. 2021.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main" xml:id="_zwqygDt">Behavior priors for efficient reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tirumala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galashov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Tirumala, A. Galashov, H. Noh, L. Hasenclever, R. Pascanu, J. Schwarz, G. Desjardins, W. M. Czarnecki, A. Ahuja, Y. W. Teh et al., &quot;Behavior priors for efficient reinforcement learning,&quot; 2020.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_b9rK628">Hierarchical reinforcement learning, sequential behavior, and the dorsal frontostriatal system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lewarne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Averbeck</surname></persName>
		</author>
		<idno type="DOI">10.1162/jocn_a_01869</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2C9k9mG">Journal of Cognitive Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1307" to="1325" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Janssen, C. LeWarne, D. Burk, and B. B. Averbeck, &quot;Hierarchical reinforcement learning, sequential behavior, and the dorsal frontostriatal system,&quot; Journal of Cognitive Neuroscience, vol. 34, no. 8, pp. 1307- 1325, 2022.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_gPpgYJp">An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tsiakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Papakostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Theofanidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Makedon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3056540.3076191</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KzSMyCS">Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments</title>
		<meeting>the 10th International Conference on PErvasive Technologies Related to Assistive Environments</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-06">Jun. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Tsiakas, M. Papakostas, M. Theofanidis, M. Bell, R. Mihalcea, S. Wang, M. Burzo, and F. Makedon, &quot;An interactive multisensing framework for personalized human robot collaboration and assistive training using reinforcement learning,&quot; in Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments. ACM, Jun. 2017.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_9TVtXbw">Methods for robot behavior adaptation for cognitive neurorehabilitation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kubota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Riek</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-control-042920-093225</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qFrdkFN">Annual Review of Control, Robotics, and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="135" />
			<date type="published" when="2022-05">May 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Kubota and L. D. Riek, &quot;Methods for robot behavior adaptation for cognitive neurorehabilitation,&quot; Annual Review of Control, Robotics, and Autonomous Systems, vol. 5, no. 1, pp. 109-135, May 2022.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_jbH84Kn">Intelligent health monitoring system modeling based on machine learning and agent technology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Elouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ellouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ltifi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
		<idno type="DOI">10.3233/mgs-200329</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2zZFkqK">Multiagent and Grid Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="226" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Elouni, H. Ellouzi, H. Ltifi, and M. B. Ayed, &quot;Intelligent health monitoring system modeling based on machine learning and agent technology,&quot; Multiagent and Grid Systems, vol. 16, no. 2, pp. 207-226, 2020.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_v55nXPs">From personalized timely notification to healthy habit formation: a feasibility study of reinforcement learning approaches on synthetic data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lisowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peleg</surname></persName>
		</author>
		<idno type="DOI">10.1109/cbms52027.2021.00061</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xsqZ8W5">SMARTERCARE@ AI* IA</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7" to="18" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Lisowska, S. Wilk, and M. Peleg, &quot;From personalized timely noti- fication to healthy habit formation: a feasibility study of reinforcement learning approaches on synthetic data.&quot; in SMARTERCARE@ AI* IA, 2021, pp. 7-18.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_63HTSD8">Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozdoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NyNz2SQ">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">338</biblScope>
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. Yom-Tov, G. Feraru, M. Kozdoba, S. Mannor, M. Tennenholtz, and I. Hochberg, &quot;Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system,&quot; Journal of Medical Internet Research, vol. 19, no. 10, p. e338, Oct. 2017.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_mC7Kzrb">Electronic health records based reinforcement learning for treatment optimizing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_e4dvaPM">Information Systems</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">101878</biblScope>
			<date type="published" when="2022-02">Feb. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Li, Z. Wang, W. Lu, Q. Zhang, and D. Li, &quot;Electronic health records based reinforcement learning for treatment optimizing,&quot; Information Systems, vol. 104, p. 101878, Feb. 2022.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_nWcwhYW">A deep reinforcement learning method for multimodal data fusion in action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/lsp.2021.3128379</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XR5ZSgf">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="120" to="124" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Guo, Q. Liu, and E. Chen, &quot;A deep reinforcement learning method for multimodal data fusion in action recognition,&quot; IEEE Signal Processing Letters, vol. 29, pp. 120-124, 2022.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_YJdNTx7">Framu: Attention-based machine unlearning using federated reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ywzUjNw">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Shaik, X. Tao, L. Li, H. Xie, T. Cai, X. Zhu, and Q. Li, &quot;Framu: Attention-based machine unlearning using federated reinforce- ment learning,&quot; IEEE Transactions on Knowledge and Data Engineer- ing, 2024.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_CTb5cJt">Lio-a personal robot assistant for human-robot interaction and care applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mišeikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Caroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duchamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mišeikienė</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zwilling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Castelbajac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Früh</surname></persName>
		</author>
		<idno type="DOI">10.1109/lra.2020.3007462</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZCFcJDF">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5339" to="5346" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Mišeikis, P. Caroni, P. Duchamp, A. Gasser, R. Marko, N. Mišeikienė, F. Zwilling, C. De Castelbajac, L. Eicher, M. Früh et al., &quot;Lio-a personal robot assistant for human-robot interaction and care applications,&quot; IEEE Robotics and Automation Letters, vol. 5, no. 4, pp. 5339-5346, 2020.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_fmjsvyX">Reinforcement learning in healthcare: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477600</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_H8vuaQ4">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021-11">nov 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Yu, J. Liu, S. Nemati, and G. Yin, &quot;Reinforcement learning in healthcare: A survey,&quot; ACM Comput. Surv., vol. 55, no. 1, nov 2021.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_jYxTbqb">Exploring the landscape of machine unlearning: A comprehensive survey and taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.06360</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">T. Shaik, X. Tao, H. Xie, L. Li, X. Zhu, and Q. Li, &quot;Exploring the landscape of machine unlearning: A comprehensive survey and taxonomy,&quot; arXiv preprint arXiv:2305.06360, 2023.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main" xml:id="_YDqFvxy">Canberra hospital and health services clinical procedure</title>
		<author>
			<persName><forename type="first">V</forename><surname>Signs</surname></persName>
		</author>
		<idno type="DOI">10.47363/jimrr/2023(2)124</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. Signs, &quot;Canberra hospital and health services clinical procedure,&quot; 2021.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_fvwKGdG">Deep-reinforcementlearning-based autonomous uav navigation with sparse rewards</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/jiot.2020.2973193</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ue59xue">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="6180" to="6190" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Wang, J. Wang, J. Wang, and X. Zhang, &quot;Deep-reinforcement- learning-based autonomous uav navigation with sparse rewards,&quot; IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6180-6190, 2020.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_6CznSS2">Deep PPG: Large-scale heart rate estimation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Indlekofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Laerhoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gHtuASc">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">3079</biblScope>
			<date type="published" when="2019-07">Jul. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Reiss, I. Indlekofer, P. Schmidt, and K. V. Laerhoven, &quot;Deep PPG: Large-scale heart rate estimation with convolutional neural networks,&quot; Sensors, vol. 19, no. 14, p. 3079, Jul. 2019.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_9gRftyx">Introducing wesad, a multimodal dataset for wearable stress and affect detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duerichen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Van Laerhoven</surname></persName>
		</author>
		<idno type="DOI">10.1145/3242969.3242985</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9hFrkQa">Proceedings of the 20th ACM international conference on multimodal interaction</title>
		<meeting>the 20th ACM international conference on multimodal interaction</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="400" to="408" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Schmidt, A. Reiss, R. Duerichen, C. Marberger, and K. Van Laer- hoven, &quot;Introducing wesad, a multimodal dataset for wearable stress and affect detection,&quot; in Proceedings of the 20th ACM international conference on multimodal interaction, 2018, pp. 400-408.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_65R4aBD">A runtime monitoring framework to enforce invariants on reinforcement learning agents exploring complex environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mallozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pelliccione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tei</surname></persName>
		</author>
		<idno type="DOI">10.1109/rose.2019.00011</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NMaxYZJ">IEEE/ACM 2nd International Workshop on Robotics Software Engineering (RoSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05">2019. May 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Mallozzi, E. Castellano, P. Pelliccione, G. Schneider, and K. Tei, &quot;A runtime monitoring framework to enforce invariants on reinforcement learning agents exploring complex environments,&quot; in 2019 IEEE/ACM 2nd International Workshop on Robotics Software Engineering (RoSE). IEEE, May 2019.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_8VXEJBS">Autonomous tracking using a swarm of UAVs: A constrained multi-agent reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QMbzXUS">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="13" to="702" />
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y.-J. Chen, D.-K. Chang, and C. Zhang, &quot;Autonomous tracking using a swarm of UAVs: A constrained multi-agent reinforcement learning approach,&quot; IEEE Transactions on Vehicular Technology, vol. 69, no. 11, pp. 13 702-13 717, Nov. 2020.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_ucYhUXB">Multi-agent actor-critic for mixed cooperative-competitive environments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pieter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CJA5Y8P">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch, &quot;Multi-agent actor-critic for mixed cooperative-competitive environ- ments,&quot; Advances in neural information processing systems, vol. 30, 2017.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_j729WXm">Monotonic value function factorisation for deep multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samvelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>De Witt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foerster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MUPcNM4">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">178</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson, &quot;Monotonic value function factorisation for deep multi- agent reinforcement learning,&quot; Journal of Machine Learning Research, vol. 21, no. 178, pp. 1-51, 2020.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_6EgwBvJ">Optimizing decision-making processes in times of covid-19: using reflexivity to counteract informationprocessing failures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Schippers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Rus</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.650525</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_76wMKHJ">Frontiers in psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">650525</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. C. Schippers and D. C. Rus, &quot;Optimizing decision-making pro- cesses in times of covid-19: using reflexivity to counteract information- processing failures,&quot; Frontiers in psychology, vol. 12, p. 650525, 2021.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_mwSJzF2">Graph-enabled reinforcement learning for time series forecasting with adaptive intelligence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shaik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/tetci.2024.3398024</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_H5q35S6">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Shaik, X. Tao, H. Xie, L. Li, J. Yong, and Y. Li, &quot;Graph-enabled reinforcement learning for time series forecasting with adaptive in- telligence,&quot; IEEE Transactions on Emerging Topics in Computational Intelligence, 2024.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
