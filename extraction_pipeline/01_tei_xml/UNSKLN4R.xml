<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_zpctdYZ">Large Language Models for Time Series: A Survey</title>
				<funder>
					<orgName type="full">Google</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100006785</idno>
				</funder>
				<funder ref="#_EcZHGse">
					<orgName type="full">ACE</orgName>
				</funder>
				<funder>
					<orgName type="full">Cisco-UCSD Sponsored</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA</orgName>
				</funder>
				<funder ref="#_PwUeAkc">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_27YUS9m">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000002</idno>
				</funder>
				<funder>
					<orgName type="full">Semiconductor Research Corporation</orgName>
					<orgName type="abbreviated">SRC</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000028</idno>
				</funder>
				<funder ref="#_XCxaSCR">
					<orgName type="full">NSF CAREER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Ranak</roleName><forename type="first">Xiyuan</forename><surname>Zhang</surname></persName>
							<email>xiyuanzh@ucsd.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of California , San Diego</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roy</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of California , San Diego</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rajesh</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
							<email>rgupta@ucsd.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of California , San Diego</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
							<email>jshang@ucsd.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of California , San Diego</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_Ty5dp9M">Large Language Models for Time Series: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1A4DAE015D25A3544ECA9652FAD61FA4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_ZWbXFAn"><p xml:id="_5XzEpam"><s xml:id="_c4RRV2E">Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision.</s><s xml:id="_uWNSJuu">Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance.</s><s xml:id="_dN9cgmc">This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis.</s><s xml:id="_hvMwGvb">We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis.</s><s xml:id="_MP5yEfh">We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools.</s><s xml:id="_Rudz4Pw">Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets in diverse domains, and discusses the challenges and future opportunities of this emerging field.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_3TYnJEb">Introduction</head><p xml:id="_SbDycJn"><s xml:id="_2XAdpwm">Time series analysis plays a critical role in a variety of fields, including climate modeling, traffic management, healthcare monitoring and finance analytics.</s><s xml:id="_ZBsBszX">Time series analysis comprises a wide range of tasks such as classification <ref type="bibr">[Liu et al., 2023b]</ref>, forecasting <ref type="bibr" target="#b24">[Gruver et al., 2023]</ref>, anomaly detection, and imputation.</s><s xml:id="_7WYhsYG">Traditionally, these tasks have been tackled using classical signal processing techniques such as time-frequency analysis and decomposition-based approaches.</s><s xml:id="_z8jgVUA">More recently, deep learning approaches like Convolutional Neural Networks (CNNs), Long Short-Term Memory networks (LSTMs) <ref type="bibr">[Zhang et al., 2023a]</ref>, and Transformers <ref type="bibr">[Jin et al., 2023a]</ref> have revolutionized this field and proved effective in extracting meaningful patterns from time series data, making them the primary approaches of time series analysis in various application domains.</s></p><p xml:id="_XHX8eVQ"><s xml:id="_NSAMcSz">LLM Forecasting Classification Text Generation Anomaly Detection EEG Finance Traffic Audio Multiple Tasks Diverse Domains</s></p><p xml:id="_ZFsJwyB"><s xml:id="_X9VAj4g">Table IoT Robotics ECG Interpolation Time Series Generation In recent years, Large Language Models (LLMs) have gained substantial attention particularly in the fields of Natural Language Processing (NLP) and Computer Vision (CV).</s><s xml:id="_vGK8Etq">Prominent models such as GPT-4 have transformed the landscape of text processing by offering unprecedented accuracy in tasks such as text generation, translation, sentiment analysis, question answering and summarization.</s><s xml:id="_wvEFuVd">In the CV domain, Large Multimodal Models (LMMs) have also facilitated advancements in image recognition, object detection, and generative tasks, leading to more intelligent and capable visual systems <ref type="bibr" target="#b16">[Girdhar et al., 2023]</ref>.</s><s xml:id="_Rkk3jJy">Inspired by these successes, researchers are now exploring the potential of LLMs in the realm of time series analysis, expecting further breakthroughs, as shown in Figure <ref type="figure" target="#fig_1">1</ref>.</s><s xml:id="_563KggS">While several surveys offer a broad perspective on large models for time series in general <ref type="bibr">[Jin et al., 2023b;</ref><ref type="bibr" target="#b41">Ma et al., 2023]</ref>, these do not specifically focus on LLMs or the key challenge of bridging modality gap, which stems from LLMs being originally trained on discrete textual data, in contrast to the continuous numerical nature of time series.</s></p><p xml:id="_PMp7e8B"><s xml:id="_FEjSqzx">Our survey uniquely contributes to the existing literature by emphasizing how to bridge such modality gap and transfer knowledge from LLMs for time series analysis.</s><s xml:id="_nNSqS9C">Our survey also covers more diverse application domains, ranging from climate, Internet of Things (IoT), to healthcare, traffic management, and finance.</s><s xml:id="_qske7Y4">Moreover, certain intrinsic properties of time series, like continuity, auto-regressiveness, and dependency on the sampling rate, are also shared by audio, speech, and music data.</s><s xml:id="_amBN5gw">Therefore, we also present representative LLM-based works from these domains to explore how we can use LLMs for other types of time series.</s><s xml:id="_VaCYcgt">We present a comprehensive taxonomy by categorizing these methodologies into five distinct groups, as shown in Figure 2. If we outline typical LLM-driven NLP pipelines in five stages -input text, tokenization, embedding, LLM, output -then each category of our taxonomy targets one specific stage in this pipeline.</s><s xml:id="_HfjAyFW">Specifically, (i) Prompting (input stage) treats time series data as raw text and directly prompts LLMs with time series; (ii) Time Series Quantization (tokenization stage) discretizes time series as special tokens for LLMs to process; (iii) Aligning (embedding stage) designs time series encoder to align time series embeddings with language space; (iv) Vision as Bridge (LLM stage) connects time series with Vision-Language Models (VLM) by employing visual representations as a bridge; (v) Tool Integration (output stage) adopts LLMs to output tools to benefit time series analysis.</s><s xml:id="_u2pK4ap">Beyond this taxonomy, our survey also compiles an extensive list of existing multimodal datasets that incorporate both time series and text.</s><s xml:id="_8MhtBtZ">We conclude our paper by discussing future research directions in this emerging and promising field.</s></p><p xml:id="_Cu9Bxjf"><s xml:id="_Vg8UvTy">We maintain an up-to-date Github repository 1 which includes all the papers and datasets discussed in the survey.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_PpVKWYG">Background and Problem Formulation</head><p xml:id="_ky2nfyA"><s xml:id="_A6pSUny">Large language models are characterized by their vast number of parameters and extensive training data.</s><s xml:id="_zRGnHE2">They excel in understanding, generating, and interpreting human language, and recently represent a significant advancement in artificial intelligence.</s><s xml:id="_4j45bpU">The inception of LLMs can be traced back to models like GPT-2, BERT, BART, and T5, which laid the foundational architecture.</s><s xml:id="_5fFCHCK">Over time, the evolution of these models has been marked by increasing complexity and capabilities, such as LLAMA-2, PaLM, and GPT-4.</s><s xml:id="_6n5g3rt">More recently, researchers have developed multimodal large language models to integrate and interpret multiple forms of data, such as text, images, and time series, to achieve a more comprehensive understanding of information.</s></p><p xml:id="_FGfXjZW"><s xml:id="_XY43zWy">1 <ref type="url" target="https://github.com/xiyuanzh/awesome-llm-time-series">https://github.com/xiyuanzh/awesome-llm-time-series</ref></s><s xml:id="_RU9jN8P">This survey focuses on how LLMs could benefit time series analysis.</s><s xml:id="_WVakSpr">We first define the mathematical formulation for the input and output, which may contain time series or (and) text depending on the downstream tasks, as well as the models.</s><s xml:id="_JnA3ynP">Input.</s><s xml:id="_P8CEVve">Denoted as x, composed of time series x s ∈ R T ×c and optional text data x t represented as strings, where T, c represent the sequence length and the number of features.</s><s xml:id="_rwRRsQn">Output.</s><s xml:id="_UUS6zmv">Denoted as y and may represent time series, text or numbers depending on the specific downstream task.</s><s xml:id="_RbjG4UV">For time series generation or forecasting task, y represents generated time series y s or predicted k-step future time series y T +1:T +k s .</s><s xml:id="_vVy7AA3">For text generation task, such as report generation, y represents text data y t .</s><s xml:id="_fYFbEtw">For time series classification or regression task, y represents numbers indicating the predicted classes or numerical values.</s><s xml:id="_ajP7z4b">Model.</s><s xml:id="_McTmPgx">We use f θ parameterized by θ, g ϕ parameterized by ϕ, and h ψ parameterized by ψ to represent language, time series and vision models, where f θ is typically initialized from pre-trained large language models.</s><s xml:id="_QRzrmr6">We optimize parameters θ, ϕ and ψ through loss function L.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_4XQnAab">Taxonomy</head><p xml:id="_crFhsMf"><s xml:id="_GPZqpn4">In this section, we detail our taxonomy of applying LLMs for time series analysis, categorized by five groups.</s><s xml:id="_5bceWE3">We summarize the representative works, mathematical formulation, advantages and limitations of each category in Table <ref type="table" target="#tab_4">1</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_SyJdCzf">Prompting</head><p xml:id="_bf3a72m"><s xml:id="_p3387Mp">Number-Agnostic Tokenization.</s><s xml:id="_PCUxTzX">The method treats numerical time series as raw textual data and directly prompts existing LLMs.</s><s xml:id="_XSCFgyp">For example, PromptCast <ref type="bibr" target="#b74">[Xue and Salim, 2022]</ref> proposes prompt-based time series forecasting by converting numerical time series into text prompts and forecasting time series in a sentence-to-sentence manner.</s><s xml:id="_W7Zhmmh">The input prompts are composed of context and questions following pre-defined templates, e.g., "From {t 1 } to {t obs }, the average temperature of region {U m } was {x m t } degree on each</s></p><p xml:id="_k5QZFPM"><s xml:id="_QXEJUz7">Time Series Encoder 1 2 3 … K 2 i 3 K Time Series Decoder Codebook D D D Embedding (a) VQ-VAE based quantization method.</s><s xml:id="_fCG8y5w">Feature Extraction D Embedding 2 1 3 1 K-Means Masking Encoding (b) K-Means based quantization method.</s><s xml:id="_QA6ub2n">Figure 3: Two types of index-based quantization methods.</s><s xml:id="_crzcvqn">day.</s><s xml:id="_mKaWUYU">What is the temperature going to be on {t obs }?" Similar prompting methods have been applied to forecast Placeof-Interest (POI) customer flows (AuxMobLCast) and user's next location (LLM-Mob).</s><s xml:id="_8rSwuu2">Recent works also prompt PaLM-24B for health-related tasks such as activity recognition and daily stress estimate [Liu et al., 2023b].</s><s xml:id="_kAzPWp7">For example, they prompt the model to "classify the following accelerometer data in meters per second squared as either walking or running: 0.052, 0.052, 0.052, 0.051, 0.052, 0.055, 0.051, 0.056, 0.06, 0.064".</s><s xml:id="_29KCXnd">Other examples include extracting historical price features such as open, close, high, and low prices to prompt ChatGPT in a zero-shot fashion [Xie et al., 2023a].</s></p><p xml:id="_ztHENCm"><s xml:id="_qWpyKub">Number-Specific Tokenization.</s><s xml:id="_3PEts5J">More recently, LLM-Time <ref type="bibr" target="#b24">[Gruver et al., 2023]</ref> pointed out that Byte Pair Encoding (BPE) tokenization has the limitation of breaking a single number into tokens that don't align with the digits, leading to inconsistent tokenization across different floating point numbers and complicating arithmetic operations.</s><s xml:id="_4hnVAnM">Therefore, following LLMs such as LLaMA and PaLM, they propose to insert spaces between digits to ensure distinct tokenization of each digit and use a comma (",") to separate each time step in a time series.</s><s xml:id="_xAdmpqF">They also scale time series to optimize token usage and keep fixed precision (e.g., two digits of precision) to efficiently manage context length.</s><s xml:id="_4KHVcJk">For example, they convert "0.123, 1.23, 12.3, 123.0" to "1 2 , 1 2 3 , 1 2 3 0 , 1 2 3 0 0".</s><s xml:id="_sSGyFfR">Meanwhile, BloomberGPT <ref type="bibr" target="#b68">[Wu et al., 2023]</ref> trains on financial data with text and numerical data and places each digit in its own chunk to better handle numbers.</s><s xml:id="_ftPkavH">Using similar space-prefixed tokenization, recent works also show that large language models are general pattern machines capable of sequence transformation, completion and improvement.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_bQzJvcU">Quantization</head><p xml:id="_kQj5KwE"><s xml:id="_9SMuKg5">Quantization based method converts numerical data into discrete representations as input to LLMs.</s><s xml:id="_jyj6a5e">This approach can be further divided into two main categories based on the discretization technique employed.</s></p><p xml:id="_XGD6pBG"><s xml:id="_dA5aCNG">Discrete Indices from VQ-VAE.</s><s xml:id="_UNRjSzT">The first type of quantization method transforms continuous time series into discrete indices as tokens.</s><s xml:id="_MW44bMQ">Among them one of the most popular methods is training a Vector Quantized-Variational AutoEncoder (VQ-VAE), which learns a codebook</s></p><formula xml:id="formula_0">C = {c i } K i=1 of K D-dimensional codewords c i ∈ R D</formula><p xml:id="_dDycr7R"><s xml:id="_nzNESsG">to capture the latent representations, as illustrated in Figure <ref type="figure">3a</ref>.</s><s xml:id="_EBpkJkH">The method identifies the nearest neighbor k i of each step i of the encoded time series representation g ϕ (x s ) ∈ R T S ×D in the codebook (S denotes the cumulative stride of VQ-VAE encoder), and uses the corresponding indices k as the quantized input to language models:</s></p><formula xml:id="formula_1">q i = c ki , k i = arg min j ∥g ϕ (x s ) i -c j ∥ 2 , k = [k i ] T S i=1 . (1)</formula><p xml:id="_FNN3BFQ"><s xml:id="_5pz2ZDr">Based on VQ-VAE, Auto-TTE <ref type="bibr" target="#b10">[Chung et al., 2023]</ref> quantizes ECGs into discrete formats and generates 12-lead ECG signals conditioned on text reports.</s><s xml:id="_TYVNnpn">DeWave <ref type="bibr" target="#b14">[Duan et al., 2023]</ref> adapts VQ-VAE to derive discrete codex encoding and aligns it with pre-trained BART for open-vocabulary EEGto-text translation tasks.</s><s xml:id="_sBYGsZZ">TOTEM <ref type="bibr" target="#b59">[Talukder and Gkioxari, 2023</ref>] also quantizes time series through VQ-VAE as input to Transformers for multiple downstream applications such as forecasting, classification, and translation.</s><s xml:id="_xx3YaWk">In the audio domain, UniAudio <ref type="bibr" target="#b76">[Yang et al., 2023]</ref> tokenizes different types of target audio using Residual Vector Quantization (RVQ) (a hierarchy of multiple vector quantizers) and supports 11 audio generation tasks.</s><s xml:id="_9KPBqjH">VioLA unifies various crossmodal tasks involving speech and text by converting speech utterances to discrete tokens through RVQ.</s><s xml:id="_pRpVaSE">AudioGen learns discrete audio representations using vector quantization layers and generates audio samples conditioned on text inputs.</s><s xml:id="_JRvTKgZ">Discrete Indices from K-Means.</s><s xml:id="_G9hMh2n">Apart from employing VQ-VAE, researchers have also explored K-Means clustering for index-based tokenization, which uses the centroid indices as discretized tokens, as shown in Figure <ref type="figure">3b</ref>.</s><s xml:id="_e5hgGMq">Such methods are mostly applied in the audio domain.</s><s xml:id="_fMB9JUg">For example, SpeechGPT shows capability to perceive and generate multimodal contents using K-Means based discrete unit extractor.</s><s xml:id="_tdmH5Yp">AudioLM discretizes codes produced by a neural audio codec using K-means clustering to achieve high-quality synthesis.</s><s xml:id="_DwQeM5z">It also combines discretized activations of language models pre-trained on audio using RVQ to capture long-term structure.</s><s xml:id="_j53H3SK">Following the same quantization procedure, Au-dioPaLM <ref type="bibr" target="#b53">[Rubenstein et al., 2023]</ref> aligns PaLM-2 and Au-dioLM with a joint vocabulary that can represent speech and text with discrete tokens.</s><s xml:id="_RDmruef">Discrete Indices from Other Techniques.</s><s xml:id="_ARPnSEC">Apart from the VQ-VAE and K-Means based time-domain quantization, Fre-qTST <ref type="bibr" target="#b37">[Li et al., 2023]</ref> utilizes frequency spectrum as a common dictionary to discretize time series into frequency units with weights for downstream forecasting task.</s><s xml:id="_nP4GJMC">Text Categories.</s><s xml:id="_BQuHmFH">The second type of quantization converts numerical data into pre-defined text categories, which is primarily adopted in financial domain.</s><s xml:id="_b2EgAwn">For example, TDML <ref type="bibr" target="#b78">[Yu et al., 2023]</ref> categorizes the weekly price fluctuations into 12 bins represented as "Di" or "Ui", where "D" indicates a decrease in price and "U" means an increase, and i = 1, 2, 3, 4, 5, 5+ represents the level of price change.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_dR99AnC">Aligning</head><p xml:id="_FAEJkcn"><s xml:id="_76WqnTA">The third type of works trains a separate encoder for time series, and aligns the encoded time series to the semantic space</s></p><p xml:id="_AJytZJx"><s xml:id="_bYUyveP">Time Series Encoder Text Encoder Similarity LLM Matching (a) Aligning by similarity matching (Type one).</s><s xml:id="_cDBzNRK">Time Series Embed Text Embed Encoder Decoder Time Series Output Text Output LLM Prompts</s></p><p xml:id="_frNDaSM"><s xml:id="_qD5aHcx">(b) Aligning with large language models as backbones (Type two), where the output could be time series (e.g., forecasting) or text (e.g., EEG-to-text) depending on the downstream tasks. of language models.</s><s xml:id="_zcXUgym">These works can be further categorized into two groups based on their specific aligning strategies, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>.</s></p><p xml:id="_MYfnt9V"><s xml:id="_S4XHRuy">Similarity Matching through Contrastive Loss.</s><s xml:id="_RX4mPyV">The first type of method aligns the time series embeddings with text embeddings through similarity matching, such as minimizing the contrastive loss:</s></p><formula xml:id="formula_2">L = - 1 B B i=1 log exp(sim(g ϕ (x si ), f θ (x ti ))) 1 γ B k=1 exp(sim(g ϕ (x si ), f θ (x tk ))) 1 γ ,</formula><p xml:id="_HQc8gn8"><s xml:id="_seQdQuK">(2) where B, γ represent batch size and temperature parameter that controls distribution concentrations, and sim represents similarity score, typically computed as inner product:</s></p><formula xml:id="formula_3">sim(g ϕ (x si ), f θ (x ti )) = ⟨g ϕ (x si ), f θ (x ti )⟩.</formula><p xml:id="_JCMp8Zv"><s xml:id="_ScAC9W2">(3)</s></p><p xml:id="_VT7KZZV"><s xml:id="_Qzc2jaG">For instance, ETP <ref type="bibr">[Liu et al., 2023a]</ref> integrates contrastive learning based pre-training to align electrocardiography (ECG) signals with textual reports.</s><s xml:id="_7QgC8rJ">Contrastive framework is also used to align 17 clinical measurements collected in Intensive Care Unit (ICU) to their corresponding clinical notes <ref type="bibr" target="#b32">[King et al., 2023]</ref>.</s><s xml:id="_eNg6P3Q">TEST <ref type="bibr" target="#b57">[Sun et al., 2023]</ref> uses contrastive learning to generate instance-wise, feature-wise, and text-prototype-aligned time series embeddings to align with text embeddings.</s><s xml:id="_BSu3gu9">TENT <ref type="bibr">[Zhou et al., 2023b</ref>] aligns text embeddings with IoT sensor signals through a unified semantic space using contrastive learning.</s><s xml:id="_vB2nYEc">JoLT <ref type="bibr" target="#b4">[Cai et al., 2023]</ref> utilizes Querying Transformer (Q-Former) optimized with contrastive loss to align time series and text representations.</s></p><p xml:id="_ZnXkbNR"><s xml:id="_t4uPhaE">Similarity Matching through Other Losses.</s><s xml:id="_mYjjmTQ">Apart from contrastive loss, other loss functions are also employed to optimize similarity matching between time series embeddings and text embeddings.</s><s xml:id="_PEhPE2r">ECG-LLM <ref type="bibr" target="#b51">[Qiu et al., 2023]</ref> aligns the distribution between ECG and language embedding from ECG statements with an Optimal Transport based loss function to train an ECG report generation model.</s><s xml:id="_Fb2tPD8">MTAM <ref type="bibr" target="#b26">[Han et al., 2022]</ref> uses various aligning techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to align electroencephalography (EEG) features with their corresponding language descriptions.</s><s xml:id="_6t97vb3">LLMs as Backbones.</s><s xml:id="_YcsqMKE">The second type of aligning method directly uses large language models as backbones following time series embedding layers.</s><s xml:id="_z2aZdtg">EEG-to-Text [Wang and <ref type="bibr" target="#b65">Ji, 2022]</ref> feeds EEG embeddings to pre-trained BART for open vocabulary EEG-To-Text decoding and EEG-based sentiment classification.</s><s xml:id="_nTaNhps">GPT4TS <ref type="bibr">[Zhou et al., 2023a]</ref> uses patching embeddings as input to frozen pre-trained GPT-2 where the positional embedding layers and self-attention blocks are retained during time series fine-tuning.</s><s xml:id="_XjqDBwZ">The method provides a unified framework for seven time series tasks, including few-shot or zero-shot learning.</s><s xml:id="_j5YHD3N">Following GPT4TS, researchers further incorporated seasonal-trend decomposition (TEMPO <ref type="bibr" target="#b6">[Cao et al., 2023]</ref>), two-stage finetuning (LLM4TS <ref type="bibr" target="#b8">[Chang et al., 2023]</ref>), domain descriptions (UniTime), graph attention mechanism (GATGPT), and spatial-temporal embedding module (ST-LLM).</s><s xml:id="_xNv8DHY">Time-LLM <ref type="bibr">[Jin et al., 2023a]</ref> reprograms time series data into text prototypes as input to LLaMA-7B.</s><s xml:id="_sepFUVg">It also provides natural language prompts such as domain expert knowledge and task instructions to augment input context.</s><s xml:id="_yWfkPsT">Lag-Llama builds univariate probabilistic time series forecasting model based on LLaMA architecture.</s><s xml:id="_4bwBeCF">In the audio, speech and music domains, researchers have also designed dedicated encoders to embed speech (WavPrompt, Speech LLaMA), music (MU-LLaMA), and general audio inputs (LTU <ref type="bibr" target="#b18">[Gong et al., 2023]</ref>, SALMONN <ref type="bibr" target="#b61">[Tang et al., 2023]</ref>), and feed the embeddings to large language models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4" xml:id="_CPpDAFq">Vision as Bridge</head><p xml:id="_WUbqhme"><s xml:id="_NH3cF7s">Time series data can be effectively interpreted or associated with visual representations, which align closer with textual data and have demonstrated successful integrations with large language models.</s><s xml:id="_m5ZRhA3">Therefore, researchers have also leveraged vision modality as a bridge to connect time series with LLMs.</s><s xml:id="_P4NJgXe">Paired Data.</s><s xml:id="_73kZTcZ">ImageBind <ref type="bibr" target="#b16">[Girdhar et al., 2023]</ref> uses imagepaired data to bind six modalities (images, text, audio, depth, thermal, and Inertial Measurement Unit (IMU) time series) and learn a joint embedding space, enabling new emergent alignments and capabilities.</s><s xml:id="_DTEVrGv">PandaGPT <ref type="bibr" target="#b55">[Su et al., 2023]</ref> further combines the multimodal encoders from ImageBind and large language models to enable visual and auditory instruction-following capabilities.</s><s xml:id="_RWhkqbe">IMU2CLIP <ref type="bibr" target="#b43">[Moon et al., 2022]</ref> aligns IMU time series with video and text, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP).</s><s xml:id="_CU4Rj3G">AnyMAL <ref type="bibr" target="#b45">[Moon et al., 2023]</ref> builds upon IMU2CLIP by training a lightweight adapter to project the IMU embeddings into the text token embedding space of LLaMA-2-70B.</s><s xml:id="_6krdmYY">It is also capable of transforming data from other modalities, such as images, videos, audio, into the same text embedding space.</s><s xml:id="_aVWFmxw">Physics Relationships.</s><s xml:id="_Nzg9Dwd">IMUGPT <ref type="bibr" target="#b36">[Leng et al., 2023]</ref>  Time Series Plots as Images.</s><s xml:id="_fqx3GVY">CLIP-LSTM <ref type="bibr" target="#b66">[Wimmer and Rekabsaz, 2023]</ref> transforms stock market data into sequences of texts and images of price charts, and leverages pre-trained CLIP vision-language model to generate features for downstream forecasting.</s><s xml:id="_BAAE9bg">Insight Miner <ref type="bibr">[Zhang et al., 2023b]</ref> converts time series windows into images using lineplot, and feeds images into vision language model LLaVA to generate time series trend descriptions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5" xml:id="_FrxfMnA">Tool</head><p xml:id="_KAqJW8F"><s xml:id="_CxsWKcr">This type of method does not directly use large language models to process time series.</s><s xml:id="_vdKJb5w">Instead, it applies large language models to generate indirect tools z(•), such as code and API calls, to benefit time series related tasks.</s></p><p xml:id="_yCchEsc"><s xml:id="_SCRBU8w">Code.</s><s xml:id="_78fqDUW">CTG++ <ref type="bibr" target="#b84">[Zhong et al., 2023]</ref> applies GPT-4 to generate differentiable loss functions in a code format from text descriptions to guide the diffusion model to generate traffic trajectories.</s><s xml:id="_cdn5W9d">With this two-step translation, the large language model and diffusion model efficiently bridge the gap between user intent and traffic simulation.</s><s xml:id="_RgCvZHe">API Call.</s><s xml:id="_DYsB8CM">ToolLLM <ref type="bibr" target="#b49">[Qin et al., 2023]</ref> introduces a general tool-use framework composed of data construction, model training, and evaluation.</s><s xml:id="_U9wVtGQ">This framework includes API calls for time series tasks such as weather and stock forecasting.</s><s xml:id="_cXmJP2w">Text Domain Knowledge.</s><s xml:id="_GG295Hf">SHARE [Zhang et al., 2023a] exploits the shared structures in human activity label names and proposes a sequence-to-sequence structure to generate label names as token sequences to preserve the shared label structures.</s><s xml:id="_wVU4Yfp">It applies GPT-4 to augment semantics of label names.</s><s xml:id="_xPdk3fe">GG-LLM [Graule and Isler, 2023] leverages LLaMA-2 to encode world knowledge of common human behavioral patterns to predict human actions without further training.</s><s xml:id="_A4THka3">SCRL-LG [Ding et al., 2023] leverages LLaMA-7B as stock feature selectors to extract meaningful representations from news headlines, which are subsequently employed in reinforcement learning for precise feature alignments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_mTrwzKp">Comparison within the Taxonomy</head><p xml:id="_DDCWvMP"><s xml:id="_HP5bjWS">We compare the five categories of our taxonomy and provide general guidelines for which category to choose based on considerations of data, model, efficiency and optimization.</s></p><p xml:id="_TMzmdq9"><s xml:id="_F4CVWBp">Data.</s><s xml:id="_s7nw8gu">When no training data is available and the objective is to apply LLM for time series in an zero-shot fashion, it is preferable to use prompting-based methods.</s><s xml:id="_BuZgTDs">This is because direct prompting enables the utilization of pre-trained language models' inherent capabilities without fine-tuning.</s><s xml:id="_Jnj37Sw">However, representing numbers as strings can diminish the semantic value intrinsically tied to numerical data.</s><s xml:id="_7zMkg3U">Therefore, with adequate training data, quantization or aligningbased methods become more advantageous.</s><s xml:id="_5YDhk55">As shown in Figure <ref type="figure" target="#fig_2">2</ref>, these two categories are the most extensively studied ones in existing literature.</s><s xml:id="_8QdH4Cq">Furthermore, if time series data can be interpreted or associated with visual representations, these representations can be incorporated to utilize the intrinsic knowledge embedded in the vision modality or pre-trained vision-language models.</s></p><p xml:id="_NH5AtKq"><s xml:id="_yudRjeF">Model.</s><s xml:id="_8ZSkXhA">Prompting and tool integration methods tend to apply billion-parameter models as they often apply off-theself LLMs without architectural modifications.</s><s xml:id="_VvgCqaj">By contrast, aligning and quantization methods vary from million to billion-parameter models, depending on the specific application requirements and available computational resources.</s></p><p xml:id="_5BsXvjQ"><s xml:id="_BDx2Tmf">Efficiency.</s><s xml:id="_JAX8pea">Prompting-based methods are not efficient for numerical data with high precision, as well as multivariate time series as it requires transforming each dimension into separate univariate time series, resulting in extremely long input.</s><s xml:id="_BrDhse7">They are also less efficient for long-term predictions due to the computational demands of generating long sequences.</s><s xml:id="_YQnnUYG">These methods are more effective when dealing with simple numerical data that is richly interwoven with textual information, such as opening and closing stock prices in financial news articles.</s><s xml:id="_XCVgcKB">By contrast, quantization and aligning meth- ods are more efficient to handle long sequences, as time series are typically down-sampled or segmented into patches before feeding into large language models.</s></p><p xml:id="_a59gSKP"><s xml:id="_f9QbcSg">Optimization.</s><s xml:id="_DvgmreZ">Depending on the specific discretization technique, quantization based method may require a twostage training process (such as first training the VQ-VAE model), which may result in sub-optimal performance compared with that achieved through end-to-end training in aligning methods.</s><s xml:id="_AzbceEB">Using large language models as indirect tools empowers LLMs with more capabilities to manage numerical data, but also raises the level of complexity to optimize both LLMs and other components in an end-to-end fashion.</s><s xml:id="_9rAReNU">Therefore, existing works of tool integration typically employ off-the-shelf LLMs without further fine-tuning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_SmgZEH3">Multimodal Datasets</head><p xml:id="_Pk2WtuA"><s xml:id="_Yk7kgQ4">Applying LLMs for time series benefits from the availability of multimodal time series and text data.</s><s xml:id="_gCksXyw">In this section, we introduce representative multimodal datasets organized by their respective domains (Table <ref type="table" target="#tab_6">2</ref>).</s><s xml:id="_AUJgeUT">Due to space limit, additional datasets are listed in our Github repository<ref type="foot" target="#foot_11">foot_11</ref> .</s></p><p xml:id="_dzdKZdk"><s xml:id="_5HbXZnZ">Internet of Things (IoT).</s><s xml:id="_rNz9yCS">Human activity recognition is an important task in IoT domain, which identifies human activities given time series collected with IoT devices (such as IMU sensors).</s><s xml:id="_gaQWb6g">The corresponding text data are the labels or text descriptions of these activities.</s><s xml:id="_ZQfmRyj">Ego4D <ref type="bibr" target="#b22">[Grauman et al., 2022]</ref> presents 3,670 hours of daily-life activity data with multiple modalities, including IMU time series, and dense temporallyaligned textual descriptions of the activities.</s><s xml:id="_jZ2HJet">Ego-Exo4D further offers three kinds of paired natural language datasets including expert commentary, narrate-and-act descriptions provided by the participants themselves, and atomic action descriptions similar as Ego4D.</s><s xml:id="_wr3cr2D">DeepSQA <ref type="bibr" target="#b72">[Xing et al., 2021]</ref> presents a generalized Sensory Question Answering (SQA) framework to facilitate querying raw sensory data related to human activities using natural language.</s></p><p xml:id="_XNVPkT7"><s xml:id="_wRX8vG8">Finance.</s><s xml:id="_y2Zngcv">PIXIU <ref type="bibr">[Xie et al., 2023b]</ref> presents multi-task and multi-modal instruction tuning data in the financial domain with 136K data samples.</s><s xml:id="_FBX9qTM">It contains both financial natural language understanding and prediction tasks, and covers 9 datasets of multiple modalities such as text and time series.</s><s xml:id="_Qt6QxDX">MoAT <ref type="bibr" target="#b34">[Lee et al., 2023]</ref> constructs multimodal datasets with textual information paired with time series for each timestep, such as news articles extracted with relevant keywords, mostly covering finance related domains such as fuel, metal, stock and bitcoin.</s></p><p xml:id="_nhWVBcY"><s xml:id="_Sn2NXzD">Healthcare.</s><s xml:id="_V8bsFyf">Zuco datasets <ref type="bibr" target="#b28">[Hollenstein et al., 2019]</ref> contain simultaneous eye-tracking and EEG during natural reading and during annotation.</s><s xml:id="_pkAMw7P">PTB-XL <ref type="bibr" target="#b63">[Wagner et al., 2020]</ref> offers comprehensive metadata regarding ECG annotated by expert cardiologists, covering information such as ECG reports, diagnostic statements, diagnosis likelihoods, and signal-specific properties.</s><s xml:id="_w5Vaf6s">Based on PTB-XL, ECG-QA <ref type="bibr" target="#b47">[Oh et al., 2023]</ref> introduces the first Question Answering dataset for ECG analysis, containing 70 question templates that cover a wide range of clinically relevant ECG topics.</s></p><p xml:id="_95GSGg4"><s xml:id="_HFUZBQy">Audio/Music/Speech.</s><s xml:id="_ZhunJNJ">AudioSet is a collection of 2 million 10-second audio clips excised from YouTube videos and labeled with the sounds that the clip contains from a set of 527 labels.</s><s xml:id="_dFQc4yX">OpenAQA-5M <ref type="bibr" target="#b18">[Gong et al., 2023]</ref> dataset consists of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples.</s><s xml:id="_bzZde9u">MusicCaps <ref type="bibr" target="#b0">[Agostinelli et al., 2023]</ref> is a high-quality music caption dataset, including 5.5K music clips.</s><s xml:id="_2tURbhM">MTG-Jamendo is a dataset with 55,000 audio songs in various languages.</s><s xml:id="_YtRf94a">Libri-Light is an English dataset encompassing 60,000 hours of speech data.</s><s xml:id="_EyuZTea">Common-Voice <ref type="bibr" target="#b2">[Ardila et al., 2019]</ref> is a multilingual speech dataset consisting of 7,335 validated hours in 60 languages.</s><s xml:id="_RUWMJyd">These datasets offer valuable benchmarks for multimodal time series and text analysis.</s><s xml:id="_euRwNnf">These contain both time series focused tasks, including classification, which is evaluated using accuracy and macro-F1 scores, and forecasting, which utilizes metrics such as MSE, MAE, RMSE, and MAPE, as well as NLP focused tasks such as captioning, question answering, and translation, assessed through BLEU, ROUGE, METEOR, and EM scores, among others.</s></p><p xml:id="_VMuw5Bf"><s xml:id="_wTburuU">6 Challenges and Future Directions</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" xml:id="_JjCysFy">Theoretical Understanding</head><p xml:id="_5R3urn4"><s xml:id="_EzXcMbA">Existing works empirically show the benefits of applying LLMs for time series analysis.</s><s xml:id="_Zj2BYqc">For example, recent works have empirically shown that large language models learn linear representations of space and time across multiple scales that are robust to prompting variations.</s><s xml:id="_Vtjpm4r">Despite these empirical findings, there remains a gap in theoretical understanding of how models, primarily trained on textual data, can effectively interpret numerical time series.</s><s xml:id="_cb3edvG">As a preliminary theoretical analysis, it is proved that Transformer models can universally approximate arbitrary continuous sequenceto-sequence functions on a compact domain <ref type="bibr" target="#b80">[Yun et al., 2019]</ref>.</s><s xml:id="_CgpH3mK">Additionally, GPT4TS <ref type="bibr">[Zhou et al., 2023a]</ref> theoretically shows that such generic capability of large language models can be related to Principal Component Analysis (PCA), as minimizing the gradient with respect to the self-attention layer shares similarities with PCA.</s><s xml:id="_gPQvVqM">Further investigations on the generalizability of large language models on numerical data is essential to establish solid understanding of the synergy between LLMs and time series analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_P3mdBFa">Multimodal and Multitask Analysis</head><p xml:id="_BHnXA47"><s xml:id="_dnj2mfY">Existing papers that apply LLMs for time series analysis mostly focus on single modality and single task at a time, such as forecasting, classification, text generation, and do not support simultaneous multimodal and multitask analysis.</s><s xml:id="_PJGNpNX">In computer vision and audio domains, models such as Unified-IO and UniAudio <ref type="bibr" target="#b76">[Yang et al., 2023]</ref> have unified multiple input modalities into a sequence of discrete vocabulary tokens to support multiple tasks within a single transformer-based architecture.</s><s xml:id="_PMxzqwW">More research into leveraging LLMs for multimodal and multitask analysis would lead to more powerful time series foundation models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" xml:id="_gesyZ6Z">Efficient Algorithms</head><p xml:id="_EEYtnPc"><s xml:id="_nW9DcZS">Time series, especially those that are multivariate or possess long history information may increase the computational complexity for existing large language models.</s><s xml:id="_zNT7uPe">Patching (treating each segmented patch as a token) has been a widely adopted strategy to improve performance as well as reduce complexity, but large patches may obscure the semantic information of time series and negatively impact the performance.</s><s xml:id="_gggeMVB">Therefore, developing more efficient algorithms is especially crucial for facilitating large-scale time series analysis with LLMs and enhancing interactions with end users.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4" xml:id="_H8dMWDD">Combining Domain Knowledge</head><p xml:id="_k4VPGJB"><s xml:id="_edGhzjA">Combining existing statistical domain knowledge with LLMs may further boost the model's capability for time series analysis.</s><s xml:id="_ZpEuHDx">For example, TEMPO <ref type="bibr" target="#b6">[Cao et al., 2023]</ref> applies time series seasonal-trend decomposition and treats decomposed components as different semantic inductive biases as input to the pre-trained transformer.</s><s xml:id="_U8yTVfG">FreqTST <ref type="bibr" target="#b37">[Li et al., 2023]</ref> leverages insights from the frequency domain by tokenizing single time series into frequency units with weights for downstream forecasting.</s><s xml:id="_nZQnacc">Further incorporating domain knowledge, such as wavelet decomposition, auto-correlation analysis, and empirical mode decomposition may augment LLMs' capabilities in analyzing time series data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5" xml:id="_bJakgyM">Customization and Privacy</head><p xml:id="_zsw69pF"><s xml:id="_MNVVDvD">Existing works on large language models and time series analysis typically train a global model for all end users.</s><s xml:id="_ugCacwB">Training customized models for different users based on the global model may bring further benefits and flexibility.</s><s xml:id="_F9g3EnY">Another important consideration is privacy, especially as many time series data are collected in private settings for clinical purposes or smart home applications.</s><s xml:id="_h8weZg5">Federated learning offers a solution by enabling the training of machine learning models across multiple decentralized devices holding local data samples.</s><s xml:id="_mXrEAzt">Advancing research into model customization and user privacy preservation like federated learning would broaden the utility of LLM-empowered time series analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_ac2PCNN">Conclusion</head><p xml:id="_gKrtQYx"><s xml:id="_WxsjS8S">We present the first survey that systematically analyzes the categorization of transferring knowledge from large language models for numerical time series analysis: direct prompting, time series quantization, aligning, the use of the vision modality to connect text and time series, and the integration of large language models with other analytical tools.</s><s xml:id="_Rcsc4J3">For each category, we introduce their mathematical formulation, representative works, and compare their advantages and limitations.</s><s xml:id="_xV5mAbG">We also introduce representative multimodal text and time series datasets in various domains such as healthcare, IoT, finance, and audio.</s><s xml:id="_xCGtnMw">Concluding the paper, we outline the challenges and emerging directions for potential future research of LLM-empowered time series analysis.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc><div><p xml:id="_Ur62ekE"><s xml:id="_wHqsgu5">Figure1: Large language models have recently been applied for various time series tasks in diverse application domains.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc><div><p xml:id="_XKyKkVJ"><s xml:id="_NDDzYDu">Figure 2: Left: Taxonomy of LLMs for time series analysis (prompting, quantization, aligning which is further categorized into two groups as detailed in Figure 4, vision as bridge, tool integration).</s><s xml:id="_vf7FnVy">For each category, key distinctions are drawn in comparison to the standard LLM pipeline shown at the top of the figure.</s><s xml:id="_R5Aek8a">Right: We present representative works for each category, sorted by their publication dates.</s><s xml:id="_TNb6Rg9">The use of arrows indicates that later works build upon earlier studies.</s><s xml:id="_UfVHmN5">Dark(light)-colored boxes represent billion(million)-parameter models.</s><s xml:id="_rN939X5">Icons to the left of the text boxes represent the application domains of domain-specific models, with icons' meanings illustrated in Figure 1.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc><div><p xml:id="_a8yAwgU"><s xml:id="_uJWzZU8">Figure 4: Two types of aligning based methods.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc><div><p><s xml:id="_23hDd4d">generates IMU data from ChatGPT-augmented text descriptions.It first generates 3D human motion from text using pretrained motion synthesis model, and derives IMU data from 3D motion based on physics relationships of motion kinetics.Summary of five major categories of applying LLMs for time series analysis, including their respective subcategories, representative works, mathematical formulations, advantages and limitations.</s><s xml:id="_bMb8Mk9">q and xv represent text-based quantization process and image data.</s></p></div></figDesc><table><row><cell>Method</cell><cell>Subcategory</cell><cell>Representative Works</cell><cell cols="2">Equations</cell><cell>Advantages</cell><cell>Limitations</cell></row><row><cell>Prompting</cell><cell cols="2">Number-Agnostic PromptCast [Xue and Salim, 2022] Number-Specific LLMTime [Gruver et al., 2023]</cell><cell cols="2">y = f θ (xs, xt)</cell><cell cols="2">easy to implement; lose semantics; zero-shot capability not efficient</cell></row><row><cell></cell><cell>VQ-VAE</cell><cell>DeWave [Duan et al., 2023]</cell><cell cols="2">ki = arg minj ∥g ϕ (xs)i -cj∥2</cell><cell>flexibility of</cell><cell>may require</cell></row><row><cell>Quantization</cell><cell>K-Means</cell><cell>AudioPaLM [Rubenstein et al., 2023]</cell><cell>k = [ki]</cell><cell>T S i=1 , y = f θ (k, xt)</cell><cell>index and time</cell><cell>two-stage</cell></row><row><cell></cell><cell>Text Categories</cell><cell>TDML [Yu et al., 2023]</cell><cell cols="2">y = f θ (q(xs), xt)</cell><cell>series conversion</cell><cell>training</cell></row><row><cell>Aligning</cell><cell>Similarity Match</cell><cell>ETP [Liu et al., 2023a] MATM [Han et al., 2022]</cell><cell cols="2">y = g ϕ (xs) L = sim(g ϕ (xs), f θ (xt))</cell><cell>align semantics of different modalities;</cell><cell>complicated design and</cell></row><row><cell></cell><cell>LLM Backbone</cell><cell>GPT4TS [Zhou et al., 2023a]</cell><cell cols="2">y = f θ (g ϕ (xs), xt)</cell><cell>end-to-end training</cell><cell>fine-tuning</cell></row><row><cell>Vision as</cell><cell>Paired Data</cell><cell>ImageBind [Girdhar et al., 2023]</cell><cell cols="2">L = sim(g ϕ (xs), h ψ (xv))</cell><cell>additional visual</cell><cell>not hold</cell></row><row><cell>Bridge</cell><cell>TS Plots</cell><cell>[Wimmer and Rekabsaz, 2023]</cell><cell cols="2">y = h ψ (xs)</cell><cell>knowledge</cell><cell>for all data</cell></row><row><cell>Tool</cell><cell>Code API</cell><cell>CTG++ [Zhong et al., 2023] ToolLLM [Qin et al., 2023]</cell><cell cols="2">z = f θ (xt) y = z(xs)</cell><cell cols="2">empower LLM with more abilities not end-to-end optimization</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc><div><p xml:id="_Ay5CPAx"><s xml:id="_ayZtN5n">Summary of representative time series and text multimodal datasets.</s></p></div></figDesc><table><row><cell>Domain</cell><cell>Dataset</cell><cell>Size</cell><cell>Major Modalities</cell><cell>Task</cell></row><row><cell>IoT</cell><cell>Ego4D 2 [Grauman et al., 2022] DeepSQA 3 [Xing et al., 2021]</cell><cell cols="3">3, 670h data, 3.85M narrations text, IMU, video, audio, 3D classification, forecasting 25h data, 91K questions text, imu classification, QA</cell></row><row><cell>Finance</cell><cell>PIXIU 4 [Xie et al., 2023b] MoAT 5 [Lee et al., 2023]</cell><cell>136K instruction data 6 datasets, 2K timesteps</cell><cell>text, tables text, time series</cell><cell>5 NLP tasks, forecasting forecasting</cell></row><row><cell></cell><cell>Zuco 2.0 6 [Hollenstein et al., 2019]</cell><cell>739 sentences</cell><cell>text, eye-tracking, EEG</cell><cell>classification, text generation</cell></row><row><cell>Healthcare</cell><cell>PTB-XL 7 [Wagner et al., 2020]</cell><cell>60h data, 71 unique statements</cell><cell>text, ECG</cell><cell>classification</cell></row><row><cell></cell><cell>ECG-QA 8 [Oh et al., 2023]</cell><cell>70 question templates</cell><cell>text, ECG</cell><cell>classification, QA</cell></row><row><cell>Audio</cell><cell>OpenAQA-5M 9 [Gong et al., 2023]</cell><cell>5.6M (audio, QA) tuples</cell><cell>text, audio</cell><cell>tagging, classification</cell></row><row><cell>Music</cell><cell>MusicCaps 10 [Agostinelli et al., 2023]</cell><cell>5.5K music clips</cell><cell>text, music</cell><cell>captioning, generation</cell></row><row><cell>Speech</cell><cell>CommonVoice 11 [Ardila et al., 2019]</cell><cell>7, 335h in 60 languages</cell><cell>text, speech</cell><cell>ASR, translation</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_M9xUMXx"><s xml:id="_kkU8Gj6">Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence Survey Track</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p xml:id="_useQ9TS"><s xml:id="_wuUpZyr">https://ego4d-data.org/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p xml:id="_fTcN5Ng"><s xml:id="_5TGEXqe">https://github.com/nesl/DeepSQA</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p xml:id="_VKQrwNu"><s xml:id="_dpr5Tjh">https://github.com/chancefocus/PIXIU</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p xml:id="_6n3RAYq"><s xml:id="_rTJrSwN">https://openreview.net/pdf?id=uRXxnoqDHH</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p xml:id="_pNu6ERq"><s xml:id="_qtKcWdc">https://osf.io/2urht/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p xml:id="_bUbwZ7M"><s xml:id="_rt67VAQ">https://physionet.org/content/ptb-xl/1.0.3/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p xml:id="_RfYjcGx"><s xml:id="_MCyKk5B">https://github.com/Jwoo5/ecg-qa</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p xml:id="_UqgvRWx"><s xml:id="_2H3mBNS">https://github.com/YuanGongND/ltu</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p xml:id="_bYXDQUN"><s xml:id="_fmzRyc4">https://www.kaggle.com/datasets/googleai/musiccaps</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p xml:id="_3EDWX7z"><s xml:id="_gDCQSz9">https://commonvoice.mozilla.org/en/datasets</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p xml:id="_6vMURwx"><s xml:id="_q2vrTqe">https://github.com/xiyuanzh/awesome-llm-time-series</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_9KqVwvZ">Acknowledgements</head><p xml:id="_Dgx2BPt"><s xml:id="_urpu9cp">Our work is supported in part by <rs type="funder">ACE</rs>, one of the seven centers in <rs type="grantNumber">JUMP 2.0</rs>, a <rs type="funder">Semiconductor Research Corporation</rs> (SRC) program sponsored by <rs type="funder">DARPA</rs>.</s><s xml:id="_Y8qkUgm">Our work is also sponsored by <rs type="funder">NSF CAREER</rs> Award <rs type="grantNumber">2239440</rs>, <rs type="funder">NSF</rs> <rs type="programName">Proto-OKN</rs> Award <rs type="grantNumber">2333790</rs>, <rs type="funder">NIH</rs> <rs type="programName">Bridge2AI Center Program</rs> under award <rs type="grantNumber">1U54HG012510-01</rs>, <rs type="funder">Cisco-UCSD Sponsored</rs> Research Project, as well as generous gifts from <rs type="funder">Google</rs>, Adobe, and Teradata.</s><s xml:id="_NsNrAht">Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and should not be interpreted as necessarily representing the views, either expressed or implied, of the <rs type="institution">U.S. Government.The U.S. Government</rs> is authorized to reproduce and distribute reprints for government purposes not withstanding any copyright annotation hereon.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EcZHGse">
					<idno type="grant-number">JUMP 2.0</idno>
				</org>
				<org type="funding" xml:id="_XCxaSCR">
					<idno type="grant-number">2239440</idno>
				</org>
				<org type="funding" xml:id="_PwUeAkc">
					<idno type="grant-number">2333790</idno>
					<orgName type="program" subtype="full">Proto-OKN</orgName>
				</org>
				<org type="funding" xml:id="_27YUS9m">
					<idno type="grant-number">1U54HG012510-01</idno>
					<orgName type="program" subtype="full">Bridge2AI Center Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Agostinelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Agostinelli et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_Pm4mqa6">Musiclm: Generating music from text</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zalán</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Borsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Verzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Caillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Tagliasacchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.11325</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Cail- lon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating music from text. arXiv:2301.11325, 2023.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ardila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ardila et al., 2019]</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_PKNfNKw">Common voice: A massively-multilingual speech corpus</title>
		<author>
			<persName><forename type="first">Rosana</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsay</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06670</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. Common voice: A massively-multilingual speech corpus. arXiv:1912.06670, 2019.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.5194/tc-2020-91-rc1</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cai et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_47ZWZT3">Jolt: Jointly learned representations of language and timeseries</title>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mononito</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Choudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><surname>Dubrawski</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v38i21.30423</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fcgtGxU">NeurIPS Deep Generative Models for Health Workshop</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yifu Cai, Mononito Goswami, Arjun Choudhry, Arvind Srinivasan, and Artur Dubrawski. Jolt: Jointly learned representations of language and time- series. In NeurIPS Deep Generative Models for Health Workshop, 2023.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.5194/acp-2016-553-rc2</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cao et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_ecASPKN">Tempo: Prompt-based generative pre-trained transformer for time series forecasting</title>
		<author>
			<persName><forename type="first">Defu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiang</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04948</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-based generative pre-trained transformer for time series forecasting. arXiv:2310.04948, 2023.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chang et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Ching</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chih</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3719207</idno>
		<idno type="arXiv">arXiv:2308.08469</idno>
		<title level="m" xml:id="_vG9csve">Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv:2308.08469, 2023.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Chung</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chung et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_PPSZ8mk">Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinical text reports</title>
		<author>
			<persName><forename type="first">Hyunseung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joonmyoung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ki-Hyun</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Sung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Kf7x5Ff">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note type="raw_reference">Hyunseung Chung, Jiho Kim, Joon- myoung Kwon, Ki-Hyun Jeon, Min Sung Lee, and Edward Choi. Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinical text reports. In ICASSP, pages 1- 5. IEEE, 2023.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ding</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ding et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_HKkvBDz">Integrating stock features and global information via large language models for enhanced stock return prediction</title>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingcheng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuliu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongming</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.05627</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yujie Ding, Shuai Jia, Tianyi Ma, Bingcheng Mao, Xiuze Zhou, Liuliu Li, and Dongming Han. Integrating stock features and global information via large language models for enhanced stock return predic- tion. arXiv:2310.05627, 2023.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Duan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Duan et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_nyPFN7b">Dewave: Discrete encoding of eeg waves for eeg to text translation</title>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Teng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2Xj8mZG">NeurIPS</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yiqun Duan, Charles Zhou, Zhen Wang, Yu-Kai Wang, and Chin-teng Lin. Dewave: Discrete en- coding of eeg waves for eeg to text translation. In NeurIPS, 2023.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Girdhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Girdhar et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_U2EKqaN">Imagebind: One embedding space to bind them all</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalyan</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr52729.2023.01457</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Pc4c6HV">CVPR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15180" to="15190" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Ar- mand Joulin, and Ishan Misra. Imagebind: One embed- ding space to bind them all. In CVPR, pages 15180-15190, 2023.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.5194/acp-2018-376-rc1</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gong et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_2gaQwSF">Listen, think, and understand</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<idno type="DOI">10.1109/asru57964.2023.10389742</idno>
		<idno type="arXiv">arXiv:2305.10790</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuan Gong, Hongyin Luo, Alexander H Liu, Leonid Karlinsky, and James Glass. Listen, think, and understand. arXiv:2305.10790, 2023.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Isler</forename><surname>Graule</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Graule and Isler, 2023]</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Graule</surname></persName>
		</author>
		<author>
			<persName><surname>Isler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.20034</idno>
		<title level="m" xml:id="_hTQa9GK">Gg-llm: Geometrically grounding large language models for zero-shot human activity forecasting in human-aware task planning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Moritz A Graule and Volkan Isler. Gg-llm: Geometrically grounding large language models for zero-shot human activity forecasting in human-aware task planning. arXiv:2310.20034, 2023.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Grauman</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.19588/fig-2</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grauman et al., 2022]</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_RTcTAh7">Ego4d: Around the world in 3,000 hours of egocentric video</title>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XWh4yWW">CVPR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18995" to="19012" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Ro- hit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, pages 18995-19012, 2022.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Gruver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gruver et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main" xml:id="_AsBjVse">Large language models are zero-shot time series forecasters</title>
		<author>
			<persName><forename type="first">Nate</forename><surname>Gruver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikai</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2310.07820</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. arXiv:2310.07820, 2023.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1016/s1369-7021(22)00193-6</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Han et al., 2022]</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_SzCB2ft">An empirical exploration of cross-domain alignment between language and electroencephalogram</title>
		<author>
			<persName><forename type="first">William</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jielin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.06348</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">William Han, Jielin Qiu, Jiacheng Zhu, Mengdi Xu, Douglas Weber, Bo Li, and Ding Zhao. An empirical exploration of cross-domain alignment between language and electroencephalogram. arXiv:2208.06348, 2022.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hollenstein</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.19053/fig-3</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hollenstein et al., 2019]</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Nora</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Troendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Langer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00903</idno>
		<title level="m" xml:id="_uvGC2VW">Zuco 2.0: A dataset of physiological recordings during natural reading and annotation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nora Hollenstein, Marius Troen- dle, Ce Zhang, and Nicolas Langer. Zuco 2.0: A dataset of physiological recordings during natural reading and an- notation. arXiv:1912.00903, 2019.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main" xml:id="_Kz78gmM">Time-llm: Time series forecasting by reprogramming large language models</title>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01728</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jin et al., 2023a] Ming Jin, Shiyu Wang, Lintao Ma, Zhix- uan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv:2310.01728, 2023.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main" xml:id="_wgwFCmq">Large models for time series and spatio-temporal data: A survey and outlook</title>
		<author>
			<persName><forename type="first">Jin</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2310.10196</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jin et al., 2023b] Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, et al. Large models for time series and spatio-temporal data: A survey and out- look. arXiv:2310.10196, 2023.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>King</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">King et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_Yffuvcn">Multimodal pretraining of medical time series and notes</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobak</forename><forename type="middle">J</forename><surname>Mortazavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4sEFS62">ML4H</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="244" to="255" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ryan King, Tianbao Yang, and Bobak J Mortazavi. Multimodal pretraining of medical time series and notes. In ML4H, pages 244-255. PMLR, 2023.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_TCafnVe">Moat: Multi-modal augmented time series forecasting</title>
		<author>
			<persName><forename type="first">Geon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Geon Lee, Wenchao Yu, Wei Cheng, and Haifeng Chen. Moat: Multi-modal augmented time series forecasting. 2023.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main" xml:id="_ZQPA8sX">Generating virtual on-body accelerometer data from virtual textual descriptions for human activity recognition</title>
		<author>
			<persName><surname>Leng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.03187</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Leng et al., 2023] Zikang Leng, Hyeokhyen Kwon, and Thomas Plötz. Generating virtual on-body accelerometer data from virtual textual descriptions for human activity recognition. arXiv:2305.03187, 2023.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_SzvVa4H">Modeling time series as text sequence a frequencyvectorization transformer for time series forecasting</title>
		<author>
			<persName><forename type="first">Junkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Junkai Li, Weizhi Ma, and Yang Liu. Modeling time series as text sequence a frequency- vectorization transformer for time series forecasting. 2023.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.07145</idno>
		<title level="m" xml:id="_nJCGqjy">Etp: Learning transferable ecg representations via ecg-text pre-training</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu et al., 2023a] Che Liu, Zhongwei Wan, Sibo Cheng, Mi Zhang, and Rossella Arcucci. Etp: Learning transferable ecg representations via ecg-text pre-training. arXiv:2309.07145, 2023.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main" xml:id="_pVZNjye">Large language models are few-shot health learners</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15525</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu et al., 2023b] Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming- Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Pa- tel. Large language models are few-shot health learners. arXiv:2305.15525, 2023.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ma et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenjing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.10716</idno>
		<title level="m" xml:id="_wnVGXyu">A survey on time-series pre-trained models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong Yu, and James T Kwok. A survey on time-series pre-trained models. arXiv:2305.10716, 2023.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Moon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Moon et al., 2022]</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main" xml:id="_tU8jQQq">Imu2clip: Multimodal contrastive learning for imu motion sensors from egocentric videos and text</title>
		<author>
			<persName><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Dirafzoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aparajita</forename><surname>Saraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Damavandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.14395</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Alireza Dirafzoon, Aparajita Saraf, Amy Bearman, and Babak Damavandi. Imu2clip: Multimodal contrastive learning for imu motion sensors from egocen- tric videos and text. arXiv:2210.14395, 2022.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>Moon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Moon et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main" xml:id="_QHNdwVr">Anymal: An efficient and scalable anymodality augmented language model</title>
		<author>
			<persName><forename type="first">Seungwhan</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Fu</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakash</forename><surname>Murugesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16058</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. Anymal: An efficient and scalable any- modality augmented language model. arXiv:2309.16058, 2023.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName><surname>Oh</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.18888/fig-2</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Oh et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Jungwoo</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongsu</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyubok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Myoung</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.15681</idno>
		<title level="m" xml:id="_aHPTrBW">Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jungwoo Oh, Seongsu Bae, Gyubok Lee, Joon-myoung Kwon, and Edward Choi. Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram. arXiv:2306.15681, 2023.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName><surname>Qin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qin et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main" xml:id="_5PjmmtF">Toolllm: Facilitating large language models to master 16000+ real-world apis</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yining</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaxi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16789</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yujia Qin, Shihao Liang, Yining Ye, Kun- lun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv:2307.16789, 2023.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName><surname>Qiu</surname></persName>
		</author>
		<idno type="DOI">10.5194/gmd-2018-256-rc2</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qiu et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main" xml:id="_Fx69s3x">Transfer knowledge from natural language to electrocardiography: Can we detect cardiovascular disease through language models?</title>
		<author>
			<persName><forename type="first">Jielin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emerson</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.09017</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jielin Qiu, William Han, Jiacheng Zhu, Mengdi Xu, Michael Rosenberg, Emerson Liu, Dou- glas Weber, and Ding Zhao. Transfer knowledge from natural language to electrocardiography: Can we de- tect cardiovascular disease through language models? arXiv:2301.09017, 2023.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><surname>Rubenstein</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj-cs.2869/table-1</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rubenstein et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main" xml:id="_gvJZwCz">Audiopalm: A large language model that can speak and listen</title>
		<author>
			<persName><forename type="first">Chulayuth</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><surname>Asawaroengchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dung</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zalán</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Félix</forename><surname>Borsos</surname></persName>
		</author>
		<author>
			<persName><surname>De Chaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalia</forename><forename type="middle">El</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Badawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Kharitonov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.12925</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Au- diopalm: A large language model that can speak and listen. arXiv:2306.12925, 2023.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Su et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main" xml:id="_n6dKJ2Z">Pandagpt: One model to instruction-follow them all</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.16355</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. arXiv:2305.16355, 2023.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.19824/fig-8</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sun et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenda</forename><surname>Hong</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.4460608</idno>
		<idno type="arXiv">arXiv:2308.08241</idno>
		<title level="m" xml:id="_dQxCxdP">Test: Text prototype aligned embedding to activate llm&apos;s ability for time series</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chenxi Sun, Yaliang Li, Hongyan Li, and Shenda Hong. Test: Text prototype aligned embedding to activate llm&apos;s ability for time series. arXiv:2308.08241, 2023.</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gkioxari</forename><surname>Talukder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Talukder and Gkioxari, 2023]</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main" xml:id="_KQCKMsQ">Time series modeling at scale: A universal representation across tasks and domains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sabera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Talukder</surname></persName>
		</author>
		<author>
			<persName><surname>Gkioxari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sabera J Talukder and Geor- gia Gkioxari. Time series modeling at scale: A universal representation across tasks and domains. 2023.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.5194/bg-2016-113-ac3</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tang et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main" xml:id="_7dDWrt9">Salmonn: Towards generic hearing abilities for large language models</title>
		<author>
			<persName><forename type="first">Changli</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.13289</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for large language models. arXiv:2310.13289, 2023.</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wagner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wagner et al., 2020]</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_54EcNSC">Ptb-xl, a large publicly available electrocardiography dataset</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf-Dieter</forename><surname>Bousseljot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Kreiseler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatima</forename><forename type="middle">I</forename><surname>Lunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schaeffter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zhWHRYN">Scientific data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Patrick Wagner, Nils Strodthoff, Ralf- Dieter Bousseljot, Dieter Kreiseler, Fatima I Lunze, Woj- ciech Samek, and Tobias Schaeffter. Ptb-xl, a large pub- licly available electrocardiography dataset. Scientific data, 7(1):154, 2020.</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_b438qhq">Open vocabulary electroencephalography-to-text decoding and zero-shot sentiment classification</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><forename type="middle">;</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhailong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_r5sjaQe">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang and Ji, 2022] Zhenhailong Wang and Heng Ji. Open vocabulary electroencephalography-to-text decoding and zero-shot sentiment classification. In AAAI, 2022.</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Rekabsaz</forename><surname>Wimmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wimmer and Rekabsaz, 2023]</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main" xml:id="_WXXVvqY">Leveraging vision-language models for granular market change prediction</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navid</forename><surname>Rekabsaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.10166</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Christopher Wimmer and Navid Rekabsaz. Leveraging vision-language models for granular market change prediction. arXiv:2301.10166, 2023.</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.5194/gmd-2017-220-rc2</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main" xml:id="_Xyad23T">Bloomberggpt: A large language model for finance</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Dabravolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhanjan</forename><surname>Kambadur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Mann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.17564</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prab- hanjan Kambadur, David Rosenberg, and Gideon Mann. Bloomberggpt: A large language model for finance. arXiv:2303.17564, 2023.</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main" xml:id="_qwVzkD7">The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges</title>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.05351</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xie et al., 2023a] Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. The wall street neo- phyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges. arXiv:2304.05351, 2023.</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main" xml:id="_XBHJEWU">Pixiu: A large language model, instruction data and evaluation benchmark for finance</title>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.05443</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xie et al., 2023b] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. Pixiu: A large language model, instruction data and evaluation benchmark for finance. arXiv:2306.05443, 2023.</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xing et al., 2021]</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_WPeeqFJ">Deepsqa: Understanding sensor data via question answering</title>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alun</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wpADT96">IoTDI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tianwei Xing, Luis Garcia, Federico Cerutti, Lance Kaplan, Alun Preece, and Mani Srivastava. Deepsqa: Understanding sensor data via question answer- ing. In IoTDI, 2021.</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Salim</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xue and Salim, 2022]</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main" xml:id="_Y3MkwGv">Promptcast: A new prompt-based learning paradigm for time series forecasting</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flora</forename><forename type="middle">D</forename><surname>Salim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hao Xue and Flora D Salim. Prompt- cast: A new prompt-based learning paradigm for time se- ries forecasting. 2022.</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main" xml:id="_qHWS4sR">Uniaudio: An audio foundation model toward universal audio generation</title>
		<author>
			<persName><forename type="first">Dongchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuankai</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixin</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.00704</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dongchao Yang, Jinchuan Tian, Xu Tan, Rongjie Huang, Songxiang Liu, Xuankai Chang, Jiatong Shi, Sheng Zhao, Jiang Bian, Xixin Wu, et al. Uniaudio: An audio foundation model toward universal audio gener- ation. arXiv:2310.00704, 2023.</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main" xml:id="_nz5kptn">Temporal data meets llm-explainable financial time series forecasting</title>
		<author>
			<persName><forename type="first">Xinli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujing</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11025</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xinli Yu, Zheng Chen, Yuan Ling, Shu- jing Dong, Zongyi Liu, and Yanbin Lu. Temporal data meets llm-explainable financial time series forecasting. arXiv:2306.11025, 2023.</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yun</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yun et al., 2019]</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main" xml:id="_nKrB7u9">Are transformers universal approximators of sequence-to-sequence functions?</title>
		<author>
			<persName><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10077</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Ku- mar. Are transformers universal approximators of sequence-to-sequence functions? arXiv:1912.10077, 2019.</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_wCHQUEa">Unleashing the power of shared label structures for human activity recognition</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3583780.3615101</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qe6CEhQ">CIKM</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="3340" to="3350" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang et al., 2023a] Xiyuan Zhang, Ranak Roy Chowd- hury, Jiayun Zhang, Dezhi Hong, Rajesh K Gupta, and Jingbo Shang. Unleashing the power of shared label struc- tures for human activity recognition. In CIKM, pages 3340-3350, 2023.</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_p2bstja">Insight miner: A time series analysis dataset for crossdomain alignment with natural language</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.7717/peerj.19814/fig-2</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Z74mKPe">NeurIPS AI4Science</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang et al., 2023b] Yunkai Zhang, Yawen Zhang, Ming Zheng, Kezhen Chen, Chongyang Gao, Ruian Ge, Siyuan Teng, Amine Jelloul, Jinmeng Rao, Xiaoyuan Guo, et al. Insight miner: A time series analysis dataset for cross- domain alignment with natural language. In NeurIPS AI4Science, 2023.</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title/>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhong et al., 2023]</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<author>
			<persName><forename type="first">Ziyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baishakhi</forename><surname>Ray</surname></persName>
		</author>
		<idno type="DOI">10.1109/icra48891.2023.10161463</idno>
		<idno type="arXiv">arXiv:2306.06344</idno>
		<title level="m" xml:id="_fepMyMU">Language-guided traffic simulation via scene-level diffusion</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, and Baishakhi Ray. Language-guided traffic sim- ulation via scene-level diffusion. arXiv:2306.06344, 2023.</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main" xml:id="_hDUVPhM">One fits all: Power general time series analysis by pretrained lm</title>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11939</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou et al., 2023a] Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained lm. arXiv:2302.11939, 2023.</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main" xml:id="_szmyK5t">Tent: Connect language models with iot sensors for zero-shot activity recognition</title>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.08245</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou et al., 2023b] Yunjiao Zhou, Jianfei Yang, Han Zou, and Lihua Xie. Tent: Connect language mod- els with iot sensors for zero-shot activity recognition. arXiv:2311.08245, 2023.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
