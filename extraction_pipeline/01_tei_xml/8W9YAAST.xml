<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_vFKMAtH">A Primer on Reinforcement Learning in Medicine for Clinicians Check for updates</title>
				<funder>
					<orgName type="full">National Center for Advancing Translational Sciences (GNN)</orgName>
				</funder>
				<funder ref="#_Ewm2ZKY">
					<orgName type="full">Clinical and Translational Science Awards</orgName>
					<orgName type="abbreviated">CTSA</orgName>
				</funder>
				<funder ref="#_eCcsUqx">
					<orgName type="full">NIH/NIDDK</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Pushkala</forename><surname>Jayaraman</surname></persName>
							<idno type="ORCID">0000-0003-1159-8354</idno>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Desman</surname></persName>
							<idno type="ORCID">0000-0002-5411-6637</idno>
						</author>
						<author>
							<persName><forename type="first">Moein</forename><surname>Sabounchi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Girish</forename><forename type="middle">N</forename><surname>Nadkarni</surname></persName>
							<idno type="ORCID">0000-0001-6319-4314</idno>
						</author>
						<author>
							<persName><forename type="first">Ankit</forename><surname>Sakhuja</surname></persName>
						</author>
						<title level="a" type="main" xml:id="_vFFtcKW">A Primer on Reinforcement Learning in Medicine for Clinicians Check for updates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CEDFD15E65E990FF915E429CD14F2696</idno>
					<idno type="DOI">10.1038/s41746-024-01316-0</idno>
					<note type="submission">Received: 8 June 2024; Accepted: 24 October 2024;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_xqtf5xX"><p xml:id="_XpNahRw"><s xml:id="_n2bbAcB">Reinforcement Learning (RL) is a machine learning paradigm that enhances clinical decision-making for healthcare professionals by addressing uncertainties and optimizing sequential treatment strategies.</s><s xml:id="_Gpnq8VM">RL leverages patient-data to create personalized treatment plans, improving outcomes and resource efficiency.</s><s xml:id="_bC7ckyt">This review introduces RL to a clinical audience, exploring core concepts, potential applications, and challenges in integrating RL into clinical practice, offering insights into efficient, personalized, and effective patient care.</s></p><p xml:id="_VZGKyJE"><s xml:id="_dfPxUFa">In healthcare, making the right decisions is crucial, as professionals face complex choices daily, from diagnosis to treatment planning and resource allocation.</s><s xml:id="_eafj5eh">Patient care needs strategies that consider both immediate actions and long-term consequences.</s><s xml:id="_xAzTfaE">Reinforcement Learning (RL), a branch of machine learning, offers a way to enhance decision-making by learning optimal strategies through trial and error and addressing sequential decision-making challenges.</s><s xml:id="_MvM26Qx">RL can model uncertainties, personalize treatments based on patient data, and adapt interventions in real-time, leading to improved patient outcomes, optimized resource utilization, and more efficient healthcare delivery.</s></p><p xml:id="_pZvd87A"><s xml:id="_4bhhqSn">RL is particularly well-suited for critical care due to availability of granular data allowing it to accurately model patient conditions, predict outcomes, and optimize treatment pathways using ICU data.</s><s xml:id="_KrBUSz6">However, RL can also be effectively applied in many other healthcare domains.</s><s xml:id="_eFeMbDS">RL's capacity to continuously learn from data can significantly improve clinical trial outcomes and healthcare practices.</s></p><p xml:id="_cgJ5SpR"><s xml:id="_6gmzCus">This review introduces RL to clinicians and explores its applications for personalized treatment decisions across a range of clinical domains.</s><s xml:id="_zUb7RHJ">It also emphasizes the unique challenges associated with integrating RL into medical research and practice.</s><s xml:id="_M9yh7wr">By doing so, it equips clinicians to critically evaluate the clinical relevance of RL research and highlights its transformative potential in shaping the future of healthcare delivery.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FtwCZaB">RLbasic concepts</head><p xml:id="_mPky7mZ"><s xml:id="_rXCXGrN">Reinforcement Learning (RL) is a machine learning approach which trains agents to learn decision-making strategies or functions ('policy') through continuous interaction with their environment.</s><s xml:id="_pwTZWyE">This interaction involves a process ('states') of trial ('action') and error 1 inspired by human learning, where the agent receives feedback-either in the form of rewards for successful actions or penalties for unsuccessful ones.</s><s xml:id="_QhC8uUG">Over time, this feedback allows the agent to improve its strategy and maximize expected cumulative rewards.</s></p><p xml:id="_qXC48rK"><s xml:id="_GcwGsV8">An intuitive analogy is learning to ride a bicycle.</s><s xml:id="_AvEjTX9">Initially, the learner may face difficulties in balancing, receiving negative reinforcement (e.g., falling) when balance is lost.</s><s xml:id="_NC3YyQZ">As balance is achieved, positive reinforcement (e.g., staying upright) encourages repeating those successful actions.</s><s xml:id="_qyEWSNC">Similarly, RL agents refine their actions based on the feedback they receive, gradually developing a policy that enhances their decision-making.</s></p><p xml:id="_fbZBytR"><s xml:id="_USkQ5Au">Key components of RL include agent, environment, state, action reward and policy (Table <ref type="table">1</ref>).</s></p><p xml:id="_yvRRCRB"><s xml:id="_RR9e9wr">Mathematically, RL can be described using the framework of Markov Decision Processes 2,3 (MDPs).</s><s xml:id="_aQBc9f9">An MDP is represented as a tuple (S, A, P, R, γ) where S is state, A is action, P is transition probability, R is reward and γ is discount factor (Table <ref type="table">2</ref>).</s><s xml:id="_aPcbk72">MDPs offer a structured approach for modeling decision-making problems in which an agent interacts with an environment across discrete time steps.</s><s xml:id="_bAqz9sm">MDPs provide a flexible and widely applicable framework for modeling various decision-making problems, including robotics, game playing, finance, healthcare, and more.</s><s xml:id="_YPyhqqt">They serve as the foundation for many algorithms and techniques in reinforcement learning, allowing agents to learn effective decision-making strategies in complex and uncertain environments.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DCBfVYr">Delving deeper into RL concepts</head><p xml:id="_psv2rSd"><s xml:id="_ZCSFA8x">We have presented a detailed overview of RL concepts designed specifically for clinicians, equipping them with the tools to critically assess literature and understand how it can be integrated into clinical practice.</s><s xml:id="_5hzATQu">For a deeper dive into the mathematical principles that underpin RL in clinical decisionmaking, the Supplementary Note 1 provides a more rigorous exploration (Supplementary Information).</s><s xml:id="_ekgWhks">The main objective of RL is for an agent to learn a policy that maximizes cumulative rewards.</s><s xml:id="_HGMCnZZ">This can be approached through either model-based or model-free RL methods, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Z8xJ9y2">Model-based RL</head><p xml:id="_jJCPUgF"><s xml:id="_5rNzKpC">In model-based learning, the agent learns the model of the environment's dynamics, including transition probabilities and expected rewards <ref type="bibr" target="#b3">4</ref> .</s><s xml:id="_SW2aPsB">This model allows the agent to simulate possible future states and outcomes, facilitating efficient planning and decision-making.</s><s xml:id="_VBhJrYQ">By using the model to simulate trajectories, the agent can anticipate the consequences of its actions without directly interacting with the environment.</s><s xml:id="_JcbZ2m7">Let's take an example a robot learning to navigate a maze.</s><s xml:id="_zqEPAhW">In model-based RL, the robot would construct a model of the maze's layout and dynamics.</s><s xml:id="_nXjBWkK">This model might include information about the maze's structure, such as walls and corridors, as well as the outcomes of actions taken by the robot (e.g., moving forward, turning left or right).</s><s xml:id="_PeS6UHf">By simulating possible trajectories using this model, the robot can then anticipate the consequences of its actions and plan its path through the maze accordingly.</s><s xml:id="_xejVbfq">AlphaZero 5 by DeepMind is an example of model-based RL algorithm implemented using the Monte Carlo Tree Search paradigm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hhduuWT">Model-free RL</head><p xml:id="_3FcRgHf"><s xml:id="_2rgtYcU">Another way for the robot to navigate the maze would be to learn directly from experience, updating its policy based on observed rewards and transitions without explicitly modeling the environment.</s><s xml:id="_FZC2Sx7">This would be an example of model-free <ref type="bibr" target="#b5">6</ref> RL that focuses solely on learning the value of states or state-action pairs through trial and error.</s><s xml:id="_Fc5eu2j">This can be achieved in three ways -value-based, policy-based or hybrid manner.</s></p><p xml:id="_s5bUfRK"><s xml:id="_YNn5nxP">Value-based RL.</s><s xml:id="_N36E2qN">Value-based RL focuses on estimating the value of being in a particular state or taking a particular action, and then using these value estimates to make decisions that maximize cumulative rewards <ref type="bibr" target="#b6">7</ref> .</s><s xml:id="_BxEmFFN">At the core of value-based RL is the concept of the value function, which is a measure of how "good" it is to be in a state i.e. it represents the expected cumulative reward achievable from a given state or state-action pair.</s></p><p xml:id="_6A2d3ak"><s xml:id="_k8hdhjs">The agent can have two different learning strategies <ref type="bibr" target="#b7">8</ref> in value-based RL: on-policy and off-policy; on-policy methods <ref type="bibr" target="#b8">9</ref> update the agent's policy while it interacts with the environment.</s><s xml:id="_DTwybh5">This means the data used for learning comes from the same policy being updated.</s><s xml:id="_HppSwHQ">In other words, the agent learns from its own experiences and updates its policy based on those experiences.</s><s xml:id="_cdaMDcm">For instance, think of a person learning to play a video game.</s><s xml:id="_KgmeQdF">They continuously adjust their strategy based on their current approach, refining their skills as they go.</s></p><p xml:id="_RtjDdKZ"><s xml:id="_kDFNJAV">An example of an on-policy method is SARSA 10 (State-Action-Reward-State-Action).</s><s xml:id="_KAfPHj4">In SARSA, the agent observes the current state, takes an action according to its current policy, receives a reward, observes the next state, updates the policy and then takes another action accordingly.</s><s xml:id="_uYs5WZC">The Q-values are updated based on these stateaction-reward-state-action sequences, ensuring that the learning and acting policies are consistent.</s><s xml:id="_FpuQRng">Off-policy methods <ref type="bibr" target="#b10">11</ref> , in contrast, separate the learning and behavior policies, allowing the agent to learn from experiences collected under a different policy than the one being improved.</s><s xml:id="_wYmbKs2">In other words, the agent learns from data generated by one policy while attempting to optimize a different policy.</s><s xml:id="_vCkzCrA">An analogy might be a chef learning to cook by studying recipes from various sources before developing their own unique style.</s></p><p xml:id="_NC5z7RT"><s xml:id="_NHwwjSN">A classic example of off-policy RL is Q-learning <ref type="bibr" target="#b11">12</ref> .</s><s xml:id="_Xx3qyNz">In Q-learning, the agent maintains a table of Q-values, where each entry represents the estimated expected cumulative reward for taking a specific action in a specific state.</s><s xml:id="_5wMEDPG">The off-policy nature of Q-learning arises from the fact that the agent learns the value of state-action pairs under one policy (often an exploratory behavior policy) while executing a different policy (the target policy) to gather data.</s><s xml:id="_4D89Hbw">This behavior policy is typically exploratory, employing strategies like ε-greedy or Boltzmann exploration to balance exploration and exploitation.</s><s xml:id="_VE3PBsh">Importantly, the Q-values are updated independently of the behavior policy, enabling the agent to learn from experiences gathered under various policies.</s><s xml:id="_eh24gdF">This decoupling of learning and behavior policies allows Q-learning to effectively explore the environment while learning optimal actionselection strategies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9ychtDu">Agent</head><p xml:id="_XfnmQxB"><s xml:id="_zNADxt6">The entity learning to make decisions and take actions within an environment.</s></p><p xml:id="_uHwU3K6"><s xml:id="_vYVZ4yx">The person learning to ride is the agent.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rJkepDk">Environment</head><p xml:id="_BB6E5pT"><s xml:id="_NC7bweA">The external system with which the agent interacts.</s><s xml:id="_th9J7mH">The physical world within which the agent rides the bike or a simulated environment in the context of machine learning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cCrzaw8">State</head><p xml:id="_JH4sFE7"><s xml:id="_NStNCtE">A representation of the current situation or condition of the environment.</s></p><p xml:id="_eXWdzGu"><s xml:id="_CbmJ32F">The state could include factors such as the rider's speed, posture, and proximity to obstacles.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Y48PWRC">Action</head><p xml:id="_Q7BcsCQ"><s xml:id="_4N3PmVG">The decision or choice made by the agent that affects the state of the environment.</s></p><p xml:id="_WQGa9V7"><s xml:id="_AnJ2Psr">Actions could include pedaling, steering, or braking.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SJJUfDK">Reward</head><p xml:id="_v5ARxGS"><s xml:id="_w4gTwDg">Feedback from the environment that evaluates the goodness or badness of an action taken by the agent.</s><s xml:id="_Az6WkuD">Rewards serve as signals to reinforce or discourage certain behaviors.</s></p><p xml:id="_DfUkc7s"><s xml:id="_aEqdcgq">Falling off the bicycle could result in a negative reward, while successfully riding without falling could yield a positive reward</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_D5TkHxy">Policy</head><p xml:id="_CvwWxPj"><s xml:id="_MxGAYzM">The strategy or rule that the agent follows to select sequence of actions based on its current state.</s></p><p xml:id="_W8uPUVH"><s xml:id="_qKNHJwu">Learning to ride the bike could be constituted as the "learned policy".</s><s xml:id="_UpNNZYx">A discount factor of 0 would mean that only immediate rewards will be considered, while a discount factor of 1 would mean that future rewards would be valued equally to immediate rewards.</s></p><p xml:id="_ZbNEr3h"><s xml:id="_ZE5Uadn">Policy-based RL.</s><s xml:id="_5GFBJ2S">Another approach is policy-based learning.</s><s xml:id="_breRns3">Policybased RL directly learns a policy, which is a mapping from states to actions, without explicitly estimating the value of states or state-action pairs <ref type="bibr" target="#b12">13</ref> .</s><s xml:id="_gKGbctG">The policy specifies the probability distribution over actions given to states.</s><s xml:id="_GAz6mcC">The objective is to find the policy that maximizes the expected cumulative rewards.</s><s xml:id="_RGubyJx">For instance, in the REINFORCE algorithm <ref type="bibr" target="#b13">14</ref> , the policy parameters are updated based on the gradient of the expected return with respect to the policy parameters.</s><s xml:id="_fccN6Yr">Policy-based methods excel in handling stochastic policies and exploring the action space effectively.</s><s xml:id="_Dk9YWX5">However, they may demand more data and computational resources to converge compared to value-based methods.</s></p><p xml:id="_uGUcCym"><s xml:id="_9Bn7dW9">Hybrid RL.</s><s xml:id="_AqukrT6">A third class of methods known as hybrid method incorporates both, value-based and policy-based approaches in the same framework.</s><s xml:id="_PA4y5kn">The Actor-Critic 15 class of methods are examples of this approach.</s><s xml:id="_v4rXqUH">Thus, RL offers a diverse range of strategies for agents to learn optimal policies in complex environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_X7wVWXy">Applications of RL</head><p xml:id="_9yp7UAx"><s xml:id="_f7ZqNwB">RL has achieved notable real-world applications.</s><s xml:id="_EZQF3gM">DeepMind's AlphaZero 5 was an early success in mastering Atari games <ref type="bibr" target="#b15">16</ref> like Breakout and Ms. Pac-Man, and widely-known for surpassing human-level performance using Deep Q-Networks (DQN) algorithms.</s><s xml:id="_BRYQpnW">DeepRL has also revolutionized board games like chess <ref type="bibr" target="#b16">17</ref> and plays a crucial role in autonomous driving <ref type="bibr" target="#b17">18</ref> , where agents learn to navigate complex environments safely and efficiently.</s><s xml:id="_Vssh973">In robotics, RL has enabled robots to learn skills such as object grasping <ref type="bibr" target="#b18">19</ref> , assembly, and navigation in unstructured environments.</s><s xml:id="_KJeTFmw">RL's applications extend to large language models, including ChatGPT <ref type="bibr" target="#b19">20</ref> where RL helps improve conversational abilities by learning from user feedback to generate more contextually relevant and coherent text over time.</s><s xml:id="_GwamxAf">Additionally, RL has also been used for generating personalized recommendations <ref type="bibr" target="#b20">21</ref> for subscription-based viewers, content moderation on social media <ref type="bibr" target="#b21">22</ref> platforms, and smart grid systems <ref type="bibr" target="#b22">23</ref> .</s><s xml:id="_fEhwDbe">In each of these examples, the RL agent interacts with an environment, receiving feedback based on its actions and using this feedback to learn and improve its decision-making over time.</s><s xml:id="_xGDSaMa">Such interaction is characteristic of 'online RL', where the agent learns directly from its experiences, making decisions, observing outcomes, and receiving immediate feedback.</s><s xml:id="_dJEWz92">This feedback loop allows the agent to adjust its behavior in real-time, optimizing its actions to maximize cumulative rewards.</s><s xml:id="_9dbB4NN">Online RL algorithms are transforming digital health clinical trials <ref type="bibr" target="#b23">24</ref> by personalizing treatment interventions using real-time participant data.</s><s xml:id="_2WXF2rp">These algorithms dynamically adapt to individual characteristics and responses, facilitating real-time decision-making.</s><s xml:id="_4STDJvv">Their capacity for handling user heterogeneity, distribution shifts, and non-stationarity allows them to modify treatments based on continuous data inputs, facilitating real-time decision-making.</s><s xml:id="_TmzU9Bp">However, in cases where real-time interactions are not feasible or ethical, such as in clinical settings, offline RL offers a safer alternative.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_bWyX78P">Offline RL</head><p xml:id="_z3mq2Bf"><s xml:id="_KAhvAnh">While the previously discussed algorithms illustrate a class of learning based on iteratively collecting data by interacting with an environment and using those interactions to improve decision-making, many scenarios preclude this online interaction.</s><s xml:id="_hxFUw2N">In online learning, any adjustments to the learned policy requires collecting new data following this new policy, which may need to be done millions of times <ref type="bibr" target="#b24">25</ref> during training.</s><s xml:id="_V8V4a8N">Safety, resource availability, cost, time, and lack of a simulation environment 26 may require data collection only once.</s><s xml:id="_RUBVyTq">Moreover, the increasing quantity of large, retrospective datasets that already exist can be repurposed to optimize agents on vast and diverse behaviors that would otherwise be inaccessible to obtain from live experimentation.</s><s xml:id="_b5dBJ2K">These constraints provide significant motivation to effectively convert traditional online RL as an "offline" problem <ref type="bibr" target="#b26">27</ref> .</s></p><p xml:id="_eMKujNT"><s xml:id="_yaEkFSM">Offline Reinforcement Learning (RL) is designed for scenarios where direct interaction with an environment is limited or infeasible.</s><s xml:id="_sxQR8tj">This limitation removes the agent's ability to explore new actions, making it crucial for the dataset to contain high-reward behavior for effective learning.</s><s xml:id="_5C7F2Tm">Additionally, offline RL algorithms must contend with counterfactual ).</s><s xml:id="_yb6f5B2">This document is to confirm that Pushkala Jayaraman has been granted a license to use the BioRender content, including icons, templates and other original artwork, appearing in the attached completed graphic pursuant to BioRender's Academic License Terms.</s><s xml:id="_wrfurjZ">This license permits BioRender content to be sublicensed for use in journal publications.</s><s xml:id="_GNDangf">All rights and ownership of BioRender content are reserved by BioRender.</s><s xml:id="_esCaeGJ">All completed graphics must be accompanied by the following citation: "Created with BioRender.com".</s><s xml:id="_yy2gnhA">BioRender content included in the completed graphic is not licensed for any commercial uses beyond publication in a journal.</s><s xml:id="_3vaYYTV">For any commercial use of this figure, users may, if allowed, recreate it in BioRender under an Industry BioRender Plan.</s><s xml:id="_U63EF9J">For any questions regarding this document, or other questions about publishing with BioRender refer to our BioRender Publication Guide or contact BioRender Support at support@biorender.com.</s></p><p xml:id="_jgWtdS3"><s xml:id="_nvVFxJ7">reasoning, where the aim is not to simply imitate past behaviors but to improve upon them.</s><s xml:id="_YMgjCXC">If the policy deviates significantly from the behavior policy learned from the generated dataset, it can cause a distribution shift <ref type="bibr" target="#b27">28</ref> , distribution shift, leading to discrepancies between expected and real-world outcomes.</s><s xml:id="_PHJQwvc">These challenges would necessitate careful algorithm design and benchmarking <ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">30</ref> .</s></p><p xml:id="_kSufFVR"><s xml:id="_qwrSnNX">Offline RL has evolved through various algorithmic advancements and is an active field of research.</s><s xml:id="_E9EBvFZ">Early approaches, like Behavioral Cloning (BC), focused on mimicking expert demonstrations via supervised <ref type="bibr" target="#b31">31</ref> learning but struggled in generalizing to new or unseen scenarios.</s><s xml:id="_wENbdkh">More sophisticated methods, such as Batch Constrained Q-learning (BCQ) and Conservative Q-Learning (CQL) <ref type="bibr" target="#b32">32</ref> , build on the Q-learning paradigm <ref type="bibr" target="#b33">33</ref> while incorporating constraints to prevent the model from overestimating rewards for actions poorly represented in the dataset thereby allowing agents to cautiously propose new actions.</s><s xml:id="_zHCpz42">Further innovations include Implicit Q-Learning <ref type="bibr" target="#b34">34</ref> (IQL) which estimates optimal action-value functions indirectly to avoid challenges associated with explicit Q-value modeling.</s><s xml:id="_gM87s2n">Additional models like Behavior Regularized Off-Policy Q-Learning <ref type="bibr" target="#b35">35</ref> , Behavior Regularized Off-Policy Q-Learning <ref type="bibr" target="#b36">36</ref> , Conservative Policy Iteration (CPI) <ref type="bibr" target="#b37">37</ref> , Dueling Network Architecture <ref type="bibr" target="#b38">38</ref> , Soft Behavior-regularized Actor Critic(SBAC) <ref type="bibr" target="#b39">39</ref> , and Adversarially Trained Actor-Critic (ATAC) <ref type="bibr" target="#b40">40</ref> address specific challenges associated with learning from fixed datasets of offline experiences.</s><s xml:id="_kt5xQsj">While they offer promising avenues for applying RL in scenarios with limited or no interaction with the environment, they also come with their own limitations and trade-offs (Table <ref type="table" target="#tab_2">3</ref>).</s></p><p xml:id="_6gUcXTe"><s xml:id="_qMEHngf">After training a policy, assessing its performance (evaluation) without direct interaction in live environments or simulations poses challenges.</s><s xml:id="_eNgANEK">However, Off-Policy Evaluation (OPE) methods such as Importance Sampling (IS) and Fitted-Q Evaluation <ref type="bibr" target="#b41">41</ref> (FQE) offer a solution by predicting the trained policy's performance using historical data and probabilistically estimates a policy value.</s><s xml:id="_bKvZpWZ">However, these methods are prone to bias or high variance, depending on the similarity between the evaluation policy and the behavior policy in the dataset.</s><s xml:id="_GJZc7PY">The Doubly Robust <ref type="bibr" target="#b42">42</ref> (DR) method mitigates these issues by combining IS and model-based predictions, leveraging bias and variance mitigations of both methods.</s><s xml:id="_Hk5fhnY">The newer DIstribution Correction Estimation (DICE)-based methods, such as DualDICE <ref type="bibr" target="#b43">43</ref> and GenDICE <ref type="bibr" target="#b44">44</ref> , address evaluation by minimizing a divergence measure to target a core issue of distributional shift in OPE and offline RL.</s><s xml:id="_nmRRAbd">As offline RL continues to advance, refining these methods remains a key area of research.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PpJ5KgZ">Applications of RL in medicine</head><p xml:id="_A3P3ReT"><s xml:id="_KUkUcQZ">The inherent trial-and-error methodology of training RL agents endows it with considerable efficacy, albeit presenting formidable hurdles for its integration into healthcare settings.</s><s xml:id="_n5W8kkt">As a result, most RL applications in healthcare rely on simulated environments or offline learning approaches <ref type="bibr" target="#b45">45</ref> .</s><s xml:id="_WtGBuE3">For example, RL has been applied in simulated environments to optimize the personalized dosing of propofol, a sedative commonly used in intensive care units to ensure appropriate sedation <ref type="bibr" target="#b46">46</ref> .</s><s xml:id="_tjaRdfd">RL has also been applied to identify individualized chemotherapy regimens <ref type="bibr" target="#b47">[47]</ref><ref type="bibr" target="#b48">[48]</ref><ref type="bibr" target="#b49">[49]</ref> such as dynamic treatment regimens (DTRs) <ref type="bibr" target="#b50">50</ref> for cancer patients, optimizing chemotherapy dosing strategies.</s><s xml:id="_4ZQb5ZS">Additionally, RL has been shown to optimize insulin dosing for type 1 diabetics 51 using the FDA approved University of Virginia/ Padova type-1 diabetes simulator.</s><s xml:id="_TBZeCS9">RL has also shown potential in mitigating Parkinson's disease symptoms by optimizing combinations of medications <ref type="bibr" target="#b52">52</ref> and in improving breast cancer screenings using envelope Q-learning <ref type="bibr" target="#b53">53</ref> .</s><s xml:id="_pyewmrN">Another prominent example includes applying Q-learning with expert-assisted rewards for diagnosing skin cancer <ref type="bibr" target="#b54">54</ref> .</s><s xml:id="_8mUXGvx">These examples illustrate the use of RL's sequential decision-making capabilities across diverse medical conditions.</s></p><p xml:id="_7dDxswE"><s xml:id="_vkh9skE">The expansion of RL applications in medicine has predominantly focused on offline RL, especially in the realm of critical care.</s><s xml:id="_MKDqMbH">With access to detailed and granular patient datasets <ref type="bibr" target="#b55">[55]</ref><ref type="bibr" target="#b56">[56]</ref><ref type="bibr" target="#b57">[57]</ref> , datasets, researchers have been able to leverage offline RL to optimize treatment decisions.</s><s xml:id="_teV2M9r">A pioneering example is the work by Nemati et al., who utilized de-identified patient data to develop a deep RL algorithm for optimizing <ref type="bibr" target="#b58">58</ref> heparin dosing policies in critical care settings.</s><s xml:id="_jP99teV">By using activated partial thromboplastin time as a measure of anticoagulant effect, they were able to dynamically adjust dosing based on patient-specific data, accounting for temporal changes observed in electronic medical records.</s><s xml:id="_j4C72jS">Further work by Lin et al. expanded on this by employing a deep deterministic policy gradient framework <ref type="bibr" target="#b59">59</ref> with continuous state-action spaces where they demonstrated that significant deviations between RL-recommended dosing and clinician-prescribed doses correlated with increased complications in patients.</s></p><p xml:id="_fVmfq5y"><s xml:id="_FVhzbs9">The focus of offline RL in critical care primarily targets two crucial aspects: managing sepsis through fluid and vasopressor optimization, and ventilator management for critically ill patients.</s><s xml:id="_tjvr9BY">Initial studies like those by Komorowski et al. applied the SARSA algorithm to sepsis treatment, setting a foundation that was expanded upon by Raghu et al. using Dueling Double Deep Q-Networks <ref type="bibr" target="#b60">60</ref> to refine state-action modeling.</s><s xml:id="_72waJ7N">Subsequently, a sepsis dosing sample-efficient DRL treatment model <ref type="bibr" target="#b61">61</ref> with episodic control utilized the MIMIC III dataset to increase the longevity of sepsis patients.</s><s xml:id="_htjpJNw">Komorowski et al. further built on these advancements leading to the development <ref type="bibr" target="#b62">62</ref> of "AI Clinician" to predict treatment strategies that outperformed real-world clinician decisions.</s><s xml:id="_8Y5bTcg">Further innovations include Wu et al.'s introduction of weighted dueling double deep Q-networks with embedded human expertize (WD3QNE) <ref type="bibr" target="#b63">63</ref> to align closer with clinician strategies (choosing a clinician-based Q function), particularly for patients with specific needs indicated by SOFA scores.</s></p><p xml:id="_rDNKbbJ"><s xml:id="_x72aNS8">Offline RL has also been pivotal in developing weaning strategies for sedation in mechanically ventilated patients, improving safety and outcome metrics.</s><s xml:id="_aGnhu2W">Furthermore, efforts like Kondrup 64 et al.'s "DeepVent" -a Conservative Q-Learning (CQL) offline RL algorithm and Prasad et al.'s dueling network models integrate deep learning with traditional RL methods to wean sedation <ref type="bibr" target="#b65">65</ref> for mechanically ventilated patients while ensuring hemodynamic stability and minimizing re-intubations.</s><s xml:id="_VFKemBg">While Peine et al. focused on using Q learning to optimize mechanical ventilation settings 66 such as tidal volume, positive end expiratory pressure and FiO2 among critically ill patients, Kondrup et.</s><s xml:id="_U3E6Y3Z">al emphasized more conservative approaches to tackle the overestimation of Q values <ref type="bibr" target="#b67">67</ref> and improving accuracy of clinical decisionmaking.</s><s xml:id="_7AFCEN9">Recently, Hengst et al. developed a guideline-informed RL framework for mechanical ventilation <ref type="bibr" target="#b68">68</ref> that incorporated patient-specific masking actions and violation penalties, improving guideline adherence and outperformance in mortality prediction.</s><s xml:id="_Rs3GSFY">However, challenges remain.</s><s xml:id="_gkS9etY">Saghafian's work <ref type="bibr" target="#b69">69</ref> highlights ambiguity in offline RL, presenting a model to manage "New Onset Diabetes After Transplantation" (NODAT) through augmented and safe V-learning, illustrating the need for more robust RL approaches when handling medical uncertainty.</s></p><p xml:id="_xTxj2G8"><s xml:id="_fW3uVCB">Furthermore, RL in DTRs has faced issues with evaluation metrics and standard baselines.</s><s xml:id="_Q9Ea5df">Luo et al. addressed these challenges, proposing DTR-Bench 70 a benchmarking framework that standardizes evaluation in areas such as diabetes, cancer chemotherapy, and sepsis treatment <ref type="bibr" target="#b71">71</ref> .</s><s xml:id="_Z57fmHs">Additionally, RL's role in robotic-assisted surgery (RAS) <ref type="bibr" target="#b72">72</ref> (RAS) continues to evolve, with RL-based systems enhancing adaptability and efficiency through computer vision and RL algorithms, particularly in automating surgical tasks such as knot-tying to save time and improve precision.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_yDQpsuV">Challenges of RL in medicine</head><p xml:id="_eCFb9PR"><s xml:id="_9nc8d5y">The versatility of suitable tasks highlights RL's potential and the transformative impact it could have across various facets of healthcare.</s><s xml:id="_j37yAkU">Through ongoing research and developments, RL will be instrumental in redefining how healthcare professionals' approach daily complex decision-making challenges and assist the discovery of new state-of-the-art care practices.</s><s xml:id="_aZM2jHB">Despite the promise of RL in healthcare, there exist several ongoing challenges and areas of research (Fig. <ref type="figure" target="#fig_1">2</ref>) -</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FujmGW3">State space formulation challenges</head><p xml:id="_6WEYZSD"><s xml:id="_zMfUzsC">In offline RL, the state space is defined by the available data.</s><s xml:id="_JgZsevx">Healthcare data, particularly electronic health records, can be high-dimensional presenting challenges in preprocessing and selecting relevant features.</s><s xml:id="_wQRZVjt">Data quality issues such as missing values, noise, and inconsistencies further complicate state space formulation.</s><s xml:id="_vkPK3Pf">Healthcare data may also exhibit biases due to factors such as demographic disparities, clinical practice variations, and data collection methods <ref type="bibr" target="#b73">73</ref> .</s><s xml:id="_kWVXZjW">These biases can lead to distribution shifts between the offline data and the target policy, affecting the generalizability and performance of learned policies.</s><s xml:id="_MhAJshY">Finally, RL problems are formulated using MDPs which assume that the next state is only dependent on the current state and current action, which may not always be true in medicine.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kc32jDw">Reward formulation challenges</head><p xml:id="_S6rb8Dr"><s xml:id="_PAcCyQk">Designing reward functions in healthcare RL involves subjective judgments and complex trade-offs.</s><s xml:id="_YQa7Twd">Clinical outcomes, patient wellbeing, resource utilization, and adherence to medical guidelines are all factors that may need to be considered.</s><s xml:id="_U23tVDM">Healthcare interventions often have long-term consequences, leading to sparse and delayed feedback on the efficacy of actions.</s><s xml:id="_zXw6DGD">Defining rewards that accurately capture the impact of actions over time while addressing the delay in feedback presents a significant challenge.</s><s xml:id="_S92uWFP">This is seen in multiple studies that determine the dosing of vasopressors or management of mechanical ventilators based on all-cause in-hospital mortality <ref type="bibr" target="#b74">74</ref> .</s><s xml:id="_6tsmT6k">Inverse reinforcement learning (IRL) offers a potential solution by deriving reward functions from observed behaviors or data, thus estimating rewards during the learning process <ref type="bibr" target="#b75">75</ref> .</s><s xml:id="_UXyFmM8">However, IRL also necessitates extensive domain knowledge and often depends on heuristics, given the complexity of clinical data and uncertainties inherent in the decisionmaking process.</s><s xml:id="_wQdqwxA">Further research is necessary to fully realize the potential of this promising approach.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4uAEMBQ">Action formulation challenges</head><p xml:id="_Xtz6za6"><s xml:id="_NTaqh5W">Healthcare interventions often span a spectrum from discrete choices (e.g., medication dosage, and treatment options) to continuous adjustments (e.g., Value-based off-policy method where the agent's goal is to find an optimal policy by maximizing the expected value of the total rewards through iterative interactions with the environment when the model is not known.</s></p><p xml:id="_mqruuvj"><s xml:id="_9HwKBb7">Q-learning selects the action that yields the highest expected value which results in selected actions having consistently overestimated values.</s></p><p xml:id="_nYFet2p"><s xml:id="_xwXFFXh">Deep Q-Network (DQN) <ref type="bibr" target="#b15">16</ref> DQN uses deep neural networks to represent the Q-function rather than a simple table of values.</s></p><p xml:id="_JdUnvPy"><s xml:id="_qfQNhxk">DQN suffers from overestimation of action values and sensitivity to hyperparameters leading to computationally intensive training processes.</s></p><p xml:id="_SgMJ94Q"><s xml:id="_XDmr33u">Double Deep Q-Network (DDQN) <ref type="bibr" target="#b85">85</ref> The DDQN is an improvement over the DQN algorithm as it reduces the overestimation of action values by decoupling the selection and evaluation of actions, using two value function estimates by employing two separate neural networks.</s></p><p xml:id="_mfN9MEp"><s xml:id="_KqFqqZj">DDQN also suffers from potential overestimation bias due to shared target and online networks and increased computational complexity from maintaining two separate networks.</s></p><p xml:id="_FAmQMK6"><s xml:id="_KGtr9F2">Batch Constrained Q-Learning (BCQ) <ref type="bibr" target="#b33">33</ref> It is as an off-policy algorithm that constrains exploration to improve policy learning and address overestimation bias in Q-learning.</s></p><p xml:id="_yzdtNqx"><s xml:id="_5zsNpAw">BCQ's learned policy is akin to robust imitation learning rather than true reinforcement learning when exploratory data is limited.</s></p><p xml:id="_G3xFYGK"><s xml:id="_uDKKUn6">Conservative Q-Learning 32 CQL penalizes actions not well-supported by the dataset to mitigate overestimation bias, promoting safer policy learning in reinforcement learning scenarios.</s></p><p xml:id="_U534FTK"><s xml:id="_vjsuswY">CQL suffers from potential underestimation of action values due to conservative updates, leading to suboptimal policies, and increased computational complexity from the additional penalty term, impacting training efficiency.</s></p><p xml:id="_WURU5ZN"><s xml:id="_jhbyhwK">Implicit Q-Learning (IQL) <ref type="bibr" target="#b34">34</ref> IQL addresses the challenges of traditional Q-learning methods by leveraging implicit estimation techniques for improved policy optimization.</s><s xml:id="_55xzbwp">It estimates the optimal action-value function indirectly, without explicitly modeling Q-values.</s></p><p xml:id="_MWHBy2d"><s xml:id="_QxmX7k8">IQL learned policy depends on the distributions of actions.</s><s xml:id="_4BVQKN5">The performance regresses when the data distribution is skewed toward sub-optimal actions in specific states.</s></p><p xml:id="_Pd8u2VA"><s xml:id="_C43uQS7">Conservative Policy Iteration (CPI) <ref type="bibr" target="#b37">37</ref> CPI balances exploration and exploitation by penalizing deviations from observed behavior, aiming to converge to a riskaverse policy with improved performance in uncertain environments.</s></p><p xml:id="_N9JzUHB"><s xml:id="_RVwtT9e">CPI suffers from conservative policies that may overly adhere to past behavior, potentially hindering exploration and innovation in dynamic environments.</s><s xml:id="_hkp44kn">Additionally, CPI's computational complexity can escalate with larger datasets, impacting scalability in real-world applications.</s></p><p xml:id="_ABKECgH"><s xml:id="_6DmFnBK">Behavior Regularized Off-Policy Q-Learning (BRAC) <ref type="bibr" target="#b35">35</ref> BRAC introduces behavior regularization, which encourages the agent to prioritize actions that are consistent with the behavior observed in the dataset, leading to improved learning stability and performance.</s></p><p xml:id="_mU5mwCD"><s xml:id="_9CbgP4v">The major limitations include difficulty in balancing exploration and exploitation, as well as challenges in effectively tuning the regularization parameter to achieve optimal performance across different environments and datasets.</s></p><p xml:id="_nk5yng7"><s xml:id="_p58H6M2">Dueling Network Architecture (DNA) <ref type="bibr" target="#b38">38</ref> The DNA architecture consists of two streams of fully connected layers that represent the value and advantage functions separately.</s><s xml:id="_DpBYYfh">It enables more efficient learning by allowing the agent to focus on valuable state information while independently estimating the advantage of different actions.</s></p><p xml:id="_pjQj7uH"><s xml:id="_SeeM8yq">DNA can suffer from increased complexity and increased computational requirements including a lack of interpretability and potential issues with generalizability.</s></p><p xml:id="_H8gCxbv"><s xml:id="_VJUk92v">Soft Behavior-regularized Actor-Critic (SBAC) <ref type="bibr" target="#b39">39</ref> SBAC incorporates behavior regularization by penalizing the policy for deviating from a behavior policy derived from past experience.</s><s xml:id="_VRfBxnd">This approach balances exploration and exploitation by leveraging previously collected data to improve learning efficiency.</s></p><p xml:id="_nZctBQA"><s xml:id="_eASqGNh">Major limitations of SBAC include potential inefficiency in rapidly changing environments due to reliance on past behavior and the challenge of appropriately tuning the behavior regularization parameter, which can complicate the optimization process.</s></p><p xml:id="_2j4B4HG"><s xml:id="_wQGUcu2">Adversarially Trained Actor-Critic (ATAC) <ref type="bibr" target="#b40">40</ref> In ATAC, 2 networks are trainedactor and critic.</s><s xml:id="_brHxuwd">Actor, which is responsible for selecting actions, is trained against a worst-case behavior policy estimated by the critic network.</s><s xml:id="_XDJsbUc">This adversarial training enhances the actor's ability to perform well even under the most challenging conditions, promoting robustness and stability in learning.</s></p><p xml:id="_J2a89A7"><s xml:id="_H5a6cQx">ATAC suffers from need for increased computational complexity due to the adversarial training process, and potential challenges in effectively balancing the adversarial training objectives, which may affect convergence and performance stability.</s></p><p xml:id="_GMR7uUh"><s xml:id="_CfbRyGM"><ref type="url" target="https://doi.org/10.1038/s41746-024-01316-0">https://doi.org/10.1038/s41746-024-01316-0</ref></s><s xml:id="_Mv2ZgtB">infusion rates).</s><s xml:id="_v6tN6rc">Balancing the granularity of this high-dimensional action representation to capture the complexity of clinical decisions while allowing for optimization of learning algorithms is challenging.</s><s xml:id="_Yn6P7rA">Healthcare actions are also subject to various constraints and safety considerations, such as drug interactions, physiological limits, and clinical guidelines.</s><s xml:id="_5uxQrE3">Ensuring that RL policies adhere to these constraints while still allowing for effective learning and adaptation presents another significant challenge.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_32BRjtF">Evaluation and implementation challenges</head><p xml:id="_8PwhWSF"><s xml:id="_8TDxfFT">Evaluation of RL algorithms remain a pivotal challenge in healthcare due to safety, ethics, and cost concerns.</s><s xml:id="_PMxRFnF">Applied RL practitioners often rely on simulated environments as testbenches before deployment.</s><s xml:id="_Qunedup">When simulators are unavailable, reliable Off-Policy Evaluation (OPE) methods become indispensable for selecting optimal policies across domains.</s><s xml:id="_T5Vh75V">Various innovative approaches, including fitted Q-estimation <ref type="bibr" target="#b41">41</ref> (FQE) and importance sampling-based <ref type="bibr" target="#b42">42</ref> methods, and marginalized sampling-based methods <ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b76">76</ref> have emerged.</s><s xml:id="_c3mRk7x">Despite their complexity, studies have demonstrated the suitability of relatively simple methods like FQE for policy <ref type="bibr" target="#b77">77,</ref><ref type="bibr" target="#b78">78</ref> selection.</s><s xml:id="_z2UweW4">However, challenges persist, and all methods can be affected by issues with state representations, deviations from observed behavior policies, and confounders.</s><s xml:id="_ydHF3MT">While OPE continues to evolve, real-world deployment is essential for comprehensive evaluation.</s><s xml:id="_jW28KUX">Despite the abundance of literature applying RL to healthcare, there's a scarcity of clinical trials prospectively analyzing RL models.</s><s xml:id="_na8vuS9">Notable examples include the REINFORCE trial of a RL-based text messaging program for type 2 diabetes treatment adherence <ref type="bibr" target="#b79">79</ref> and a proof-of-concept trial of insulin control for type 2 diabetes <ref type="bibr" target="#b80">80</ref> .</s><s xml:id="_jXAhzDm">However, conducting RL trials in high-risk domains remains challenging, as safety considerations take precedence.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kAXPXfZ">Future directions</head><p xml:id="_Wg2NvrU"><s xml:id="_uTPMXFf">The integration of large language models (LLMs) <ref type="bibr" target="#b81">81,</ref><ref type="bibr" target="#b82">82</ref> offers significant potential for incorporating reinforcement learning (RL) into automated clinical decision-support systems in healthcare.</s><s xml:id="_PKbRq6c">A review of PubMed publications (Fig. <ref type="figure" target="#fig_2">3</ref>) underscores the growing interest in RL within medical research.</s><s xml:id="_3bevqsS">Although RL-related studies in healthcare remain relatively few, the steady rise in publications indicates a burgeoning recognition of RL's capacity to revolutionize clinical practice.</s><s xml:id="_Bsxz5zT">RL's ability to optimize treatment decisions, manage resources, and improve patient outcomes through dynamic learning from clinical data is still underutilized but highly promising.</s><s xml:id="_68sHP2P">RL also enables continuous adaptation, facilitating progressively sophisticated applications that enhance care quality.</s><s xml:id="_eC2wZNT">Inverse Reinforcement Learning (IRL) <ref type="bibr" target="#b75">75</ref> , when combined with Federated Learning (FL), can offer even greater advantages, particularly in maintaining patient privacy <ref type="bibr" target="#b83">83</ref> while learning optimal action policies across hospitals.</s><s xml:id="_VXYE3Mm">This approach is especially impactful in critical care settings like ICUs, where RL can optimize sedation and ventilation management while protecting data security.</s><s xml:id="_9YPpqRu">The goal of RL in healthcare extends to optimizing interventions during clinical trials to enhance treatment efficacy and support comprehensive post-trial analyzes, contributing valuable insights for future interventions and policies.</s><s xml:id="_mqRKqkw">However, applying RL in areas like robotic-assisted surgery presents challenges, including the need for large, diverse datasets for imitation learning and the ability to adapt to unforeseen surgical anomalies.</s><s xml:id="_Ay4EeFh">Developing effective model functions that can accurately interpret sensory data <ref type="bibr" target="#b72">72</ref> and guide surgical actions remains a complex task.</s><s xml:id="_A8HjUjt">Additionally, integrating advanced computer vision models to interpret and learn from the surgical environment, automating repetitive surgical tasks reliably, and achieving precise instrument localization with pixel-wise segmentation are all ongoing challenges that are potential opportunities in the advancement of RL in surgical robotics.</s><s xml:id="_4GkjkGw">Resource variability and the need for domain-specific expertize also impact RL performance in healthcare, requiring models to evolve with varying practice patterns, patient demographics, and clinical standards.</s><s xml:id="_W5JyFmM">While more and more RL models are developed using real-time clinical data <ref type="bibr" target="#b84">84</ref> , iterative refinements are essential to capture additional complexities in RL models to improve treatment efficacy, yet balancing the simplicity of the model with comprehensive patient care remains a formidable challenge.</s><s xml:id="_VeuauJs">Furthermore, the need for domain expertize to define reward functions and conduct clinical evaluations adds another layer of complexity, requiring specialized knowledge and considerable time investment.</s><s xml:id="_rnM53Nx">Additionally, enabling RL models to provide real-time decision support in critical care settings, while ensuring that decisions align with clinical best practices and meet patient-specific needs, continues to be a significant challenge <ref type="bibr" target="#b84">84</ref> .</s><s xml:id="_mQSr9nt">Despite these challenges, RL continues to offer substantial opportunities, with future directions likely to include more personalized treatments, real-time adaptive interventions, and enhanced decisionmaking tools for clinicians, ultimately revolutionizing healthcare delivery.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KzD4tHg">Conclusion</head><p xml:id="_bHXpGXH"><s xml:id="_7QwaPAn">In conclusion, the integration of Reinforcement Learning (RL) into healthcare holds immense promise for transforming clinical decision-making and improving patient care through enhanced precision and personalization.</s><s xml:id="_ncnyPJE">Addressing the challenges in the development and deployment of RL algorithms is essential to fully realize its potential for more efficient and effective patient care.</s><s xml:id="_tv2U44s">RL, with its sequential decision-making capabilities, is uniquely positioned to shift artificial intelligence in healthcare from predictive models to actionable, real-time tools, empowering clinicians to make data-driven, personalized decisions at the bedside.</s><s xml:id="_YJfSPAv">Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material.</s><s xml:id="_QXCF9dj">You do not have permission under this licence to share adapted material derived from this article or parts of it.</s><s xml:id="_GDDCEzg">The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.</s><s xml:id="_WXU6KTk">If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</s><s xml:id="_386ZSYc">To view a copy of this licence, visit <ref type="url" target="http://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by- nc-nd/4.0/</ref>.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 |</head><label>1</label><figDesc><div><p xml:id="_SQxW3Tv"><s xml:id="_FdsD2jX">Fig. 1 | Types of Reinforcement Learning.</s><s xml:id="_r8NMYbx">An overview of the sub-categories of reinforcement learning is presented in Fig. 1.</s><s xml:id="_FYWTJZt">Each major sub-category has an example of a published RL model.</s><s xml:id="_eMExyTS">Source: Original.</s><s xml:id="_TyjFJE5">License: Created with BioRender.com.</s><s xml:id="_FwgTkJH">Agreement number: SF26VQJRKW for NPJDM (Issued on May 30th, 2024).</s><s xml:id="_Cp4tc52">This document is to confirm that Pushkala Jayaraman has been granted a license to use the BioRender content, including icons, templates and other original artwork, appearing in the attached completed graphic pursuant to BioRender's Academic License Terms.</s><s xml:id="_g4JfGzm">This license permits BioRender content to be sublicensed</s></p></div></figDesc><graphic coords="3,60.49,49.72,479.96,226.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 |</head><label>2</label><figDesc><div><p xml:id="_ZpRh8m6"><s xml:id="_3Bm6g4S">Fig.2| Challenges of reinforcement learning.</s><s xml:id="_gKhBqsD">A visual description of the ongoing challenges in RL is provided in Fig.2.</s><s xml:id="_Dbt8TWF">These could be categorized as challenges in formulation of state-space, action-space and reward, challenges in evaluation and challenges with deployment into a production environment.</s><s xml:id="_wCyeupz">Source: Original.</s><s xml:id="_UaHwgFT">License: Created with BioRender.com.</s><s xml:id="_vtArvAA">Agreement number: SA26VQJVTH for NPJDM (Issued on May 30th, 2024).</s><s xml:id="_Bzv8dHJ">This document is to confirm that Pushkala Jayaraman has been granted a license to use the BioRender content, including icons, templates and other original artwork, appearing in the attached completed graphic pursuant to BioRender's Academic License Terms.</s><s xml:id="_nWkzhpe">This license permits BioRender content to be sublicensed for use in journal publications.</s><s xml:id="_ZXHXkAF">All rights and ownership of BioRender content are reserved by BioRender.</s><s xml:id="_SxJwjwd">All completed graphics must be accompanied by the following citation: "Created with BioRender.com".</s><s xml:id="_gpjMu6z">BioRender content included in the completed graphic is not licensed for any commercial uses beyond publication in a journal.</s><s xml:id="_ehsZUZ4">For any commercial use of this figure, users may, if allowed, recreate it in BioRender under an Industry BioRender Plan.</s><s xml:id="_KqmrS89">For any questions regarding this document, or other questions about publishing with BioRender refer to our BioRender Publication Guide or contact BioRender Support at support@biorender.com.</s></p></div></figDesc><graphic coords="6,60.49,49.72,479.96,327.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 |</head><label>3</label><figDesc><div><p xml:id="_TKmufeD"><s xml:id="_Q25unr5">Fig. 3 | Publication trends (PubMed) of ML, DL and RL in healthcare and clinical studies.</s><s xml:id="_p5m6Qsg">A review of the growth of PubMed-indexed publications underscores the growing interest in RL within medical research.</s><s xml:id="_6MWtMpP">Although RL-related studies remain relatively few, the steady rise in publications indicates a burgeoning recognition of RL's capacity to revolutionize clinical practice.</s><s xml:id="_3XrF4eZ">Source: Original.</s><s xml:id="_Kq8beey">Created from PubMed statistics.</s><s xml:id="_DV9kxfF">License: Publication Trends (PubMed) of ML, DL and RL in Healthcare and Clinical Studies © 2024 by Pushkala Jayaraman &amp; Jacob Desman is licensed under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International.</s><s xml:id="_d4qN6uh">To view a copy of this license, visit https://creativecommons.org/ licenses/by-nc-nd/4.0/.</s></p></div></figDesc><graphic coords="7,60.49,49.72,479.96,280.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 |</head><label>1</label><figDesc><div><p xml:id="_Ackfw6r"><s xml:id="_adJ9VbF">Key components of reinforcement learning</s></p></div></figDesc><table><row><cell>Terms</cell><cell>Definitions</cell><cell>Examples in the context of "learning to ride a bike"</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 |</head><label>2</label><figDesc><div><p xml:id="_QNUVvEy"><s xml:id="_VTUXFpF">Components of Markov Decision Processes</s></p></div></figDesc><table><row><cell>Component name</cell><cell>Description</cell></row><row><cell>State (S)</cell><cell>The set of States (s∈S)</cell></row><row><cell>Action (A)</cell><cell>The set of Actions (a∈A)</cell></row><row><cell>Probability scores (P)</cell><cell>Denotes transition probabilities, specifying the likelihood of moving from one state to another when the agent takes a particular action</cell></row><row><cell>Rewards (R)</cell><cell>Denotes the positive or negative rewards received upon transitioning from state s to s' (s, s'∈S) by taking an action a</cell></row><row><cell>discount factor (γ)</cell><cell>Determines the importance of future rewards compared to immediate ones in the calculation of cumulative rewards.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 |</head><label>3</label><figDesc><div><p xml:id="_TeM54hd"><s xml:id="_EatAHr9">Examples of Offline Reinforcement Learning Algorithms</s></p></div></figDesc><table><row><cell>Offline RL algorithm</cell><cell>Conceptual framework</cell><cell>Key limitations</cell></row><row><cell>Behavioral Cloning (BC) 31</cell><cell>BC learns a policy by imitating expert behavior from a fixed dataset</cell><cell>BC can suffer from compounding errors when the learned policy</cell></row><row><cell></cell><cell>of expert demonstrations</cell><cell>diverges from the expert behavior, leading to poor generalization</cell></row><row><cell></cell><cell></cell><cell>and performance in new situations</cell></row><row><cell>Q-Learning 12</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p xml:id="_9UYCSgu"><s xml:id="_KDWWJ2M">The Charles Bronfman Institute for Personalized Medicine (CBIPM), Icahn School of Medicine at Mount Sinai, New York, NY, USA.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p xml:id="_SsVcjsR"><s xml:id="_v4sQsPz">Samuel Bronfman Department of Medicine Division of Data Driven and Digital Medicine (D3M), Icahn School of Medicine at Mount Sinai, New York, NY, USA.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p xml:id="_WTx2VXx"><s xml:id="_3gwyNP4">Division of Nephrology, Department of Medicine, Icahn School of Medicine at Mount Sinai, New York, NY, USA.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p xml:id="_PZD9Zpg"><s xml:id="_D7HGYfz">Institute for Critical Care Medicine, Icahn School of Medicine at Mount Sinai, New York, NY, USA.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p xml:id="_mZTSUFh"><s xml:id="_vc4HEbX">These authors jointly supervised this work: Girish N. Nadkarni, Ankit Sakhuja.e-mail:</s><s xml:id="_MxNzDhr">girish.nadkarni@mountsinai.org; ankit.sakhuja@mssm.edu</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p xml:id="_SYDtgB3"><s xml:id="_cqYB9Cr">npj Digital Medicine | (2024) 7:337</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p xml:id="_cxFHU6t"><s xml:id="_3da5hyu">© The Author(s) 2024 https://doi.org/10.1038/s41746-024-01316-0</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_yceHtz8">Acknowledgements</head><p xml:id="_ra8cA4H"><s xml:id="_sgW7B4R">This work was supported by <rs type="funder">NIH/NIDDK</rs> grant <rs type="grantNumber">K08DK131286</rs> (AS), <rs type="funder">Clinical and Translational Science Awards (CTSA)</rs> grant <rs type="grantNumber">UL1TR004419</rs> from the <rs type="funder">National Center for Advancing Translational Sciences (GNN)</rs> and the computational resources and staff expertize provided by <rs type="institution">Scientific Computing</rs> at the <rs type="institution">Icahn School of Medicine at Mount Sinai</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eCcsUqx">
					<idno type="grant-number">K08DK131286</idno>
				</org>
				<org type="funding" xml:id="_Ewm2ZKY">
					<idno type="grant-number">UL1TR004419</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZeQzRMW">Author contributions</head><p xml:id="_K2bsh3h"><s xml:id="_S5YMuuZ">Conceptualization: A.S, G.N.N, P.J. Methodology: P.J, J.D, M.S, A.S. Investigation: P.J, J.D, M.S, A.S. Visualization: P.J, J.D. Funding acquisition: A.S, G.N.N.</s><s xml:id="_yQvh7Je">Supervision: A.S, G.N.N.</s><s xml:id="_j6Vavcf">Writing: P.J, J.D, M.S, A.S. Revisions: P.J, J.D, M.S, A.S, G.N.N.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_T5E4ZHG">Competing interests</head><p xml:id="_bUXX9yV"><s xml:id="_YWRJT6B">GNN reports grants, personal fees, and non-financial support from Renalytix.</s><s xml:id="_qzKFCaK">GNN reports non-financial support from Pensieve Health, personal fees from AstraZeneca, personal fees from BioVie, personal fees from GLG Consulting, and personal fees from Siemens Healthineers from outside the submitted work.</s><s xml:id="_fdgpgQR">GNN is also serves as the Associate Editor for NPJDM.</s><s xml:id="_FrxGV3F">None of the other authors have any other competing interests to declare.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_nauRnFs">Additional information</head><p xml:id="_mZH7PGb"><s xml:id="_yWJcvzZ">Supplementary information The online version contains supplementary material available at <ref type="url" target="https://doi.org/10.1038/s41746-024-01316-0">https://doi.org/10.1038/s41746-024-01316-0</ref>.</s></p><p xml:id="_bzGWArr"><s xml:id="_BjNJFdN">Correspondence and requests for materials should be addressed to Girish N. Nadkarni or Ankit Sakhuja.</s></p><p xml:id="_r6SeNKz"><s xml:id="_EqbcFAD">Reprints and permissions information is available at <ref type="url" target="http://www.nature.com/reprints">http://www.nature.com/reprints</ref></s><s xml:id="_YmfMyyz">Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_dDrMB98">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA, US</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed</note>
	<note type="raw_reference">Sutton, R. S. &amp; Barto, A. G. Reinforcement learning: An introduction, 2nd ed, (The MIT Press, Cambridge, MA, US, 2018).</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_UmjegSQ">A Markovian decision process</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SsEcvGb">J. Math. Mech</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="679" to="684" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bellman, R. A Markovian decision process. J. Math. Mech. 6, 679-684 (1957).</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_h7mNg8w">Algorithms for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-29946-9_4</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Szepesvári, C. Algorithms for Reinforcement Learning, (Springer International Publishing, 2022).</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_z8Q3K5D">Model-based Reinforcement Learning: A Survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Joost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Catholijn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<pubPlace>now</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Thomas, M. M., Joost, B., Aske, P. &amp; Catholijn, M. J. Model-based Reinforcement Learning: A Survey, (now, 2023).</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_r8cehJD">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xu7UFNn">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Silver, D. et al. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362, 1140-1144 (2018).</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_fjXD4Av">Model-Based or Model-Free, a Review of Approaches in Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QdHD5SV">2020 International Conference on Computing and Data Science (CDS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="219" to="221" />
		</imprint>
	</monogr>
	<note type="raw_reference">Huang, Q. Model-Based or Model-Free, a Review of Approaches in Reinforcement Learning. In 2020 International Conference on Computing and Data Science (CDS) 219-221 (2020).</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_X9PqHHy">Modern value based reinforcement learning: a chronological review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mckenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mcdonnell</surname></persName>
		</author>
		<idno type="DOI">10.1109/access.2022.3228647</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_p6hGCCW">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="134704" to="134725" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McKenzie, M. C. &amp; McDonnell, M. D. Modern value based reinforcement learning: a chronological review. IEEE Access 10, 134704-134725 (2022).</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mackworth</surname></persName>
		</author>
		<title level="m" xml:id="_33srXtP">Artificial Intelligence: Foundations of Computational Agents</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Poole, D. L. &amp; Mackworth, A. K. Artificial Intelligence: Foundations of Computational Agents, (Cambridge University Press, Cambridge, 2017).</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_fBnK37H">On-line Q-learning using connectionist systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rummery, G. A. &amp; Niranjan, M. On-line Q-learning using connectionist systems. (1994).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_d7a2uNN">Reinforcement learning with replacing eligibility traces</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CUGQCCA">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="123" to="158" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Singh, S. P. &amp; Sutton, R. S. Reinforcement learning with replacing eligibility traces. Mach. Learn. 22, 123-158 (1996).</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_Yz5N8kP">A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Prudencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maximo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Colombini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SBgPVxh">IEEE Trans Neural Netw Learn Syst PP</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Prudencio, R. F., Maximo, M. &amp; Colombini, E. L. A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. IEEE Trans Neural Netw Learn Syst PP(2023).</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_HhpJcYp">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_J4JHtqr">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Watkins, C. J. C. H. &amp; Dayan, P. Q-learning. Mach. Learn. 8, 279-292 (1992).</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_YXWWhab">A Review of Off-Policy Evaluation in Reinforcement Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Uehara, M., Shi, C. &amp; Kallus, N. A Review of Off-Policy Evaluation in Reinforcement Learning. (2022).</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_FqjMzSn">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00992696</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RxjWkAU">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn. 8, 229-256 (1992).</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_EpYM39C">Actor-critic algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2BB93kd">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Konda, V. &amp; Tsitsiklis, J. Actor-critic algorithms. Advances in neural information processing systems 12 (1999).</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_2j63p8z">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_msTbsyB">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529-533 (2015).</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_9cgGdhY">Giraffe: Using Deep Reinforcement Learning to Play Chess</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lai, M. Giraffe: Using Deep Reinforcement Learning to Play Chess. (2015).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_q56Xrvy">Deep reinforcement learning for autonomous driving: a survey</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Kiran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rp6TEcb">IEEE Trans. Intell. Transportation Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="4909" to="4926" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kiran, B. R. et al. Deep reinforcement learning for autonomous driving: a survey. IEEE Trans. Intell. Transportation Syst. 23, 4909-4926 (2022).</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_GqteuDv">Reinforcement learning in robotics: a survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FeTeR32">Int. J. Robot. Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1238" to="1274" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kober, J., Bagnell, J. A. &amp; Peters, J. Reinforcement learning in robotics: a survey. Int. J. Robot. Res. 32, 1238-1274 (2013).</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_xKcr5xN">A brief overview of ChatGPT: The history, status quo and potential future development</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/jas.2023.123618</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VB5tvyJ">IEEE/CAA J. Autom. Sin</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1122" to="1136" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, T. et al. A brief overview of ChatGPT: The history, status quo and potential future development. IEEE/CAA J. Autom. Sin. 10, 1122-1136 (2023).</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main" xml:id="_maCBbFj">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772758</idno>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, L., Chu, W., Langford, J. &amp; Schapire, R. E. A contextual-bandit approach to personalized news article recommendation. (ACM).</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main" xml:id="_54pGcne">Horizon: Facebook&apos;s open source applied reinforcement learning platform</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gauci</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SylQKinLi4" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gauci, J. et al. Horizon: Facebook&apos;s open source applied reinforcement learning platform. https://openreview.net/forum?id= SylQKinLi4 (2019).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_jm2qe3k">Review on the research and practice of deep learning and reinforcement learning in smart grids</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.17775/cseejpes.2018.00520</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hzx5CBa">CSEE J. Power Energy Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="362" to="370" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, D., Han, X. &amp; Deng, C. Review on the research and practice of deep learning and reinforcement learning in smart grids. CSEE J. Power Energy Syst. 4, 362-370 (2018).</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main" xml:id="_acZK5xM">Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Trella</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.17003</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Trella, A. L., et al. Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials. arXiv preprint arXiv:2402.17003 (2024).</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_egBnV2j">A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Prudencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R O A</forename><surname>Maximo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Colombini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qrnbmGQ">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Prudencio, R. F., Maximo, M. R. O. A. &amp; Colombini, E. L. A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems. IEEE Transactions on Neural Networks and Learning Systems, 1-0 (2024).</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main" xml:id="_YYNqWYp">Noise and the reality gap: the use of simulation in evolutionary robotics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jakobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P &amp;</forename><surname>Husbands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Harvey</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-59496-5_337</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="704" to="720" />
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Jakobi, N, Husbands, P &amp; Harvey, I. Noise and the reality gap: the use of simulation in evolutionary robotics. Springer, Berlin, p 704-720 (1995).</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main" xml:id="_UnUazMR">Offline reinforcement learning: Tutorial, review, and perspectives on open problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00129-005-1701-5</idno>
		<idno>. abs/ 2005.01643</idno>
		<ptr target="https://arxiv.org/abs/2005.01643" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Levine, S., Kumar, A., Tucker, G. &amp; Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. abs/ 2005.01643. https://arxiv.org/abs/2005.01643 (2020).</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_XpGVeMC">Wasserstein distributionally robust optimization: Theory and applications in machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Esfahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shafieezadeh-Abadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_aJChFfP">Operations research &amp; management science in the age of analytics</title>
		<imprint>
			<publisher>Informs</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="130" to="166" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kuhn, D., Esfahani, P. M., Nguyen, V. A. &amp; Shafieezadeh-Abadeh, S. Wasserstein distributionally robust optimization: Theory and applications in machine learning. In Operations research &amp; management science in the age of analytics 130-166 (Informs, 2019).</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_JzKh2Df">A Benchmark of in-the-Wild Distribution Shifts</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Ctu4t6J">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page">139</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Koh, P. W. et al. WILDS: A Benchmark of in-the-Wild Distribution Shifts. In Proceedings of the 38th International Conference on Machine Learning, 139 (eds.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Marina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<title level="m" xml:id="_bdh3QAA">PMLR, Proceedings of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
	<note type="raw_reference">Marina, M. &amp; Tong, Z.) 5637-5664 (PMLR, Proceedings of Machine Learning Research, 2021).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main" xml:id="_NnyEXyE">Datasets for deep data-driven reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=px0-N3_KjA" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fu, J., Kumar, A., Nachum, O., Tucker, G. &amp; Levine, S. Datasets for deep data-driven reinforcement learning. https://openreview.net/ forum?id=px0-N3_KjA (2021).</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_9UBj9bn">A Framework for Behavioural Cloning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sammut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cGgerRB">Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bain, M. &amp; Sammut, C. A Framework for Behavioural Cloning. in Machine Intelligence 15 (1995).</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_m4B27Sc">Conservative q-learning for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_H3pZtju">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1179" to="1191" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kumar, A., Zhou, A., Tucker, G. &amp; Levine, S. Conservative q-learning for offline reinforcement learning. Adv. Neural Inf. Process. Syst. 33, 1179-1191 (2020).</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_nabfVaE">Off-policy deep reinforcement learning without exploration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jyHEHJz">International conference on machine learning 2052-2062</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fujimoto, S., Meger, D. &amp; Precup, D. Off-policy deep reinforcement learning without exploration. In International conference on machine learning 2052-2062 (PMLR, 2019).</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_GdEpaR7">Offline reinforcement learning with implicit q-learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=68n2s9ZJWF8" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_6K5TSVD">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kostrikov, I., Nair, A. &amp; Levine, S. Offline reinforcement learning with implicit q-learning. In International Conference on Learning Representations. https://openreview.net/forum?id=68n2s9ZJWF8 (2022).</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJg9hTNKPH" />
		<title level="m" xml:id="_buPcSkK">Behavior regularized offline reinforcement learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, Y., Tucker, G. &amp; Nachum, O. Behavior regularized offline reinforcement learning. https://openreview.net/forum?id= BJg9hTNKPH, https://openreview.net/forum?id=BJg9hTNKPH (2020).</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_HfSN5HX">Stabilizing off-policy q-learning via bootstrapping error reduction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sXKQ8NZ">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kumar, A., Fu, J., Soh, M., Tucker, G. &amp; Levine, S. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in neural information processing systems 32 (2019).</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_4CZzGtY">Deep conservative policy iteration</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vieillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i04.6070</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TujRj7g">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6070" to="6077" />
		</imprint>
	</monogr>
	<note type="raw_reference">Vieillard, N., Pietquin, O. &amp; Geist, M. Deep conservative policy iteration. in Proceedings of the AAAI Conference on Artificial Intelligence, 34 6070-6077 (2020).</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_QSkP734">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_edgGxF7">International conference on machine learning 1995-2003</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, Z., et al. Dueling network architectures for deep reinforcement learning. in International conference on machine learning 1995-2003 (PMLR, 2016).</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main" xml:id="_4smK8Yv">Offline reinforcement learning with soft behavior regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<idno>. abs/2110.07395</idno>
		<ptr target="https://arxiv.org/abs/2110.07395" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu, H., Zhan, X., Li, J. &amp; Yin, H. Offline reinforcement learning with soft behavior regularization. abs/2110.07395. https://arxiv.org/abs/2110. 07395 (2021).</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_H42rw9k">Adversarially trained actor critic for offline reinforcement learning</title>
		<author>
			<persName><forename type="first">C.-A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<idno type="DOI">10.2514/6.2022-2078.vid</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3rfPUU5">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3852" to="3878" />
		</imprint>
	</monogr>
	<note type="raw_reference">Cheng, C.-A., Xie, T., Jiang, N. &amp; Agarwal, A. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning 3852-3878 (PMLR, 2022).</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_YFMG7BN">Batch policy learning under constraints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voloshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_dZrKwjy">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3703" to="3712" />
		</imprint>
	</monogr>
	<note type="raw_reference">Le, H., Voloshin, C. &amp; Yue, Y. Batch policy learning under constraints. in International Conference on Machine Learning 3703-3712 (PMLR, 2019).</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_EtmHmfg">Doubly robust off-policy value evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TAX7WbD">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="652" to="661" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jiang, N. &amp; Li, L. Doubly robust off-policy value evaluation for reinforcement learning. in International conference on machine learning 652-661 (PMLR, 2016).</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_AdmzfY2">Behavior-agnostic estimation of discounted stationary distribution corrections</title>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Dualdice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dJgY3gR">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nachum, O., Chow, Y., Dai, B. &amp; Li, L. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Advances in neural information processing systems 32 (2019).</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_spKjF3p">Generalized offline estimation of stationary values</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><surname>Gendice</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkxlcnVFwB" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_5Gm3zb8">international Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, R., Dai, B., Li, L. &amp; Schuurmans, D. Gendice: Generalized offline estimation of stationary values. In international Conference on Learning Representations. https://openreview.net/forum?id= HkxlcnVFwB (2020).</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_CPD8RqE">Reinforcement learning in healthcare: a survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477600</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SActm4f">ACM Comput. Surv. (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu, C., Liu, J., Nemati, S. &amp; Yin, G. Reinforcement learning in healthcare: a survey. ACM Comput. Surv. (CSUR) 55, 1-36 (2021).</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_xxbbtZW">An Adaptive Neural Network Filter for Improved Patient State Estimation in Closed-Loop Anesthesia Control</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Borera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Doufas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
		<idno type="DOI">10.1109/ictai.2011.15</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ruWAY9d">IEEE 23rd International Conference on Tools with Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
	<note type="raw_reference">Borera, E. C., Moore, B. L., Doufas, A. G. &amp; Pyeatt, L. D. An Adaptive Neural Network Filter for Improved Patient State Estimation in Closed- Loop Anesthesia Control. in 2011 IEEE 23rd International Conference on Tools with Artificial Intelligence 41-46 (2011).</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_yFmfS3s">Reinforcement learning design for cancer clinical trials</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DBWSPwZ">Stat. Med</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3294" to="3315" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhao, Y., Kosorok, M. R. &amp; Zeng, D. Reinforcement learning design for cancer clinical trials. Stat. Med 28, 3294-3315 (2009).</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_vA86cUU">Drug scheduling of cancer chemotherapy based on natural actor-critic approach</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NSxn76P">Biosystems</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ahn, I. &amp; Park, J. Drug scheduling of cancer chemotherapy based on natural actor-critic approach. Biosystems 106, 121-129 (2011).</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_nrkVCeP">Reinforcement learning for optimal scheduling of glioblastoma treatment with temozolomide</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ebrahimi Zade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shahabi Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soltani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TTJS8wY">Computer Methods Prog. Biomedicine</title>
		<imprint>
			<biblScope unit="volume">193</biblScope>
			<biblScope unit="page">105443</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ebrahimi Zade, A., Shahabi Haghighi, S. &amp; Soltani, M. Reinforcement learning for optimal scheduling of glioblastoma treatment with temozolomide. Computer Methods Prog. Biomedicine 193, 105443 (2020).</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_vvJjh6e">Reinforcement learning strategies in cancer chemotherapy treatments: A review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shiranthika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sumathipala</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cmpb.2022.107280</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eCnaQNT">Comput Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page">107280</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang, C. Y., Shiranthika, C., Wang, C. Y., Chen, K. W. &amp; Sumathipala, S. Reinforcement learning strategies in cancer chemotherapy treatments: A review. Comput Methods Prog. Biomed. 229, 107280 (2023).</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_22bNbWN">The university of Virginia/Padova type 1 diabetes simulator matches the glucose traces of a clinical trial</title>
		<author>
			<persName><forename type="first">R</forename><surname>Visentin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dalla Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kovatchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cobelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EtTaykp">Diabetes Technol. Ther</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="428" to="434" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Visentin, R., Dalla Man, C., Kovatchev, B. &amp; Cobelli, C. The university of Virginia/Padova type 1 diabetes simulator matches the glucose traces of a clinical trial. Diabetes Technol. Ther. 16, 428-434 (2014).</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_Eg5scSu">Computational medication regimen for Parkinson&apos;s disease using reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suescun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Schiess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cckAW7b">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">9313</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kim, Y., Suescun, J., Schiess, M. C. &amp; Jiang, X. Computational medication regimen for Parkinson&apos;s disease using reinforcement learning. Sci. Rep. 11, 9313 (2021).</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_k4Rq3vj">Optimizing risk-based breast cancer screening policies with reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-024-01316-0</idno>
		<ptr target="https://doi.org/10.1038/s41746-024-01316-0" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_8FBXXJg">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="136" to="143" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yala, A. et al. Optimizing risk-based breast cancer screening policies with reinforcement learning. Nat. Med. 28, 136-143 (2022). https://doi.org/10.1038/s41746-024-01316-0</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_BDG8YBv">A reinforcement learning model for AI-based decision support in skin cancer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hUJWggh">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1941" to="1946" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Barata, C. et al. A reinforcement learning model for AI-based decision support in skin cancer. Nat. Med. 29, 1941-1946 (2023).</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_her6nZP">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zXUxDNc">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johnson, A. E. et al. MIMIC-III, a freely accessible critical care database. Sci. Data 3, 160035 (2016).</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_Sa7aDAr">MIMIC-IV, a freely accessible electronic health record dataset</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2WeMawu">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Johnson, A. E. W. et al. MIMIC-IV, a freely accessible electronic health record dataset. Sci. Data 10, 1 (2023).</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_deBJa3E">The eICU Collaborative Research Database, a freely available multi-center database for critical care research</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2018.178</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KNj9BzN">Sci. Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180178</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pollard, T. J. et al. The eICU Collaborative Research Database, a freely available multi-center database for critical care research. Sci. Data 5, 180178 (2018).</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_hudJkpJ">Optimal medication dosing from suboptimal clinical examples: a deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
		<idno type="DOI">10.1109/embc.2016.7591355</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jadc8vN">Annu Int Conf. IEEE Eng. Med Biol. Soc</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="2978" to="2981" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nemati, S., Ghassemi, M. M. &amp; Clifford, G. D. Optimal medication dosing from suboptimal clinical examples: a deep reinforcement learning approach. Annu Int Conf. IEEE Eng. Med Biol. Soc. 2016, 2978-2981 (2016).</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_Hqj6xrX">A deep deterministic policy gradient approach to medication dosing and surveillance in the ICU</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ew98TFY">Annu Int Conf. IEEE Eng. Med Biol. Soc</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="4927" to="4931" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lin, R., Stanley, M. D., Ghassemi, M. M. &amp; Nemati, S. A deep deterministic policy gradient approach to medication dosing and surveillance in the ICU. Annu Int Conf. IEEE Eng. Med Biol. Soc. 2018, 4927-4931 (2018).</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main" xml:id="_2dX9Ged">Deep reinforcement learning for sepsis treatment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00350-017-4558-4</idno>
		<idno>. abs/ 1711.09602</idno>
		<ptr target="http://arxiv.org/abs/1711.09602" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Raghu, A. et al. Deep reinforcement learning for sepsis treatment. abs/ 1711.09602. http://arxiv.org/abs/1711.09602 (2017).</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_jwdTwh4">The treatment of sepsis: an episodic memory-assisted deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tAVesSr">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="11034" to="11044" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liang, D., Deng, H. &amp; Liu, Y. The treatment of sepsis: an episodic memory-assisted deep reinforcement learning approach. Appl. Intell. 53, 11034-11044 (2023).</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_Uzqup86">The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care</title>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Badawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Faisal</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-018-0213-5</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dPxUNnJ">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1716" to="1720" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Komorowski, M., Celi, L. A., Badawi, O., Gordon, A. C. &amp; Faisal, A. A. The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nat. Med. 24, 1716-1720 (2018).</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_WyVhKJH">A value-based deep reinforcement learning model with human expertise in optimal treatment of sepsis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_u8JNTHu">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, X., Li, R., He, Z., Yu, T. &amp; Cheng, C. A value-based deep reinforcement learning model with human expertise in optimal treatment of sepsis. NPJ Digit Med. 6, 15 (2023).</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_MWQ5B2k">Towards safe mechanical ventilation treatment using deep offline reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kondrup</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v37i13.26862</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_k2cD8jh">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="15696" to="15702" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kondrup, F. et al. Towards safe mechanical ventilation treatment using deep offline reinforcement learning. Proc. AAAI Conf. Artif. Intell. 37, 15696-15702 (2024).</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main" xml:id="_96PXR2S">A reinforcement learning approach to weaning of mechanical ventilation in intensive care units</title>
		<author>
			<persName><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Draugelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Engelhardt</surname></persName>
		</author>
		<idno>. abs/1704.06300</idno>
		<ptr target="http://arxiv.org/abs/1704.06300" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Prasad, N., Cheng, L.F., Chivers, C., Draugelis, M. &amp; Engelhardt, B. E. A reinforcement learning approach to weaning of mechanical ventilation in intensive care units. abs/1704.06300. http://arxiv.org/ abs/1704.06300 (2017).</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_C5Bzmwd">Development and validation of a reinforcement learning algorithm to dynamically optimize mechanical ventilation in critical care</title>
		<author>
			<persName><forename type="first">A</forename><surname>Peine</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-021-00388-6</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cX8GHew">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peine, A. et al. Development and validation of a reinforcement learning algorithm to dynamically optimize mechanical ventilation in critical care. NPJ Digit Med. 4, 32 (2021).</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_FjRJmrt">Double Q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cpuKrYc">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hasselt, H. Double Q-learning. Advances in neural information processing systems 23 (2010).</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_C8bapPt">Guideline-informed reinforcement learning for mechanical ventilation in critical care</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hengst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JbU3rSH">Artif. Intell. Med</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page">102742</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">den Hengst, F. et al. Guideline-informed reinforcement learning for mechanical ventilation in critical care. Artif. Intell. Med. 147, 102742 (2024).</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_QtzGJFx">Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saghafian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6aBFr22">Management Science</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Saghafian, S. Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach. Management Science (2023).</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_mj69bpV">Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Watkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=xtKWwB6lzT" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_YmKtUhw">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luo, Z., Pan, Y., Watkinson, P. &amp; Zhu, T. Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=xtKWwB6lzT (2024).</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main" xml:id="_xv6P7an">An in silico Environment and Benchmark Platform for Reinforcement Learning Based Dynamic Treatment Regime</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2405.18610</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Luo, Z. et al. DTR-Bench: An in silico Environment and Benchmark Platform for Reinforcement Learning Based Dynamic Treatment Regime. arXiv preprint arXiv:2405.18610 (2024).</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_ZHKBF9S">A guide to deep learning in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MYbtRCH">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Esteva, A. et al. A guide to deep learning in healthcare. Nat. Med 25, 24-29 (2019).</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_aQjJFnD">Artificial intelligence, bias and clinical safety</title>
		<author>
			<persName><forename type="first">R</forename><surname>Challen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mUUkw7D">BMJ Qual. Saf</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="231" to="237" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Challen, R. et al. Artificial intelligence, bias and clinical safety. BMJ Qual. Saf. 28, 231-237 (2019).</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main" xml:id="_PGDGY7U">Reinforcement learning for clinical decision support in critical care: comprehensive review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cs5aN7E">J. Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">18477</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, S. et al. Reinforcement learning for clinical decision support in critical care: comprehensive review. J. Med Internet Res 22, e18477 (2020).</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_M5bW5TJ">Inverse reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Zypr8An">BMC Med Inf. Decis. Mak</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yu, C., Liu, J. &amp; Zhao, H. Inverse reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units. BMC Med Inf. Decis. Mak. 19, 57 (2019).</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main" xml:id="_ZbhwvBV">Algaedice: Policy gradient from arbitrary experience</title>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<idno>. abs/1912.02074</idno>
		<ptr target="http://arxiv.org/abs/1912.0207" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nachum, O. et al. Algaedice: Policy gradient from arbitrary experience. abs/1912.02074. http://arxiv.org/abs/1912.0207 (2019).</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_tZTymxR">Off-policy evaluation via the regularized lagrangian</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hxV5M9q">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6551" to="6561" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang, M., Nachum, O., Dai, B., Li, L. &amp; Schuurmans, D. Off-policy evaluation via the regularized lagrangian. Adv. Neural Inf. Process. Syst. 33, 6551-6561 (2020).</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main" xml:id="_SjtHNJJ">Empirical study of off-policy policy evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Voloshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<idno>. abs/1911.06854</idno>
		<ptr target="http://arxiv.org/abs/1911.06854" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Voloshin, C., Le, H. M., Jiang, N. &amp; Yue, Y. Empirical study of off-policy policy evaluation for reinforcement learning. abs/1911.06854. http:// arxiv.org/abs/1911.06854 (2019).</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_tmsuSKm">The impact of using reinforcement learning to personalize communication on medication adherence: findings from the REINFORCE trial</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lauffenburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JugXSxW">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lauffenburger, J. C. et al. The impact of using reinforcement learning to personalize communication on medication adherence: findings from the REINFORCE trial. NPJ Digit Med. 7, 39 (2024).</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main" xml:id="_vVPS3Qf">Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MguYQsX">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2633" to="2642" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, G. et al. Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial. Nat. Med. 29, 2633-2642 (2023).</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main" xml:id="_6Cg52vZ">Embracing large language models for medical applications: opportunities and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karabacak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Margetis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nxW68Ak">Cureus</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">39305</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Karabacak, M. &amp; Margetis, K. Embracing large language models for medical applications: opportunities and challenges. Cureus 15, e39305 (2023).</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_kDq2yY9">The future landscape of large language models in medicine</title>
		<author>
			<persName><forename type="first">J</forename><surname>Clusmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EBFksg9">Commun. Med (Lond.)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">141</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Clusmann, J. et al. The future landscape of large language models in medicine. Commun. Med (Lond.) 3, 141 (2023).</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_RwU2mNN">Federated inverse reinforcement learning for smart icus with differential privacy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Gvs8DRY">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="19117" to="19124" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gong, W. et al. Federated inverse reinforcement learning for smart icus with differential privacy. IEEE Internet Things J. 10, 19117-19124 (2023).</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main" xml:id="_9jsvDdQ">Reinforcement learning for intensive care medicine: actionable clinical insights from novel approaches to reward shaping and off-policy model evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Roggeveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7h8xKQ8">Intensive Care Med Exp</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Roggeveen, L. F. et al. Reinforcement learning for intensive care medicine: actionable clinical insights from novel approaches to reward shaping and off-policy model evaluation. Intensive Care Med Exp. 12, 32 (2024).</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main" xml:id="_tJMgj6n">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rXzSEMB">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Van Hasselt, H., Guez, A. &amp; Silver, D. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, 30 (2016).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
