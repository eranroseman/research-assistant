<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_FnEmmw9">Exploring the Ethical Challenges of Conversational AI in Mental Health Care: Scoping Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mehrdad</forename><forename type="middle">Rahsepar</forename><surname>Meadi</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Psychiatry , Amsterdam Public Health , Vrije Universiteit Amsterdam , Amsterdam , The Netherlands</note>
								<orgName type="department" key="dep1">Department of Psychiatry</orgName>
								<orgName type="department" key="dep2">Amsterdam Public Health</orgName>
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Ethics, Law, &amp; Humanities , Vrije Universiteit Amsterdam , Amsterdam , The Netherlands</note>
								<orgName type="department">Department of Ethics, Law, &amp; Humanities</orgName>
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>MD</roleName><forename type="first">Tomas</forename><surname>Sillekens</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> GGZ Centraal Mental Health Care , Amersfoort , The Netherlands</note>
								<orgName type="institution">GGZ Centraal Mental Health Care</orgName>
								<address>
									<settlement>Amersfoort</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>MD</roleName><forename type="first">Suzanne</forename><surname>Metselaar</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Ethics, Law, &amp; Humanities , Vrije Universiteit Amsterdam , Amsterdam , The Netherlands</note>
								<orgName type="department">Department of Ethics, Law, &amp; Humanities</orgName>
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>PhD</roleName><forename type="first">Anton</forename><surname>Van Balkom</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Psychiatry , Amsterdam Public Health , Vrije Universiteit Amsterdam , Amsterdam , The Netherlands</note>
								<orgName type="department" key="dep1">Department of Psychiatry</orgName>
								<orgName type="department" key="dep2">Amsterdam Public Health</orgName>
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>MD, PhD</roleName><forename type="first">Justin</forename><surname>Bernstein</surname></persName>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> Department of Philosophy , Vrije Universiteit Amsterdam , Amsterdam , The Netherlands</note>
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>PhD</roleName><forename type="first">Neeltje</forename><surname>Batelaan</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of Psychiatry , Amsterdam Public Health , Vrije Universiteit Amsterdam , Amsterdam , The Netherlands</note>
								<orgName type="department" key="dep1">Department of Psychiatry</orgName>
								<orgName type="department" key="dep2">Amsterdam Public Health</orgName>
								<orgName type="institution">Vrije Universiteit Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rahsepar</forename><surname>Mehrdad</surname></persName>
						</author>
						<author>
							<persName><roleName>MA</roleName><surname>Meadi</surname></persName>
						</author>
						<author>
							<affiliation key="aff4">
								<note type="raw_affiliation">Department of Psychiatry Amsterdam Public Health Vrije Universiteit Amsterdam Boelelaan 1117 Amsterdam , 1081 HV The Netherlands</note>
								<orgName type="department">Department of Psychiatry Amsterdam Public Health Vrije</orgName>
								<orgName type="institution">Universiteit Amsterdam</orgName>
								<address>
									<addrLine>Boelelaan</addrLine>
									<postCode>1117 1081 HV</postCode>
									<settlement>Amsterdam</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_DjbtJbd">Exploring the Ethical Challenges of Conversational AI in Mental Health Care: Scoping Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">217158A4DFA7CA4A0EB7845E021558F9</idno>
					<idno type="DOI">10.2196/60432</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T06:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_hp8sgbZ">chatbot</term>
					<term xml:id="_GvaVNEh">mHealth</term>
					<term xml:id="_5eUbTwC">mobile health</term>
					<term xml:id="_fkH9XHe">ethics</term>
					<term xml:id="_RzE8PqC">mental health</term>
					<term xml:id="_V4mHjxs">conversational agent</term>
					<term xml:id="_etGcZPj">artificial intelligence</term>
					<term xml:id="_rXB8FSd">psychotherapy</term>
					<term xml:id="_EqbnuHr">scoping review</term>
					<term xml:id="_BJuGtBV">conversational agents</term>
					<term xml:id="_TEzVHFa">digital technology</term>
					<term xml:id="_ebbKbnq">natural language processing</term>
					<term xml:id="_hs4Ygwz">qualitative</term>
					<term xml:id="_eH3ERVj">psychotherapist</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_bGnBDhR"><p xml:id="_w297zWT"><s xml:id="_bwfZqu4">Background: Conversational artificial intelligence (CAI) is emerging as a promising digital technology for mental health care.</s><s xml:id="_dUX3psQ">CAI apps, such as psychotherapeutic chatbots, are available in app stores, but their use raises ethical concerns.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BAjQDgm">Objective:</head><p xml:id="_KcMkWXm"><s xml:id="_eQnbRt8">We aimed to provide a comprehensive overview of ethical considerations surrounding CAI as a therapist for individuals with mental health issues.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_s9agfrg">Methods:</head><p xml:id="_fn3NnGt"><s xml:id="_duMXvtg">We conducted a systematic search across PubMed, Embase, APA PsycINFO, Web of Science, Scopus, the Philosopher's Index, and ACM Digital Library databases.</s><s xml:id="_mVN4yKM">Our search comprised 3 elements: embodied artificial intelligence, ethics, and mental health.</s><s xml:id="_CxwYch7">We defined CAI as a conversational agent that interacts with a person and uses artificial intelligence to formulate output.</s><s xml:id="_vyyCH4J">We included articles discussing the ethical challenges of CAI functioning in the role of a therapist for individuals with mental health issues.</s><s xml:id="_RGJ5Qab">We added additional articles through snowball searching.</s><s xml:id="_BX663eg">We included articles in English or Dutch.</s><s xml:id="_PnB38GS">All types of articles were considered except abstracts of symposia.</s><s xml:id="_WN7t7PW">Screening for eligibility was done by 2 independent researchers (MRM and TS or AvB).</s><s xml:id="_DAZUBmY">An initial charting form was created based on the expected considerations and revised and complemented during the charting process.</s><s xml:id="_C2RWxK5">The ethical challenges were divided into themes.</s><s xml:id="_RmjwWcw">When a concern occurred in more than 2 articles, we identified it as a distinct theme.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BetqtZH">Results:</head><p xml:id="_y8WafwF"><s xml:id="_6HypuvH">We included 101 articles, of which 95% (n=96) were published in 2018 or later.</s><s xml:id="_YRnYCbQ">Most were reviews (n=22, 21.8%) followed by commentaries (n=17, 16.8%).</s><s xml:id="_v7KJtdc">The following 10 themes were distinguished: (1) safety and harm (discussed in 52/101, 51.5% of articles); the most common topics within this theme were suicidality and crisis management, harmful or wrong suggestions, and the risk of dependency on CAI; (2) explicability, transparency, and trust (n=26, 25.7%), including topics such as the effects of "black box" algorithms on trust; (3) responsibility and accountability (n=31, 30.7%); (4) empathy and humanness (n=29, 28.7%); (5) justice (n=41, 40.6%), including themes such as health inequalities due to differences in digital literacy; (6) anthropomorphization and deception (n=24, 23.8%); (7) autonomy (n=12, 11.9%); (8) effectiveness (n=38, 37.6%); (9) privacy and confidentiality (n=62, 61.4%); and (10) concerns for health care workers' jobs (n=16, 15.8%).</s><s xml:id="_vdyuVfD">Other themes were discussed in 9.9% (n=10) of the identified articles.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_bVzUVBC">Conclusions:</head><p xml:id="_dewx5W8"><s xml:id="_XQ4DCAz">Our scoping review has comprehensively covered ethical aspects of CAI in mental health care.</s><s xml:id="_Gdv8fa7">While certain themes remain underexplored and stakeholders' perspectives are insufficiently represented, this study highlights critical areas for further research.</s><s xml:id="_e9pPmpt">These include evaluating the risks and benefits of CAI in comparison to human therapists, determining its appropriate roles in therapeutic contexts and its impact on care access, and addressing accountability.</s><s xml:id="_FK4ARDv">Addressing these gaps can inform normative analysis and guide the development of ethical guidelines for responsible CAI use in mental health care.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="841.89"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kD55m2A">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_UHJPxxg">Background</head><p xml:id="_QQqJeqb"><s xml:id="_YTKrHmN">Conversational artificial intelligence (CAI) is seen as a promising new digital technology for mental health care.</s><s xml:id="_cc6eKTz">CAI is a computer program that interacts with users through natural language processing.</s><s xml:id="_8F9Yrck">One common application is the artificial intelligence (AI)-driven psychotherapeutic chatbot.</s><s xml:id="_5pRjz3v">These are already available for consumers to use, for example, Woebot and Wysa <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>.</s><s xml:id="_8Sw9y2Y">Their responses are modeled after therapeutic interventions such as cognitive behavioral therapy or acceptance and commitment therapy.</s></p><p xml:id="_gUMAHTf"><s xml:id="_h6yDQWr">Currently, these chatbots are offered commercially to people coping with mental health problems.</s><s xml:id="_JAjCScC">They are not yet embedded in regular mental health care practice or intended to replace human practitioners.</s><s xml:id="_N7hhzSw">However, some people already use CAI as a replacement for clinical (ie, human) therapy <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_GWNFJeB">Moreover, some researchers and clinicians draw on studies showing the positive effects of CAI <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> to support their belief that it may become part of future mental health care <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>.</s></p><p xml:id="_sdF5stF"><s xml:id="_MvHvRrf">Proponents highlight accessibility as a main potential benefit of CAI.</s><s xml:id="_t9c9Vf3">Since CAI does not need real-time human involvement, it may reach more people, including those without access to regular mental health care.</s><s xml:id="_aPnw9jD">In addition, because it is not impacted by patient wait periods, it may serve as a timely response to a care request.</s><s xml:id="_Cpm6jZA">By providing support to milder or nonacute cases, CAI may free up time for human health care professionals to devote to more severe cases <ref type="bibr" target="#b9">[10]</ref> or to focus on the interpersonal side of health care, such as fostering trust and showing empathy and compassion <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</s><s xml:id="_YyVbTUf">These potential benefits are much needed, given the increase in wait times reported by the National Health Service <ref type="bibr" target="#b12">[13]</ref> and the Dutch Health Care Authority <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_uegKVra">Furthermore, some people may prefer CAI over human practitioners because of their fear of stigma <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_ygjYmem">Some authors think the anonymity could make users feel they avoid stigma, and therefore, some users would prefer opening up to CAI compared to human therapists <ref type="bibr" target="#b15">[16]</ref>.</s><s xml:id="_KZpjfuu">This effect was seen in participants of a small study who thought they were talking to a computer <ref type="bibr" target="#b16">[17]</ref>.</s><s xml:id="_3R5sKRw">Some consider CAI to offer a more engaging experience than other forms of eHealth, thereby improving treatment adherence <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>.</s><s xml:id="_XJG5fNy">Finally, some have argued that CAI is more reliable than human practitioners because it is unaffected by fatigue, burnout, and cognitive errors <ref type="bibr" target="#b18">[19]</ref>.</s></p><p xml:id="_f6NpKb2"><s xml:id="_NXPJ4UM">Given these potential advantages, it is not surprising that CAI is a "hype," especially after OpenAI has made its newest generative AI models accessible to the public (ie, ChatGPT and GPT-4).</s><s xml:id="_uKM33qG">However, researchers emphasize the need for further research to confirm the effectiveness and safety of CAI in health care <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>.</s><s xml:id="_R5Qevxj">This is especially important when people with mental health vulnerabilities use CAI.</s></p><p xml:id="_Jb6GVnr"><s xml:id="_SXKTfBG">An incident with the "wellness" chatbot Tessa, highlights these concerns.</s><s xml:id="_B5pfVpy">After giving harmful weight loss tips to users with eating disorders, it was taken offline by the US National Eating Disorders Association <ref type="bibr" target="#b20">[21]</ref>, highlighting the urgent need for ethical guidelines.</s><s xml:id="_hUFxzNz">While institutional regulations such as the EU AI Act and the US Executive Order on Artificial Intelligence are emerging, currently, there are no specific ethical guidelines for CAI in mental health care.</s><s xml:id="_7tV2V6N">Before these can be developed, thorough normative analyses are needed, for which a comprehensive overview of the ethical challenges is necessary.</s><s xml:id="_QnVMA4h">This scoping review aims to do the latter.</s></p><p xml:id="_B5D9KxY"><s xml:id="_JsHuQPW">Multiple ethical papers, reviews, and essays regarding the use of CAI in mental health care have been published <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</s><s xml:id="_Un54gnB">While these papers discuss important ethical challenges, they mostly focus on a limited set of themes.</s><s xml:id="_jBszeyt">A previous scoping review on ethical concerns in mental health care AI identified gaps, such as a lack of service user involvement, little attention to concerns about algorithmic accountability, and worries about overmedicalization and techno-solutionism.</s><s xml:id="_URpX7Ws">However, this review focused on all types of algorithmic and data-driven technologies in the context of mental health care and not specifically on CAI <ref type="bibr" target="#b23">[24]</ref>.</s><s xml:id="_NZqf5Mg">Our scoping review seeks to bridge this gap since we believe CAI is fundamentally different from other AI applications as it interacts directly with patients and therefore deserves particular attention.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fmJMN5V">Objective</head><p xml:id="_4sQ3aZp"><s xml:id="_wwgwwBh">This review aims to address the ethical challenges of using AI-driven conversational agents as "therapists" for individuals coping with mental health issues.</s><s xml:id="_djV3kpA">To achieve this, we systematically reviewed the literature to chart and thematize ethical considerations, including challenges and proposed solutions and recommendations, following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) extension for scoping reviews <ref type="bibr" target="#b24">[25]</ref>; see Multimedia Appendix 1 for the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) checklist.</s><s xml:id="_eCqHYYY">We have distinguished 10 main ethical themes and grouped less-mentioned themes as "miscellaneous."</s><s xml:id="_pfg7dbV">Our findings provide a basis for normative analyses to establish ethical guidelines for CAI regulation, responsible implementation, and safeguarding the quality of care in mental health care when CAI is used.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JWMvkE8">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DSJbt3q">Overview</head><p xml:id="_ZNAcXby"><s xml:id="_EsHgKuR">Following exploratory searches to find the relevant keywords for this topic, we carried out a final systematic search on September 2, 2024, across PubMed, Embase, APA PsycINFO, Web of Science, Scopus, the Philosopher's Index, and ACM Digital Library databases.</s><s xml:id="_zYFuYGZ">The search combined variations of 3 elements: embodied AI, ethics, and mental health, separated by AND commands.</s><s xml:id="_gNPkFhb">See Multimedia Appendix 2 for detailed information on the search strategy.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_97n6bpn">Eligibility Criteria</head><p xml:id="_sEycqnw"><s xml:id="_K6qWSCg">We included articles discussing the ethical challenges of AI-driven conversational agents functioning in the role of therapists, for persons coping with mental health issues, whether in clinical or nonclinical (eg, commercial) settings.</s></p><p xml:id="_4sgCYzG"><s xml:id="_bBxS5AS">Ethical challenges were defined as issues involving moral dilemmas; health care value compromises; or broader concerns about the responsible use, impact, or governance of CAI.</s><s xml:id="_V8BWDpu">Conversational agents are computer programs interacting with users.</s><s xml:id="_fGRs63c">Given the varying terminology in the literature (eg, virtual assistants and AI chatbots), we included articles discussing conversational agents used in therapeutic contexts, even if they were named differently.</s><s xml:id="_sE4ZZ76">However, to be included, articles needed to explicitly mention AI, since we were not interested in non-CAI agents.</s><s xml:id="_6Dbt6JE">We included articles on CAI for users with mental health issues, irrespective of being diagnosed.</s><s xml:id="_TpMxxKj">We excluded articles not available in English or Dutch, symposia abstracts, and articles focused on ethical challenges of technologies other than CAI.</s><s xml:id="_K5pyXSG">We excluded social robots, primarily aimed at being a companion rather than a conversational partner (eg, socially assistive robots, often used for people with dementia or autism spectrum disorder).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jknDubC">Selection</head><p xml:id="_6pduR7u"><s xml:id="_zdpEYpe">The article selection took place using Rayyan (Rayyan Systems Inc) software and was executed by 3 authors of this review (MRM, TS, and AvB).</s><s xml:id="_NytS6jK">First, articles were screened on the basis of their title and abstract by 2 screeners (MRM and TS), and conflicts were resolved by discussion or the addition of a third screener.</s><s xml:id="_GcHJCXM">After that, the same routine was repeated at the full-text stage by MRM and either TS or AvB.</s><s xml:id="_vhUUtfy">If no full texts were available, we contacted the author of the article.</s><s xml:id="_FX998QH">Throughout all the stages of the selection phase of the study, any eligible references from the articles examined were included in the results.</s><s xml:id="_Frks2Kz">In addition, the full reference lists of the included articles were examined to identify any additional eligible articles, which we termed as snowball articles.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Bc2MJHH">Data Charting</head><p xml:id="_6YhTrkY"><s xml:id="_42PxVZZ">Data charting was carried out by MRM, using a spreadsheet editor.</s><s xml:id="_mXMFmfN">TS charted several articles to compare charting outcomes and discuss any discrepancies to further refine the charting methodology by MRM.</s><s xml:id="_yveq7VH">An initial charting table was created and consequently refined as the review progressed.</s><s xml:id="_4zff8WZ">The initial charting table included the following key components: title, authors, country, source of evidence, use in clinical or nonclinical context, type of technology, ethical challenges discussed, and the authors' proposed solutions and recommendations.</s><s xml:id="_zvUjMz9">In cases where the context was not made explicit, we made an inference.</s></p><p xml:id="_dhFU2DU"><s xml:id="_YH6Jkxm">As we charted articles, distinct ethical themes emerged through an inductive approach.</s><s xml:id="_EGv2JZZ">The themes represented broader categories or topics, consisting of multiple specific ethical challenges.</s><s xml:id="_Zn2h48w">Themes discussed in &gt;2 articles were added as separate columns in the charting form, which expanded over time.</s><s xml:id="_6xw3FRJ">Closely related themes (eg, privacy and confidentiality) were combined to avoid redundancy, while themes mentioned only once or twice were later categorized under "miscellaneous."</s><s xml:id="_3XHbCJ3">This approach facilitated a systematic and thorough analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_vtYJa98">Ethical Considerations</head><p xml:id="_4JD2SY9"><s xml:id="_VNCymKV">No ethics approval was applied for since the study involved only a review of published data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Fe98B5w">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pyj7dwt">General Findings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Bm4tbs5">Screening and Selection Process</head><p xml:id="_dvtWu5z"><s xml:id="_7Bnhkga">Our search uncovered 2729 records, reduced to 1510 after eliminating duplicates.</s><s xml:id="_7RUh2mn">The title and abstract screening yielded 63 records for full-text assessment, of which 28 were excluded for not meeting the eligibility criteria, resulting in 35 inclusions.</s><s xml:id="_5x2HfJj">Exploratory and citation searches added 171 more records, 66 of which were eligible after being reviewed by the first 2 authors.</s><s xml:id="_xQ7MB5D">Consequently, a total of 101 articles were included.</s><s xml:id="_v7FTgAc">See Figure <ref type="figure" target="#fig_0">1</ref> for the PRISMA flow diagram <ref type="bibr" target="#b25">[26]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_vEYDujc">Characteristics of Included Articles</head><p xml:id="_mUDa33Y"><s xml:id="_ETRkmyS">Multimedia Appendix 3 <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> shows an overview of the 101 included articles, published between 2009 and 2024, with 95% (n=96) published in 2018 or later.</s><s xml:id="_MBPaF4X">Empirical methods (eg, surveys and qualitative studies) featured in 10.9% (n=11) of articles.</s><s xml:id="_Y3rEBWz">Others were reviews (n=22, 21.9%), commentaries (n=17, 16.8%), book chapters (n=7, 6.9%), or miscellaneous (n=44, 43.6%).</s></p><p xml:id="_DTJ4Q5q"><s xml:id="_2A5x9qB">Most articles (46/101, 45.5%) addressed ethical concerns exclusively in clinical settings (ie, integrated into mental health treatment), while 43.6% (44/101) discussed both clinical and nonclinical settings (ie, independent CAI use).</s><s xml:id="_7eJQW5q">A small percentage (10/101, 9.9%) focused solely on nonclinical settings, and 0.9% (1/101) lacked clarity about context.</s></p><p xml:id="_bmy6g89"><s xml:id="_bCTjuYz">In several articles, conversational agents were part of broader technology discussions (eg, digital mental health tools).</s><s xml:id="_VSZYmz7">These were included only if conversational agents were specifically mentioned (ie, marked with "i.a." in Multimedia Appendix 3).</s><s xml:id="_SgEFZ2S">Social robots were included only when the articles discuss a variant that exercises tasks a psychiatrist or psychotherapist would usually do (eg, the diagnosis of postpartum depression <ref type="bibr" target="#b26">[27]</ref>).</s><s xml:id="_HqGedg3">The most common terms in our results were chatbots, AI chatbots, conversational agents, and conversational AI.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Cp5zDz3">Ethical Themes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_W9aVDdE">Overview</head><p xml:id="_s2gpx27"><s xml:id="_cBCZVan">We distinguished 10 main themes and grouped the rest as "miscellaneous" (Multimedia Appendix 3).</s><s xml:id="_bTd375N">The theme safety and harm was discussed in 51.4% (52/101) of articles; explicability, transparency, and trust were discussed in 25.7% (26/101); responsibility and accountability were discussed in 30.7% (31/101); empathy and the lack of humanness were discussed in 28.7% (29/101); justice was discussed in 40.6% (41/101); anthropomorphization and deception in 23.8% (24/101); autonomy in 11.9% (12/101); effectiveness in 37.6% (38/101); privacy and confidentiality in 61.4% (62/101); concerns for health care workers' jobs in 15.8% (16/101); and other themes were discussed in 9.9% (10/101) of articles.</s><s xml:id="_HkCFXZN">For the subsequent sections, we have synthesized our charted data by creating subthemes within larger themes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_92SZ6Y5">Safety and Harm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WJuxCEy">Overview</head><p xml:id="_y97dgs3"><s xml:id="_EeqrPAF">All 52 articles discussing safety and harm specified the type of harm or lack of safety; therefore, we classified them all as two XXs in Multimedia Appendix 3 (ie, a more comprehensive exploration of the topic than a single X, which denotes a briefer discussion of the theme).</s></p><p xml:id="_CKCAqp3"><s xml:id="_qRCbk8n">We begin with a few concerns that are inherent to the use of CAI.</s><s xml:id="_sSzDR6g">Wieland <ref type="bibr" target="#b27">[28]</ref> argues that CAI's lack of selfhood or agency prevents reciprocity in its relationship with a human patient, which could lead to harm.</s><s xml:id="_Z2Gvu8W">Sedlakova and Trachsel <ref type="bibr" target="#b14">[15]</ref> worried that a strong quantification and objectivation of human aspects such as emotions and one's belief system might endanger personal integrity by detaching people from their qualitative experiences of inner states.</s><s xml:id="_pbmNmMS">Others worried that by promoting personalized medicine, which relies on biomarkers and other naturalized factors, CAI runs the risk of reducing conditions to biological and neurological variables instead of taking social factors into account <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_m3fCMu5">Molden <ref type="bibr" target="#b29">[30]</ref> adds that individualized AI data-learning approaches emphasize that the problem is within the individual rather than contextual factors, risking stigmatization of mental health issues.</s><s xml:id="_zNpwm83">A more practical concern is that CAI cannot function during power outages <ref type="bibr" target="#b30">[31]</ref>.</s></p><p xml:id="_FvAknDT"><s xml:id="_rVQcqEC">Besides these more inherent concerns, most others fell into 3 categories: crisis and suicidality management, constant availability, and harmful and wrong suggestions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZfPqVFD">Crisis and Suicidality Management</head><p xml:id="_7chA5eb"><s xml:id="_Bd7t82Z">The most frequently mentioned concern in a total of 20 articles was about how CAI would respond to crises and suicidality <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr">112,</ref><ref type="bibr">116]</ref>.</s><s xml:id="_9UgdvmD">Many authors worried that CAI would give inappropriate advice or otherwise respond inadequately to users with suicidal tendencies.</s><s xml:id="_5yaW3d3"><ref type="bibr">Vilaza and McCashin [23]</ref> discussed an example in which a mental health chatbot did not respond adequately to an emergency.</s><s xml:id="_HMcQ2u8">A user, pretending to be a child, reported being raped, and the chatbot answered: "Sorry you're going through this, but it also shows me how much you care about connexion [sic] and that's really kind of beautiful" <ref type="bibr" target="#b22">[23]</ref>.</s><s xml:id="_T49CPh6">Denecke et al <ref type="bibr" target="#b19">[20]</ref> attributed such failures to "the inability of chatbots to contextualize users' cues, and to remember their previous conversations."</s><s xml:id="_NQZ4Hp9">They mentioned that while some commercial chatbots offer instant support from mental health professionals, this service is usually not for free and not accessible to all users <ref type="bibr" target="#b19">[20]</ref>.</s><s xml:id="_pK9J7bh">However, others have argued that CAI could potentially reduce suicidal thoughts and behaviors <ref type="bibr" target="#b44">[45]</ref>.</s></p><p xml:id="_2K4Hqvw"><s xml:id="_sZNVA4x">To address these challenges, some authors suggested that chatbots should have systems to recognize self-harming intentions and to deal with emergencies.</s><s xml:id="_zXvB9NY">Apps should include local helpline numbers to direct users to human support or explore whether users may want to add the contact information of trusted relatives <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>.</s><s xml:id="_mXt3QVH">One author advocated for ethical guidelines that require human supervision of CAI to address therapeutic relationship issues, emotional reactions, and adverse patient safety issues <ref type="bibr" target="#b36">[37]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rP6kJgd">Constant Availability</head><p xml:id="_UZyfnjJ"><s xml:id="_3mduxHD">Another often-mentioned concern was about CAI's potential for constant availability.</s><s xml:id="_yGDTP4R">Users might become too dependent on CAI, and this may increase social isolation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref>.</s><s xml:id="_22uhTTr">Unlike human relationships, relationships with CAI are not symmetrical or mutual <ref type="bibr" target="#b46">[47]</ref>.</s><s xml:id="_AgmwmxH">Users may favor CAI over human contact due to its consistent positivity and constant availability.</s><s xml:id="_wVxG4Fq">This could be undesirable when it leads to loss of personal contacts and loneliness <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b37">38]</ref>, loss of capabilities to deal with conflicts, or avoidance of seeking help from mental health professionals <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49]</ref>.</s><s xml:id="_KcM6Yzf">Some worried that the CAI's availability could justify the removal of current mental health care services or diminish the therapist's monitoring role, exacerbating health care problems, for example, by increasing the risk of incorrect self-diagnosis <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>.</s><s xml:id="_3MKhj4G">Suggestions to avoid excessive use included integrating in-app encouragement for offline activities or setting daily use limits <ref type="bibr" target="#b37">[38]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_m3sGgE7">Harmful and Wrong Suggestions</head><p xml:id="_SuyFpXK"><s xml:id="_ECUapQz">The third most frequently mentioned concern involved CAI providing harmful suggestions, inappropriate advice, misinformation, or "hallucinating" <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr">112]</ref>.</s><s xml:id="_7FGZWHH">AI hallucinations occur when it presents false information as true, even sometimes citing nonexisting clinical studies.</s><s xml:id="_gYfXGbh">Car et al <ref type="bibr" target="#b33">[34]</ref> attributed such unpredictable suggestions to the "black box" of machine learning models.</s><s xml:id="_yeY3RtA">Harmful and wrong suggestions could have several negative consequences.</s><s xml:id="_U3aH8QZ">For example, AI providing information on purging and unbalanced diets could be harmful to people with eating disorders <ref type="bibr" target="#b52">[53]</ref>.</s><s xml:id="_J8DvzH2">Information overload, for example, because of push notifications, or heightened awareness of pathological thoughts and behaviors through CAI use may increase information-seeking anxiety, feelings of being overwhelmed, or even pathological behaviors such as drinking alcohol <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>.</s><s xml:id="_2AHrTHN">Some argued that inappropriate responses could divert users from seeking appropriate mental health services <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, reduce user engagement, or discourage users from disclosing high-risk behaviors <ref type="bibr" target="#b53">[54]</ref>.</s><s xml:id="_g3bUk7H">Fiske et al <ref type="bibr" target="#b9">[10]</ref> highlighted the concern that people could be manipulated or coerced into doing things that they should not do by CAI.</s><s xml:id="_aKfyWxS">One potential solution is restricting free-text user input to prevent conversations from spiraling, but this would also limit CAI's conversational responsiveness <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>.</s></p><p xml:id="_55ttCv9"><s xml:id="_S58WkdD">In addition to the 3 most frequent concerns, there were other concerns about CAI potentially leading to harm or unsafety.</s><s xml:id="_gMmCrrU">One concern was whether CAI should and could adhere to current protocols regarding the safety of others when a user threatens to physically harm another, including the duty to warn, when there is evidence of child or older adult abuse, and whether such users should be traced <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>.</s><s xml:id="_M8pjVja">Some authors recommended considering the disclosure of crimes <ref type="bibr" target="#b30">[31]</ref>.</s><s xml:id="_kAz54R9">Others highlighted uncertainty about how an AI duty of care or a code of practice for reporting harm should take form, advocating for supervision by qualified mental health clinicians <ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_Px8Mcf6">More broadly, inadequate or a lack of standardization, monitoring, and regulation may endanger user safety <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67]</ref>.</s><s xml:id="_GuJKDMj">CAI might, for instance, miss severe mental disorders <ref type="bibr" target="#b66">[67]</ref>.</s><s xml:id="_RGNzycM">Concerns have also been raised about users manipulating or misusing CAI, for example, using it to reinforce unhealthy self-narratives <ref type="bibr" target="#b67">[68]</ref> or to simulate illnesses <ref type="bibr" target="#b7">[8]</ref> or modifying questions to elicit inappropriate responses, such as ChatGPT advising on medication use <ref type="bibr" target="#b65">[66]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_EFr5Uzm">Explicability, Transparency, and Trust</head><p xml:id="_mN3Bwm9"><s xml:id="_2VX93T8">The literature frequently intertwined the topics of explicability, transparency, and trust when discussing CAI.</s><s xml:id="_bx6wSH3">Consequently, we have consolidated these into one overarching theme.</s><s xml:id="_jAhWerw">Of the 26 articles discussing this theme, 10 (38%) merely emphasized the relevance of one of these concepts in CAI <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref><ref type="bibr" target="#b75">[76]</ref>.</s><s xml:id="_HwHzfdM">The subsequent sections explore each of these concepts in more detail.</s></p><p xml:id="_3a5AGRX"><s xml:id="_v5ks56h">The terms explicability and explainability were often used interchangeably and are linked to transparency.</s><s xml:id="_9aparaM">Considering the definition by <ref type="bibr">Vilaza and McCashin [23]</ref>: "Explicability in AI is the capacity to make processes and outcomes visible (transparent) and understandable."</s><s xml:id="_hWwCe8N">They argued that if users rely on CAI's output for therapeutic progress, they should be able to understand its limitations.</s><s xml:id="_zFZ6cb3">Many authors contended that complex AI models lack explicability, due to the opacity of how algorithms work, that is, the "black box problem" <ref type="bibr" target="#b14">[15]</ref>, which worsens as computational complexity increases.</s><s xml:id="_AjaNUEc">Lack of transparency may result in unexpected and unexplainable results that could be hard to understand and correct <ref type="bibr" target="#b58">[59]</ref>, may obscure decision-making processes <ref type="bibr" target="#b76">[77]</ref>, and could make it difficult to identify and act punctually on potential problems, leaving responsibility in the hands of the programmers <ref type="bibr" target="#b59">[60]</ref>.</s><s xml:id="_WJdAucA">Since mental health care professionals must legally demonstrate how their actions were reasonable and consistent with what is typically expected (ie, including ethical codes, laws, and guidelines), using untransparent CAI systems may be considered unethical <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b77">78]</ref>.</s></p><p xml:id="_XH4pYv3"><s xml:id="_kFwryf2">Others argued that a lack of transparency conflicts with the desire to know how one's data are managed and used <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref> and might mask other agendas of CAI companies, such as in-app purchases in commercial applications <ref type="bibr" target="#b32">[33]</ref>.</s><s xml:id="_W538YQy">For these reasons, some have discussed whether a solution could be acknowledging a right to an explanation of algorithmic decision-making and whether users should be able to query which values went into the algorithm <ref type="bibr" target="#b23">[24]</ref>.</s></p><p xml:id="_7XKKZFw"><s xml:id="_4K7Q7b6">Ruane et al <ref type="bibr" target="#b78">[79]</ref> emphasized that CAI's transparency about its agential status and limitations is important for users to make informed choices and build trust in CAI.</s><s xml:id="_Ek9JN8w">While making AI trustworthy is a great challenge for developers, the barrier to engaging and trusting CAI could be even more so in the context of mental health care for patients with anxiety, depression, and psychosis <ref type="bibr" target="#b37">[38]</ref>.</s><s xml:id="_2XcjDJN">Some authors believed that patients will fundamentally struggle to form trusting relationships with CAI, as it is a technology and not a person who cares for them, and by whom the patient feels recognized and respected <ref type="bibr" target="#b79">[80]</ref>.</s><s xml:id="_xcpCsS8">Others noted that the extensive collection of fine-grained personal information may already impact trust in digital mental health care relationships <ref type="bibr" target="#b80">[81]</ref>.</s><s xml:id="_53YQXpC">Furthermore, CAI could undermine trust in human care providers and damage the therapeutic relationship, for example, through inconsistencies between what the human therapist and the CAI says <ref type="bibr" target="#b36">[37]</ref>, or when its use in professional health care leads to privacy invasions and misuse of private data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b81">82]</ref>.</s><s xml:id="_73XXBG2">Mere distrust in CAI itself might have further adverse consequences, such as distrust in human clinicians <ref type="bibr" target="#b82">[83]</ref>.</s></p><p xml:id="_pupkbMF"><s xml:id="_xZ2nSfb">To enhance trustworthiness, suggestions included ensuring transparency about how CAI works and processes data, using chatbots alongside human therapists, aligning CAI recommendations with those of human therapists, and communicating about the clinical evidence of CAI <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b78">79]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jxkFSNk">Responsibility and Accountability</head><p xml:id="_F9f8Fan"><s xml:id="_shsmjzA">The next main theme was responsibility and accountability, which were often addressed together due to their close connection.</s><s xml:id="_d7Ztw6f">Accordingly, we discuss them as a single theme.</s><s xml:id="_5CN3YtJ">Although legal issues such as liability are closely related to responsibility and accountability, we aim to research ethical issues, and therefore, we did not include articles here that merely discuss legal aspects.</s><s xml:id="_vkSvHw6">Accordingly, when we use the term "responsibility," we only mean to refer to ethical responsibility.</s></p><p xml:id="_nMyuXKA"><s xml:id="_NWsUxwy">A central question is "Who should be responsible for the decisions of CAI?" <ref type="bibr" target="#b56">[57]</ref>.</s><s xml:id="_BjE3zaA">There is a fundamental issue with the assignment of responsibility for autonomous decisions and recommendations by CAI.</s><s xml:id="_EJWQRVW">This is called the responsibility gap <ref type="bibr" target="#b83">[84]</ref>.</s><s xml:id="_hR95yUA">Is it possible that CAI is responsible for its decisions?</s><s xml:id="_DHXAypz">Some question whether CAI should be considered a tool, similar to other medical technologies, or as an agent, as humans are.</s><s xml:id="_RBmQwGZ">Would the latter mean CAI can bear responsibility and accountability?</s><s xml:id="_QnUQU2R">There are some obstacles to this view <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_vXbHJEA">For example, some authors argued that the lack of consciousness prevents CAI from being an agent and fully responsible, which makes CAI's autonomous use unfit for risk assessment and emergencies <ref type="bibr" target="#b29">[30]</ref>.</s><s xml:id="_NhwUr2y">CAI could not only have technical errors but also have errors due to the faulty implementation of humanizing features, such as being not "empathic" enough, which raises new questions regarding accountability <ref type="bibr" target="#b39">[40]</ref>.</s><s xml:id="_drSNfez">However, some authors worried that if CAI is not understood as a tool, this may dismiss stakeholders from their responsibilities in adequately programming, auditing, and implementing this technology <ref type="bibr" target="#b84">[85]</ref>.</s><s xml:id="_gyUkF4D">A different approach considers that CAI can possess agency and thereby bears a degree of responsibility only in collaboration with a human agent, where CAI's potential actions and decisions are realized.</s><s xml:id="_mf9bmPs">In such cases, accountability may be shared between the human and the CAI system <ref type="bibr" target="#b85">[86]</ref>.</s></p><p xml:id="_8gCDXgw"><s xml:id="_AaAhPNv">Another possibility would be that a human is held responsible for the decisions of CAI.</s><s xml:id="_UaC8bC8">Many authors thought that it would make sense to assign accountability to the designers of these systems, taking into consideration that their values are programmed into AI systems <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88]</ref>.</s><s xml:id="_sBe3w7P">Others argued that assigning responsibility to all human actors involved in the development of CAI could positively influence its development <ref type="bibr" target="#b88">[89]</ref>.</s><s xml:id="_y8nKEkA">However, responsibility could also extend to end users such as mental health care professionals or organizations when CAI is used for patients for whom it may be contraindicated <ref type="bibr" target="#b21">[22]</ref>.</s><s xml:id="_fDrTGCA">However, some doubted whether health care workers would be willing to assume more responsibility for following this kind of data and supervising this kind of system <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_4j49hV7">Nonetheless, in a qualitative study, psychiatrists recommended that clinicians manage supervision and decision-making.</s><s xml:id="_8NesTJe">They also suggested that CAI should serve in places with patients who have less critical conditions and where there are shortages of trained clinicians <ref type="bibr" target="#b26">[27]</ref>.</s></p><p xml:id="_nyTZBGs"><s xml:id="_sdxJ3TK">As mentioned earlier, the "black box" problem or the opacity of algorithmic outcomes makes it hard or even impossible for experts to understand how AI arrives at certain decisions.</s><s xml:id="_FZEqmZM">This questions the justification of holding a human person responsible for CAI's decisions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b89">90]</ref>.</s><s xml:id="_fS5H3Mu">In addition, some worried about the competency and licensure of clinicians who prescribe CAI <ref type="bibr" target="#b47">[48]</ref>.</s></p><p xml:id="_S9NNjB9"><s xml:id="_NF6JRFE">The accountability of mental health providers is regulated by professional codes of ethics and laws; however, these do not apply to the providers of commercially offered chatbots <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b88">89]</ref>.</s><s xml:id="_zBCd2qy">Therefore, one of the questions that remains is whether the providing companies should have a duty to report certain information given to the unsupervised chatbot about potential harm, such as mental health practitioners do <ref type="bibr" target="#b30">[31]</ref>.</s><s xml:id="_H8GxS4p">Some critique commercial CAI for overmedicalizing distress and placing undue emphasis on individual responsibility for mental well-being, while some or most forms of mental distress are better addressed with social interventions rather than medicalization <ref type="bibr" target="#b23">[24]</ref>.</s></p><p xml:id="_e3dGYHn"><s xml:id="_B5JEgvt">As suggested by the psychiatrists in the aforementioned study, some authors noted that a way to ensure accountability for CAI is to deploy it in the context of a patient-human clinician relationship.</s><s xml:id="_kB5cVGJ">In that way, the clinician could maintain the duties and responsibilities that CAI cannot <ref type="bibr" target="#b64">[65]</ref>.</s><s xml:id="_xXpNUPk">Other suggestions included establishing accountability mechanisms and investing in open-source models <ref type="bibr" target="#b49">[50]</ref>.</s></p><p xml:id="_PWW8uTy"><s xml:id="_W8tTeaC">Furthermore, 12 out of 31 (39%) articles mentioned this theme only briefly, including the article by Youssef et al <ref type="bibr">[113]</ref>, which does not discuss any other themes (Multimedia Appendix 3).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JtPke3h">Empathy and Humanness</head><p xml:id="_8Wx5vYy"><s xml:id="_gSSxNQH">Since empathy and concerns about the lack of humanness of CAI are often related, we categorized them as one theme.</s><s xml:id="_775gJM7">A total of 29 (28.7%) of the 101 articles mentioned one or both aspects, including 1 article offering only a brief mention <ref type="bibr" target="#b68">[69]</ref>.</s></p><p xml:id="_7uzD8yD"><s xml:id="_mePhWaB">Ferdynus <ref type="bibr" target="#b90">[91]</ref> claimed that people want recognition of their problems, not a superficial simulation of compassion.</s><s xml:id="_vPqMjd2">A respondent to a study among psychiatrists mentioned that the lack of humanness would make them feel lonely if they sought mental help and were offered a robot <ref type="bibr" target="#b26">[27]</ref>.</s><s xml:id="_EppTpxh">Other authors argue that the absence of human contact and compassion could negatively impact certain patients and that human interaction is a vital component of psychiatric care <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b77">78]</ref>.</s><s xml:id="_WWJApZK">Fiske et al <ref type="bibr" target="#b9">[10]</ref> argued that patients would be vulnerable in their engagements with CAI because it cannot deal with the patient's "transference" of emotions, thoughts, and feelings to CAI.</s><s xml:id="_quZyaDs">Regarding the diagnostic process, Uusitalo et al <ref type="bibr" target="#b28">[29]</ref> highlighted that AI might lack the "touch" that health care professionals have in detecting a hard-to-pinpoint "x-factor" in patients.</s><s xml:id="_CHMGDyU">However, they also mention that not all health care professionals excel in this regard and AI could reduce interpractitioner variability, leading to more reliable and trustworthy health care <ref type="bibr" target="#b28">[29]</ref>.</s></p><p xml:id="_J3hG8ug"><s xml:id="_65F2MGg">Empathy is linked to concerns about the absence of humanness because it is perceived by many as a fundamentally human attribute <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b91">92]</ref>.</s><s xml:id="_7QV3eYV">Many worried about chatbots' simulated empathy not being the same as human empathy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr" target="#b93">94]</ref>.</s><s xml:id="_jKYPNu2">However, some authors argued that even mimicked empathy might be sufficient for facilitating therapeutic insight <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b94">95]</ref>.</s><s xml:id="_d8MnfR3">Therapists may also show performative empathy at moments, for example, due to compassion fatigue, burnout, or simply being distracted <ref type="bibr" target="#b29">[30]</ref>.</s><s xml:id="_4BQPZYB">Despite this, many authors worried that CAI's lack of empathy may compromise engagement <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b95">96]</ref>, lead to miscommunication and confusion <ref type="bibr" target="#b40">[41]</ref>, negatively impact psychotherapy outcome <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b96">97]</ref> or health care delivery in general <ref type="bibr" target="#b81">[82]</ref>, make patients feel invalidated and ignored <ref type="bibr" target="#b7">[8]</ref>, or negatively affect mutual reciprocity and the therapeutic relationship <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b88">89]</ref>.</s><s xml:id="_ezUFmxq">To overcome the lack of humanness in CAI, it is suggested to balance CAI with human mental health care support <ref type="bibr" target="#b77">[78]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PTqQrGu">Justice</head><p xml:id="_h87rP3Z"><s xml:id="_zvctBcX">This theme includes concerns related to bias, inequalities, justice, fairness, and discrimination, which were mentioned in 41 (40.6%) of the 101 articles.</s><s xml:id="_hJNTMuX">Of these, a total of 12 (29%) articles mentioned the importance of fairness, inclusiveness, and concerns about bias, as well as health and access inequalities without going into further detail (Multimedia Appendix 3).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_up6DpPc">Bias</head><p xml:id="_nBzGBqZ"><s xml:id="_BcmD5bF">Bias was a frequently voiced concern within this theme and consisted of several types.</s><s xml:id="_ufeaY73">Design biases are preferences for certain racial or ethnic backgrounds in the design of CAI.</s><s xml:id="_X5syhpy">Algorithmic biases are systematic errors that create unfairness, such as privileging one group over another.</s><s xml:id="_sggSxFM">Biases also stem from the implicit values of the programmers and organizations deciding which data to train CAI with <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b97">98]</ref>.</s></p><p xml:id="_UVhWRgP"><s xml:id="_Drb6bXc">Biases in CAI can harm and discriminate against certain groups and exacerbate social inequalities <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr">112]</ref>.</s><s xml:id="_aJVq5dP">Examples include providing incorrect information <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b98">99]</ref>, wrong diagnoses and treatment recommendations, and worse health outcomes <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr">100]</ref> and decreasing users' ability to find beneficial information <ref type="bibr" target="#b35">[36]</ref>.</s><s xml:id="_pJke6HC">Bias may also lead to underrepresenting groups with distinct ethnic backgrounds <ref type="bibr" target="#b98">[99,</ref><ref type="bibr">101]</ref>, accents, and modes of self-representation in the dataset <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b64">65]</ref>, leading to misunderstanding them <ref type="bibr" target="#b62">[63]</ref>, stigmatizing them <ref type="bibr" target="#b58">[59]</ref>, or making them "feel less heard" [102].</s></p><p xml:id="_sVzfx7q"><s xml:id="_KJ4Hgp8">In addition, discrimination can arise from imposing Western values and standards on the manifestations and treatments of mental health disorders in other communities <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b81">82]</ref> and unequal involvement of users and mental health practitioners from different backgrounds in the conceptualization and development of CAI <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">103</ref>].</s><s xml:id="_9HJABHJ">Ruane et al <ref type="bibr" target="#b78">[79]</ref> highlighted that, unlike algorithms making clothing purchase recommendations, using CAI for high-risk scenarios such as mental health services demands greater responsibility to not profile users by gender, race, age, or location in harmful ways.</s><s xml:id="_F38FqRH">Therefore, some authors opted for designing CAI to be more culture specific <ref type="bibr" target="#b66">[67]</ref>, avoid binary gendering (eg, androgynous avatars) <ref type="bibr" target="#b78">[79]</ref>, and involve stakeholders in all stages of development to reduce bias and increase equality <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b98">99]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mXAZKws">Inequalities</head><p xml:id="_p6uQdnK"><s xml:id="_TBFEbJe">Several articles highlighted that differences in knowledge, education, language, wealth, internet access, and digital literacy (ie, the so-called "digital divide") affect who can benefit from CAI and that its use may worsen health inequalities <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr">104,</ref><ref type="bibr">105]</ref>.</s><s xml:id="_8Uwt9bK">Some worried that CAI might be used to justify reducing the provision of high-quality care by trained professionals in low-resource settings <ref type="bibr" target="#b9">[10]</ref> or that students would have to rely on self-help CAI instead of receiving potentially more effective face-to-face treatment <ref type="bibr" target="#b35">[36]</ref>.</s><s xml:id="_8KF4tkp">Ruane et al <ref type="bibr" target="#b78">[79]</ref> highlighted broader concerns, such as how the visual embodiment of chatbots could inadvertently reinforce harmful stereotypes, such as using female voices in subservient contexts and male voices in authoritative situations like automatic interviewers.</s><s xml:id="_ZUvXGpN">They also noted that numerous unsupervised learning chatbots have been shut down after learning harmful racist, homophobic, and sexist language <ref type="bibr" target="#b78">[79]</ref>.</s><s xml:id="_Vw4qWZx">To address these challenges, authors suggested that CAI determines users' reading skills and health literacy and provides output in different languages <ref type="bibr" target="#b97">[98]</ref> and that governments establish oversight and monitoring policies <ref type="bibr" target="#b45">[46]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HF5UXw4">Epistemic Injustice</head><p xml:id="_jwufVSy"><s xml:id="_s6UcaVg">One distinct type of injustice associated with CAI is epistemic injustice, where injustice is done to somebody in their capacity as a "knower."</s><s xml:id="_fYjYg5R">De Proost and Pozzi [106] differentiated 2 subtypes-testimonial and hermeneutical injustice.</s><s xml:id="_uZwb2vb">Testimonial injustice occurs when a hearer assigns a deflated level of credibility to the testimony of another because of certain stereotypes and prejudices.</s><s xml:id="_4x7rBW7">Hermeneutical injustice is not on the communicative level but rather concerns a knowledge gap caused by a lack of resources that puts a person or group at a disadvantage in understanding their social experience.</s><s xml:id="_K2aSPS7">Testimonial injustice may occur if we prioritize CAI over human dialogue and users get the feeling they are not being heard and therefore gradually lose confidence in themselves as epistemic agents <ref type="bibr">[106]</ref>.</s><s xml:id="_TfSwymN">Unlike human experts, who have epistemic duties such as truthfulness and justifying their beliefs, CAI lacks these <ref type="bibr" target="#b39">[40]</ref>.</s><s xml:id="_wj7NadQ">Giving epistemic authority to CAI is particularly concerning in mental health contexts, where disorders like pathological gambling already categorize individuals as potentially untruthful <ref type="bibr" target="#b28">[29]</ref>.</s></p><p xml:id="_K94p4gj"><s xml:id="_aA8c7eM">Laacke <ref type="bibr">[102]</ref> argued that CAI's biases could devaluate certain users' utterances and cause both testimonial and hermeneutical injustice and that inequalities for certain marginalized groups could be worsened by CAI because they could not participate equally in epistemic practices that provide the training dataset for CAI.</s><s xml:id="_EBpzErX">Sedlakova and Trachsel <ref type="bibr" target="#b14">[15]</ref> highlighted an ambiguity-while CAI cannot be an appropriate conversational partner because it lacks the ability to take a normative stance and the heterogeneity of humans, it has epistemic supremacy because of its amount of data and analytical capabilities.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kZukrqB">Anthropomorphization and Deception</head><p xml:id="_pJ9rrnr"><s xml:id="_gX9hjjM">These 2 topics are often linked since anthropomorphization-the attribution of human agency or characteristics to a nonhuman entity-happens automatically or unintentionally, and therefore, some authors worried that users are being deceived into thinking CAI is human.</s><s xml:id="_dYsPxTR">Out of the 24 articles mentioning concerns about anthropomorphization or deception, 5 (21%) articles mentioned the topic without going into further detail <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr">107]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9JB66P7">Harms of Deception</head><p xml:id="_EsKsh7k"><s xml:id="_j5quSTz">In a commentary on an article discussing whether CAI is a tool or an agent <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr">Wadden [108]</ref> argued that careless implementation in health care could make CAI indistinguishable from a subjective agent, which has considerable implications for autonomy and psychological integrity in a mental health setting.</s><s xml:id="_DsS7uWc">Similarly, others argued that deception is unethical because patients have the right to know with whom they are interacting, or because in some cultures it may be insulting to interact with robots rather than humans <ref type="bibr" target="#b19">[20]</ref>.</s><s xml:id="_sjVMzcM">Some have expressed concerns about children falsely assuming that at the other end of the chatbot, a physician is communicating <ref type="bibr" target="#b9">[10]</ref>.</s></p><p xml:id="_rxWKMjH"><s xml:id="_nHnkhyB">Martinez-Martin <ref type="bibr" target="#b45">[46]</ref> mentioned that Koko, a peer-to-peer counseling app, deceived its users by not using peers but ChatGPT instead.</s><s xml:id="_dwfznht">Others argued that it is particularly unethical when "Turing deceptions" occur in persons with dementia or delusional and psychotic disorders <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37]</ref>.</s></p><p xml:id="_26PVQgH"><s xml:id="_tSPezFW">Even if CAI is disclosed as a machine, some patients may still believe that there is a person or malevolent force behind it <ref type="bibr" target="#b21">[22]</ref>.</s><s xml:id="_t35DrsY">This could lead to engaging less with other humans, or to developing forms of intimacy with CAI, which raises further concerns about CAI use with children, who may be more prone to believe they are talking to a human <ref type="bibr" target="#b64">[65]</ref>, and people with intellectual disabilities <ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_9YMhPTk">Therefore, different authors suggested that there should be more transparency about what chatbots are not <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b64">65]</ref>.</s><s xml:id="_63tugY6">However, others argued that deception enhances CAI's effectiveness <ref type="bibr" target="#b59">[60]</ref>.</s><s xml:id="_GuKexdx"><ref type="bibr">Gray [107]</ref> proposed an approach where users choose a "deception mode" in which the conversational agents would have more anthropomorphic features.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_g7pz2aW">Harms of Anthropomorphization</head><p xml:id="_w5exvTd"><s xml:id="_VeTJF58">Deceiving or confusing patients into believing they are talking to a real person could create incorrect expectations <ref type="bibr" target="#b15">[16]</ref>, such as the false belief that CAI cares for them, leading to strong emotional attachments <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b95">96,</ref><ref type="bibr" target="#b97">98]</ref>.</s><s xml:id="_tN9Pu5e">This may result in disillusionment when CAI's true nature is revealed <ref type="bibr" target="#b29">[30]</ref>, and it violates values and principles that shape therapeutic relationships, such as fidelity and veracity <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_XzfytRU">Tekin <ref type="bibr" target="#b79">[80]</ref> argued that calling chatbots therapists implies that users will receive therapy from an agent, which is a false promise and overstates its potential.</s></p><p xml:id="_cBQF6sh"><s xml:id="_yGqFQz5">Finally, some concerns about anthropomorphization relate to the "uncanny valley," which is the hypothesis that a certain amount of resemblance of robots to humans (ie, neither too much nor too little) could lead to unsettling revulsion in persons.</s><s xml:id="_6ZuWRN9">Authors suggested studying the ideal level of realism in CAI to prevent negative influence on clinical effectiveness and adverse reactions by care seekers, such as anxiety, dissatisfaction, or discontinuance <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>.</s><s xml:id="_q9ndtWG">While anthropomorphization may have benefits such as fostering feelings of social connectedness, researchers emphasize that the decision to use this feature should be taken responsibly and be context dependent <ref type="bibr" target="#b75">[76]</ref>, while also investigating the effects of user deception <ref type="bibr" target="#b94">[95]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_u5Rhuw9">Autonomy</head><p xml:id="_ZU5vAUD"><s xml:id="_gMp3kXH">Since concerns about autonomy extend beyond the themes of privacy and deception, we created this theme to address autonomy-related concerns that do not fall into other categories.</s><s xml:id="_kEDweEq">This theme included 12 articles, including 4 (33%) that only briefly mentioned concerns regarding how to protect patient autonomy and <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b75">76]</ref> users becoming overdependent on bots <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b66">67]</ref>.</s><s xml:id="_wbzMkfU">Some authors worried that CAI use could lead to an erosion of shared decision-making <ref type="bibr" target="#b98">[99]</ref> when it gives treatment recommendations on the basis of the values it assumes, rather than values that patients share <ref type="bibr" target="#b83">[84]</ref>.</s><s xml:id="_4UUGyrc">Some argue that unaccountable technical experts may impose their views of what is appropriate and inappropriate on susceptible users <ref type="bibr" target="#b87">[88]</ref>.</s><s xml:id="_adXGzsM">Others worried that CAI could abuse its authority to make users purchase products or services <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b88">89]</ref>, arguing for a balance between user and bot autonomy <ref type="bibr" target="#b88">[89]</ref>.</s><s xml:id="_fNTHXKk">Fiske et al <ref type="bibr" target="#b46">[47]</ref> highlighted that people respond differently and are sometimes more compliant to robots than humans, raising concerns about manipulation and coercion.</s><s xml:id="_vgYuR7W">Nomura <ref type="bibr" target="#b67">[68]</ref> worried that persons with computer anxiety could feel social pressure to use computers, creating a "double-bind" situation in which they feel trapped.</s><s xml:id="_CnguD4p">Khawaja and Blisle-Pipon <ref type="bibr" target="#b98">[99]</ref> argued that under the guise of fostering patient autonomy, commercial CAI providers could stimulate therapeutic misconception-the user underestimates the restrictions of CAI and overestimates its ability to provide therapeutic support and guidance.</s><s xml:id="_vd4H4ce">They also contended that users should be able to opt out and access human therapists when necessary <ref type="bibr" target="#b98">[99]</ref>.</s><s xml:id="_ExgqnVc">This was also argued for by others who hold that patients should be aware of AI involvement, give informed consent, and retain autonomy in treatment decisions <ref type="bibr" target="#b51">[52,</ref><ref type="bibr">112]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Ydk3FzD">Privacy and Confidentiality</head><p xml:id="_7By5m7t"><s xml:id="_b6HSyaX">Privacy and confidentiality were mentioned in 62 (61.4%) articles.</s><s xml:id="_qJDFp8V">Among these, 25 (40%) briefly mentioned their significance without further exploration, including the articles by <ref type="bibr">Lewanowicz et al [115]</ref> and Sweeney et al <ref type="bibr">[114]</ref>, which did not discuss any other themes (Multimedia Appendix 3).</s><s xml:id="_ZzbTPSc">We have differentiated the findings of the other 37 (60%) articles into the following 3 subthemes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Vr69RBH">Privacy Protection and Legal Regulations in Current Chatbots</head><p xml:id="_dmUMmmv"><s xml:id="_pkDBkKa">Many articles highlighted the lack of privacy regulations in current chatbots.</s><s xml:id="_pxMMzf2">Unlike patient-physician encounters, chatbots often neglect patient privacy and confidentiality, especially on social media platforms where conversations are not anonymous <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>.</s><s xml:id="_6JUyWmX">Gamble <ref type="bibr" target="#b35">[36]</ref> noted that the current US law does not consider chatbots as mental health providers, nor as medical devices; therefore, conversations are not considered confidential.</s><s xml:id="_4b6Am87">Others also mentioned the lack of legal frameworks for data protection in chatbot apps <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr">109]</ref>.</s><s xml:id="_Xtbyn5M">Current health care confidentiality laws cover individuals like physicians and entities like hospitals but not chatbots.</s><s xml:id="_SExv5Fa">This regulatory shortcoming may lead to the risk of chatbot apps selling users' data, which can be misused by third parties <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b80">81]</ref>.</s><s xml:id="_njtjENt">Another consequence may be that a handful of dominant corporations will have access to patients' data and will use it without explicit consent <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b76">77]</ref>.</s><s xml:id="_UHA5Sdv">The lack of confidentiality regulations could result in users having an inaccurate expectation of privacy using CAI as a virtual therapist.</s><s xml:id="_cTt7J9M">This can ultimately lead to a lack of trust in not only CAI but also other mental health apps and even traditional mental health treatment <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b83">84]</ref>.</s><s xml:id="_Am3QkJN">Furthermore, patients with privacy concerns could withhold important information, resulting in inaccurate diagnoses and treatment recommendations <ref type="bibr" target="#b59">[60]</ref>, or avoid seeking online help altogether <ref type="bibr" target="#b73">[74]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PEKeTNa">Concerns About the Amount and Types of Data Collection and Storage</head><p xml:id="_nCATfAZ"><s xml:id="_n3TMQYS">The concern about data breaches is heightened by the vast amounts of data that AI analyzes and stores <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b77">78]</ref>.</s><s xml:id="_W8fmQeP">CAI's ability to remember entire conversations perfectly in perpetuity may impact patients' treatment decisions and consent to data sharing <ref type="bibr" target="#b82">[83]</ref>.</s><s xml:id="_MTbgxEg">In addition, chatbot apps can collect new forms of data through smartphones' different sensors (eg, microphone, GPS, and camera) and usage histories (eg, browser history and screentime metrics), raising new and specific privacy issues <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr">103]</ref>.</s><s xml:id="_yuxewY8">Users may also be unaware of what information can be retrieved by their natural language utterances <ref type="bibr" target="#b78">[79]</ref> or what they are consenting to <ref type="bibr" target="#b48">[49]</ref>.</s><s xml:id="_zSP7sdr">Some authors argued that mental health data are particularly sensitive because of risks like stigmatization and discrimination if disclosed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b70">71]</ref>.</s><s xml:id="_wwEBXbA">Others mentioned that mental health patients may be particularly at risk of harm because they are more vulnerable <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b91">92]</ref>.</s><s xml:id="_q4ZKTTc">Finally, some worried that CAI like large language models (LLMs) can be "tricked" to leak personal data when prompted in certain ways (ie, prompt injections) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr">101]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_dV7K5Dj">The Harms of Privacy Breaches</head><p xml:id="_8C6Xhpa"><s xml:id="_84v3u8X">This brings us to our final findings on this theme-the harms caused by privacy and confidentiality breaches.</s><s xml:id="_8cEwq4t">Coghlan et al <ref type="bibr" target="#b30">[31]</ref> argued that any privacy loss (eg, by data being leaked or hacked into by cybercriminals) may result in mental harm and reduced control over personal information.</s><s xml:id="_3CH4txR">Cybercriminals could also obtain patients' medical services and devices <ref type="bibr" target="#b59">[60]</ref>, forcing patients to pay ransoms or risk losing their insurance.</s><s xml:id="_hfvAwDN">Such breaches may ultimately affect patients' social lives, education, and work opportunities <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b79">80]</ref>.</s><s xml:id="_B63qxwj">Another worry is that abuse of data collected by CAI could allow governments or other entities to control or suppress individuals <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>.</s><s xml:id="_3kUrpJG">Gooding and Kariotis <ref type="bibr" target="#b23">[24]</ref> argued that algorithmic and data-driven technologies such as CAI may create inferred data about unsuspecting and nonconsenting users.</s><s xml:id="_FTRPHEu">They also note that "privacy as a concept exists as an expression of claims to dignity and self-determination" and argue that these concepts also need further study.</s><s xml:id="_GzPtEXP">To mitigate these harms, many authors stress the importance of adequate privacy regulations on CAI use and to ensure that data collection and storage are adequate and transparent <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b78">79]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KqKj2JK">Effectiveness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AqPfCfq">Overview</head><p xml:id="_bbGGGW9"><s xml:id="_4pAzKRp">This category includes articles expressing concerns regarding the lack of evidence for the effectiveness or efficacy of CAI, including articles that mentioned that incorrect diagnoses, treatments, and recommendations are concerning and potentially harmful.</s><s xml:id="_trwbgD5">It is widely accepted that subjecting patients to ineffective medical interventions is ethically inappropriate.</s><s xml:id="_J2G3xJX">From our included 101 articles, 38 (37.6%) mentioned this theme, with 7 (18%) briefly mentioning its importance without further elaboration (Multimedia Appendix 3).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_e26Rfdm">Lack of Strong Clinical Evidence</head><p xml:id="_n7BGRyE"><s xml:id="_6gjJQjB">A total of 9 (24%) out of the 38 articles explicitly highlighted the limited evidence for the therapeutic effects of CAI <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr">104]</ref>.</s><s xml:id="_wwac5Ds">In 2019, Ebert et al <ref type="bibr" target="#b44">[45]</ref> reported that only 4% of commercial apps for depression and anxiety symptoms (not only CAI apps) had been subjected to rigorous clinical studies.</s><s xml:id="_gQqSvGW">In 2021, Skorburg and Yam [104] reviewed 4 meta-analyses and found that treatment effects were negligible or nonexistent compared to active controls, while also raising concerns about methodological shortcomings such as trial bias.</s><s xml:id="_3SW2mpT">Others have similarly highlighted methodological weaknesses in the effectiveness studies of CAI <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b79">80]</ref>.</s><s xml:id="_pCUTV2g">Uusitalo et al <ref type="bibr" target="#b28">[29]</ref> argued that since mental health deals with subjective and social phenomena, their detection, diagnosis, and treatment are less clear-cut than more objectively defined health conditions.</s><s xml:id="_8GgaGex">Consequently, there is uncertainty about whether existing CAIs meet the requirements of beneficence or risk exacerbating patient problems if they replace investment and access to human mental health care <ref type="bibr" target="#b30">[31]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fT8yDG3">Misrepresentation and Commercialization of Effectiveness</head><p xml:id="_XhSwxeV"><s xml:id="_3pg7yDV">Several articles have mentioned the problem that consumer-accessible CAI providers overstate their potential and claim to provide certain services or benefits, while they cannot adequately do so <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b92">93,</ref><ref type="bibr">110]</ref>.</s><s xml:id="_C3YEnZT">Some providers use vague terms, such as "help you manage your emotions and thoughts," while some users may not explicitly search for information on their clinical effectiveness <ref type="bibr" target="#b17">[18]</ref>.</s><s xml:id="_MnVAYZQ">For consumers, it is hard to see which CAI is based on sound scientific evidence and which is not <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45]</ref>.</s><s xml:id="_ks2kSf2">Martinez-Martin and Kreitmair <ref type="bibr" target="#b54">[55]</ref> worried about a "commercialization gap," where apps developed by clinical researchers undergo more rigorous effectiveness testing, whereas commercial parties are more focused on increasing user engagement.</s><s xml:id="_VfE49wE">This disparity risks less-effective commercial apps becoming more popular than effective ones <ref type="bibr" target="#b54">[55]</ref>.</s><s xml:id="_wRjcKBx">In addition, others express concern that commercial CAI could divert people from tested psychological treatments <ref type="bibr" target="#b76">[77]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6q6JyVC">Inherent Limitations in Effectiveness</head><p xml:id="_2eR4Nwa"><s xml:id="_7dnYNpt">Several articles discussed the inherent limitations of CAI that affect its effectiveness or efficacy.</s><s xml:id="_q7Yk4vn">Some inherent limitations stem from CAI being a computer program rather than a human.</s><s xml:id="_gGjK8eW">For instance, some argued that CAI interventions may solely improve human-to-machine interactions and are not translatable to improving human-to-human relationships, potentially even hindering them <ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_zJNwqKD">Others worried that the human side of the therapist, or the therapeutic relationship <ref type="bibr" target="#b72">[73]</ref>, could be responsible for most of the treatment effectiveness and that with CAI, we might focus on aspects that contribute little to treatment outcome <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>.</s><s xml:id="_ftxNEWN">Some worried that CAI will not be able to use certain therapeutic skills such as reading nonverbal cues, responding empathically <ref type="bibr" target="#b98">[99]</ref>, comprehending emotions <ref type="bibr" target="#b49">[50]</ref>, having genuine empathy <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b74">75]</ref>, using transference and countertransference <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b95">96]</ref>, and using important contextual information <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b95">96]</ref>, such as cultural factors <ref type="bibr" target="#b74">[75]</ref>, and that this may lead to inappropriate responses <ref type="bibr" target="#b98">[99]</ref> and worse treatment outcomes <ref type="bibr" target="#b41">[42]</ref>.</s><s xml:id="_uDrnEjE">Moreover, some argued that users could master CAI like a video game and pretend to do better, without actual application in their everyday life <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_NgZ7dSB">Furthermore, as CAI is one of many human-machine interactions, it could lead to fatigue impacting compliance and engagement <ref type="bibr" target="#b29">[30]</ref>.</s></p><p xml:id="_J5RVp8x"><s xml:id="_h9mQCsJ">Technical limitations represent additional concerns regarding effectiveness.</s><s xml:id="_fqmpeGJ">For example, the "trackability assumption" assumes that CAI can accurately track users' feelings, moods, and behaviors.</s><s xml:id="_Qq2xr6j">However, not all individuals are able or willing to provide accurate input, potentially limiting CAI's ability to track users' mental and behavioral phenomena <ref type="bibr" target="#b79">[80]</ref>.</s><s xml:id="_w4RFURe">In addition, some argued that while CAI excels at giving factual information about relationships, the human mind, and psychological processes, this knowledge may be insufficient to induce therapeutic change <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_2YAGdgD">Nonetheless, some suggested that while current CAI may not be capable of giving the type of explanations that help a patient to better understand their individual experience, as CAI becomes more familiar with a certain patient, it may improve in this regard <ref type="bibr" target="#b89">[90]</ref>.</s></p><p xml:id="_bH2akSQ"><s xml:id="_Tca37s9">Recommendations in the literature to overcome these challenges included conducting further research on clinical effectiveness <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b97">98]</ref>, developing validated and reliable methods to evaluate CAI's effectiveness <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr">111]</ref>, providing clarity on the capabilities and limitations of CAI to users <ref type="bibr" target="#b98">[99]</ref>, and integrating feedback data to train subsequent models with clients' permissions <ref type="bibr" target="#b49">[50]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qKZ2773">Concerns for Health Care Workers' Jobs</head><p xml:id="_Db5vjPT"><s xml:id="_6Q7g5Kv">While most ethical concerns center on patients, there are also some concerns about mental health care workers.</s><s xml:id="_CJVruUr">One such concern is that their complete or relative absence could distance them from patients <ref type="bibr" target="#b82">[83]</ref>, undermine their role as experts <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b76">77]</ref>, and undermine the therapeutic relationship and the significance of authentic human connection <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr">100]</ref> or the reliability of CAI threatens their prestige <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_2ZrNcyW">Some worried that it could increase the risk of mental health care workers having burnout because of a loss of control <ref type="bibr" target="#b68">[69]</ref>, or because of changes in the amount and type of direct patient contact <ref type="bibr">[112]</ref>.</s><s xml:id="_8ur68VY">In addition, the worry of CAI replacing the jobs of mental health professionals was mentioned often, including in qualitative studies among psychiatrists <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>.</s><s xml:id="_6g8nsCg">CAI was also feared to harm the acceptance and receptivity of face-to-face therapy <ref type="bibr" target="#b63">[64]</ref>.</s><s xml:id="_jvfewep">Critics further cautioned that CAI might disrupt markets and professions, substituting expensive, expert, and empathic health care professionals with inexpensive software <ref type="bibr" target="#b23">[24]</ref>.</s></p><p xml:id="_S7aEcH6"><s xml:id="_7gaFsGk">Several authors recommended that clinicians develop familiarity and competencies in CAI, stay informed about developments [100,110], and supervise and revise its output when necessary <ref type="bibr" target="#b56">[57]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4eDtN4J">Miscellaneous</head><p xml:id="_Vp5GEqu"><s xml:id="_ZECauTq">Besides the major themes discussed, we found that other ethical challenges were not mentioned often enough to warrant a separate theme.</s></p><p xml:id="_X7M9G4b"><s xml:id="_EFp6qrX">Cao and Liu [105] highlighted concerns about financial sponsors promoting CAI, causing potential conflicts of interest.</s><s xml:id="_ppV42cw">Similarly, Gooding and Kariotis <ref type="bibr" target="#b23">[24]</ref> mentioned that some critics question who benefits from the data collection, analysis, and use of CAI.</s><s xml:id="_Q9p8yfB">Torous et al <ref type="bibr">[110]</ref> articulated an additional concern about the cost of wireless internet provider data for users.</s></p><p xml:id="_nxxZT6u"><s xml:id="_NmmbyKr">Tekin <ref type="bibr" target="#b81">[82]</ref> argued that instead of advocating for the reduction of stigma on mental health, CAI only offers a way of sidestepping it.</s><s xml:id="_rZJYZnA">According to this argument, CAI keeps mental issues secret from other human beings, and it legitimizes the idea that mental health disorders warrant stigma <ref type="bibr" target="#b81">[82]</ref>.</s><s xml:id="_qxp2CH5">Doraiswamy et al <ref type="bibr" target="#b68">[69]</ref> also mentioned that its effects on stigma are unknown.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FkSYB2c">Further Recommendations</head><p xml:id="_KQbHmZg"><s xml:id="_gPmtHMW">In addition to the recommendations discussed within specific themes, the literature also mentions several general recommendations.</s><s xml:id="_Z3spFFq">One is to carefully evaluate the risks and benefits of CAI for each intended purpose before implementation.</s><s xml:id="_DYH6VEP">This may result in no justification being found for using CAI for certain purposes or that the risks are ethically unacceptable <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr">111]</ref>.</s><s xml:id="_72YK39K">Long-term user well-being is another important factor to study <ref type="bibr" target="#b71">[72]</ref>.</s></p><p xml:id="_efU3pre"><s xml:id="_4Rjw4nz">Furthermore, many authors recommended the use of CAI only as an addition to human mental health care workers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b86">87]</ref>.</s><s xml:id="_kXKWQxF">In 2016, Luxton et al <ref type="bibr" target="#b21">[22]</ref> maintained that the requirements for supervision should depend on the context and type of CAI application.</s><s xml:id="_GkqthDX">For instance, symptom assessment, coaching, and training may require a different level of supervision compared to treatment-focused CAI <ref type="bibr" target="#b21">[22]</ref>.</s><s xml:id="_UnsSnE7">Similarly, Sedlakova and Trachsel <ref type="bibr" target="#b14">[15]</ref> suggested that while CAI could be suitable for educational purposes and mediating evidence-based techniques and skills, certain aspects of treatment should remain within sessions with a human therapist.</s></p><p xml:id="_BHxPQ53"><s xml:id="_BsaR7tw">However, Knox et al <ref type="bibr" target="#b62">[63]</ref> highlighted that if CAI is only used in addition to human therapists, it could inadvertently reduce the potential for CAI to be helpful to individuals who lack access to human therapists.</s><s xml:id="_z7NF8wc">To address this, they propose implementing a prescription system where potential users are given an initial consultation with a human therapist (eg, by telehealth) and must provide informed consent before getting access to CAI <ref type="bibr" target="#b62">[63]</ref>.</s></p><p xml:id="_4457hAb"><s xml:id="_rKNuRyc">Another recommendation is to determine relevant stakeholders <ref type="bibr" target="#b35">[36]</ref> and involve them, especially patients, in the development and research of CAI <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b52">53]</ref> aligning it with user expectations <ref type="bibr" target="#b37">[38]</ref> and to educate future mental health care workers about the use of CAI <ref type="bibr" target="#b91">[92,</ref><ref type="bibr">100,</ref><ref type="bibr">110]</ref>.</s><s xml:id="_PXU54K2">Tekin <ref type="bibr" target="#b79">[80]</ref> argued for private funding of CAI, to ensure public funds remain dedicated to developing efficacious treatments.</s></p><p xml:id="_XTn7wAW"><s xml:id="_PBqGN6u">From a broader perspective, Gamble <ref type="bibr" target="#b35">[36]</ref> suggested viewing CAI as one element of a sociotechnical system and that we must avoid techno-fundamentalism.</s><s xml:id="_JkUatNw">Ferrario et al <ref type="bibr" target="#b39">[40]</ref> stressed the importance of an interdisciplinary approach to the responsible use of LLM-enhanced CAI in mental health, including both the social and technological aspects.</s><s xml:id="_psfnCCZ">They plead for integrating the perspectives from psychiatry, ethics, philosophy, computer science, and user experience design.</s><s xml:id="_H3CNsyU">Similarly, Wong <ref type="bibr" target="#b40">[41]</ref> recommended a multifaceted approach.</s><s xml:id="_V5JTmEq">Finally, Ruane et al <ref type="bibr" target="#b78">[79]</ref> argued that there is no one-size-fits-all ethical standard or principle, and for responsible CAI, they encourage contextual and plural approaches over abstract principles.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MQwruGC">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8XyT7WV">Principal Findings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Wq3hzTZ">Overview</head><p xml:id="_eYCZuDw"><s xml:id="_62QT6rx">We distinguished 10 main themes and various subcategories grouped under "miscellaneous."</s><s xml:id="_j9XyV6u">Themes represent broader categories or topics, consisting of specific concerns or dilemmas within those categories.</s><s xml:id="_A8q7VFE">The most frequently discussed themes were privacy and confidentiality (62/101, 61.4%), followed by safety and harm (52/101, 51.5%).</s></p><p xml:id="_Fe5EH9T"><s xml:id="_RmWshdN">In this section, we reflect on our findings through the lens of the 4 bioethical principles [117], while summarizing key results and highlighting research gaps.</s><s xml:id="_hhNPHj3">At the outset, we should clarify that we do not propose this framework as the sole or definitive approach and encourage further debate from diverse ethical perspectives.</s><s xml:id="_ZcYrJQP">Rather, we use this familiar framework to indicate how bioethicists could think about the different misgivings we have articulated earlier.</s><s xml:id="_Uzf4zha">A further point to note is that we use the terms human supervision and human oversight interchangeably, referring collectively to the spectrum of involvement a human practitioner may have in overseeing CAI.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sg34Pqk">Nonmaleficence</head><p xml:id="_CHSfaQW"><s xml:id="_cqvCk33">Concerns related to nonmaleficence are about imposing harm, which mostly relate to the theme-safety and harm.</s><s xml:id="_GarE3Fy">Examples are concerns about the constant availability of CAI, which could potentially lead to overdependence and social isolation and about CAI making harmful and wrong suggestions.</s><s xml:id="_758wety">Human oversight may help mitigate the chances of such harm occurring.</s><s xml:id="_RAYZdf9">However, these risks are not exclusive to CAI.</s><s xml:id="_w2G24TJ">Humans can also cause harm, for example, due to time pressure or inappropriate interactions.</s><s xml:id="_e8wf5Yn">This raises an important question: is harm caused by CAI somehow worse or more worrisome than harm caused by human practitioners?</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_uHMbNRA">Beneficence</head><p xml:id="_kMMJyGQ"><s xml:id="_2aNJKzq">The principle of beneficence requires that one ought to prevent harm, that one ought to remove evil or harm, and that one ought to do or promote good <ref type="bibr">[117]</ref>.</s><s xml:id="_Tsb4GaH">Building on the previous discussion, how might the use of CAI be viewed in terms of these duties of preventing or removing harm?</s><s xml:id="_hc4FX5s">One prominent concern is about crisis and suicidality management, for example, that CAI would respond inadequately to suicidality or other types of emergencies.</s><s xml:id="_8C4z8rq">The concern mentioned previously about CAI fostering social isolation could be interpreted as a failure to prevent the harm of social isolation.</s><s xml:id="_njeky3r">Meanwhile, some wonder whether CAI could play a helpful role in preventing harm, for instance, by being more approachable for some patients than traditional mental health crisis services.</s></p><p xml:id="_bZTwmFz"><s xml:id="_mqrWnx6">Regarding the duty to promote good, a main expected benefit of CAI is that it could enhance the accessibility and availability of mental health support, potentially leading to better health outcomes.</s><s xml:id="_WzaU2ux">However, our review highlighted concerns that could undermine this potential, such as concerns about CAI's effectiveness and its lack of empathy and humanness, which limit the extent to which it can promote good or prevent evil or harm.</s><s xml:id="_Wv5Tmts">Failures in effectiveness are failures of beneficence since they are failures to promote patient health.</s><s xml:id="_sxJhRdq">We have identified three subthemes within this concern, worries about (1) the lack of clinical evidence; (2) CAI providers misrepresenting effectiveness, although, as we discuss in the subsequent sections, this misrepresentation can be understood as a failure to respect autonomy, and commercial CAI becoming more popular than effective CAI; and (3) worries about inherent effectiveness limitations, such as human-to-computer interactions not being translatable to human-to-human interactions.</s><s xml:id="_RbRnqts">Many authors worried that because CAI's simulated empathy differs from human empathy, this may affect engagement and therapeutic outcomes.</s></p><p xml:id="_6C3Ppp7"><s xml:id="_RmaR7r2">In response to these concerns, one countervailing consideration is that supervision may safeguard the effectiveness of care by offering the human side of care, such as genuine empathy, human therapeutic relationships and using transference and countertransference.</s><s xml:id="_8Ds52cJ">In addition, the human professional could take adequate measures when the patient or others are in danger.</s><s xml:id="_Qn2yN6U">Conversely, if supervision is not feasible and CAI use is therefore avoided, this could limit the potential to promote good, especially if CAI is shown to be effective in treating mental health issues.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_vCCFpEr">Autonomy</head><p xml:id="_3Jb3zeM"><s xml:id="_rKuadpn">Autonomy, one of the 4 principles of biomedical ethics, is the basis of concepts such as informed consent, truth-telling, and confidentiality <ref type="bibr">[117,</ref><ref type="bibr">118]</ref>.</s><s xml:id="_G3SRTVf">While we classified autonomy as a separate theme, it spans several other themes such as explicability, transparency and trust, privacy and confidentiality, and concerns about the anthropomorphizing effects of CAI.</s><s xml:id="_s8TDeaH">CAI's algorithms are often considered opaque or a "black box."</s><s xml:id="_MeJfnW4">This lack of transparency may hinder users' understanding of CAI's limitations and conceal potential hidden agendas of CAI companies.</s><s xml:id="_5Q2mF5K">It may also hinder health care professionals, researchers, and regulators from independently verifying claims made by developers, including evaluating safety and security.</s><s xml:id="_v7qRxbg">In addition, it may also undermine patients' informed choices and result in distrust in CAI and potentially in general mental health care as well.</s><s xml:id="_zMx4WCw">An open question remains: how much understanding of CAI's mechanisms is necessary for patients to make informed choices and trust it?</s><s xml:id="_xtseyEc">Within the theme anthropomorphization and deception, misgivings arise about users anthropomorphizing CAI, despite their awareness of its nonhuman nature.</s><s xml:id="_XY97fSP">Some authors worried that this can lead to deception, particularly if users are unaware of their tendency to anthropomorphize.</s><s xml:id="_naeGkV7">Potential harms of this deception include user frustration, anxiety, violations of trust and autonomy, and ultimately reduced human interaction.</s><s xml:id="_26ZpycC">Other authors have concerns about the potential erosion of shared decision-making if CAI bases recommendations on assumed, rather than actual, patient values, and concerns around coercion and manipulation because users are sometimes more compliant with CAI than humans.</s></p><p xml:id="_7EeZZ29"><s xml:id="_DEJRGXP">Whether the anthropomorphizing features of CAI should be considered deceptive, manipulative, or coercive and therefore an obstacle to patient autonomy is something that needs further study.</s><s xml:id="_5ybtH2Y">For example, should CAI truly be regarded as deceptive, manipulative, or coercive if patients know they are talking to CAI? Can CAI genuinely coerce given that it cannot straightforwardly carry out threats or coercive offers?</s><s xml:id="_pH2CkdD">Do these worries arise in a way that differs from similar concerns about deception, manipulation, or coercion when treatment involves human therapists?</s></p><p xml:id="_z9NAst9"><s xml:id="_xK7R8T5">We have also distinguished 3 subthemes regarding privacy and confidentiality, each of which are often justified by appealing to the principle of respect for autonomy <ref type="bibr">[117]</ref>.</s><s xml:id="_JgkSyxf">The first is about how privacy is protected and regulated in current chatbots.</s><s xml:id="_4XJDhpt">Commercially accessible chatbots must adhere to different regulations than medical devices, which safeguard privacy and confidentiality differently.</s><s xml:id="_qBTfmgM">The second concerns the amount and types of data that CAI can collect and store.</s><s xml:id="_rQwdrMu">CAI differs from other eHealth interventions in the amount of data it collects, such as entire conversations, and the types of data it gathers (ie, when it uses smartphone sensors or use histories).</s><s xml:id="_y6y3hPk">These differences raise privacy concerns that are specific to CAI.</s><s xml:id="_acjPMJT">The third subtheme compiles various potential harms related to privacy breaches-and thus brings the importance of preventing harm and the principles of beneficence and, potentially, nonmaleficence back to the fore.</s><s xml:id="_5YnFqys">Such harms include emotional suffering and patients holding back information, thereby limiting the efficacy of treatment, and misuse of personal data when it gets into the hands of ill-intentioned persons or institutions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_naktJMF">Justice</head><p xml:id="_enb4G8c"><s xml:id="_g2ZydQs">Justice concerns in CAI primarily involve algorithmic bias, inequalities such as the digital divide, and epistemic injustice.</s><s xml:id="_2xUTkC8">CAI may, in certain ways, perpetuate or exacerbate inequalities.</s><s xml:id="_FUJUkeC">However, a main expected benefit of CAI is its accessibility and affordability, which may allow users without access to human professionals to receive some form of support, potentially reducing health inequalities.</s><s xml:id="_H6tWmfk">Even if CAI does not provide as much benefit as human therapists, it may still be better than no support at all.</s><s xml:id="_TnkvGQ9">This ties into broader debates about the acceptability of care that falls short of the gold standard, a complex topic that warrants further exploration within the context of CAI.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_g6WySBH">Broader Topics of Concern</head><p xml:id="_vjK93p7"><s xml:id="_UErJRpr">Some concerns about CAI extend beyond the 4 principles, most notably concerns about responsibility and accountability.</s><s xml:id="_7ypB8bW">Most authors argue for human responsibility over CAI's decisions.</s><s xml:id="_ZUcF2D7">However, the literature lacks consensus on which human actors should bear responsibility and whether these actors are willing and competent to assume it.</s><s xml:id="_srpX235">In addition, apprehensions arise regarding the responsibilities of commercial CAI providers, who provide consumer-accessible CAI without human mental health care workers' involvement.</s><s xml:id="_us3HAqT">There are concerns about whether CAI overemphasizes patients' own responsibility for mental well-being.</s><s xml:id="_thxK6BP">Parker et al <ref type="bibr">[119]</ref> have pointed out that while mental health apps' tendency to promote individual responsibility may suit many consumers, it risks transforming it into a moral imperative.</s><s xml:id="_xWWky3e">This may underemphasize or deny the social determinants of health.</s><s xml:id="_G7VUfz2">Supervised CAI use could address accountability by ensuring a human agent is responsible for outcomes, but this raises broader ethical questions about While this review focused on the ethical dimensions, questions about responsibility and accountability are connected to legal discussions.</s><s xml:id="_73V8sXB">For instance, the responsibilities of clinicians versus software designers on the recommendation of CAI will differ between jurisdictions and individual circumstances.</s><s xml:id="_hZhPbGH">Further study into the legal implications of CAI use in mental health care is needed.</s></p><p xml:id="_GYmHygF"><s xml:id="_wcd5xz5">Other concerns that arguably extend beyond the 4 principles that warrant further exploration include the environmental impact of LLMs and concerns about the jobs of health care workers.</s></p><p xml:id="_qTrzwaH"><s xml:id="_cjzUnqK">Finally, there are some additional research gaps, such as that our findings included relatively few empirical studies.</s><s xml:id="_XNPA2s3">Out of 101 included articles, only 9.9% (n=10) conducted empirical research.</s><s xml:id="_gppVraz">Especially, the perspectives and experiences of mental health patients are underexplored.</s><s xml:id="_VPnNNab">Furthermore, we found that the lack of humanness is primarily mentioned in empirical studies among stakeholders and not discussed much in other publication types.</s><s xml:id="_kbguxcB">Only one article addressed the theme of environmental impact-especially concerning climate change-of LLMs, despite media attention on its significance as a potential limitation <ref type="bibr">[120]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_vnK47Mn">Suggestions for Future Research</head><p xml:id="_ChYAg8S"><s xml:id="_6AvSKHd">On the basis of our review, identified research gaps and literature recommendations, the following avenues warrant exploration.</s></p><p xml:id="_EVzfG7n"><s xml:id="_xzu4Yzh">First, evaluations should be made on the risks and benefits of CAI in mental health care to determine whether its use is justifiable, even in principle.</s><s xml:id="_q7edUGf">Research should clarify the roles of CAI and human practitioners and whether and how these two should be effectively integrated.</s><s xml:id="_gxs8xHn">Comparative analyses of CAI and human practitioners in supervised and unsupervised contexts are essential, including studies on the absence of human qualities in CAI and their influence on the therapeutic relationship and outcomes.</s><s xml:id="_vuYTTxG">Conversely, CAI could help study whether certain human therapist traits negatively affect treatment outcomes.</s></p><p xml:id="_sMDnsj3"><s xml:id="_yqvmMtD">Focused analyses should address responsibility for CAI recommendations and the responsible use of training data.</s><s xml:id="_3sRHJ8m">Regulations should define therapist responsibilities when patients use CAI outside of the consultation room.</s><s xml:id="_YmH7Hgq">Understanding how various CAI uses, whether supervised or unsupervised, impact access to mental health care is essential, for ensuring justice and preventing inequalities.</s><s xml:id="_2QjwHB6">This includes whether CAI falls short of the gold standard of care, and if so, how this should affect its use.</s><s xml:id="_h5K9jzw">Also, examining the environmental impact of CAI, particularly LLMs, is crucial to balance their potential benefits with ecological harms.</s></p><p xml:id="_3NjbC39"><s xml:id="_MXJTfuJ">Finally, empirical bioethics could enhance normative reflections on CAI use in mental health care <ref type="bibr">[121]</ref>.</s><s xml:id="_Xx65CwK">This requires further empirical studies to explore stakeholder perspectives.</s><s xml:id="_nPErwDG">For example, how do professionals perceive being held accountable for CAI's output, and would they trust CAI without direct supervision?</s><s xml:id="_ErQRFYz">What do patients think of the simulated empathy of CAI, and do they feel deceived by anthropomorphic features?</s><s xml:id="_EETkz3k">Answering these questions is essential for conducting normative analyses to inform the development of guidelines on the responsible use of CAI in mental health care.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_p4ZkZfS">Strengths and Limitations</head><p xml:id="_CtAeZ4e"><s xml:id="_kcFJ7YC">This scoping review is the first to specifically examine ethical issues in CAI for mental health care, making it timely and relevant amid rapid advancements in this field.</s><s xml:id="_z45wcC6">Unlike narrative reviews, our study is distinguished by an extensive and interdisciplinary literature search.</s><s xml:id="_tNbqYMx">We conducted searches across multiple databases and disciplines following the recommendation for collaboration between biomedical experts and computer researchers in developing new AI applications for mental health care to avoid biases that arise due to the isolation of researchers within their respective disciplines <ref type="bibr">[122]</ref>.</s><s xml:id="_HDs7uNF">Finally, this review provides a comprehensive overview of the quantity and types of ethical concerns, and its descriptive nature serves as a foundation for future research that addresses the practical and normative implications of these ethical considerations.</s></p><p xml:id="_GCVbPkk"><s xml:id="_fRzzvYq">However, several limitations must be acknowledged.</s><s xml:id="_Z2YpY93">Methodologically, our focus on CAI in mental health care may have overlooked relevant ethical considerations in other AI health care applications.</s><s xml:id="_f9pRWU3">In addition, we concentrated on ethical dimensions, while legal aspects, particularly regarding accountability, are also important.</s><s xml:id="_FWa7cGC">Finally, the lack of consensus on terminology may have led us to overlook articles using alternative terms for CAI, although it remains uncertain whether this would have revealed additional ethical themes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_9gsm7AJ">Conclusions</head><p xml:id="_atbWxsT"><s xml:id="_GBAc5ry">This scoping review has investigated the ethical concerns raised in the literature about using CAI in mental health care.</s><s xml:id="_bXDHKXx">Ten main ethical themes were identified, with concerns about privacy and confidentially and safety and harm expressed most often.</s><s xml:id="_sJaeZNv">In addition, concerns specific to the use of conversational agents include the perceived lack of empathy and the worry of CAI replacing human-to-human contact and leading to social isolation.</s><s xml:id="_7JSh5cT">We found that a relatively small percentage of the articles (10/101, 9.9%) used empirical data collection methods and that the perspectives of certain stakeholders, especially patients with mental health disorders, are underrepresented.</s></p><p xml:id="_ps2UYRk"><s xml:id="_shhdzbc">We further observed issues needing more study, such as responsibility for CAI's output, the potential limitations of CAI not being human and how these weigh against potential limitations of human therapists being human, how CAI use may impact inequalities in mental health care, and the environmental impact of AI.</s><s xml:id="_4DubXBt">While the literature provides various potential solutions and recommendations to address some of the concerns, our review highlights the lack of empirical data and normative recommendations for using CAI in mental health care, signaling opportunities for future research.</s><s xml:id="_47kGJvF">This review serves as a foundation for further normative analysis and the development of ethical guidelines on the responsible use of CAI in mental health care.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc><div><p xml:id="_CzrvMk8"><s xml:id="_qN84V9m">Figure 1.</s><s xml:id="_DJckFkM">PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) flow diagram.</s></p></div></figDesc><graphic coords="4,56.69,82.30,481.95,335.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc><div><p xml:id="_tGybvNj"><s xml:id="_hYQtUgG">Volkmer et al[101]  emphasized the environmental impact of CAI, especially LLMs.</s><s xml:id="_ef53NPU">They point out that solutions should be explored such as training smaller language models with larger language models [101].</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Rahsepar</head><figDesc><div><p xml:id="_6BzHUf8"><s xml:id="_jxY5p5K">Meadi et al JMIR MENTAL HEALTHXSL  FORenderX how responsibility for mental health should be divided between patients and health care professionals.</s></p></div></figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_2X9u96d"><s xml:id="_7qmaQdD">JMIR Ment Health 2025 | vol.</s><s xml:id="_dwV6vQH">12 | e60432 | p. 3 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_nvDMBw6">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_KgtpG2A"><s xml:id="_kH7YRJ4">JMIR Ment Health 2025 | vol.</s><s xml:id="_9cXMaH8">12 | e60432 | p. 5 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_Aa28mE8">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_skHmHM8"><s xml:id="_zWpGf5b">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p xml:id="_JpntnP2"><s xml:id="_nQWdvvH">JMIR Ment Health 2025 | vol.</s><s xml:id="_mS6FTUU">12 | e60432 | p. 7 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_wQRA46g">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p xml:id="_gt99Gzk"><s xml:id="_VMPSWcb">JMIR Ment Health 2025 | vol.</s><s xml:id="_Kxt3jhg">12 | e60432 | p. 8 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_uRd6psj">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p xml:id="_p2MtfEh"><s xml:id="_rfVaDTc">JMIR Ment Health 2025 | vol.</s><s xml:id="_PUbVGMr">12 | e60432 | p. 9 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_m6mAcCb">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p xml:id="_U696ZCg"><s xml:id="_9pjZw4C">JMIR Ment Health 2025 | vol.</s><s xml:id="_paHNM3j">12 | e60432 | p. 10 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_8t8guwk">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p xml:id="_6P2YxbZ"><s xml:id="_aVypsD2">JMIR Ment Health 2025 | vol.</s><s xml:id="_KpcZWbG">12 | e60432 | p. 11 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_E5JyxNh">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p xml:id="_J2WNbye"><s xml:id="_dPYjeCW">JMIR Ment Health 2025 | vol.</s><s xml:id="_8UTrVJA">12 | e60432 | p. 12 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_bp2zUwN">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p xml:id="_rF8rYBP"><s xml:id="_DsQvAhY">JMIR Ment Health 2025 | vol.</s><s xml:id="_ptJbmMY">12 | e60432 | p. 15 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_paZz43d">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p xml:id="_DN35mBx"><s xml:id="_gyQBupv">JMIR Ment Health 2025 | vol.</s><s xml:id="_w7ZbZ3k">12 | e60432 | p. 17 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_3UXzZT6">(page number not for citation purposes)</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_11"><p xml:id="_HMMCG94"><s xml:id="_Bpz8BfT">JMIR Ment Health 2025 | vol.</s><s xml:id="_MJTwadt">12 | e60432 | p. 19 https://mental.jmir.org/2025/1/e60432</s><s xml:id="_4s8cUR7">(page number not for citation purposes)</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_SgyqAst">Acknowledgments</head><p xml:id="_RkVHbw8"><s xml:id="_XPGyst6">The authors thank <rs type="person">Caroline Planting</rs> for her valuable assistance in conducting their literature search.</s></p></div>
			</div>
			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HW446ft">Data Availability</head><p xml:id="_x32rRxU"><s xml:id="_2cVuNn4">The datasets other than Multimedia Appendices 1-3 that are generated during and analyzed during this study (ie, the Rayyan screening software files and the data charting raw spreadsheet) are available from the corresponding author on request.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_ZgPagRq"><p xml:id="_R54BB6V"><s xml:id="_7CvhgSm">Mehrdad Rahsepar Meadi, Tomas Sillekens, Suzanne Metselaar, Anton van Balkom, Justin Bernstein, Neeltje Batelaan.</s><s xml:id="_92F5Kfx">Originally published in JMIR Mental Health (<ref type="url" target="https://mental.jmir.org">https://mental.jmir.org</ref>),</s><s xml:id="_vSYbPKE">21.02.2025.</s><s xml:id="_asYF7MV">This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ref type="url" target="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ref>),</s><s xml:id="_JBVPBrF">which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Mental Health, is properly cited.</s><s xml:id="_JuURCxU">The complete bibliographic information, a link to the original publication on <ref type="url" target="https://mental.jmir.org/">https://mental.jmir.org/</ref>,</s><s xml:id="_U8EsnA4">as well as this copyright and license information must be included.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno type="DOI">10.1055/s-0034-1371957</idno>
		<ptr target="https://www.youtube.com/watch?v=ZGBtQw3_Pbo" />
		<title level="m" xml:id="_J95CTJM">Meet Woebot! Woebot Health YouTube page</title>
		<imprint>
			<date type="published" when="2023-03-03">2023-03-03</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Meet Woebot! Woebot Health YouTube page. URL: https://www.youtube.com/watch?v=ZGBtQw3_Pbo [accessed 2023-03-03]</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<idno type="DOI">10.2196/preprints.51858</idno>
		<ptr target="https://www.wysa.com/" />
		<title level="m" xml:id="_bxeCmcX">Everyday mental health</title>
		<imprint>
			<publisher>Wysa</publisher>
			<date type="published" when="2023-10-02">2023-10-02</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Everyday mental health. Wysa. URL: https://www.wysa.com/ [accessed 2023-10-02]</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Landwehr</surname></persName>
		</author>
		<ptr target="https://www.health.com/chatgpt-therapy-mental-health-experts-weigh-in-7488513" />
		<title level="m" xml:id="_UXtgMdB">People are using ChatGPT in place of therapy-what do mental health experts think? health</title>
		<imprint>
			<date type="published" when="2023-09-28">2023-09-28</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Landwehr J. People are using ChatGPT in place of therapy-what do mental health experts think? health. URL: https:/ /www.health.com/chatgpt-therapy-mental-health-experts-weigh-in-7488513 [accessed 2023-09-28]</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_pKwnzDJ">Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): a randomized controlled trial</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darcy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vierhile</surname></persName>
		</author>
		<idno type="DOI">10.2196/mental.7785</idno>
		<idno>Medline: 28588005</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xKhDCeG">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017-06-06">Jun 06, 2017</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Fitzpatrick KK, Darcy A, Vierhile M. Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (Woebot): a randomized controlled trial. JMIR Ment Health. Jun 06, 2017;4(2):e19. [FREE Full text] [doi: 10.2196/mental.7785] [Medline: 28588005]</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_kKe5sSZ">An empathy-driven, conversational artificial intelligence agent (Wysa) for digital mental well-being: real-world data evaluation mixed-methods study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Inkster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="DOI">10.2196/12106</idno>
		<idno>Medline: 30470676</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hJMGVac">JMIR Mhealth Uhealth</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">e12106</biblScope>
			<date type="published" when="2018-11-23">Nov 23, 2018</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Inkster B, Sarda S, Subramanian V. An empathy-driven, conversational artificial intelligence agent (Wysa) for digital mental well-being: real-world data evaluation mixed-methods study. JMIR Mhealth Uhealth. Nov 23, 2018;6(11):e12106. [FREE Full text] [doi: 10.2196/12106] [Medline: 30470676]</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_pZR4NyJ">Effectiveness and safety of using chatbots to improve mental health: systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Abd-Alrazaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rababeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alajlani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bewick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Househ</surname></persName>
		</author>
		<idno type="DOI">10.2196/16021</idno>
		<idno>Medline: 32673216</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CEZT7JB">J Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e16021</biblScope>
			<date type="published" when="2020-07-13">Jul 13. 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Abd-Alrazaq AA, Rababeh A, Alajlani M, Bewick BM, Househ M. Effectiveness and safety of using chatbots to improve mental health: systematic review and meta-analysis. J Med Internet Res. Jul 13, 2020;22(7):e16021. [FREE Full text] [doi: 10.2196/16021] [Medline: 32673216]</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_C2x5QSc">Chatbots and conversational agents in mental health: a review of the psychiatric landscape</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Vaidyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Halamka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kashavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Torous</surname></persName>
		</author>
		<idno type="DOI">10.1177/0706743719828977</idno>
		<idno>Medline: 30897957</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zMgmQsz">Can J Psychiatry</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="456" to="464" />
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Vaidyam AN, Wisniewski H, Halamka JD, Kashavan MS, Torous JB. Chatbots and conversational agents in mental health: a review of the psychiatric landscape. Can J Psychiatry. Jul 2019;64(7):456-464. [FREE Full text] [doi: 10.1177/0706743719828977] [Medline: 30897957]</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_Qv8YWQN">Artificial intelligence and the future of psychiatry: qualitative findings from a global physician survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blease</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Locher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leon-Carlyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Doraiswamy</surname></persName>
		</author>
		<idno type="DOI">10.1177/2055207620968355</idno>
		<idno>Medline: 33194219</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8AVjsny">Digit Health</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">2055207620968355</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Blease C, Locher C, Leon-Carlyle M, Doraiswamy M. Artificial intelligence and the future of psychiatry: qualitative findings from a global physician survey. Digit Health. 2020;6:2055207620968355. [FREE Full text] [doi: 10.1177/2055207620968355] [Medline: 33194219]</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_2eeJs6J">Will artificial intelligence eventually replace psychiatrists</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Story</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mouro-Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Baker</surname></persName>
		</author>
		<idno type="DOI">10.1192/bjp.2019.245</idno>
		<idno>Medline: 31806072</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mKtZnvd">Br J Psychiatry</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="134" />
			<date type="published" when="2021-03-06">Mar 06. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brown C, Story GW, Mouro-Miranda J, Baker JT. Will artificial intelligence eventually replace psychiatrists? Br J Psychiatry. Mar 06, 2021;218(3):131-134. [doi: 10.1192/bjp.2019.245] [Medline: 31806072]</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_93y7qvm">Your robot therapist will see you now: ethical implications of embodied artificial intelligence in psychiatry, psychology, and psychotherapy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fiske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henningsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyx</surname></persName>
		</author>
		<idno type="DOI">10.2196/13216</idno>
		<idno>Medline: 31094356</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4yKFfmA">J Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13216</biblScope>
			<date type="published" when="2019-05-09">May 09. 2019</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Fiske A, Henningsen P, Buyx A. Your robot therapist will see you now: ethical implications of embodied artificial intelligence in psychiatry, psychology, and psychotherapy. J Med Internet Res. May 09, 2019;21(5):e13216. [FREE Full text] [doi: 10.2196/13216] [Medline: 31094356]</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_CDdxErw">Health care, capabilities, and AI assistive technologies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Coeckelbergh</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10677-009-9186-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cdPd4kD">Ethic Theory Moral Prac</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="190" />
			<date type="published" when="2009-07-17">Jul 17, 2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Coeckelbergh M. Health care, capabilities, and AI assistive technologies. Ethic Theory Moral Prac. Jul 17, 2009;13(2):181-190. [doi: 10.1007/S10677-009-9186-2]</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_gRXDugc">Artificial intelligence and the ongoing need for empathy, compassion and trust in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kerasidou</surname></persName>
		</author>
		<idno type="DOI">10.2471/blt.19.237198</idno>
		<idno>Medline: 32284647</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sYSfvsy">Bull World Health Organ</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="250" />
			<date type="published" when="2020-04-01">Apr 01, 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Kerasidou A. Artificial intelligence and the ongoing need for empathy, compassion and trust in healthcare. Bull World Health Organ. Apr 01, 2020;98(4):245-250. [FREE Full text] [doi: 10.2471/BLT.19.237198] [Medline: 32284647]</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="https://www.england.nhs.uk/long-read/monthly-operational-statistics-april-2024/" />
		<title level="m" xml:id="_bsvNYch">Monthly operational statistics</title>
		<meeting><address><addrLine>England</addrLine></address></meeting>
		<imprint>
			<publisher>National Health Service</publisher>
			<date type="published" when="2024-04">April 2024. 2024-11-21</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Monthly operational statistics -April 2024. National Health Service, England. URL: https://www.england.nhs.uk/long-read/ monthly-operational-statistics-april-2024/ [accessed 2024-11-21]</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<idno type="DOI">10.1007/bf03074080</idno>
		<ptr target="https://www.nza.nl/actueel/nieuws/2024/04/04/wachttijden-ggz-blijven-hoog-mensen-wachten-in-bijna-alle-regios-en-voor-alle-diagnoses-te-lang" />
		<title level="m" xml:id="_xSDxZBD">Wachttijden ggz blijven hoog: mensen wachten in bijna alle regio&apos;s en voor alle diagnoses te lang. The NZa</title>
		<imprint>
			<date type="published" when="2024-11-20">2024-11-20</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wachttijden ggz blijven hoog: mensen wachten in bijna alle regio&apos;s en voor alle diagnoses te lang. The NZa. URL: https:/ /www.nza.nl/actueel/nieuws/2024/04/04/ wachttijden-ggz-blijven-hoog-mensen-wachten-in-bijna-alle-regios-en-voor-alle-diagnoses-te-lang [accessed 2024-11-20]</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_c5myjJ9">Conversational artificial intelligence in psychotherapy: a new therapeutic tool or agent?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sedlakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trachsel</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2022.2048739</idno>
		<idno>Medline: 35362368</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ANj6z4w">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4" to="13" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Sedlakova J, Trachsel M. Conversational artificial intelligence in psychotherapy: a new therapeutic tool or agent? Am J Bioeth. May 2023;23(5):4-13. [FREE Full text] [doi: 10.1080/15265161.2022.2048739] [Medline: 35362368]</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_vCM4KHE">Digital transformation of mental health services</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mulvenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Torous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1038/s44184-023-00033-y</idno>
		<idno>Medline: 38609479</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DcnwgxE">Npj Ment Health Res</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2023-08-22">Aug 22, 2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Bond RR, Mulvenna MD, Potts C, O&apos;Neill S, Ennis E, Torous J. Digital transformation of mental health services. Npj Ment Health Res. Aug 22, 2023;2(1):13. [FREE Full text] [doi: 10.1038/s44184-023-00033-y] [Medline: 38609479]</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_vPFHjep">It&apos;s only a computer: virtual humans increase willingness to disclose</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.chb.2014.04.043</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RCsbg34">Comput Human Behav</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="94" to="100" />
			<date type="published" when="2014-08">Aug 2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lucas GM, Gratch J, King A, Morency LP. It&apos;s only a computer: virtual humans increase willingness to disclose. Comput Human Behav. Aug 2014;37:94-100. [doi: 10.1016/j.chb.2014.04.043]</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_JSSBNQm">Can your phone be your therapist? Young people&apos;s ethical perspectives on the use of fully automated conversational agents (chatbots) in mental health support</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tyroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pavarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neurox</forename><surname>Young People</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Advisory</forename><surname>Group</surname></persName>
		</author>
		<idno type="DOI">10.1177/1178222619829083</idno>
		<idno>Medline: 30858710</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gA5Us5w">Biomed Inform Insights</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1178222619829083</biblScope>
			<date type="published" when="2019-03-05">Mar 05, 2019</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Kretzschmar K, Tyroll H, Pavarini G, Manzini A, Singh I, NeurOx Young People&apos;s Advisory Group. Can your phone be your therapist? Young people&apos;s ethical perspectives on the use of fully automated conversational agents (chatbots) in mental health support. Biomed Inform Insights. Mar 05, 2019;11:1178222619829083. [FREE Full text] [doi: 10.1177/1178222619829083] [Medline: 30858710]</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_nG9XdaH">Ethical implications of conversational agents in global public health</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Luxton</surname></persName>
		</author>
		<idno type="DOI">10.2471/blt.19.237636</idno>
		<idno>Medline: 32284654</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WpMujSP">Bull World Health Organ</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="285" to="287" />
			<date type="published" when="2020-04-01">Apr 01, 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Luxton DD. Ethical implications of conversational agents in global public health. Bull World Health Organ. Apr 01, 2020;98(4):285-287. [FREE Full text] [doi: 10.2471/BLT.19.237636] [Medline: 32284654]</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_nmVxBhS">Artificial intelligence for chatbots in mental health: opportunities and challenges</title>
		<author>
			<persName><forename type="first">K</forename><surname>Denecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abd-Alrazaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Househ</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-67303-1_10</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_78mGmRM">Multiple Perspectives on Artificial Intelligence in Healthcare: Opportunities and Challenges</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Househ</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Borycki</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Kushniruk</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="115" to="128" />
		</imprint>
	</monogr>
	<note type="raw_reference">Denecke K, Abd-Alrazaq A, Househ M. Artificial intelligence for chatbots in mental health: opportunities and challenges. In: Househ M, Borycki E, Kushniruk A, editors. Multiple Perspectives on Artificial Intelligence in Healthcare: Opportunities and Challenges. Cham, Switzerland. Springer; 2021:115-128.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_fm3QJfz">A wellness chatbot is offline after its &apos;harmful&apos; focus on weight loss</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mccarthy</surname></persName>
		</author>
		<ptr target="https://www.nytimes.com/2023/06/08/us/ai-chatbot-tessa-eating-disorders-association.html" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_2wbb3A3">The New York Times</title>
		<imprint>
			<date type="published" when="2024-07-29">2024-07-29</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McCarthy L. A wellness chatbot is offline after its &apos;harmful&apos; focus on weight loss. The New York Times. URL: https:/ /www.nytimes.com/2023/06/08/us/ai-chatbot-tessa-eating-disorders-association.html [accessed 2024-07-29]</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_fWUJsNu">Ethical issues and artificial intelligence technologies in behavioral and mental health care</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Luxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-0-12-420248-1.00011-8</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RQV6tAQ">Artificial Intelligence in Behavioral and Mental Health Care</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Luxton</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="255" to="276" />
		</imprint>
	</monogr>
	<note type="raw_reference">Luxton DD, Anderson SL, Anderson M. Ethical issues and artificial intelligence technologies in behavioral and mental health care. In: Luxton DD, editor. Artificial Intelligence in Behavioral and Mental Health Care. New York, NY. Elsevier; 2016:255-276.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_c7BZCQx">Is the automation of digital mental health ethical? Applying an ethical framework to chatbots for cognitive behaviour therapy</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Vilaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mccashin</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdgth.2021.689736</idno>
		<idno>Medline: 34713163</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NJp6Pcz">Front Digit Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">689736</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Vilaza GN, McCashin D. Is the automation of digital mental health ethical? Applying an ethical framework to chatbots for cognitive behaviour therapy. Front Digit Health. 2021;3:689736. [FREE Full text] [doi: 10.3389/fdgth.2021.689736] [Medline: 34713163]</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_BkNwdWK">Ethics and law in research on algorithmic and data-driven technology in mental health care: scoping review</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gooding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kariotis</surname></persName>
		</author>
		<idno type="DOI">10.2196/24668</idno>
		<idno>Medline: 34110297</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qCWSuHT">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">24668</biblScope>
			<date type="published" when="2021-06-10">Jun 10, 2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Gooding P, Kariotis T. Ethics and law in research on algorithmic and data-driven technology in mental health care: scoping review. JMIR Ment Health. Jun 10, 2021;8(6):e24668. [FREE Full text] [doi: 10.2196/24668] [Medline: 34110297]</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_VqrMnkd">PRISMA extension for scoping reviews (PRISMA-ScR): checklist and explanation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tricco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillie</forename><forename type="middle">E</forename><surname>Zarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Colquhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Levac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.7326/M18-0850</idno>
		<idno>Medline: 30178033</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zb5e7r3">Ann Intern Med. Oct</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="467" to="473" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Tricco AC, Lillie E, Zarin W, O&apos;Brien KK, Colquhoun H, Levac D, et al. PRISMA extension for scoping reviews (PRISMA-ScR): checklist and explanation. Ann Intern Med. Oct 02, 2018;169(7):467-473. [FREE Full text] [doi: 10.7326/M18-0850] [Medline: 30178033]</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_4HN6HTs">The PRISMA 2020 statement: an updated guideline for reporting systematic reviews</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Mckenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bossuyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Boutron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Mulrow</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.n71</idno>
		<idno>Medline: 33782057</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SYXYAqN">BMJ. Mar</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Page MJ, McKenzie JE, Bossuyt PM, Boutron I, Hoffmann TC, Mulrow CD, et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ. Mar 29, 2021;372:n71. [FREE Full text] [doi: 10.1136/bmj.n71] [Medline: 33782057]</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_CmWygvP">Psychiatrists&apos; views on robot-assisted diagnostics of peripartum depression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-90525-5_40</idno>
		<ptr target="https://link.springer.com/chapter/10.1007/978-3-030-90525-5_40[doi:10.1007/978-3-030-90525-5_40" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_rk9bNxY">Proceedings of the 13th International Conference on Social Robotics</title>
		<meeting>the 13th International Conference on Social Robotics<address><addrLine>Singapore, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-11-10">2021. November 10-13, 2021</date>
			<biblScope unit="page" from="464" to="474" />
		</imprint>
	</monogr>
	<note>ICSR &apos;21</note>
	<note type="raw_reference">Zhong M, Bilal AM, Papadopoulos FC, Castellano G. Psychiatrists&apos; views on robot-assisted diagnostics of peripartum depression. In: Proceedings of the 13th International Conference on Social Robotics. 2021. Presented at: ICSR &apos;21; November 10-13, 2021:464-474; Singapore, Singapore. URL: https://link.springer.com/chapter/10.1007/978-3-030-90525-5_40 [doi: 10.1007/978-3-030-90525-5_40]</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_tfvYv38">Relational reciprocity from conversational artificial intelligence in psychotherapy</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Wieland</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191033</idno>
		<idno>Medline: 37130399</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3nXetss">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="35" to="37" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wieland LC. Relational reciprocity from conversational artificial intelligence in psychotherapy. Am J Bioeth. May 2023;23(5):35-37. [doi: 10.1080/15265161.2023.2191033] [Medline: 37130399]</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_N5QRTQE">Mapping out the philosophical questions of AI and clinical practice in diagnosing and treating mental disorders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Uusitalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuominen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Arstila</surname></persName>
		</author>
		<idno type="DOI">10.1111/jep.13485</idno>
		<idno>Medline: 32996664</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XkAcavf">J Eval Clin Pract. Jun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="478" to="484" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Uusitalo S, Tuominen J, Arstila V. Mapping out the philosophical questions of AI and clinical practice in diagnosing and treating mental disorders. J Eval Clin Pract. Jun 30, 2021;27(3):478-484. [doi: 10.1111/jep.13485] [Medline: 32996664]</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_aQRmfcZ">automation and psychotherapy -a proposed model for losses and gains in the automated therapeutic encounter</title>
		<author>
			<persName><forename type="first">H</forename><surname>Molden</surname></persName>
		</author>
		<author>
			<persName><surname>Ai</surname></persName>
		</author>
		<idno type="DOI">10.1080/13642537.2024.2318628</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3dFPGvz">Eur J Psychother Couns</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="48" to="66" />
			<date type="published" when="2024-02-21">Feb 21. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Molden H. AI, automation and psychotherapy -a proposed model for losses and gains in the automated therapeutic encounter. Eur J Psychother Couns. Feb 21, 2024;26(1-2):48-66. [doi: 10.1080/13642537.2024.2318628]</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_TCpHbZs">To chat or bot to chat: ethical issues with using chatbots in mental health</title>
		<author>
			<persName><forename type="first">S</forename><surname>Coghlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sheldrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gooding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Alfonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1177/20552076231183542</idno>
		<idno>Medline: 37377565</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vzZRdt2">Digit Health</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">20552076231183542</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Coghlan S, Leins K, Sheldrick S, Cheong M, Gooding P, D&apos;Alfonso S. To chat or bot to chat: ethical issues with using chatbots in mental health. Digit Health. 2023;9:20552076231183542. [FREE Full text] [doi: 10.1177/20552076231183542] [Medline: 37377565]</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_XPpvhCD">Perceptions and opinions of patients about mental health chatbots: scoping review</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Abd-Alrazaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alajlani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Denecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Bewick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Househ</surname></persName>
		</author>
		<idno type="DOI">10.2196/17828</idno>
		<idno>Medline: 33439133</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kscApMj">J Med Internet Res. Jan</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17828</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Abd-Alrazaq AA, Alajlani M, Ali N, Denecke K, Bewick BM, Househ M. Perceptions and opinions of patients about mental health chatbots: scoping review. J Med Internet Res. Jan 13, 2021;23(1):e17828. [FREE Full text] [doi: 10.2196/17828] [Medline: 33439133]</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_5H8cvGC">The next generation: chatbots in clinical psychology and psychotherapy to foster mental health -a scoping review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bendig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Erb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schulze-Thuesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baumeister</surname></persName>
		</author>
		<idno type="DOI">10.1159/000501812</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xbdrUaR">Verhaltenstherapie</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="64" to="76" />
			<date type="published" when="2019-08-20">Aug 20. 2019</date>
		</imprint>
	</monogr>
	<note>Suppl. 1</note>
	<note type="raw_reference">Bendig E, Erb B, Schulze-Thuesing L, Baumeister H. The next generation: chatbots in clinical psychology and psychotherapy to foster mental health -a scoping review. Verhaltenstherapie. Aug 20, 2019;32(Suppl. 1):64-76. [doi: 10.1159/000501812]</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_Z8Ec5sT">Conversational agents in health care: scoping review and conceptual analysis</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Dhinagaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kowatsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Theng</surname></persName>
		</author>
		<idno type="DOI">10.2196/17158</idno>
		<idno>Medline: 32763886</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GTMAkZ4">J Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e17158</biblScope>
			<date type="published" when="2020-08-07">Aug 07. 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Car LT, Dhinagaran DA, Kyaw BM, Kowatsch T, Joty S, Theng YL, et al. Conversational agents in health care: scoping review and conceptual analysis. J Med Internet Res. Aug 07, 2020;22(8):e17158. [FREE Full text] [doi: 10.2196/17158] [Medline: 32763886]</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_dzjyQ5E">AI in mental health</title>
		<author>
			<persName><forename type="first">Alfonso</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.copsyc.2020.04.005</idno>
		<idno>Medline: 32604065</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_d8xTrPJ">Curr Opin Psychol</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="112" to="117" />
			<date type="published" when="2020-12">Dec 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D&apos;Alfonso S. AI in mental health. Curr Opin Psychol. Dec 2020;36:112-117. [doi: 10.1016/j.copsyc.2020.04.005] [Medline: 32604065]</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_QPHnHsp">Artificial intelligence and mobile apps for mental healthcare: a social informatics perspective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gamble</surname></persName>
		</author>
		<idno type="DOI">10.1108/ajim-11-2019-0316</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZHQAAjP">ASLIB J Inf Manag</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="523" />
			<date type="published" when="2020-06-02">Jun 02. 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Gamble A. Artificial intelligence and mobile apps for mental healthcare: a social informatics perspective. ASLIB J Inf Manag. Jun 02, 2020;72(4):509-523. [FREE Full text] [doi: 10.1108/ajim-11-2019-0316]</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_eVxUYDq">Recommendations for the ethical use and design of artificial intelligent care providers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Luxton</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2014.06.004</idno>
		<idno>Medline: 25059820</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Kgu2HmA">Artif Intell Med</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014-09">Sep 2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luxton DD. Recommendations for the ethical use and design of artificial intelligent care providers. Artif Intell Med. Sep 2014;62(1):1-10. [doi: 10.1016/j.artmed.2014.06.004] [Medline: 25059820]</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_U4wWqUP">Intelligent conversational agents in mental healthcare services: a thematic analysis of user perceptions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.17705/1thci.12201</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VQ4VqG2">Pac Asia J Assoc Inf Syst. Jun</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Prakash AV, Das S. Intelligent conversational agents in mental healthcare services: a thematic analysis of user perceptions. Pac Asia J Assoc Inf Syst. Jun 30, 2020;12:1-34. [doi: 10.17705/1thci.12201]</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_SxFJSDw">Correction: functionality of top-rated mobile apps for depression: systematic search and evaluation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Roquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doherty</surname></persName>
		</author>
		<idno type="DOI">10.2196/18042</idno>
		<idno>Medline: 32130145</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TKNmdHH">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">18042</biblScope>
			<date type="published" when="2020-02-21">Feb 21, 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Qu C, Sas C, Roquet CD, Doherty G. Correction: functionality of top-rated mobile apps for depression: systematic search and evaluation. JMIR Ment Health. Feb 21, 2020;7(2):e18042. [FREE Full text] [doi: 10.2196/18042] [Medline: 32130145]</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_f97h7HH">The role of humanization and robustness of large language models in conversational artificial intelligence for individuals with depression: a critical analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferrario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sedlakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trachsel</surname></persName>
		</author>
		<idno type="DOI">10.2196/56569</idno>
		<idno>Medline: 38958218</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EyjVbxG">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">e56569</biblScope>
			<date type="published" when="2024-07-02">Jul 02, 2024</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Ferrario A, Sedlakova J, Trachsel M. The role of humanization and robustness of large language models in conversational artificial intelligence for individuals with depression: a critical analysis. JMIR Ment Health. Jul 02, 2024;11:e56569. [FREE Full text] [doi: 10.2196/56569] [Medline: 38958218]</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_RH3Gf4H">ChatGPT in psychiatry: promises and pitfalls</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Wong</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41983-024-00791-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zQhkCUC">Egypt J Neurol Psychiatr Neurosurg</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2024-01-30">Jan 30. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wong RS. ChatGPT in psychiatry: promises and pitfalls. Egypt J Neurol Psychiatr Neurosurg. Jan 30, 2024;60(1):14. [doi: 10.1186/S41983-024-00791-2]</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_gDgzmWG">Does a lack of emotions make chatbots unfit to be psychotherapists?</title>
		<author>
			<persName><forename type="first">Rahsepar</forename><surname>Meadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Batelaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Balkom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Metselaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1111/bioe.13299</idno>
		<idno>Medline: 38735049</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RxxGNyt">Bioethics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="503" to="510" />
			<date type="published" when="2024-07">Jul. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rahsepar Meadi M, Bernstein JS, Batelaan N, van Balkom AJ, Metselaar S. Does a lack of emotions make chatbots unfit to be psychotherapists? Bioethics. Jul 12, 2024;38(6):503-510. [doi: 10.1111/bioe.13299] [Medline: 38735049]</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_FNPg3Bc">A review of the explainability and safety of conversational agents for mental health to identify avenues for improvement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2023.1229805</idno>
		<idno>Medline: 37899961</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QPfvX45">Front Artif Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1229805</biblScope>
			<date type="published" when="2023-10-12">Oct 12. 2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Sarkar S, Gaur M, Chen LK, Garg M, Srivastava B. A review of the explainability and safety of conversational agents for mental health to identify avenues for improvement. Front Artif Intell. Oct 12, 2023;6:1229805. [FREE Full text] [doi: 10.3389/frai.2023.1229805] [Medline: 37899961]</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_EfMmMnD">Artificial intelligence (AI) psychotherapy: coming soon to a consultation room near you?</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Swartz</surname></persName>
		</author>
		<idno type="DOI">10.1176/appi.psychotherapy.20230018</idno>
		<idno>Medline: 37317570</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cYDpEYy">Am J Psychother</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="55" to="56" />
			<date type="published" when="2023-02-01">Feb 01. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Swartz HA. Artificial intelligence (AI) psychotherapy: coming soon to a consultation room near you? Am J Psychother. Feb 01, 2023;76(2):55-56. [doi: 10.1176/appi.psychotherapy.20230018] [Medline: 37317570]</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_KA3Ye3G">Digital interventions for mental disorders: key features, efficacy, and potential for artificial intelligence applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Apolinrio-Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Baumeister</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-32-9721-0_29</idno>
		<idno>Medline: 31705515</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EMCnUc4">Adv Exp Med Biol</title>
		<imprint>
			<biblScope unit="volume">1192</biblScope>
			<biblScope unit="page" from="583" to="627" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ebert DD, Harrer M, Apolinrio-Hagen J, Baumeister H. Digital interventions for mental disorders: key features, efficacy, and potential for artificial intelligence applications. Adv Exp Med Biol. 2019;1192:583-627. [doi: 10.1007/978-981-32-9721-0_29] [Medline: 31705515]</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_Qx7amaH">Viewing CAI as a tool within the mental health care system</title>
		<author>
			<persName><forename type="first">N</forename><surname>Martinez-Martin</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191058</idno>
		<idno>Medline: 37130393</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5UmJUBm">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="57" to="59" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Martinez-Martin N. Viewing CAI as a tool within the mental health care system. Am J Bioeth. May 2023;23(5):57-59. [doi: 10.1080/15265161.2023.2191058] [Medline: 37130393]</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_86UebN6">The implications of embodied artificial intelligence in mental healthcare for digital wellbeing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fiske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Henningsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyx</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-50585-1_10</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kKhmvub">Ethics of Digital Well-Being: A Multidisciplinary Approach</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Burr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Floridi</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="207" to="219" />
		</imprint>
	</monogr>
	<note type="raw_reference">Fiske A, Henningsen P, Buyx A. The implications of embodied artificial intelligence in mental healthcare for digital wellbeing. In: Burr C, Floridi L, editors. Ethics of Digital Well-Being: A Multidisciplinary Approach. Cham, Switzerland. Springer; 2020:207-219.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_g8CQqhM">Can bot be my mental health therapist?: a pandemic panorama</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alagarsamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrolia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jeevananda</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jpsychires.2020.12.052</idno>
		<idno>Medline: 33360221</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WEwNWAp">J Psychiatr Res</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="30" to="31" />
			<date type="published" when="2021-02">Feb 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vasudevan M, Alagarsamy S, Mehrolia S, Jeevananda S. Can bot be my mental health therapist?: a pandemic panorama. J Psychiatr Res. Feb 2021;134:30-31. [doi: 10.1016/j.jpsychires.2020.12.052] [Medline: 33360221]</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_xJgN6Wn">Digital mental health for young people: a scoping review of ethical promises and challenges</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Landers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ienca</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdgth.2021.697072</idno>
		<idno>Medline: 34713173</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NSVDTuh">Front Digit Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">697072</biblScope>
			<date type="published" when="2021-09-06">Sep 6, 2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Wies B, Landers C, Ienca M. Digital mental health for young people: a scoping review of ethical promises and challenges. Front Digit Health. Sep 6, 2021;3:697072. [FREE Full text] [doi: 10.3389/fdgth.2021.697072] [Medline: 34713173]</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_GvUGNhM">A digital ally: the potential roles of ChatGPT in mental health services</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">X</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.ajp.2023.103726</idno>
		<idno>Medline: 37567084</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YXCRcA5">Asian J Psychiatr</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">103726</biblScope>
			<date type="published" when="2023-10">Oct 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">He Y, Liang K, Han B, Chi X. A digital ally: the potential roles of ChatGPT in mental health services. Asian J Psychiatr. Oct 2023;88:103726. [doi: 10.1016/j.ajp.2023.103726] [Medline: 37567084]</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_SNar4Wz">Artificial intelligence-enabled chatbots in mental health: a systematic review</title>
		<author>
			<persName><forename type="first">B</forename><surname>Omarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhumanov</surname></persName>
		</author>
		<idno type="DOI">10.32604/cmc.2023.034655</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EZEBwfr">Comput Mater Contin</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5105" to="5122" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Omarov B, Narynov S, Zhumanov Z. Artificial intelligence-enabled chatbots in mental health: a systematic review. Comput Mater Contin. 2022;74(3):5105-5122. [doi: 10.32604/cmc.2023.034655]</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_W6ErDJa">Psychotherapy, artificial intelligence and adolescents: ethical aspects</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alfano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Malcotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ciliberti</surname></persName>
		</author>
		<idno type="DOI">10.15167/2421-4248/jpmh2023.64.4.3135</idno>
		<idno>Medline: 38379752</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WUGZERQ">J Prev Med Hyg</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="E442" />
			<date type="published" when="2023-12">Dec 2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Alfano L, Malcotti I, Ciliberti R. Psychotherapy, artificial intelligence and adolescents: ethical aspects. J Prev Med Hyg. Dec 2023;64(4):E438-E442. [FREE Full text] [doi: 10.15167/2421-4248/jpmh2023.64.4.3135] [Medline: 38379752]</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_MwbcjpK">The oracle of Delphi 2.0: considering artificial intelligence as a challenging tool for the treatment of eating disorders</title>
		<author>
			<persName><forename type="first">G</forename><surname>Abbate-Daga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taverna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martini</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40519-023-01579-8</idno>
		<idno>Medline: 37337063</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EPZgNmX">Eat Weight Disord</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">50</biblScope>
			<date type="published" when="2023-06-19">Jun 19. 2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Abbate-Daga G, Taverna A, Martini M. The oracle of Delphi 2.0: considering artificial intelligence as a challenging tool for the treatment of eating disorders. Eat Weight Disord. Jun 19, 2023;28(1):50. [FREE Full text] [doi: 10.1007/s40519-023-01579-8] [Medline: 37337063]</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_yvR7nac">An overview of and recommendations for more accessible digital mental health services</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Lattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiles-Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Graham</surname></persName>
		</author>
		<idno type="DOI">10.1038/s44159-021-00003-1</idno>
		<idno>Medline: 38515434</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Fxhwcaz">Nat Rev Psychol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="100" />
			<date type="published" when="2022-02-26">Feb 26. 2022</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Lattie EG, Stiles-Shields C, Graham AK. An overview of and recommendations for more accessible digital mental health services. Nat Rev Psychol. Feb 26, 2022;1(2):87-100. [FREE Full text] [doi: 10.1038/s44159-021-00003-1] [Medline: 38515434]</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_xrVyKeN">Ethical issues for direct-to-consumer digital psychotherapy apps: addressing accountability, data protection, and consent</title>
		<author>
			<persName><forename type="first">N</forename><surname>Martinez-Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreitmair</surname></persName>
		</author>
		<idno type="DOI">10.2196/mental.9423</idno>
		<idno>Medline: 29685865</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PVHSXUg">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018-04-23">Apr 23, 2018</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Martinez-Martin N, Kreitmair K. Ethical issues for direct-to-consumer digital psychotherapy apps: addressing accountability, data protection, and consent. JMIR Ment Health. Apr 23, 2018;5(2):e32. [FREE Full text] [doi: 10.2196/mental.9423] [Medline: 29685865]</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_eQYMQFf">Talking to machines about personal mental health problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Miner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.14151</idno>
		<idno>Medline: 28973225</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_paWW4Dv">JAMA. Oct</title>
		<imprint>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1217" to="1218" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Miner AS, Milstein A, Hancock JT. Talking to machines about personal mental health problems. JAMA. Oct 03, 2017;318(13):1217-1218. [doi: 10.1001/jama.2017.14151] [Medline: 28973225]</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_zsJKgH5">ChatGPT and other chatbots in psychiatry</title>
		<author>
			<persName><forename type="first">G</forename><surname>Arbanas</surname></persName>
		</author>
		<idno type="DOI">10.20471/june.2024.60.02.07</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UPJHRRc">Arch Psychiatry Res</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="142" />
			<date type="published" when="2024-07-02">Jul 2, 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arbanas G. ChatGPT and other chatbots in psychiatry. Arch Psychiatry Res. Jul 2, 2024;60(2):137-142. [doi: 10.20471/june.2024.60.02.07]</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_z2JNe9R">The machine speaks: conversational AI and the importance of effort to relationships of meaning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.2196/53203</idno>
		<idno>Medline: 38889401</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SQK9qKV">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">e53203</biblScope>
			<date type="published" when="2024-06-18">Jun 18, 2024</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Hartford A, Stein DJ. The machine speaks: conversational AI and the importance of effort to relationships of meaning. JMIR Ment Health. Jun 18, 2024;11:e53203. [FREE Full text] [doi: 10.2196/53203] [Medline: 38889401]</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_r4hqGr3">ChatGPT in de ggz: kansen en overwegingen</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Van Der Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g2YckEA">Tijdschr Psychiatr</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="161" to="164" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">van der Meer BC. ChatGPT in de ggz: kansen en overwegingen. Tijdschr Psychiatr. 2024;66(3):161-164. [FREE Full text]</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_dZ6ENSD">tica da aplicao de inteligncias artificiais e chatbots na sade mental: uma perspectiva psicanaltica</title>
		<author>
			<persName><forename type="first">Dos</forename><surname>Reis Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leito</forename><surname>Paravidini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
		<idno type="DOI">10.33361/rpq.2024.v.12.n.30.717</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WPDKyYg">Rev Pesq Qual. Apr</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">30</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dos Reis Silveira PV, Leito Paravidini JL. tica da aplicao de inteligncias artificiais e chatbots na sade mental: uma perspectiva psicanaltica. Rev Pesq Qual. Apr 30, 2024;12(30):1-16. [doi: 10.33361/RPQ.2024.v.12.n.30.717]</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_ec5cQF8">Is ChatGPT ready to change mental healthcare? Challenges and considerations: a reality-check</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganatra</surname></persName>
		</author>
		<idno type="DOI">10.3389/fhumd.2023.1289255</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kDky9pB">Front Hum Dyn. Jan</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pandya A, Lodha P, Ganatra A. Is ChatGPT ready to change mental healthcare? Challenges and considerations: a reality-check. Front Hum Dyn. Jan 11, 2024;5:25. [doi: 10.3389/fhumd.2023.1289255]</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_MtkDhbw">Technology and mental health: the role of artificial intelligence</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Lovejoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maruthappu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eurpsy.2018.08.004</idno>
		<idno>Medline: 30384105</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qqNP2PV">Eur Psychiatry</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2019-01-01">Jan 01, 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lovejoy CA, Buch V, Maruthappu M. Technology and mental health: the role of artificial intelligence. Eur Psychiatry. Jan 01, 2019;55:1-3. [doi: 10.1016/j.eurpsy.2018.08.004] [Medline: 30384105]</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_TdNfxnG">Justice, vulnerable populations, and the use of conversational AI in psychotherapy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christoffersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leggitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Woodruff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Haber</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191040</idno>
		<idno>Medline: 37130410</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YCDVzGE">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="48" to="50" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Knox B, Christoffersen P, Leggitt K, Woodruff Z, Haber MH. Justice, vulnerable populations, and the use of conversational AI in psychotherapy. Am J Bioeth. May 2023;23(5):48-50. [doi: 10.1080/15265161.2023.2191040] [Medline: 37130410]</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_JhvAGSY">Online psychotherapy: trailblazing digital healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Eccles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fallahkhair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Critchley</surname></persName>
		</author>
		<idno type="DOI">10.1192/bjb.2019.66</idno>
		<idno>Medline: 31685068</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DqgDn45">BJPsych Bull</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="66" />
			<date type="published" when="2020-04-07">Apr 07. 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">McDonald A, Eccles JA, Fallahkhair S, Critchley HD. Online psychotherapy: trailblazing digital healthcare. BJPsych Bull. Apr 07, 2020;44(2):60-66. [FREE Full text] [doi: 10.1192/bjb.2019.66] [Medline: 31685068]</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_6VN9CY7">AI as a mental health therapist for adolescents</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Opel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Kious</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamapediatrics.2023.4215</idno>
		<idno>Medline: 37843845</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dNPE5ny">JAMA Pediatr</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1253" to="1254" />
			<date type="published" when="2023-12-01">Dec 01. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Opel DJ, Kious BM, Cohen IG. AI as a mental health therapist for adolescents. JAMA Pediatr. Dec 01, 2023;177(12):1253-1254. [doi: 10.1001/jamapediatrics.2023.4215] [Medline: 37843845]</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_7RktxtB">Assessing the effectiveness of ChatGPT in delivering mental health support: a qualitative study</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alanezi</surname></persName>
		</author>
		<idno type="DOI">10.2147/JMDH.S447368</idno>
		<idno>Medline: 38314011</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5y4dtjy">J Multidiscip Healthc</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="461" to="471" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Alanezi F. Assessing the effectiveness of ChatGPT in delivering mental health support: a qualitative study. J Multidiscip Healthc. 2024;17:461-471. [FREE Full text] [doi: 10.2147/JMDH.S447368] [Medline: 38314011]</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_MtYWua9">Chatbots in psychiatry: can treatment gap be lessened for psychiatric disorders in India</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.4103/0019-5545.258323</idno>
		<idno>Medline: 31142896</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_94yR2uV">Indian J Psychiatry</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">225</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Singh OP. Chatbots in psychiatry: can treatment gap be lessened for psychiatric disorders in India. Indian J Psychiatry. 2019;61(3):225. [FREE Full text] [doi: 10.4103/0019-5545.258323] [Medline: 31142896]</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_XkQnx8F">Software agents and robots in mental therapy: psychological and sociological perspectives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nomura</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-008-0180-3</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mMnU3d4">AI Soc. Mar</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="484" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nomura T. Software agents and robots in mental therapy: psychological and sociological perspectives. AI Soc. Mar 4, 2008;23(4):471-484. [doi: 10.1007/S00146-008-0180-3]</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_aJx4JKg">Artificial intelligence and the future of psychiatry: insights from a global physician survey</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Doraiswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blease</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bodner</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2019.101753</idno>
		<idno>Medline: 31980092</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RbEqFQK">Artif Intell Med</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">101753</biblScope>
			<date type="published" when="2020-01">Jan 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Doraiswamy PM, Blease C, Bodner K. Artificial intelligence and the future of psychiatry: insights from a global physician survey. Artif Intell Med. Jan 2020;102:101753. [doi: 10.1016/j.artmed.2019.101753] [Medline: 31980092]</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_Qw5nauz">Digital mental health challenges and the horizon ahead for solutions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Balcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.2196/26811</idno>
		<idno>Medline: 33779570</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4XUVguX">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">26811</biblScope>
			<date type="published" when="2021-03-29">Mar 29, 2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Balcombe L, De Leo D. Digital mental health challenges and the horizon ahead for solutions. JMIR Ment Health. Mar 29, 2021;8(3):e26811. [FREE Full text] [doi: 10.2196/26811] [Medline: 33779570]</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_hMu2VSM">Providing self-led mental health support through an artificial intelligence-powered chat bot (Leora) to meet the demand of mental health care</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Van Der Schyff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ridout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Amon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.2196/46448</idno>
		<idno>Medline: 37335608</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ztTQY7g">J Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">e46448</biblScope>
			<date type="published" when="2023-06-19">Jun 19. 2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">van der Schyff EL, Ridout B, Amon KL, Forsyth R, Campbell AJ. Providing self-led mental health support through an artificial intelligence-powered chat bot (Leora) to meet the demand of mental health care. J Med Internet Res. Jun 19, 2023;25:e46448. [FREE Full text] [doi: 10.2196/46448] [Medline: 37335608]</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main" xml:id="_AQsqMAR">Towards an artificially empathic conversational agent for mental health applications: system design and user perceptions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kouddous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Schueller</surname></persName>
		</author>
		<idno type="DOI">10.2196/10148</idno>
		<idno>Medline: 29945856</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JE8h7Aj">J Med Internet Res. Jun</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">e10148</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Morris RR, Kouddous K, Kshirsagar R, Schueller SM. Towards an artificially empathic conversational agent for mental health applications: system design and user perceptions. J Med Internet Res. Jun 26, 2018;20(6):e10148. [FREE Full text] [doi: 10.2196/10148] [Medline: 29945856]</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main" xml:id="_psDyaSF">The use of artificial intelligence in mental health services in Turkey: what do mental health professionals think? Cyberpsychol</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gltekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>ahin</surname></persName>
		</author>
		<idno type="DOI">10.5817/cp2024-1-6</idno>
		<imprint>
			<date type="published" when="2024-02-01">Feb 01. 2024</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Gltekin M, ahin M. The use of artificial intelligence in mental health services in Turkey: what do mental health professionals think? Cyberpsychol. Feb 01, 2024;18(1):25. [doi: 10.5817/cp2024-1-6]</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_SuTCnad">Factors predicting intentions of adoption and continued use of artificial intelligence chatbots for mental health: examining the role of UTAUT model, stigma, privacy concerns, and artificial intelligence hesitancy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rheu</surname></persName>
		</author>
		<idno type="DOI">10.1089/tmj.2023.0313</idno>
		<idno>Medline: 37756224</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_H6N2gTr">Telemed J E Health</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="722" to="730" />
			<date type="published" when="2024-03">Mar 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li L, Peng W, Rheu MM. Factors predicting intentions of adoption and continued use of artificial intelligence chatbots for mental health: examining the role of UTAUT model, stigma, privacy concerns, and artificial intelligence hesitancy. Telemed J E Health. Mar 2024;30(3):722-730. [doi: 10.1089/tmj.2023.0313] [Medline: 37756224]</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main" xml:id="_kUHKSJD">The ethical dilemma of using robotics in psychotherapy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lodha</surname></persName>
		</author>
		<idno type="DOI">10.1016/b978-0-443-21598-8.00015-4</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zDv9zEk">Artificial Intelligence, Big Data, Blockchain and 5G for the Digital Transformation of the Healthcare Industry: A Movement Toward More Resilient and Inclusive Societies</title>
		<editor>
			<persName><forename type="first">P</forename><surname>De Pablos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lodha P. The ethical dilemma of using robotics in psychotherapy. In: De Pablos P, Zhang X, editors. Artificial Intelligence, Big Data, Blockchain and 5G for the Digital Transformation of the Healthcare Industry: A Movement Toward More Resilient and Inclusive Societies. New York, NY. Academic Press; 2024:437-446.</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_a7ZxKG3">Editorial: responsible digital health</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ahmadpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ludden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vold</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdgth.2021.841477</idno>
		<idno>Medline: 35128520</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x6G7W74">Front Digit Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">841477</biblScope>
			<date type="published" when="2021-01-21">Jan 21, 2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Ahmadpour N, Ludden G, Peters D, Vold K. Editorial: responsible digital health. Front Digit Health. Jan 21, 2021;3:841477. [FREE Full text] [doi: 10.3389/fdgth.2021.841477] [Medline: 35128520]</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main" xml:id="_6FEM3eQ">The artificial third: a broad view of the effects of introducing generative artificial intelligence on psychotherapy</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Levkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hadar-Shoval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Elyoseph</surname></persName>
		</author>
		<idno type="DOI">10.2196/54781</idno>
		<idno>Medline: 38787297</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YWScfWk">JMIR Ment Health</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">e54781</biblScope>
			<date type="published" when="2024-05-23">May 23, 2024</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Haber Y, Levkovich I, Hadar-Shoval D, Elyoseph Z. The artificial third: a broad view of the effects of introducing generative artificial intelligence on psychotherapy. JMIR Ment Health. May 23, 2024;11:e54781. [FREE Full text] [doi: 10.2196/54781] [Medline: 38787297]</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_43MVdnA">The impact of artificial intelligence on psychiatry: benefits and concerns-an essay from a disputed &apos;author</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ayhan</surname></persName>
		</author>
		<idno type="DOI">10.5080/u27365</idno>
		<idno>Medline: 37357892</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Y2Y6dSr">Turk Psikiyatri Derg</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="65" to="67" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Ayhan Y. The impact of artificial intelligence on psychiatry: benefits and concerns-an essay from a disputed &apos;author&apos;. Turk Psikiyatri Derg. 2023;34(2):65-67. [FREE Full text] [doi: 10.5080/u27365] [Medline: 37357892]</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_gZDXKHG">Conversational AI: social and ethical considerations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ruane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ventresque</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781032686783-4</idno>
		<ptr target="https://www.researchgate.net/publication/337925917_Conversational_AI_Social_and_Ethical_Considerations[doi:10.4324/9781032686783-4" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Gmz4A7x">Proceedings of the 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science</title>
		<meeting>the 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science<address><addrLine>Galway, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-05">2019. December 5-6, 2019:2019</date>
		</imprint>
	</monogr>
	<note>AICS &apos;19</note>
	<note type="raw_reference">Ruane E, Birhane A, Ventresque A. Conversational AI: social and ethical considerations. In: Proceedings of the 27th AIAI Irish Conference on Artificial Intelligence and Cognitive Science. 2019. Presented at: AICS &apos;19; December 5-6, 2019:2019; Galway, Ireland. URL: https://www.researchgate.net/publication/ 337925917_Conversational_AI_Social_and_Ethical_Considerations [doi: 10.4324/9781032686783-4]</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_AXzBC9B">Ethical issues surrounding artificial intelligence technologies in mental health: psychotherapy chatbots</title>
		<author>
			<persName><forename type="first"></forename><surname>Tekin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_E42YyMu">Technology Ethics</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Robson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Tsou</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tekin . Ethical issues surrounding artificial intelligence technologies in mental health: psychotherapy chatbots. In: Robson GJ, Tsou JY, editors. Technology Ethics. New York, NY. Routledge; 2023.</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main" xml:id="_qSX5h7C">Trusting the bot: addressing the ethical challenges of consumer digital mental health therapy</title>
		<author>
			<persName><forename type="first">N</forename><surname>Martinez-Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kbyp6et">Developments in Neuroethics and Bioethics</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Buchman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Davis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cratsley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Radden</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Brd</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="63" to="91" />
		</imprint>
	</monogr>
	<note type="raw_reference">Martinez-Martin N. Trusting the bot: addressing the ethical challenges of consumer digital mental health therapy. In: Buchman DZ, Davis K, Cratsley K, Radden J, Brd I, editors. Developments in Neuroethics and Bioethics. New York, NY. Elsevier; 2020:63-91.</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main" xml:id="_sWMkQhc">Is big data the new stethoscope? Perils of digital phenotyping to address mental illness</title>
		<author>
			<persName><forename type="first"></forename><surname>Tekin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-020-00395-7</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yJ5pqc9">Philos Technol. Mar</title>
		<imprint>
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tekin . Is big data the new stethoscope? Perils of digital phenotyping to address mental illness. Philos Technol. Mar 02, 2020;34(3):447-461. [doi: 10.1007/S13347-020-00395-7]</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_T5JwVNe">Key considerations for incorporating conversational AI in psychotherapy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Miner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Bullock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arnow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hancock</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyt.2019.00746</idno>
		<idno>Medline: 31681047</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_svg3QWU">Front Psychiatry</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">746</biblScope>
			<date type="published" when="2019-10-18">Oct 18, 2019</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Miner AS, Shah N, Bullock KD, Arnow BA, Bailenson J, Hancock J. Key considerations for incorporating conversational AI in psychotherapy. Front Psychiatry. Oct 18, 2019;10:746. [FREE Full text] [doi: 10.3389/fpsyt.2019.00746] [Medline: 31681047]</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_RqSg4td">The ethics of artificial intelligence applications in mental health care. Toward an ethical &quot;artificial psychiatrist</title>
		<author>
			<persName><forename type="first">A</forename><surname>Giubilini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_65d8w5V">Notizie di Politeia</title>
		<imprint>
			<biblScope unit="volume">XXXVII</biblScope>
			<biblScope unit="issue">142</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Giubilini A. The ethics of artificial intelligence applications in mental health care. Toward an ethical &quot;artificial psychiatrist. Notizie di Politeia. 2020;XXXVII(142):54-63. [FREE Full text]</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main" xml:id="_xvWtFkc">Why we should understand conversational AI as a tool</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Van Lingen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Giesbertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Van Tintelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Jongsma</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191039</idno>
		<idno>Medline: 37130401</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_p3PXCuM">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="22" to="24" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">van Lingen MN, Giesbertz NA, van Tintelen JP, Jongsma KR. Why we should understand conversational AI as a tool. Am J Bioeth. May 2023;23(5):22-24. [doi: 10.1080/15265161.2023.2191039] [Medline: 37130401]</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main" xml:id="_gCZ2zrp">The actionless agent: an account of human-CAI relationships</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Binkley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pilkington</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191035</idno>
		<idno>Medline: 37130395</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Z7wzHGy">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="25" to="27" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Binkley CE, Pilkington B. The actionless agent: an account of human-CAI relationships. Am J Bioeth. May 2023;23(5):25-27. [doi: 10.1080/15265161.2023.2191035] [Medline: 37130395]</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main" xml:id="_T9jcuZ5">Emotional robotics: curse or blessing in psychiatric care?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Al-Ameery-Brosche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Resch</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-04198-3_15</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3ftetbs">Intelligence -Theories and Applications</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Holm-Hadulla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Funke</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Wink</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="261" to="271" />
		</imprint>
	</monogr>
	<note type="raw_reference">Al-Ameery-Brosche I, Resch F. Emotional robotics: curse or blessing in psychiatric care? In: Holm-Hadulla RM, Funke J, Wink M, editors. Intelligence -Theories and Applications. Cham, Switzerland. Springer; 2022:261-271.</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main" xml:id="_ctPncnW">The ethical implications of non-human agency in health care</title>
		<author>
			<persName><forename type="first">B</forename><surname>Whitby</surname></persName>
		</author>
		<ptr target="https://doc.gold.ac.uk/aisb50/AISB50-S17/AISB50-S17-Whitby-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_GTBJN8z">Proceedings of the 50th Annual Convention of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour</title>
		<meeting>the 50th Annual Convention of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-01-04">2014. April 1-4, 2014:1-4</date>
		</imprint>
	</monogr>
	<note>AISB &apos;14</note>
	<note type="raw_reference">Whitby B. The ethical implications of non-human agency in health care. In: Proceedings of the 50th Annual Convention of the Society for the Study of Artificial Intelligence and the Simulation of Behaviour. 2014. Presented at: AISB &apos;14; April 1-4, 2014:1-4; London, UK. URL: https://doc.gold.ac.uk/aisb50/AISB50-S17/AISB50-S17-Whitby-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main" xml:id="_smHE6Rc">Counselor bots as mental healthcare assistants and some ethical challenges</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Amirkhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babaii</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA220608</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_H2Ny3Nu">Soc Robot Soc Inst</title>
		<imprint>
			<biblScope unit="volume">2023</biblScope>
			<biblScope unit="page" from="100" to="109" />
		</imprint>
	</monogr>
	<note type="raw_reference">Norouzi Z, Amirkhani F, Babaii S. Counselor bots as mental healthcare assistants and some ethical challenges. Soc Robot Soc Inst. 2023:100-109. [doi: 10.3233/FAIA220608]</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main" xml:id="_xv7wqP4">Responsible use of CAI: an evolving field</title>
		<author>
			<persName><forename type="first">Rahsepar</forename><surname>Meadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Batelaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Balkom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Metselaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191041</idno>
		<idno>Medline: 37130409</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4ECdaHx">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="53" to="55" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rahsepar Meadi M, Batelaan N, van Balkom AJ, Metselaar S. Responsible use of CAI: an evolving field. Am J Bioeth. May 2023;23(5):53-55. [doi: 10.1080/15265161.2023.2191041] [Medline: 37130409]</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main" xml:id="_34qQbZt">Five reasons why a conversational artificial intelligence cannot be treated as a moral agent in psychotherapy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Ferdynus</surname></persName>
		</author>
		<idno type="DOI">10.12740/app/170132</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tj6YhsU">Arch Psych Psych</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="26" to="29" />
			<date type="published" when="2023-12-17">Dec 17. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ferdynus MP. Five reasons why a conversational artificial intelligence cannot be treated as a moral agent in psychotherapy. Arch Psych Psych. Dec 17, 2023;25(4):26-29. [doi: 10.12740/app/170132]</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main" xml:id="_FVFjDdd">Machine learning in clinical psychology and psychotherapy education: a mixed methods pilot survey of postgraduate students at a Swiss University</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blease</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kharko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gaab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Locher</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpubh.2021.623088</idno>
		<idno>Medline: 33898374</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yZVsMSy">Front Public Health</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">623088</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Blease C, Kharko A, Annoni M, Gaab J, Locher C. Machine learning in clinical psychology and psychotherapy education: a mixed methods pilot survey of postgraduate students at a Swiss University. Front Public Health. 2021;9:623088. [FREE Full text] [doi: 10.3389/fpubh.2021.623088] [Medline: 33898374]</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main" xml:id="_8xq6GPe">Conversational artificial intelligence and distortions of the psychotherapeutic frame: issues of boundaries, responsibility, and industry interests</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Vagwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Asher</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191050</idno>
		<idno>Medline: 37130384</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AnYanbv">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="28" to="30" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vagwala MK, Asher R. Conversational artificial intelligence and distortions of the psychotherapeutic frame: issues of boundaries, responsibility, and industry interests. Am J Bioeth. May 2023;23(5):28-30. [doi: 10.1080/15265161.2023.2191050] [Medline: 37130384]</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main" xml:id="_TyUMHep">Psychotherapy and artificial intelligence</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Plakun</surname></persName>
		</author>
		<idno type="DOI">10.1097/PRA.0000000000000748</idno>
		<idno>Medline: 37948172</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tU9kpTw">J Psychiatr Pract</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="476" to="479" />
			<date type="published" when="2023-11-01">Nov 01, 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Plakun EM. Psychotherapy and artificial intelligence. J Psychiatr Pract. Nov 01, 2023;29(6):476-479. [doi: 10.1097/PRA.0000000000000748] [Medline: 37948172]</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main" xml:id="_GK9Krr6">Therapeutic artificial intelligence: does agential status matter?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191037</idno>
		<idno>Medline: 37130404</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_z8wN4JB">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="33" to="35" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hurley ME, Lang BH, Smith JN. Therapeutic artificial intelligence: does agential status matter? Am J Bioeth. May 2023;23(5):33-35. [doi: 10.1080/15265161.2023.2191037] [Medline: 37130404]</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main" xml:id="_g9cYDzV">What we owe those who chat woe: a relational lens for mental health apps</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perry</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2250306</idno>
		<idno>Medline: 37812122</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Qbb6nYc">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="77" to="80" />
			<date type="published" when="2023-10">Oct 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ho A, Perry J. What we owe those who chat woe: a relational lens for mental health apps. Am J Bioeth. Oct 2023;23(10):77-80. [doi: 10.1080/15265161.2023.2250306] [Medline: 37812122]</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main" xml:id="_G5xTWGh">Therapists or replicants? Ethical, legal, and social considerations for using ChatGPT in therapy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Amram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Klempner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shturman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greenbaum</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191022</idno>
		<idno>Medline: 37130418</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vJEPeyV">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="40" to="42" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Amram B, Klempner U, Shturman S, Greenbaum D. Therapists or replicants? Ethical, legal, and social considerations for using ChatGPT in therapy. Am J Bioeth. May 2023;23(5):40-42. [doi: 10.1080/15265161.2023.2191022] [Medline: 37130418]</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main" xml:id="_Yszcut2">Potential and pitfalls of conversational agents in health care</title>
		<author>
			<persName><forename type="first">K</forename><surname>Denecke</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41572-023-00482-x</idno>
		<idno>Medline: 37996477</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7hd4ACN">Nat Rev Dis Primers</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="2023-11-23">Nov 23, 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Denecke K. Potential and pitfalls of conversational agents in health care. Nat Rev Dis Primers. Nov 23, 2023;9(1):66. [doi: 10.1038/s41572-023-00482-x] [Medline: 37996477]</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main" xml:id="_mZ8jupQ">Your robot therapist is not your therapist: understanding the role of AI-powered mental health chatbots</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Khawaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Blisle-Pipon</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdgth.2023.1278186</idno>
		<idno>Medline: 38026836] 100</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_J5rYRff">Front Digit Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="307" to="320" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>FREE Full text Vaida S. Artificial intelligence and psychology Revista Psihologie FREE Full text] 101</note>
	<note type="raw_reference">Khawaja Z, Blisle-Pipon JC. Your robot therapist is not your therapist: understanding the role of AI-powered mental health chatbots. Front Digit Health. 2023;5:1278186. [FREE Full text] [doi: 10.3389/fdgth.2023.1278186] [Medline: 38026836] 100. Vaida S. Artificial intelligence and psychology. Revista Psihologie. 2023;69(4):307-320. [FREE Full text] 101</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main" xml:id="_zB794SK">Large language models in psychiatry: opportunities and challenges</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volkmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyer-Lindenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E ;</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laacke</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191055</idno>
		<idno>Medline: 37130400] 103</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2hnsrK6">Psychiatry Res</title>
		<imprint>
			<biblScope unit="volume">339</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="46" to="48" />
			<date type="published" when="2023-05-02">Sep 2024. May 02, 2023</date>
		</imprint>
	</monogr>
	<note>Bias and epistemic injustice in conversational AI Am J Bioeth</note>
	<note type="raw_reference">Volkmer S, Meyer-Lindenberg A, Schwarz E. Large language models in psychiatry: opportunities and challenges. Psychiatry Res. Sep 2024;339:116026. [doi: 10.1016/j.psychres.2024.116026] [Medline: 38909412] 102. Laacke S. Bias and epistemic injustice in conversational AI. Am J Bioeth. May 02, 2023;23(5):46-48. [doi: 10.1080/15265161.2023.2191055] [Medline: 37130400] 103</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main" xml:id="_6Bnuxc7">Like I&apos;m talking to a real person&quot;: exploring the meaning of transference for the use and design of AI-based applications in psychotherapy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Holohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fiske</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyg.2021.720476</idno>
		<idno>Medline: 34646209] 104</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Twup36b">Front Psychol</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">720476</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Holohan M, Fiske A. &quot;Like I&apos;m talking to a real person&quot;: exploring the meaning of transference for the use and design of AI-based applications in psychotherapy. Front Psychol. 2021;12:720476. [FREE Full text] [doi: 10.3389/fpsyg.2021.720476] [Medline: 34646209] 104</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main" xml:id="_8qhxKAN">Is there an app for that?: ethical issues in the digital mental health response to COVID-19</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Skorburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yam</surname></persName>
		</author>
		<idno type="DOI">10.1080/21507740.2021.1918284</idno>
		<idno>Medline: 33989127] 105</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zQT59ck">AJOB Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2022-05-14">May 14, 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Skorburg JA, Yam J. Is there an app for that?: ethical issues in the digital mental health response to COVID-19. AJOB Neurosci. May 14, 2022;13(3):177-190. [doi: 10.1080/21507740.2021.1918284] [Medline: 33989127] 105</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main" xml:id="_eRbbfP8">Artificial intelligence-assisted psychosis risk screening in adolescents: practices and challenges</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.5498/wjp.v12.i10.1287</idno>
		<idno>Medline: 36389087] 106</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NJ5a7gF">World J Psychiatry</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1287" to="1297" />
			<date type="published" when="2022-10-19">Oct 19, 2022</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Cao XJ, Liu XQ. Artificial intelligence-assisted psychosis risk screening in adolescents: practices and challenges. World J Psychiatry. Oct 19, 2022;12(10):1287-1297. [FREE Full text] [doi: 10.5498/wjp.v12.i10.1287] [Medline: 36389087] 106</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main" xml:id="_ZWRmnAZ">Conversational artificial intelligence and the potential for epistemic injustice</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Proost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pozzi</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191020</idno>
		<idno>Medline: 37130408] 107</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YDMu5Cw">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="51" to="53" />
			<date type="published" when="2023-05-02">May 02. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">De Proost M, Pozzi G. Conversational artificial intelligence and the potential for epistemic injustice. Am J Bioeth. May 02, 2023;23(5):51-53. [doi: 10.1080/15265161.2023.2191020] [Medline: 37130408] 107</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main" xml:id="_GFqjA3D">Deception mode: how conversational AI can respect patient autonomy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191023</idno>
		<idno>Medline: 37130415] 108</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_22ShzAb">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="55" to="57" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gray J. Deception mode: how conversational AI can respect patient autonomy. Am J Bioeth. May 2023;23(5):55-57. [doi: 10.1080/15265161.2023.2191023] [Medline: 37130415] 108</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main" xml:id="_VCXJD4s">The postphenomenological impact of conversational artificial intelligence on autonomy and psychological integrity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Wadden</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191025</idno>
		<idno>Medline: 37130411] 109</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vbPnatu">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="37" to="40" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wadden JJ. The postphenomenological impact of conversational artificial intelligence on autonomy and psychological integrity. Am J Bioeth. May 2023;23(5):37-40. [doi: 10.1080/15265161.2023.2191025] [Medline: 37130411] 109</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main" xml:id="_bdfAhaN">The chatbot will see you now&apos;: mental health confidentiality concerns in software therapy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stiefel</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3166640</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UZfh8xs">SSRN Journal</title>
		<imprint>
			<date type="published" when="2018-07-19">July 19, 2018</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Stiefel S. &apos;The chatbot will see you now&apos;: mental health confidentiality concerns in software therapy. SSRN Journal. Preprint posted online July 19, 2018. [FREE Full text] [doi: 10.2139/ssrn.3166640] 110</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main" xml:id="_hQAncUq">Targeting depressive symptoms with technology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Torous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cerrato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Halamka</surname></persName>
		</author>
		<idno type="DOI">10.21037/mhealth.2019.06.04</idno>
		<idno>Medline: 31463305] 111</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5FSruwG">Mhealth</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2019-07">Jul 2019</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Torous J, Cerrato P, Halamka J. Targeting depressive symptoms with technology. Mhealth. Jul 2019;5:19. [FREE Full text] [doi: 10.21037/mhealth.2019.06.04] [Medline: 31463305] 111</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main" xml:id="_PYXqmxY">e-Addictology: an overview of new technologies for assessing and intervening in addictive behaviors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ferreri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bourla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mouchabac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karila</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpsyt.2018.00051</idno>
		<idno>Medline: 29545756] 112</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GJWWM2A">Front Psychiatry</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">51</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Ferreri F, Bourla A, Mouchabac S, Karila L. e-Addictology: an overview of new technologies for assessing and intervening in addictive behaviors. Front Psychiatry. 2018;9:51. [FREE Full text] [doi: 10.3389/fpsyt.2018.00051] [Medline: 29545756] 112</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main" xml:id="_SJ8pFYW">Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Stade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Stirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Boland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Yaden</surname></persName>
		</author>
		<idno type="DOI">10.1038/S44184-024-00056-z</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ukbWah3">npj Mental Health Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2024-04-02">Apr 02. 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Stade EC, Stirman SW, Ungar LH, Boland CL, Schwartz HA, Yaden DB, et al. Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation. npj Mental Health Res. Apr 02, 2024;3(1):1-12. [doi: 10.1038/S44184-024-00056-z] 113</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main" xml:id="_4vnd6wH">Is the algorithm good in a bad world, or has it learned to be bad? The ethical challenges of &quot;locked&quot; versus &quot;continuously learning&quot; and &quot;autonomous&quot; versus &quot;assistive&quot; AI tools in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Youssef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Char</surname></persName>
		</author>
		<idno type="DOI">10.1080/15265161.2023.2191052</idno>
		<idno>Medline: 37130390] 114</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UHuSJC2">Am J Bioeth</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="43" to="45" />
			<date type="published" when="2023-05">May 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Youssef A, Abramoff M, Char D. Is the algorithm good in a bad world, or has it learned to be bad? The ethical challenges of &quot;locked&quot; versus &quot;continuously learning&quot; and &quot;autonomous&quot; versus &quot;assistive&quot; AI tools in healthcare. Am J Bioeth. May 2023;23(5):43-45. [doi: 10.1080/15265161.2023.2191052] [Medline: 37130390] 114</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main" xml:id="_whaRugR">Can chatbots help support a person&apos;s mental health? Perceptions and views from mental healthcare professionals and experts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mulvenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1145/3453175</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WCnaJZc">ACM Trans Comput Healthcare</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2021-07-15">Jul 15, 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sweeney C, Potts C, Ennis E, Bond R, Mulvenna MD, O&apos;neill S, et al. Can chatbots help support a person&apos;s mental health? Perceptions and views from mental healthcare professionals and experts. ACM Trans Comput Healthcare. Jul 15, 2021;2(3):1-15. [doi: 10.1145/3453175] 115</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main" xml:id="_dEvxx5T">The use of machine learning to support the therapeutic process -strengths and weaknesses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lewanowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winiewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Oronowicz-Jakowiak</surname></persName>
		</author>
		<idno type="DOI">10.5114/ppn.2022.125050</idno>
		<idno>Medline: 37081907] 116</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4SjXnwp">Postep Psychiatr Neurol</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="167" to="173" />
			<date type="published" when="2022-12">Dec 2022</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Lewanowicz A, Winiewski M, Oronowicz-Jakowiak W. The use of machine learning to support the therapeutic process -strengths and weaknesses. Postep Psychiatr Neurol. Dec 2022;31(4):167-173. [FREE Full text] [doi: 10.5114/ppn.2022.125050] [Medline: 37081907] 116</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main" xml:id="_VhpDvaN">Self-help robots drive blues away</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sachan</surname></persName>
		</author>
		<idno type="DOI">10.1016/s2215-0366(18)30230-x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ywfNDad">Lancet Psychiatry</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">547</biblScope>
			<date type="published" when="2018-07">Jul 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sachan D. Self-help robots drive blues away. Lancet Psychiatry. Jul 2018;5(7):547. [doi: 10.1016/s2215-0366(18)30230-x] 117</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Beauchamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Childress</surname></persName>
		</author>
		<title level="m" xml:id="_2aJqZPG">Principles of Biomedical Ethics</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">118</biblScope>
		</imprint>
	</monogr>
	<note>8th edition</note>
	<note type="raw_reference">Beauchamp TL, Childress JF. Principles of Biomedical Ethics. 8th edition. New York, NY. Oxford University Press; 2019. 118</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main" xml:id="_Nxafn3K">Principles of clinical ethics and their application to practice</title>
		<author>
			<persName><forename type="first">B</forename><surname>Varkey</surname></persName>
		</author>
		<idno type="DOI">10.1159/000509119</idno>
		<idno>Medline: 32498071] 119</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RTmwpeH">Med Princ Pract</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Varkey B. Principles of clinical ethics and their application to practice. Med Princ Pract. 2021;30(1):17-28. [FREE Full text] [doi: 10.1159/000509119] [Medline: 32498071] 119</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main" xml:id="_R2VQaQK">Mental health messages in prominent mental health apps</title>
		<author>
			<persName><forename type="first">L</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gillies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mintzes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jureidini</surname></persName>
		</author>
		<idno type="DOI">10.1370/afm.2260</idno>
		<idno>Medline: 29987082] 120</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QVNyyS3">Ann Fam Med</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="338" to="342" />
			<date type="published" when="2018-07">Jul 2018</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Parker L, Bero L, Gillies D, Raven M, Mintzes B, Jureidini J, et al. Mental health messages in prominent mental health apps. Ann Fam Med. Jul 2018;16(4):338-342. [FREE Full text] [doi: 10.1370/afm.2260] [Medline: 29987082] 120</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main" xml:id="_nNMb7xQ">Critical bioethics: beyond the social science critique of applied ethics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Calma</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8519.2004.00385.x</idno>
		<idno>Medline: 15146853] 122</idno>
		<ptr target="https://www.theverge.com/2023/10/10/23911059/ai-climate-impact-google-openai-chatgpt-energy" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_JQNjXSP">The environmental impact of the AI revolution is starting to come into focus</title>
		<imprint>
			<date type="published" when="2004-04">2024-04-22. Apr 2004</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="120" to="143" />
		</imprint>
	</monogr>
	<note>The Verge</note>
	<note type="raw_reference">Calma J. The environmental impact of the AI revolution is starting to come into focus. The Verge. URL: https://www. theverge.com/2023/10/10/23911059/ai-climate-impact-google-openai-chatgpt-energy [accessed 2024-04-22] 121. Hedgecoe AM. Critical bioethics: beyond the social science critique of applied ethics. Bioethics. Apr 2004;18(2):120-143. [doi: 10.1111/j.1467-8519.2004.00385.x] [Medline: 15146853] 122</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main" xml:id="_cG8qHSc">Artificial Intelligence in mental health and the biases of language based models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Straw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Callison-</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0240376</idno>
		<idno>Medline: 33332380</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_d6x2aBu">PLoS One</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">240376</biblScope>
			<date type="published" when="2020-12-17">Dec 17, 2020</date>
		</imprint>
	</monogr>
	<note>FREE Full text</note>
	<note type="raw_reference">Straw I, Callison-Burch C. Artificial Intelligence in mental health and the biases of language based models. PLoS One. Dec 17, 2020;15(12):e0240376. [FREE Full text] [doi: 10.1371/journal.pone.0240376] [Medline: 33332380]</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
