<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_rCqxfyh">A fast and effective detection framework for whole-slide histopathology image analysis</title>
				<funder ref="#_938ekXZ">
					<orgName type="full">Wuhan Municipal Health Commission of China</orgName>
				</funder>
				<funder ref="#_4HGV3tV">
					<orgName type="full">National Natural Science Foundation of China</orgName>
					<orgName type="abbreviated">NSFC</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/501100001809</idno>
				</funder>
				<funder ref="#_dT5fEH6">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">© 2021 Ruan et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.</p>
				</availability>
				<date type="published" when="2021-05-12">May 12, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Ruan</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Information Engineering , Wuhan University of Technology , Wuhan , China ,</note>
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhikui</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Information Engineering , Wuhan University of Technology , Wuhan , China ,</note>
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenchen</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Information Engineering , Wuhan University of Technology , Wuhan , China ,</note>
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanglu</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Information Engineering , Wuhan University of Technology , Wuhan , China ,</note>
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingfan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> School of Information Engineering , Wuhan University of Technology , Wuhan , China ,</note>
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">Wuhan University of Technology</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Junqiu</forename><surname>Yue</surname></persName>
							<email>yuejunqiu@hotmail.com</email>
							<idno type="ORCID">0000-0002-1984-8445</idno>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Pathology , Huazhong University of Science and Technology , Tongji Medical College , Hubei Cancer Hospital , Wuhan , China</note>
								<orgName type="department" key="dep1">Department of Pathology</orgName>
								<orgName type="department" key="dep2">Tongji Medical College</orgName>
								<orgName type="institution" key="instit1">Huazhong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Hubei Cancer Hospital</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">University of Engineering &amp; Technology , Taxila , PAKISTAN</note>
								<orgName type="institution">University of Engineering &amp; Technology</orgName>
								<address>
									<settlement>Taxila</settlement>
									<country key="PK">PAKISTAN</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_cJheyXd">A fast and effective detection framework for whole-slide histopathology image analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-12">May 12, 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">DF9A2C506FC9383576BD0B7276E33E61</idno>
					<idno type="DOI">10.1371/journal.pone.0251521</idno>
					<note type="submission">Received: January 20, 2021 Accepted: April 27, 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T07:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_PWneTgu"><p xml:id="_xjsz6uj"><s xml:id="_9eYmSNu">Pathologists generally pan, focus, zoom and scan tissue biopsies either under microscopes or on digital images for diagnosis.</s><s xml:id="_dWDwykg">With the rapid development of whole-slide digital scanners for histopathology, computer-assisted digital pathology image analysis has attracted increasing clinical attention.</s><s xml:id="_KThnT3s">Thus, the working style of pathologists is also beginning to change.</s><s xml:id="_w9mP7KB">Computer-assisted image analysis systems have been developed to help pathologists perform basic examinations.</s><s xml:id="_TgkyNk2">This paper presents a novel lightweight detection framework for automatic tumor detection in whole-slide histopathology images.</s><s xml:id="_9qZGr2R">We develop the Double Magnification Combination (DMC) classifier, which is a modified DenseNet-40 to make patch-level predictions with only 0.3 million parameters.</s><s xml:id="_twaUhZv">To improve the detection performance of multiple instances, we propose an improved adaptive sampling method with superpixel segmentation and introduce a new heuristic factor, local sampling density, as the convergence condition of iterations.</s><s xml:id="_8dpQsAe">In postprocessing, we use a CNN model with 4 convolutional layers to regulate the patch-level predictions based on the predictions of adjacent sampling points and use linear interpolation to generate a tumor probability heatmap.</s><s xml:id="_JkKjZn3">The entire framework was trained and validated using the dataset from the Camelyon16 Grand Challenge and Hubei Cancer Hospital.</s><s xml:id="_Bkxcm3H">In our experiments, the average AUC was 0.95 in the test set for pixel-level detection.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Qqmt3Pc">Introduction</head><p xml:id="_tnPuteQ"><s xml:id="_HMa24Nr">In the past 100 years, pathologists have used microscopy to observe glass slides for clinical and pharmaceutical research, and more importantly, for providing definitive disease diagnoses to guide patient treatment and management decisions <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_nyz8WVf">With the rapid development of wholeslide digital scanners for histopathology, computer-assisted digital pathology image analysis has increasingly attracted clinical attention <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_ShAB3Sv">In this rapidly growing field of digital pathology, computer-assisted image analysis systems have been confirmed to help pathologists diagnose tumors and cancer subtypes.</s><s xml:id="_kC3pjkE">In clinical practice, accurately distinguishing regions (normal and tumor) in digital pathology images is an important task that helps pathologists perform basic examinations and complement their opinion <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_zNh9H4N">Thus, the workload of pathologists would be greatly reduced without any loss in sensitivity at the patient level.</s><s xml:id="_pJanVqh">Pathologists can focus on making more complex and detailed diagnoses to ultimately provide more accurate results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</s></p><p xml:id="_GcvGsXf"><s xml:id="_DYDqXF9">Whole-slide digital scanners have become more prevalent in clinical hospitals and make it easier to digitize, store, share, visualize and analyze histopathology slides.</s><s xml:id="_JABEVtK">Moreover, as one of the newest forms of "big data", whole-slide images (WSIs) in histopathology are constantly being produced every day.</s><s xml:id="_XD7AqW8">Typically, each WSI could have a full spatial resolution of 80K × 80K pixels and is approximately 2 GB in compressed storage size at 40× magnification.</s><s xml:id="_rYtzmt4">This high volume of data requires the development of a fast and effective processing pipeline for analyzing digital image data.</s></p><p xml:id="_8KwTFkc"><s xml:id="_utyFFyn">In recent years, there has been increasing interest in developing computer-assisted image analysis methods in pathology.</s><s xml:id="_rEwg3u5">A variety of competitions have emerged to promote intelligent algorithm research on digital tumor histopathology.</s><s xml:id="_FcqRZ2Q">The early competition task was to perform cell segmentation and image-related feature extraction.</s><s xml:id="_M5CZjCT">The tasks are the classification and grading of more complex whole-slide pathological images.</s></p><p xml:id="_rS36m2W"><s xml:id="_uQ6dsFV">The classification and grading of pathological images is the last step in the automatic analysis of pathological sections, and it is also a crucial step.</s><s xml:id="_kuSDEPS">In recent years, with the powerful tool of deep learning, researchers have applied CNNs in various cancer detection tasks and achieved good results.</s><s xml:id="_fyQ2nCj">The champion team of Camelyon16, Wang <ref type="bibr" target="#b5">[6]</ref>, obtained an area under the receiver operating characteristic curve (AUC) of 0.925 for WSI classification using Goo-gLeNet and random forest classifier with feature engineering.</s><s xml:id="_RRpaH5s">Cruz-Roa <ref type="bibr" target="#b6">[7]</ref> proposed HASHI based on a patch-based classifier with a 2-layer CNN, probability gradient from a heatmap, and Quasi-Monte Carlo sampling for WSI.</s><s xml:id="_G6DcQCp">The adaptive sampling algorithm used in this paper is derived from this method.</s></p><p xml:id="_4U2pMRN"><s xml:id="_MMzrxHa">Han <ref type="bibr" target="#b8">[8]</ref> proposed a multiclassification task to identify subordinate classes of breast cancer that uses a combination model of CNNs to analyze breast cancer histopathological images from the BreaKHis dataset.</s><s xml:id="_NtNx7qn">Valkonen <ref type="bibr" target="#b9">[9]</ref> extracted a large number of quantitative descriptors of image texture, spatial structure, and distribution of nuclei and applied a random forest model to output confidence values indicating the likelihood of cancer cells.</s><s xml:id="_QZyR86V">Xu <ref type="bibr" target="#b10">[10]</ref> used a pretrained AlexNet to extract the features of input patches and trained a linear SVM for segmentation in the MICCAI brain tumor challenge.</s><s xml:id="_DJH45yw">Wan <ref type="bibr" target="#b11">[11]</ref> constructed combinations of feature sets, including pixel-, object-, and semantic-level features derived from CNN, and utilized multiple SVM classifiers to determine breast cancer grades.</s><s xml:id="_FtAJgCq">Bayramoglu <ref type="bibr" target="#b12">[12]</ref> proposed a multitask CNN to predict both malignancy and image magnification levels simultaneously to improve performance on the BreaKHis dataset.</s><s xml:id="_8SPUMaE">Alsubaie <ref type="bibr" target="#b13">[13]</ref> proposed a deep CNN under multi-resolution to perform lung adenocarcinoma pattern classification.</s><s xml:id="_SAuaACS">Sirinukunwattana <ref type="bibr" target="#b14">[14]</ref> presented a segmentation performance comparison of 10 different network architectures for histology image classification problems.</s></p><p xml:id="_uyDFSU8"><s xml:id="_gBrSHjt">In the BACH challenge of ICIAR 2018, one of the tasks consisted of performing pixel-wise labeling of clinical hematoxylin-eosin-stained histopathological WSIs in four classes.</s><s xml:id="_wMVD8tz">Many new methods for the automatic classification of breast cancer biopsies were proposed, and CNN dominated the challenge <ref type="bibr" target="#b15">[15]</ref>.</s><s xml:id="_yrq89FD">In <ref type="bibr" target="#b16">[16]</ref>, a fully convolutional network based on DenseNet <ref type="bibr" target="#b17">[17]</ref> was proposed for performing pixel-wise labeling of WSIs.</s><s xml:id="_sybnGm6">In <ref type="bibr" target="#b18">[18]</ref>, a two-stage patch-based approach was proposed, which consisted of an autoencoder to extract image features and an image-wise CNN to perform the classification of the whole image.</s><s xml:id="_2zCWdhq">In <ref type="bibr" target="#b19">[19]</ref>, an ensemble of four modified Inception-V3 models was proposed for increasing the generalization capability of different networks trained on random subsets of training data.</s><s xml:id="_P3JnCxT">For WSI, a sliding window was used to uniformly extract patches, and a refined heatmap using ResNet-34 was used to reduce potential misclassifications.</s><s xml:id="_MgC7tYd"><ref type="bibr" target="#b20">[20]</ref> used an ImageNet pre-trained on DenseNet-161 for the segmentation of WSIs.</s><s xml:id="_4Jmb8NU"><ref type="bibr" target="#b21">[21]</ref> used an encoder-decoder network.</s><s xml:id="_DakZjVH">The encoder is composed of five convolutional processing blocks that integrate dense skip connections, group and dilated convolutions, and a self-attention mechanism following SENet <ref type="bibr" target="#b22">[22]</ref>, and the decoder follows the U-Net <ref type="bibr" target="#b23">[23]</ref> structure with skip connections between the down-sample and up-sample.</s></p><p xml:id="_KZzCc7c"><s xml:id="_K2BZ9q9">Li <ref type="bibr" target="#b24">[24]</ref> proposed a neural conditional random field (NCRF) deep learning framework to detect cancer metastasis in WSIs.</s><s xml:id="_9gZzuea">NCRF considers 9 spatially adjacent patches through a fully connected CRF, which is incorporated on top of a CNN feature extractor based on ResNet.</s><s xml:id="_KqAs5tp">Tokunaga <ref type="bibr" target="#b25">[25]</ref> aggregated three expert CNNs based on U-Net by using three different magnification images and used a modified Xception <ref type="bibr" target="#b26">[26]</ref> model to adaptively change the weight of each expert network depending on the input image.</s><s xml:id="_CYQ6XKb">Li <ref type="bibr" target="#b27">[27]</ref> developed a graph convolutional neural network to learn global topological representations of WSI for providing more accurate survival risk predictions.</s><s xml:id="_y7cHKmV">Wang <ref type="bibr" target="#b28">[28]</ref> proposed a recalibrated multi-instance network for adaptively aggregating the patch information to image-level prediction of whole slide gastric image, which improved image-level classification accuracy by assigning different weights to each instance.</s><s xml:id="_k6vYUVh">Sun <ref type="bibr" target="#b29">[29]</ref> applied U-Net to extract pixel-level features and adopt multiple classic finetuned CNN to obtain patch-level features, then jointed them by a hierarchical conditional random field method to localize abnormal (cancer) regions in gastric histopathology images.</s></p><p xml:id="_ucU4R6n"><s xml:id="_CBfeJ4A">In recent years, deep learning in solving image classification tasks, such as classification on ImageNet, has been greatly successful.</s><s xml:id="_8VqXGRe">Deep convolutional neural network (DCNN) models have been reported to surpass human performance.</s><s xml:id="_nrFSF7R">These models are typically used to process relatively small-sized natural images (200 × 200 pixels), but WSI is over hundreds of times the size of a natural image.</s><s xml:id="_TPE7Yk8">Therefore, most pathology image analysis methods take a patch-based classification approach that first segments a large image into small patches and then classifies each patch.</s><s xml:id="_nNp3wZu">This piecemeal approach has limited their analysis to small regions of interest (ROIs) within the larger WSI.</s><s xml:id="_pFXhff8">Thus, the overall size of the neural network can be allocated in the GPU memory.</s><s xml:id="_7H6VrRG">The issue associated with this approach is the need to use a sampling mechanism to traverse the entire pathology image.</s><s xml:id="_eGh4JdP">Dense uniform or regular sampling is one of the practical options, but the efficiency is not high.</s><s xml:id="_dTNzKRS">Even if there is no overlap between sample patches, a full detection process for the WSI is required to extract tens or hundreds of thousands of patches.</s><s xml:id="_JnrgcPY">In contrast, the adaptive sampling method is a more effective strategy for dealing with WSIs because it adaptively chooses regions with high uncertainty of a tissue patch being cancerous or not.</s><s xml:id="_usM6DHu">For regions wherein the predictor has a greater uncertainty about cancer and normal tissue classification, more patch samples will be classified to improve the confidence of the adaptive sampling method for those regions of ambiguity <ref type="bibr" target="#b6">[7]</ref>.</s></p><p xml:id="_JmkhgaH"><s xml:id="_cQJpByN">To establish a complete WSI processing pipeline, there are still some issues to discuss after the patch-based classification and adaptive sampling mechanism are selected, such as, how to develop an efficient and accurate classifier, what is the more appropriate convergence condition of the iterative sampling process, how to use images under a wider range of magnifications?</s></p><p xml:id="_55K2chM"><s xml:id="_NbkQpDK">Here, we present a deep learning-based approach for the identification of tumor metastasis on WSIs from the Camelyon16 dataset <ref type="bibr" target="#b30">[30]</ref>.</s><s xml:id="_28jdS3W">In summary, the main contributions of our study are as follows:</s></p><p xml:id="_ts2xTJQ"><s xml:id="_5R4vsJT">• Based on High-throughput Adaptive Sampling for Whole-slide Histopathology Image analysis (HASHI) <ref type="bibr" target="#b6">[7]</ref>, we propose an improved adaptive sampling method with superpixel segmentation and introduce a new heuristic factor, with local sampling density as the convergence condition of iterations to improve the detection effect of multiple instances.</s></p><p xml:id="_SbDt6cz"><s xml:id="_Fy3KNQp">• We develop the Double Magnification Combination (DMC) classifier, which is a modified DenseNet-40 to make patch-level predictions to discriminate tumor patches from normal patches.</s><s xml:id="_tJycH2y">The lightweight network use 20× and 40× magnification images with only 0.3 million parameters, and uses the large-margin Gaussian Mixture (L-GM) loss function <ref type="bibr" target="#b31">[31]</ref> to improve the generalization performance.</s></p><p xml:id="_SrCDqJX"><s xml:id="_7v73ndY">• In postprocessing, we train a CNN model with 4 convolutional layers to regulate the patchlevel predictions based on the predictions of adjacent sampling points.</s></p><p xml:id="_79f48MH"><s xml:id="_m8YfzZw">The source code for our approach has been made publicly available at <ref type="url" target="https://gitee.com/w3STeam/Pathological-images">https://gitee.com/  w3STeam/Pathological-images</ref> and <ref type="url" target="https://github.com/JustinRuan/Pathological-images">https://github.com/JustinRuan/Pathological-images</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3DqZk3z">Materials and methods</head><p xml:id="_s3PYBpT"><s xml:id="_Hs3nutX">Our tumor metastasis detection framework consists of a patch-based classifier, an improved adaptive sampling method, and a postprocessing filter.</s><s xml:id="_Y9afBgn">The complete pipeline is divided into two stages, namely, the sampling stage and the postprocessing stage, as shown in Fig <ref type="figure" target="#fig_1">1</ref>.</s><s xml:id="_KqbV4aH">To focus our training data set on regions of the slide most likely to contain tumor metastasis, we first identified tissue within the WSI and excluded background white space.</s><s xml:id="_EJST7A6">There are many methods based on threshold segmentation, such as <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b11">11]</ref>.</s><s xml:id="_jrUrr9v">We adopt a fixed-level threshold segmentation method in the HSV color space to exclude the obvious background region.</s><s xml:id="_RgHvabV">The final mask images were generated by combining the masks from the S and V channels.</s><s xml:id="_kXkhTBW">The constraint of effective coverage is that the threshold of the V channel is between 0.2 and 0.8, and the threshold of the S channel is greater than 0.1.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jbSGUEE">Patch extraction and preprocessing</head><p xml:id="_7gE9zHR"><s xml:id="_UMbGKqU">According to the detection results and pathologist's annotation, we extracted four types of patches: normal, tumor, edge inner, and edge outer.</s><s xml:id="_NjfY3hT">The labeling of a patch is determined by the proportion of the tumor area within the patch.</s><s xml:id="_QRdqbjq">When the proportion of tumor area is less than 50%, this patch is normal (Label 0); otherwise, it is tumor (Label 1).</s><s xml:id="_7UQ8HB2">For the labeling under the combination of double magnifications, we adopted the "or" logic here.</s><s xml:id="_dWGGMxQ">Here We used morphological methods to extract edge regions and increased the number of training samples at the edge of annotations to improve the performance of the classifier.</s><s xml:id="_JVXBwmw">Because the patches at the edge of annotations are usually transitional regions from tumor tissue to normal tissue, most of the "hard examples" are concentrated in these positions.</s></p><p xml:id="_FxX6kMH"><s xml:id="_ty5qUh7">At a sampling point, we simultaneously extracted two patches under 20× and 40× magnifications, and the size of the patches was 256×256, as shown in Fig 2 .</s><s xml:id="_bz2GDwF">Preprocessing and normalization were not applied to these saved patches to preserve the inherent fluctuation characteristics caused by staining.</s><s xml:id="_cJvJepW">These are also what the classifier needs to fit.</s><s xml:id="_yer6puW">We obtained a total of 1,694,228 patches, with 1,336,704 labeled as normal, 230,966 labeled as tumor, 57,384 labeled as edge inner, and 69,174 labeled as edge outer.</s><s xml:id="_QPXpDMv">It is worth noting that we balanced the number of positive and negative samples in a WSI.</s><s xml:id="_yGfZwNA">The sampling interval in the normal regions is larger than that in the tumor region.</s><s xml:id="_kaKXMCE">In this way, the number of negative samples in a WSI does not exceed 5~6 times the number of positive samples.</s><s xml:id="_EsxDWNz">Then, we constructed several balanced training sample sets (1:1) through random sampling for improving the performance of the patch-based classifier.</s></p><p xml:id="_jHbK2qE"><s xml:id="_N3Envc6">For sampling and predicting, we first used the threshold segmentation method mentioned above to calculate the effective region of the slide.</s><s xml:id="_zeYHThZ">Then, we directly extracted the 20× and 40× patches at pseudo-random sampling points and input them into the patch-based classifier.</s><s xml:id="_xT8x4rf">As in training, these extracted patches do not require any preprocessing or normalization.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DWZrPP5">Architecture of the patch-based classifier</head><p xml:id="_B8HBd3U"><s xml:id="_CZfkcSk">To explore the appropriate classifier structure, We chose nine classic pre-trained ImageNet networks to test the patch-based classifier under three magnifications.</s><s xml:id="_geqbNsq">We replaced the original top layer with a new one to connect each feature extraction part, which consists of a Global Average Pool (GAP) and two fully-connected (FC) layers.</s><s xml:id="_mXr5qWV">We used the prepared patches on three different magnifications to fine-tuning the top layer of each transfer model and tested the accuracy of these models.</s><s xml:id="_az3gbHj">All testing results of transfer-learning are shown in S1 Table in S1 Appendix.</s><s xml:id="_eCrnQ3C">According to the results of transfer-learning, the DenseNet family has the best feature extraction performance for pathological image blocks.</s><s xml:id="_3DWTn2b">The patches under 20× have the best distinguishable characteristics that can be extracted by CNN, as a result of the balance of the texture details and texture range in view.</s><s xml:id="_RkKjmzH">Although the 10× patches have a larger field of view, they are down-sampled to the same size resulting in the loss of texture and degradation of classification performance.</s><s xml:id="_xFDy5QX">Compared to 20×, the classifiers under 10× are a little worse.</s><s xml:id="_Qx4CQxt">Under 40×, the field of view in a patch becomes very small in a patch.</s><s xml:id="_zxQjUUR">When the patches are extracted from the transitional zone from tumor to normal near the edge of annotations, these patches under 40× are no significant and typical texture features, and even look the same as the patches in normal regions.</s><s xml:id="_wvUTvs3">So, it is difficult to train a better classifier under this magnification individually.</s><s xml:id="_VRBqHkv">On the other hand, the patch-based classifier calculates a tumor feature based on an entire 256 × 256 image, and the calculated patch-level prediction is stored in a tumor feature map based on the central coordinate of this patch.</s><s xml:id="_dzYyjfZ">Thus, with the same image size, the prediction under higher magnification can more accurately represent the tumor feature (probability) at the sampling point (the center of a patch).</s><s xml:id="_WNKf4qQ">From the perspective of spatial location, we argue that the prediction of a patch with the same size under 40× is more accurately express the tumor feature at the center of the patch, and facilitate the generation of more detailed segmentation boundaries.</s><s xml:id="_KZXVpty">Moreover, in the pixel segmentation experiment, the accuracy under the 20× and 40× magnifications alone is better than that under the 10× alone.</s></p><p xml:id="_zARazWa"><s xml:id="_QKevVCn">Pathologists usually check images by changing their magnification and scope in the WSI.</s><s xml:id="_5Qputfm">Ways to use images under a wider range of magnifications are worth studying.</s><s xml:id="_aKqJv4h">Inspired by this, we investigated the patch-based classifier with multiple magnifications.</s><s xml:id="_Yv2YJdT">We finally chose the combination of patches under 20× and 40× as inputs at the same sampling coordinates.</s></p><p xml:id="_E2adEUr"><s xml:id="_DNCZc44">Our patch-based classifier was derived from DenseNet40 (= 3x6x2+2x1+1+1 = 40).</s><s xml:id="_pv2CpKT">The network consists of three dense blocks defined in DenseNet.</s><s xml:id="_UCH6aDc">Each block consists of 6 dense layers that each contains two convolution layers.</s><s xml:id="_hWHhDGn">Between two adjacent dense blocks, there is a translation layer that consists of one convolution layer.</s><s xml:id="_wCPYAfN">And only two transport layers are used here.</s><s xml:id="_Mr5TkTg">The network also contains a convolutional layer at the input and a fully connected layer at the top.</s><s xml:id="_57Re7bm">We called it the Double Magnification Combination (DMC) patched-based classifier.</s><s xml:id="_WRtXHuy">The network contains two inputs and three outputs, as shown in Fig 3 .</s><s xml:id="_rP3SXcX">Our modified network has only 0.3 million parameters.</s><s xml:id="_XYQzQEs">The growth rate ('k' in <ref type="bibr" target="#b17">[17]</ref>) is set to16 to reduce the parameters of the model by using very narrow layers, at the same time, keeping up with the performance of our patch-based classifier.</s></p><p xml:id="_NteTvE4"><s xml:id="_TUhzcSn">To improve accuracy and generalization, the same network was used to process two types of patches under 20× and 40×.</s><s xml:id="_rw8Ppmj">The two patches at the corresponding sampling points are put into the same batch in a fixed order.</s><s xml:id="_eAkMqnG">Each three-channel image generated separately a 384-dimensional feature by DenseNet.</s><s xml:id="_XUmkZVA">Because each sampling point includes two input images (patches), we stitch the two-output feature of the same sample point into a 696-dimensional vector and consider it as the fusion feature of the sampling point.</s></p><p xml:id="_8amffea"><s xml:id="_ZhKCADh">After the output features are continuously stitched and split, the main classifier uses double-magnifications patches at the same sampling point for prediction, while the outputs of the auxiliary classifier under single magnification are only used as additional outputs for training.</s><s xml:id="_XKnxrCh">We used the regularization term to drive the output features of the auxiliary classifier under 20× and 40× to obey the same Gaussian mixture distribution.</s><s xml:id="_EYkkzGr">The final outputs here are 2-dimensional features, not probabilities.</s><s xml:id="_zj9ujWH">The "SoftMax" layer is not included in the network because our adaptive sampling algorithm mainly uses feature space.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NYEXAKD">Training</head><p xml:id="_ZVkYFXQ"><s xml:id="_WNmgSff">We used the training data set with two magnifications to train our patch-based classifier.</s><s xml:id="_KkeNFNv">To encourage the robustness and generalization of our network, we used two loss functions with three prediction outputs (ỹ 20 , ỹ40 , and ỹ).</s><s xml:id="_BwsStAK">Among them, ỹ20 corresponds to the predictions of patches only under 20×, ỹ40 corresponds to the predictions under 40×, and ỹ refers to the prediction of the fusion feature.</s><s xml:id="_APtE3s7">Here, y refers to the ground truth under double magnification, y 20 and y 40 and so forth.</s></p><p xml:id="_t4BVkzq"><s xml:id="_XuNGThN">The first loss function uses only the cross-entropy loss between ỹ and y.</s><s xml:id="_4ptmzjy">Here, L CE is the cross-entropy loss function.</s></p><formula xml:id="formula_0">loss 1 ¼ L CE ðỹ; yÞ<label>ð1Þ</label></formula><p xml:id="_UQMmMzk"><s xml:id="_cPwBqAT">Through loss 1 , the accuracy of the main classifier is improved, and the fusion features under double magnification can be better discovered and utilized.</s><s xml:id="_pynUqZP">After loss 1 is backpropagated, the temporarily generated graph used to compute the gradient of the network needs to be preserved.</s><s xml:id="_SDD27WF">Next, we perform the backpropagation of the second loss in (2), which consists of the average of cross-entropy loss under single magnification and a regularization term based on the large-margin Gaussian Mixture (L-GM) loss <ref type="bibr" target="#b31">[31]</ref>.</s><s xml:id="_puatJQ4">The regularization term simultaneously drives the deep model to generate the same Gaussian mixture-distributed features under two different magnifications.</s><s xml:id="_3TwsT8t">Because of its use, the generalization capability of the trained model is improved.</s></p><formula xml:id="formula_1">loss 2 ¼ 0:5 � ½L CE ðỹ 20 ; y 20 Þ þ L CE ðỹ 40 ; y 40 Þ�<label>ð2Þ</label></formula><p xml:id="_SYMZasE"><s xml:id="_dbp9tU2">þw � L GM ð½ỹ 20 ỹ40 �; ½y 20 y 40 �Þ</s></p><p xml:id="_kpFNNwA"><s xml:id="_mexhccC">Here, the coefficient of the first term is 0.5 indicates that the two cross-entropy losses at both magnifications have the same weight, that is, the classification error is reflected in their average under both magnifications.</s><s xml:id="_nK2JG7c">L-GM loss includes a nonnegative hyper-parameter α for controlling the expected margin between two classes in the training set.</s><s xml:id="_SQXnSBe">And its default value is 1.0 in <ref type="bibr" target="#b31">[31]</ref>.</s><s xml:id="_R7QKxPZ">We followed this setting.</s><s xml:id="_VT7Aa8T">And w is the weight of the regularization term L GM , which is 0.001 by default.</s><s xml:id="_ddK9BpS">Without data amplification, we trained the whole network for 40 epochs with a learning rate of 1 × 10 -3 and 40 epochs with a learning rate of 1 × 10 -4 .</s><s xml:id="_sFJ38GJ">The training was performed using PyTorch.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CAXE69d">Improved adaptive sampling method</head><p xml:id="_x8BMJaJ"><s xml:id="_W3SSrtS">HASHI <ref type="bibr" target="#b6">[7]</ref> provided a feasible solution for slide-level scanning and prediction on WSIs.</s><s xml:id="_KBvuPJD">After training a patch-based CNN classifier, HASHI extracts patches from the WSI using Quasi-Monte Carlo sampling and predicts the tumor probabilities of these patches.</s><s xml:id="_GdvKGPh">These predictions are used to build an interpolated probability map, which is used to identify suspicious regions for further sampling.</s><s xml:id="_zv78mkF">The newly sampled patches are used to produce an improved probability map estimation.</s><s xml:id="_5ypcauQ">The iterative process does not end until the limit of the maximum iterations is reached, and the final probability map is produced.</s></p><p xml:id="_uuN9hzq"><s xml:id="_25B8ewW">Our inspiration comes mainly from HASHI, and the main objective of our improved method is to try to optimize the following aspects.</s></p><p xml:id="_dsyGYEv"><s xml:id="_ZTU2zk2">• Change in the algorithmic structure.</s><s xml:id="_PUeQKSx">At the initial sampling, a regular sampling process based on superpixel segmentation is added.</s><s xml:id="_WwgbZve">After an adaptive sampling of the full slide, an iterative process based on the partial superpixels is added.</s></p><p xml:id="_mME9Awq"><s xml:id="_Y7ZTd7c">• Change in the selection conditions of the sampling points.</s><s xml:id="_5nDGakU">The original algorithm sorts by the gradient of the probability map, then select the coordinates within the larger half for sampling.</s><s xml:id="_gtpqV4p">Our algorithm uses a cluster-based heuristic factor to select sampling points based on feature gradients.</s></p><p xml:id="_PTa8Xxj"><s xml:id="_2B2Tvct">• Change in the convergence condition.</s><s xml:id="_6PqnuKV">Compared to the maximum number of iterations in the predecessor, we introduced a new statistical factor, local sampling density, to judge whether the iterations should be terminated.</s></p><p xml:id="_DjKJZAP"><s xml:id="_6dzbFqV">Regular sampling during initialization.</s><s xml:id="_UCtec2F">For the detection of whole-slide images, the general standard multiple instance assumption needs to be considered.</s><s xml:id="_uxMc9Sy">In HASHI, sampling points are more likely to be enriched near a larger area of the tumor region.</s><s xml:id="_3deb5Pd">A larger area has a longer edge, which corresponds to a tumor probability gradient change.</s><s xml:id="_vb3caUS">The adaptive sampling algorithm preferentially detects these locations where the tumor probability gradient changes are large.</s><s xml:id="_eKfdzYw">In contrast, small tumor areas do not have significant tumor gradient changes, which may result in under-sampling in certain suspicious regions.</s><s xml:id="_W2sumdA">Also, when the areas of the tumor regions in the WSI are small, the iterative sampling process may untimely terminate due to the limit of the number of iterations.</s><s xml:id="_dBNJWR3">To avoid this, we have to increase the number of maximum iterations or the number of samples per iteration, which means that it is necessary to guarantee a minimum number of samples.</s></p><p xml:id="_Chw6Beb"><s xml:id="_XsZCrAv">We extracted the thumbnail I of a WSI X under 1.25× magnification (Level 5) and separated it using the SLIC <ref type="bibr" target="#b32">[32]</ref> algorithm (compactness = 20).</s><s xml:id="_eWKxDrS">The boundaries B of the segmented superpixel regions S were extracted.</s><s xml:id="_dVGR2EZ">Then, we performed regular sampling at uniform spatial intervals on the boundaries of S. Here, the number of superpixels S is proportional to the area of the WSI.</s><s xml:id="_sgcnxap">The area of each superpixel S is approximately 1000 pixels under 1.25×, which is equivalent to the area of four 256×256 patches under 20×.</s><s xml:id="_fbq3qvy">In this way, a set of center coordinates C R of patches is obtained and used to generate the first gradient map in the feature space.</s><s xml:id="_q5ERxw3">Unlike the original algorithm, our gradient maps are based on features rather than probabilities.</s><s xml:id="_dguMWG2">Because the feature space has a larger dynamic range than the probability space, more edge details are obtained.</s></p><p xml:id="_kQPQ7bB"><s xml:id="_gC9fUfe">Adaptive sampling within full scope.</s><s xml:id="_PSuTqw5">The first stage strategy extracts the random coordinates C A of N A sampling points using Quasi-Monte Carlo sampling and merges them with the previous regular coordinates C R .</s><s xml:id="_vYurDyK">Here, we chose Halton sequences <ref type="bibr" target="#b33">[33]</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref> to generate the coordinates of the sampling points.</s><s xml:id="_CJcFNAJ">The patches were extracted in pairs under double magnification and put into our two-input classifier.</s><s xml:id="_86azSVg">The predications of these patches produced an initial coarse estimation of a linear interpolated feature map M feat .</s><s xml:id="_P7D6r5e">Then, we generated a gradient map M grad of the estimated feature map using the Sobel algorithm.</s><s xml:id="_TuZSUm6">Next, Mini Batch K-Means clustering <ref type="bibr" target="#b36">[36]</ref> was applied on M grad to partition the feature gradients into two clusters.</s><s xml:id="_WADkGq2">At least one of the cluster centers μ 0 of gradients is close to zero, which corresponds to flat regions in the gradient map (typical tumor or normal regions in the WSI).</s><s xml:id="_X5n5nNd">If another cluster center μ 1 is also close to zero, no significant edges are found in the current M grad .</s><s xml:id="_3Xaft9V">The edges of M grad correspond to the regions with large gradient change, that is, the uncertain or suspected tumor regions.</s><s xml:id="_Vmm7DSh">The cluster center μ 1 corresponding to the possible gradient edge should have a larger value.</s></p><p xml:id="_3HZ4u7w"><s xml:id="_t4eKA5S">Here, we introduced a new heuristic factor f grad to determine whether the edges of M grad are found, as shown in <ref type="bibr" target="#b2">(3)</ref>.</s><s xml:id="_mjRqek9">The value range of f grad is from 0 to 0.3.</s></p><formula xml:id="formula_2">f grad ¼ minð0:5 � ðm 0 þ m 1 Þ; 0:3Þ<label>ð3Þ</label></formula><p xml:id="_YrfWgpU"><s xml:id="_UdvsZGs">If f grad is greater than the threshold T grad (0.03), our adaptive sampling algorithm only focuses on the position where the feature gradient is greater than f grad .</s><s xml:id="_7KkHeDs">Otherwise, the sampling algorithm continues to pseudorandomly search sampling coordinates in the full image.</s><s xml:id="_ApzfzTC">In summary, the generation algorithm of the sampling points is divided into three cases.</s><s xml:id="_2WrYaqa">The first case is to randomly generate sampling points using a Halton sequence in the full scope of a WSI.</s><s xml:id="_WxppwsP">The second case is that none of the uncertain or suspected tumor regions have been found in M grad .</s><s xml:id="_rWGWKJ5">The generation algorithm pseudorandomly searches sampling coordinates and preferentially selects sampling coordinates with higher gradients in its 16×16 neighborhood under 1.25×.</s><s xml:id="_Ewn4yWx">This is equivalent to reducing the size of the gradient map M grad to the original one-sixteenth size through maximum pooling.</s><s xml:id="_5uMqBym">The generation algorithm searches for sampling coordinates based on this reduced gradient map to enhance its overall discovery capabilities.</s><s xml:id="_xZCFDr4">The third case is that the generation algorithm focuses on searching for suspicious regions when f grad is greater than T grad .</s><s xml:id="_Cqdx2hn">At this time, only the coordinates whose corresponding gradient is greater than f grad will be selected.</s><s xml:id="_w48Aq6K">If the algorithm cannot find enough sample points that satisfy the constraint at once, then it will look again in the neighborhood of the sample points just selected.</s></p><p xml:id="_FbRCHWw"><s xml:id="_6XxZQ7C">Through iterative adaptive sampling, M grad is continuously refined until the convergence condition of the first sampling stage is reached.</s><s xml:id="_Wb4X6c7">Here, we introduced a new statistical factor, local sampling density ρ, which is the number of previous sampling points in the neighborhood of a new sampling point.</s><s xml:id="_RfP2tud">There are two forms of neighborhoods here.</s><s xml:id="_k555NcB">One of them, ρ dt , is to define the range of the neighborhood by distance.</s><s xml:id="_s7DcyXm">The other, ρ sp , is defined by belonging to the same superpixel, which is only used in the second stage.</s></p><formula xml:id="formula_3">r dt ðc i Þ ¼ X Ið_ c j jk_ c j À c i k &lt; εÞ<label>ð4Þ</label></formula><formula xml:id="formula_4">r sp ðc i Þ ¼ X Ið_ c j jc i ; _ c j 2 s k Þ; s k 2 S<label>ð5Þ</label></formula><p xml:id="_p7hwee8"><s xml:id="_6kSNFh4">Here, c i is the coordinate of the sampling point i in the current iteration.</s><s xml:id="_EdPum4H">_ c j is the coordinate of j in previous iterations.</s><s xml:id="_3Exhdjx">The symbol k�k is a distance function, such as Chebyshev Distance.</s><s xml:id="_hBaYNAz">ε indicates the size of the neighborhood of sampling points.</s><s xml:id="_tZdRkXF">s k is a superpixel in the segmentation results S.</s></p><p xml:id="_z6MmnaJ"><s xml:id="_2wsAnMM">When the average of ρ dt of the current iteration is greater than the threshold T ρ , the sampling process will enter the second stage.</s><s xml:id="_rRyvD7N">An adaptive sampling process similar to the first stage is performed in a part of the superpixels.</s><s xml:id="_rVRytgm">We usually set T ρ to 1 or 2, since setting it to a larger value has little effect on the results but takes more time.</s><s xml:id="_zfM7skZ">See the S1 Appendix for this algorithm pseudocode.</s></p><p xml:id="_BhYMqvF"><s xml:id="_KB47Z7H">Adaptive sampling within enabled superpixels.</s><s xml:id="_yfcj5kF">Once the average of ρ dt reaches the threshold T ρ , the adaptive sampling algorithm will further explore regions where the sampling density is low but the tumor probability is high.</s><s xml:id="_Gekenj3">Therefore, we excluded part of a WSI based on the superpixels obtained earlier.</s><s xml:id="_KMJ2u3m">We counted the number of sample points contained in each superpixel, that is, the local sampling density ρ sp .</s><s xml:id="_UQuqDc6">Based on the interpolation feature map and the gradient map, the two maximums f max ðs i Þ and ĝ max ðs i Þ in the ith superpixel were also calculated.</s><s xml:id="_wZjjtm6">Here, we used three thresholds T ρ , T sp f and T sp g to determine which regions need further inspection.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_uUfRvrz">S enable ¼ ðs</head><formula xml:id="formula_5">i jr sp ðs i Þ &lt; T r ^f max ðs i Þ &gt; T sp f ^ĝ max ðs i Þ &gt; T sp g Þ<label>ð6Þ</label></formula><p xml:id="_gtYCxr6"><s xml:id="_2atn2eK">Because the output of our binary patch-based classifier is the tumor feature of a patch, if we use the Sigmoid function to regress a feature into a tumor probability, the feature value -1 corresponds to the tumor probability of 27%.</s><s xml:id="_Qg6medR">Here the threshold T sp f represents the lower limit of the tumor feature in a superpixel, generally set to -1.</s><s xml:id="_pPWczGu">When there is a feature larger than T sp f in a superpixel, it means that there is a point with a tumor probability greater than 27% inside.</s><s xml:id="_KeWwQAq">In the next iteration, such superpixels will be further explored.</s><s xml:id="_4k8qtb8">T sp g is 0.1; this gradient threshold constrains the regions in S enable from being too flat.</s><s xml:id="_c33cbmG">Because such flat regions are generally far from the boundaries of the tumor, too much sampling does not contribute much.</s><s xml:id="_7w5Ac9X">ρ sp (s i ) indicates whether the superpixel s i is fully sampled.</s></p><p xml:id="_GWD2bKa"><s xml:id="_cZEGGbM">When S enable is updated, the adaptive sampling process is executed again until the average of ρ dt reaches T ρ .</s><s xml:id="_qQS4atP">By iteratively updating S enable and sampling, S enable finally becomes an empty set, and the entire algorithm will end.</s><s xml:id="_DXUPmEH">It should be noted that if the two thresholds T sp f and T sp g are sufficiently small, such as -3 and 0, the entire sampling process will degenerate into uniform sampling.</s><s xml:id="_TtYwhqK">Fig 4E shows the contour lines of the tumor probability map with different colors.</s><s xml:id="_amRTxNS">The red line indicated the ground truth.</s></p><p xml:id="_qEMEczH"><s xml:id="_HERDZdv">Please refer to the S1 Appendix for the detailed process of the sampling algorithm.</s></p><p xml:id="_GtTqR9f"><s xml:id="_TTXsUqe">Algorithm 1: Adaptive gradient-based sampling Input: M: CNN-trained model X: WSI T: maximum iterations N A : number of sample points extracted A: area of each superpixel d: spaced intervals of sampling T grad , T ρ , T sp f and T sp g : thresholds M grad , f grad , H; . . .</s><s xml:id="_699w48x">� S, C R regular sampling based on superpixels (X,A,d) S enable = S For i = 1 to in T do: C A sampling point generation (N A , M grad , f grad , T grad , S enable ,. .</s><s xml:id="_NbzHjAf">.)</s><s xml:id="_7dQaYzS">C = C R [ C A ; i ¼ 1 C A ; i &gt; 1 ( Predictions F patch classification (M, C) M feat feature map interpolation (F, C) M grad feature gradient (M feat ) μ 0 , μ 1 clustering (M grad ) f grad = min(0.5</s><s xml:id="_VadeUf9">� (μ 0 +μ 1 ),0.3)</s><s xml:id="_bYJfgku">avg r dt average local sampling density within the neighborhood (H; C) H ððc i ; f i Þjc i 2 C; f i ¼ Fðc i ÞÞ [ H If avg r dt &gt; T r : ρ sp local sampling density within superpixels (H; S) S enable update enabled regions (S; r sp ; T r ; T sp f ; T sp g ) If S enable is �: Return H</s></p><p xml:id="_xabSaKt"><s xml:id="_zcM26hN">Here, C R refers to the set of center coordinates of patches, which are obtained by regular sampling based on superpixel segmentation.</s><s xml:id="_MP2b98c">C A refers to the set of center coordinates of patches, which are obtained by random sampling (Quasi-Monte Carlo) process.</s><s xml:id="_9uK8rZq">We only returned the coordinates c i and predictions f i (1-dim feature) of each sample point in all iterations.</s><s xml:id="_VEXyNBX">Next, the postprocessing generates a heat map of tumor probability.</s></p><p xml:id="_AUXEZCy"><s xml:id="_jbZMsDA">In our experiments, a slide was required to extract nearly 59000 patches of size 256×256 on average using uncovered regular sampling under 20×.</s><s xml:id="_BwE5njR">Our sampling method with parameters T ρ = 1 and N A = 2000 only needed to extract an average of 7400 patches, which is only 1/8 of the workload of the uncovered regular sampling.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wkVguk9">Postprocessing</head><p xml:id="_rKVxfFk"><s xml:id="_ZjzSQnE">In postprocessing, each obtained prediction is adjusted based on the predictions of its neighboring sampling points.</s><s xml:id="_dtzUduf">A CNN model with 4 convolutional layers was trained to regulate the patch-level predictions under 1.25×, as shown in Table <ref type="table">1</ref>.</s><s xml:id="_p9xRftZ">We can think of this as adaptive filtering of patch-level features, so we also called it a slide filter.</s><s xml:id="_tyxD82r">The input of the slide filter is a 64×64 single-channel matrix centered at each sample point, which includes the feature of its adjacent sampling points.</s><s xml:id="_a4ewbD2">If the sample point is in a tumor region, it is a tumor/positive sample and labeled as 1; otherwise, it is a normal/negative sample and labeled as 0. Corresponding to 20×, the size of the input matrix is 1024×1024 pixels, and the area is equivalent to the 16 nonoverlapping patches used in the patch-based classifier.</s></p><p xml:id="_zt9pv2A"><s xml:id="_9RXMFsY">According to the pathologist's annotations and the obtained predictions, we generated a training set of 188360 balanced samples (the ratio of normal to tumor samples is 1:1).</s><s xml:id="_vJ8FxWD">The loss function used in training consists of two parts: cross-entropy loss and L-GM loss.</s><s xml:id="_sE5R4jh">The network was trained on patches of shape = 64×64 pixels, with batches of size = 200, and weight of L-GM loss = 0.001.</s></p><p xml:id="_E7gpemQ"><s xml:id="_BS8TSbN">The corrected patch-level predictions are generated by a weighted average with the new patch-level predictions and the original prediction.</s><s xml:id="_zRdabb8">Then, according to the corrected predictions and sampling coordinates, a tumor probability heat map is generated by the Sigmoid function and linear interpolation.</s><s xml:id="_vac2ug3">Here, we did not use fully-conv (FC) net to directly generate a heatmap under 1.25×, because this required higher hardware requirements (GPU memory capacity).</s></p><p xml:id="_H3qpe9B"><s xml:id="_Wc984cv">We used the tumor probability heatmap to compute the evaluation for each WSI.</s><s xml:id="_S5T8BBh">In</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_XNBkPT9">Results &amp; discussion</head><p xml:id="_AueFZEh"><s xml:id="_qcERkAu">This paper used the H&amp;E-stained WSIs of the Camelyon16 challenge, which is aimed at detecting metastasis on the WSIs of lymph node sections <ref type="bibr" target="#b5">[6]</ref>, and 40 H&amp;E-stained WSIs provided by Hubei Cancer Hospital (HCH).</s><s xml:id="_RuGv8HD">We used these datasets to detect tumor regions.</s><s xml:id="_t3Amaqw">The tumor regions in the HCH dataset are generally large and typical, as shown in Fig 6 .</s><s xml:id="_dtKxsHG">In the two subplots, the blue regions are the marked tumor regions.</s><s xml:id="_SEKCUQC">In Fig <ref type="figure">6B</ref>, <ref type="figure">a</ref> green region is the excluded region.</s><s xml:id="_ZZanvhR">In terms of the number of tumor regions per slide, the test set samples in Camelyon16 contain an average of 33 compared to an average of 11 for the HCH samples.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WrugK7r">Evaluating the patch-based classifier</head><p xml:id="_Z2nXdHa"><s xml:id="_zFtrGak">In this section, we mainly evaluate the performance of the patch-based classifier.</s><s xml:id="_gpbrwBu">To compare the classification performance of different networks, we used the prepared dataset to evaluate their patch-level F1 scores, as shown in Table <ref type="table">2</ref>. Here, "40×" refers to the DenseNet-40 with a single input under 40×.</s><s xml:id="_6wnDmBu">"DMC" refers to our modified DenseNet-40 with two magnification inputs, "DMC 40×" refers to the auxiliary classifier only for 40×, "DMC 40×+20×" refers to the main classifier using fusion features under two magnifications.</s><s xml:id="_nTH8qpY">"L-GM" refers to the L-GM loss used in training.</s><s xml:id="_TPrX9hP">From the patch-level results of the classifier, the performance of the auxiliary classifier under 40× of DMC is similar to the single-input classifier under 40×, and that of the auxiliary classifier under 20× is better than the corresponding single-input classifier.</s><s xml:id="_gEeHtz6">When</s></p><p xml:id="_AZgxaEa"><s xml:id="_Fm8ZyWt">Table 1.</s><s xml:id="_f2TkYdv">The slide filter (CNN model) in postprocessing.</s><s xml:id="_jytEgaM">Layer (type) Output Shape Param Conv2d+ReLU [32, 64, 64] 320 Conv2d+ReLU+MaxPool2d [32, 32, 32] 9,248 Conv2d+ReLU+MaxPool2d [48, 16, 16] 13,872 Conv2d+ReLU+MaxPool2d [64, 8, 8] 27,712 AvgPool2d [64, 1, 1] 0 Linear [2] 130 Total params 51,282 <ref type="url" target="https://doi.org/10.1371/journal.pone.0251521.t001">https://doi.org/10.1371/journal.pone.0251521.t001</ref></s><s xml:id="_H9Czuq2">Fig 6.</s><s xml:id="_5UDFyGX">Examples of WSI in the two datasets.</s><s xml:id="_Sm5g4vr"><ref type="url" target="https://doi.org/10.1371/journal.pone.0251521.g006">https://doi.org/10.1371/journal.pone.0251521.g006</ref></s></p><p xml:id="_Wa3sdEs"><s xml:id="_2hZAdrV">the fused features under both magnifications are used at the same time, the performance was improved by nearly 2~3%.</s><s xml:id="_VKWFQU6">Because a pair of patches overlap at the center point and the field of view is different, so the spatial attention mechanism was introduced.</s><s xml:id="_TXh4B7v">For the performance of patch-level detection, our experience is the use of L-GM loss in training has no significant effect on single-input or dual-input classifiers.</s><s xml:id="_7AtAXVz">Next, we evaluated the performance of pixel-level detection, and this evaluation more tested the generalization capability of patch-based classifiers.</s><s xml:id="_xQzWmq6">Using the pathologist's annotation as the ground truth, ROC analysis at the pixel level heat map was performed, and the measures used for comparing the algorithms were F1 score and area under the ROC curve (AUC).</s><s xml:id="_AjHNNXU">In Table <ref type="table">2</ref>, the pixel level detection results all used our proposed adaptive sampling algorithm, and the parameter configuration for our model involved the threshold T ρ of the local sampling density (T ρ = 1) with 2000 samples per iteration (N A = 2000) and the area of each superpixel A = 1000 pixels with spatial intervals of regular sampling d = 60 pixels under 1.25× magnification.</s><s xml:id="_WMPucTh">Besides, the pixels with tumor probability greater than 0.5 were considered positive in the heat map.</s></p><p xml:id="_G8uqgNc"><s xml:id="_m28aebj">Regarding the F1 scores in Table <ref type="table">2</ref>, there is not much difference in accuracy between single-input or dual-input patch-based classifiers, but there is a significant difference in the results of pixel-level segmentation.</s><s xml:id="_dW6HMJP">In the pixel-level segmentation task, the F1 scores of each patchbased classifier are much lower than the scores of the classifier during training and testing of patches.</s><s xml:id="_VaWcHfy">Note that, the loss of the pixel-level segmentation task is not used to optimize the performance of patch-based classifiers; it represents the generalization performance of the classifier.</s><s xml:id="_2G9VQRW">This is because, although we extracted millions of patches from WSI for training these patch-based classifiers, the input images during our adaptive random sampling are almost impossible to be the same as those in the training set.</s><s xml:id="_hgXXy3x">In other words, the patches extracted during the adaptive sampling are more varied.</s><s xml:id="_R3WYCyp">Moreover, the prediction error at any sampling point has an impact on the accuracy of the segmentation boundary near it.</s><s xml:id="_aPyPjzA">The superposition effect brought by the sampling mechanism makes it possible to obtain correct results only when the robustness of the patch-based classifier is sufficient.</s><s xml:id="_HpaRfyY">Regarding the F1 score of pixel level, the performance of our adaptive sampling algorithm on DMC is nearly 20% higher than that of the classifiers with single magnification.</s><s xml:id="_hTsAUT7">This shows the advantages of the dual input structure.</s></p><p xml:id="_DgXmTyq"><s xml:id="_VD2Y8z9">From the detection results of the pixel level, L-GM loss is necessary for DCM.</s><s xml:id="_3JZBMSu">The use of L-GM loss increases the margin between the centers of the two classes, During the adaptive sampling process, more the features of sampling patches fall into this gap, resulting in the deterioration of the detection results.</s><s xml:id="_CmStgYt">The contours of the probability of 0.5 at the heat map</s></p><p xml:id="_ayt5Ubf"><s xml:id="_jJ2k7ZM">Table 2.</s><s xml:id="_7n9qYWY">The classifier detection performance.</s><s xml:id="_6k8NV6Y">Methodology Patch Level Pixel Level Train Test Train Test F1(Normal) F1(Tumor) F1(Avg) F1(Normal) F1(Tumor) F1(Avg) F1 AUC F1 AUC 40× 0.9546 0.9548 0.9547 0.9711 0.8827 0.9269 0.5170 0.9616 0.4595 0.9030 20× 0.9567 0.9570 0.9568 0.9701 0.8805 0.9253 0.5738 0.9286 0.5036 0.8571 DMC 40× 0.9512 0.9514 0.9513 0.9711 0.8844 0.9277 ----20× 0.9712 0.9713 0.9712 0.9802 0.9242 0.9522 ----40×+20× 0.9737 0.9740 0.9738 0.9810 0.9275 0.9542 0.6007 0.8454 0.5518 0.8630 DMC+L-GM 40× 0.9517 0.9524 0.9520 0.9714 0.8872 0.9293 ----20× 0.9702 0.9702 0.9702 0.9811 0.9277 0.9544 ----40×+20× 0.9723 0.9725 0.9724 0.9815 0.9296 0.9556 0.7111 0.9681 0.6121 0.9279 <ref type="url" target="https://doi.org/10.1371/journal.pone.0251521.t002">https://doi.org/10.1371/journal.pone.0251521.t002</ref></s></p><p xml:id="_vpDwen6"><s xml:id="_DDbMsax">generated by DMC are closer to the ground truth.</s><s xml:id="_9RMCcmt">Regarding ROC AUC of pixel level, the heat map generated by DMC with L-GM loss is also the best.</s><s xml:id="_Z84pKsF">On the WSIs of the training set, DMC performs similarly to the single input classifier under 40×.</s><s xml:id="_6nHWaAS">But the score of DMC is 2.5% higher than the classifier under 40× on the WSIs of the test set.</s><s xml:id="_ZGGAB6C">We think that the higher the AUC, the better the selection and prediction of sampling points.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_XP7rp8D">Evaluating adaptive sampling algorithms</head><p xml:id="_N8Rs9mn"><s xml:id="_k4Drvw6">In this section, we mainly evaluate the performance of the sampling algorithms by comparing the probability heatmaps using the same DMC.</s><s xml:id="_z5RnMch">As before, the measures used for comparing the algorithms were the F1 score and AUC at the pixel level.</s><s xml:id="_xnkMqVx">Table <ref type="table">3</ref> shows the pixel detection performance comparison between HASHI and our sampling method on tumor samples.</s><s xml:id="_xn7Hgqu">In the parameters of HASHI, the number of samples per iteration was fixedly set to 400, and the maximum iterations T was set to 20, 30, and 40, respectively.</s><s xml:id="_MErvmdD">In our sampling method, we evaluated both non-post-processing and post-processing (The slide filter was marked as 'SF' in Table <ref type="table">3</ref>).</s><s xml:id="_5GuX7qe">We report the average F1 score and AUC for these approaches with the Camelyon16 and HCH datasets.</s><s xml:id="_jXEQExE">Here the experiment WSIs were divided into three groups: Camelyon16 Train, Camelyon16 Test, and HCH Test.</s><s xml:id="_MWFZsVh">The patches for training the DMC classifier were extracted from WSIs of training data of Camelyon16.</s><s xml:id="_bZrbwJQ">In other words, a part of the sampled patches may exist in the training set.</s><s xml:id="_uzdUteq">So, the classifier had higher classification performance for such WSIs.</s><s xml:id="_xAjPpjJ">The other two datasets were never seen by the patch-based classifier.</s></p><p xml:id="_rSwaaGd"><s xml:id="_rKkkJaz">Compared to the F1 score of the patch level, the score of the pixel level did have a significant decline.</s><s xml:id="_Ue6HmtJ">On the other hand, AUC at pixel level was still relatively high, and that of our method exceeded 0.95 on all datasets.</s><s xml:id="_6wum9rD">Because the DMC classifier was trained on labeled patches and had not been trained using pixel-level labeled data on WSIs.</s><s xml:id="_zrU8hvP">Therefore, the probability heatmap is better in overall probability prediction, but the contours of the probability of 0.5 at the heat map were still a little bit different from the ground truth.</s></p><p xml:id="_SXXDj3p"><s xml:id="_pPyXKkd">For HASHI, when the accuracy of the classifier is sufficient on Group Camelyon16 Train, both F1 and AUC will increase as the number of sampling points increases.</s><s xml:id="_Y59UPJu">On the other verification groups, F1 and AUC did not improve even if the number of sampling points was doubled.</s><s xml:id="_mVfHE7g">Because for the verification groups, the F1 score of the tumor patches was 0.9296, which was 0.04 lower than that of the training set in Table <ref type="table">2</ref>.</s></p><p xml:id="_fBSeBPW"><s xml:id="_ESFPXvM">Compared with HASHI, our proposed adaptive sampling method has better results.</s><s xml:id="_fEzeZGZ">The F1 and AUC of our method with post-processing are the highest of all tests.</s><s xml:id="_yUr3dN2">Our F1 score is at least 5.8% higher than its predecessors, and AUC is at least 3.2% higher than that.</s><s xml:id="_yCWXQs9">Regarding the slide filter, the post-processing has a 1.6% improvement on F1 and 1.9% on AUC.</s><s xml:id="_djKRnFq">It is worth noting that our F1 and AUC without slide filter were not significantly different from</s></p><p xml:id="_nbnbkdt"><s xml:id="_2qJtPUV">Table 3.</s><s xml:id="_2JqkyZZ">The pixel-level detection performance on different sampling algorithms with DMC classifier.</s><s xml:id="_wHQ6Gh4">Methodology Camelyon16 Train Camelyon16 Test HCH Test Number of sampling points F1 AUC F1 AUC F1 AUC Our method 0.7111±0.1839</s><s xml:id="_CRzcncW">0.9681±0.0782</s><s xml:id="_sKMc2rn">0.6121±0.2631</s><s xml:id="_bJg5McA">0.9279±0.1028</s><s xml:id="_fQThuMn">0.6999±0.2041</s><s xml:id="_a4NMsmJ">0.9342±0.0485</s><s xml:id="_aNs9nua">7402±2028 Our method2 � 0.7113±0.1813</s><s xml:id="_4xmKXKW">0.9782±0.0524</s><s xml:id="_rfpfHer">0.6173±0.2720</s><s xml:id="_HCZwKsd">0.9527±0.0819</s><s xml:id="_fU7a7Tr">0.7439±0.1929</s><s xml:id="_7ZHs4sK">0.9577±0.0342</s><s xml:id="_VfCtzHn">7402±2028 HASHI T = 20 0.5879±0.2633</s><s xml:id="_gmKABVC">0.9393±0.0978</s><s xml:id="_HrzyhKs">0.5695±0.3090</s><s xml:id="_9s2XgcY">0.8782±0.1815</s><s xml:id="_u8JPEJ9">0.6810±0.2269</s><s xml:id="_rVkDgHx">0.9451±0.0424</s><s xml:id="_K68huv9">8000 HASHI T = 30 0.6129±0.2682</s><s xml:id="_FaZKpYw">0.9660±0.0491</s><s xml:id="_kjueMjy">0.5425±0.3317</s><s xml:id="_wA5Qt7F">0.8451±0.2259</s><s xml:id="_h658DbD">0.6844±0.2257</s><s xml:id="_NWDhhAA">0.9415±0.0447</s><s xml:id="_H3rCXfC">12000 HASHI T = 40 0.6424±0.2379</s><s xml:id="_NjdayeZ">0.9690±0.0425</s><s xml:id="_hSqv4Zs">0.5574±0.3122</s><s xml:id="_semzkn3">0.8787±0.1864</s><s xml:id="_G6huren">0.6832±0.2311</s><s xml:id="_7EmzYuG">0.9409±0.0454</s><s xml:id="_VRhTtUr">16000 � Our method2 is the method using Slide Filter in postprocessing.</s><s xml:id="_6XQB2rd"><ref type="url" target="https://doi.org/10.1371/journal.pone.0251521.t003">https://doi.org/10.1371/journal.pone.0251521.t003</ref></s></p><p xml:id="_6vrnnvR"><s xml:id="_6jhSU5B">HASHI on Group HCH.</s><s xml:id="_DEBrjxn">This was not the case with Group Camelyon16 Test.</s><s xml:id="_cFWANDg">This is because the area of the tumor regions in each slide of HCH is on average 8 to 9 times larger than Camelyon16, but the number of regions is generally relatively small.</s><s xml:id="_vnMxSrA">In other words, the detection target is relatively significant.</s><s xml:id="_VU4ev6p">Therefore, HASHI is more suitable for the detection of such WSIs, and our proposed method can detect more and smaller tumor regions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RwHVgpm">Two evaluation in Camelyon16</head><p xml:id="_6rrZGjT"><s xml:id="_u9f7Z7G">In this section, we briefly present two evaluation results in Camelyon16: Slide-based Evaluation and Lesion-based Evaluation.</s></p><p xml:id="_rMhRGPV"><s xml:id="_RWwxeCn">Slide-based evaluation.</s><s xml:id="_M4496Qb">This evaluation task is to distinguish between slides containing metastasis and normal slides and rank them by the area under ROC curve (AUC) <ref type="bibr" target="#b5">[6]</ref>.</s><s xml:id="_ZYYNr4J">For the slide-based classification task, the postprocessing method takes a prediction result for each WSI as input and produces a single probability of tumor for the entire WSI as output.</s><s xml:id="_DVrDTZr">Here, we extracted 5 statistical features from the positive part of the predictions F , whose tumor probability is greater than 0.5.</s><s xml:id="_KfBPgXD">These features included the number of sample points that meet the probability requirements, the maximum tumor probability among them, and a normalized histogram with three bins based on these tumor probabilities.</s><s xml:id="_XRnreKc">We computed these features over the predictions across all cases, and we trained and compared 4 classifiers to discriminate whether a WSI includes tumor regions.</s><s xml:id="_YTC4MWc">The merits of the algorithms will be assessed for discriminating between slides containing metastasis and normal slides.</s><s xml:id="_NYTsmEG">Receiver operating characteristic (ROC) analysis at the slide level will be performed, and the measure used for comparing the algorithms will be the area under the ROC curve (AUC) <ref type="bibr" target="#b37">[37]</ref>.</s><s xml:id="_rdeV9ST">On the independent test cases, the Lagrangian-based S 3 VM <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b39">39]</ref> model achieved an AUC of 0.9920, as shown in Fig 7 .</s><s xml:id="_r8zBEyv">Our score is very close to the score (0.9935) of the top-ranked team on the leaderboard of the Camelyon16 ISBI challenge <ref type="bibr" target="#b37">[37]</ref>.</s></p><p xml:id="_F824Z6C"><s xml:id="_ZrVnP5j">Lesion-based evaluation.</s><s xml:id="_Puynq5D">The second evaluation task is to test the detection/ localization performance, which is summarized using free-response operating characteristic (FROC) curves <ref type="bibr" target="#b37">[37]</ref>.</s><s xml:id="_dmjuga3">For lesion-based detection, a pair probability and corresponding coordinate of each predicted cancer lesion within the WSI need to be given with few false positives.</s><s xml:id="_6ySdJpD">Our approach is similar to <ref type="bibr" target="#b40">[40]</ref>, which used a non-maxima suppression method.</s><s xml:id="_ghtNagw">In contrast, we used the Isolation Forest algorithm <ref type="bibr" target="#b41">[41]</ref> and the K-means (K = 2) clustering to find automatic segmentation thresholds for a tumor probability heatmap.</s></p><p xml:id="_NREuGqS"><s xml:id="_R8mqaeM">The FROC curve is defined as the plot of sensitivity versus the average number of false positives per image <ref type="bibr" target="#b37">[37]</ref>.</s><s xml:id="_Sj4FJhk">As shown in Fig <ref type="figure" target="#fig_9">8</ref>, our method achieved a score of 0.7694 at 1 FP per WSI on the training cases and a score of 0.7373 at 1 FP on the test cases.</s><s xml:id="_jDTfebV">Table <ref type="table">4</ref> shows the comparison with other methods using Camelyon16.</s><s xml:id="_8HR87zj">Our score has reached the level of human performance.</s><s xml:id="_cXwvsgj">However, there is still a large gap between the current best score (0.8533), which was achieved by Fast ScanNet <ref type="bibr" target="#b42">[42]</ref>.</s><s xml:id="_88eCHAg">Fast ScanNet used a fully convolutional network without an up-sampling path to generate a probability heatmap with a much smaller size than the input image, then performed a dense scan on ROIs and stitched the predictions into a complete heatmap.</s><s xml:id="_Jf9BgKa">The label of the patch-based training sample of Fast ScanNet was pixel-level and our classifier used patch level, it is not difficult to understand that the former performed well in FROC of the pixel-level detection.</s><s xml:id="_6zq4DF4">From our heat map, our proposed method usually combines multiple smaller tumor regions into one larger region for reporting.</s><s xml:id="_rGcgtPy">In the FROC measurement, this caused many small tumor regions to be detected but not reported.</s><s xml:id="_n5Pq8JQ">Another phenomenon is that the F1 score is not high (&lt;0.75)</s><s xml:id="_mgsPx2S">and the AUC is high (&gt;0.95) in Table <ref type="table">3</ref>.</s></p><p xml:id="_7AhEuJZ"><s xml:id="_VWjQWYE">On the other hand, Fast ScanNet still needs to completely scan the entire area to be detected, but our adaptive sampling algorithm does not need to do this.</s><s xml:id="_keBfejX">At the same time, Fast ScanNet used an FC net to generate heat maps, and a large number of large-size (2866×2866) feature maps were produced during the convolution process.</s><s xml:id="_wtSuYhR">Therefore, it put forward higher requirements for the memory capacity of GPU.</s><s xml:id="_yw78X39">And our method only needs to use 256×256 input images, as long as your computer can run PyTorch, you can complete the WSI detection task.</s><s xml:id="_vnTfWqx">When computing resources are limited, our proposed algorithm is a feasible and effective method.</s></p><p xml:id="_6sp3HJ2"><s xml:id="_YkubkJ8">Next, we discuss this issue in detail in the next section.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_QuAqfQp">Model runtime efficiency</head><p xml:id="_pYgKf3m"><s xml:id="_r5gkrSc">Due to the use of a lightweight network, the computational complexity is relatively low, and approximately 286.2 pairs of double-magnification patches can be predicted per second.</s><s xml:id="_qj8empA">The performance test was performed on a PC with a 3.2 GHz Intel i7-8700 CPU with 16 GB of memory and an NVIDIA GeForce GTX 1080 8 GB.</s><s xml:id="_PsC8XQZ">The core of our proposed model is the DMC classifier, which is called thousands of times.</s><s xml:id="_6QWZRfr">However, it only contains 306,498 parameters.</s><s xml:id="_dTHyKXA">Compared with the patch-based classic network, the parameter size of VGG16 is 460 times that of our model, the size of GoogLeNet is 80 times, the size of ResNet-50 is 85 times, and the size of DenseNet-121 is 27 times.</s><s xml:id="_ptpKQVZ">Therefore, the DMC classifier only takes one second to predict nearly 300 pairs of 256×256 patches from the saved small JPG files.</s></p><p xml:id="_wXVWrGj"><s xml:id="_YBjH78Y">A heat map of a WSI under 1.25× contains an average of 15.1 million pixels.</s><s xml:id="_XXgwPKU">When a full dense scan of a WSI is performed at equal intervals without coverage under 20×, it is necessary to extract and predict approximately 59,000 patches of size 256×256.</s><s xml:id="_TwKfxDX">As shown in Table <ref type="table">3</ref>, when the number of samples per iteration (400) was fixed in HASHI, the number of extracted patches of per WSI is directly proportional to the maximum iterations T. When T was equal to 20, HASHI here needed to predict 8000 samples, which accounted for 13.6% of the number of the full dense scan.</s><s xml:id="_H6QApCn">Our algorithm uses a cluster-based heuristic factor to select sampling points, instead of selecting the larger half ones of the gradient in HASHI.</s><s xml:id="_yjcaXjG">For scanning a slide, the number of patches extracted by our sampling algorithm is different for each slide.</s><s xml:id="_N3JqqWq">Table 4. Detection performance comparison with Camelyon16.</s><s xml:id="_4sx8S2V">Team AUC FROC Human performance 0.9660 0.7325 HMS and MIT 0.9935 0.8074 Our method 0.9920 0.7373 Fast ScanNet-16 0.9875 0.8533 HMS, Gordon Center, MGH 0.9763 0.7600 CUHK 0.9415 0.7030 EXB Research 0.9156 0.5111 DeepCare, Inc. 0.8833 0.2430 Middle East Tech.</s><s xml:id="_kdNXzT9">Uni.</s><s xml:id="_eg4SVzC">0.8632 0.3822 NLP LOGIX Co. 0.8298 0.3859 Smart Imaging Tech.</s><s xml:id="_MnGh2kz">Co. 0.8207 0.3385 Univ. of Toronto 0.8149 0.3822 Radboud Uni.</s><s xml:id="_VYtGsVN">0.7786 0.5748 <ref type="url" target="https://doi.org/10.1371/journal.pone.0251521.t004">https://doi.org/10.1371/journal.pone.0251521.t004</ref></s></p><p xml:id="_Y9Yn6hK"><s xml:id="_7fUxRse">Typically, about 7400 samples are extracted for each slide by our method, which accounted for 9.1 ~15.9% of the number of the full dense scan.</s><s xml:id="_KRHFJY3">The computational complexity using the local sampling density (T ρ = 1) is equivalent to HASHI using the parameter T = 20.</s><s xml:id="_BSQGPuT">In Table <ref type="table">3</ref>, the detection results of our method are better.</s><s xml:id="_2tSSt9B">On the WSIs of the training set, the F1 score and AUC are 12.3% and 3.9% higher than HASHI.</s><s xml:id="_wyfqTxn">On the WSIs of the two test sets, the F1 score and AUC have improved by 5.5% and 4.3% on average compared to HASHI.</s><s xml:id="_M3xzjQd">In time consumption, our method usually takes 2 to 3 minutes to complete a WSI with i7 CPU and single GTX 1080 8 GB.</s><s xml:id="_t24tFYP">Our proposed method reduces the detection area by at least 85% in the adaptive sampling manner and saves the computing load of each sampling point with the lightweight network.</s><s xml:id="_mA7ZArf">Through the divide-and-conquer approach, the need for the memory capacity of GPU is drastically reduced, and at the same time, detection effects can meet the needs of preliminary screening in clinical diagnosis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KgJmzz6">Conclusion</head><p xml:id="_NEhKNcQ"><s xml:id="_9JAjDwN">We proposed a novel lightweight detection framework for automatic tumor detection in whole-slide histopathology images.</s><s xml:id="_4sN6vyJ">Compared to classic CNN models, our DMC model with dual inputs and three outputs is easier to train, with higher computational efficiency with only 0.3 million parameters.</s><s xml:id="_SVwHKwK">Our improved adaptive sampling method uses a new heuristic factor as the convergence condition of iterations for improving the detection performance of multiple instances, which is only 1/8 of the workload of the uncovered regular sampling.</s><s xml:id="_HgB4SHU">In postprocessing, the patch-level predictions are regulated based on the predictions of adjacent sampling points to improve the pixel level and lesion level accuracy.</s><s xml:id="_7tSBjg5">Our experiments revealed that our method also has reached the state of the art on the pixel level and lesion level detection of gigapixel pathology slides with limited computing resources.</s><s xml:id="_mBydP5d">In clinical practice, the ability to use more computer resources for detecting whole-slide images will greatly promote the practical application of automatic diagnostic technology.</s></p><p xml:id="_qfBBjv6"><s xml:id="_TSgeeKr">With the continuous popularization of breast cancer screening, more and more early-stage breast cancers containing carcinoma in situ have been discovered.</s><s xml:id="_DgY7XGB">On whole-slide images, how to accurately identify the presence and proportion of carcinoma in situ and invasive cancer is extremely important for selecting the appropriate treatment and the best benefit for the patient.</s><s xml:id="_8zPuBFx">In future work, we aim to study region detection of carcinoma in situ and invasive cancer.</s><s xml:id="_gUDeYjp">We could explore a new clustering model for encoding histology WSI to analyze the texture features on tissue structure in a larger field of view.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_qAM7nub"><s xml:id="_W4k9dG5">Our model was trained with the Camelyon16 dataset, which consists of 400 WSIs total, split into 270 WSIs for training and 130 WSIs for testing.</s><s xml:id="_PRWq7qy">Here, the extraction of patches was divided into two cases: extraction for generating a training set and extraction in adaptive sampling for detection.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 1 .</head><label>1</label><figDesc><div><p xml:id="_X2RrvxV"><s xml:id="_WSahvkf">Fig 1.</s><s xml:id="_bCEaFJ9">An overview of our proposed workflow.</s><s xml:id="_NbbKVjV">https://doi.org/10.1371/journal.pone.0251521.g001</s></p></div></figDesc><graphic coords="4,43.20,424.52,532.80,258.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig 2 .</head><label>2</label><figDesc><div><p xml:id="_amFp2sJ"><s xml:id="_AUjU7PW">Fig 2. Extract two magnification patches at a sampling point.</s><s xml:id="_uwyChFv">https://doi.org/10.1371/journal.pone.0251521.g002</s></p></div></figDesc><graphic coords="5,43.20,301.61,532.80,390.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig 3 .</head><label>3</label><figDesc><div><p xml:id="_TJKP2yB"><s xml:id="_6uw2jYf">Fig 3. DMC patch-based classifier architecture.</s><s xml:id="_mtKMcNz">https://doi.org/10.1371/journal.pone.0251521.g003</s></p></div></figDesc><graphic coords="7,43.20,75.91,532.80,386.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc><div><p xml:id="_g2HcN3r"><s xml:id="_fFdqS6z">Fig 4A shows the 112 (purple) sampling points generated in the first round of the adaptive sampling, and 12 of which were generated by regular sampling during initialization.</s><s xml:id="_JMDvNQY">In Fig 4B-4D, the next three rounds of sampling are shown here, and the 4th round reached the termination condition T ρ .</s><s xml:id="_a5AkfDJ">The sampling points were densely generated where the predicted tumor probability exceeds 0.3 and they had a larger gradient in the estimated feature space.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 4 .</head><label>4</label><figDesc><div><p xml:id="_FqsJ77h"><s xml:id="_SuT3AvX">Fig 4. The process of the adaptive sampling.</s><s xml:id="_ZDvmUn9">https://doi.org/10.1371/journal.pone.0251521.g004</s></p></div></figDesc><graphic coords="11,43.20,305.23,532.80,377.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><figDesc><div><p xml:id="_qfCwdm2"><s xml:id="_2F8KWGB">Fig 5, the contour lines of the probability maps are shown in (A).</s><s xml:id="_VzCGNV8">(C) is a partial enlargement of the lower right corner of (A).</s><s xml:id="_RRBREsU">Here, the contours with different colors correspond to different probabilities.</s><s xml:id="_ZsRQctt">(B) and (D) show the predictions corresponding to the left side using our adaptive sampling method.</s><s xml:id="_ZvkvKfv">The yellow regions in (B) and (D) indicate the ground truth.</s><s xml:id="_EQRR7Cx">We gave more examples in the S1 Appendix.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 5 .Fig 6 .</head><label>56</label><figDesc><div><p xml:id="_TdA8sXM"><s xml:id="_zDB3DrB">Fig 5. Tumor probability heatmap and predictions of sampling points.</s><s xml:id="_h7dq7M9">https://doi.org/10.1371/journal.pone.0251521.g005</s></p></div></figDesc><graphic coords="14,72.00,78.01,504.00,383.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 7 .</head><label>7</label><figDesc><div><p xml:id="_U7snfwW"><s xml:id="_nB2mgfp">Fig 7. Receiver Operating Characteristic (ROC) curve of slide-based classification.</s><s xml:id="_PpKQ44P">https://doi.org/10.1371/journal.pone.0251521.g007</s></p></div></figDesc><graphic coords="18,200.01,78.01,301.44,285.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig 8 .</head><label>8</label><figDesc><div><p xml:id="_JeykNET"><s xml:id="_4QAJjqb">Fig 8. FROC curve of the lesion-based detection.</s><s xml:id="_5C48Ku4">https://doi.org/10.1371/journal.pone.0251521.g008</s></p></div></figDesc><graphic coords="19,200.01,78.01,353.25,302.23" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_Xd9XYZ2"><s xml:id="_8uBSrTy">PLOS ONE | https://doi.org/10.1371/journal.pone.0251521May</s><s xml:id="_zmHRZtF">12, 2021</s></p></note>
		</body>
		<back>

			<div type="funding">
<div xml:id="_23f5UnY"><p xml:id="_qtVe8fm"><s xml:id="_NXbn7kT">This work was supported by research grants <rs type="grantNumber">81300042</rs> (to JY) from the <rs type="funder">National Natural Science Foundation of China</rs>, [<rs type="grantNumber">2014]41</rs> (to JY) from the "<rs type="programName">Training Project for Young and Middleaged Medical Talents</rs>" from the <rs type="funder">Wuhan Municipal Health Commission of China</rs> and <rs type="grantNumber">WJ2019H124</rs> (to JY) Health commission of <rs type="programName">Hubei Province scientific research</rs> project.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4HGV3tV">
					<idno type="grant-number">81300042</idno>
				</org>
				<org type="funding" xml:id="_938ekXZ">
					<idno type="grant-number">2014]41</idno>
					<orgName type="program" subtype="full">Training Project for Young and Middleaged Medical Talents</orgName>
				</org>
				<org type="funding" xml:id="_dT5fEH6">
					<idno type="grant-number">WJ2019H124</idno>
					<orgName type="program" subtype="full">Hubei Province scientific research</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_VQ95Kp4"><p xml:id="_yfUfNv6"><s xml:id="_hGXAbEe">The data are held in a public repository, <ref type="url" target="https://camelyon17.grand-challenge.org/Data/">https://camelyon17.grand- challenge.org/Data/</ref>.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ynW7fTc">Supporting information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_xsnFkv4">S1 Appendix. (DOCX)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_z6kFVbh">Author Contributions</head><p xml:id="_8kfbZM3"><s xml:id="_j7xSHPE">Data curation: Junqiu Yue.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_t8wgWtq">Funding acquisition: Junqiu Yue.</head><p xml:id="_vFmaCby"><s xml:id="_qR9SzkD">Methodology: Jun Ruan.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_putDScw">Project administration: Junqiu Yue.</head><p xml:id="_R5Pn7Ze"><s xml:id="_yYgvZEx">Software: Jun Ruan, Zhikui Zhu, Chenchen Wu, Guanglu Ye, Jingfan Zhou.</s></p><p xml:id="_nSe87SB"><s xml:id="_bwufj4Q">Validation: Zhikui Zhu, Chenchen Wu, Guanglu Ye, Jingfan Zhou.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_vWmsHHY">Writing -original draft: Jun Ruan.</head><p xml:id="_wauqMxz"><s xml:id="_QBUW5T4">Writing -review &amp; editing: Jun Ruan, Junqiu Yue.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_Ddjmr9w">Automated Histology Analysis: Opportunities for signal processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovacevic</surname></persName>
		</author>
		<idno type="DOI">10.1109/msp.2014.2346443</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jtQBzws">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mccann MT, Ozolek JA, Castro CA, Parvin B, Kovacevic J. Automated Histology Analysis: Opportuni- ties for signal processing. IEEE Signal Processing Magazine. 2015; 32(1):78-87.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_Ep8DeJr">Breast Cancer Histopathology Image Analysis: A Review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jpw</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<idno type="DOI">10.1109/tbme.2014.2303852</idno>
		<idno type="PMID">24759275</idno>
		<ptr target="https://doi.org/10.1109/TBME.2014.2303852" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_pWYXwQy">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1400" to="1411" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Veta M, Pluim JPW, Van Diest PJ, Viergever MA. Breast Cancer Histopathology Image Analysis: A Review. IEEE Transactions on Biomedical Engineering. 2014; 61(5):1400-11. https://doi.org/10.1109/ TBME.2014.2303852 PMID: 24759275</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_5xCZqz4">Histopathological Image Analysis: A Review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Gurcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Can</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yener</surname></persName>
		</author>
		<idno type="DOI">10.1109/rbme.2009.2034865</idno>
		<idno type="PMID">20671804</idno>
		<ptr target="https://doi.org/10.1109/RBME.2009.2034865" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_byhqjZU">IEEE Reviews in Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="171" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gurcan MN, Boucheron LE, Can A, Madabhushi A, Rajpoot NM, Yener B. Histopathological Image Analysis: A Review. IEEE Reviews in Biomedical Engineering. 2009; 2(2):147-71. https://doi.org/10. 1109/RBME.2009.2034865 PMID: 20671804</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_UgGgGBj">Computational pathology: Challenges and promises for tissue analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compmedimag.2011.02.006</idno>
		<idno type="PMID">21481567</idno>
		<ptr target="https://doi.org/10.1016/j.compmedimag.2011.02.006" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_DzpFTjP">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="515" to="530" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fuchs TJ, Buhmann JM. Computational pathology: Challenges and promises for tissue analysis. Com- puterized Medical Imaging and Graphics. 2011; 35(7):515-30. https://doi.org/10.1016/j.compmedimag. 2011.02.006 PMID: 21481567</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_P9xwba8">Computational Pathology: A Path Ahead</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bry</surname></persName>
		</author>
		<idno type="DOI">10.5858/arpa.2015-0093-sa</idno>
		<ptr target="https://doi.org/10.5858/arpa.2015-0093-SAPMID:26098131" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_FnhDyA2">Archives of Pathology &amp; Laboratory Medicine</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Louis DN, Feldman M, Carter AB, Dighe AS, Pfeifer JD, Bry L, et al. Computational Pathology: A Path Ahead. Archives of Pathology &amp; Laboratory Medicine. 2016; 140(1):41-50. https://doi.org/10.5858/ arpa.2015-0093-SA PMID: 26098131</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_dtJtupd">Deep Learning for Identifying Metastatic Breast Cancer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gargeya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Irshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Beck</surname></persName>
		</author>
		<idno>arXiv:</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qusDzYp">Quantitative Methods</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang D, Khosla A, Gargeya R, Irshad H, Beck AH. Deep Learning for Identifying Metastatic Breast Cancer. arXiv: Quantitative Methods. 2016.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_AGKFCcJ">High-throughput adaptive sampling for whole-slide histopathology image analysis (HASHI) via convolutional neural networks: Application to invasive breast cancer detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cruz-Roa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basavanhally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shih</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0196828</idno>
		<idno type="PMID">29795581</idno>
		<idno type="PMCID">PMC5967747</idno>
		<ptr target="https://doi.org/10.1371/journal.pone.0196828" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_yPdepKz">PLoS One</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">196828</biblScope>
			<date type="published" when="2018-05-26">2018. 2018/05/26</date>
		</imprint>
	</monogr>
	<note>Tomaszewski also serve on the scientific advisory board for the digital pathology company Inspirata Inc</note>
	<note type="raw_reference">Cruz-Roa A, Gilmore H, Basavanhally A, Feldman M, Ganesan S, Shih N, et al. High-throughput adap- tive sampling for whole-slide histopathology image analysis (HASHI) via convolutional neural networks: Application to invasive breast cancer detection. PLoS One. 2018; 13(5):e0196828. Epub 2018/05/26. https://doi.org/10.1371/journal.pone.0196828 PMID: 29795581; PubMed Central PMCID: PMC5967747 the digital pathology company Inspirata Inc. Drs Madabhushi, Feldman, Ganesan, and Tomaszewski also serve on the scientific advisory board for the digital pathology company Inspirata Inc.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_HSDr52d">also has an equity stake in Inspirata Inc. and</title>
		<author>
			<persName><surname>Dr</surname></persName>
		</author>
		<author>
			<persName><surname>Madabhushi</surname></persName>
		</author>
		<imprint>
			<publisher>Elucid Bioimaging Inc</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Dr. Madabhushi also has an equity stake in Inspirata Inc. and Elucid Bioimaging Inc.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_bQVpgua">Breast Cancer Multi-classification from Histopathological Images with Structured Deep Learning Model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-017-04075-z</idno>
		<idno type="PMCID">PMC5482871</idno>
		<ptr target="https://doi.org/10.1038/s41598-017-04075-zPMID:28646155" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_SCPcj6d">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4172</biblScope>
			<date type="published" when="2017-06-25">2017. 2017/06/25</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Han Z, Wei B, Zheng Y, Yin Y, Li K, Li S. Breast Cancer Multi-classification from Histopathological Images with Structured Deep Learning Model. Sci Rep. 2017; 7(1):4172. Epub 2017/06/25. https://doi. org/10.1038/s41598-017-04075-z PMID: 28646155; PubMed Central PMCID: PMC5482871.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_xQSUmB2">Metastasis detection from whole slide images using local features and random forests</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valkonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kartasalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liimatainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nykter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Latonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruusuvuori</surname></persName>
		</author>
		<idno type="DOI">10.1002/cyto.a.23089</idno>
		<ptr target="https://doi.org/10.1002/cyto.a.23089PMID:28426134" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_AJ3YgKB">Cytometry Part A</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="555" to="565" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Valkonen M, Kartasalo K, Liimatainen K, Nykter M, Latonen L, Ruusuvuori P. Metastasis detection from whole slide images using local features and random forests. Cytometry Part A. 2017; 91(6):555-65. https://doi.org/10.1002/cyto.a.23089 PMID: 28426134</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_sKJWTfg">Large scale tissue histopathology image classification, segmentation, and visualization via deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-017-1685-x</idno>
		<idno type="PMCID">PMC5446756</idno>
		<ptr target="https://doi.org/10.1186/s12859-017-1685-xPMID:28549410" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_d2RXSGQ">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">281</biblScope>
			<date type="published" when="2017-05-28">2017. 2017/05/28</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu Y, Jia Z, Wang LB, Ai Y, Zhang F, Lai M, et al. Large scale tissue histopathology image classification, segmentation, and visualization via deep convolutional activation features. BMC Bioinformatics. 2017; 18(1):281. Epub 2017/05/28. https://doi.org/10.1186/s12859-017-1685-x PMID: 28549410; PubMed Central PMCID: PMC5446756.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_4uBeUmv">Automated grading of breast cancer histopathology using cascaded ensemble with combination of multi-level image features</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.05.084</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fCMxYz3">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="34" to="44" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wan T, Cao J, Chen J, Qin Z. Automated grading of breast cancer histopathology using cascaded ensemble with combination of multi-level image features. Neurocomputing. 2017; 229:34-44.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_wkenSaH">Deep learning for magnification independent breast cancer histopathology image classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bayramoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">¨j</forename><surname>Heikkila</surname></persName>
		</author>
		<author>
			<persName><surname>Editors</surname></persName>
		</author>
		<idno type="DOI">10.1109/icpr.2016.7900002</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_X27Tazd">23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bayramoglu N, Kannala J, Heikkila ¨J, editors. Deep learning for magnification independent breast can- cer histopathology image classification. 2016 23rd International Conference on Pattern Recognition (ICPR); 2016.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_DgAbSjQ">A Multi-resolution Deep Learning Framework for Lung Adenocarcinoma Growth Pattern Classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alsubaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khurram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><surname>Editors</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-95921-4_1</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Alsubaie N, Shaban M, Snead D, Khurram A, Rajpoot N, editors. A Multi-resolution Deep Learning Framework for Lung Adenocarcinoma Growth Pattern Classification 2018; Cham: Springer Interna- tional Publishing.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_ef25yzt">Improving Whole Slide Segmentation Through Visual Context-A Systematic Study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Alham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Verrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jbXY3h6">arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sirinukunwattana K, Alham NK, Verrill C, Rittscher J. Improving Whole Slide Segmentation Through Visual Context-A Systematic Study. arXiv: Computer Vision and Pattern Recognition. 2018.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_Mh9kpqf">BACH: Grand challenge on breast cancer histology images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aresta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arau ´jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chennamsetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Safwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">V</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.05.010</idno>
		<idno type="PMID">31226662</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.05.010" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_sr9ZDZc">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="122" to="139" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Aresta G, Arau ´jo T, Kwok S, Chennamsetty SS, Safwan M, Alex V, et al. BACH: Grand challenge on breast cancer histology images. Medical Image Analysis. 2019; 56:122-39. https://doi.org/10.1016/j. media.2019.05.010 PMID: 31226662</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_CB7vRrp">Candy Cane: Breast Cancer Pixel-Wise Labeling with Fully Convolutional Densenets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Galal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanchez-Freire</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93000-8_93</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Galal S, Sanchez-Freire V, editors. Candy Cane: Breast Cancer Pixel-Wise Labeling with Fully Convo- lutional Densenets2018; Cham: Springer International Publishing.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_7FtbvnP">Densely Connected Convolutional Networks. computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Der</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.243</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Huang G, Liu Z, Der Maaten LV, Weinberger KQ, editors. Densely Connected Convolutional Networks. computer vision and pattern recognition; 2017.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_WQJ4bH5">Two-Stage Convolutional Neural Network for Breast Cancer Histology Image Classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aminpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4yf9XNc">15th International Conference on Image Analysis and Recognition</title>
		<meeting><address><addrLine>ICIAR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nazeri K, Aminpour A, Ebrahimi M, editors. Two-Stage Convolutional Neural Network for Breast Cancer Histology Image Classification. 15th International Conference on Image Analysis and Recognition, ICIAR 2018; 2018.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_cYZ8HEC">Ensemble Network for Region Identification in Breast Histopathology Slides</title>
		<author>
			<persName><forename type="first">B</forename><surname>Marami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prastawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeineh</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93000-8_98</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YAwKrYf">15th International Conference on Image Analysis and Recognition</title>
		<meeting><address><addrLine>ICIAR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marami B, Prastawa M, Chan M, Donovan M, Fernandez G, Zeineh J, editors. Ensemble Network for Region Identification in Breast Histopathology Slides. 15th International Conference on Image Analysis and Recognition, ICIAR 2018; 2018.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_eN397Ya">Assessment of Breast Cancer Histology Using Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Walz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Braunewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RkYYyK9">15th International Conference on Image Analysis and Recognition</title>
		<meeting><address><addrLine>ICIAR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kohl M, Walz C, Ludwig F, Braunewell S, Baust M, editors. Assessment of Breast Cancer Histology Using Densely Connected Convolutional Networks. 15th International Conference on Image Analysis and Recognition, ICIAR 2018; 2018.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_QxXWgkD">Micro and Macro Breast Histology Image Analysis by Partial Network Re-use</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mnn</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93000-8_102</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YPR3bva">15th International Conference on Image Analysis and Recognition</title>
		<meeting><address><addrLine>ICIAR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vu QD, To MNN, Kim E, Kwak JT, editors. Micro and Macro Breast Histology Image Analysis by Partial Network Re-use. 15th International Conference on Image Analysis and Recognition, ICIAR 2018; 2018.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_KEZabnw">Squeeze-and-Excitation Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2019.2913372</idno>
		<idno type="PMID">31034408</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2019.2913372" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_XsgzsAs">IEEE Trans Pattern Anal Mach Intell</title>
		<imprint>
			<date type="published" when="2019-04-30">2019. 2019/04/30</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hu J, Shen L, Albanie S, Sun G, Wu E. Squeeze-and-Excitation Networks. IEEE Trans Pattern Anal Mach Intell. 2019. Epub 2019/04/30. https://doi.org/10.1109/TPAMI.2019.2913372 PMID: 31034408.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_xxZZfDM">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><surname>Editors</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-24574-4_28</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_C7UrQzX">18th International Conference on Medical Image Computing and Computer-Assisted Intervention, MICCAI</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ronneberger O, Fischer P, Brox T, editors. U-net: Convolutional networks for biomedical image seg- mentation. 18th International Conference on Medical Image Computing and Computer-Assisted Inter- vention, MICCAI 2015; 2015.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_DreEKMb">Cancer Metastasis Detection With Neural Conditional Random Field</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><forename type="middle">W</forename></persName>
		</author>
		<idno>arXiv:</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_M7P83YH">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li Y, Ping W. Cancer Metastasis Detection With Neural Conditional Random Field. arXiv: Computer Vision and Pattern Recognition. 2018.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main" xml:id="_x57uTgZ">Adaptive Weighting Multi-Field-Of-View CNN for Semantic Segmentation in Pathology. computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Teramoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoshizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bise</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.01288</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tokunaga H, Teramoto Y, Yoshizawa A, Bise R, editors. Adaptive Weighting Multi-Field-Of-View CNN for Semantic Segmentation in Pathology. computer vision and pattern recognition; 2019.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_x7Cb5S6">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="DOI">10.1109/Cvpr.2017.195</idno>
		<ptr target="https://doi.org/10.1109/Cvpr.2017.195WOS:000418371401090" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_WkjvnW6">Proc Cvpr Ieee</title>
		<meeting>Cvpr Ieee</meeting>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chollet F. Xception: Deep Learning with Depthwise Separable Convolutions. Proc Cvpr Ieee. 2017:1800-7. https://doi.org/10.1109/Cvpr.2017.195 WOS:000418371401090.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_agfrsum">Graph CNN for Survival Analysis on Whole Slide Pathological Images</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-00934-2%5F20</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-00934-2_20WOS:000477921700020" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Pm9CqrR">Lect Notes Comput Sc</title>
		<imprint>
			<biblScope unit="volume">11071</biblScope>
			<biblScope unit="page" from="174" to="182" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li RY, Yao JW, Zhu XL, Li YQ, Huang JZ. Graph CNN for Survival Analysis on Whole Slide Pathological Images. Lect Notes Comput Sc. 2018; 11071:174-82. https://doi.org/10.1007/978-3-030-00934-2_20 WOS:000477921700020.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_DHP9RUd">RMDL: Recalibrated multi-instance deep learning for whole slide gastric image classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.101549</idno>
		<idno type="PMID">31499320</idno>
		<ptr target="https://doi.org/10.1016/j.media.2019.101549" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_FcvYNC7">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101549</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang S, Zhu Y, Yu L, Chen H, Lin H, Wan X, et al. RMDL: Recalibrated multi-instance deep learning for whole slide gastric image classification. Medical Image Analysis. 2019; 58:101549. https://doi.org/10. 1016/j.media.2019.101549 PMID: 31499320</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_H466mh8">Gastric histopathology image segmentation using a hierarchical conditional random field</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bbe.2020.09.008</idno>
		<ptr target="https://doi.org/10.1016/j.bbe.2020.09.008" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_AyWBaX2">Biocybernetics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1535" to="1555" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sun C, Li C, Zhang J, Rahaman MM, Ai S, Chen H, et al. Gastric histopathology image segmentation using a hierarchical conditional random field. Biocybernetics and Biomedical Engineering. 2020; 40 (4):1535-55. https://doi.org/10.1016/j.bbe.2020.09.008.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_tEwekYH">Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer</title>
		<author>
			<persName><forename type="first">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Van Diest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karssemeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.14585</idno>
		<ptr target="https://doi.org/10.1001/jama.2017.14585PMID:29234806" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_sk46ztJ">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2199" to="2210" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ehteshami Bejnordi B, Veta M, Johannes van Diest P, van Ginneken B, Karssemeijer N, Litjens G, et al. Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer. JAMA. 2017; 318(22):2199-210. https://doi.org/10.1001/jama.2017. 14585 PMID: 29234806</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main" xml:id="_wnh8u6H">Rethinking Feature Distribution for Loss Functions in Image Classification. computer vision and pattern recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00950</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wan W, Zhong Y, Li T, Chen J, editors. Rethinking Feature Distribution for Loss Functions in Image Classification. computer vision and pattern recognition; 2018.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_qesN7Na">SLIC Superpixels Compared to State-ofthe-Art Superpixel Methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2012.120</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2012.120PMID:22641706" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_naUkywU">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Achanta R, Shaji A, Smith K, Lucchi A, Fua P, Susstrunk S. SLIC Superpixels Compared to State-of- the-Art Superpixel Methods. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2012; 34 (11):2274-82. https://doi.org/10.1109/TPAMI.2012.120 PMID: 22641706</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_jgZHRRm">An improved low discrepancy sequence for multidimensional quasi Monte Carlo integration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Braaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weller</surname></persName>
		</author>
		<idno type="DOI">10.1016/0021-9991(79)90019-6</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4Cfwh3g">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="258" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Braaten E, Weller G. An improved low discrepancy sequence for multidimensional quasi Monte Carlo integration. Journal of Computational Physics. 1979; 33(2):249-58.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_wmguFZZ">Generalized Halton sequences in 2008: A comparative study</title>
		<author>
			<persName><forename type="first">H</forename><surname>Faure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lemieux</surname></persName>
		</author>
		<idno type="DOI">10.1145/1596519.1596520</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4axfDTz">ACM Transactions on Modeling and Computer Simulation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Faure H, Lemieux C. Generalized Halton sequences in 2008: A comparative study. ACM Transactions on Modeling and Computer Simulation. 2009; 19(4):15.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_yexMkMb">Evolutionary optimization of low-discrepancy sequences</title>
		<author>
			<persName><forename type="first">F</forename><surname>De Rainville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teytaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laurendeau</surname></persName>
		</author>
		<idno type="DOI">10.1145/2133390.2133393</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8uC7Gtn">ACM Transactions on Modeling and Computer Simulation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">De Rainville F, Gagne C, Teytaud O, Laurendeau D. Evolutionary optimization of low-discrepancy sequences. ACM Transactions on Modeling and Computer Simulation. 2012; 22(2):9.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main" xml:id="_hfQhqpw">Web-scale k-means clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772862</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1177" to="1178" />
		</imprint>
	</monogr>
	<note>the web conference</note>
	<note type="raw_reference">Sculley D. Web-scale k-means clustering. the web conference. 2010:1177-8.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<idno type="DOI">10.21428/55f96fb6</idno>
		<ptr target="https://camelyon16.grand-challenge.org/" />
		<title level="m" xml:id="_Mb2F48v">The Camelyon16 ISBI challenge [2019-11-8</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">The Camelyon16 ISBI challenge [2019-11-8]. Available from: https://camelyon16.grand-challenge.org/.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_E4gqMXz">A Simple and Effective Lagrangian-Based Combinatorial Algorithm for S3VMs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bagattini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cappanera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schoen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-72926-8_21</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer International Publishing</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Bagattini F, Cappanera P, Schoen F, editors. A Simple and Effective Lagrangian-Based Combinatorial Algorithm for S3VMs 2018; Cham: Springer International Publishing.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_wWQta92">Lagrangean-Based Combinatorial Optimization for Large-Scale S3VMs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bagattini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cappanera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schoen</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2017.2766704</idno>
		<idno type="PMID">29990111</idno>
		<ptr target="https://doi.org/10.1109/TNNLS.2017.2766704" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Bq7NUYP">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4426" to="4435" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bagattini F, Cappanera P, Schoen F. Lagrangean-Based Combinatorial Optimization for Large-Scale S3VMs. IEEE Transactions on Neural Networks and Learning Systems. 2018; 29(9):4426-35. https:// doi.org/10.1109/TNNLS.2017.2766704 PMID: 29990111</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_AAaCv7n">Detecting Cancer Metastases on Gigapixel Pathology Images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Gadepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvprw50498.2020.00138</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2bpugqd">arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu Y, Gadepalli KK, Norouzi M, Dahl GE, Kohlberger T, Venugopalan S, et al. Detecting Cancer Metas- tases on Gigapixel Pathology Images. arXiv: Computer Vision and Pattern Recognition. 2017.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main" xml:id="_eZqFZHE">Isolation Forest. international conference on data mining</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu FT, Ting KM, Zhou Z, editors. Isolation Forest. international conference on data mining; 2008.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_rHQeYms">Fast ScanNet: Fast and Dense Analysis of Multi-Gigapixel Whole-Slide Images for Cancer Metastasis Detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2019.2891305</idno>
		<ptr target="https://doi.org/10.1109/TMI.2019.2891305PMID:30624213" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_RacfWMS">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1948" to="1958" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lin H, Chen H, Graham S, Dou Q, Rajpoot NM, Heng P. Fast ScanNet: Fast and Dense Analysis of Multi-Gigapixel Whole-Slide Images for Cancer Metastasis Detection. IEEE Transactions on Medical Imaging. 2019; 38(8):1948-58. https://doi.org/10.1109/TMI.2019.2891305 PMID: 30624213</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
