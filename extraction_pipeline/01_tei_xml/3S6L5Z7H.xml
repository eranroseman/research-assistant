<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_XPp3vxr">Group-driven Reinforcement Learning for Personalized mHealth Intervention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of CSE , University of Texas at Arlington , TX , 76013 , USA</note>
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<postCode>76013</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Statistics , Univeristy of Michigan , Ann Arbor , MI 48109 , USA</note>
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Univeristy of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Statistics , Univeristy of Michigan , Ann Arbor , MI 48109 , USA</note>
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Univeristy of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of CSE , University of Texas at Arlington , TX , 76013 , USA</note>
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<postCode>76013</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Department of Statistics , Univeristy of Michigan , Ann Arbor , MI 48109 , USA</note>
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Univeristy of Michigan</orgName>
								<address>
									<postCode>48109</postCode>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Department of CSE , University of Texas at Arlington , TX , 76013 , USA</note>
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<postCode>76013</postCode>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_Bwhd5MF">Group-driven Reinforcement Learning for Personalized mHealth Intervention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D7C98736B168ECC1784FB0536E28FCD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_qT2Xhhq"><p xml:id="_H3Mmafx"><s xml:id="_kPAgxZ9">Due to the popularity of smartphones and wearable devices nowadays, mobile health (mHealth) technologies are promising to bring positive and wide impacts on people's health.</s><s xml:id="_GmUdjzE">State-of-the-art decisionmaking methods for mHealth rely on some ideal assumptions.</s><s xml:id="_JebRSjG">Those methods either assume that the users are completely homogenous or completely heterogeneous.</s><s xml:id="_hwptWsC">However, in reality, a user might be similar with some, but not all, users.</s><s xml:id="_jhMnrtD">In this paper, we propose a novel group-driven reinforcement learning method for the mHealth.</s><s xml:id="_Zfa8Vr6">We aim to understand how to share information among similar users to better convert the limited user information into sharper learned RL policies.</s><s xml:id="_mPsEf7k">Specifically, we employ the K-means clustering method to group users based on their trajectory information similarity and learn a shared RL policy for each group.</s><s xml:id="_tg3w32U">Extensive experiment results have shown that our method can achieve clear gains over the state-of-the-art RL methods for mHealth.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_m9SSRZW">Introduction</head><p xml:id="_4UGG4BW"><s xml:id="_RQRBMfz">In the wake of the vast population of smart devices 1 users worldwide, mobile health (mHealth) technologies become increasingly popular among the scientist communities.</s><s xml:id="_tyU3P4Z">The goal of mHealth is to use smart devices as great platforms to collect and analyze raw data (weather, location, social activity, stress, etc.).</s><s xml:id="_6qtFeRt">Based on that, the aim is to provide in-time interventions to device users according to their ongoing status and changing needs, helping users to lead healthier lives, such as reducing the alcohol abuse <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> and the obesity management <ref type="bibr" target="#b14">[15]</ref>.</s></p><p xml:id="_RM3cSKd"><s xml:id="_eHuaZdu">Formally, the tailoring of mHealth intervention is modeled as a sequential decision making (SDM) problem.</s><s xml:id="_SWD9GY9">It aims to learn the optimal decision rule to decide when, where and how to deliver interventions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref> to best serve users.</s><s xml:id="_jrCdwsv">This is a new research topic.</s><s xml:id="_Ab5At7k">Currently, there are two types of reinforcement learning (RL) methods for mHealth with distinct assumptions: (a) the off-policy, batch RL <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> assumes that all users in the mHealth are completely homogenous: they share all information and learn an identical RL for all the users; (b) the on-policy, online RL <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23]</ref> assumes that all users are completely different: they share no information and run a separate RL for each user.</s><s xml:id="_mbaVwah">The above assumptions are good as a start for the mHealth study.</s><s xml:id="_NwSh4aH">However, 1 smartphones and wearable devices such as the Fitbit Fuelband and Jawbone etc. arXiv:1708.04001v1</s><s xml:id="_hNA95Tq">[cs.LG] 14 Aug 2017 when mHealth are applied to more practical situations, they have the following drawbacks: (a) the off-policy, batch RL method ignore the fact that the behavior of all users may be too complicated to be modeled with an identical RL, which leads to potentially large biases in the learned policy; (b) for the on-policy, online RL method, an individual user's trajectory data is hardly enough to support a separate RL learning, which is likely to result in unstable policies that contain lots of variances.</s></p><p xml:id="_taxTwQQ"><s xml:id="_56YDjP6">A more realistic assumption lies between the above two extremes: a user may be similar to some, but not all, users and similar users tend to have similar behaviors <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>.</s><s xml:id="_RvH8xW6">In this paper, we propose a novel group driven RL for the mHealth.</s><s xml:id="_eADPFCF">It is in an actor-critic setting <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_PBux3YF">The core idea is to find the similarity (cohesion) network for the users.</s><s xml:id="_Dkbnkgm">Specifically, we employ the clustering method to mine the group information.</s><s xml:id="_Gwkb7cD">Taking the group information into consideration, we learn K (i.e. the number of groups) shared RLs for K groups of users respectively; each RL learning procedure makes use of all the data in that group.</s><s xml:id="_C8r2QSN">Such implementation balances the conflicting goals of reducing the complexity of data while enriching the number of samples for each RL learning process.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_YJ3NMHm">Preliminaries</head><p xml:id="_zwyHWYf"><s xml:id="_PyZUaAn">The Markov Decision Process (MDP) provides a mathematical tool to model the dynamic system <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref>.</s><s xml:id="_sbPsKpG">It is defined as a 5-tuple {S, A, P, R, γ}, where S is the state space and A is the (finite) action space.</s><s xml:id="_na8ttVD">The state transition model P : S × A × S → [0, 1] indicates the probability of transiting from one state s to another s under a given action a. R : S × A → R is the corresponding reward, which is assumed to be bounded over the state and action spaces.</s><s xml:id="_GQYFnxN">γ ∈ [0, 1) is a discount factor that reduces the influence of future rewards.</s><s xml:id="_UMSW77J">The stochastic policy π (• | s) determines how the agent acts with the system by providing each state s with a probability distribution over all the possible actions.</s><s xml:id="_6WhGnMX">We consider the parameterized stochastic policy, i.e., π θ (a | s), where θ is the unkown coefficients.</s></p><p xml:id="_MhdF4UJ"><s xml:id="_zuaSMzg">Formally, the quality of a policy π is evaluated by a value function Q π (s, a) ∈ R |S|×|A| .</s><s xml:id="_msNW3R4">It specifies the total amount of rewards (called return) an agent can achieve when starting from state s, first choosing action a and then following the policy π.</s><s xml:id="_JKwFusW">It is defined as follows <ref type="bibr" target="#b3">[4]</ref>:</s></p><formula xml:id="formula_0">Q π (s, a) = E ai∼π,si∼P ∞ t=0 γ t R (s t , a t ) | s 0 = s, a 0 = a .<label>(1)</label></formula><p xml:id="_nbdcd9m"><s xml:id="_wEshD44">The goal of various RL methods is to learn an optimal policy π * that maximizes the Q-value for all the state-action pairs <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_xtZWt8P">The objective is π θ * = arg max θ J(θ).</s><s xml:id="_d3yQbbq">Such procedure is called the actor updating <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_M2qV9jm">Here</s></p><formula xml:id="formula_1">J (θ) = s∈S d ref (s) a∈A π θ (a | s) Q π θ (s, a) ,<label>(2)</label></formula><p xml:id="_SH87ZHb"><s xml:id="_cyz5yfb">where d ref (s) is a reference distribution over states; Q π θ (s, a) is the value for the parameterized policy π θ .</s><s xml:id="_mhwDaJN">It is obvious that we need the estimation of Q π θ (s, a) (i.e.</s><s xml:id="_E7Kwus5">critic updating) to determine the objective function <ref type="bibr" target="#b1">(2)</ref>.</s><s xml:id="_nmhS6H8">Since in mHealth the state space is very large, it is impossible to directly estimate the Q-value because of the high storage requirement.</s><s xml:id="_A2bYcCW">Instead, the linear approximation alleviates this problem by assuming that Q π is in a low dimensional space <ref type="bibr" target="#b15">[16]</ref>: Q w = w T x (s, a) ≈ Q π where x (s, a) is a feature processing step that combines the information in the state and action.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_G9Mrm4f">Cohesion Discovery for the RL learning</head><p xml:id="_QpuAycp"><s xml:id="_FAWBZdK">Given a set of N users, each user is with a trajectory of T points.</s><s xml:id="_kV2HEk4">Thus in total, we have</s></p><formula xml:id="formula_2">N T = N × T tuples D = {D n | n = 1, • • • , N } for all the N users, where D n = {U i = (s i , a i, r i , s i ) | i = 1, • • • , T } summarizes all the T tuples for the n-th user and U i = (s i , a i, r i , s i ) is the i-th tuple in D n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_WjE9t3C">Pooled-RL and Separate RL (Separ-RL)</head><p xml:id="_s5eXW63"><s xml:id="_69Vd9eM">The first type of RL methods (i.e.</s><s xml:id="_hkRmWs5">Pooled-RL) assumes that all the N users are completely homogenous (i.e.</s><s xml:id="_BKkZh5C">same MDPs); they share all information and run an identical RL for all users <ref type="bibr" target="#b12">[13]</ref>.</s><s xml:id="_Uen2YaZ">In this setting, the critic updating, with an aim of seeking for solutions to satisfy the Linear Bellman equation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, is</s></p><formula xml:id="formula_3">w = f (w) = arg min h 1 |D| U i ∈D x (si, ai) h -ri + γy s i ; θ w 2 2 + ζc h 2 2 ,<label>(3)</label></formula><p xml:id="_MavCS9X"><s xml:id="_TEjb3Hr">where w = f (w) is a fixed point problem; x i = x (s i , a i ) is the value feature at time point i;</s></p><formula xml:id="formula_4">y i = y (s i ; θ) = a∈A x (s i , a) π θ (a | s i )</formula><p xml:id="_dZkRDy5"><s xml:id="_pZUDJ35">is the feature at the next time point; ζ c is a tuning parameter.</s><s xml:id="_D6ZY4EX">The least-square temporal difference for Q-value (LSTDQ) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> provides a closed-form solver for (3) as follows</s></p><formula xml:id="formula_5">w =   ζcI + 1 |D| U i ∈D xi (xi -γyi)   -1   1 |D| U i ∈D xiri   .<label>(4)</label></formula><p xml:id="_CaWJ34G"><s xml:id="_5zXbyWy">Since</s></p><formula xml:id="formula_6">d ref (s) is generally unavailable, the T -trial objective for (2) is θ = arg max θ 1 |D| Ui∈D a∈A Q (s i , a; w) π θ (a|s i ) - ζ a 2 θ 2 2 ,<label>(5)</label></formula><p xml:id="_avXGTb2"><s xml:id="_NcPss4d">where Q (s i , a; w) = x (s i , a) w is the newly estmated Q-value via the critic updating (4) and ζ a is the tuning parameter to prevent overfitting 2 .</s><s xml:id="_4z8rgfp">The Pooled-RL works well when all the N users are very similar.</s><s xml:id="_2ER3MtE">However, there are great behavior discrepancies among users in the mHealth study because they have different ages, races, incomes, religions, education levels etc.</s><s xml:id="_mAKpg2v">Such case makes the current Pooled-RL too simple to simultaneously fit all the N different users' behaviors.</s><s xml:id="_UU2gaZh">It easily results in lots of biases in the learned value and policy.</s><s xml:id="_gBXYEAH">The state-of-the-art deep learning methods can be a great idea to deal with this problem <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20]</ref>.</s></p><p xml:id="_q3Gbcmx"><s xml:id="_ymFg92T">The second type of RL methods (Separ-RL), such as Lei's online contextual bandit for mHealth <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref>, assume that all users are completely heterogeneous.</s><s xml:id="_6dYCxJu">They share no information and run a separate online RL for each user.</s><s xml:id="_fNcJkq6">The objective functions are very similar with (3), ( <ref type="formula" target="#formula_5">4</ref>), <ref type="bibr" target="#b4">(5)</ref>.</s><s xml:id="_qPD4sHB">This method should be great when the data for each user is very large in size.</s><s xml:id="_xasG4mx">In this case, the learned policy is expected be successfully adapted to the user's status.</s><s xml:id="_XEgWYvH">However, it generally costs a lot (time and other resources) to collect enough data for the Separ-RL learning.</s><s xml:id="_XK7BQWt">Taking the HeartSteps for example, it takes 42 days to do the trial collecting 210 tuples per user.</s><s xml:id="_qCmfDhN">What is worse, there are missing and noises in the data, which will surely reduce the effective sample size.</s><s xml:id="_Unf4MyT">The problem of small sample size will easily lead to some unstable policies that contain lots of variances.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_2PpeSj5">Group driven RL learning (Gr-RL)</head><p xml:id="_DaGQbNb"><s xml:id="_UM3cnQd">We observe that users in mHealth are generally similar with some (but not all) users in the sense that they may have some similar features, such as age, gender, race, religion, education level, income and other socioeconomic status <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_jaWP2B3">To this end, we propose a group based RL for mHealth to understand how to share information across similar users to improve the performance.</s><s xml:id="_YvkhUfm">Specifically, the users are assumed to be grouped together and likely to share information with others in the same group.</s><s xml:id="_rmCB9ux">The main idea is to divide the N users into K groups, and learn a separate RL model for each group.</s><s xml:id="_GgBwBPG">The samples of users in a group are pooled together, which not only ensures the simplicity of the data for each RL learning compared with that of the Pooled-RL, but also greatly enriches the samples for the RL learning compared with that of the Separ-RL, with an average increase of (N/K -1) × 100% on sample size (cf.</s><s xml:id="_TzcbD8p">Section 3.1).</s></p><p xml:id="_TwDarbn"><s xml:id="_kjJB5wM">To cluster the N users, we employ one of the most benchmark clustering method, i.e., K-means.</s><s xml:id="_DuH8tVX">The behavior information (i.e.</s><s xml:id="_sDddrdR">states and rewards) in the trajectory is processed as the feature.</s><s xml:id="_Q2yC3Cv">Specifically, the T tuples of a user are stacked together</s></p><formula xml:id="formula_7">z n = [s 1 , r 1 , • • • , s T , r T ] .</formula><p xml:id="_XdPqvyA"><s xml:id="_fXm8kWx">With this new feature, we have the objective for clustering as</s></p><formula xml:id="formula_8">J = N n=1 K k=1 r nk z n -µ k 2</formula><p xml:id="_mSM2n8n"><s xml:id="_RpJH8BF">, where µ k is the k-th cluster center and r nk ∈ {0, 1} is the binary indicator variable that describes which of the K clusters the data z n belongs to.</s><s xml:id="_hcdwaVW">After the clustering step, we have the group information</s></p><formula xml:id="formula_9">{G k | k = 1, • • • , K}, each</formula><p xml:id="_a74CFPT"><s xml:id="_ywytd8t">of which includes a set of similar users.</s><s xml:id="_q5yeTeH">With the clustering results, we have the new objective for the critic updating as</s></p><formula xml:id="formula_10">w k = f (w k ) = h * k , where h * k is estimated from min h k 1 |G k | Ui∈G k x i h k -(r i + γy i w k ) 2 2 + ζ c h k 2 2 , for k ∈ {1, • • • , K} (6)</formula><p xml:id="_FxxB3Mp"><s xml:id="_cGyeB5M">which could be solved via the LSTDQ.</s><s xml:id="_7Wq29UK">The objective for the actor updating is max</s></p><formula xml:id="formula_11">θ k 1 |G k | Ui∈G k a∈A Q (s i , a; w k ) π θ k (a|s i ) - ζ a 2 θ k 2 2 , for k ∈ {1, • • • , K} .<label>(7</label></formula><p xml:id="_kVg6hvM"><s xml:id="_CmXrwmV">) By properly setting the value of K, we could balance the conflicting goal of reducing the discrepancy between connected users while increasing the number of samples for each RL learning: (a) a small K is suited for the case where T is small and the users are generally similar; (b) while a large K is adapted to the case where T is large and users are generally different from others.</s><s xml:id="_FRdN3Dz">Besides, we find that the proposed method is a generalization of the conventional Pooled-RL and Separ-RL: (a) when K = 1, the proposed method is equivalent to the Pooled-RL; (b) when K = N , our method is equivalent to the Separ-RL.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_kehTXtx">Experiments</head><p xml:id="_f5mYXaf"><s xml:id="_cxXjPKt">There are three RL methods for comparison: (a) the Pooled-RL that pools the data across all users and learn an identical policy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> for all users; (b) the Separ-RL, which learns a separate RL policy for each user by only using his or her data <ref type="bibr" target="#b9">[10]</ref>; (c) The group driven RL (Gr-RL) is the proposed method.</s></p><p xml:id="_EhGpETh"><s xml:id="_KftY2MK">The HeartSteps<ref type="foot" target="#foot_1">foot_1</ref> is used for the evaluation.</s><s xml:id="_TW8WQge">It is a 42-day mHealth intervention that aims to increase the users' steps they take everyday by providing some positive suggestions, such as going for a walk after long sitting <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_YqymQCq">In our study, there are two choices for a policy {0, 1}: a = 1 indicates sending the positive intervention, while a = 0 means no intervention <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_nXRpUat">Specifically, the parameterized stochastic policy is assumed to be in the form π θ (a | s) = exp[-θ φ(s,a)] a exp[-θ φ(s,a)] , where θ ∈ R q is the unkown variance and φ (•, •) is the feature processing method for the parameterized policy , i.e., φ (s, a) = [as , a] ∈ R m .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_XQrpXPJ">Experiments Settings</head><p xml:id="_Pw6yRUm"><s xml:id="_BgBDx8T">A trajectory of T tuples D T = {(s i , a i , r i )} T i=1 are collected from each user via the micro-randomized trial <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref>.</s><s xml:id="_RZ4Qtxh">The initial state is sampled from the Gaussian distribution S 0 ∼ N p {0, Σ}, where Σ is the p × p covariance matrix with predefined elements.</s><s xml:id="_c2XeUzn">The policy of selecting action a t = 1 is drawn from the random policy with a probability of 0.5 to provide interventions, i.e. µ (1 | s t ) = 0.5 for all states s t .</s><s xml:id="_ba5TjfF">For t ≥ 1, the state and immediate reward are generated as follows  where β = {β i } 14 i=1 are the main parameters for the MDP; {ξ t,i } p i=1 ∼ N 0, σ 2 s is the noise in the state (9) and t ∼ N 0, σ 2 r is the noise in the reward model <ref type="bibr" target="#b8">(9)</ref>.</s><s xml:id="_ZwS88Xn">To simulate N users that are similar but not identical, we need N different βs, each of which is similar with a set of others.</s><s xml:id="_tcKntJv">Formally, there are two steps to obtain β for the i-th user: (a) select the m-th basic β, i.e. β basic m , which determines which group the i-th user belong to; (b) add the noise 1,268.6 1,267.3</s><s xml:id="_MZnWNY8">1,266.3</s><s xml:id="_zKEptp6">1,255.3</s><s xml:id="_d2eAhUY">1,357.6 1,348.9</s><s xml:id="_mx9cTZz">1,279 1,441.3</s><s xml:id="_XWSF5Nw">1,538.6 1,289.5 1,446.3</s><s xml:id="_3grFqaq">1,500.6</s></p><formula xml:id="formula_12">S t,1 = β 1 S t-1,1 + ξ t,1 , S t,2 = β 2 S t-1,2 + β 3 A t-1 + ξ t,2 ,<label>(8)</label></formula><formula xml:id="formula_13">S t,3 = β 4 S t-1,3 + β 5 S t-1,3 A t-1 + β 6 A t-1 + ξ t,</formula><formula xml:id="formula_14">β i = β basic m + δ i , for i ∈ {1, 2, • • • , N m }</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_XS37ZVq">Evaluation Metric and Results</head><p xml:id="_xesxBpe"><s xml:id="_Bn6uy7R">In the experiments, the expectation of long run average reward (ElrAR) E [η π θ ] is proposed to evaluate the quality of a learned policy π θ <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</s><s xml:id="_Q4fBJ9a">Intuitively in the HeartSteps application, ElrAR measures the average step a user could take each day when he or she is provided by the intervention via the learned policy π θ .</s><s xml:id="_Syqd8MG">Specifically, there are two steps to achieve the ElrAR <ref type="bibr" target="#b13">[14]</ref>: (a) get the η π θ for each user by averaging the rewards over the last 4, 000 elements in the long run trajectory with a total number of 5, 000 tuples; (a) ElrAR E [η π θ ] is achieved by averaging over the η π θ 's of all users.</s><s xml:id="_6XupRrg">The experiment results of three RL methods are summarized in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_2">1</ref>, that is: (a) Pooled-RL, (b) Separ-RL, (c) Gr-RL K=3 and Gr-RL K=7 .</s><s xml:id="_2B8CK7r">K = 3, 7 is the number of cluster centers in our algorithm, which is set different from the true number of groups M = 5.</s><s xml:id="_GApK7u9">Such setting is to show that Gr-RL does not require the true value of M .</s><s xml:id="_xXDxxXZ">There are two sub-tables in Table <ref type="table" target="#tab_0">1</ref>.</s><s xml:id="_NJ7kRNm">The top sub-table summarizes the experiment results of three RL methods under six γ settings (i.e. the discount reward) when the trajectory is short, i.e.</s><s xml:id="_RQDUn3D">T = 42.</s><s xml:id="_qSbDYsn">While the bottom one displays the results when the trajectory is long, i.e.</s><s xml:id="_fQrMykK">T = 100.</s><s xml:id="_gudpVV5">Each row shows the results under one discount factor, γ = 0, • • • , 0.95; the last row shows the average perforance over all the six γ settings.</s></p><p xml:id="_by6CUcK"><s xml:id="_Yt9qVaX">As we shall see, Gr-RL K=3 and Gr-RL K=7 generally perform similarly and are always among the best.</s><s xml:id="_R4jqwBx">Such results demonstrate that our method doesn't require the true value of groups and is robust to the value of K <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6]</ref>.</s><s xml:id="_gUqXVnW">In average, the proposed method improves the ElrAR by 82.4 and 80.3 steps when T = 42 as well as 49.8 and 51.7 steps when T = 100, compared with the best result of the state-of-the-art methods, i.e.</s><s xml:id="_CrrBVFG">Separ-RL.</s><s xml:id="_NpEp8JJ">There are two interesting observations: (1) the improvement of our method decreases as the trajectory length T increases; (2) when the trajectory is short, i.e.</s><s xml:id="_tMDrVK2">T = 42, it is better to set small Ks, which emphasizes the enriching of dataset; while the trajectory is long, i.e.</s><s xml:id="_SYY8fhu">T = 100, it is better to set large Ks to simplify the data for each RL learning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_qbjea89">Conclusions and Discussion</head><p xml:id="_jyuNgAp"><s xml:id="_Q7RVu4R">In this paper, we propose a novel group driven RL method for the mHealth.</s><s xml:id="_KDWsfNC">Compared with the state-of-the-art RL methods for mHealth, it is based on a more practical assumption that admits the discrepancies between users and assumes that a user should be similar with some (but not all) users.</s><s xml:id="_ReuJ5jv">The proposed method is able to balance the conflicting goal of reducing the discrepancy between pooled users while increasing the number of samples for each RL learning.</s><s xml:id="_6mdXBdA">Extensive experiment results verify that our method gains obvious advantages over the state-of-the-art RL methods in the mHealth.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_Yt4fCFe"><s xml:id="_mWcnkGM">(a) γ = 0 means the contextual bandit<ref type="bibr" target="#b9">[10]</ref>, (b) 0 &lt; γ &lt; 1 indicates the discounted reward RL.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 = 2 = 3 = 4 = 5 =</head><label>12345</label><figDesc><div><p xml:id="_6VsuqCG"><s xml:id="_32Kzq4P">to make each user different from others, where N m indicates the number of users in the m-th group, δ i ∼ N (0, σ b I 14 ) is the noise and I 14 ∈ R 14×14 is an identity matrix.</s><s xml:id="_uWnq6X5">The value of σ b specifies how different the users are.</s><s xml:id="_bAGKMJW">Specially in our experiment, we set M = 5 groups (each group has N m = 10 people, leading to N = 50 users involved in the experiment).</s><s xml:id="_WYVAHFk">The basic βs for the M groups are set as follows β basic [0.40, 0.25, 0.35, 0.65, 0.10, 0.50, 0.22, 2.00, 0.15, 0.20, 0.32, 0.10, 0.45, 800] β basic [0.45, 0.35, 0.40, 0.70, 0.15, 0.55, 0.30, 2.20, 0.25, 0.25, 0.40, 0.12, 0.55, 700] β basic [0.35, 0.30, 0.30, 0.60, 0.05, 0.65, 0.28, 2.60, 0.35, 0.45, 0.45, 0.15, 0.50, 650] β basic [0.55, 0.40, 0.25, 0.55, 0.08, 0.70, 0.26, 3.10, 0.25, 0.35, 0.30, 0.17, 0.60, 500] β basic [0.20, 0.50, 0.20, 0.62, 0.06, 0.52, 0.27, 3.00, 0.15, 0.15, 0.50, 0.16, 0.70, 450] ,Besides, the noises are set σ s = σ r = 1 and σ β = 0.01.</s><s xml:id="_V6cV2Pp">Other variances are p = 3, q = 4, ζ a = ζ c = 0.01.</s><s xml:id="_jqYvWgp">The feature procesing for the value estimation is x (s, a) = [1, s , a, s a] ∈ R 2p+2 for all the compared methods.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc><div><p xml:id="_a4XtktC"><s xml:id="_J4aBFrY">Fig. 1: Average reward of 3 RL methods: a) Pooled-RL, (b) Separ-RL, (c) Gr-RLK=3 and Gr-RLK=7.</s><s xml:id="_cbggStQ">The left sub-figure shows the results when the trajectory is short, i.e.</s><s xml:id="_YUPnuma">T = 42; the right one shows the results when T = 100.</s><s xml:id="_7PRC2BG">A larger value is better.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc><div><p><s xml:id="_cmh2xWj">3 , S t,j = β 7 S t-1,j + ξ t,j , for j = 4, ..., pR t = β 14 × [β 8 + A t × (β 9 + β 10 S t,1 + β 11 S t,2 ) + β 12 S t,1 -β 13 S t,3 + t ] ,(9)The average reward of three RL methods when the discount factor γ changes from 0 to 0.95: (a) Pooled-RL, (b) Separ-RL, (c) Gr-RLK=3 and Gr-RLK=7.A larger value is better.The bold value is the best and the blue italic value is the 2nd best.The value of γ specifies different RL methods:</s></p></div></figDesc><table><row><cell>γ</cell><cell cols="3">Average reward (T = 42) Pooled-RL Separ-RL Gr-RLK=3</cell><cell>Gr-RLK=7</cell></row><row><cell cols="5">0 1268.6±68.2 1255.3±62.3 1279.0±66.6 1289.5±64.5</cell></row><row><cell cols="5">0.2 1268.1±68.3 1287.6±76.8 1318.3±62.5 1337.3±56.7</cell></row><row><cell cols="5">0.4 1267.6±68.4 1347.0±54.1 1368.8±57.6 1389.7±50.7</cell></row><row><cell cols="5">0.6 1267.3±68.5 1357.6±57.9 1441.3±48.2 1446.3±46.7</cell></row><row><cell cols="5">0.8 1266.8±68.7 1369.4±51.6 1513.9±38.8 1484.0±44.5</cell></row><row><cell cols="5">0.95 1266.3±68.7 1348.9±53.4 1538.6±34.3 1500.6±42.8</cell></row><row><cell>Avg.</cell><cell>1267.4</cell><cell>1327.6</cell><cell>1410.0</cell><cell>1407.9</cell></row><row><cell>γ</cell><cell></cell><cell cols="2">Average reward (T = 100)</cell><cell></cell></row><row><cell cols="5">0 1284.4±64.1 1271.1±70.7 1293.5±62.1 1294.9±63.7</cell></row><row><cell cols="5">0.2 1285.8±63.9 1301.2±65.6 1329.6±58.5 1332.9±58.7</cell></row><row><cell cols="5">0.4 1287.1±63.8 1370.1±49.1 1385.5±52.1 1393.0±49.2</cell></row><row><cell cols="5">0.6 1288.5±63.6 1409.3±42.2 1452.9±44.3 1459.6±40.9</cell></row><row><cell cols="5">0.8 1289.9±63.4 1435.0±37.6 1519.0±39.5 1518.0±38.5</cell></row><row><cell cols="5">0.95 1291.2±63.2 1441.9±35.9 1547.2±37.2 1540.6±38.1</cell></row><row><cell>Avg.</cell><cell>1287.8</cell><cell>1371.4</cell><cell>1421.3</cell><cell>1423.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p xml:id="_GYfUceK"><s xml:id="_Uf4af9m">In case of large feature spaces, one can recursively update w via (4) and θ in (5) to reduce the computational cost.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p xml:id="_qWaRhbN"><s xml:id="_KTjd8Dd">i.e., a dataset from the mobile health study, called HeartSteps<ref type="bibr" target="#b1">[2]</ref>, to approximate the generative model.</s></p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_xWzHuFR">Semisupervised hyperspectral image classification via discriminant analysis and robust regression</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/jstars.2015.2471176</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WDMZz5u">IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Cheng, F. Zhu, S. Xiang, Y. Wang, and C. Pan. Semisupervised hyperspectral image classification via discriminant analysis and robust regression. IEEE J. of Selected Topics in Applied Earth Observations and Remote Sensing, 9(2):595-608, 2016.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_n6aV6gZ">Randomised trials for the fitbit generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1740-9713.2015.00863.x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QHRuQgY">Significance</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2016-12">Dec 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Dempsey, P. Liao, P. Klasnja, I. Nahum-Shani, and S. A. Murphy. Randomised trials for the fitbit generation. Significance, 12(6):20 -23, Dec 2016.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_TqsKV5W">Algorithmic survey of parametric value function approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Geist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2013.2247418</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SD4pZvu">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="845" to="867" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Geist and O. Pietquin. Algorithmic survey of parametric value function approx- imation. IEEE Transactions on Neural Networks and Learning Systems, 24(6):845- 867, 2013.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_ZJvTGDg">A survey of actorcritic reinforcement learning: Standard and natural policy gradients</title>
		<author>
			<persName><forename type="first">I</forename><surname>Grondman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A D</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsmcc.2012.2218595</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Cafq3eg">IEEE Trans. Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1291" to="1307" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Grondman, L. Busoniu, G. A. D. Lopes, and R. Babuska. A survey of actor- critic reinforcement learning: Standard and natural policy gradients. IEEE Trans. Systems, Man, and Cybernetics, 42(6):1291-1307, 2012.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_sqePPHt">A smartphone application to support recovery from alcoholism: a randomized clinical trial</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mctavish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B .</forename></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamapsychiatry.2013.4642</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PsVyDMm">JAMA Psychiatry</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Gustafson, F. McTavish, M. Chih, A. Atwood, R. Johnson, M. B. ..., and D. Shah. A smartphone application to support recovery from alcoholism: a ran- domized clinical trial. JAMA Psychiatry, 71(5), 2014.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_dRmQvF2">Learning-based fully 3d face reconstruction from a single image</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/icassp.2016.7471957</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_A37vjUV">Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1651" to="1655" />
		</imprint>
	</monogr>
	<note type="raw_reference">X. Hu, Y. Wang, F. Zhu, and C. Pan. Learning-based fully 3d face reconstruction from a single image. In Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on, pages 1651-1655. IEEE, 2016.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_NHsT4BU">Regularization and feature selection in least-squares temporal difference learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553442</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_py4NsAw">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Z. Kolter and A. Y. Ng. Regularization and feature selection in least-squares temporal difference learning. In International Conference on Machine Learning (ICML), pages 521-528, 2009.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_qQCM9m4">Least-squares policy iteration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-30164-8_468</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dgv7Dgx">J. of Machine Learning Research (JLMR)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. G. Lagoudakis and R. Parr. Least-squares policy iteration. J. of Machine Learning Research (JLMR), 4:1107-1149, 2003.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_5bbJJkc">An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<idno type="DOI">10.1109/tevc.2025.3529503/mm1</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">H. Lei. An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention. PhD thesis, University of Michigan, 2016.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_WR76bR9">An actor-critic contextual bandit algorithm for personalized interventions using mobile devices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FmgRA25">NIPS 2014 Workshop: Personalization: Methods and Applications</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Lei, A. Tewari, and S. Murphy. An actor-critic contextual bandit algorithm for personalized interventions using mobile devices. In NIPS 2014 Workshop: Person- alization: Methods and Applications, pages 1 -9, 2014.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_3vwRbsj">A label propagation method using spatial-spectral consistency for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1080/01431161.2015.1125547</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zNAFcNq">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="191" to="211" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Li, Y. Wang, S. Xiang, J. Duan, F. Zhu, and C. Pan. A label propagation method using spatial-spectral consistency for hyperspectral image classification. International Journal of Remote Sensing, 37(1):191-211, 2016.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_335h8Zr">Prediction models for network-linked data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno>CoRR, abs/1602.01192</idno>
		<imprint>
			<date type="published" when="2016-02">February 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Li, E. Levina, and J. Zhu. Prediction models for network-linked data. CoRR, abs/1602.01192, February 2016.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_B7DXSmp">Constructing just-in-time adaptive interventions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ff9sGY7">Phd Section Proposal</title>
		<imprint>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Liao, A. Tewari, and S. Murphy. Constructing just-in-time adaptive interven- tions. Phd Section Proposal, pages 1-49, 2015.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_VD6gwBD">A batch, off-policy, actor-critic algorithm for optimizing the average reward</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<idno>CoRR, abs/1607.05047</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Murphy, Y. Deng, E. B. Laber, H. R. Maei, R. S. Sutton, and K. Witkiewitz. A batch, off-policy, actor-critic algorithm for optimizing the average reward. CoRR, abs/1607.05047, 2016.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_yQ5GgEa">A text message-based intervention for weight loss: randomized controlled trial</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zabinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Griswold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Norman</surname></persName>
		</author>
		<idno type="DOI">10.2196/jmir.1100</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GnQXJDP">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Patrick, F. Raab, M. Adams, L. Dillon, M. Zabinski, C. Rock, W. Griswold, and G. Norman. A text message-based intervention for weight loss: randomized controlled trial. Journal of Medical Internet Research, 11(1), 2009.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main" xml:id="_3rqUQ3z">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
	<note type="raw_reference">R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, USA, 2nd edition, 2012.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_ESknax6">Reinforcement learning for context aware segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Merrifield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-23626-6_77</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RnpDFrM">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="627" to="634" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Wang, R. D. Merrifield, and G. Yang. Reinforcement learning for context aware segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 627-634, 2011.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_Ub36q5Z">Robust hyperspectral unmixing with correntropy-based metric</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7dNnXTp">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4027" to="4040" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Wang, C. Pan, S. Xiang, and F. Zhu. Robust hyperspectral unmixing with correntropy-based metric. IEEE Transactions on Image Processing, 24(11):4027- 4040, 2015.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_vuY5cza">Development and evaluation of a mobile intervention for heavy drinking and smoking among college studen</title>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leigh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirouac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larimer</surname></persName>
		</author>
		<idno type="DOI">10.1037/a0034747</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pn3GeY7">Psychology of Addictive Behaviors</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Witkiewitz, S. Desai, S. Bowen, B. Leigh, M. Kirouac, and M. Larimer. Devel- opment and evaluation of a mobile intervention for heavy drinking and smoking among college studen. Psychology of Addictive Behaviors, 28(3), 2014.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_27QHyaG">Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cZWmt3r">ACM Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<imprint>
			<publisher>ACM-BCB</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Xu, S. Wang, F. Zhu, and J. Huang. Seq2seq fingerprint: An unsupervised deep molecular embedding for drug discovery. In ACM Conference on Bioinformatics, Computational Biology, and Health Informatics (ACM-BCB), 2017.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_FghASwA">Deep correlational learning for survival prediction from multi-modality datay</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-66185-8_46</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AG6WweN">International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Yao, X. Zhu, F. Zhu, and J. Huang. Deep correlational learning for survival prediction from multi-modality datay. In International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), 2017.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_7bqmJ8r">10,000+ times accelerated robust subset selection (ARSS)</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v29i1.9565</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uTuEtcT">Proc. Assoc. Adv. Artif. Intell. (AAAI)</title>
		<meeting>Assoc. Adv. Artif. Intell. (AAAI)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3217" to="3224" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, B. Fan, X. Zhu, Y. Wang, S. Xiang, and C. Pan. 10,000+ times accelerated robust subset selection (ARSS). In Proc. Assoc. Adv. Artif. Intell. (AAAI), pages 3217-3224, 2015.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_HNUMGsx">Effective warm start for the online actor-critic reinforcement learning based mhealth intervention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3233547.3233553</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wHBKfzt">The Multi-disciplinary Conference on Reinforcement Learning and Decision Making</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu and P. Liao. Effective warm start for the online actor-critic reinforcement learning based mhealth intervention. In The Multi-disciplinary Conference on Re- inforcement Learning and Decision Making, pages 6 -10, 2017.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main" xml:id="_KGzPvG8">Cohesion-based online actor-critic reinforcement learning for mhealth intervention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3233547.3233553</idno>
		<idno type="arXiv">arXiv:1703.10039</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, P. Liao, X. Zhu, Y. Yao, and J. Huang. Cohesion-based online actor-critic reinforcement learning for mhealth intervention. arXiv:1703.10039, 2017.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main" xml:id="_vBQ7H4H">Effective spectral unmixing via robust representation and learning-based sparsity</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1109/tip.2014.2363423</idno>
		<idno>CoRR, abs/1409.0685</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, Y. Wang, B. Fan, G. Meng, and C. Pan. Effective spectral unmixing via robust representation and learning-based sparsity. CoRR, abs/1409.0685, 2014.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_cAvVezm">Structured sparse method for hyperspectral unmixing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2013.11.014</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MH8fEwc">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="101" to="118" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, Y. Wang, S. Xiang, B. Fan, and C. Pan. Structured sparse method for hyperspectral unmixing. ISPRS Journal of Photogrammetry and Remote Sensing, 88:101-118, 2014.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_6c5jB5Z">Wsisa: Making survival prediction from whole slide histopathological images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.725</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_w6YVPND">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7234" to="7242" />
		</imprint>
	</monogr>
	<note type="raw_reference">X. Zhu, J. Yao, F. Zhu, and J. Huang. Wsisa: Making survival prediction from whole slide histopathological images. In IEEE Conference on Computer Vision and Pattern Recognition, pages 7234 -7242, 2017.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
