<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_ASTXqJ4">Explainability for artificial intelligence in healthcare: a multidisciplinary perspective</title>
				<funder ref="#_BZUZEen">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Julia</forename><surname>Amann</surname></persName>
							<email>julia.amann@hest.ethz.ch</email>
							<idno type="ORCID">0000-0003-2155-5286</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Health Ethics and Policy Lab , Department of Health Sciences and Technology , ETH Zurich , Hottingerstrasse 10 , 8092 Zurich , Switzerland</note>
								<orgName type="department" key="dep1">Health Ethics and Policy Lab</orgName>
								<orgName type="department" key="dep2">Department of Health Sciences and Technology</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Hottingerstrasse 10</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>1</label> Health Ethics and Policy Lab , Department of Health Sciences and Technol- ogy , ETH Zurich , Hottingerstrasse 10 , 8092 Zurich , Switzerland.</note>
								<orgName type="department" key="dep1">Health Ethics and Policy Lab</orgName>
								<orgName type="department" key="dep2">Department of Health Sciences and Technol- ogy</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Hottingerstrasse 10</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessandro</forename><surname>Blasimme</surname></persName>
							<idno type="ORCID">0000-0003-2155-5286</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Health Ethics and Policy Lab , Department of Health Sciences and Technology , ETH Zurich , Hottingerstrasse 10 , 8092 Zurich , Switzerland</note>
								<orgName type="department" key="dep1">Health Ethics and Policy Lab</orgName>
								<orgName type="department" key="dep2">Department of Health Sciences and Technology</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Hottingerstrasse 10</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>1</label> Health Ethics and Policy Lab , Department of Health Sciences and Technol- ogy , ETH Zurich , Hottingerstrasse 10 , 8092 Zurich , Switzerland.</note>
								<orgName type="department" key="dep1">Health Ethics and Policy Lab</orgName>
								<orgName type="department" key="dep2">Department of Health Sciences and Technol- ogy</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Hottingerstrasse 10</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Effy</forename><surname>Vayena</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Health Ethics and Policy Lab , Department of Health Sciences and Technology , ETH Zurich , Hottingerstrasse 10 , 8092 Zurich , Switzerland</note>
								<orgName type="department" key="dep1">Health Ethics and Policy Lab</orgName>
								<orgName type="department" key="dep2">Department of Health Sciences and Technology</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Hottingerstrasse 10</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>1</label> Health Ethics and Policy Lab , Department of Health Sciences and Technol- ogy , ETH Zurich , Hottingerstrasse 10 , 8092 Zurich , Switzerland.</note>
								<orgName type="department" key="dep1">Health Ethics and Policy Lab</orgName>
								<orgName type="department" key="dep2">Department of Health Sciences and Technol- ogy</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<addrLine>Hottingerstrasse 10</addrLine>
									<postCode>8092</postCode>
									<settlement>Zurich</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dietmar</forename><surname>Frey</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>2</label> Charité Lab for Artificial Intelligence in Medicine-CLAIM , Charité -Universitätsmedizin Berlin , Berlin , Germany.</note>
								<orgName type="department">Charité Lab for Artificial Intelligence in Medicine-CLAIM</orgName>
								<orgName type="institution">Charité -Universitätsmedizin Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vince</forename><forename type="middle">I</forename><surname>Madai</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>2</label> Charité Lab for Artificial Intelligence in Medicine-CLAIM , Charité -Universitätsmedizin Berlin , Berlin , Germany.</note>
								<orgName type="department">Charité Lab for Artificial Intelligence in Medicine-CLAIM</orgName>
								<orgName type="institution">Charité -Universitätsmedizin Berlin</orgName>
								<address>
									<settlement>Berlin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>3</label> School of Computing and Digital Technology , Faculty of Computing, Engineering and the Built Environment , Birmingham City University , Birmingham , UK.</note>
								<orgName type="department" key="dep1">School of Computing and Digital Technology</orgName>
								<orgName type="department" key="dep2">Faculty of Computing, Engineering and the Built Environment</orgName>
								<orgName type="institution">Birmingham City University</orgName>
								<address>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_qA8Zn47">Explainability for artificial intelligence in healthcare: a multidisciplinary perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08ECF2AB66DD3BAECAEE28410744DCE7</idno>
					<idno type="DOI">10.1186/s12911-020-01332-6</idno>
					<note type="submission">Received: 22 July 2020 Accepted: 15 November 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T11:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_SvvxeGz">Artificial intelligence</term>
					<term xml:id="_XHRGHT9">Machine learning</term>
					<term xml:id="_hSNyNWk">Explainability</term>
					<term xml:id="_qA9uSWB">Interpretability</term>
					<term xml:id="_T6zFpCa">Clinical decision support</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_E6m4ZYs"><p xml:id="_kTyBe22"><s xml:id="_M5pQSnG">Background: Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare.</s><s xml:id="_KU7Yzmw">Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism.</s><s xml:id="_WX2g36K">Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration.</s><s xml:id="_6UeeNwQ">This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PASDSCn">Methods:</head><p xml:id="_PNWBR6a"><s xml:id="_VeCJBcj">Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives.</s><s xml:id="_bcmsnWx">Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the "Principles of Biomedical Ethics" by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI.</s></p><p xml:id="_85VzVcx"><s xml:id="_pvqnnfg">Results: Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice.</s><s xml:id="_a5DvaDw">From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective.</s><s xml:id="_z8b8HtX">When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability.</s><s xml:id="_hzprFBe">Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI.</s><s xml:id="_Zn96rwr">We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hhMxw9k">Conclusions:</head><p xml:id="_9Dk5un4"><s xml:id="_QMeffk6">To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_83FETZG">Background</head><p xml:id="_j7XFm2q"><s xml:id="_U4EHRyz">All over the world, healthcare costs are skyrocketing.</s><s xml:id="_4u9j34r">Increasing life expectancy, soaring rates of chronic diseases, and the continuous development of costly new therapies contribute to this trend.</s><s xml:id="_5ZG3VQ5">Thus, it comes as no surprise that scholars predict a grim future for the sustainability of healthcare systems throughout the world.</s><s xml:id="_dYB2QyX">Artificial intelligence (AI) promises to alleviate the impact of these developments by improving healthcare and making it more cost-effective <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_U5U8ygD">In clinical practice, AI often comes in the form of clinical decision support systems (CDSS), assisting clinicians in diagnosis of disease and treatment decisions.</s><s xml:id="_zBA2wpk">Where conventional CDSS match the characteristics of individual patients to an existing knowledge base, AI-based CDSSs apply AI models trained on data from patients matching the use-case at hand.</s><s xml:id="_M3x5bu5">Yet, despite its undeniable potential, AI is not a universal solution.</s><s xml:id="_2JNUmV6">As history has shown, technological progress always goes hand in hand with novel questions and significant challenges.</s><s xml:id="_XgCdHqw">Some of these challenges are tied to the technical properties of AI, others relate to the legal, medical, and patient perspectives, making it necessary to adopt a multidisciplinary perspective.</s></p><p xml:id="_xvbmc2Z"><s xml:id="_5u8W2wB">In this paper, we take such a multidisciplinary view on a major medical AI challenge: explainability.</s><s xml:id="_wdmVeHY">In its essence, explainability can be understood as a characteristic of an AI-driven system allowing a person to reconstruct why a certain AI came up with the presented predictions.</s><s xml:id="_cZKVXE4">An important point to note here is that explainability has many facets and, unfortunately, the terminology of explainability is not well defined.</s><s xml:id="_7vyafDw">Other terms such as interpretability and/or transparency are often used synonymously <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</s><s xml:id="_bN8brEh">We thus simply refer to explainability or explainable AI throughout the manuscript and add the necessary context for understanding.</s></p><p xml:id="_wR3CdhB"><s xml:id="_rER5BXr">Explainability is a heavily debated topic with farreaching implications that extend beyond the technical properties of AI.</s><s xml:id="_eMgp4tW">Even though research indicates that AI algorithms can outperform humans in certain analytical tasks (e.g.</s><s xml:id="_kCnFVJM">pattern recognition in imaging), the lack of explainability has been criticized in the medical domain <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_2exh2T5">Legal and ethical uncertainties surrounding this issue may impede progress and prevent novel technologies from fulfilling their potential to improve patient and population health.</s><s xml:id="_8gBMpGD">Yet, without thorough consideration of the role of explainability in medical AI, these technologies may forgo core ethical and professional principles, disregard regulatory issues, and cause considerable harm <ref type="bibr" target="#b4">[5]</ref>.</s></p><p xml:id="_4n8E5PA"><s xml:id="_JE4kHwB">To contribute to the discourse on explainable AI in medicine, this paper seeks to draw attention to the interdisciplinary nature of explainability and its implications for the future of healthcare.</s><s xml:id="_37Vzzfy">In particular, our work focuses on the relevance of explainability for CDSS.</s><s xml:id="_a4X2XTv">The originality of our work lies in the fact that we look at explainability from multiple perspectives that are often regarded as independent and separable from each other.</s><s xml:id="_bJkXEze">This paper has two central aims: <ref type="bibr" target="#b0">(1)</ref> to provide a comprehensive assessment of the role of explainability in CDSS for use in clinical practice and; <ref type="bibr" target="#b1">(2)</ref> to make an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_5N43E95">Methods</head><p xml:id="_2F5gWEp"><s xml:id="_g4TESdP">Taking AI-based CDSS as a case in point, we discuss the relevance of explainability for medical AI from the technological, legal, medical, and patient perspective.</s><s xml:id="_Uenfk2D">To this end, we performed a conceptual analysis of the pertinent literature on explainable AI in these domains.</s><s xml:id="_XfGnqYh">In our analysis, we aimed to identify aspects relevant to determining the necessity and role of explainability for each domain, respectively.</s><s xml:id="_XU6vrwV">Drawing on these different perspectives, we then conclude by distilling the ethical implications of explainability for the future use of AI in the healthcare setting.</s><s xml:id="_PkjbpPT">We do the latter by examining explainability against the four ethical principles of autonomy, beneficence, non-maleficence, and justice.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BsHvCrj">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mmJJSnZ">The technological perspective</head><p xml:id="_9eE23E2"><s xml:id="_VhVxp5U">From the technological perspective, we will explore two issues.</s><s xml:id="_NkpeQfk">First, what explainability methods are and, second, where they are applied in medical AI development.</s></p><p xml:id="_jHWY5ht"><s xml:id="_XadJuvU">With regards to methodology, explainability can either be an inherent characteristic of an algorithm or can be approximated by other methods <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_DHFTgFM">The latter is highly important for methods that have until recently been labeled as "black-box models" such as artificial neural network (ANN) models.</s><s xml:id="_ssZ3nKV">To explain their predictions, however, numerous methods exist today <ref type="bibr" target="#b5">[6]</ref>.</s><s xml:id="_9BevFZD">Importantly, however, inherent explainability will, in general, be more accurate than methods that only approximate explainability <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_7cDdqHJ">This can be attributed to the complex characteristics of many modern machine learning methods.</s><s xml:id="_trPu59e">In ANNs, for example, the inner workings of sometimes millions of weights between artificial neurons need to be interpreted in a way that humans can understand.</s><s xml:id="_Fm6EGPS">Thus, contrasting methods with inherent explainability have a crucial advantage.</s><s xml:id="_Edjd2b9">However, these methods are usually also traditional methods, such as linear or logistic regression.</s><s xml:id="_a9gtNyV">For many use cases, there is an inferiority of these traditional methods in performance compared to modern state-of-the-art methods such as ANNs <ref type="bibr" target="#b6">[7]</ref>.</s><s xml:id="_z5Kkvkb">Thus, there is a trade-off between performance and explainability, and this trade-off is a big challenge for the developers of clinical decision support systems.</s><s xml:id="_6sC6htS">It should be noted that some assume that this trade-off does not exist in reality, but it is a mere artifact of suboptimal modelling approaches, as pointed out by Rudin et al. <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_TZVbgjK">While the work of Rudin et al. is important to raise attention to the shortcomings of approximating explainability methods, it is likely that some approximating methods, in contrast to the notion of <ref type="bibr" target="#b1">[2]</ref>, have value given the complex nature of explaining machine learning models.</s><s xml:id="_y7cHxaM">Additionally, while we can make the qualitative assessment that inherent explainability is likely better than approximated explainability, there exist only exploratory initial attempts to rank explainability methods quantitatively <ref type="bibr" target="#b7">[8]</ref>.</s><s xml:id="_ugssxRQ">Notwithstanding, for many applications-and generally in AI product development-there is a de facto preference for modern algorithms such as ANNs.</s><s xml:id="_WYqneKW">Additionally, it cannot be ruled out that for some applications such modern methods do exhibit actual higher performance.</s><s xml:id="_t8gZfDB">This necessitates to critically assess explainability methods further, both with regards to technical development, e.g. for methods ranking and optimization of methods for certain inputs, and with regards to the role of explainability from a multiple stakeholder view as done in the current work.</s></p><p xml:id="_hMyQ5qz"><s xml:id="_xWqRvDn">From the development point-of-view, explainability will regularly be helpful for developers to sanity check their AI models beyond mere performance.</s><s xml:id="_4hKaU6D">For example, it is highly beneficial to rule out that the prediction performance is based on meta-data rather than the data itself.</s><s xml:id="_dfHfyQ9">A famous non-medical example was the classification task to discern between huskies and wolves, where the prediction was solely driven by the identification of a snowy background rather than real differences between huskies and wolves <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_5x8zD5F">This phenomenon is also called a "Clever Hans" phenomenon <ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_dUvfxJU">Clever Hans phenomena are also found in medicine.</s><s xml:id="_t2KKqwF">An example is the model developed by researchers from Mount Sinai hospital which performed very well in distinguishing highrisk patients from non-high-risk patients based on x-ray imaging.</s><s xml:id="_AFyUWgF">However, when the tool was applied outside of Mount Sinai, the performance plummeted.</s><s xml:id="_xRD5nCK">As it turned out the AI model did not learn clinically relevant information from the images.</s><s xml:id="_yEvM7Ta">In analogy to the snowy background in the example introduced above, the prediction was based on hardware related meta-data tied to the specific x-ray machine that was used to image the highrisk ICU patients exclusively at Mount Sinai <ref type="bibr" target="#b10">[11]</ref>.</s><s xml:id="_DHbYUnY">Thus, the system was able to distinguish only which machine was used for imaging and not the risk of the patients.</s><s xml:id="_krTfFCU">Explainability methods allow developers to identify these types of errors before AI tools go into clinical validation and the certification process, as the Clever Hans predictors (snowy background, hardware information) would be identified as prediction relevant by the explainability methods rather than meaningful features from a domain perspective.</s><s xml:id="_mFTNtpb">This saves time and development costs.</s><s xml:id="_BtwMyZj">It should be noted that explainability methods aimed at developers to provide insight into their models have different prerequisites than systems aimed at technologically unsavvy end-users such as clinical doctors and patients.</s><s xml:id="_f34aFFq">For developers, these methods can be more complex in their approach and visualization.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_nY9dD8f">The legal perspective</head><p xml:id="_wfmg6Ax"><s xml:id="_9ADcRB2">From the legal perspective, the question arises if and, if yes, to what extent explainability in AI is legally required.</s><s xml:id="_MBESE5D">Taking the cue from other fields such as public administration, transparency and traceability have to meet even higher standards when it comes to health care and the individual patient <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_n6efgYw">As shown above, artificial intelligence approaches such as machine learning and deep learning have the potential to significantly advance the quality of health care.</s><s xml:id="_nfZB3T2">Identifying patterns in diagnostics, anomaly detection and, in the end, providing decision support are already changing standards of care and clinical practice.</s><s xml:id="_hfAVUue">To fully exploit these opportunities for improving patients' outcomes and saving lives by advancing detection, prevention, and treatment of diseases, the sensitive issues of data privacy and security, patient consent, and autonomy have to be fully considered.</s><s xml:id="_q6VrWwm">This means that from a legal perspective, data-its acquisition, storage, transfer, processing, and analysis-will have to comply with all laws, regulations and further legal requirements.</s><s xml:id="_3DuE7sf">In addition, the law and its interpretation and implementation have to constantly adapt to the evolving state-of-the-art in technology <ref type="bibr" target="#b12">[13]</ref>.</s><s xml:id="_VkfEsrx">Even when fulfilling all of these rather obvious requirements the question remains if the application of AI-driven solutions and tools demand explainability.</s><s xml:id="_fQEsrwv">In other words, do doctors and patients need information not only about the results that are provided but also about the characteristics and features these results are based upon, and the respective underlying assumptions.</s><s xml:id="_KXgAEHg">And, might the necessary inclusion of other stakeholders require an understanding and explainability of algorithms and models.</s></p><p xml:id="_kZxYJYd"><s xml:id="_C9mpkEv">From a Western legal point-of-view, we identified three core fields for explainability: (1) Informed consent, (2) Certification and approval as medical devices (acc.</s><s xml:id="_n95jzEK">to Food and Drug Administration/FDA and Medical Device Regulation/MDR) and (3) Liability.</s></p><p xml:id="_yJmBX9R"><s xml:id="_hwxjJs3">Personal health data may be only processed by law after the individual consents to its use.</s><s xml:id="_BQnqbFh">In the absence of general laws facilitating the use of personal data and information, this informed consent is the standard for today's use of patient data in AI applications <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_AbaHBRJ">This is particularly challenging since the consent has to be specified in advance, i.e. the purpose of the given project and its aims have to be outlined.</s><s xml:id="_4cwxwaV">The natural advantage of AI is that it does not necessitate pre-selection of features and can identify novel patterns or find new biomarkers.</s><s xml:id="_pUj42zD">If restricted to specific purposes-as required for informed consent-this unique advantage might not be fully exploitable.</s><s xml:id="_n2YPXnH">For obtaining informed consent for diagnostic procedures or interventions the law requires individual and comprehensive information about and understanding of these processes.</s><s xml:id="_JPxT8cQ">In the case of AI-based decision support, the underlying processes and algorithms have therefore to be explained to the individual patient.</s><s xml:id="_JNyEDXX">Just like in the case of obtaining consent for undergoing an MR imaging procedure, the patient might not necessarily need to know every detail but certainly has to be informed about core principles, and especially the risks.</s><s xml:id="_ckGUTwN">Yet, contrary to an MR imaging procedure, physicians are unable to provide this type of information for an opaque CDSS.</s><s xml:id="_h4sZDE7">What physicians should at least be able to provide are explanations around two principles:</s></p><p xml:id="_Kr2AC3k"><s xml:id="_xwXJWvs">(1) the agent view of AI, i.e. what it takes as input; what it does with the environment; and what it produces as output, and (2) explaining the training of the mapping which produces the output by letting it learn from exampleswhich encompasses unsupervised, supervised, and reinforcement learning.</s><s xml:id="_mxavB3V">Yet, it is important to note that for AI-based CDSS the extent of the information is a priori highly difficult to define, has to be adjusted to the respective use case, and will certainly need clarification from the legislative bodies.</s><s xml:id="_deKBX5E">For this, a framework for defining the "right" level of explainability, as Beaudouin et al. put it <ref type="bibr" target="#b14">[15]</ref>, should be developed.</s><s xml:id="_k5EskwQ">Clearly, this also raises important questions about the role and of physicians, underscoring the need for tailored training and professional development in the area of medical AI.</s></p><p xml:id="_gZAfNYc"><s xml:id="_zCvx8jQ">With regard to certification and approval as medical devices, the respective bodies have been slow to introduce requirements for explainable AI and its implications on the development and marketing of products.</s><s xml:id="_ZUHjS2s">In a recent discussion paper, the FDA facilitates in its total product lifecycle approach (TPLC) the constant development and improvement of AI-based medical products.</s><s xml:id="_eCAxhkN">Explainability is not mentioned but an "Appropriate level of transparency (clarity) of the output and the algorithm aimed at users" is required <ref type="bibr" target="#b15">[16]</ref>.</s><s xml:id="_AnPNNSn">This is mainly aimed at the functions of the software and its modifications over time.</s><s xml:id="_SF7QfkJ">The MDR does not specifically regulate the need for explainability with regard to medical devices that use artificial intelligence and machine learning in particular.</s><s xml:id="_BZH8xJf">However, also here, the need for accountability and transparency are set and the evolution of xAI might lead the legislative and the notified bodies to change the regulations and their interpretation accordingly.</s></p><p xml:id="_4FaSGtx"><s xml:id="_8qmV4qg">In conclusion, both FDA and MDR are currently rather vaguely requiring explainability, i.e. information for traceability, transparency, and explainability of development of ML/DL models that inform medical treatment.</s><s xml:id="_jUuYGR7">Most certainly, these requirements will be defined more precisely in the future mandating producers of AI-based medical devices/software to provide insight into the training and testing of the models, the data, and the overall development processes.</s><s xml:id="_WhdhxkH">We would also like to mention that there is a current debate on whether the General Data Protection Regulation (GDPR) in the European Union requires the use of explainable AI in tools working with patient data <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</s><s xml:id="_dzJHj85">Also here, it cannot be ruled out that the currently ambiguous phrasings will be amended in favor of one that promotes explainability in the future.</s></p><p xml:id="_MGCKd2w"><s xml:id="_jc4waGA">Finally, the question arises, to what extent the patient has to be made aware that treatment decisions such as those derived by a clinical decision support system might rely on AI and the legal and litigation question if the physician adhered to the recommendation or overruled the machine.</s><s xml:id="_KH8q3xG">For the US, as Cohen laid out, there is currently no clear-cut answer to what extent the integration of ML/ DL into clinical decision-making has to be disclosed with regard to liability <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_qq8JkRV">Hacker et al. argue that legally it is likely that explainability will be a prerequisite from a contract and tort law perspective where doctors may have to use a certain tool to avoid the threat of a medical malpractice lawsuit <ref type="bibr" target="#b16">[17]</ref>.</s><s xml:id="_VRKpgmz">The final answer to this lies with the courts, however, and will be given rather sooner than later as an increasing number of AI-based systems will be in use.</s></p><p xml:id="_Wp45Et5"><s xml:id="_yGk4jeE">Taken together, the legal implications of introducing AI technologies into health care are significant and the constant conflict between innovation and regulation needs careful orchestration.</s><s xml:id="_46h235u">Potentially life-saving just as new cancer medication or antibiotics, AI-based decision support needs guidelines and legal crash barriers to avoid existential infringement on patients' rights and autonomy.</s><s xml:id="_Q9RCyUa">Explainability is an essential quality in this context and we would argue that performance is only sufficient in cases, where it is not possible to provide explainability.</s><s xml:id="_jnvJQGu">Overall, there is a strong need for explainability in legal aspects and opening the black box has become essential and will prove to be the watershed moment for the application of AI in medicine.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8zqPfs9">The medical perspective</head><p xml:id="_4cs4GQr"><s xml:id="_j5JQJ4s">From the medical perspective, the first consideration is what distinguishes AI-based clinical decision support from established diagnostic tools, such as advanced laboratory testing for example?</s><s xml:id="_C4BHaqS">Especially as they do exhibit considerable overlaps: Both can provide results used for CDSSs, for both performance is a key issue, and their results are documentable.</s><s xml:id="_XBHEk49">We also understand the inner working of laboratory testing, as it is often the case with other diagnostic tests, such as imaging, so they would not be regarded as black box methods.</s><s xml:id="_J3mA3Jh">On the other hand, for these methods we cannot explain the result of any individual test.</s><s xml:id="_NwxJa55">This makes it evident that from a medical perspective, we need to distinguish two levels of explainability.</s><s xml:id="_2tytksQ">First level explainability allows us to understand how the system arrives at conclusions in general.</s></p><p xml:id="_yyq3zpj"><s xml:id="_UnE2w3q">In analogy to laboratory testing, where we know which biological and biochemical reactions lead to the results, we can provide feature importance rankings that explain which inputs are important for the AI-based CDSSs.</s><s xml:id="_UG8MKYp">Second level explainability allows us to identify which features were important for an individual prediction.</s><s xml:id="_jQkw7bc">Individual predictions can be safe-checked for patterns that might indicate a false prediction, e.g. in case of unusual feature distribution in an out-of-sample case.</s><s xml:id="_VfVg5bQ">This second level explainability will regularly be available for AI-based CDSS but not for other diagnostic tests.</s><s xml:id="_7wfppE3">This also has implications for the presentation of explainability results to doctors (and patients).</s><s xml:id="_PaRuzm2">Depending on the clinical use case and the risk attributed to that particular use case, first level explanations might be sufficient, whereas other use cases will regularly require second level explanations to safe-guard patients.</s></p><p xml:id="_Yv7jHGQ"><s xml:id="_puqDRQx">To date, clinical validation is currently the first widely discussed requirement for a medical AI system.</s><s xml:id="_YStFQh8">Explainability is often only considered on second thought.</s><s xml:id="_hF2Z3cA">The reason for this seems obvious: Medical AI systems and especially CDSSs, whether AI-powered or not, have to undergo a rigorous validation process to meet regulatory standards and achieve medical certification <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_2SKSUc2">Once this process is completed successfully, there is proof that the system can perform in the highly heterogeneous real-world clinical setting.</s><s xml:id="_BJcAhVp">Here, it is important to understand how clinical validation is measured.</s><s xml:id="_ahDb2Km">A common performance indicator is prediction performance, often referred to as prediction accuracy.</s><s xml:id="_ja8fGKy">Different measures exist for prediction accuracy, tailored to certain usecases, but their common characteristic is that they reflect the prediction quality and thus general clinical usefulness of a model.</s><s xml:id="_QPwxdXv">Thus, one of the main goals of model development is to increase prediction performance and provide low error rates.</s><s xml:id="_d9nTYh8">And, indeed, AI-powered systems have been shown to produce overall lower error rates than traditional methods <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>.</s></p><p xml:id="_Z4SnVrq"><s xml:id="_Mt82fa3">Despite all efforts, however, AI systems cannot provide perfect accuracy owing to different sources of error.</s><s xml:id="_GKRZsn4">For one, because of naturally imperfect datasets in medicine (e.g.</s><s xml:id="_A3u7DUG">due to noise or recording errors), it is basically impossible to develop a model without any errors.</s><s xml:id="_YBA7rmT">These errors are random errors.</s><s xml:id="_sqxrT33">Thus, there will always be certain cases of false positive and false negative predictions.</s><s xml:id="_RZQx8Q4">For another, a particularly important source of error is AI bias.</s><s xml:id="_2SsaUh4">AI bias leads to systematic errors, a systematic deviation from the expected prediction behavior of the AI tool.</s><s xml:id="_BfaCjHm">Ideally, the data used for training fully represent the population in which the AI tool is later applied.</s><s xml:id="_AX2Pjgm">A major goal of AI in healthcare product development is to approximate this ideal state via thorough clinical validation and development on heterogeneous data sources <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_3beU5FR">While this ensures that AI bias can be reduced to a minimum, it will still be almost impossible to generate AI tools without any trace of bias.</s><s xml:id="_bHHUdQD">If bias is present, then there will be prediction errors in patients not representing the training sample.</s><s xml:id="_35HhcsU">Taken together, both random and systematic sum up to the total number of errors that physicians and patients will encounter in the clinical setting, even when a fully validated high-performing AI system is used.</s><s xml:id="_AHHnZ2F">This is why, from a medical point-of-view, not only clinical validation but also explainability plays an instrumental role in the clinical setting.</s><s xml:id="_FDTdCKf">Explainability enables the resolution of disagreement between an AI system and human experts, no matter on whose side the error in judgment is situated.</s><s xml:id="_yuMtsZj">It should be noted that this will succeed mostly in cases of systematic error, of AI bias, rather than in cases of random error.</s><s xml:id="_EjRHbbF">Random errors are much harder to identify and will likely go unnoticed in case of agreement between the tool and the physician or will lead to situations of disagreement between the tool and the physician.</s><s xml:id="_GSp6cA9">This situation is discussed in the ethical considerations section.</s><s xml:id="_X8StJ9g">Explainability results are usually represented visually or through natural language explanations.</s><s xml:id="_NGUAaYt">Both show the clinicians how different factors contributed to the final recommendation.</s><s xml:id="_wz7wHmJ">In other words, explainability can assist clinicians in evaluating the recommendations provided by a system based on their experience and clinical judgment.</s><s xml:id="_yV4NzF4">This allows them to make an informed decision whether or not to rely on the system's recommendations and can, consequently, strengthen their trust in the system.</s><s xml:id="_JWayZJM">Particularly in cases where the CDSS produces recommendations that are strongly out of line with a clinician's expectations, explainability allows verification whether the parameters taken into account by the system make sense from a clinical point-of-view.</s><s xml:id="_2jax3wn">By laying open the inner workings of the CDSS, explainability can, thus, assist clinicians in identifying false positives and false negatives more easily.</s><s xml:id="_Va99QpZ">As clinicians identify instances in which the system performs poorly, they can report these cases back to developers to foster quality assurance and product improvement.</s><s xml:id="_SHkUZZY">Given these considerations, explainability may be a key driver for the uptake of AI-driven CDSS in clinical practice, as trust in these systems is not yet established <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>.</s><s xml:id="_nY6K9T8">Here, it is important to note that any use of AI-based CDSS may influence a physician in reaching a decision.</s><s xml:id="_su2UrGA">It will, therefore, be of critical importance to establish transparent documentation on how recommendations were derived.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3HPCf2u">The patient perspective</head><p xml:id="_wRX9V3K"><s xml:id="_yFj2mp4">Looking at the issue of explainability from the patient perspective raises the question of whether the use of AI-powered decision aids is compatible with the inherent values of patient-centered care.</s><s xml:id="_ejX9cqq">Patient-centered care aims to be responsive to and respectful of individual patients' values and needs <ref type="bibr" target="#b23">[24]</ref>.</s><s xml:id="_aCuTGfj">It considers patients as active partners in the care process, emphasizing their right to choice and control over medical decisions.</s><s xml:id="_fJCd2qD">A key component of patient-centered care is shared decisionmaking aimed at identifying the treatment best suited to the individual patients' situation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</s><s xml:id="_ZjTjMPe">It involves an open conversation between the patient and the clinician, where the clinician informs the patient about the potential risks and benefits of available courses of action and the patient discusses their values and priorities <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</s></p><p xml:id="_kCRaM9P"><s xml:id="_D4289Sj">Several evidence-based tools have been developed to facilitate shared decision-making, among them, socalled conversation aids <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_DUs6ctj">Unlike patient decision aids (which are used by the patient in preparation prior to the clinical encounter), conversation aids are designed for use within the clinical encounter to guide the patient and clinician through the shared decision-making process <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>.</s><s xml:id="_vue8VBz">They incorporate established medical facts about their conditions and, by synthesizing available information, they can help patients to understand their individual risks and outcomes, to explore the available options, and to determine which course of action best fits their goals and priorities <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>.</s><s xml:id="_ZstbR3P">So, what if individual risk was not calculated using established risk prediction models but instead relied on a validated, yet not explainable, datadriven approach?</s><s xml:id="_77eTu49">Would it make a difference from the patient's perspective?</s><s xml:id="_hPUaXZ6">Seeking to address these questions, it was recently argued that so-called 'black-box medicine' conflicts with core ideals of patient-centered medicine <ref type="bibr" target="#b32">[33]</ref>.</s><s xml:id="_D2PaUcC">Since clinicians are no longer able to fully comprehend the inner workings and calculations of the decision aid they are not able to explain to the patient how certain outcomes or recommendations were derived <ref type="bibr" target="#b32">[33]</ref>.</s></p><p xml:id="_X49NJGZ"><s xml:id="_mU9BHAe">Explainability can address this issue by providing clinicians and patients with a personalized conversation aid that is based on the patient's individual characteristics and risk factors.</s><s xml:id="_aSRwnqQ">By simulating the impact of different treatment or lifestyle interventions, an explainable AI decision aid could help to raise patients' choice awareness and support clinicians in eliciting patient values and preferences <ref type="bibr" target="#b33">[34]</ref>.</s><s xml:id="_gG4Zuwa">As described previously, explainability provides a visual representation or natural language explanation of how different factors contributed to the final risk assessment.</s><s xml:id="_F7cwCyV">Yet, to interpret system-derived explanations and probabilities, patients rely on the clinician's ability to understand and convey these explanations in a way that is accurate and understandable.</s><s xml:id="_3YMB3Ku">If used appropriately, explainable AI decision support systems may not only contribute to patients feeling more knowledgeable and better informed but could also promote more accurate risk perceptions <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>.</s><s xml:id="_7JwxTFx">This may, in turn, boost patients' motivation to engage in shared decisionmaking and to act upon risk-relevant information <ref type="bibr" target="#b34">[35]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wsQwBpP">Ethical implications</head><p xml:id="_6eRMN2T"><s xml:id="_JHGV46k">With the increasing penetration of AI-powered systems in healthcare, there is a necessity to explore the ethical issues accompanying this imminent paradigm shift.</s><s xml:id="_hy4rXB2">A commonly applied and well-fitting ethical framework when assessing biomedical ethical challenges are the "Principles of Biomedical Ethics" by Beauchamp and Childress <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> introducing four key principles: autonomy, beneficence, nonmaleficence, and justice <ref type="bibr" target="#b35">[36]</ref>.</s><s xml:id="_xcWKwMk">While principlism is not the only available bioethical framework, it is a very useful basic practical framework with high acceptance both in research and medical settings <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>.</s><s xml:id="_F4hST2r">Thus, in the following, we assess explainability with regards to the aforementioned four principles.</s></p><p xml:id="_2eQw8Pd"><s xml:id="_DhntPaB">Concerning autonomy, explainability has implications for patients and physicians alike <ref type="bibr" target="#b30">[31]</ref>.</s><s xml:id="_qkBJzyE">One of the major safeguards of patients' autonomy is represented by informed consent, that is an autonomous, generally written authorization with which the patient grants a doctor his or her permission to perform a given medical act <ref type="bibr" target="#b38">[39]</ref>.</s><s xml:id="_kSMS2tF">Proper informed consent is premised upon exhaustive and understandable information regarding the nature and risks of a medical procedure, and lack of undue interference with the patient's voluntary decision to undergo the procedure.</s><s xml:id="_6g9xpsS">At the moment, an ethical consensus has not yet emerged as to whether disclosing the use of an opaque medical AI algorithm should be a mandatory requirement of informed consent.</s><s xml:id="_VSfnt3A">A failure to disclose the use of an opaque AI system may undermine patients' autonomy and negatively impact the doctor-patient relationship, jeopardizing patients' trust, and might violate the compliance with clinical recommendations.</s><s xml:id="_x6XESsX">If the patient were to find out in hindsight that a clinician's recommendation was derived from an opaque AI system, this may lead the patient to not only challenge the recommendation but might also lead to a justified request for explanation-which in the case of an opaque system, the clinician would not be able to provide.</s><s xml:id="_sTrygzS">Opaque medical AI can, therefore, represent an obstacle to the provision of accurate information and thus potentially jeopardize informed consent.</s><s xml:id="_cMqN6yj">Appropriate ethical and explainability standards are therefore important to safeguard the autonomy-preserving function of informed consent.</s></p><p xml:id="_A9tVGCd"><s xml:id="_T8S2UTC">Attention should be paid to the risk that the introduction of opaque AI into medical decision making may foster paternalism by limiting opportunities for patients to express their expectations and preferences regarding medical procedures <ref type="bibr" target="#b38">[39]</ref>.</s><s xml:id="_nUTAuYz">A necessary prerequisite for shared decision making is full autonomy of the patient, but full autonomy can only be achieved if the patient is presented with a range of meaningful options to choose from <ref type="bibr" target="#b39">[40]</ref>.</s><s xml:id="_Qa8xddD">In this respect, patients' opportunities to exert their autonomy regarding medical procedures get reduced as opaque AI becomes more central to medical decision making.</s><s xml:id="_KXjUnJr">In particular, the challenge that arises with opaque CDSS is that it remains unclear whether and how patient values and preferences are accounted for by the model.</s><s xml:id="_q5YDRJT">This state of affairs could be addressed by means of "value-flexible" AI that provides different options for the patient <ref type="bibr" target="#b40">[41]</ref>.</s><s xml:id="_NY2jbXp">We further argue that explainability is a necessary step towards value-flexible AI.</s><s xml:id="_2uBWZ7t">The patient needs to be able to understand which variables play an important role in the inner workings of the AI system to determine-with the aid of the doctorwhether the goals and weighting of the AI system align with their values or not.</s><s xml:id="_cHkpR9e">For example, AI systems primed for "survival" as the outcome might not be aligned with the value of patients for whom a "reduction of suffering" is more important <ref type="bibr" target="#b40">[41]</ref>.</s><s xml:id="_8QMgSuK">Lastly, when a choice is made, patients need to be able to trust an AI system to decide with confidence and autonomy to follow its guidance <ref type="bibr" target="#b41">[42]</ref>.</s><s xml:id="_YnrQ9xR">This is not possible when the AI model is opaque.</s><s xml:id="_ZnEBccz">Therefore, explainability is-both from the physician's and patient's point-of-view-an ethical prerequisite for systems supporting critical medical decision making.</s></p><p xml:id="_8f2xZmr"><s xml:id="_5yd9899">While the principles of beneficence and non-maleficence are related, they nonetheless shed light on different aspects, also with regards to explainability.</s><s xml:id="_MycnYaE">Beneficence urges physicians to maximize patient benefits.</s><s xml:id="_xvW9HeN">When applying AI-based systems, physicians are thus expected to use the tools in a manner that promotes the optimal outcome for the respective patient.</s><s xml:id="_Mgs5yn7">Yet, to provide patients with the most appropriate options to promote their health and wellbeing, physicians need to be able to use the full capabilities of the system.</s><s xml:id="_psZVduh">This implies that physicians have knowledge of the system beyond a robotic application in a certain clinical use case, allowing them to reflect on the system's output.</s><s xml:id="_2CRSrcD">For physicians, explainability in the form of visualizations or natural language explanations enables confident clinical decisions instead of having to simply trust an automated output.</s><s xml:id="_VySTAZ2">They can critically assess the system-derived outcomes and make their own judgments whether the results seem trustworthy or not.</s><s xml:id="_zdFAzTe">This allows them to adapt predictions and recommendations to individual circumstances where necessary.</s><s xml:id="_XBG9M7m">As such, clinicians can not only reduce the risk of eliciting false hope or creating false despair but can also flag potentially inappropriate interventions using their clinical judgment <ref type="bibr" target="#b42">[43]</ref>.</s><s xml:id="_srUYT4w">This is especially important when we imagine a situation where a physician and an AI system are in disagreement, a situation that is not easily resolved <ref type="bibr" target="#b41">[42]</ref>.</s><s xml:id="_ByrDA7W">Fundamentally, this is a question of epistemic authority, and it is unclear how physicians should decide whether they can trust the epistemic authority of a black box model enough to defer to its decision <ref type="bibr" target="#b41">[42]</ref>.</s><s xml:id="_8K3PDGy">Grote et al. <ref type="bibr" target="#b41">[42]</ref> argue that in the case of opaque AI there is not enough epistemic support for deference.</s><s xml:id="_p6VcyY3">Moreover, they further argue that confronted with a black-box system, clinical decision support might not enhance the capabilities of physicians, but rather limit them.</s><s xml:id="_DQfH8ZS">Here, physicians might be forced into "defensive medicine" where they dogmatically follow the output of the machine to avoid being questioned or held accountable <ref type="bibr" target="#b41">[42]</ref>.</s><s xml:id="_MmzYAc8">Such a situation would cause a serious threat to physician autonomy.</s><s xml:id="_4PadetH">Additionally, physicians will rarely have the time to perform an in-depth analysis of why their clinical judgement is in disagreement with the AI system.</s><s xml:id="_k2jsPwM">Thus, looking merely at a performance output is not sufficient in the clinical context.</s><s xml:id="_WHu9pU4">The optimal outcome for all patients can only be expected with healthcare staff that can make informed decisions when to apply an AI-powered CDSS and how to interpret its results.</s><s xml:id="_MaPQUfM">It is thus hard to imagine how beneficence in the context of medical AI can be fulfilled with any "black box" application.</s></p><p xml:id="_B76SbSh"><s xml:id="_2zdSAax">The need for explainability is also evident when assessing the principle of non-maleficence in the context of medical AI.</s><s xml:id="_C8SmEvT">Non-maleficence states that physicians have a fundamental duty not to harm their patients either intentionally or through excessive or inappropriate use of medical means.</s><s xml:id="_qaJS2QU">Why is performance not enough?</s><s xml:id="_w7pjyjm">It has been argued that a black box medical AI-based only on validated maximized performance is ethically justifiable even if the causal mechanisms behind a given AI-prescribed intervention remain opaque to the clinician <ref type="bibr" target="#b43">[44]</ref>.</s><s xml:id="_N46bAd9">Reliance on anecdotal or purely experiential evidence about the efficacy of a given treatment is indeed still quite common in medicine.</s><s xml:id="_QFMSAQd">Yet this is no excuse to forego explanations as a major requirement of sound clinical judgment when such an explanation is indeed possible.</s><s xml:id="_VguzpgG">Recent progress in elucidating at least the principal features of AI models, while not providing full mechanistic explanations of AI-decisions, create a prima facie ethical obligation to reduce opacity and increase the interpretability of medical AI.</s><s xml:id="_spEaRJv">Failure to do so would mean intentionally undermining a physician's capacity to control for possible misclassifications of individual clinical cases due, for instance, to excessive bias or variance in training datasets.</s><s xml:id="_VjxT868">We thus conclude that also with regards to beneficence and non-maleficence, explainability is a necessary characteristic of clinically applied AI systems.</s></p><p xml:id="_AAK7PYw"><s xml:id="_2dYftvW">The principle of justice postulates that people should have equal access to the benefits of medical progress without ethically unjustified discrimination of any particular individuals or social group <ref type="bibr" target="#b35">[36]</ref>.</s><s xml:id="_e8EG7EA">Some AI systems, however, violate this principle.</s><s xml:id="_hJp2YYT">Recently, for example, Obermeyer et al. reported on a medical AI system discriminating against people of color <ref type="bibr" target="#b4">[5]</ref>.</s><s xml:id="_REMZKnQ">Explainability can support developers and clinicians to detect and correct such biases-a major potential source for injustice-ideally at the early stage of AI development and validation, e.g. by identification of important features indicating a bias in the model.</s><s xml:id="_ZMUykPF">However, for explainability to fulfill this purpose, the relevant stakeholder groups must be sensitized to the risk of bias and its potential consequences for individuals' health and wellbeing.</s><s xml:id="_eJbxEca">At times, it might be tempting to prioritize accuracy and simply refrain from investing resources into developing explainable AI.</s><s xml:id="_CmjDdSz">Yet to ensure that AI-powered decision support systems realize their potential, developers, and clinicians need to be attentive to the potential flaws and limitations of these new tools.</s><s xml:id="_wakafNQ">Thus, also from the justice perspective, explainability becomes an ethical prerequisite for the development and application of AI-based clinical decision support.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3zHDsEk">Conclusion</head><p xml:id="_XsqzJNP"><s xml:id="_N3p8DES">In this paper, we explored the role of explainable AI in clinical decision support systems from the technological, legal, medical, and patient perspectives.</s><s xml:id="_ubmjSGP">In doing so, we have shown that explainability is a multifaceted concept that has far-reaching implications for the various stakeholder groups involved.</s><s xml:id="_BtyTkeQ">Medical AI poses challenges to developers, medical professionals, and legislators as it requires a reconsideration of roles and responsibilities.</s><s xml:id="_wFfxzYQ">Based on our analysis, we consider explainability a necessary requirement to address these challenges in a sustainable manner that is compatible with professional norms and values.</s></p><p xml:id="_mRfFrbU"><s xml:id="_dgMh5D8">Notably, a move towards opaque algorithms in CDSS may inadvertently lead to a revival of paternalistic concepts of care that relegate patients to passive spectators in the medical decision-making process.</s><s xml:id="_vJ3xzzk">It might also bring forward a new type of medicine where physicians become slaves to the tool's output to avoid legal and medical repercussions.</s><s xml:id="_hvjWnw3">And, last but not least, opaque systems might provoke a faulty allocation of resources violating their just distribution.</s><s xml:id="_62QVAGb">In this paper, we have argued that explainability can help to ensure that patients remain at the center of care and that together with clinicians they can make informed and autonomous decisions about their health.</s><s xml:id="_kdexDBP">Moreover, explainability can promote the just distribution of available resources.</s></p><p xml:id="_EAZ6S7W"><s xml:id="_6Q4rxG4">We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health.</s><s xml:id="_D3n27U2">Further work is needed to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration to tackle these challenges with joined forces.</s></p></div>		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_XTmpmVJ">Acknowledgements</head><p xml:id="_feU8FRR"><s xml:id="_zPzQWUV">The authors would like to thank <rs type="person">Dr. Nora A. Tahy</rs> for review of the manuscript.</s></p></div>
			</div>
			<div type="funding">
<div><head xml:id="_q6cVCSf">Funding</head><p xml:id="_RQTbXPv"><s xml:id="_EzNEa5q">This research has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement No. <rs type="grantNumber">777107</rs> (PRECISE4Q).</s><s xml:id="_jaQFbAX">The funding body had no role in the study design, the collection, analysis, and interpretation of the data nor the preparation of the manuscript.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_BZUZEen">
					<idno type="grant-number">777107</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_dsJ5x9Q">Availability of data and materials</head><p xml:id="_7U5ErkV"><s xml:id="_GqVTKbE">Not applicable.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_RNMeyJy"><p xml:id="_YxJzV4m"><s xml:id="_aYR6TmH">Abbreviations AI: Artificial intelligence; ANN: Artificial neural network; CDSS: Clinical decision support system; FDA: Food and Drug Administration; GDPR: General Data Protection Regulation; ICU: Intensive care unit; MDR: Medical Device Regulation; TPLC: Total product lifecycle approach.</s></p><p xml:id="_6Zwj9tP"><s xml:id="_kHZ62hx">Authors' contributions JA: Conceptualization; analysis, writing-original draft; writing-review and editing.</s><s xml:id="_SsaytyH">AB: analysis; writing-original draft; writing-review and editing.</s><s xml:id="_Mcqk9wB">EV: analysis; writing-original draft; writing-review and editing.</s><s xml:id="_x8FGKKQ">DF: analysis; writing-original draft; writing-review and editing.</s><s xml:id="_yPT7Zj7">VIM: conceptualization; analysis, writing-original draft; Writing-review and editing.</s><s xml:id="_NF86Cvx">All authors read and approved the final manuscript.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Y7PRdeA">Ethics approval and consent to participate</head><p xml:id="_rMbrhZZ"><s xml:id="_tQJGx46">Not applicable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_XQDxQYP">Consent for publication</head><p xml:id="_FAV2P75"><s xml:id="_7CPm8Z4">Not applicable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AQhRuGN">Competing interests</head><p xml:id="_W8GJYwM"><s xml:id="_yMgG8wJ">The authors declare no competing interests.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_75TjKFf">Publisher's Note</head><p xml:id="_WVGV5Uv"><s xml:id="_V8jD4kZ">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_ce5nsvW">From bit to bedside: a practical framework for artificial intelligence product development in healthcare</title>
		<author>
			<persName><forename type="first">D</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Madai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VTYQ7Kw">Adv Intell Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2000052</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Higgins D, Madai VI. From bit to bedside: a practical framework for artificial intelligence product development in healthcare. Adv Intell Syst. 2020;2:2000052.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_y27FJNf">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0048-x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Qdd9qQy">Nat Mach Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rudin C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat Mach Intell. 2019;1:206-15.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_VjmcKnp">What does explainable AI really mean? A new conceptualization of perspectives</title>
		<author>
			<persName><forename type="first">D</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Besold</surname></persName>
		</author>
		<idno>ArXiv171000794 Cs. 2017</idno>
		<ptr target="http://arxiv.org/abs/1710.00794" />
		<imprint>
			<date type="published" when="2019-09-03">3 Sept 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Doran D, Schulz S, Besold TR. What does explainable AI really mean? A new conceptualization of perspectives. ArXiv171000794 Cs. 2017. http://arxiv .org/abs/1710.00794 . Accessed 3 Sept 2019.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_WyhKg5F">Clinical decision support in the era of artificial intelligence</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Shortliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sepúlveda</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2018.17163</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8p7Qvne">JAMA</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="2199" to="2200" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shortliffe EH, Sepúlveda MJ. Clinical decision support in the era of artificial intelligence. JAMA. 2018;320:2199-200.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_S34b9bX">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aax2342</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jqcsbYD">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Obermeyer Z, Powers B, Vogeli C, Mullainathan S. Dissecting racial bias in an algorithm used to manage the health of populations. Science. 2019;366:447-53.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main" xml:id="_eNwGvY9">Explainable AI: interpreting, explaining and visualizing deep learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Editors</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-28954-6" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Samek W, Montavon G, Vedaldi A, Hansen LK, Müller K-R, editors. Explain- able AI: interpreting, explaining and visualizing deep learning. Berlin: Springer; 2019. https ://doi.org/10.1007/978-3-030-28954 -6.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_RsYcwh5">A guide to deep learning in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_T9bsd2x">Nat Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Esteva A, Robicquet A, Ramsundar B, Kuleshov V, DePristo M, Chou K, et al. A guide to deep learning in healthcare. Nat Med. 2019;25:24-9.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_Qq7n8dm">Towards quantification of explainability in explainable artificial intelligence methods</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghafoor</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-96630-0_4</idno>
		<idno>ArXiv191110104 Cs Q-Fin. 2019</idno>
		<ptr target="http://arxiv.org/abs/1911.10104" />
		<imprint>
			<date type="published" when="2020-10-02">2 Oct 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Islam SR, Eberle W, Ghafoor SK. Towards quantification of explainability in explainable artificial intelligence methods. ArXiv191110104 Cs Q-Fin. 2019. http://arxiv .org/abs/1911.10104 . Accessed 2 Oct 2020.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_PUYrEwG">Toward interpretable machine learning: transparent deep neural networks and beyond</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1109/jproc.2021.3060483</idno>
		<idno>ArXiv200307631 Cs Stat. 2020</idno>
		<ptr target="http://arxiv.org/abs/2003.07631" />
		<imprint>
			<date type="published" when="2020-10-02">2 Oct 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Samek W, Montavon G, Lapuschkin S, Anders CJ, Müller K-R. Toward interpretable machine learning: transparent deep neural networks and beyond. ArXiv200307631 Cs Stat. 2020. http://arxiv .org/abs/2003.07631 . Accessed 2 Oct 2020.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_pW3sGBf">Unmasking Clever Hans predictors and assessing what machines really learn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-019-08987-4</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hRaJYdh">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1096</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lapuschkin S, Wäldchen S, Binder A, Montavon G, Samek W, Müller K-R. Unmasking Clever Hans predictors and assessing what machines really learn. Nat Commun. 2019;10:1096.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_7hwxmSK">Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Badgeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Titano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Oermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_B6UXxeB">PLOS Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1002683</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zech JR, Badgeley MA, Liu M, Costa AB, Titano JJ, Oermann EK. Vari- able generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLOS Med. 2018;15:e1002683.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_Ek4sQAc">What&apos;s in the box? The legal requirement of explainability in computationally aided decisionmaking in public administration</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Slosser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiesener</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3402974</idno>
		<ptr target="https://doi.org/10.2139/ssrn.3402974" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Social Science Research Network</publisher>
			<pubPlace>Rochester</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">SSRN Scholarly Paper</note>
	<note type="raw_reference">Olsen HP, Slosser JL, Hildebrandt TT, Wiesener C. What&apos;s in the box? The legal requirement of explainability in computationally aided decision- making in public administration. SSRN Scholarly Paper. Rochester: Social Science Research Network; 2019. https ://doi.org/10.2139/ssrn.34029 74.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_GnKHESA">Artificial intelligence in healthcare: a critical analysis of the legal and ethical implications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Schönberger</surname></persName>
		</author>
		<idno type="DOI">10.1093/ijlit/eaz004</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fpppmuP">Int J Law Inf Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="171" to="203" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schönberger D. Artificial intelligence in healthcare: a critical analysis of the legal and ethical implications. Int J Law Inf Technol. 2019;27:171-203.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_gqRwCyY">Informed consent and medical artificial intelligence: what to tell the patient? SSRN Scholarly Paper</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3529576</idno>
		<ptr target="https://doi.org/10.2139/ssrn.3529576" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Social Science Research Network</publisher>
			<pubPlace>Rochester, NY</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Cohen IG. Informed consent and medical artificial intelligence: what to tell the patient? SSRN Scholarly Paper. Rochester, NY: Social Science Research Network; 2020. https ://doi.org/10.2139/ssrn.35295 76.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_95yvb8g">Identifying the &quot;right&quot; level of explanation in a given situation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Beaudouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bounie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eagan</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3604924</idno>
		<ptr target="https://doi.org/10.2139/ssrn.3604924" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_ct83Pdg">SSRN Electron J</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beaudouin V, Bloch I, Bounie D, Clémençon S, d&apos; Alché-Buc F, Eagan J, et al. Identifying the &quot;right&quot; level of explanation in a given situation. SSRN Electron J. 2020. https ://doi.org/10.2139/ssrn.36049 24.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<ptr target="https://www.fda.gov/files/medical%20devices/published/US-FDA-Artificial-Intelligence-and-Machine-Learning-Discussion-Paper.pdf" />
		<title level="m" xml:id="_53tUQ6h">Proposed regulatory framework for modifications to artificial intelligence/machine learning (AI/ML)-based Software as a Medical Device</title>
		<imprint>
			<publisher>SaMD</publisher>
			<date type="published" when="2020-07-05">2020. 5 July 2020</date>
		</imprint>
		<respStmt>
			<orgName>FDA</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">FDA. Proposed regulatory framework for modifications to artificial intel- ligence/machine learning (AI/ML)-based Software as a Medical Device (SaMD). 2020. https ://www.fda.gov/files /medic al%20dev ices/publi shed/ US-FDA-Artifi cial -Intel ligen ce-and-Machi ne-Learn ing-Discu ssion -Paper .pdf. Accessed 5 July 2020.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_rax7PzD">Explainable AI under contract and tort law: legal incentives and technical challenges</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Krestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naumann</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3513433</idno>
		<ptr target="https://papers.ssrn.com/abstract=3513433" />
		<imprint>
			<date type="published" when="2020-02-13">2020. 13 Feb 2020</date>
			<pubPlace>Rochester, NY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Social Science Research Network</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">SSRN Scholarly Paper</note>
	<note type="raw_reference">Hacker P, Krestel R, Grundmann S, Naumann F. Explainable AI under contract and tort law: legal incentives and technical challenges. SSRN Scholarly Paper. Rochester, NY: Social Science Research Network; 2020. https ://paper s.ssrn.com/abstr act=35134 33. Accessed 13 Feb 2020.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_jMJgMEm">Machine learning in medicine: opening the new data protection black box</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ferretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blasimme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QtBZFcK">Eur Data Prot Law Rev EDPL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">320</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ferretti A, Schneider M, Blasimme A. Machine learning in medicine: opening the new data protection black box. Eur Data Prot Law Rev EDPL. 2018;4:320.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_XNqU6JV">Can machine-learning improve cardiovascular risk prediction using routine clinical data?</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Garibaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Qureshi</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0174944</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xy6SumF">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">174944</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Weng SF, Reps J, Kai J, Garibaldi JM, Qureshi N. Can machine-learning improve cardiovascular risk prediction using routine clinical data? PLoS ONE. 2017;12:e0174944.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_TnPUjDJ">Machine learning outperforms ACC/AHA CVD risk calculator in MESA</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vrigkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Budoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naghavi</surname></persName>
		</author>
		<idno type="DOI">10.1161/jaha.118.009476</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rPnnsGJ">J Am Heart Assoc</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">e009476</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kakadiaris IA, Vrigkas M, Yen AA, Kuznetsova T, Budoff M, Naghavi M. Machine learning outperforms ACC/AHA CVD risk calculator in MESA. J Am Heart Assoc. 2018;7:e009476.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_yQnfdnu">A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><forename type="middle">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2019.101723</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YjxRy3d">Artif Intell Med</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="101723" to="101723" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu T, Fan W, Wu C. A hybrid machine learning approach to cerebral stroke prediction based on imbalanced medical dataset. Artif Intell Med. 2019;101:101723-101723.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_grSPsWW">Machine intelligence in healthcare-perspectives on trustworthiness, explainability, usability, and transparency</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Cutillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Foschini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mackintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Mandl</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-020-0254-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VMF3z9N">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cutillo CM, Sharma KR, Foschini L, Kundu S, Mackintosh M, Mandl KD. Machine intelligence in healthcare-perspectives on trustworthiness, explainability, usability, and transparency. NPJ Digit Med. 2020;3:1-5.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main" xml:id="_dx4u9Gt">What clinicians want: contextualizing explainable machine learning for clinical end use</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tonekaboni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mccradden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<idno>ArXiv190505134 Cs Stat. 2019</idno>
		<ptr target="http://arxiv.org/abs/1905.05134" />
		<imprint>
			<date type="published" when="2019-09-03">3 Sept 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tonekaboni S, Joshi S, McCradden MD, Goldenberg A. What clinicians want: contextualizing explainable machine learning for clinical end use. ArXiv190505134 Cs Stat. 2019. http://arxiv .org/abs/1905.05134 . Accessed 3 Sept 2019.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<idno type="DOI">10.1056/nejm200108303450917</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/books/NBK222274/" />
		<title level="m" xml:id="_KdMFq2Q">Crossing the quality chasm: a new health system for the 21st century</title>
		<meeting><address><addrLine>Washington, DC; US</addrLine></address></meeting>
		<imprint>
			<publisher>National Academies Press</publisher>
			<date type="published" when="2001-05-21">2001. 21 May 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Institute of Medicine (US) Committee on Quality of Health Care in America. Crossing the quality chasm: a new health system for the 21st century. Washington, DC: National Academies Press (US); 2001. http:// www.ncbi.nlm.nih.gov/books /NBK22 2274/. Accessed 21 May 2020.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_xUkqwn5">Shared decision making-the pinnacle patient-centered care</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Edgman-Levitan</surname></persName>
		</author>
		<idno type="DOI">10.1056/nejmp1109283</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uBJephm">N Engl J Med</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="780" to="781" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Barry MJ, Edgman-Levitan S. Shared decision making-the pinnacle patient-centered care. N Engl J Med. 2012;366:780-1.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_3jkbXMF">What is shared decision making? (and What it is not)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kunneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Montori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Castaneda-Guarderas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Hess</surname></persName>
		</author>
		<idno type="DOI">10.1111/acem.13065</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kBRtwbe">Acad Emerg Med</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1320" to="1324" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kunneman M, Montori VM, Castaneda-Guarderas A, Hess EP. What is shared decision making? (and What it is not). Acad Emerg Med. 2016;23:1320-4.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_4pDfr7D">Availability of patient decision aids for stroke prevention in atrial fibrillation: a systematic review</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Grande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Elwyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coylewright</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ahj.2017.05.014</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XgqbqfT">Am Heart J</title>
		<imprint>
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">O&apos;Neill ES, Grande SW, Sherman A, Elwyn G, Coylewright M. Availability of patient decision aids for stroke prevention in atrial fibrillation: a system- atic review. Am Heart J. 2017;191:1-11.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_usTaPPj">Shared decision-making in atrial fibrillation: navigating complex issues in partnership with the patient</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Brito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kunneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Hargraves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zeballos-Palacios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Montori</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10840-018-0465-5</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UPZEU8S">J Interv Card Electrophysiol</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="159" to="163" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Noseworthy PA, Brito JP, Kunneman M, Hargraves IG, Zeballos-Palacios C, Montori VM, et al. Shared decision-making in atrial fibrillation: navigating complex issues in partnership with the patient. J Interv Card Electro- physiol. 2019;56:159-63.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_Ne64YhU">Impact of decision aids used during clinical encounters on clinician outcomes and consultation length: a systematic review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Dobler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Gionfriddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Alvarez-Villalobos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Ospina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Spencer-Bonilla</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmjqs-2018-008022</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mVq8bKb">BMJ Qual Saf</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="499" to="510" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dobler CC, Sanchez M, Gionfriddo MR, Alvarez-Villalobos NA, Ospina NS, Spencer-Bonilla G, et al. Impact of decision aids used during clinical encounters on clinician outcomes and consultation length: a systematic review. BMJ Qual Saf. 2019;28:499-510.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_bzXJchw">Subclinical and device-detected atrial fibrillation: pondering the knowledge gap: a scientific statement from the American Heart Association</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Noseworthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elkind</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Joglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xgbkbbV">Circulation</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="944" to="963" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Noseworthy PA, Kaufman ES, Chen LY, Chung MK, Elkind Mitchell SV, Joglar JA, et al. Subclinical and device-detected atrial fibrillation: ponder- ing the knowledge gap: a scientific statement from the American Heart Association. Circulation. 2019;140:e944-63.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_6ApNgXD">Normalization of a conversation tool to promote shared decision making about anticoagulation in patients with atrial fibrillation within a practical randomized trial of its effectiveness: a cross-sectional study</title>
		<author>
			<persName><forename type="first">G</forename><surname>Spencer-Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Organick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kunneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giblon</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13063-020-04305-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sC9a8rB">Trials</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">395</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Spencer-Bonilla G, Thota A, Organick P, Ponce OJ, Kunneman M, Giblon R, et al. Normalization of a conversation tool to promote shared decision making about anticoagulation in patients with atrial fibrillation within a practical randomized trial of its effectiveness: a cross-sectional study. Trials. 2020;21:395.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_J3CeV6U">Should heart age calculators be used alongside absolute cardiovascular disease risk assessment?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Glasziou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Irwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doust</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12872-018-0760-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vcQgJtN">BMC Cardiovasc Disord</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bonner C, Bell K, Jansen J, Glasziou P, Irwig L, Doust J, et al. Should heart age calculators be used alongside absolute cardiovascular disease risk assessment? BMC Cardiovasc Disord. 2018;18:19.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_EpGMvz2">Artificial intelligence and patient-centered decisionmaking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bjerring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13347-019-00391-6</idno>
		<ptr target="https://doi.org/10.1007/s13347-019-00391-6" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_4bc2mpv">Philos Technol</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bjerring JC, Busch J. Artificial intelligence and patient-centered decision- making. Philos Technol. 2020. https ://doi.org/10.1007/s1334 7-019-00391 -6.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_R53z8V7">Importance of clarifying patients&apos; desired role in shared decision making to match their level of engagement with their preferences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Politi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Dizon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Frosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kuzemchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Stiggelbout</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.f7066</idno>
		<ptr target="https://doi.org/10.1136/bmj.f7066" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_vsxnK5y">BMJ</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Politi MC, Dizon DS, Frosch DL, Kuzemchak MD, Stiggelbout AM. Impor- tance of clarifying patients&apos; desired role in shared decision making to match their level of engagement with their preferences. BMJ. 2013. https ://doi.org/10.1136/bmj.f7066 .</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_rt5hSrx">Decision aids for people facing health treatment or screening decisions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Stacey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Légaré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Eden</surname></persName>
		</author>
		<idno type="DOI">10.1002/14651858.cd001431.pub5</idno>
		<ptr target="https://doi.org/10.1002/14651858.CD001431.pub5" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_EeFRHtb">Cochrane Database Syst Rev</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Stacey D, Légaré F, Lewis K, Barry MJ, Bennett CL, Eden KB, et al. Decision aids for people facing health treatment or screening decisions. Cochrane Database Syst Rev. 2017. https ://doi.org/10.1002/14651 858.CD001 431. pub5.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_fpT8KSD">Principles of biomedical ethics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Beauchamp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-05">May-2008. 2008</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Beauchamp TL. Principles of biomedical ethics. Paperback May-2008. New York: Oxford University Press; 2008.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_mvABCBP">Defending the four principles approach as a good basis for good medical practice and therefore for good medical ethics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wUv7Man">J Med Ethics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="111" to="116" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gillon R. Defending the four principles approach as a good basis for good medical practice and therefore for good medical ethics. J Med Ethics. 2015;41:111-6.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_gfWq4Vb">Principles alone cannot guarantee ethical AI</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-019-0114-4</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_r2J3VD9">Nat Mach Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="501" to="507" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mittelstadt B. Principles alone cannot guarantee ethical AI. Nat Mach Intell. 2019;1:501-7.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Faden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Beauchamp</surname></persName>
		</author>
		<title level="m" xml:id="_gvRXyBm">A history and theory of informed consent</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Faden RR, Beauchamp TL. A history and theory of informed consent. Oxford: Oxford University Press; 1986.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Raz</surname></persName>
		</author>
		<idno type="DOI">10.1093/0198248075.001.0001/acprof-9780198248071</idno>
		<ptr target="https://doi.org/10.1093/0198248075.001.0001/acprof-9780198248071" />
		<title level="m" xml:id="_BY95WWp">The Morality of Freedom</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Raz J. The Morality of Freedom. Oxford: Oxford University Press; 2020. https ://doi.org/10.1093/01982 48075 .001.0001/acpro f-97801 98248 071.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_HKYMzx6">Computer knows best? The need for value-flexibility in medical AI</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mcdougall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g5rgjSU">J Med Ethics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="156" to="160" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McDougall RJ. Computer knows best? The need for value-flexibility in medical AI. J Med Ethics. 2019;45:156-60.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_t7R6YaP">On the ethics of algorithmic decision-making in healthcare</title>
		<author>
			<persName><forename type="first">T</forename><surname>Grote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Berens</surname></persName>
		</author>
		<idno type="DOI">10.1136/medethics-2019-105586</idno>
		<ptr target="https://doi.org/10.1136/medethics-2019-105586" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_d9mn9nh">J Med Ethics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grote T, Berens P. On the ethics of algorithmic decision-making in health- care. J Med Ethics. 2019. https ://doi.org/10.1136/medet hics-2019-10558 6.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_tSaRyyD">Ethical considerations about artificial intelligence for prognostication in intensive care</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Proft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Heerden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sviri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Van Heerden</surname></persName>
		</author>
		<idno type="DOI">10.1186/s40635-019-0286-6</idno>
		<ptr target="https://doi.org/10.1186/s40635-019-0286-6" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_erYNVDJ">Intensive Care Med Exp</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beil M, Proft I, van Heerden D, Sviri S, van Heerden PV. Ethical consid- erations about artificial intelligence for prognostication in intensive care. Intensive Care Med Exp. 2019. https ://doi.org/10.1186/s4063 5-019-0286-6.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_9pq4hzR">Artificial intelligence and black-box medical decisions: accuracy versus explainability</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>London</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9CnDzH4">Hastings Cent Rep</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">London AJ. Artificial intelligence and black-box medical decisions: accu- racy versus explainability. Hastings Cent Rep. 2019;49:15-21.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
