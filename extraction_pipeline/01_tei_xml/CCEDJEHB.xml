<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_DEEE3hH">Multimodal machine learning enables AI chatbot to diagnose ophthalmic diseases and provide high-quality medical responses Check for updates</title>
				<funder ref="#_NJdBknx">
					<orgName type="full">Shanghai Hospital Development Center</orgName>
				</funder>
				<funder>
					<orgName type="full">Key Laboratory of Myopia and Related Eye Diseases, Chinese Academy of Medical Sciences, Shanghai, China</orgName>
				</funder>
				<funder>
					<orgName type="full">Key Laboratory of Public Health Safety of Ministry of</orgName>
				</funder>
				<funder ref="#_bTFfVtG #_5KMR7w7 #_jH48gHD">
					<orgName type="full">Natural Science Foundation of Jiangsu Province of China</orgName>
				</funder>
				<funder ref="#_Jx6MCGh">
					<orgName type="full">National Natural Science Foundation of China</orgName>
					<orgName type="abbreviated">NSFC</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/501100001809</idno>
				</funder>
				<funder ref="#_cabawPM #_pvGnbrp #_VuQeS5X">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_ZqjqnMc">
					<orgName type="full">Zhejiang Key Research and Development Project</orgName>
				</funder>
				<funder ref="#_mP8CXQC">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruiqi</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qian</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jing</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyu</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingxu</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingjing</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lejin</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wenjun</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunqiu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinghan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengfei</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weiyi</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaofeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qin</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiawei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xintong</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weifang</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dehui</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Baoqing</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jingtao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kang</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongguang</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jinwei</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangjia</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxu</forename><surname>Hong</surname></persName>
							<idno type="ORCID">0000-0001-9912-633X</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Shi</surname></persName>
							<email>shifei@suda.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>zhangrui936@163.com</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinjian</forename><surname>Chen</surname></persName>
							<email>xjchen@suda.edu.cn</email>
							<idno type="ORCID">0000-0002-0871-293X</idno>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
							<email>dr_zhaochen@fudan.edu.cn</email>
							<idno type="ORCID">0000-0003-1373-7637</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Eye Institute and Department of Ophthalmology , Eye &amp; ENT Hospital , Fudan University , Shanghai , China.</note>
								<orgName type="department" key="dep1">Eye Institute</orgName>
								<orgName type="department" key="dep2">Department of Ophthalmology</orgName>
								<orgName type="department" key="dep3">Eye &amp; ENT Hospital</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> NHC Key Laboratory of Myopia and Related Eye</note>
								<orgName type="laboratory">NHC Key Laboratory of Myopia and Related Eye</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_KYhs7CC">Multimodal machine learning enables AI chatbot to diagnose ophthalmic diseases and provide high-quality medical responses Check for updates</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FAE505A326711CB911F8C1DA1E84416</idno>
					<idno type="DOI">10.1038/s41746-025-01461-0</idno>
					<note type="submission">Received: 22 July 2024; Accepted: 15 January 2025;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_kuZjjUj"><p xml:id="_ymuqgbU"><s xml:id="_enu5vCX">Chatbot-based multimodal AI holds promise for collecting medical histories and diagnosing ophthalmic diseases using textual and imaging data.</s><s xml:id="_mRCpGnG">This study developed and evaluated the ChatGPT-powered Intelligent Ophthalmic Multimodal Interactive Diagnostic System (IOMIDS) to enable patient self-diagnosis and self-triage.</s><s xml:id="_4mWD6AP">IOMIDS included a text model and three multimodal models (text + slit-lamp, text + smartphone, text + slit-lamp + smartphone).</s><s xml:id="_jRSmf2S">The performance was evaluated through a two-stage cross-sectional study across three medical centers involving 10 subspecialties and 50 diseases.</s><s xml:id="_ek6sy2C">Using 15640 data entries, IOMIDS actively collected and analyzed medical history alongside slit-lamp and/or smartphone images.</s><s xml:id="_zMSYBSS">The text + smartphone model showed the highest diagnostic accuracy (internal: 79.6%, external: 81.1%), while other multimodal models underperformed or matched the text model (internal: 69.6%, external: 72.5%).</s><s xml:id="_mCqZ99P">Moreover, triage accuracy was consistent across models.</s><s xml:id="_npJ2gy3">Multimodal approaches enhanced response quality and reduced misinformation.</s><s xml:id="_x5bMJMY">This proof-of-concept study highlights the potential of chatbot-based multimodal AI for self-diagnosis and self-triage.</s><s xml:id="_j6w97rW">(The clinical trial was registered on June 26, 2023, on ClinicalTrials.gov</s><s xml:id="_r6DjPJD">under the registration number NCT05930444.).</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_KkVFjkZ"><p xml:id="_NzfBy42"><s xml:id="_EhpQTxj">In the medical field, health data is inherently multimodal, encompassing both physical measurements and natural-language narratives 1 .</s><s xml:id="_fMwXCVZ">Ophthalmology, a discipline that heavily relies on multimodal information, requires detailed patient histories and visual examinations <ref type="bibr" target="#b1">2</ref> .</s><s xml:id="_gngmBe8">Consequently, multimodal machine learning is becoming increasingly important for medical diagnostics in ophthalmology.</s><s xml:id="_bMCwGNP">Previous studies on ophthalmic diagnostic models have underscored the substantial potential of image recognition AI in automating tasks that demand clinical expertize <ref type="bibr" target="#b2">3</ref> .</s><s xml:id="_Y7dv99q">Recently, chatbot-based multimodal generative AI has emerged as a promising avenue towards advancing precision health, integrating health data from both imaging and textual perspectives.</s><s xml:id="_wbjM26F">However, commonly utilized public models like GPT-4V and Google's VLM, while demonstrating some diagnostic capabilities, are currently deemed inadequate for clinical decision-making in ophthalmology <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref> .</s><s xml:id="_ueHU4bQ">In addition, these current models have not yet achieved the capability to actively collect patient histories through natural language human-computer interactions or to accurately interpret images acquired with non-specialist ophthalmic equipment such as smartphones.</s><s xml:id="_47vCV7t">Overcoming these challenges in multimodal AI is crucial and could pave the way for self-diagnosis of ophthalmic conditions in home settings, yielding substantial socioeconomic benefits <ref type="bibr" target="#b5">6</ref> .</s><s xml:id="_XHDfj4B">Therefore, there is both a need and potential for further development of multimodal AI models to diagnose and triage ophthalmic diseases.</s></p><p xml:id="_gNS8zUx"><s xml:id="_mZJK2Xt">In the domain of gathering medical histories through humancomputer interactions, the introduction of prompt engineering-a streamlined, data-and parameter-efficient technique for aligning large language models with the intricate demands of medical history inquiries-represents a significant advancement <ref type="bibr" target="#b6">7</ref> .</s><s xml:id="_VWGWfan">Expanding on this innovation, we propose an interactive ophthalmic consultation system that utilizes AI chatbots' robust text analysis capabilities to autonomously generate inquiries about a patient's medical history based on their chief complaints.</s><s xml:id="_naCgWXg">While previous research has not directly utilized chatbots for collecting ophthalmic medical histories, evidence indicates that current AI chatbots can provide precise and detailed responses to ophthalmic queries, such as retina-related multiple choice questions, myopia-related open-ended questions, and questions about urgency triage <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> .</s><s xml:id="_kB7JJPC">Furthermore, these chatbots excel in delivering high-quality natural language responses to medical inquiries, often exhibiting greater empathy than human doctors <ref type="bibr" target="#b10">11</ref> .</s><s xml:id="_Rdh9DT6">Therefore, we propose that the system should be capable of formulating comprehensive diagnoses and tailored recommendations based on the patient's responses.</s><s xml:id="_P6MQjYe">A significant drawback of previous studies on linguistic models is their reliance on simulated patient histories or perspectives created by researchers, which lack validation in real-world, large-scale clinical settings.</s><s xml:id="_q3YCAKC">This limitation raises uncertainties about the practical applications and autonomous deployment capabilities of these models.</s><s xml:id="_a63fV7Y">Hence, it is imperative to develop and validate an embodied conversational agent in authentic clinical settings, where patients themselves contribute data.</s></p><p xml:id="_7qfenZc"><s xml:id="_gY2kER3">Regarding ophthalmic imaging, previous research has primarily focused on slit-lamp photographs, yielding promising results.</s><s xml:id="_fEDJ2VK">For instance, a novel end-to-end fully convolutional network has been developed to diagnose infectious keratitis using corneal photographs <ref type="bibr" target="#b11">12</ref> .</s><s xml:id="_3MnNgbv">Recently, exploration into smartphone videos has shown their effectiveness in diagnosing pediatric eye diseases, potentially aiding caregivers in identifying visual impairments in children <ref type="bibr" target="#b12">13</ref> .</s><s xml:id="_kJH4DuT">Algorithms using smartphone-acquired photographs have also proven valuable in measuring anterior segment depth, which is particularly useful for screening primary angle closure glaucoma <ref type="bibr" target="#b13">14</ref> .</s><s xml:id="_C5sRQJJ">Together, these studies underscore the utility of both slit-lamp photographs and smartphone-acquired images in addressing diverse challenges in ophthalmic diagnostics.</s><s xml:id="_5yEPztT">However, a significant limitation of previous AI imaging and multimodal studies is their narrow focus on single diseases.</s><s xml:id="_9BdBuQE">These pipelines often operate independently within their domains, lacking integration across different fields to enhance overall functionality <ref type="bibr" target="#b14">15</ref> .</s><s xml:id="_HDpqw6S">For example, applying a model designed for herpes zoster ophthalmicus to triage a patient with cataracts may yield irrelevant outputs <ref type="bibr" target="#b15">16</ref> .</s><s xml:id="_aruNqns">Moreover, the reliance on single disease types increases the risk of poor model generalizability due to spectrum bias-a disparity in disease prevalence between the model's development population and its intended application population <ref type="bibr" target="#b16">17</ref> .</s><s xml:id="_Mg6rXdt">This limitation is particularly problematic for home-based self-diagnosis and self-triage.</s><s xml:id="_F6Rqcy9">To address these challenges, a recent developed and evaluated a novel machine learning system optimized for ophthalmic triage, using data from 9825 patients across 95 conditions <ref type="bibr" target="#b17">18</ref> .</s><s xml:id="_kq6qZhx">Thus, multimodal AI capable of diagnosing and triaging multiple ophthalmic diseases is essential for improving care across diverse populations.</s></p><p xml:id="_P6sJdSK"><s xml:id="_2p9HjMs">Based on the aforementioned context, our study aims to develop an Intelligent Ophthalmic Multimodal Interactive Diagnostic System (IOMIDS), an embodied conversational agent integrated with the AI chatbot ChatGPT.</s><s xml:id="_zGaFVAc">This system is designed for multimodal diagnosis and triage using eye images captured by slit-lamp or smartphone, alongside medical history.</s><s xml:id="_79yKVau">Clinical evaluations will be conducted across three centers, focusing on a comprehensive investigation of 50 prevalent ophthalmic conditions.</s><s xml:id="_B6JKJZS">The primary objective is to assess diagnostic effectiveness, with a secondary focus on triage performance across 10 ophthalmic subspecialties.</s><s xml:id="_cwCU7D9">Our research aims to explore the application of AI in complex clinical settings, incorporating data contributions not only from researchers but also directly from patients, thereby simulating real-world scenarios to ensure the practicality of AI technology in ophthalmic care.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mgv8s8g">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gxjqEEp">Overview of the study and datasets</head><p xml:id="_muGvDRT"><s xml:id="_VFtYVEc">We conducted this study at three centers, collecting 15640 data entries from 9825 subjects (4554 male, 5271 female) to develop and evaluate the IOMIDS system (Fig. <ref type="figure" target="#fig_0">1a</ref>).</s><s xml:id="_3rWUPgv">Among these, 6551 entries belong to the model development dataset, 912 entries belong to the silent evaluation dataset, and 8177 entries belong to the clinical trial dataset (Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>).</s><s xml:id="_HX54qeM">In detail, we first collected a doctor-patient communication dialog dataset of 450 entries to train the text model through prompt engineering.</s><s xml:id="_nCVvtvE">Next, to assess the diagnostic and triage efficiency of the text model, we collected Dataset A (Table <ref type="table" target="#tab_0">1</ref>), consisting of simulated patient data derived from outpatient records.</s><s xml:id="_fpnRrWU">We then gathered two image datasets (Table <ref type="table" target="#tab_0">1</ref>, Dataset B and Dataset C) for training and validating image diagnostic models, which contain only images and the corresponding image-based diagnostic data.</s><s xml:id="_DEvQyy3">Dataset D, Dataset E and Dataset F (Table <ref type="table" target="#tab_0">1</ref>) were then collected to evaluate image diagnostic model performance and develop a text-image multimodal model.</s><s xml:id="_RP4vKaX">These datasets include both patient medical histories and anterior segment images.</s><s xml:id="_nazWCkT">Following in silico development of the IOMIDS program, we collected a silent evaluation dataset to compare the diagnostic and triage efficacy among different models (Table <ref type="table" target="#tab_0">1</ref>, Dataset G).</s><s xml:id="_Av8pAwT">The early clinical evaluation consists of internal evaluation (Shanghai center) and external evaluation (Nanjing and Suqian), with 3519 entries from 2292 patients in Shanghai, 2791 entries from 1748 patients in Nanjing, and 1867 entries from 1192 patients in Suqian.</s><s xml:id="_frX9fw2">Comparison among these centers reveals significant differences in subspecialties, disease classifications, gender, age, and laterality (Supplementary Table <ref type="table" target="#tab_0">1</ref>), suggesting that these factors may influence model performance and should be considered in further analyses.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sUqT3eJ">Development of the IOMIDS system</head><p xml:id="_bZbcCDQ"><s xml:id="_4BVYB9z">To develop the text model, we categorized doctor-patient dialogs according to chief complaint themes (Supplementary Table <ref type="table">2</ref>).</s><s xml:id="_x9HWx8x">Three researchers independently reviewed the dataset and each selected a set of 90 dialogs for training.</s><s xml:id="_8MBwErg">Based on these dialogs, we used prompt engineering (Fig. <ref type="figure" target="#fig_0">1b</ref>) to develop an embodied conversational agent with ChatGPT.</s><s xml:id="_Kn2ubjN">After comparison, the most effective set of 90 dialogs (Supplementary Data 1) was identified, finalizing the text model for further research.</s><s xml:id="_Z6vA9yg">These included 11 dialogs on "dry eye", 10 on "itchy eye", 10 on "red eye", 7 on "eye swelling", 10 on "eye pain", 8 on "eye discharge", 5 on "eye masses", 13 on "blurry vision", 6 on "double vision", 6 on "eye injuries or foreign bodies", and 4 on "proptosis".</s><s xml:id="_V3UMq3Q">This text model can reliably generate questions related to the chief complaint and provide a final response based on the patient's answers, which includes diagnostic, triage, and other preventive, therapeutic, and care guidance.</s></p><p xml:id="_YyHW2Kt"><s xml:id="_Tng8r8h">After developing the text model, we evaluated its performance using Dataset A (Table <ref type="table" target="#tab_0">1</ref>).</s><s xml:id="_G9kefAY">The results demonstrated varying diagnostic accuracy across diseases (Fig. <ref type="figure">2a</ref>).</s><s xml:id="_R9xzQvV">Specifically, the model performed least effectively for primary anterior segment diseases (cataract, keratitis, and pterygium), achieving only 48.7% accuracy (Supplementary Fig. <ref type="figure">2a</ref>).</s><s xml:id="_5FaTAks">To identify conditions that did not meet development goals, we analyzed the top 1-3 diseases in each subspecialty.</s><s xml:id="_zzjkuvm">The results showed that the following did not achieve the targets of sensitivity ≥ 90% and specificity ≥ 95% (Fig. <ref type="figure">2a</ref>): keratitis, pterygium, cataract, glaucoma, and thyroid eye disease.</s><s xml:id="_DQqBTdt">Clinical experience suggests slit-lamp and smartphone captured images are valuable for diagnosing cataracts, keratitis, and pterygium.</s><s xml:id="_jnGq8me">Therefore, development efforts of image-based diagnostic model would focus on these three conditions.</s></p><p xml:id="_Ep37VG4"><s xml:id="_Jrq4ach">Beyond diagnosis, the chatbot effectively provided triage information.</s><s xml:id="_PFrdt9m">Statistical analysis revealed high overall triage accuracy (88.3%), significantly outperforming diagnostic accuracy (84.0%;</s><s xml:id="_JCz9QYj">Fig. <ref type="figure">2b</ref>; Fisher's exact test, P = 0.0337).</s><s xml:id="_6GF6DNF">All subspecialties achieved a negative predictive value ≥ 95%, and all, except optometry (79.7%) and retina (77.6%), achieved a positive predictive value ≥ 85% (Dataset A in Supplementary Data 2).</s><s xml:id="_TaFVdMP">Thus, eight out of ten subspecialties met the predefined developmental targets.</s></p><p xml:id="_DDP8pms"><s xml:id="_EuFyKXT">Future multimodal model development will focus on enhancing diagnostic capabilities while utilizing the text model's triage prompts without additional refinement.</s></p><p xml:id="_uKc9zh2"><s xml:id="_3YStzpH">To develop a multimodal model combining text and images, we first created two image-based diagnostic models based on Dataset B and Dataset C (Table <ref type="table" target="#tab_0">1</ref>), with 80% of the images used for training and 20% for validation.</s><s xml:id="_cwM2tVn">The slit-lamp model achieved disease-specific accuracies of 79.2% for cataract, 87.6% for keratitis, and 98.4% for pterygium (Supplementary Fig. <ref type="figure">2b</ref>).</s><s xml:id="_7dmwFj6">The smartphone model achieved disease-specific accuracies of 96.2% for cataract, 98.4% for keratitis, and 91.9% for pterygium (Supplementary Fig. <ref type="figure">2c</ref>).</s><s xml:id="_3kcw8fJ">After developing the image diagnostic models, we collected Dataset D, Dataset E and Dataset F (Table <ref type="table" target="#tab_0">1</ref>), which included both imaging results and patient history.</s><s xml:id="_Z6Wn5bF">Clinical diagnosis requires integrating medical history and eye imaging features, so clinical and image diagnoses may not always Patients with eye discomfort can interact with IOMIDS using natural language.</s><s xml:id="_nHZCVcE">This interaction enables IOMIDS to gather patient medical history, guide them in capturing eye lesion photos with a smartphone or uploading slit-lamp images, and ultimately provide disease diagnosis and ophthalmic subspecialty triage information.</s><s xml:id="_MU7cbNR">b Both the text model and the multimodal models follow a similar workflow for text-based modules.</s><s xml:id="_EQwruxU">After a patient inputs their chief complaint, it is classified by the chief complaint classifier using keywords, triggering relevant question and analysis prompts.</s><s xml:id="_xu3eAfx">The question prompt guides ChatGPT to ask specific questions to gather the patient's medical history.</s><s xml:id="_VhAjURM">The analysis prompt considers the patient's gender, age, chief complaint, and medical history to generate a preliminary diagnosis.</s><s xml:id="_CgtcbB9">If no image information is provided, IOMIDS provides the preliminary diagnosis along with subspecialty triage and prevention, treatment, and care guidance as the final response.</s><s xml:id="_Mp2Pdkf">If image information is available, the diagnosis prompt integrates image analysis with the preliminary diagnosis to provide a final diagnosis and corresponding guidance.</s><s xml:id="_ywDEGFt">c The text + image multimodal model is divided into text + slit-lamp, text + smartphone, and text + slit-lamp + smartphone models based on image acquisition methods.</s><s xml:id="_xu8skfJ">For smartphone-captured images, YOLOv7 segments the image to isolate the affected eye, removing other facial information, followed by analysis using a ResNet50-trained diagnostic model.</s><s xml:id="_EaywCD8">Slit-lamp captured images skip segmentation and are directly analyzed by another ResNet50-trained model.</s><s xml:id="_GGuqrV5">Both diagnostic outputs undergo threshold processing to exclude non-relevant diagnoses.</s><s xml:id="_GtTYbMD">The image information is then integrated with the preliminary diagnosis derived from textual information via the diagnosis prompt to form the multimodal model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ukqnbNg">Diagnosis Prompt</head><p xml:id="_FnrmERZ"><s xml:id="_UADf39V">Chief Complaint Classifier a b c align (Supplementary Fig. <ref type="figure" target="#fig_2">3a</ref>).</s><s xml:id="_uDAcKN3">To address this, we used image information only to rule out diagnoses.</s><s xml:id="_qpySBmk">Using image diagnosis as the gold standard, we plotted the receiver operating characteristic (ROC) curves for cataract, keratitis, and pterygium in Dataset D (Supplementary Fig. <ref type="figure" target="#fig_2">3b</ref>) and Dataset E (Supplementary Fig. <ref type="figure" target="#fig_2">3c</ref>).</s><s xml:id="_3b6vuTP">The threshold &gt;0.363 provided high specificity for all three conditions (cataract 83.5%, keratitis 99.2%, pterygium 96.6%) in Dataset D and was used to develop the text + slit-lamp multimodal model.</s><s xml:id="_XkNgD6m">Similarly, in Dataset E, the threshold &gt;0.315 provided high specificity for all three conditions (cataract 96.8%, keratitis 98.5%, pterygium 95.0%) and was used to develop the text + smartphone multimodal model.</s><s xml:id="_PRWVuCM">In the text + slit-lamp + smartphone multimodal model, we tested two methods to combine the results from slit-lamp and smartphone images.</s><s xml:id="_3cyAEzS">The first method used the union of the diagnoses excluded by each model, while the second used the intersection.</s><s xml:id="_yqhTDmX">Testing on Dataset F showed that the first method achieved significantly higher accuracy (52.2%, Supplementary Fig. <ref type="figure" target="#fig_2">3d</ref>) than the second method (31.9%,</s><s xml:id="_fGFpWfj">Supplementary Fig. <ref type="figure" target="#fig_2">3e</ref>; Fisher's exact test, P &lt; 0.0001).</s><s xml:id="_xxJyVsp">Therefore, we applied the first method in all subsequent evaluations for the text + slit-lamp + smartphone model.</s><s xml:id="_venrA7M">Using clinical diagnosis as the gold standard, the diagnostic accuracy of all multimodal models significantly improved compared to the text model; specifically, the text + slit-lamp model increased from 32.0% to 65.5% (Fisher's exact text, P &lt; 0.0001), the text + smartphone model increased from 41.6% to 64.2% (Fisher's exact test, P &lt; 0.0001), and the text + slitlamp + smartphone model increased from 37.4% to 52.2% (Fisher's exact test, P = 0.012).</s><s xml:id="_hgz6x8K">Therefore, we successfully developed four models for the IOMIDS system: the unimodal text model, the text + slit-lamp multimodal model, the text + smartphone multimodal model, and the text + slitlamp + smartphone multimodal model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_42maKvN">Silent evaluation of diagnostic and triage performance</head><p xml:id="_8CnmF4x"><s xml:id="_nRxNvPq">During the silent evaluation phase, Dataset G was collected to validate the diagnostic and triage performance of the IOMIDS system.</s><s xml:id="_fn9KXwy">Although the diagnostic performance for cataract, keratitis, and pterygium (Dataset G in Supplementary Data 3) did not meet the established clinical goal, significant improvements in diagnostic accuracy were observed for all multimodal models compared to the text model (Fig. <ref type="figure">2c</ref>).</s><s xml:id="_YaR9AfM">The Sankey diagram revealed that in the text model, 70.8% of cataract cases and 78.3% of pterygium cases were misclassified as "others" (Fig. <ref type="figure">2d</ref>).</s><s xml:id="_marCg9w">In the "others" category, the text + slit-lamp multimodal model correctly identified 88.2% of cataract cases and 63.0% of pterygium cases.</s><s xml:id="_KrP9Eaf">The text + smartphone multimodal model performed even better, correctly diagnosing 93.3% of cataract cases and 80.0% of pterygium cases.</s><s xml:id="_vAYd6yp">Meanwhile, the text + slit-lamp + smartphone multimodal model accurately identified 90.5% of cataract cases and 68.2% of pterygium cases in the same category.</s></p><p xml:id="_fQtqMc3"><s xml:id="_bZvvNMy">Regarding triage accuracy, the overall performance improved with the multimodal models.</s><s xml:id="_snUN9zT">However, the accuracy for cataract triage notably decreased, dropping from 91.7% to 62.5% in the text + slit-lamp model (Fig. <ref type="figure">2c</ref>, Fisher's exact test, P = 0.0012), to 58.3% in the text + smartphone model (Fig. <ref type="figure">2c</ref>, Fisher's exact test, P = 0.0003), and further to 53.4% in the text + slit-lamp + smartphone model (Fig. <ref type="figure">2c</ref>, Fisher's exact test, P = 0.0001).</s><s xml:id="_vtfFdEA">Moreover, neither the text model nor the three multimodal models met the established clinical goal in any subspecialty (Supplementary Data 2).</s></p><p xml:id="_G2f6ER9"><s xml:id="_7e8XrzW">We also investigated whether medical histories in the outpatient electronic system alone were sufficient for the text model to achieve accurate diagnostic and triage results.</s><s xml:id="_79PgGRw">We randomly sampled 104 patients from Dataset G and re-entered their medical dialogs into the text model (Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>).</s><s xml:id="_nVzJDep">For information not recorded in the outpatient history, responses were given as "no information available".</s><s xml:id="_dZ5nGku">The results showed a significant decrease in diagnostic accuracy, dropping from 63.5% -20.2% (Fisher's exact test, P &lt; 0.0001), while triage accuracy remained relatively unchanged, only slightly decreasing from 72.1% -70.2% (Fisher's exact test, P = 0.8785).</s><s xml:id="_wjr7wat">This study suggests that while triage accuracy of the text model is not dependent on dialog completeness, diagnostic accuracy is affected by the completeness of the answers provided.</s><s xml:id="_XAuGu7k">Therefore, thorough responses to AI chatbot queries are crucial in clinical applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YkrE4Vx">Evaluation in real clinical settings with trained researchers</head><p xml:id="_HQTRje4"><s xml:id="_krPh8tU">The clinical trial involved two parts: researcher-collected data and patiententered data (Table <ref type="table">2</ref>).</s><s xml:id="_D5dG9g5">There was a significant difference in the words input and the duration of input between researchers and patients.</s><s xml:id="_bcGyu8m">For researchercollected data, the average was 38.5 ± 8.2 words and 58.2 ± 13.5 s, while for patient-entered data, the average was 55. 5 ± 10.3 words (t-test, P = 0.002) and 128.8 ± 27.1 s (t-test, P &lt; 0.0001).</s><s xml:id="_43PBWu2">We first assessed the diagnostic performance during the researcher-collected data phase.</s><s xml:id="_3dkknpv">For the text model across six datasets (Dataset 1-3, 6-8 in Supplementary Data 3), the following number of diseases met the clinical goal for diagnosing ophthalmic diseases: 16 out of 46 diseases (46.4% of all cases) in Dataset 1, 16 out of 32 diseases (16.4% of all cases) in Dataset 2, 18 out of 28 diseases (61.4% of all cases) in Dataset 3, 19 out of 48 diseases (43.9% of all cases) in Dataset 6, 14 out of 28 diseases (35.3% of all cases) in Dataset 7, and 11 out of 42 diseases (33.3% of all cases) in Dataset 8. Thus, less than half of the cases during the researcher-collected data phase met the clinical goal for diagnosis.</s></p><p xml:id="_ThyEBdA"><s xml:id="_9NG3vaN">Next, we investigated the subspecialty triage accuracy of the text model across various datasets (Dataset 1-3, 6-8 in Supplementary Data 2).</s><s xml:id="_YmH5dqP">Our findings revealed that during internal validation, the cornea subspecialty achieved the clinical goal for triaging ophthalmic diseases.</s><s xml:id="_SveaYmz">In external validation, the general outpatient clinic, cornea subspecialty, optometry subspecialty, and glaucoma subspecialty also met these clinical criteria.</s><s xml:id="_dPpDtkA">We further compared the diagnostic and triage outcomes of the text model across six datasets.</s><s xml:id="_byXvD3m">Data analysis demonstrated that triage accuracy exceeded diagnostic accuracy in most datasets (Supplementary Fig. <ref type="figure" target="#fig_3">4a-c</ref>, <ref type="figure">e-g</ref>).</s><s xml:id="_dwFFfWe">Specifically, triage accuracy was 88.7% compared to diagnostic accuracy of 69.3% in Dataset 1 (Fig. <ref type="figure" target="#fig_2">3a</ref>; Fisher's exact test, P &lt; 0.0001), 84.1% compared to 62.4% in Dataset 2 (Fisher's exact test, P &lt; 0.0001), 82.5% compared to 75.4% in Dataset 3 (Fisher's exact test, P = 0.3508), 85.7% compared to 68.6% in Dataset 6 (Fig. <ref type="figure" target="#fig_2">3a</ref>; Fisher's exact test, P &lt; 0.0001), 80.5% compared to 66.5% in Dataset 7 (Fisher's exact test, P &lt; 0.0001), and 84.5% compared to 65.1% in Dataset 8 (Fisher's exact test, P &lt; 0.0001).</s><s xml:id="_ng8H3uX">This suggests that while the text model may not meet clinical diagnostic needs, it could potentially fulfill clinical triage requirements.</s></p><p xml:id="_uj59UQW"><s xml:id="_Qpn2buU">We then investigated the diagnostic performance of multimodal models in Dataset 2, 3, 7, and 8 (Supplementary Data 3).</s><s xml:id="_nQxxbXb">Both the text + slitlamp model and text + smartphone model demonstrated higher overall diagnostic accuracy compared to the text model in internal and external validations, with statistically significant improvements noted for the text + smartphone model in Dataset 8 (Fig. <ref type="figure" target="#fig_2">3c</ref>).</s><s xml:id="_ZRAVZns">The clinical goal for diagnosing ophthalmic diseases was achieved by 11 out of 32 diseases (13.8% of all cases) in Dataset 2, 21 out of 28 diseases (70.2% of all cases) in Dataset 3, 11 out of 28 diseases (28.5% of all cases) in Dataset 7, and 15 out of 42 diseases (50.6% of all cases) in Dataset 8.</s><s xml:id="_vEntjdT">The text + smartphone model outperformed the text model by meeting the clinical goal for diagnosis in more cases and disease types.</s><s xml:id="_m2TxXK6">For some other diseases that did not meet the clinical goal for diagnosis, significant improvements in diagnostic accuracy were also found within the multimodal models (Fig. <ref type="figure" target="#fig_2">3b</ref>).</s><s xml:id="_ycjXH3z">Therefore, the multimodal model exhibited better diagnostic performance compared to the text model.</s></p><p xml:id="_YYHTVcT"><s xml:id="_MsehAqS">Regarding to triage, some datasets of the multimodal models showed a minor decrease in accuracy compared to the text model, however, these differences were not statistically significant (Fig. <ref type="figure" target="#fig_2">3c</ref>).</s><s xml:id="_KNAt9gQ">Unlike the silent evaluation phase, in clinical applications, neither of the two multimodal models demonstrated a notable decline in triage accuracy across different diseases, including cataract (Supplementary Fig. <ref type="figure" target="#fig_4">5</ref>).</s><s xml:id="_Gv8AGJA">In summary, data collected by researchers indicated that multimodal models outperformed the text model in diagnostic accuracy but were slightly less efficient in triage.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_u3cBte5">Evaluation in real clinical settings with untrained patients</head><p xml:id="_5fYn5At"><s xml:id="_EaRAHVW">During the patient-entered data phase, considering the convenience of smartphones, we focused on the text model, the text + smartphone model, and the text + slit-lamp + smartphone model.</s><s xml:id="_W6EcbYg">First, we compared the triage accuracy.</s><s xml:id="_rvKdMwR">Consistent with the researcher-collected data phase, the overall triage accuracy of the multimodal models were slightly lower than that of the text model, but this difference was not statistically significant (Fig. <ref type="figure" target="#fig_2">3c</ref>).</s><s xml:id="_symWR56">For subspecialties, in both internal and external validation, the text and multimodal models for the general outpatient clinic and glaucoma met the clinical goals for triaging ophthalmic diseases.</s><s xml:id="_tUfXjMW">Additionally, internal validation showed that the multimodal models met these standards for cornea, optometry, and retina subspecialties.</s><s xml:id="_5nTrEm5">In external validation, the text model met the standards for cornea and retina, while the multimodal models met the standards for cataract and retina.</s><s xml:id="_qrPwrkZ">These results suggest that both the text model and multimodal models meet the triage requirements when patients input their own data.</s></p><p xml:id="_M8wDBpc"><s xml:id="_bewnwJc">Next, we compared the diagnostic accuracy of the text model and the multimodal models.</s><s xml:id="_esyjJWq">Results revealed that in both internal and external validations, all diseases met the specificity criterion of ≥ 95%.</s><s xml:id="_DWPUnNk">In Dataset 4, the text model met the clinical criterion of sensitivity ≥ 75% in 15 out of 42 diseases (40.5% of cases), while the text + smartphone multimodal model met this criterion in 24 out of 42 diseases (78.6% of cases).</s><s xml:id="_RUgtHpY">In Dataset 5, the text model achieved the sensitivity threshold of ≥ 75% in 14 out of 40 diseases (48.3% of cases), while the text + slit-lamp + smartphone multimodal model met this criterion in only 10 out of 40 diseases (35.0% of cases).</s><s xml:id="_6uA4XxJ">In Dataset 9, the text model achieved the clinical criterion in 24 out of 43 diseases (57.1%), while the text + smartphone model met the criterion in 28 out of 43 diseases (81.9%).</s><s xml:id="_ttKVfXu">In Dataset 10, the text model achieved the criterion in 25 out of 42 diseases (62.5%), whereas the text + slit-lamp + smartphone model met the criterion in 22 out of 42 diseases (50.8%).</s><s xml:id="_6xHJnnn">This suggests that the text + smartphone model outperforms the text model, while the text + slit-lamp + smartphone model does not.</s><s xml:id="_egDxMGU">Further statistical analysis confirmed the superiority of text + smartphone model when comparing its diagnostic accuracy with the text model in both Dataset 4 and Dataset 9 (Fig. <ref type="figure" target="#fig_2">3c</ref>).</s><s xml:id="_QPd5jeP">We also conducted an analysis of diagnostic accuracy for individual diseases, identifying significant improvements for certain diseases (Fig. <ref type="figure" target="#fig_2">3b</ref>).</s><s xml:id="_Q34KDP8">These findings collectively show that during the patiententered data phase, the text + smartphone model not only meets triage requirements but also delivers better diagnostic performance than both the text model and the text + slit-lamp + smartphone model.</s></p><p xml:id="_ZCyC5ts"><s xml:id="_aV5eSfq">We further compared the diagnostic and triage accuracy of the text model in Dataset 4 and Dataset 9. Consistent with previous findings, both internal validation (triage: 80.4%, diagnosis: 69.6%; Fisher's exact test, P &lt; 0.0001) and external validation (triage: 84.7%, diagnosis: 72.5%; Fisher's exact test, P &lt; 0.0001) demonstrated significantly higher triage accuracy compared to diagnostic accuracy for the text model (Supplementary Fig. <ref type="figure" target="#fig_3">4d</ref>, <ref type="figure">h</ref>).</s><s xml:id="_hkjgn5q">Examining individual diseases, cataract exhibited notably higher triage accuracy than diagnostic accuracy in internal validation (Dataset 4: triage 76.8%, diagnosis 51.2%; Fisher's exact test, P = 0.0011) and external validation (Dataset 9: triage 87.3%, diagnosis 58.2%; Fisher's exact test, P = 0.0011).</s><s xml:id="_wNM3F8f">Interestingly, in Dataset 4, the diagnostic accuracy for myopia (94.0%) was significantly higher (Fisher's exact test, P = 0.0354) than the triage accuracy (80.6%), indicating that the triage accuracy of the text model may not be influenced by diagnostic accuracy.</s><s xml:id="_fwtKEUM">Subsequent regression analysis is necessary to investigate the factors determining triage accuracy.</s></p><p xml:id="_3JZZjZN"><s xml:id="_DbBZQ6M">Due to varying proportions of the disease classifications across the three centers (Supplementary Table <ref type="table" target="#tab_0">1</ref>), we further explored changes in diagnostic and triage accuracy within each classification.</s><s xml:id="_NgJRK6E">Results revealed</s></p><p xml:id="_9jTbVxd"><s xml:id="_mcnuf4c">Table 2 | Summary of the clinical datasets used in this study</s></p><p xml:id="_reDmEWm"><s xml:id="_KnSxAFH">Internal evaluation (n = 2292) External evaluation (n = 2940) Datasets Dataset 1 (n = 1065) Dataset 2 (n =189) Dataset 3 (n = 57) Dataset 4 (n = 813) Dataset 5 (n = 168) Dataset 6 (n = 1222) Dataset 7 (n = 221) Dataset 8 (n = 688) Dataset 9 (n = 524) Dataset 10 (n = 285) Data providers Researchers Researchers Researchers Patients Patients Researchers Researchers Researchers Patients Patients Model Unimodal model Text Text Text Text Text Text Text Text Text Text Multimodal model NA Text + slit-lamp Text + smartphone Text + smartphone Text + slit-lamp + smartphone NA Text + slit-lamp Text + smartphone Text + smartphone Text + slit-lamp + smartphone Subspecialty, n (%)</s></p><p xml:id="_Bv5vsVs"><s xml:id="_5qzrasH">General outpatient clinic 358 (33.6%) 68 (36.0%) 4 (7.0%)</s><s xml:id="_87wd68z">123 (15.1%) 22 (13.0%)</s><s xml:id="_FyB7AMq">186 (15.2%) 20 (9.0%) 144 (20.9%) 142 (27.1%)</s><s xml:id="_4hNQvPF">48 (16.8%)</s><s xml:id="_m6gz28c">Cornea 201 (18.9%) 26 (13.8%) 16 (28.1%)</s><s xml:id="_FUHwwHr">208 (25.6%) 28 (16.6%)</s><s xml:id="_SbDwYbb">244 (20.0%) 38 (17.2%) 168 (24.4%) 114 (21.8%) 32 (11.2%)</s><s xml:id="_Zzxdyy4">Cataract 123 (11.5%) 16 (8.5%)</s><s xml:id="_eDnwPuG">4 (7.0%)</s><s xml:id="_5M7JhsB">131 (16.1%) 34 (20.1%)</s><s xml:id="_7CtcFGM">127 (10.4%) 39 (17.6%) 63 (9.2%) 62 (11.8%) 38 (13.3%)</s><s xml:id="_BAgUxhA">Optometry 85 (8.0%) 12 (6.3%)</s><s xml:id="_HVHpncc">21 (36.8%)</s><s xml:id="_5qjjfsy">81 (10.0%) 25 (14.8%)</s><s xml:id="_pg4rHJN">228 (18.7%) 51 (23.1%) 99 (14.4%)</s><s xml:id="_ceqWdmm">71 (13.5%) 49 (17.2%)</s><s xml:id="_MyN65wB">Glaucoma 120 (11.3%) 6 (3.2%) 2 (3.5%) 64 (7.9%) 10 (5.9%) 81 (6.6%) 27 (12.2%)</s><s xml:id="_wGxBTVv">25 (3.6%) 31 (5.9%) 27 (9.5%)</s><s xml:id="_kX6tVMR">Retina 78 (7.3%) 20 (10.6%) 1 (1.8%) 76 (9.3%) 13 (7.7%)</s><s xml:id="_FvPeNNv">99 (8.1%) 18 (8.1%)</s><s xml:id="_TShUMQ6">90 (13.1%)</s><s xml:id="_5ca6ZpJ">50 (9.5%)</s><s xml:id="_g7mCeZa">29 (10.2%)</s><s xml:id="_6sMDWPS">Orbit 42 (3.9%)</s><s xml:id="_EqXqcsz">29 (15.3%) 4 (7.0%)</s><s xml:id="_eT2PMB9">69 (8.5%) 16 (9.5%)</s><s xml:id="_dqrHZJf">145 (11.9%) 8 (3.6%) 42 (6.1%)</s><s xml:id="_2w68HtX">27 (5.2%)</s><s xml:id="_6YaSu39">18 (6.3%)</s><s xml:id="_scXwHnc">Strabismus 20 (1.9%) 3 (1.6%) 3 (5.3%)</s><s xml:id="_FPWw8Bt">16 (2.0%)</s><s xml:id="_DWzjyQz">7 (4.1%)</s><s xml:id="_p5jRwqG">64 (5.2%) 10 (4.5%) 26 (3.8%) 11 (2.1%) 22 (7.7%)</s><s xml:id="_Kc5eVAq">Emergency 27 (2.5%) 5 (2.6%) 2 (3.5%) 40 (4.9%) 7 (4.1%)</s><s xml:id="_kHVPvAs">37 (3.0%) 5 (2.3%) 27 (3.9%) 12 (2.3%) 13 (4.6%)</s><s xml:id="_W5u6Vpm">Neuro-ophthalmology 11 (1.0%) 4 (2.1%) 0 5 (0.6%) 6 (3.6%) 11 (0.9%) 5 (2.3%) 4 (0.6%) 4 (0.8%) 9 (3.2%)</s><s xml:id="_xKUhB99">Disease classification, n (%) Primary anterior segment diseases (cataract, keratitis, pterygium) a 153 (14.4%) 65 (34.4%) 9 (15.8%)</s><s xml:id="_dky5vVW">113 (13.9%) 32 (19.0%) 192 (15.7%) 61 (27.6%) 117 (17.0%) 75 (14.3%)</s><s xml:id="_2bST7wu">46 (16.1%)</s><s xml:id="_fQncUAk">Other anterior segment diseases 289 (27.1%) 68 (36.0%) 15 (26.3%) 219 (26.9%) 64 (38.1%) 372 (30.4%) 81 (36.7%) 232 (33.7%) 140 (26.7%)</s><s xml:id="_UuZMkXk">72 (25.3%)</s><s xml:id="_k4t5hqa">Vision disorders 318 (29.9%) 6 (3.2%) 14 (24.6%)</s><s xml:id="_4FHXh9F">248 (30.5%) 31 (18.5%)</s><s xml:id="_eGF737c">299 (24.5%) 8 (3.6%) 157 (22.8%) 140 (26.7%)</s><s xml:id="_F7RV5KQ">85 (29.8%)</s><s xml:id="_U2ugWjW">Intraorbital diseases and emergency 121 (11.4%) 27 (14.3%)</s><s xml:id="_BGnA5Uj">6 (10.5%) 89 (10.9%) 14 (8.3%) 94 (7.7%) 20 (9.0%) 46 (6.7%) 45 (8.6%) 31 (10.9%)</s><s xml:id="_wjpQPru">Fundus and optic nerve disorders 121 (11.4%) 17 (9.0%)</s><s xml:id="_dZ3etCZ">7 (12.3%)</s><s xml:id="_KjCW7Ks">95 (11.7%)</s><s xml:id="_bBmAFf9">21 (12.5%)</s><s xml:id="_nHmg7QR">167 (13.7%) 26 (11.8%) 88 (12.8%) 76 (14.5%) 38 (13.3%)</s><s xml:id="_y43HU3j">Eyelid diseases 63 (5.9%) 6 (3.2%) 6 (10.5%) 49 (6.0%) 6 (3.6%) 98 (8.0%) 25 (11.3%)</s><s xml:id="_zqnvuqp">48 (7.0%)</s><s xml:id="_8T2qMhU">48 (9.2%) 13 (4.6%)</s><s xml:id="_MNbpRME">Sex, n (%) Male 515 (48.4%) 90 (47.6%) 32 (56.1%) 373 (45.9%) 91 (54.2%) 600 (49.1%) 105 (47.5%) 337 (49.0%) 258 (49.2%) 129 (45.3%)</s><s xml:id="_QKQSNmp">Female 550 (51.6%) 99 (52.4%) 25 (43.9%)</s><s xml:id="_mCtY4RA">440 (54.1%) 77 (45.8%) 622 (50.9%) 116 (52.5%) 351 (51.0%) 266 (50.8%) 156 (54.7%)</s><s xml:id="_yvvmvbS">Age (mean ± SD) 38.0 ± 24.5 53.1 ± 19.3 39.9 ± 26.0 37.6 ± 24.7 42.3 ± 18.0 40.8 ± 23.8 53.1 ± 18.9 43.1 ± 22.6 37.9 ± 24.9 41.5 ± 22.6 Laterality, n (%) Unilateral 427 (40.1%) 100 (52.9%) 22 (38.6%)</s><s xml:id="_qbbXChf">357 (43.9%) 74 (44.0%) 434 (35.5%) 107 (48.4%) 242 (35.2%) 186 (35.5%) 118 (41.4%)</s><s xml:id="_zenDndN">Bilateral 638 (59.9%) 89 (47.1%) 35 (61.4%) 456 (56.1%) 94 (56.0%) 788 (64.5%) 114 (51.6%) 446 (64.8%) 338 (64.5%) 167 (58.6%)</s><s xml:id="_naSUr6v">Visit type, n (%) b First visit 877 (82.3%) 131 (69.3%) 49 (86.0%)</s><s xml:id="_ckP6Fya">656 (80.7%) 136 (81.0%) 995 (81.4%) 157 (71.0%) 553 (80.4%) 436 (83.2%) 235 (82.5%)</s><s xml:id="_hfxaPuF">Follow-up 188 (17.7%) 58 (30.7%) 8 (14.0%) 157 (19.3%) 32 (19.0%) 227 (18.6%) 64 (29.0%) 135 (19.6%) 88 (16.8%)</s><s xml:id="_U9rCBTu">50 (17.5%)</s><s xml:id="_nQyRRXM">a Due to the critical role of cataract, keratitis, and pterygium in image model development, they are classified and analyzed separately.</s><s xml:id="_P5dHvDW">b</s></p><p xml:id="_TmWz6h6"><s xml:id="_mz5qMr4">The 'first visit' category encompasses patients receiving their initial diagnosis, whereas the that, regardless of whether data was researcher-collected or patient-reported, diagnostic accuracy for primary anterior segment diseases (cataract, keratitis, pterygium) was significantly higher in the multimodal model compared to the text model in both internal and external validation (Fig. <ref type="figure" target="#fig_2">3c</ref>).</s><s xml:id="_rcc5BPq">Further analysis of cataract, keratitis, and pterygium across Datasets 2, 3, 4, 7, 8, and 9 (Fig. <ref type="figure" target="#fig_2">3b</ref>) also showed that, similar to the silent evaluation phase, multimodal model diagnostic accuracy for cataract significantly improved compared to the text model in most datasets.</s><s xml:id="_PzDEK2V">Pterygium and keratitis exhibited some improvement but showed no significant change across most datasets due to sample size limitations.</s><s xml:id="_U2swgZm">For the other five major disease categories, multimodal model diagnostic accuracy did not consistently improve and even significantly declined in some categories (Supplementary Fig. <ref type="figure">6</ref>).</s><s xml:id="_ayrky6y">These findings indicate that the six major disease categories may play crucial roles in influencing the diagnostic performance of the models, underscoring the need for further detailed investigation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HNaHbgA">Comparison of diagnostic performance in different models</head><p xml:id="_PW4jGdc"><s xml:id="_DQnKqKS">To further compare the diagnostic accuracy of different models across various datasets, we conducted comparisons within six major disease categories.</s><s xml:id="_dDp8fPN">The results revealed significant differences in diagnostic accuracy among the models across these categories (Fig. <ref type="figure" target="#fig_3">4a</ref>).</s><s xml:id="_NmWk6He">For example, when comparing the text + smartphone model (Datasets 4, 9) to the text model (Datasets 1, 6), both internal and external validations showed higher diagnostic accuracy for the former in primary anterior segment diseases, other anterior segment diseases, and intraorbital diseases and emergency categories compared to the latter (Fig. <ref type="figure" target="#fig_3">4a</ref>, <ref type="figure">b</ref>).</s><s xml:id="_EvkFDEx">Interestingly, contrary to previous findings within datasets, comparisons across datasets demonstrated a notable decrease in diagnostic accuracy for the text + slit-lamp model (Dataset 1 vs 2, Dataset 6 vs 7) and the text + slit-lamp + smartphone model (Dataset 4 vs 5, Dataset 9 vs 10) in the categories of other anterior segment diseases and vision disorders in both internal and external validations (Fig. <ref type="figure" target="#fig_3">4a</ref>).</s><s xml:id="_sAsVT2v">This suggests that, in addition to the model used and the disease categories, other potential factors may influence the model's diagnostic accuracy.</s></p><p xml:id="_Vfys7nf"><s xml:id="_GsAetzq">We then conducted univariate and multivariate regression analyses to explore factors influencing diagnostic accuracy.</s><s xml:id="_Sx7YAb4">Univariate analysis revealed that seven factors (age, laterality, number of visits, disease classification, model, data provider, and words input) significantly influence diagnostic accuracy (Supplementary Table <ref type="table" target="#tab_4">3</ref>).</s><s xml:id="_ZDwKNxg">In multivariate analysis, six factors (age, laterality, number of visits, disease classification, model, and words input) remained significant, while the data provider was no longer a critical factor (Fig. <ref type="figure" target="#fig_3">4c</ref>).</s><s xml:id="_ksAUGaR">Subgroup analysis of follow-up cases showed that only the model type significantly influenced diagnostic accuracy (Fig. <ref type="figure" target="#fig_3">4c</ref>).</s><s xml:id="_g9f6bCM">For first-visit patients, three factors (age, disease classification, and model) were still influential.</s><s xml:id="_K3McXHU">Further analysis across different age groups within each disease classification revealed that the multimodal models generally outperformed or performed comparably to the text model in most disease categories (Table <ref type="table" target="#tab_4">3</ref>).</s><s xml:id="_NSUqDrH">However, all multimodal models, including the text + slit-lamp model (OR: 0.21 [0.04-0.97]), the text + smartphone model (OR: 0.17 [0.09-0.32]),</s><s xml:id="_k2W8HmY">and the text + slit-lamp + smartphone model (OR: 0.16 [0.03-0.38]),</s><s xml:id="_fc6kPgT">showed limitations in diagnosing visual disorders in patients over 45 years old compared to the text model (Table <ref type="table" target="#tab_4">3</ref>).</s><s xml:id="_swhNTBa">Additionally, both the text + slit-lamp model (OR: 0.34 [0.20-0.59])</s><s xml:id="_Ybr2h2N">and the text + slit-lamp + smartphone model (OR: 0.67 [0.43-0.89])</s><s xml:id="_vqgadwb">were also less effective for diagnosing other anterior segment diseases in this age group.</s><s xml:id="_NydNXYY">In conclusion, for follow-up cases, both text + slit-lamp and text + smartphone models are suitable, with a preference for the text + smartphone model.</s><s xml:id="_Uf6Whdm">For first-visit patients, the text + smartphone model is recommended, but its diagnostic efficacy for visual disorders in patients over 45 years old (such as presbyopia) may be inferior to that of the text model.</s></p><p xml:id="_Tx9Eh4Z"><s xml:id="_Yj4ENCY">We also performed a regression analysis on triage accuracy.</s><s xml:id="_GfeZ9np">In the univariate logistic regression, the center and data provider significantly influenced triage accuracy.</s><s xml:id="_zG7X4eR">Multivariate regression analysis showed that only the data provider significantly impacted triage accuracy, with patiententered data significantly improving accuracy (OR: 1.40 [1.25-1.56]).</s><s xml:id="_cbaukmm">Interestingly, neither model type nor diagnostic accuracy affects triage outcomes.</s><s xml:id="_S5wcT8Y">Considering the previous data analysis results from the patiententered data phase, both the text model and the text + smartphone model are recommended as self-service triage tools for patients in clinical applications.</s><s xml:id="_Ku54g6q">Collectively, among the four models developed in our IOMIDS system, the text + smartphone model is more suitable for patient selfdiagnosis and self-triage compared to the other models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_QxGChfY">Model interpretability</head><p xml:id="_JkGxb3h"><s xml:id="_exgBt2m">In subgroup analysis, we identified limitations in the diagnostic accuracy for all multimodal models for patients over 45 years old.</s><s xml:id="_zhJym7f">The misdiagnosed cases in this age group were further analyzed to interpret the limitations.</s></p><p xml:id="_pT6Easj"><s xml:id="_aXfzYW4">Both the text + slit-lamp model (Datasets 2, 7) and the text + slit-lamp + smartphone model (Datasets 5, 10) frequently misdiagnosed other anterior segment and visual disorders as cataracts or keratitis.</s><s xml:id="_fdtfVZK">For instance, with the text + slit-lamp + smartphone model, glaucoma (18 cases, 69.2%) and conjunctivitis (22 cases, 38.6%) were often misdiagnosed as keratitis, while presbyopia (6 cases, 54.5%) and visual fatigue (11 cases, 28.9%) were commonly misdiagnosed as cataracts.</s><s xml:id="_zypAD9J">In contrast, both the text model (Datasets 1-10) and the text + smartphone model (Datasets 3, 4, 8, 9) had relatively low misdiagnosis rates for cataracts (text: 23 cases, 3.5%; text + smartphone: 91 cases, 33.7%) and keratitis (text: 16 cases, 2.4%; text + smartphone: 25 cases, 9.3%).</s><s xml:id="_Tvdc9dp">These results suggest that in our IOMIDS system, the inclusion of slit-lamp images, whether in the text + slit-lamp model or the text + slit-lamp + smartphone model, may actually hinder diagnostic accuracy due to the high false positive rate for cataracts and keratitis.</s></p><p xml:id="_xrZDZGa"><s xml:id="_SBmUvd8">We then examined whether these misdiagnoses could be justified through image analysis.</s><s xml:id="_nDK5F28">First, we reviewed the misdiagnosed cataract cases.</s></p><p xml:id="_axdNHPs"><s xml:id="_wY3wNd9">In the text + slit-lamp model, 30 images (91.0%) were consistent with a cataract diagnosis.</s><s xml:id="_ux73BH5">However, clinically, they were mainly diagnosed with glaucoma (6 cases, 20.0%) and dry eye syndrome (5 cases, 16.7%).</s><s xml:id="_wWabrER">Similarly, in the text + smartphone model, photographs of 80 cases (88.0%) were consistent with a cataract diagnosis.</s><s xml:id="_abkCv5v">Clinically, these cases were primarily diagnosed with refractive errors (20 cases), retinal diseases (15 cases), and dry eye syndrome (8 cases).</s><s xml:id="_Q6BmXS6">We then analyzed the class activation maps of the two multimodal models.</s><s xml:id="_r8G6Qxm">Both models showed regions of interest for cataracts near the lens (Supplementary Fig. <ref type="figure">7</ref>), in accordance with clinical diagnostic principles.</s><s xml:id="_3XaATsR">Thus, these multimodal models can provide some value for cataract diagnosis based on images but may lead to discrepancies with the final clinical diagnosis.</s></p><p xml:id="_FnKCcUg"><s xml:id="_FGuFgFS">Next, we analyzed cases misdiagnosed as keratitis by the text + slitlamp model.</s><s xml:id="_zFxqDrV">The results showed that only one out of 25 cases had an anterior segment photograph consistent with keratitis, indicating a high false-positive rate for keratitis with the text + slit-lamp model.</s><s xml:id="_gfArqVW">We then conducted a detailed analysis of the class activation maps generated by this model during clinical application.</s><s xml:id="_5PXVbxu">The areas of interest for keratitis were centered around the conjunctiva rather than the corneal lesions (Supplementary Fig. <ref type="figure">7a</ref>).</s><s xml:id="_4fTw3uc">Thus, the model appears to interpret conjunctival congestion as indicative of keratitis, contributing to the occurrence of falsepositive results.</s><s xml:id="_xEQyECP">In contrast, the text + smartphone model displayed areas of interest for keratitis near the corneal lesions (Supplementary Fig. <ref type="figure">7b</ref>), which aligns with clinical diagnostic principles.</s><s xml:id="_kktMJ4A">Taken together, future research should focus on refining the text + slit-lamp model for keratitis diagnosis and prioritize optimizing the balance between text-based and image-based information to enhance diagnostic accuracy across both multimodal models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_uUWeBqH">Inter-model variability and inter-expert variability</head><p xml:id="_jMYcTjj"><s xml:id="_wZNJK6c">We further evaluated the diagnostic accuracy of GPT4.0 and the domestic large language model (LLM) Qwen using Datasets 4, 5, 9, and 10.</s><s xml:id="_zeZXe2c">Additionally, we invited three trainees and three junior doctors to independently diagnose these diseases.</s><s xml:id="_TJqqszT">Since the text + smartphone model performed the best in the IOMIDS system, we compared its diagnostic accuracy with that of the other two LLMs and ophthalmologists with varying levels of experience (Fig. <ref type="figure" target="#fig_4">5a-b</ref>).</s><s xml:id="_KGqNAvZ">The text + smartphone model (80.0%) outperformed GPT4.0 (71.7%, χ² test, P = 0.033) and showed similar accuracy to the mean performance of trainees (80.6%).</s><s xml:id="_4Ba7GYT">Among the three LLMs, Qwen performed the poorest, comparable to the level of a junior doctor.</s><s xml:id="_9b6UgTy">However, all three LLMs fell short of expert-level performance, suggesting there is still potential for improvement.</s></p><p xml:id="_rNcfxD6"><s xml:id="_ksZJsJW">We then analyzed the agreement between the answers provided by the LLMs and ophthalmologists (Fig. <ref type="figure" target="#fig_4">5b</ref>).</s><s xml:id="_WuTW9Jm">Agreement among expert ophthalmologists, who served as the gold standard in our study, was generally strong (κ: 0.85-0.95).</s><s xml:id="_VT3q2Qq">Agreement among trainee doctors was moderate (κ: 0.69-0.83),</s><s xml:id="_ZEBWqaS">as was the agreement among junior doctors (κ: 0.69-0.73).</s><s xml:id="_9knDqcT">However, the agreement among the three LLMs was weaker (κ: 0.48-0.63).</s><s xml:id="_N2ccaRD"><ref type="url" target="https://doi.org/10.1038/s41746-025-01461-0">https://doi.org/10.1038/s41746-025-01461-0</ref></s></p><p xml:id="_cHCJDdv"><s xml:id="_EG9dEe9">Notably, the text + smartphone model in IOMIDS showed better agreement with experts (κ: 0.72-0.80)</s><s xml:id="_a8Wj2XP">compared to the other two LLMs (GPT4.0:</s><s xml:id="_Cjq5znZ">0.55-0.78;</s><s xml:id="_MMc6u2e">Qwen: 0.52-0.75).</s><s xml:id="_fk4r4A2">These results suggest that the text + smartphone model in IOMIDS demonstrates the best alignment with experts among the three LLMs.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qXHjr9F">Evaluation of user satisfaction and response quality</head><p xml:id="_Gw3mNhs"><s xml:id="_6gfugVQ">The IOMIDS responses not only contained diagnostic and triage results but also provided guidance on prevention, treatment, care, and follow-up (Fig. <ref type="figure" target="#fig_4">5c</ref>).</s><s xml:id="_8JvubYK">We first analyzed both researcher and patient satisfaction with these responses.</s><s xml:id="_fBEsZSw">Satisfaction was evaluated by researchers during the model development phase and the clinical trial phase; satisfaction was evaluated by patients during the clinical trial phase, regardless of the data collection method.</s><s xml:id="_9ERJsfA">Researchers rated satisfaction score significantly higher (4.63 ± 0.92) than patients (3.99 ± 1.46; t-test, P &lt; 0.0001; Fig. <ref type="figure" target="#fig_4">5c</ref>).</s><s xml:id="_jRww5NT">Patient ratings did not differ between researcher-collected (3.98 ± 1.45) and selfentered data (4.02 ± 1.49; t-test, P = 0.3996).</s><s xml:id="_a7z8mpN">Researchers frequently rated chatbot responses as very satisfied (82.5%), whereas patient ratings varied, with 20.2% finding responses not satisfied (11.7%) or slightly satisfied (8.5%), and 61.9% rating them very satisfied.</s><s xml:id="_7Hgq3BA">Further demographic analysis between these patient groups revealed that the former (45.7 ± 23.8 years) were significantly older than the latter (37.8 ± 24.4 years; t-test, P &lt; 0.0001), indicating greater acceptance and positive evaluation of AI chatbots among younger individuals.</s></p><p xml:id="_w3S6Ydw"><s xml:id="_kTDVXB2">Next, we evaluated the response quality between multimodal models and the text model (Fig. <ref type="figure" target="#fig_4">5d</ref>).</s><s xml:id="_Z6aYF8a">The multimodal models exhibited significantly higher overall information quality (4.06 ± 0.12 vs. 3.82 ± 0.14; t-test, P = 0.0031) and better understandability (78.2% ± 1.3% vs. 71.1% ± 0.7%; ttest, P &lt; 0.0001) than the text model.</s><s xml:id="_awXSseT">Additionally, the multimodal models showed significantly lower misinformation scores (1.02±0.05 vs. 1.23 ± 0.11; t-test, P = 0.0003) compared to the text model.</s><s xml:id="_aUB2FsU">Notably, the empathy score statistically decreased in multimodal models compared to the text model (3.51 ± 0.63 vs. 4.01 ± 0.56; t-text, P &lt; 0.0001), indicating lower empathy in chatbot responses from multimodal models.</s><s xml:id="_JQcBb8V">There were no significant differences in terms of grade level (readability), with both the text model and multimodal models being suitable for users at a grade 3 literacy level.</s><s xml:id="_mRWxQcM">These findings suggest that multimodal models generate highquality chatbot responses with good readability.</s><s xml:id="_HV54fpr">Future studies may focus on enhancing the empathy of these multimodal models to better suit clinical applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fudBYeF">Discussion</head><p xml:id="_ZaspnMG"><s xml:id="_J8ETRJH">The Intelligent Ophthalmic Multimodal Interactive Diagnostic System (IOMIDS) is designed to diagnose ophthalmic diseases using multimodal information and provides comprehensive medical advice, including subspecialty triage, prevention, treatment, follow-up, and care.</s><s xml:id="_ds5NyBB">During development, we created four models: a text-based unimodal model, which is an embodied conversational agent integrated with ChatGPT, and three multimodal models that combine medical history information from interactive conversations with eye images for a more thorough analysis.</s><s xml:id="_S6TtDW3">In clinical evaluations, the multimodal models significantly improved diagnostic performance over the text model for anterior segment diseases such as cataract, keratitis, and pterygium in patients aged 45 and older.</s><s xml:id="_MUYWDrZ">Previous studies also demonstrated the strength of multimodal models over unimodal models, showing that multimodal model outperformed image-only model in identifying pulmonary diseases and predicting adverse clinical outcomes in COVID-19 patients <ref type="bibr" target="#b18">19</ref> .</s><s xml:id="_kYNBf7f">Thus, multimodal models are more suitable for analyzing medical information than unimodal models.</s></p><p xml:id="_tAKsuMJ"><s xml:id="_Vz9KjNg">Notably, the text + smartphone model in the IOMIDS system demonstrated the highest diagnostic accuracy, outperforming current multimodal LLMs like GPT4.0 and Qwen.</s><s xml:id="_prkm8vq">However, while this model approaches trainee-level performance, it still falls short of matching the accuracy of expert ophthalmologists.</s><s xml:id="_RUhFyTn">GPT4.0 itself achieved accuracy only slightly higher than junior doctors.</s><s xml:id="_9eBfEJQ">Previous studies have similarly indicated that while LLMs show promise in supporting ophthalmic diagnosis and For the model type factor, the text model was used as the reference.</s></p><p xml:id="_tGgeTTu"><s xml:id="_wpFcuF2">NA not applicable <ref type="url" target="https://doi.org/10.1038/s41746-025-01461-0">https://doi.org/10.1038/s41746-025-01461-0</ref></s></p><p xml:id="_WZKZt9G"><s xml:id="_ZTSs7xc">education, they lack the nuanced precision of trained specialists, particularly in complex cases <ref type="bibr" target="#b19">20</ref> .</s><s xml:id="_Pt4Sux7">For instance, Shemer et al. tested ChatGPT's diagnostic accuracy in a clinical setting and found it lower than that of ophthalmology residents and attending physicians <ref type="bibr" target="#b20">21</ref> .</s><s xml:id="_TY5aF3t">Nonetheless, it completed diagnostic tasks significantly faster than human evaluators, highlighting its potential as an efficient adjunct tool.</s><s xml:id="_YaTKzuS">Future research should focus on refining intelligent diagnostic models for challenging and complex cases, with iterative improvements aimed at enhancing diagnostic accuracy and clinical relevance.</s></p><p xml:id="_KgEddtZ"><s xml:id="_XDBNYjk">Interestingly, the text + smartphone model outperformed the text + slit-lamp model in diagnosing cataract, keratitis, and pterygium in patients under 45 years old.</s><s xml:id="_gAQbzX4">Even though previous studies have shown significant progress in detecting these conditions using smartphone photographs <ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref> , there is insufficient evidence to support the finding that the text + slit-lamp model is less efficient than the text + smartphone model.</s><s xml:id="_kRcR3er">To address this issue, we first thoroughly reviewed the class activation maps of both models.</s><s xml:id="_ZK2c33G">We found that the slit-lamp model often focused on the conjunctival hyperemia region rather than the corneal lesion area in keratitis cases, leading to more false-positive diagnoses.</s><s xml:id="_x6nJajS">This mismatch between modelidentified areas of interest and clinical lesions suggests a flaw in our slit-lamp image analysis <ref type="bibr" target="#b11">12</ref> .</s><s xml:id="_wEm49fd">Additionally, we analyzed the imaging characteristics of the training datasets (Dataset B and Dataset C) for image-based diagnostic models.</s><s xml:id="_HjxBJuz">Dataset B exhibited a large proportion of the conjunctival region in images, particularly in keratitis cases, which often displayed extensive conjunctival redness.</s><s xml:id="_y8WsvkF">Conversely, Dataset C, comprising smartphone images, showed a smaller proportion of the conjunctival region and facilitated reducing bias towards conjunctival hyperemia in keratitis cases.</s><s xml:id="_Sr92asY">Overall, refining the anterior segment image dataset may enhance the diagnostic accuracy of the text + slit-lamp model.</s></p><p xml:id="_c74BsNR"><s xml:id="_45ytNpB">Notably, the text + smartphone model has demonstrated advantages in diagnosing orbital diseases, even though it was not specifically trained for these conditions.</s><s xml:id="_VZpbXSQ">These findings highlight the need and potential for further enhancement of the text + smartphone model in diagnosing both orbital and eyelid diseases.</s><s xml:id="_R9NaTsS">Additionally, there was no significant difference in diagnostic capabilities for retinal diseases between the text + slit-lamp model, the text + smartphone model, or the text + slit-lamp + smartphone model compared to the text-only model, which aligns with our expectations.</s><s xml:id="_rgtKyM3">This suggests that appearance images may not significantly contribute to diagnosing retinal diseases, consistent with clinical practice.</s><s xml:id="_9zPBpgS">Several studies have successfully developed deep learning models for accurately detecting retinal diseases using fundus photos <ref type="bibr" target="#b24">25</ref> , optical coherence tomography (OCT) images <ref type="bibr" target="#b25">26</ref> , and other eye-related images.</s><s xml:id="_YdE2FdE">Therefore, IOMIDS could benefit from functionalities to upload retinal examination images and enable comprehensive diagnosis, thereby improving the efficiency of diagnosing retinal diseases.</s><s xml:id="_B7zjXb7">Furthermore, we found that relying solely on medical histories from the outpatient electronic system was insufficient for the text model to achieve accurate diagnoses.</s><s xml:id="_37ZXjqm">This suggests that IOMIDS may gather clinically relevant information that doctors often overlook or fail to record in electronic systems.</s><s xml:id="_SzeysVR">Thus, future system upgrades could involve aiding doctors in conducting preliminary interviews and compiling initial medical histories to reduce their workload.</s></p><p xml:id="_7sPrDGY"><s xml:id="_hTHmRRx">Regarding subspecialty triage, consistent with prior research, the text model demonstrates markedly superior accuracy in triage compared to diagnosis <ref type="bibr" target="#b26">27</ref> .</s><s xml:id="_cDHPMwT">Additionally, we observed an intriguing phenomenon: triage accuracy is influenced not by diagnostic accuracy but by the data collector.</s><s xml:id="_MFYnTV4">Specifically, patients' self-input data resulted in significantly improved triage accuracy compared to data input by researchers.</s><s xml:id="_CbqUYJN">Upon careful analysis of the differences, we found that patient-entered data tends to be more conversational, whereas researcher-entered data tends to use medical terminology and concise expressions.</s><s xml:id="_HasKMMb">A prior randomized controlled trial (RCT) investigated how different social roles of chatbots influence the chatbot-user relationship, and results suggested that adjusting chatbot roles can enhance users' intentions toward the chatbot <ref type="bibr" target="#b27">28</ref> .</s><s xml:id="_5VVAbCm">However, no RCT study is available to investigate how users' language styles influence chatbot results.</s><s xml:id="_eCc59aM">Based on our study, we propose that if IOMIDS is implemented in home or community settings without researcher involvement, everyday conversational language in self-reports does not necessarily impair its performance.</s><s xml:id="_BhaQb7f">Therefore, IOMIDS may serve as a decision support system for self-triage to enhance healthcare efficiency and provide cost-effectiveness benefits.</s></p><p xml:id="_HQ5nJtY"><s xml:id="_WJWsuuD">Several areas for improvement were identified for our study.</s><s xml:id="_8bWwttt">Firstly, due to sample size limitations during the model development phase, we were unable to develop a combined image model for slit-lamp and smartphone images.</s><s xml:id="_43BzaeW">Instead, we integrated the results of the slit-lamp and smartphone models using logical operations, which led to suboptimal performance of the text + slit-lamp + smartphone model.</s><s xml:id="_73bHzUw">In fact, previous studies involving multiple image modalities have achieved better results <ref type="bibr" target="#b28">29</ref> .</s><s xml:id="_RhdMxJG">Therefore, it will be necessary to develop a dedicated multimodal model for slit-lamp and smartphone images in future work.</s><s xml:id="_pZxs6mX">Moreover, the multimodal model showed lower empathy compared to the text model, possibly due to its more objective diagnosis prompts contrasting with conversational styles.</s><s xml:id="_3S5Z4X5">Future upgrades will adjust the multimodal model's analysis prompts to enhance empathy in chatbot responses.</s><s xml:id="_WTxw5sq">Thirdly, older users reported lower satisfaction with IOMIDS, highlighting the need for improved human-computer interaction for this demographic.</s><s xml:id="_vZemSNr">In addition, leveraging ChatGPT's robust language capabilities and medical knowledge, we used prompt engineering to optimize for parameter efficiency, cost-effectiveness, and speed in clinical experiments.</s><s xml:id="_8e2T9Sb">However, due to limitations in OpenAI's medical capabilities, particularly its inability to pass the Chinese medical licensing exam <ref type="bibr" target="#b29">30</ref> , we aim to develop our own large language model based on real Chinese clinical dialogs.</s><s xml:id="_N9F6RXb">This model is expected to enhance diagnostic accuracy and adapt to evolving medical consensus.</s><s xml:id="_9yFYDQ8">During our study, we used GPT3.5 instead of GPT4.0 due to token usage constraints.</s><s xml:id="_dT9sXqW">Since GPT4.0 has shown superior responses to ophthalmology-related queries in recent studies <ref type="bibr" target="#b30">31</ref> , integrating GPT4.0 into IOMIDS may enhance its clinical performance.</s><s xml:id="_pe9UMP3">It is worth mentioning that the results of our study may not be applicable to other language environments.</s><s xml:id="_PaGAKfJ">Previous studies have shown that GPT responds differently to prompts in various languages, with English appearing to yield better results <ref type="bibr" target="#b31">32</ref> .</s><s xml:id="_DPxmdUY">There were also biases in linguistic evaluation, as expert assessments can vary based on language habits, semantic comprehension, and cultural values.</s><s xml:id="_R8YKm8p">Moreover, our study represents an early clinical evaluation, and comparative prospective evaluations are necessary before implementing IOMIDS in clinical practice.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ngcHhyZ">Material and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GtmHpNB">Ethics approval</head><p xml:id="_jpe5mf8"><s xml:id="_JqneXyc">The study was approved by the Institutional Review Board of Fudan Eye &amp; ENT Hospital, the Institutional Review Board of the Affiliated Eye Hospital of Nanjing Medical University, and the Institutional Review Board of Suqian First Hospital.</s><s xml:id="_Ymgm632">The study was registered on ClinicalTrials.gov</s><s xml:id="_eJdnuez">(NCT05930444) on June 26, 2023.</s><s xml:id="_ckBUTa6">It was conducted in accordance with the Declaration of Helsinki, with all participants providing written informed consent before their participation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_aHYfgPT">Study design</head><p xml:id="_RMugjve"><s xml:id="_RuD8gJq">This study aims to develop an Intelligent Ophthalmic Multimodal Interactive Diagnostic System (IOMIDS) for diagnosing and triaging ophthalmic diseases (Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>).</s><s xml:id="_pUQccPk">The IOMIDS includes four models: an unimodal text model, which is an embodied conversational agent with ChatGPT; a text + slit-lamp multimodal model, which incorporates both text and eye images captured by a slit-lamp equipment; a text + smartphone multimodal model, utilizing text along with eye images captured by a smartphone for diagnosis and triage; and a text + slit-lamp + smartphone multimodal model, which combines both image modalities with text to achieve a final diagnosis.</s><s xml:id="_2MTWcYe">Clinical validation of the three models' performance is conducted through a two-stage cross-sectional study, an initial silent evaluation stage followed by an early clinical evaluation stage, as detailed in the protocol article <ref type="bibr" target="#b32">33</ref> .</s><s xml:id="_wc6xMjx">Triage covers 10 ophthalmic subspecialties: general outpatient clinic, optometry, strabismus, cornea, cataract, glaucoma, retina, neuro-ophthalmology, orbit, and emergency.</s><s xml:id="_jYSem9K">Diagnosis involves 50 common ophthalmic diseases (Supplementary Data 4), categorized by lesion location into anterior segment diseases, fundus and optic nerve diseases, intraorbital diseases and emergencies, eyelid diseases, and visual disorders.</s><s xml:id="_Sqmc8Tv">Notably, because of the image diagnostic training conducted for cataract, keratitis, and pterygium during the development of multimodal models, anterior segment diseases are further classified into two categories: primary anterior segment diseases (including cataract, keratitis, and pterygium) and other anterior segment diseases.</s></p><p xml:id="_xNUtnDX"><s xml:id="_4YuYnuu">Collecting and formatting doctor-patient communication dialogs Doctor-patient communication dialogs were collected from the outpatient clinics of Fudan Eye &amp; ENT Hospital, covering the predetermined 50 disease types across 10 subspecialties.</s><s xml:id="_UTNJtYQ">After collection, each dialog underwent curation and formatting.</s><s xml:id="_UMTf7pV">Curation requires removing filler words and irrelevant redundant content (e.g., payment methods).</s><s xml:id="_Bbd9kM3">Formatting refers to structuring each dialog into four standardized parts: (1) chief complaint; (2) series of questions from the doctor; (3) patient's responses to each question; (4) doctor's diagnosis, triage judgment, and information on prevention, treatment, care, and follow-up.</s><s xml:id="_ujBAYuU">Subsequently, researchers (with at least 3 years of clinical experience as attending physicians) carefully reviewed the dialogs for each disease, selecting those where the doctor's questions were focused on the chief complaint, demonstrated medical reasoning, and contributed to diagnosis and triage.</s><s xml:id="_gRnPjyq">After this careful review, 90 out of 450 dialogs were selected for prompt engineering to train the text model for IOMIDS.</s><s xml:id="_98MJraB">Three researchers independently evaluated the dialogs, resulting in three sets of 90 dialogs.</s><s xml:id="_sTrNtCS">To assess the performance of the models trained with these different sets of dialogs, we created two virtual cases for each of the five most prevalent diseases across 10 subspecialties at our research institution, totaling 100 cases.</s><s xml:id="_VkY687m">The diagnostic accuracy for each set of prompts was 73%, 68%, and 52%, respectively.</s><s xml:id="_46bffD5">Ultimately, the first set of 90 dialogs (Supplementary Data 1) was chosen as the final set of prompts.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ptDc7xb">Developing a dynamic prompt system for IOMIDS</head><p xml:id="_2m4fRTt"><s xml:id="_az3TKQX">To build the IOMIDS system, we designed a dynamic prompt system that enhances ChatGPT's role in patient consultations by integrating both textual and image data.</s><s xml:id="_x4knYS5">This system supports diagnosis and triage based on either single-modal (text) or multi-modal (text and image) information.</s><s xml:id="_GJYKyTb">The overall process is illustrated in Fig. <ref type="figure" target="#fig_0">1b</ref>, with a detailed explanation provided below:</s></p><p xml:id="_SPruw4X"><s xml:id="_Ew8ctFZ">The system is grounded in a medical inquiry prompt corpus, developed by organizing 90 real-world clinical dialogs into a structured format.</s><s xml:id="_b3nZaa7">Each interview consists of four segments: "Patient's Chief Complaint," "Inquiry Questions," "Patient's Responses," and "Diagnosis and Consultation Recommendations."</s><s xml:id="_cA2mtD7">These clinical interviews are transformed into structured inquiry units, known as "prompts" within the system.</s><s xml:id="_g2AHGrx">When a patient inputs their primary complaint, the system's Chief Complaint Classifier identifies relevant keywords and matches them with corresponding prompts from the corpus.</s><s xml:id="_fYBpTRt">These selected prompts, along with the patient's initial complaint, form a question prompt that guides ChatGPT in asking about the relevant medical history related to the chief complaint.</s></p><p xml:id="_2hmDXbA"><s xml:id="_qyvz66F">After gathering the responses to these inquiries, an analysis prompt is generated.</s><s xml:id="_avuHdGw">This prompt directs ChatGPT to perform a preliminary diagnosis and triage based on the conversation history.</s><s xml:id="_u6DTx3P">The analysis prompt includes the question prompts from the previous stage, along with all questions and answers exchanged during the consultation.</s><s xml:id="_ZMkxp79">If no appearance-related or anterior segment images are provided by the patient, the system uses only this analysis prompt to generate diagnosis and triage recommendations, which are then communicated back to the patient as the final output of the text model.</s></p><p xml:id="_eJvQTdV"><s xml:id="_rVaZZUA">For cases that involve multi-modal information, we developed an additional diagnosis prompt.</s><s xml:id="_AKu6jUw">This prompt expands on the previous analysis prompt by incorporating key patient information-such as gender, age, and preliminary diagnosis/triage decisions-alongside diagnostic data obtained from slit-lamp or smartphone images.</s><s xml:id="_rJHy2C2">By combining image data with textual information, ChatGPT is able to provide more comprehensive medical recommendations, including diagnosis, triage, and additional advice based on both modalities.</s></p><p xml:id="_JFnxNZs"><s xml:id="_CbfRh9n">It is important to note that in a single consultation, the question prompt, analysis prompt, and diagnosis prompt are not independent; rather, they are interconnected and progressive.</s><s xml:id="_u7cyTCP">The question prompt is part of the analysis prompt, and the analysis prompt is integrated into the diagnosis prompt.</s></p><p xml:id="_8fpbRaK"><s xml:id="_sFzM2vW">Collecting and ground-truth labeling of images Image diagnostic data is crucial for diagnosis prompts, and to obtain this data, we need to develop an image-based diagnostic model.</s><s xml:id="_RcgP66c">Considering there are two common methods for capturing eye photos in clinical settings, both slit-lamp captured and smartphone captured eye images were collected for the development of the image-based diagnostic model.</s><s xml:id="_fVqybzY">These images encompass diseases identified as requiring image diagnosis (specifically cataract, keratitis, pterygium) through in silico evaluation of the text model (as detailed below).</s><s xml:id="_47N8GvK">For patients with different diagnoses in each eye (e.g., keratitis in one eye and dry eye in the other), each eye was included as an independent data entry.</s><s xml:id="_mABygxm">Additionally, slit-lamp and smartphone captured images for diseases with top 1-5 prevalence rates in each subspecialty were collected and categorized as "others" for training, validation, and testing of the image diagnostic model.</s></p><p xml:id="_qEwsZ5g"><s xml:id="_eHaua97">For slit-lamp images, the following criteria apply: 1. Images must be taken using the slit-lamp's diffuse light with no overexposure; 2. Both inner and outer canthi must be visible; 3. Image size must be at least 1MB, with a resolution of no less than 72 pixels/inch.</s><s xml:id="_jqphaup">For smartphone images, the following conditions must be met: 1.</s><s xml:id="_TpWydbK">The eye of interest must be naturally open; 2. Images must be captured under indoor lighting, with no overexposure; 3. The shooting distance must be within 1 meter, with focus on the eye region; 4. Image size must be at least 1MB, with a resolution of no less than 72 pixels/ inch.</s><s xml:id="_h9vmcSK">Images not meeting these requirements will be excluded from the study.</s><s xml:id="_GEWUUbE">Four specialists independently labeled images into four categories (cataract, keratitis, pterygium, and others) based solely on the image.</s><s xml:id="_xUs7B75">Consensus was reached when three or more specialists agreed on the same diagnosis; images where agreement could not be reached by at least two specialists were excluded.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3NYux6H">Developing image-based ophthalmic classification models for multimodal models</head><p xml:id="_h3e7a4j"><s xml:id="_UZ3bMqy">We developed two distinct deep learning algorithms using ResNet-50 to process images captured by slit-lamp and smartphone cameras.</s><s xml:id="_Us3Mx2t">The first algorithm was designed to detect cataracts, keratitis, and pterygium in an anterior segment image dataset (Dataset B) obtained under a slit lamp.</s><s xml:id="_E2Q7cJ4">The second algorithm targeted the detection of these conditions in a dataset (Dataset C) consisting of single-eye regions extracted from smartphone images.</s><s xml:id="_FQNs4pa">To address the challenge of non-eye facial regions in smartphone images, we collected an additional 200 images to train and validate an eyetarget detection model using YOLOv7.</s><s xml:id="_pvWSa4T">This model was specifically trained to detect eye regions by annotating the eye areas within these images.</s><s xml:id="_dS2fsmN">Of the total images, 80% were randomly assigned to the training set, and the model was trained for 300 epochs using the default learning rate and preprocessing settings specified in the YOLOv7 website (<ref type="url" target="https://github.com/WongKinYiu/yolov7">https://github.com/  WongKinYiu/yolov7</ref>).</s><s xml:id="_6BP3x4n">The remaining 20% of the images were used as a validation set, achieving a Precision of 1.0, Recall of 0.98, and mAP@.5 of 0.991.</s><s xml:id="_xDuPtbA">These images were not reused in any other experiments.</s></p><p xml:id="_tFuTQcA"><s xml:id="_gUf3maV">For the disease classification network, we created a four-class dataset consisting of cataracts, keratitis, pterygium, and "Other" categories.</s><s xml:id="_YxGyNGb">This dataset includes both anterior segment images and smartphone-captured eye images across all categories.</s><s xml:id="_WUpr4HE">The "Other" class includes normal eye images and images of various other eye conditions.</s><s xml:id="_zAfCzEY">Using a ResNet-50 model pretrained on ImageNet, we fine-tuned it on this four-class dataset for 200 epochs to optimize classification accuracy across both modalities.</s></p><p xml:id="_6n5vYyD"><s xml:id="_uvt6K94">During training, each image was resized to 224 × 224 pixels and underwent data augmentation techniques to enhance generalization.</s><s xml:id="_br69ya3">These included a 0.2 probability of random horizontal flipping, random rotations between -5 to 5 degrees, and a 0.2 probability of automatic contrast adjustments.</s><s xml:id="_MTjdjmY">White balance adjustments were also applied to standardize the images.</s><s xml:id="_8AP5ZxC">For validation and testing, images were resized to 224 × 224 pixels, underwent white balance adjustments, and were then input into the model for disease prediction.</s><s xml:id="_SyzaUj2">To improve model robustness and minimize overfitting, we employed five-fold cross-validation.</s><s xml:id="_bCdcKtG">The dataset was divided into five parts (each 20%), with four parts used for training and one part for validation in each fold.</s><s xml:id="_BYbT7aA">The final model was selected based on the highest validation accuracy, without specific constraints on sensitivity and specificity for individual models.</s></p><p xml:id="_tFSG4zq"><s xml:id="_5rk7NCX">Generating image diagnostic data for multimodal models Before being input into the diagnosis prompt, image diagnostic data underwent preprocessing.</s><s xml:id="_ubQnyAZ">Preliminary experiments revealed that when the data indicated a single diagnosis, the multimodal model might overlook patient demographics and medical history, leading to a direct image-based diagnosis.</s><s xml:id="_gbmTswy">To address this, we adjusted the image diagnostic results by excluding specific diagnoses.</s></p><p xml:id="_QCRpKds"><s xml:id="_aFVuNt3">Specifically, we modified the classification model by removing the final Softmax layer and using the scores from the fully connected (fc) layer as outputs for each category.</s><s xml:id="_RTKb7MK">These scores were then rescaled to fall within the range of [-1, 1], providing continuous diagnostic output for each image category.</s><s xml:id="_k7znKrs">The rescaled scores served as the image model's diagnostic output.</s><s xml:id="_FAZdYUn">We also collected additional datasets, Dataset D (slitlamp captured images) and Dataset E (smartphone captured images), to ensure the independence of training, validation, and testing sets.</s><s xml:id="_zA5sUhP">The model was then run on Datasets D and E, generating scores for all four categories across all images, with the image diagnosis serving as the gold standard.</s><s xml:id="_vqTUTZx">Receiver operating characteristic (ROC) curves were plotted for cataracts, keratitis, and pterygium to determine optimal thresholds for maximizing specificity across these diseases.</s><s xml:id="_Rtpkgv4">These thresholds were then established as the output standard.</s><s xml:id="_rCRbpZ9">For example, if only cataracts exceeded the threshold among the three diseases, the output label from the image diagnostic module would be "cataract".</s></p><p xml:id="_9y2TpEZ"><s xml:id="_exh4hT8">To further develop a text + slit-lamp + smartphone model, we collected Dataset F, which includes both slit-lamp and smartphone images for each individual.</s><s xml:id="_bF6d8bS">We tested two methods to combine the results from the slit-lamp and smartphone images.</s><s xml:id="_JGa7wuC">The first method used the union of diagnoses excluded by each model, while the second method used the intersection.</s><s xml:id="_QWgwwjt">For example, if the slit-lamp image excluded cataracts and the smartphone image excluded both cataracts and keratitis, the first method would exclude cataracts and keratitis, while the second method would exclude only cataracts.</s><s xml:id="_dgf5yBj">These "excluded diagnoses" were then combined with the user's analysis prompt, key patient information, and preliminary diagnosis to construct the diagnosis prompt, as shown in Fig. <ref type="figure" target="#fig_0">1c</ref>.</s><s xml:id="_Eq8ZaRn">This diagnosis prompt was then sent to ChatGPT, allowing it to generate the final multimodal diagnostic result by integrating both image-based and contextual data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hptstbM">In silico evaluation of text model</head><p xml:id="_ekq9wGe"><s xml:id="_ywAPBwP">After developing chief complaint classifiers, question prompt templates, and analysis prompt templates, we integrated these functionalities into a text model and conducted an in silico evaluation using virtual cases (Dataset A).</s><s xml:id="_raysjQu">These cases consist of simulated patient data derived from outpatient records.</s><s xml:id="_m6Xc2gB">To ensure the cohort's characteristics are representative of realworld clinical settings, we determined the total number of cases per subspecialty based on outpatient volumes over the past 3 years.</s><s xml:id="_vZKMYZw">We randomly selected cases across subspecialties to cover the predefined set of 50 disease types (Supplementary Data 4).</s><s xml:id="_jjZA4an">These 50 disease types were chosen based on their prevalence rates in each subspecialty and their diagnostic feasibility without sole dependence on physical examinations.</s><s xml:id="_3PabCFG">Our goal was to gather about 100 cases for general outpatient clinics and the cornea subspecialty, and ~50 cases for other subspecialties.</s></p><p xml:id="_83g6y4w"><s xml:id="_B743Yhj">During the evaluation process, researchers conducted data entry for each case as a new session, ensuring that chatbot responses were generated solely based on that specific case, without any prior input from other cases.</s><s xml:id="_fcvHbCc">Our primary objective was to achieve a sensitivity of ≥ 90% and a specificity of ≥ 95% for diagnosing common disease types that ranked in the top 1-3 based on outpatient volumes over the past three years within each subspecialty.</s><s xml:id="_w2NdZ35">The disease types include dry eye syndrome, allergic conjunctivitis, conjunctivitis, myopia, refractive error, visual fatigue, strabismus, keratitis, pterygium, cataract, glaucoma, vitreous opacity, retinal detachment, ptosis, thyroid eye disease, eyelid mass, chemical ocular trauma, and other eye injuries.</s><s xml:id="_8mZvQxd">For disease types that had failed to meet these performance thresholds and had the potential to be diagnosed through imaging, we would develop an image-based diagnostic system.</s><s xml:id="_KnUHEgp">Additionally, regarding triage outcomes, our secondary goal was to achieve a positive predictive value of ≥ 90% and a negative predictive value of ≥ 95%.</s><s xml:id="_7EsxE53">Since predictive values are significantly influenced by prevalence, diseases with prevalence below the 5th percentile threshold are excluded from the secondary outcome analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_C5c5VPQ">Silent evaluation and early clinical evaluation of IOMIDS</head><p xml:id="_peQ2vCW"><s xml:id="_yeH7yeE">After developing the text model and two multimodal models, all three were integrated into IOMIDS and installed on two iPhone 13 Pro devices to conduct a two-stage cross-sectional study, comprising silent evaluation and early clinical evaluation.</s><s xml:id="_UzK4SrD">During the silent evaluation, researchers collected patient gender, age, chief complaint, medical history inquiries, slit-lamp captured images, and smartphone captured images without disrupting clinical activities (Dataset G).</s><s xml:id="_9MMbGkm">If researchers encountered a specific chatbot query for which the answer could not be found in electronic medical histories, patients would be followed up with telephone interviews on the same day as their clinical visit.</s><s xml:id="_G7w84e5">Based on sample size calculations <ref type="bibr" target="#b32">33</ref> , we aimed to collect 25 cases each for cataract, keratitis, and pterygium, along with another 25 cases randomly selected for other diseases.</s><s xml:id="_RhxNX99">Following data collection, we conducted data analysis.</s><s xml:id="_WQnMCfs">If the data did not meet predefined standards, further sample expansion was considered.</s><s xml:id="_qCCxe63">These standards were set as follows: the primary outcome aimed to achieve a sensitivity of ≥ 85% and a specificity of ≥ 95% for diagnosing cataract, keratitis, and pterygium; the secondary outcome aimed to achieve a positive predictive value of ≥ 85% and a negative predictive value of ≥ 95% for subspecialty triage after excluding diseases with a prevalence below the 5th percentile threshold.</s><s xml:id="_AEvFDBC">To further investigate whether the completeness of medical histories would influence the diagnostic and triage performance of the text model, we randomly selected about half of the cases from Dataset G and re-entered the doctor-patient dialogs.</s><s xml:id="_vy4hUTH">For the chatbot queries that should be answered via telephone interviews, the researcher uniformly marked them as "no information available".</s><s xml:id="_mGbz5aa">The changes in diagnostic and triage accuracy were subsequently analyzed.</s></p><p xml:id="_3ManKWJ"><s xml:id="_mdesZGz">The early clinical evaluation was conducted at Fudan Eye &amp; ENT Hospital for internal validation and at the Affiliated Eye Hospital of Nanjing Medical University and Suqian First Hospital for external validation.</s><s xml:id="_MNkEtss">Both validations included two settings: data collection by researchers and selfcompletion by patients.</s><s xml:id="_v9rJQ4F">Data collection by researchers was conducted at the internal center from July 21 to August 20, 2023, and at the external centers from August 21 to October 31, 2023.</s><s xml:id="_M4WhqEq">Self-completion by patients took place at the internal center from November 10, 2023, to January 10, 2024, and at the external centers from January 20 to March 10, 2024.</s><s xml:id="_RjQKV6R">During the patiententered data stage, researchers guided users (patients, parents, and caregivers) through the entire process, which included selecting an appropriate testing environment, accurately entering demographic information and chief complaints, providing detailed responses to chatbot queries, and obtaining high-quality eye photos.</s><s xml:id="_B25xyzg">It is important to note that when collecting smartphone images using the IOMIDS system, we provide guidance throughout the image capture process.</s><s xml:id="_zS85zRa">Notifications are issued for problems such as excessive distance, overexposure, improper focus, misalignment, or if the eye is not open (Supplementary Fig. <ref type="figure">8</ref>).</s><s xml:id="_SAAFBGX">In both the researchercollected data phase and the patient-entered data phase, the primary goal was to achieve a sensitivity of ≥ 75% and a specificity of ≥95% for diagnosing ophthalmic diseases, excluding those with a prevalence below the 5th percentile threshold.</s><s xml:id="_TWZ7QZW">The secondary goal was to achieve a positive predictive value of ≥ 80% and a negative predictive value of ≥95% for subspecialty triage.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pchTyTz">Comparison of inter-model and model-expert agreement</head><p xml:id="_xsSZN4f"><s xml:id="_988Uamn">Five expert ophthalmologists with professor titles, three ophthalmology trainees (residents), and two junior doctors without specialized training participated in the study.</s><s xml:id="_j5RnGKa">The expert ophthalmologists were responsible for reviewing all cases, and when at least three experts reached a consensus, their diagnostic results were considered the gold standard.</s><s xml:id="_cqmzfsz">The trainees and junior doctors were involved solely in the clinical evaluation phase for Datasets 4-5 and 9-10, independently providing diagnostic results for each case.</s><s xml:id="_Q9PGgcg">Additionally, GPT-4.0 and the domestic large language model Qwen, both with image-text diagnostic capabilities, were included to generate diagnostic results for the same cases.</s><s xml:id="_Nx7CTQw">The diagnostic accuracy and consistency of these large language models were then compared with those of ophthalmologists at different experience levels.</s></p><p xml:id="_HPwjpYw"><s xml:id="_rHTqw2J">Rating for user satisfaction and response quality User satisfaction with the entire human-computer interaction experience was evaluated by both patients and researchers using a 1-5 scale (not satisfied, slightly satisfied, moderately satisfied, satisfied, and very satisfied) during early clinical evaluation study.</s><s xml:id="_Fyn84Yw">Neither the patients nor the researchers were aware of the correctness of the output when assessing satisfaction.</s><s xml:id="_xbC6tjB">Furthermore, 50 chatbot final responses were randomly selected from all datasets generated during both silent evaluation and early clinical evaluation.</s><s xml:id="_rCwgXry">Three independent researchers, who were blinded to the model types and reference standards, assessed the quality of these chatbot final responses across six aspects.</s><s xml:id="_7H7cNVg">Overall information quality was assessed using DISCERN (rated from 1 = low to 5 = high).</s><s xml:id="_ZHXqRju">Understandability and actionability were evaluated with the Patient Education Materials Assessment Tool for Printable Materials (PEMAT-P), scored from 0-100%.</s><s xml:id="_X9dcF3q">Misinformation was rated on a five-point Likert scale (from 1 = none to 5 = high).</s><s xml:id="_dfvz3CG">Empathy was also rated using a 5-point scale (not empathetic, slightly empathetic, moderately empathetic, empathetic, and very empathetic).</s><s xml:id="_kCFRrtA">Readability was analyzed using the Chinese Readability Index Explorer (CRIE; <ref type="url" target="http://www.chinesereadability.net/CRIE/?LANG=CHT">http://www.chinesereadability.net/CRIE/?LANG=CHT</ref>), which assigns scores corresponding to grade levels: 1-6 points for elementary school (grades 1-6), 7 points for middle school, and 8 points for high school.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_X9Qenn3">Statistical analysis</head><p xml:id="_TugXXuz"><s xml:id="_DxF6CEu">All data analyses were conducted using Stata/BE (version 17.0).</s><s xml:id="_aFacWwX">Continuous and ordinal variables were expressed as mean ± standard deviation.</s><s xml:id="_KS9VrRR">Categorical variables were presented as frequency (percentage).</s><s xml:id="_TEZYPkU">For baseline characteristics comparison, a two-sided t-test was used for continuous and ordinal variables, and the Chi-square test or Fisher's exact test was employed for categorical variables, as appropriate.</s><s xml:id="_NxBKhJD">Diagnosis and triage performance metrics, including sensitivity, specificity, accuracy, positive predictive values, negative predictive values, Youden index, and prevalence, were calculated for each dataset using the one-vs.-rest</s><s xml:id="_CzxNRqV">strategy.</s><s xml:id="_EVHpTSU">Diseases with a prevalence below the 5th percentile threshold were excluded from subspecialty parameter calculations.</s><s xml:id="_AVSUfA8">Reference standards and the correctness of the IOMIDS diagnosis and triage were established according to the protocol article <ref type="bibr" target="#b32">33</ref> .</s></p><p xml:id="_SYGG5VY"><s xml:id="_CCMwee9">Specifically, overall diagnostic accuracy was defined as the proportion of correctly diagnosed cases out of the total cases in each dataset, while overall triage accuracy was the proportion of correctly triaged cases out of the total cases.</s><s xml:id="_WrsQUsA">Similarly, disease-specific diagnostic and triage accuracies were calculated as the proportion of correctly diagnosed or triaged cases per disease.</s><s xml:id="_xUhVufC">Diseases were categorized into six classifications based on lesion location: primary anterior segment diseases (cataract, keratitis, pterygium), other anterior segment diseases, fundus and optic nerve diseases, intraorbital diseases and emergencies, eyelid diseases, and visual disorders.</s><s xml:id="_uuAu2Pp">Diagnostic and triage accuracies were calculated for each category.</s><s xml:id="_bYf2wUY">Fisher's exact test was used to compare accuracies of different models within each category.</s></p><p xml:id="_sFadbwk"><s xml:id="_6nQ9ZPt">Univariate logistic regression was performed to identify potential factors influencing diagnostic and triage accuracy.</s><s xml:id="_JtR3UGY">Factors with P &lt; 0.05 were further analyzed using multivariate logistic regression, with odds ratios (OR) and 95% confidence intervals (CI) calculated.</s><s xml:id="_4N5W6tr">Subgroup analyses were conducted for significant factors (e.g., disease classification) identified in the multivariate analysis.</s><s xml:id="_AfY3WAw">Notably, age was dichotomized using the mean age of all patients during the early clinical evaluation stage for inclusion in the logistic regression.</s><s xml:id="_wusSTDy">Additionally, ROC curves for cataract, keratitis, and pterygium were generated in Dataset D and Dataset E using image-based diagnosis as the gold standard.</s><s xml:id="_XhAEJfh">The area under the ROC curve (AUC) was calculated for each curve.</s><s xml:id="_RuBxErE">Agreement between answers provided by doctors and LLMs was quantified through calculation of Kappa statistics, interpreted in accordance with McHugh's recommendations <ref type="bibr" target="#b19">20</ref> .</s><s xml:id="_CWJYysU">During the evaluation of user satisfaction and response quality, a two-sided t-test was used to compare different datasets across various metrics.</s><s xml:id="_T22fJsE">Quality scores from three independent evaluators were averaged before statistical analysis.</s><s xml:id="_HdgTwbj">In this study, P values of &lt;0.05 were considered statistically significant.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 |</head><label>1</label><figDesc><div><p xml:id="_VNGtJHf"><s xml:id="_ynn8qhB">Fig.1| Overview of the workflow and functionality of IOMIDS.</s><s xml:id="_pQkJxA8">a Intelligent Ophthalmic Multimodal Interactive Diagnostic System (IOMIDS) is an embodied conversational agent integrated with ChatGPT designed for multimodal diagnosis using eye images and medical history.</s><s xml:id="_DtbKmjS">It comprises a text model and an image model.</s><s xml:id="_eTPQzQm">The text model employs classifiers for chief complaints, along with question and analysis prompts developed from real doctorpatient dialogs.</s><s xml:id="_3MRgYyG">The image model utilizes eye photos taken with a slit-lamp and/or smartphone for imagebased diagnosis.</s><s xml:id="_hxnNChu">These modules combine through diagnostic prompts to create a multimodal model.</s><s xml:id="_gheYdku">Patients with eye discomfort can interact with IOMIDS using natural language.</s><s xml:id="_6a7GrNY">This interaction enables IOMIDS to gather patient medical history, guide them in capturing eye lesion photos with a smartphone or uploading slit-lamp images, and ultimately provide disease diagnosis and ophthalmic subspecialty triage information.</s><s xml:id="_b5wFEdU">b Both the text model and the multimodal models follow a similar workflow for text-based modules.</s><s xml:id="_hAug7W4">After a patient inputs their chief complaint, it is classified by the chief complaint classifier using keywords, triggering relevant question and analysis prompts.</s><s xml:id="_8Gwhm5Z">The question prompt guides ChatGPT to ask specific questions to gather the patient's medical history.</s><s xml:id="_u3XVRSH">The analysis prompt considers the patient's gender, age, chief complaint, and medical history to generate a preliminary diagnosis.</s><s xml:id="_apMVWK7">If no image information is provided, IOMIDS provides the preliminary diagnosis along with subspecialty triage and prevention, treatment, and care guidance as the final response.</s><s xml:id="_rgHmZ9F">If image information is available, the diagnosis prompt integrates image analysis with the preliminary diagnosis to provide a final diagnosis and corresponding guidance.</s><s xml:id="_ZK9w6yF">c The text + image multimodal model is divided into text + slit-lamp, text + smartphone, and text + slit-lamp + smartphone models based on image acquisition methods.</s><s xml:id="_FjfEWuB">For smartphone-captured images, YOLOv7 segments the image to isolate the affected eye, removing other facial information, followed by analysis using a ResNet50-trained diagnostic model.</s><s xml:id="_JYEmFfR">Slit-lamp captured images skip segmentation and are directly analyzed by another ResNet50-trained model.</s><s xml:id="_Yw9qast">Both diagnostic outputs undergo threshold processing to exclude non-relevant diagnoses.</s><s xml:id="_T8YAmsJ">The image information is then integrated with the preliminary diagnosis derived from textual information via the diagnosis prompt to form the multimodal model.</s></p></div></figDesc><graphic coords="3,225.67,51.77,330.76,498.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 |</head><label>3</label><figDesc><div><p xml:id="_nYwDXqs"><s xml:id="_qcRCAtc">Fig. 3 | Internal and external evaluation of IOMIDS performance on diagnosis and triage.</s><s xml:id="_wqWYG5S">a Radar charts of disease-specific diagnosis (red) and triage (green) accuracy after clinical evaluation of the text model in internal (left, Dataset 1) and external (right, Dataset 6) centers.</s><s xml:id="_w7FEtWy">Asterisks indicate significant differences between diagnosis and triage accuracy based on Fisher's exact test.</s><s xml:id="_RKX2A9q">b Circular stacked bar charts of disease-specific diagnostic accuracy across different models from internal (left, Dataset 2-4) and external (right, Dataset 7-9) evaluations.</s><s xml:id="_Fm4cxbA">Solid bars represent the text model, while hollow bars represent multimodal models.</s><s xml:id="_3MyQ69v">Asterisks indicate significant differences in diagnostic accuracy between two models based on Fisher's exact test.</s><s xml:id="_Dr5Vqaf">c Bar charts of overall accuracy (upper) and accuracy of primary anterior segment diseases (lower) for diagnosis (red) and triage (green) across different models in Dataset 2-5 and Dataset 7-10.</s><s xml:id="_sRtUtpv">The line graphs below denote study centers (internal, external), models used (text, text + slit-lamp, text + smartphone, text + slit-lamp + smartphone), and data provider (researchers, patients).</s><s xml:id="_Dzvc3wy">* P &lt; 0.05, ** P &lt; 0.01, *** P &lt; 0.001, **** P &lt; 0.0001.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 |</head><label>4</label><figDesc><div><p xml:id="_Rty98MJ"><s xml:id="_zW2wfkW">Fig. 4 | Comparison of diagnostic performance across different models.</s><s xml:id="_Dm6TmJX">a Bar charts of diagnostic accuracy calculated for each disease classification across different models from internal (upper, Dataset 1-5) and external (lower, Dataset 6-10) evaluations.</s><s xml:id="_268uEfz">The bar colors represent disease classifications.</s><s xml:id="_98svpkh">The line graphs below denote study centers, models used, and data providers.</s><s xml:id="_Ns2Skw7">b Heatmaps of diagnostic performance metrics after internal (left) and external (right) evaluations of different models.</s><s xml:id="_hDMZ5eQ">For each heatmap, metrics in the text model and text + smartphone model are normalized together by column, ranging from -2 (blue) to 2 (red).</s><s xml:id="_X9Q3MNh">Disease types are classified into six categories and displayed by different colors.</s><s xml:id="_T9TP5sB">c Multivariate logistic regression analysis of diagnostic accuracy for all cases (left) and subgroup analysis for follow-up cases (right) during clinical evaluation.</s><s xml:id="_yUV3Hk7">The first category in each factor is used as a reference, and OR values and 95% CIs for other categories are calculated against these references.</s><s xml:id="_vQJg7Re">OR, odds ratio; CI, confidence interval; *P &lt; 0.05, **P &lt; 0.01, ***P &lt; 0.001, ****P &lt; 0.0001.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 |</head><label>5</label><figDesc><div><p xml:id="_p9kwFsR"><s xml:id="_3Hv7FFA">Fig. 5 | Assessment of model-expert agreement and the quality of chatbot responses.</s><s xml:id="_8mU2VSQ">a Comparison of diagnostic accuracy of IOMIDS (text + smartphone model), GPT4.0,</s><s xml:id="_x99NZTX">Qwen, expert ophthalmologists, ophthalmology trainees, and unspecialized junior doctors.</s><s xml:id="_53ZAnAW">The dotted lines represent the mean performance of ophthalmologists at different experience levels.</s><s xml:id="_x52hjTD">b Heatmap of Kappa statistics quantifying agreement between diagnoses provided by AI models and ophthalmologists.</s><s xml:id="_vTPfjQX">c Kernel density plots of user satisfaction rated by researchers (red) and patients (blue) during clinical evaluation.</s><s xml:id="_vMt8J43">d Example of an interactive chat with IOMIDS (left) and quality evaluation of the chatbot response (right).</s><s xml:id="_m89Nv5a">On the left, the central box displays the patient interaction process with IOMIDS: entering chief complaint, answering system questions step-by-step, uploading a standard smartphone-captured eye photo, and receiving diagnosis and triage information.</s><s xml:id="_nfH2pVF">The chatbot response includes explanations of the condition and guidance for further medical consultation.</s><s xml:id="_psTd3EG">The surrounding boxes show a researcher's evaluation of six aspects of the chatbot response.</s><s xml:id="_qqPQvcw">The radar charts on the right illustrate the quality evaluation across six aspects for chatbot responses generated by the text model (red) and the text + image model (blue).</s><s xml:id="_P9stcWr">The axes for each aspect correspond to different coordinate ranges due to varying rating scales.</s><s xml:id="_ZjnHcpd">Asterisks indicate significant differences between two models based on two-sided t-test.</s><s xml:id="_vhfMbJA">** P &lt; 0.01, *** P &lt; 0.001, **** P &lt; 0.0001.</s></p></div></figDesc><graphic coords="12,147.31,227.00,144.52,342.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 |</head><label>1</label><figDesc><div><p xml:id="_Rnz32Re"><s xml:id="_TbEHjRv">Summary of the development and silent evaluation datasets used in this study</s></p></div></figDesc><table><row><cell>In silico development (n = 3741)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 |</head><label>3</label><figDesc><div><p xml:id="_asWpAFG"><s xml:id="_EWsqjUr">Subgroup analysis of diagnostic accuracy with multivariate logistic regression for newly diagnosed patients</s></p></div></figDesc><table><row><cell>Eyelid diseases</cell><cell></cell><cell>Age ≤ 45 Age &gt; 45</cell><cell></cell><cell>0.63 0.48</cell><cell>(0.23-1.74) (0.11-2.06)</cell><cell>0.71 0.66</cell><cell>(0.42-1.22) (0.31-1.41)</cell><cell>0.55 0.63</cell><cell>(0.10-1.08) (0.27-1.83)</cell></row><row><cell>Fundus and optic nerve</cell><cell>diseases</cell><cell>Age ≤ 45 Age &gt; 45</cell><cell></cell><cell>1.02 0.97</cell><cell>(0.85-2.36) (0.38-2.51)</cell><cell>1.46 0.88</cell><cell>(0.70-3.04) (0.59-1.32)</cell><cell>0.98 0.92</cell><cell>(0.53-2.82) (0.43-1.85)</cell></row><row><cell>Intraorbital diseases and</cell><cell>emergency</cell><cell>Age ≤ 45 Age &gt; 45</cell><cell></cell><cell>3.76 1.63</cell><cell>(0.83-17.10) (0.67-3.97)</cell><cell>2.78 2.06</cell><cell>(1.39-5.56) (1.21-3.49)</cell><cell>3.16 2.03</cell><cell>(0.93-8.72) (0.72-3.58)</cell></row><row><cell>Visual disorders</cell><cell></cell><cell>Age ≤ 45 Age &gt; 45</cell><cell></cell><cell>0.26 0.21</cell><cell>(0.04-1.54) (0.04-0.97)</cell><cell>1.56 0.17</cell><cell>(1.07-2.26) (0.09-0.32)</cell><cell>0.37 0.16</cell><cell>(0.06-2.08) (0.03-0.38)</cell></row><row><cell>Other anterior segment</cell><cell>diseases</cell><cell>Age ≤ 45 Age &gt; 45</cell><cell></cell><cell>0.69 0.34</cell><cell>(0.35-1.36) (0.20-0.59)</cell><cell>2.19 1.21</cell><cell>(1.52-3.17) (0.85-1.73)</cell><cell>0.89 0.67</cell><cell>(0.53-2.48) (0.43-0.89)</cell></row><row><cell>Primary anterior segment</cell><cell>diseases</cell><cell>Age ≤ 45 Age &gt; 45</cell><cell></cell><cell>2.78 4.69</cell><cell>(0.97-7.90) (2.52-8.71)</cell><cell>3.31 6.67</cell><cell>(1.40-7.82) (4.37-10.17)</cell><cell>2.92 5.85</cell><cell>(1.23-6.73) (3.41-9.68)</cell></row><row><cell></cell><cell></cell><cell>Subgroup</cell><cell>Model (reference: text)</cell><cell>Text + slit-lamp</cell><cell></cell><cell>Text + smartphone</cell><cell></cell><cell>Text + slit-</cell><cell>lamp + smartphone</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_2hD2xqf"><s xml:id="_rwcK5Nr">npj Digital Medicine | (2025) 8:64</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_azjTEQc"><s xml:id="_D4nkYdp">'follow-up' category includes patients with a prior diagnosis.</s><s xml:id="_HF2wJtP">https://doi.org/10.1038/s41746-025-01461-0</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_kyaQge4">Acknowledgements</head><p xml:id="_pmuvYgB"><s xml:id="_jUdmpZq">We sincerely appreciated the support from <rs type="person">Yuhang Jiang</rs> (<rs type="affiliation">Jiangsu Health Vocational College</rs>) for assisting with data collection, <rs type="person">Xiang Zeng</rs> (<rs type="affiliation">Fudan University</rs>) for assisting with image formatting, and <rs type="person">Shi Yin</rs> and <rs type="person">Xingxing Wu</rs> for assisting with auditing the clinical data.</s><s xml:id="_Ky5tmDA">The author(s) declare financial support was received for the research, authorship, and/or publication of this article.</s><s xml:id="_uQa8wWQ">This work was supported by <rs type="funder">Shanghai Hospital Development Center</rs> (Grant Number <rs type="grantNumber">SHDC2023CRD013</rs>), <rs type="funder">National Key R&amp;D Program of China</rs> (Grant Number <rs type="grantNumber">2018YFA0701700</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (Grant Numbers <rs type="grantNumber">82371101</rs>, <rs type="grantNumber">U20A20170</rs>, <rs type="grantNumber">62271337</rs>, <rs type="grantNumber">62371328</rs>, <rs type="grantNumber">62371326</rs>), <rs type="funder">Zhejiang Key Research and Development Project</rs> (Grant Number <rs type="grantNumber">2021C03032</rs>), <rs type="funder">Natural Science Foundation of Jiangsu Province of China</rs> (Grant Numbers <rs type="grantNumber">BK20211308</rs>, <rs type="grantNumber">BK20231310</rs>).</s></p><p xml:id="_B7BCFP5"><s xml:id="_ntFPPud">Diseases; <rs type="funder">Key Laboratory of Myopia and Related Eye Diseases, Chinese Academy of Medical Sciences, Shanghai, China</rs>. <ref type="bibr" target="#b2">3</ref>Shanghai Key Laboratory of Visual Impairment and Restoration, Shanghai, China. <ref type="bibr" target="#b3">4</ref>MIPAV Lab, <rs type="institution">School of Electronics and Information Engineering, Soochow University, Suzhou, China</rs>. <ref type="bibr" target="#b4">5</ref></s><s xml:id="_vgkrYzD">Department of Ophthalmology, The First Affiliated Hospital, <rs type="institution">Zhejiang University School of Medicine, Hangzhou, Zhejiang</rs>, <rs type="institution">China</rs>. <ref type="bibr" target="#b5">6</ref>School of Basic Medical Sciences, Fudan University, Shanghai, China. <ref type="bibr" target="#b6">7</ref>State Key Laboratory of Genetic Engineering, <rs type="institution">Department of Computational Biology, School of Life Sciences, Fudan University, Shanghai, China</rs>. <ref type="bibr" target="#b7">8</ref></s><s xml:id="_3FU9bcE">Department of Epidemiology, <rs type="institution">School of Public Health</rs>, and The <rs type="funder">Key Laboratory of Public Health Safety of Ministry of</rs> <rs type="institution">Education, Fudan University, Shanghai, China</rs>. <ref type="bibr" target="#b8">9</ref></s><s xml:id="_H7GrVkf">Shanghai Jiao Tong University Instrument Analysis Center, Shanghai, China. <ref type="bibr" target="#b9">10</ref></s><s xml:id="_43mgtzh">The Affiliated Eye Hospital, <rs type="institution">Nanjing Medical University, Nanjing, China</rs>. <ref type="bibr" target="#b10">11</ref></s><s xml:id="_6jFGxcV">Department of Ophthalmology, Suqian First Hospital, Suqian, China. <ref type="bibr" target="#b11">12</ref></s><s xml:id="_3qME8mv">The Fourth School of Clinical Medicine, Nanjing Medical University, Nanjing, China. <ref type="bibr" target="#b12">13</ref>State Key Laboratory of Radiation Medicine and Protection, Soochow University, Suzhou, China. <ref type="bibr" target="#b13">14</ref></s><s xml:id="_7ZmPeS9">These authors contributed equally: <rs type="person">Ruiqi Ma</rs>, <rs type="person">Qian Cheng</rs>, <rs type="person">Jing Yao</rs>, <rs type="person">Zhiyu Peng</rs>, <rs type="person">Mingxu Yan</rs>, <rs type="person">Jie Lu</rs>, <rs type="person">Jingjing Liao</rs>. <ref type="bibr" target="#b14">15</ref></s><s xml:id="_q42KJqG">These authors jointly supervised this work: <rs type="person">Fei Shi</rs>, <rs type="person">Rui Zhang</rs>, <rs type="person">Xinjian Chen</rs>, <rs type="person">Chen Zhao</rs>.</s><s xml:id="_Qf7yBWr">e-mail: shifei@suda.edu.cn;</s><s xml:id="_AS5bdx4">zhangrui936@163.com;</s><s xml:id="_GJqGDJB">xjchen@suda.edu.cn;</s><s xml:id="_zdDgFwM">dr_zhaochen@fudan.edu.cn</s><s xml:id="_MtFg9Vd"><ref type="url" target="https://doi.org/10.1038/s41746-025-01461-0">https://doi.org/10.1038/s41746-025-01461-0</ref></s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NJdBknx">
					<idno type="grant-number">SHDC2023CRD013</idno>
				</org>
				<org type="funding" xml:id="_mP8CXQC">
					<idno type="grant-number">2018YFA0701700</idno>
				</org>
				<org type="funding" xml:id="_Jx6MCGh">
					<idno type="grant-number">82371101</idno>
				</org>
				<org type="funding" xml:id="_cabawPM">
					<idno type="grant-number">U20A20170</idno>
				</org>
				<org type="funding" xml:id="_pvGnbrp">
					<idno type="grant-number">62271337</idno>
				</org>
				<org type="funding" xml:id="_VuQeS5X">
					<idno type="grant-number">62371328</idno>
				</org>
				<org type="funding" xml:id="_ZqjqnMc">
					<idno type="grant-number">62371326</idno>
				</org>
				<org type="funding" xml:id="_bTFfVtG">
					<idno type="grant-number">2021C03032</idno>
				</org>
				<org type="funding" xml:id="_5KMR7w7">
					<idno type="grant-number">BK20211308</idno>
				</org>
				<org type="funding" xml:id="_jH48gHD">
					<idno type="grant-number">BK20231310</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SAuZZCf">Data availability</head><p xml:id="_QnUBPUp"><s xml:id="_5VGqGGb">The data that support the findings of this study are divided into two groups: published data and restricted data.</s><s xml:id="_5MzwsXq">The authors declare that the published data supporting the main results of this study can be obtained within the paper and its Supplementary Information.</s><s xml:id="_kSBHqer">For research purposes, a representative image deidentified using masks on patient's face in this study is available.</s><s xml:id="_tJpxdNK">In the case of noncommercial use, researchers can contact the corresponding authors for access to the raw data.</s><s xml:id="_px32vTa">Due to portrait rights and patient privacy restrictions, restricted data, including raw videos, are not provided to the public.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_z9Tx6nu">Code availability</head><p xml:id="_PFQejYR"><s xml:id="_ZBYjqqq">The code for the prompt engineering aspect of our work is embedded within a Java backend development system, making it tightly integrated with our proprietary infrastructure.</s><s xml:id="_wDw8pPB">Due to this integration, we are unable to publicly release this specific portion of the code.</s><s xml:id="_jfvzSRB">However, the image processing component of our system utilizes open-source models, specifically ResNet-50 and YOLOv7, which are readily available on GitHub and other repositories.</s><s xml:id="_eS4JXFR">Readers interested in the image processing methodologies we employed can easily access and utilize these models from the following sources: ResNet-50: <ref type="url" target="https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/resnet.py">https://github.  com/huggingface/pytorch-image-models/blob/main/timm/models/  resnet.py</ref>.</s><s xml:id="_U4uCh3q">YOLOv7: <ref type="url" target="https://github.com/WongKinYiu/yolov7">https://github.com/WongKinYiu/yolov7</ref>.</s><s xml:id="_2WpnwkT">We encourage readers to explore these repositories for implementation details and further insights into the image processing techniques used in our study.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Au3WTMR">Author contributions</head><p xml:id="_JsPWyWs"><s xml:id="_syBsfJb">R.M., Q.C., Z.P., J.Q., X.C., and C.Z. conceived and designed the study.</s><s xml:id="_J5JStRe">J.G., K.X., J.C., R.Z., H.C., X.Z., and J.H. guided the research design.</s><s xml:id="_hA6xzRn">R.M. and Z.P. collected data in the in silico development stage.</s><s xml:id="_jjZAXAa">J.L. and M.Y. collected data during the silent evaluation and early clinical evaluation stages.</s><s xml:id="_u2VD5jd">J.L., M.Y., J.W., W.X., X.L., and H.C. organized image data.</s><s xml:id="_U22cMzd">R.M., J.Y., Z.P., L.T., P.J., X.L., L.G., H.C., and X.W. audited the clinical data.</s><s xml:id="_65muBgn">Q.C., J.L., W.Z., D.X., B.N., J.W., F.S., and X.C. implemented the multimodal system.</s><s xml:id="_pjUbN8h">R.M., Q.C., J.Y., and Z.P. drafted the initial manuscript.</s><s xml:id="_ASHfmnW">L.T., J.L., M.Y., W.S., and J.W. coordinated end-user testing.</s><s xml:id="_tZ5TRdc">R.M., Q.C., and Y.Z.</s><s xml:id="_BEuWPMv">conducted statistical analysis.</s><s xml:id="_sx6DQRn">Y.Z., J.Z., B.Q., and Q.J. contributed to data collection and entry.</s><s xml:id="_brFgddw">F.S., J.Q., X.C., and C.Z. were responsible for the decision to submit the manuscript.</s><s xml:id="_bgkKNcG">All authors reviewed and revised the manuscript, approved the final version, and had access to all the data in the study.</s><s xml:id="_5v4Q86f"><ref type="url" target="https://doi.org/10.1038/s41746-025-01461-0">https://doi.org/10.1038/s41746-025-01461-0</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_HVfgjxK">Competing interests</head><p xml:id="_nqEBTxm"><s xml:id="_VCntRyU">The authors declare no competing interests.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_BzDYPkM">Multimodal generative AI for precision health</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.1056/AI-S2300233</idno>
		<ptr target="https://ai.nejm.org/doi/full/10.1056/AI-S2300233" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Meq2YR8">NEJM AI</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Poon H. Multimodal generative AI for precision health. NEJM AI https://ai.nejm.org/doi/full/10.1056/AI-S2300233 (2023).</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_fUZQzzE">Artificial intelligence and digital health in global eye health: opportunities and challenges</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1016/s2214-109x(23)00323-6</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ue9Dhx4">Lancet Glob. Health</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1432" to="1443" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tan, T. F. et al. Artificial intelligence and digital health in global eye health: opportunities and challenges. Lancet Glob. Health 11, 1432-1443 (2023).</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_qzuawv3">Development and international validation of custom-engineered and code-free deep-learning models for detection of plus disease in retinopathy of prematurity: a retrospective study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-025-01461-0</idno>
		<ptr target="https://doi.org/10.1038/s41746-025-01461-0" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ggs62x9">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="340" to="E349" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wagner, S. K. et al. Development and international validation of custom-engineered and code-free deep-learning models for detection of plus disease in retinopathy of prematurity: a retrospective study. Lancet Digit. Health 5, E340-E349 (2023). https://doi.org/10.1038/s41746-025-01461-0</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_SuZm8eA">Unveiling the clinical incapabilities: a benchmarking study of GPT-4V(ision) for ophthalmic multimodal image analysis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.1136/bjo-2023-325054</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ybKy2Yg">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1384" to="1389" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu, P. S., Chen, X. L., Zhao, Z. W. &amp; Shi, D. L. Unveiling the clinical incapabilities: a benchmarking study of GPT-4V(ision) for ophthalmic multimodal image analysis. Br. J. Ophthalmol. 108, 1384-1389 (2024).</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_BkmpHfh">Vision-language models for feature detection of macular diseases on optical coherence tomography</title>
		<author>
			<persName><forename type="first">F</forename><surname>Antaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Keane</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2024.1165</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9Au5aXW">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="573" to="576" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Antaki, F., Chopra, R. &amp; Keane, P. A. Vision-language models for feature detection of macular diseases on optical coherence tomography. JAMA Ophthalmol. 142, 573-576 (2024).</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_UZNdKa8">Cost-effectiveness and cost-utility of a digital technology-driven hierarchical healthcare screening pattern in China</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-024-47211-w</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_J4424Qd">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">3650</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, X. H. et al. Cost-effectiveness and cost-utility of a digital technology-driven hierarchical healthcare screening pattern in China. Nat. Commun. 15, 3650 (2024).</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_pSC9G5y">Large language models encode clinical knowledge</title>
		<author>
			<persName><forename type="first">K</forename><surname>Singhal</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-023-06291-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vwcmanp">Nature</title>
		<imprint>
			<biblScope unit="volume">620</biblScope>
			<biblScope unit="page" from="172" to="180" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172-180 (2023).</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_sPXwweK">Accuracy of an artificial intelligence chatbot&apos;s interpretation of clinical ophthalmic images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mihalache</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2024.0017</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ug73cTK">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="321" to="326" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mihalache, A. et al. Accuracy of an artificial intelligence chatbot&apos;s interpretation of clinical ophthalmic images. JAMA Ophthalmol. 142, 321-326 (2024).</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_hbKQ3Ke">Benchmarking large language models&apos; performances for myopia care: a comparative analysis of ChatGPT-3.5, ChatGPT-4.0, and Google Bard</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ebv95bY">Ebiomedicine</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page">104770</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lim, Z. W. et al. Benchmarking large language models&apos; performances for myopia care: a comparative analysis of ChatGPT-3.5, ChatGPT- 4.0, and Google Bard. Ebiomedicine 95, 104770 (2023).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_tEfn8gM">Artificial intelligence chatbot performance in triage of ophthalmic conditions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Arepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fromal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.jcjo.2023.07.016</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QmU3ssv">Can. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="301" to="e308" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lyons R. J., Arepalli S. R., Fromal O., Choi J. &amp; D. Jain N. Artificial intelligence chatbot performance in triage of ophthalmic conditions. Can. J. Ophthalmol. 59, e301-e308 (2023).</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_BBn7GDT">Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ayers</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamainternmed.2023.1838</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_89VaXV6">JAMA Int. Med</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page" from="589" to="596" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ayers, J. W. et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA Int. Med. 183, 589-596 (2023).</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_BrHcNTd">Class-aware attention network for infectious keratitis diagnosis using corneal photographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2022.106301</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AdXmQ8E">Comp. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">106301</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, J. H. et al. Class-aware attention network for infectious keratitis diagnosis using corneal photographs. Comp. Biol. Med. 151, 106301 (2022).</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_GCFYSG9">Early detection of visual impairment in young children using a smartphone-based deep learning system</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-022-02180-9</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hC6kFa8">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="493" to="503" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen, W. B. et al. Early detection of visual impairment in young children using a smartphone-based deep learning system. Nat. Med. 29, 493-503 (2023).</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_VTvKNB9">Smartphone-acquired anterior segment images for deep learning prediction of anterior chamber depth: a proof-ofconcept study</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="DOI">10.3389/fmed.2022.912214</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HBvsN2y">Front. Med</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">912214</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qian, C. X. et al. Smartphone-acquired anterior segment images for deep learning prediction of anterior chamber depth: a proof-of- concept study. Front. Med. 9, 912214 (2022).</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_pqyF3pW">A global review of publicly available datasets for ophthalmological imaging: barriers to access, usability, and generalisability</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<idno type="DOI">10.1016/s2589-7500(20)30240-5</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PGUw9pH">Lancet Digit. Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="51" to="E66" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Khan, S. M. et al. A global review of publicly available datasets for ophthalmological imaging: barriers to access, usability, and generalisability. Lancet Digit. Health 3, E51-E66 (2021).</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_2xbQPJc">New meaning for NLP: the trials and tribulations of natural language processing with GPT-3 in ophthalmology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ellershaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Korot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Keane</surname></persName>
		</author>
		<idno type="DOI">10.1136/bjophthalmol-2022-321141</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RQeRP4T">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="889" to="892" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nath, S., Marie, A., Ellershaw, S., Korot, E. &amp; Keane, P. A. New meaning for NLP: the trials and tribulations of natural language processing with GPT-3 in ophthalmology. Br. J. Ophthalmol. 106, 889-892 (2022).</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_hCzmDkx">The collaborative community on ophthalmic imaging roadmap for artificial intelligence in age-related Macular degeneration</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ddE9Za7">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="43" to="E59" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dow, E. R. et al. The collaborative community on ophthalmic imaging roadmap for artificial intelligence in age-related Macular degeneration. Ophthalmology 129, E43-E59 (2022).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_nUccvcP">A machine learning system to optimise triage in an adult ophthalmic emergency department: a model development and validation study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brandao-De-Resende</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eclinm.2023.102331</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aRntusw">Eclinicalmedicine</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">102331</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brandao-de-Resende, C. et al. A machine learning system to optimise triage in an adult ophthalmic emergency department: a model development and validation study. Eclinicalmedicine 66, 102331 (2023).</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_wwwERJK">A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_meqgWxu">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="743" to="755" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou, H. Y. et al. A transformer-based representation-learning model with unified processing of multimodal input for clinical diagnostics. Nat. Biomed. Eng. 7, 743-755 (2023).</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_Zn8zurn">Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Thirunavukarasu</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pdig.0000341</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YvfMGcf">PLOS Digit. Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">341</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thirunavukarasu, A. J. et al. Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study. PLOS Digit. Health 3, e0000341 (2024).</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_Y7W4H7c">Diagnostic capabilities of ChatGPT in ophthalmology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shemer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00417-023-06363-z</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dPrDb9T">Graefes Arch. Clin. Exp. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page" from="2345" to="2352" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shemer, A. et al. Diagnostic capabilities of ChatGPT in ophthalmology. Graefes Arch. Clin. Exp. Ophthalmol. 262, 2345-2352 (2024).</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_UTxuTZ9">Feasibility assessment of infectious keratitis depicted on slit-lamp and smartphone photographs using deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijmedinf.2021.104583</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HrqFDHx">Int. J. Med. Inform</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">104583</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, L. et al. Feasibility assessment of infectious keratitis depicted on slit-lamp and smartphone photographs using deep learning. Int. J. Med. Inform. 155, 104583 (2021).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_DwasvdC">Detecting cataract using smartphones</title>
		<author>
			<persName><forename type="first">B</forename><surname>Askarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Chong</surname></persName>
		</author>
		<idno type="DOI">10.1109/jtehm.2021.3074597</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fTcr9ru">IEEE J. Transl. Eng. Health Med</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3800110</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Askarian, B., Ho, P. &amp; Chong, J. W. Detecting cataract using smartphones. IEEE J. Transl. Eng. Health Med. 9, 3800110 (2021).</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_WSJmCdj">Accurate detection and grading of pterygium through smartphone by a fusion training model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1136/bjo-2022-322552</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3MKyRQb">Br. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="336" to="342" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, Y. et al. Accurate detection and grading of pterygium through smartphone by a fusion training model. Br. J. Ophthalmol. 108, 336-342 (2024).</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_S6uhVzq">Artificial intelligence for screening of multiple retinal and optic nerve diseases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamanetworkopen.2022.9960</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yDS8U75">JAMA Netw. Open</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">229960</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dong, L. et al. Artificial intelligence for screening of multiple retinal and optic nerve diseases. JAMA Netw. Open 5, e229960 (2022).</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_DjrxJnH">A multimodal imaging-based deep learning model for detecting treatment-requiring retinal vascular diseases: model development and validation study</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Pz43He3">JMIR Med. Inform</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">28868</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kang, E. Y.-C. et al. A multimodal imaging-based deep learning model for detecting treatment-requiring retinal vascular diseases: model development and validation study. JMIR Med. Inform. 9, e28868 (2021).</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_tHUp9Ad">Exploring diagnostic precision and triage proficiency: a comparative study of GPT-4 and bard in addressing common ophthalmic complaints</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zandi</surname></persName>
		</author>
		<idno type="DOI">10.3390/bioengineering11020120</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5BPXTFj">Bioeng. Basel</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zandi, R. et al. Exploring diagnostic precision and triage proficiency: a comparative study of GPT-4 and bard in addressing common ophthalmic complaints. Bioeng. Basel 11, 120 (2024).</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_5NrHcwK">The effects of health care chatbot personas with different social roles on the client-chatbot bond and usage intentions: development of a design codebook and web-based study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_p5gTfgK">J. Med. Int. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">32630</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nissen, M. et al. The effects of health care chatbot personas with different social roles on the client-chatbot bond and usage intentions: development of a design codebook and web-based study. J. Med. Int. Res. 24, e32630 (2022).</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_WqSq6by">Multimodal machine learning using visual fields and peripapillary circular OCT scans in detection of glaucomatous optic neuropathy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kysQPVR">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="171" to="180" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiong, J. et al. Multimodal machine learning using visual fields and peripapillary circular OCT scans in detection of glaucomatous optic neuropathy. Ophthalmology 129, 171-180 (2022).</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_vcjmhJ4">Performance of ChatGPT on Chinese national medical licensing examinations: a five-year examination evaluation study for physicians, pharmacists and nurses</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zong</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12909-024-05125-7</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FTYY96V">Bmc Med. Edu</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zong, H. et al. Performance of ChatGPT on Chinese national medical licensing examinations: a five-year examination evaluation study for physicians, pharmacists and nurses. Bmc Med. Edu. 24, 143 (2024).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_nyMwtF8">Popular large language model chatbots&apos; accuracy, comprehensiveness, and self-awareness in answering ocular symptom queries</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pushpanathan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isci.2023.108163</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3GFzTMQ">Iscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">108163</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pushpanathan K. et al. Popular large language model chatbots&apos; accuracy, comprehensiveness, and self-awareness in answering ocular symptom queries. Iscience 26, 108163 (2023).</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_yMYFm2M">Uncovering language disparity of ChatGPT on retinal vascular disease classification: cross-sectional study</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_b8BkJDd">J. Med. Int. Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">51926</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, X. C. et al. Uncovering language disparity of ChatGPT on retinal vascular disease classification: cross-sectional study. J. Med. Int. Res. 26, e51926 (2024).</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_KsU7T7H">Development and evaluation of multimodal AI for diagnosis and triage of ophthalmic diseases using ChatGPT and anterior segment images: protocol for a two-stage cross-sectional study</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2023.1323924</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AtANY8Q">Front. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1323924</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peng, Z. et al. Development and evaluation of multimodal AI for diagnosis and triage of ophthalmic diseases using ChatGPT and anterior segment images: protocol for a two-stage cross-sectional study. Front. Artif. Intell. 6, 1323924 (2023).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
