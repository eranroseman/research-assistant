<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_zku7m3f">Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis</title>
				<funder>
					<orgName type="full">NIHR Imperial Biomedical Research Centre</orgName>
					<orgName type="abbreviated">BRC</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/501100013342</idno>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ravi</forename><surname>Aggarwal</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Institute of Global Health Innovation , Imperial College London , London , UK.</note>
								<orgName type="department">Institute of Global Health Innovation</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Viknesh</forename><surname>Sounderajah</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Institute of Global Health Innovation , Imperial College London , London , UK.</note>
								<orgName type="department">Institute of Global Health Innovation</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guy</forename><surname>Martin</surname></persName>
							<idno type="ORCID">0000-0002-5759-962X</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Institute of Global Health Innovation , Imperial College London , London , UK.</note>
								<orgName type="department">Institute of Global Health Innovation</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S W</forename><surname>Ting</surname></persName>
							<idno type="ORCID">0000-0003-2264-7174</idno>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Singapore Eye Research Institute , Singapore National Eye Center , Singapore , Singapore.</note>
								<orgName type="institution" key="instit1">Singapore Eye Research Institute</orgName>
								<orgName type="institution" key="instit2">Singapore National Eye Center</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Karthikesalingam</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Institute of Global Health Innovation , Imperial College London , London , UK.</note>
								<orgName type="department">Institute of Global Health Innovation</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dominic</forename><surname>King</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Institute of Global Health Innovation , Imperial College London , London , UK.</note>
								<orgName type="department">Institute of Global Health Innovation</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hutan</forename><surname>Ashrafian</surname></persName>
							<email>h.ashrafian@imperial.ac.uk</email>
							<idno type="ORCID">0000-0003-1668-0672</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Institute of Global Health Innovation , Imperial College London , London , UK.</note>
								<orgName type="department">Institute of Global Health Innovation</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ara</forename><surname>Darzi</surname></persName>
							<idno type="ORCID">0000-0001-7815-7989</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Institute of Global Health Innovation , Imperial College London , London , UK.</note>
								<orgName type="department">Institute of Global Health Innovation</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akselrod</forename><surname>-Ballin</surname></persName>
						</author>
						<author>
							<persName><surname>Al-Antari</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">Hospital , Korea</note>
								<orgName type="institution">Hospital</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">Photographs Kim&apos;s Eye Hospital , Korea</note>
								<orgName type="institution">Photographs Kim&apos;s Eye Hospital</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<note type="raw_affiliation">University of Tokyo Hospital , Tokyo</note>
								<orgName type="institution">University of Tokyo Hospital</orgName>
								<address>
									<settlement>Tokyo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<note type="raw_affiliation">University of Tokyo Hospital , Tokyo</note>
								<orgName type="institution">University of Tokyo Hospital</orgName>
								<address>
									<settlement>Tokyo</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<note type="raw_affiliation"><label>a</label> Eyes Kitwe Central Hospital Eye Unit , Zambia ( ) Local Validation (Chinese Glaucoma Study Alliance) ; (</note>
								<orgName type="laboratory">Local Validation (Chinese Glaucoma Study Alliance)</orgName>
								<orgName type="institution" key="instit1">Eyes</orgName>
								<orgName type="institution" key="instit2">Kitwe Central Hospital Eye Unit</orgName>
								<address>
									<country key="ZM">Zambia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<note type="raw_affiliation"><label>b</label> ) Beijing Tongren Hospital ; (</note>
								<orgName type="institution">Beijing Tongren Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<note type="raw_affiliation"><label>c</label> ) Peking University Third Hospital ; (</note>
								<orgName type="institution">Peking University Third Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<note type="raw_affiliation"><label>d</label> ) Harbin Medical University First Hospital ; Duke University Scans Tsukazaki Hospital and Tokushima University Hospital Tsukazaki Hospital and Tokushima University Hospital K-fold cross 118</note>
								<orgName type="department">Scans Tsukazaki Hospital</orgName>
								<orgName type="institution" key="instit1">Harbin Medical University First Hospital</orgName>
								<orgName type="institution" key="instit2">Duke University</orgName>
								<orgName type="institution" key="instit3">Tokushima University Hospital</orgName>
								<orgName type="institution" key="instit4">Tsukazaki Hospital</orgName>
								<orgName type="institution" key="instit5">Tokushima University Hospital</orgName>
								<address>
									<addrLine>K-fold cross 118</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<note type="raw_affiliation">University of Chicago</note>
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_w98dw6E">Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7989F0E0C4286D4CF8E2C14C0581BC52</idno>
					<idno type="DOI">10.1038/s41746-021-00438-z</idno>
					<note type="submission">Received: 6 October 2020; Accepted: 25 February 2021;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T07:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_zYEk5TP">Retinal fundus photography Early and advanced glaucoma Preperimetric open-angle glaucoma Retinal fundus photography Glaucomatous optic discs Retinal fundus photography Breast cancer Expert reader</term>
					<term xml:id="_4Sv2nqd">histology</term>
					<term xml:id="_PmbZwn2">No Mammogram Breast cancer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_Euawwj7"><p xml:id="_gUaStgX"><s xml:id="_RmF7MMt">Deep learning (DL) has the potential to transform medical diagnostics.</s><s xml:id="_uAt6rD2">However, the diagnostic accuracy of DL is uncertain.</s><s xml:id="_VZrbVm3">Our aim was to evaluate the diagnostic accuracy of DL algorithms to identify pathology in medical imaging.</s><s xml:id="_fmbgSRb">Searches were conducted in Medline and EMBASE up to January 2020.</s><s xml:id="_Z6SDfYh">We identified 11,921 studies, of which 503 were included in the systematic review.</s><s xml:id="_BDVu9x6">Eightytwo studies in ophthalmology, 82 in breast disease and 115 in respiratory disease were included for meta-analysis.</s><s xml:id="_8SNfg3G">Two hundred twenty-four studies in other specialities were included for qualitative review.</s><s xml:id="_VbrC5Kz">Peer-reviewed studies that reported on the diagnostic accuracy of DL algorithms to identify pathology using medical imaging were included.</s><s xml:id="_GUkRt2q">Primary outcomes were measures of diagnostic accuracy, study design and reporting standards in the literature.</s><s xml:id="_uDTnBz9">Estimates were pooled using random-effects metaanalysis.</s><s xml:id="_UDySX7M">In ophthalmology, AUC's ranged between 0.933 and 1 for diagnosing diabetic retinopathy, age-related macular degeneration and glaucoma on retinal fundus photographs and optical coherence tomography.</s><s xml:id="_Q73XyXh">In respiratory imaging, AUC's ranged between 0.864 and 0.937 for diagnosing lung nodules or lung cancer on chest X-ray or CT scan.</s><s xml:id="_zgvKeez">For breast imaging, AUC's ranged between 0.868 and 0.909 for diagnosing breast cancer on mammogram, ultrasound, MRI and digital breast tomosynthesis.</s><s xml:id="_7tpShrb">Heterogeneity was high between studies and extensive variation in methodology, terminology and outcome measures was noted.</s><s xml:id="_xwE9S9Z">This can lead to an overestimation of the diagnostic accuracy of DL algorithms on medical imaging.</s><s xml:id="_HXwURdA">There is an immediate need for the development of artificial intelligence-specific EQUATOR guidelines, particularly STARD, in order to provide guidance around key issues in this field.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GQEfUs8">INTRODUCTION</head><p xml:id="_6XrZGBc"><s xml:id="_3emrCQm">Artificial Intelligence (AI), and its subfield of deep learning (DL) <ref type="bibr" target="#b0">1</ref> , offers the prospect of descriptive, predictive and prescriptive analysis, in order to attain insight that would otherwise be untenable through manual analyses <ref type="bibr" target="#b1">2</ref> .</s><s xml:id="_w5JNHWM">DL-based algorithms, using architectures such as convolutional neural networks (CNNs), are distinct from traditional machine learning approaches.</s><s xml:id="_wKeWNXW">They are distinguished by their ability to learn complex representations in order to improve pattern recognition from raw data, rather than requiring human engineering and domain expertise to structure data and design feature extractors <ref type="bibr" target="#b2">3</ref> .</s></p><p xml:id="_PMNXCG8"><s xml:id="_tpEDPx6">Of all avenues through which DL may be applied to healthcare; medical imaging, part of the wider remit of diagnostics, is seen as the largest and most promising field <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref> .</s><s xml:id="_EsRjDhn">Currently, radiological investigations, regardless of modality, require interpretation by a human radiologist in order to attain a diagnosis in a timely fashion.</s><s xml:id="_4uA2RQz">With increasing demands upon existing radiologists (especially in low-to-middle-income countries) <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> , there is a growing need for diagnosis automation.</s><s xml:id="_e4DgtAH">This is an issue that DL is able to address <ref type="bibr">9</ref> .</s></p><p xml:id="_XWYreaW"><s xml:id="_nyngzTC">Successful integration of DL technology into routine clinical practice relies upon achieving diagnostic accuracy that is noninferior to healthcare professionals.</s><s xml:id="_n37k33u">In addition, it must provide other benefits, such as speed, efficiency, cost, bolstering accessibility and the maintenance of ethical conduct.</s></p><p xml:id="_emcV9nT"><s xml:id="_P72nYhs">Although regulatory approval has already been granted by the Food and Drug Administration for select DL-powered diagnostic software to be used in clinical practice <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11</ref> , many note that the critical appraisal and independent evaluation of these technologies are still in their infancy <ref type="bibr" target="#b11">12</ref> .</s><s xml:id="_FKyv9rT">Even within seminal studies in the field, there remains wide variation in design, methodology and reporting that limits the generalisability and applicability of their findings <ref type="bibr" target="#b12">13</ref> .</s><s xml:id="_3eKqu59">Moreover, it is noted that there has been no overarching medical specialty-specific meta-analysis assessing diagnostic accuracy of DL performance, particularly in ophthalmology, respiratory medicine and breast surgery, which have the most diagnostic studies to date <ref type="bibr" target="#b12">13</ref> .</s></p><p xml:id="_Bvy8Gez"><s xml:id="_Yw9GaQ7">Therefore, the aim of this review is to (1) quantify the diagnostic accuracy of DL in speciality-specific radiological imaging modalities to identify or classify disease, and (2) to appraise the variation in methodology and reporting of DL-based radiological diagnosis, in order to highlight the most common flaws that are pervasive across the field.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CUep6gJ">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kDPrhFX">Search and study selection</head><p xml:id="_JgtEXk5"><s xml:id="_Cw5q6sG">Our search identified 11,921 abstracts, of which 9484 were screened after duplicates were removed.</s><s xml:id="_MCjzRSt">Of these, 8721 did not fulfil inclusion criteria based on title and abstract.</s><s xml:id="_djXvCuK">Seven hundred sixty-three full manuscripts were individually assessed and 260 were excluded at this step.</s><s xml:id="_bDMPxwQ">Five hundred three papers fulfilled inclusion criteria for the systematic review and contained data required for sensitivity, specificity or AUC.</s><s xml:id="_EwbQ4Yc">Two hundred seventythree studies were included for meta-analysis, 82 in ophthalmology, 115 in respiratory medicine and 82 in breast cancer (see Fig. <ref type="figure" target="#fig_0">1</ref>).</s><s xml:id="_KkpW2hS">These three fields were chosen to meta-analyse as they had the largest numbers of studies with available data.</s><s xml:id="_6FHpz2m">Two hundred twenty-four other studies were included for qualitative synthesis in other medical specialities.</s><s xml:id="_dS2ZYmt">Summary estimates of imaging and speciality-specific diagnostic accuracy metrics are described in Table <ref type="table" target="#tab_0">1</ref>.</s><s xml:id="_YP7QHEg">Units of analysis for each speciality and modality are indicated in Tables <ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_5">3</ref><ref type="table" target="#tab_8">4</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_arjK8Gk">Ophthalmology imaging</head><p xml:id="_QCcWcST"><s xml:id="_C7pEgX7">Eighty-two studies with 143 separate patient cohorts reported diagnostic accuracy data for DL in ophthalmology (see Table <ref type="table" target="#tab_1">2</ref> and Supplementary References 1).</s><s xml:id="_VBQ3Dvc">Optical coherence tomography (OCT) and retinal fundus photographs (RFP) were the two imaging modalities performed in this speciality with four main pathologies being diagnosed-diabetic retinopathy (DR), age-related macular degeneration (AMD), glaucoma and retinopathy of prematurity (ROP).</s></p><p xml:id="_R3s2uN3"><s xml:id="_wUJeYJX">Only eight studies <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> used prospectively collected data and 29 (refs. <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref> ) st</s><s xml:id="_mT9nKAw">dies validated algorithms on external datasets.</s><s xml:id="_UMghG9N">No studies provided a prespecified sample size calculation.</s><s xml:id="_ah8EjcT">Twenty-five studies <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref><ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref> compared algorithm performance against healthcare professionals.</s><s xml:id="_X63yByF">Reference standards, definitions of disease and threshold for diagnosis varied greatly as did the method of internal validation used.</s><s xml:id="_XUbapBC">There was high heterogeneity across all studies (see Table <ref type="table" target="#tab_1">2</ref>).</s></p><p xml:id="_QwSj5xK"><s xml:id="_FwWJR4v">Diabetic retinopathy: Twenty-five studies with 48 different patient cohorts reported diagnostic accuracy data for all, referable or vision-threatening DR on RFP.</s><s xml:id="_v36Q2QT">Twelve studies and 16 cohorts reported on diabetic macular oedema (DME) or early DR on OCT scans.</s><s xml:id="_jtz9vAv">AUC was 0.939 (95% CI 0.920-0.958)</s><s xml:id="_j6j8eAN">for RFP versus 1.00 (95% CI 0.999-1.000)</s><s xml:id="_CnwnH7d">for OCT.</s></p><p xml:id="_7xHJT7b"><s xml:id="_rcs4Yp3">Age-related macular degeneration: Twelve studies reported diagnostic accuracy data for features of varying severity of AMD on RFP (14 cohorts) and 11 studies in OCT (21 cohorts).</s><s xml:id="_f68KStz">AUC was 0.963 (95% CI 0.948-0.979)</s><s xml:id="_ekUsq5Z">for RFP versus 0.969 (95% CI 0.955-0.983)</s><s xml:id="_ZYkQt8v">for OCT.</s></p><p xml:id="_dJrFFvW"><s xml:id="_paVTdsy">Glaucoma: Seventeen studies with 30 patient cohorts reported diagnostic accuracy for features of glaucomatous optic neuropathy, optic discs or suspect glaucoma on RFP and five studies with 6 cohorts on OCT.</s><s xml:id="_cDCCpHz">AUC was 0.933 (95% CI 0.924-0.942)</s><s xml:id="_CdKbx36">for RFP and 0.964 (95% CI 0.941-0.986)</s><s xml:id="_kX9a5Cn">for OCT.</s><s xml:id="_exzNTAF">One study <ref type="bibr" target="#b33">34</ref> with six cohorts on RFP provided contingency tables.</s><s xml:id="_pu6Ep2M">When averaging across the cohorts, the pooled sensitivity was 0.94 (95% CI 0.92-0.96)</s><s xml:id="_Z8aVSPs">and pooled specificity was 0.95 (95% CI 0.91-0.97).</s><s xml:id="_emqFJmT">The AUC of the summary receiver-operating characteristic (SROC) curve was 0.98 (95% CI 0.96-0.99)-see</s><s xml:id="_s2FJtCj">Supplementary Fig. <ref type="figure" target="#fig_0">1</ref>.</s></p><p xml:id="_zzrertC"><s xml:id="_D7qNy4y">Retinopathy of prematurity: Three studies reported diagnostic accuracy for identifying plus diseases in ROP from RFP. Sensitivity was 0.960 (95% CI 0.913-1.008)</s><s xml:id="_Uf6qbQA">and specificity was 0.907 (95% CI 0.907-1.066).</s><s xml:id="_Db88HAU">AUC was only reported in two studies so was not pooled.</s></p><p xml:id="_8qyADZF"><s xml:id="_CFWpG5Q">Others: Eight other studies reported on diagnostic accuracy in ophthalmology either using different imaging modalities (ocular images and visual fields) or for identifying other diagnoses (pseudopapilloedema, retinal vein occlusion and retinal detachment).</s><s xml:id="_hn7EryX">These studies were not included in the meta-analysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Nfa5jPT">Respiratory imaging</head><p xml:id="_SP5a56n"><s xml:id="_jxzj6zr">One hundred and fifteen studies with 244 separate patient cohorts report on diagnostic accuracy of DL on respiratory disease (see Table <ref type="table" target="#tab_5">3</ref> and Supplementary References 2).</s><s xml:id="_wkCBmJP">Lung nodules were largely identified on CT scans, whereas chest X-rays (CXR) were used to diagnose a wide spectrum of conditions from simply being 'abnormal' to more specific diagnoses, such as pneumothorax, pneumonia and tuberculosis.</s></p><p xml:id="_j5HPe8w"><s xml:id="_AbNWZDp">Only two studies <ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63</ref> used prospectively collected data and 13 (refs. <ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref></s><s xml:id="_Ph8Mzw4">4]<ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b67">[68]</ref><ref type="bibr" target="#b68">[69]</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref><ref type="bibr" target="#b72">[73]</ref><ref type="bibr" target="#b73">[74]</ref><ref type="bibr" target="#b74">[75]</ref> ) studies validated algorithms on external data.</s><s xml:id="_P22hTzC">No studies provided a prespecified sample size calculation.</s><s xml:id="_yVxnTdx">Twentyone <ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref><ref type="bibr" target="#b64">[65]</ref><ref type="bibr" target="#b65">[66]</ref><ref type="bibr" target="#b66">[67]</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b75">[76]</ref><ref type="bibr" target="#b76">[77]</ref><ref type="bibr" target="#b77">[78]</ref><ref type="bibr" target="#b78">[79]</ref><ref type="bibr" target="#b79">[80]</ref><ref type="bibr" target="#b80">[81]</ref><ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref> studies compared algorithm performance against healthcare professionals. Reference s</s><s xml:id="_yVfBA8J">andards varied greatly as did the method of internal validation used.</s><s xml:id="_dERrRRd">There was high heterogeneity across all studies (see Table <ref type="table" target="#tab_5">3</ref>).</s></p><p xml:id="_YdWTdur"><s xml:id="_s5yrdQM">Lung nodules: Fifty-six studies with 74 separate patient cohorts reported diagnostic accuracy for identifying lung nodules on CT scans on a per lesion basis, compared with nine studies and 14 patient cohorts on CXR.</s><s xml:id="_FgrZfXt">AUC was 0.937 (95% CI 0.924-0.949)</s><s xml:id="_SUD76YB">for CT versus 0.884 (95% CI 0.842-0.925)</s><s xml:id="_J2RubcM">for CXR.</s><s xml:id="_b28KtHX">Seven studies reported on diagnostic accuracy for identifying lung nodules on CT scans on a per scan basis, these were not included in the meta-analysis.</s></p><p xml:id="_WK2UdYz"><s xml:id="_pf33Cyh">Lung cancer or mass: Six studies with nine patient cohorts reported diagnostic accuracy for identifying mass lesions or lung cancer on CT scans compared with eight studies and ten cohorts on CXR.</s><s xml:id="_mufjGVp">AUC was 0.887 (95% CI 0.847-0.928)</s><s xml:id="_H2HpfC5">for CT versus 0.864 (95% CI 0.827-0.901)</s><s xml:id="_9rfrMBJ">for CXR.</s></p><p xml:id="_cAErctT"><s xml:id="_mzwvYxe">Abnormal Chest X-ray: Twelve studies reported diagnostic accuracy for abnormal CXR with 13 different patient cohorts.</s><s xml:id="_aft74PG">AUC was 0.917 (95% CI 0.869-0.966),</s><s xml:id="_56cABBk">sensitivity was 0.873 (95% CI 0.762-0.985)</s><s xml:id="_UMP59Mk">and specificity was 0.894 (95% CI 0.860-0.929).</s></p><p xml:id="_TSw6Wge"><s xml:id="_EHkBKMQ">Pneumothorax: Ten studies reported diagnostic accuracy for pneumothorax on CXR with 14 different patient cohorts.</s><s xml:id="_6Wy6epu">AUC was 0.910 (95% CI 0.863-0.957),</s><s xml:id="_upJW9WX">sensitivity was 0.718 (95% CI 0.433-1.004)</s><s xml:id="_j5u6Daq">and specificity was 0.918 (95% CI 0.870-0.965).</s><s xml:id="_2D6XJnB">Five patient cohorts from two studies <ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b88">89</ref> provided contingency tables with raw diagnostic accuracy.</s><s xml:id="_MH6BrZZ">When averaging across the cohorts, the pooled sensitivity was 0.70 (95% CI 0.45-0.87)</s><s xml:id="_kCVudDH">and pooled  2018 16 (a) VGG-16; (b) Inception-v3; (c) ResNet50 Yes 1482 Images ADAGES and DIGS Random split No Expert consensus No Retinal fundus photography Glaucomatous optic neuropathy Das et al. 2019 VGG-16 No 1000 Images UCSD Holdout method No Expert consensus No OCT DME De Fauw et al. 2018 51 (a) U-Net (b) customised CNN No (a) 997; (b) 116 (a) Scans (Topcon device); (b) scans Moorfields, London Random split No Follow up Yes OCT Urgent referral eye disease</s></p><p xml:id="_AAZy6vp"><s xml:id="_JgVTAWT">Table 2 continued Study Model Prospective?</s><s xml:id="_ZVEYmD3">Test set Population Test datasets Type of internal validation External validation Reference standard AI vs clinician?</s><s xml:id="_BU9myvn">Imaging modality Pathology (Spectralis device) ElTanboly et al. 2016 Deep fusion classification network (DFCN) No 12 OCT scans Holdout method No NR No OCT Early DR Gargeya et al. 2017 26 CNN No (a) 15,000 (b) 1748; (c) 463 Photographs (a) EyePACS-1; (b) Messidor-2; (c) E-Opthma Random split Yes Expert consensus No Retinal fundus photography DR Gomez-Valverde et al. 2019 52 VGG-19 No 494 Photographs ESPERANZA Random split No Expert consensus Yes Retinal fundus photographs Glaucoma suspect or glaucoma Grassman et al. 2018 27 Ensemble: random forest No (a) 12,019; (b) 5555 Images (a) AREDS dataset; (b) KORA dataset Random split Yes Reading centre grader No Retinal fundus photography AMD-AREDS 9 step Gulshan et al. 2019 17 Inception-v3 Yes 3049 Photographs Prospective NA Yes Expert consensus Yes Retinal fundus photographs Referable DR Gulshan et al. 2016 28 Inception-v3 No (a) 8788; (b) 1745 Photographs (a) EyePACS-1; (b) Messidor-2 Random split Yes Reading centre grader Yes Retinal fundus photography Referable DR Hwang et al. 2019 29 (a) ResNet50; (b) VGG-16; (c) Inception-v3; (d) ResNet50; (e) VGG-16; (f ) Inception-v3 No (a-c) 3872; (d-f ) 750 Images (a-c) Department of Ophthalmology of Taipei Veterans General Hospital; (d-f) External validation Random split Yes Expert consensus Yes OCT AMD-AREDS 4 step Jammal et al. 2019 53 ResNet34 No 490 Images Randomly drawn from test sample No Reading centre grader Yes Retinal fundus photographs Glaucomatous optic neuropathy Kanagasingham et al. 2018 21 DCNN Yes 398 Patients Primary Care Practice, Midland, Western Australia NA Yes Reading centre grader No Retinal fundus photography Referable DR Karri et al. 2017 GoogLeNet No 21 Scans Duke University Random split No NR No OCT (a) DME; (b) dry AMD Keel et al. 2018 18 Inception-v3 Yes 93 Images St Vincent's Hospital Melbourne and University Hospital Geelong, Barwon Health NA Yes Reading centre grader No Retinal fundus photography Referable DR Keel et al. 2019 30 CNN No 86,202 Photographs Melbourne Collaborative Cohort Study Holdout method Yes Expert consensus No Retinal fundus photographs Neovascular AMD Kermany et al. 2018 54 Inception-v3 No (a) 1000; (b-d) 500 Scans Shiley Eye Institute of the University of California San Diego, the California Retinal Research Foundation, Medical Centre Ophthalmology Associates, the Shanghai First People's Hospital, and Beijing Tongren Eye Centre Random split No Consensus involving experts and non-experts Yes OCT (a) Choroidal neovascularisation vs DME vs drusen vs normal; (b) choroidal neovascularisation; (c) DME; (d) AMD Krause et al. 2018 31 CNN No 1958 Images EyePACS-2 Holdout method Yes Expert consensus No Retinal fundus photographs Referable DR Lee et al. 2017 VGG-16 No 2151 Scans Random split No Routine clinical notes No OCT AMD Lee et al. 2019 CNN No 200 Photographs Seoul National University Hospital Holdout method No Other imaging technique No Retinal fundus photographs Glaucoma Li et al. 2018 108 Inception-v3 No 8000 Scans Guangdong (China) Random split No Expert graders No Retinal fundus photography Glaucomatous optic neuropathy Li et al. 2019 55 VGG-16 No 1000 Images Shiley Eye Institute of the University of California San Diego, the California Retinal Research Foundation, Medical Centre Ophthalmology Associates, the Shanghai First People's Hospital, and Beijing Tongren Eye Centre Random split No Expert consensus No OCT Choroidal neovascularisation vs DME vs drusen Vs normal Li et al. 2019 OCT-NET No 859 Scans Wenzhou Medical University Random split No Expert graders No OCT Early DR Li et al. 2019 33 Inception-v3 No 800 Images Messidor-2 Random split Yes Reading centre grader No Retinal fundus photographs Referable DR Li et al. 2019 ResNet50 No 1635 Images Shanghai Zhongshan Hospital and the Shanghai First People's Hospital Random split No Reading centre grader Yes OCT DME Lin et al. 2019 109 CC-Cruiser Yes-multicentre RCT 350 Images Multicentre RCT NA NA Expert consensus Yes Slit-lamp photography Childhood cataracts Li F et al. 2018 VGG-15 No 300 Images NR Random split No NR No Visual Fields Glaucoma Table 2 continued Study Model Prospective?</s><s xml:id="_J26z5j2">Test set Population Test datasets Type of internal validation External validation Reference standard AI vs clinician?</s><s xml:id="_7Qt7uaP">Imaging modality Pathology Li Z et al. 2018 33 CNN No 35,201 Photographs NIEHS, SiMES, AusDiab Random split Yes Reading centre grader No Retinal fundus photographs Referable DR Liu et al. 2018 35 ResNet50 No (a) 754; (b) 30 Photographs (a) NR; (b) HRF Random split Yes Reading centre grader Yes Retinal fundus photographs Glaucomatous optic discs Liu et al. 2019 34 CNN No (a) 28,569; (b) 20,466; (c) 12,718; (d) 9305; (e) 29,676; (f ) 7877 validation No Expert graders No Retinal fundus photography (optos) Proliferative diabetic retinopathy Ohsugi et al. 2017 DCNN No 166 Images Tsukazaki Hospital Random split No Expert consensus No Retinal fundus photography (optos) Rhegmatogenous retinal detachment Peng et al. 2019 59 Inception-v3 No 900 Images AREDS Random split No Reading centre grader Yes Retinal fundus photography Age-related macular degeneration-AREDS 4 step Perdomo et al. 2019 OCT-NET No 2816 Images SERI-CUHK data set Random split No Expert graders No OCT DME Phan et al. 2019 DenseNet201 No 828 Images Yamanashi Koseiren Hospital No Expert consensus + further imaging No Retinal fundus photography Glaucoma Phene et al. 2019 37 Inception-v3 No (a) 1205; (b) 9642; (c) 346 Images (a) EyePACS, Inoveon, the United Kingdom Biobank, the Age-Related Eye Disease Study, and Sankara Nethralaya; (b) Atlanta Veterans Affairs (VA) Eye Clinic; (c) Dr. Shroff's Charity Eye Hospital, New Delhi, India Random split Yes Reading centre grader Yes Retinal fundus photographs Glaucomatous optic neuropathy Prahs et al. 2017 GoogLeNet No 5358 Images Heidelberg Eye Explorer, Heidelberg Engineering Random split No Expert graders No OCT Injection vs No injection for AMD Raju et al. 2017 CNN No 53,126 Images EyePACS-1 Random split No NR No Retinal fundus photography Referable DR Ramachandran et al. 2018 38 Visiona intelligent diabetic retinopathy screening platform No (a) 485; (b) 1200 Photographs (a) ODEMS; (b) Messidor NA Yes Expert graders No Retinal fundus photographs Referable DR Raumviboonsuk et al. 2019 39 Inception-v4 No (a-c) 25,348; (d) 24,332 Images National screening program for DR in Thailand NA Yes Expert consensus Yes Retinal fundus photography (a) Moderate nonproliferative DR or worse; (b) severe non-proliferative DR or worse; (c) proliferative DR; (d) referable DME</s></p><p xml:id="_VVcjmrt"><s xml:id="_XjPWXSZ">Table 2 continued Study Model Prospective?</s><s xml:id="_fa5zjaC">Test set Population Test datasets Type of internal validation External validation Reference standard AI vs clinician?</s><s xml:id="_XZAysPj">Imaging modality Pathology Redd et al. 2018 Inception-v1 and U-Net No 4861 Images Multicentre i-ROP study NR No Expert graders + further imaging No Retinal fundus photography Plus disease in ROP Rogers et al. 2019 45 Pegasus (ResNet50) No 94 Photographs EODAT NA Yes Reading centre grader Yes Retinal fundus photographs Glaucomatous optic neuropathy Sandhu et al. 2018 19 Deep fusion SNCAE Yes 160 Scans University of Waikato NA No Clinical diagnosis No Retinal fundus photographs Non-proliferative DR Sayres et al. 2019 40 Inception-v4 No 2000 Images EyePACS-2 NA Yes Expert consensus Yes Retinal fundus photographs Referable DR Shibata et al. 2018 60 (a) ResNet; (b) VGG-16 No 110 Images Matsue Red Cross Hospital Random split No Expert consensus Yes Retinal fundus photography Glaucoma Stevenson et al. 2019 Inception-v3 No (a) 2333; (b) 2283; (c) 2105 Photographs Publicly available databases Random split No Existing diagnosis from source data No Retinal fundus photographs (a) Glaucoma; (b) DR; (c) AMD Ting et al. 2017 41 VGGNet No (a) 71,896; (b) 15,798; (c) 3052; (d) 4512; (e) 1936; (f) 1052; (g) 1968; (h) 2302; (i) 1172; (j) 1254; (k) 7706; (l) 35,948; (m) 35,948 Images (a) Singapore National Diabetic Retinopathy Screening Program 2014-2015; (b) Guangdong (China); (c) Singapore Malay Eye Study; (d) Singapore Indian Eye Study; (e) Singapore Chinese Eye Study; (f) Beijing Eye Study; (g) African American Eye Disease Study; (h) Royal Victoria Eye and Ear Hospital; (i) Mexican; (j) Chinese University of Hong Kong, (k, l) Singapore National Diabetic Retinopathy Screening Program 2014-2015 Random split Yes Expert consensus No Retinal fundus photography Referable DR Ting et al. 2019 42 VGGNet No 85,902 Images Combined eight datasets NA Yes Consensus involving experts and non-experts No Retinal fundus photography (a) Any DR; (b) referable DR; (c) vision-threatening DR Treder et al. 2017 Inception-v3 No 100 Scans NR Holdout method No NR No OCT Exudative AMD van Grinsven et al. 2016 44 (a) Ses CNN 60; (b) NSesCNN170 No 1200 Images Messidor Random split Yes Existing diagnosis from source data Yes Retinal fundus photographs Retinal haemorrhage Verbraak et al. 2019 43 AlexNet/VGG No 1293 Images Netherlands Star-SHL NA Yes Expert consensus No Retinal fundus photography (a) DR-vision-threatening; (b) DR-more than mild Xu et al. 2017 CNN No 200 Photographs Kaggle Random split No Existing diagnosis from source data No Retinal fundus photographs DR Yang et al. 2019 VGGNet No 500 Photographs Intelligent Ophthalmology Database of Zhejiang Society for Mathematical Medicine in China Holdout method No Expert consensus No Retinal fundus photographs Referable DR Yoo et al. 2019 VGG-19 No 900 Scans Project Macula Random split No NR No (a) OCT; (b) retinal fundus photographs AMD Zhang et al. 2019 61 VGG-16 No 1742 Images Telemed-R screening Random split No Expert consensus Yes Retinal fundus photographs ROP Zheng et al. 2019 20 Inception-v3 Yes 102 Scans Joint Shantou International Eye Centre of Shantou University and the Chinese University of Hong Kong (JSIEC) Holdout method No NR No OCT Glaucomatous optic neuropathy  Table 3 continued Study Model Prospective?</s><s xml:id="_KSHqMBB">Test set Population Test datasets Type of internal validation External validation Reference standard AI vs clinician Imaging modality Body system/disease Liu H et al. 2019 Segmentation-based deep fusion network No 112,120 X-rays Chest X-ray14 NR No Routine clinical reports No X-ray (a) Atelectasis; (b) cardiomegaly; (c) effusion; (d) infiltration; (e) mass; (f) nodule; (g) pneumonia; (h) pneumothorax; (i) consolidation; (j) oedema; (k) emphysema; (l) fibrosis; (m) fibrosis; (n) pleural thickening; (o) hernia Majkowska et al. 2019 82 CNN No (a-d) 1818; (e-h) 1962 X-rays (a-d) Hospital group in India (Bangalore, Bhubaneshwar, Chennai, Hyderabad, New Delhi); (e-h) Chest X-ray14 Random split No Expert consensus Yes X-ray (a) Pneumothorax (b) nodule; (c) opacity; (d) fracture; (e) pneumothorax; (f ) nodule; (g) opacity; (h) fracture Monkam et al. 2018 CNN No 2600 Nodules LIDC-IDRI Random split No Expert readers No CT Nodules Nam et al. 2018 69 CNN No (a) 600; (b) 181; (c) 182; (d) 181; (e) 149 Chest radiographs (a) Internal validation; (b) Seoul National University Hospital; (c) Boromae Hospital; (d) National Cancer Centre, Korea; (e) University of California an Francisco Medical Centre Random split Yes (a) Routine clinical reports, histopathology; (b-e) histopathology, follow up, other imaging No X-ray Nodules Naqi et al. 2018 Two-level stacked autoencoder + softmax No 777 Nodules LIDC-IDRI NR No Expert readers No CT Nodules Nasrullah et al. 2019 Faster R-CNN No 2562 Nodules LIDC/IDRI NR No Expert readers No CT Nodules Nibali et al. 2017 ResNet No 166 Nodules LIDC-IDRI Random split No Expert readers No CT Nodules Nishio et al. 2018 VGG-16 No 123 Nodules Kyoto University Hospital Random split No NR No CT Nodules Onishi et al. 2019 AlexNet No 60 Nodules NR NR No Histopathology, follow up No CT Nodules Onishi et al. 2019 Wasserstein generative adversarial network No 60 Nodules Fujita Health University Hospital NR No Histopathology, follow up No CT Nodules Park et al. 2019 89 YOLO No 503 X-rays Asan Medical Centre and Seoul National University Bundang Hospital Hold-out method No Expert reader No X-ray Pneumothorax Park et al. 2019 83 CNN No 200 Images Asan Medical Centre and Seoul National University Bundang Hospital Hold-out method No Expert consensus Yes X-ray (a) Nodules; (b) opacity; (c) effusion; (d) pneumothorax; (e) abnormal chest X-ray Pasa et al. 2019 Custom CNN No 220 X-rays NIH Tuberculosis Chest Xray dataset and Belarus Tuberculosis Portal dataset Random split No NR No X-ray Tuberculosis Patel et al. 2019 84 CheXMax No 50 X-rays Stanford University Hold-out method No Expert reader, other imaging, clinical notes Yes X-ray Pneumonia Paul et al. 2018 VGG-s CNN No 237 Nodules National Lung Cancer Screening Trial Hold-out method No Expert readers, follow up No CT Nodules Pesce et al. 2019 Convolution networks with attention feedback (CONAF) No 7850 X-rays Guy's and St. Thomas' NHS Foundation Trust Random split No Routine clinical reports No X-ray Lung lesions Pezeshk et al. 2019 3D CNN No 128 Nodules LUNA16 Random split No Expert readers No CT Nodules Qin et al. 2019 70 (a) Lunit; (b) qXR (Qure.ai);</s><s xml:id="_3hh8ubD">(c) CAD4TB No 1196 X-rays Nepal and Cameroon NA Yes Expert readers Yes X-ray Tuberculosis Rajpurkar et al. 2018 85 CNN No 420 X-rays ChestXray-14 Random split No Routine clinical reports Yes X-ray (a) Atelectasis; (b) cardiomegaly; (c) consolidation; (d) oedema; (e) effusion; (f) emphysema; (g) fibrosis; (h) hernia; (i) infiltration; (j) mass; (k) nodule; (l) pleural thickening; (m) pneumonia; (n) pneumothorax Table 3 continued Study Model Prospective?</s><s xml:id="_jVGJDvt">Test set Population Test datasets Type of internal validation External validation Reference standard AI vs clinician Imaging modality Body system/disease Ren et al. 2019</s></p><p xml:id="_bUqnY2s"><s xml:id="_FW8NfcM">Manifold regularized classification deep neural network No 98 Nodules LIDC-IDRI Random split No Expert readers No CT Nodules Sahu et al. 2019 Multi-section CNN No 130 Nodules LIDC-IDRI Tenfold cross validation No Expert readers No CT Nodules Schwyzer et al. 2018 CNN No 100 Patients NR NR No NR No FDG-PET Lung cancer Setio et al. 2016 71 ConvNet No (a) 1186; (b) 50; (c) 898 (a) Nodules; (b) scans; (c) nodules LIDC-IDRI Fivefold cross validation Yes (a) Expert readers; (b, c) NR</s></p><p xml:id="_UGzm8Xr"><s xml:id="_39AtnSc">specificity was 0.94 (95% CI 0.90-0.97).</s><s xml:id="_6sCDqfw">The of the SROC curve was 0.94 (95% CI 0.92-0.96)-see</s><s xml:id="_kW2qRSz">Supplementary Fig. <ref type="figure" target="#fig_5">2</ref>. Pneumonia: Ten studies reported diagnostic accuracy for pneumonia on CXR with 15 different patient cohorts.</s><s xml:id="_NhMjh9m">AUC was 0.845 (95% CI 0.782-0.907),</s><s xml:id="_ktrMmn9">sensitivity was 0.951 (95% CI 0.936-0.965)</s><s xml:id="_JWDWhkq">and specificity was 0.716 (95% CI 0.480-0.953).</s></p><p xml:id="_X7aSBrS"><s xml:id="_p4zKqgX">Tuberculosis: Six studies reported diagnostic accuracy for tuberculosis on CXR with 17 different patient cohorts.</s><s xml:id="_QhYkwmN">AUC was 0.979 (95% CI 0.978-0.981),</s><s xml:id="_2JG3TCF">sensitivity was 0.998 (95% CI 0.997-0.999)</s><s xml:id="_WXEzwyT">and specificity was 1.000 (95% CI 0.999-1.000).</s><s xml:id="_pQmeBYs">Four patient cohorts from one study <ref type="bibr" target="#b89">90</ref> provided contingency tables with raw diagnostic accuracy.</s><s xml:id="_EGPNTFV">When averaging across the cohorts, the pooled sensitivity was 0.95 (95% CI 0.91-0.97)</s><s xml:id="_6rmTbra">and pooled specificity was 0.97 (95% CI 0.93-0.99).</s><s xml:id="_3uP54hK">The AUC of the SROC curve was 0.97 (95% CI 0.96-0.99)-see</s><s xml:id="_NWmw3Yr">Supplementary Fig. <ref type="figure">3</ref>.</s></p><p xml:id="_9kAEQvV"><s xml:id="_tfR6uS6">X-ray imaging was also used to identify atelectasis, pleural thickening, fibrosis, emphysema, consolidation, hiatus hernia, pulmonary oedema, infiltration, effusion, mass and cardiomegaly.</s><s xml:id="_sJkmuTY">CT imaging was also used to diagnose COPD, ground glass opacity and interstitial lung disease, but these were not included in the metaanalysis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fZJjyQe">Breast imaging</head><p xml:id="_evW7ejn"><s xml:id="_MBHnFKv">Eighty-two studies with 100 separate patient cohorts report on diagnostic accuracy of DL on breast disease (see Table <ref type="table" target="#tab_8">4</ref> and Supplementary References 3).</s><s xml:id="_zzapWQh">The four imaging modalities of mammography (MMG), digital breast tomosynthesis (DBT), ultrasound and magnetic resonance imaging (MRI) were used to diagnose breast cancer.</s></p><p xml:id="_WQaw6B9"><s xml:id="_GhfBuCK">No studies used prospectively collected data and eight <ref type="bibr" target="#b90">[91]</ref><ref type="bibr" target="#b91">[92]</ref><ref type="bibr" target="#b92">[93]</ref><ref type="bibr" target="#b93">[94]</ref><ref type="bibr" target="#b94">[95]</ref><ref type="bibr" target="#b95">[96]</ref><ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref> studies validated algorithms on external data.</s><s xml:id="_7Hg4FBa">No studies provided a prespecified sample size calculation.</s><s xml:id="_fgxU2DJ">Sixteen studies <ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b96">[97]</ref><ref type="bibr" target="#b97">[98]</ref><ref type="bibr" target="#b98">[99]</ref><ref type="bibr">[100]</ref><ref type="bibr">[101]</ref><ref type="bibr">[102]</ref><ref type="bibr">[103]</ref><ref type="bibr">[104]</ref><ref type="bibr">[105]</ref><ref type="bibr">[106]</ref><ref type="bibr">[107]</ref> compared algorithm performance against healthcare professionals.</s><s xml:id="_szVx7Ez">Reference standards varied greatly as did the method of internal validation used.</s><s xml:id="_QJsMv7z">There was high heterogeneity across all studies (see Table <ref type="table" target="#tab_8">4</ref>).</s></p><p xml:id="_uZ4cBY7"><s xml:id="_w6pHWEj">Breast cancer: Forty-eight studies with 59 separate patient cohorts reported diagnostic accuracy for identifying breast cancer on MMG (AUC 0.873 [95% CI 0.853-0.894]),</s><s xml:id="_vYyg7qb">22 studies and 25 patient cohorts on ultrasound (AUC 0.909 [95% CI 0.881-0.936]),</s><s xml:id="_NPZqqR6">and eight studies on MRI (AUC 0.868 [95% CI 0.850-0.886])</s><s xml:id="_W7sTwjB">and DBT (AUC 0.908 [95% CI 0.880-0.937]).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_keCjJWz">Other specialities</head><p xml:id="_sDgcd9h"><s xml:id="_4QBZnx7">Our literature search also identified 224 studies in other medical specialities reporting on diagnostic accuracy of DL algorithms to identify disease.</s><s xml:id="_PkNDEz8">These included large numbers of studies in the fields of neurology/neurosurgery (78), gastroenterology/hepatology (24) and urology (25).</s><s xml:id="_ewDPyes">Out of the 224 studies, only 55 compared algorithm performance against healthcare professionals, although 80% of studies in the field of dermatology did (see Supplementary References 4, Supplementary Table <ref type="table" target="#tab_0">1</ref> and Supplementary Fig. <ref type="figure">4</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_nuvR3m3">Variation of reporting</head><p xml:id="_Vw97a7A"><s xml:id="_9bYJet3">A key finding of our review was the large degree of variation in methodology, reference standards, terminology and reporting among studies in all specialities.</s><s xml:id="_CBrFhZz">The most common variables amongst DL studies in medical imaging include issues with the quality and size of datasets, metrics used to report performance and methods used for validation (see Table <ref type="table" target="#tab_9">5</ref>).</s><s xml:id="_cKVYu85">Only eight studies in ophthalmology imaging <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr">108,</ref><ref type="bibr">109</ref> , ten studies in respiratory imaging <ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr">110</ref> and six studies in breast imaging <ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b96">97,</ref><ref type="bibr">104,</ref><ref type="bibr">106,</ref><ref type="bibr">111</ref> mentioned adherence to the STARD-2015 guidelines or had a STARD flow diagram in the manuscript.</s></p><p xml:id="_d7FqREF"><s xml:id="_q6rCRHH">Funnel plots were produced for the diagnostic accuracy outcome measure with the largest number of patient cohorts in each medical speciality, in order to detect bias in the studies included 112 (see Supplementary Figs.</s><s xml:id="_QF2axye"><ref type="figure">5</ref><ref type="figure">6</ref><ref type="figure">7</ref>).</s><s xml:id="_H43neDW">These demonstrate that there is high risk of bias in studies detecting lung nodules on CT scans and detecting DR on RFP, but not for detecting breast cancer on MMG.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8Du8kKs">Assessment of the validity and applicability of the evidenc</head><p xml:id="_duNcpgZ"><s xml:id="_zGwxGQq">The overall risk of bias and applicability using Quality Assessment of Diagnostic Accuracies Studies 2 (QUADAS-2) led to a majority of studies in all specialities being classified as high risk, particularly with major deficiencies in regard to patient selection, flow and timing and applicability of the reference standard (see Fig. <ref type="figure" target="#fig_5">2</ref>).</s><s xml:id="_uVSsVYJ">For the patient selection domain, a high or unclear risk of bias was seen in 59/82 (72%) of ophthalmic studies, 89/115 (77%) of respiratory studies and 62/82 (76%) or breast studies.</s><s xml:id="_hKkVyHU">These were mostly related to a case-control study design and sampling issues.</s><s xml:id="_6NEKdh5">For the flow and timing domain, a high or unclear risk of bias was seen in 66/82 (80%) of ophthalmic studies, 93/115 (81%) of respiratory studies and 70/82 (85%) of breast studies.</s><s xml:id="_eA9VxeT">This was largely due to missing information about patients not receiving the index test or whether all patients received the same reference standard.</s><s xml:id="_J3D8Uwv">For the reference standard domain, concerns regarding applicability was seen in 60/82 (73%) of ophthalmic studies, 104/ 115 (90%) of respiratory studies and 78/82 (95%) of breast studies.</s><s xml:id="_qZPbdMq">This was mostly due to reference standard inconsistencies if the index test was validated on external datasets.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jUmV2RT">DISCUSSION</head><p xml:id="_TV7gUFj"><s xml:id="_av3aY8e">This study sought to (1) quantify the diagnostic accuracy of DL algorithms to identify specific pathology across distinct radiological modalities, and (2) appraise the variation in study reporting of DL-based radiological diagnosis.</s><s xml:id="_Y9EZZ3a">The findings of our specialityspecific meta-analysis suggest that DL algorithms generally have a high and clinically acceptable diagnostic accuracy in identifying disease.</s><s xml:id="_AxcFEJC">High diagnostic accuracy with analogous DL approaches was identified in all specialities despite different workflows, pathology and imaging modalities, suggesting that DL algorithms can be deployed across different areas in radiology.</s><s xml:id="_ng2yhf7">However, due to high heterogeneity and variance between studies, there is considerable uncertainty around estimates of diagnostic accuracy in this meta-analysis.</s></p><p xml:id="_DMB5zJ4"><s xml:id="_5vdeJxX">In ophthalmology, the findings suggest features of diseases, such as DR, AMD and glaucoma can be identified with a high sensitivity, specificity and AUC, using DL on both RFP and OCT scans.</s><s xml:id="_hwHnkpj">In general, we found higher sensitivity, specificity, accuracy and AUC with DL on OCT scans over RFP for DR, AMD and glaucoma.</s><s xml:id="_YnNdJnX">Only sensitivity was higher for DR on RFP over OCT.</s></p><p xml:id="_cybNacF"><s xml:id="_qrK99P4">In respiratory medicine, our findings suggest that DL has high sensitivity, specificity and AUC to identify chest pathology on CT scans and CXR.</s><s xml:id="_GbTarXw">DL on CT had higher sensitivity and AUC for detecting lung nodules; however, we found a higher specificity, PPV and F1 score on CXR.</s><s xml:id="_Ex4xhya">For diagnosing cancer or lung mass, DL on CT had a higher sensitivity than CXR.</s></p><p xml:id="_4nzYbp6"><s xml:id="_XjArywS">In breast cancer imaging, our findings suggest that DL generally has a high diagnostic accuracy to identify breast cancer on mammograms, ultrasound and DBT.</s><s xml:id="_JXYQPt9">The performance was found to be very similar for these modalities.</s><s xml:id="_WFUPFJC">In MRI, however, the diagnostic accuracy was lower; this may be due to small datasets and the use of 2D images.</s><s xml:id="_QqFjp3f">The utilisation of larger databases and multiparametric MRI may increase the diagnostic accuracy <ref type="bibr">113</ref> .</s></p><p xml:id="_tX4WaTS"><s xml:id="_YbCZemS">Extensive variation in the methodology, data interpretability, terminology and outcome measures could be explained by a lack of consensus in how to conduct and report DL studies.</s><s xml:id="_q4dMPus">The STARD-2015 checklist 114 , designed for reporting of diagnostic accuracy studies is not fully applicable to clinical DL studies .</s><s xml:id="_5JApb2M">The variation in reporting it very difficult to formally evaluate the performance of algorithms.</s><s xml:id="_XKMDGsG">Furthermore, differences in reference standards, grader capabilities, disease definitions and thresholds for diagnosis make direct comparison between studies and algorithms very difficult.</s><s xml:id="_A8Mqru8">This can only be improved with welldesigned and executed studies that explicitly address questions concerning transparency, reproducibility, ethics and effectiveness <ref type="bibr">116</ref> and specific reporting standards for AI studies <ref type="bibr">115,</ref><ref type="bibr">117</ref> .</s></p><p xml:id="_gJeEk2S"><s xml:id="_Rm2GyAW">The QUADAS-2 (ref. <ref type="bibr">118</ref></s><s xml:id="_zbymRDC">) assessment tool was used to systematically evaluate the risk of bias and any applicability concerns of the diagnostic accuracy studies.</s><s xml:id="_nGn6KX3">Although this tool was not designed for DL diagnostic accuracy studies, the evaluation allowed us to judge that a majority of studies in this field are at risk of bias or concerning for applicability.</s><s xml:id="_vBXyy2v">Of particular concern was the applicability of reference standards and patient selection.</s></p><p xml:id="_z8Uvreg"><s xml:id="_cw3VH5C">Despite our results demonstrating that DL algorithms have a high diagnostic accuracy in medical imaging, it is currently difficult to determine if they are clinically acceptable or applicable.</s><s xml:id="_BdHxfz7">This is partially due to the extensive variation and risk of bias identified in the literature to date.</s><s xml:id="_eNdsyqa">Furthermore, the definition of what threshold is acceptable for clinical use and tolerance for errors varies greatly across diseases and clinical scenarios 119 .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_xRxqBhb">Limitations in the literature</head><p xml:id="_ATJzMgZ"><s xml:id="_dbRzGFr">Dataset.</s><s xml:id="_nX3Q9Na">There are broad methodological deficiencies among the included studies.</s><s xml:id="_5kvBNxC">Most studies were performed using retrospectively collected data, using reference standards and labels that were not intended for the purposes of DL analysis.</s><s xml:id="_QDJMcaz">Minimal prospective and only two randomised studies 109,120 , evaluating the of DL algorithms in clinical settings were identified in the literature.</s><s xml:id="_EeGkUyp">Proper acquisition of test data is essential to interpret model performance in a real-world clinical setting.</s><s xml:id="_rmscNeE">Poor quality reference standards may result in the decreased model performance due to suboptimal data labelling in the validation set <ref type="bibr" target="#b27">28</ref> , which could be a barrier to understanding the true capabilities of the model on the test set.</s><s xml:id="_yhStSGd">This is symptomatic of the larger issue that there is a paucity of goldstandard, prospectively collected, representative datasets for the purposes of DL model testing.</s><s xml:id="_GJYmmkt">However, as there are many advantages to using retrospectively collected data, the resourceful use of retrospective or synthetic data with the use of labels of  DL algorithms against expert human clinicians for medical imaging.</s><s xml:id="_8j7rNnZ">This provide a more objective standard would enable better comparison of models across studies.</s><s xml:id="_RBxdrzF">Furthermore, application of the same test dataset for diagnostic performance assessment of DL algorithms versus healthcare professionals was identified in only select studies <ref type="bibr" target="#b12">13</ref> .</s><s xml:id="_YJDNjwp">This methodological deficiency limits the ability to gauge the clinical applicability of these algorithms into clinical practice.</s><s xml:id="_ZAsdACP">Similarly, this issue can extend to model-versus-model comparisons.</s><s xml:id="_g7yXAjS">Specific methods of model training or model architecture may not be described well enough to permit emulation for comparison <ref type="bibr">123</ref> .</s><s xml:id="_fYhRgs3">Thus, standards for model development and comparison against controls will be needed as DL architectures and techniques continue to develop and are applied in medical contexts.</s></p><p xml:id="_TfgJaHk"><s xml:id="_h2DQrxt">Reporting.</s><s xml:id="_7j3taN3">There was varying terminology and a lack of transparency used in DL studies with regards to the validation or test sets used.</s><s xml:id="_Z4HfQsE">The term 'validation' was identified as being used interchangeably to either describe an external test set for the final algorithm or for an internal dataset that is used to fine tune the model prior to 'testing'.</s><s xml:id="_dMpYnyW">Furthermore, the inconsistent terminology led to difficulties understanding whether an independent external test set was used to test diagnostic performance <ref type="bibr" target="#b12">13</ref> .</s><s xml:id="_b5jbu3C">Crucially, we found broad variation in the metrics used as outcomes for the performance of the DL algorithms in the literature.</s><s xml:id="_gdvykuY">Very few studies reported true positives, false positives, true negatives and false negatives in a contingency table as should be the minimum for diagnostic accuracy studies 114 .</s><s xml:id="_CTKqEBE">Moreover, some studies only reported metrics, such as dice coefficient, F1 score, competition performance metric and Top-1 accuracy that are often used in computer science, but may be unfamiliar to clinicians <ref type="bibr" target="#b12">13</ref> .</s><s xml:id="_hg5MsuA">Metrics such as AUC, sensitivity, specificity, PPV and NPV should be reported, as these are more widely understood by healthcare professionals.</s><s xml:id="_gctR7Fc">However, it is noted that NPV and PPV are dependent on the underlying prevalence of disease and as many test sets are artificially constructed or balanced, then reporting the NPV or PPV may not be valid.</s><s xml:id="_c52cqJN">The wide range of metrics reported also leads to difficulty in comparing the performance of algorithms on similar datasets.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rbdsQnH">Study strengths and limitations</head><p xml:id="_axE46rp"><s xml:id="_s8u7R9G">This systematic review and meta-analysis statistically appraises pooled data collected from 279 studies.</s><s xml:id="_RuDffZc">It is the largest study to date examining the diagnostic accuracy of DL on medical imaging.</s><s xml:id="_9Znwvcv">However, our findings must be viewed in consideration of several limitations.</s><s xml:id="_9VSYtUv">Firstly, as we believe that many studies have methodological deficiencies or are poorly reported, these studies may not be a reliable source for evaluating diagnostic accuracy.</s><s xml:id="_4pWMG2T">Consequently, the estimates of diagnostic performance provided in our meta-analysis are uncertain and may represent an overestimation of the true accuracy.</s><s xml:id="_sEnPa2V">Secondly, we did not conduct a quality assessment for the transparency of reporting in this review.</s><s xml:id="_6HUmWka">This was because current guidelines to assess diagnostic accuracy reporting standards (STARD-2015 114 ) were not designed for DL studies and are not fully applicable to the specifics and nuances of DL research 115 .</s><s xml:id="_cR9fK7b">Thirdly, due to the nature of DL studies, we were not able to perform classical statistical comparison of measures of diagnostic accuracy between different imaging modalities.</s><s xml:id="_77tJFcU">Fourthly, we were unable to separate each imaging modality into different subsets, to enable comparison across subsets and allow the heterogeneity and variance to be broken down.</s><s xml:id="_T6W8U6b">This was because our study aimed to provide an overview of the literature in each specific speciality, and it was beyond the scope of this review to examine each modality individually.</s><s xml:id="_UbX4SBA">The inherent differences in imaging technology, patient populations, pathologies and study designs meant that attempting to derive common lessons across the board did not always offer easy comparisons.</s></p><p xml:id="_emnzC3C"><s xml:id="_2Hgvw69">Finally, our review concentrated on DL for speciality-specific medical imaging, and therefore it may not be appropriate to generalise our findings to other forms of medical imaging or AI studies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FTpvBGJ">Future work</head><p xml:id="_3j4AsqN"><s xml:id="_MkQMxfZ">For the quality of DL research to flourish in the future, we believe that the adoption of the following recommendations are required as a starting point.</s></p><p xml:id="_YwBngDx"><s xml:id="_rBSjQhm">Availability of large, open-source, diverse anonymised datasets with annotations.</s><s xml:id="_ccsGa8P">This can be achieved through governmental support and will enable greater reproducibility of DL models 124 .</s></p><p xml:id="_Sskz38T"><s xml:id="_YFruNtv">Collaboration with academic centres to utilise their expertise in pragmatic trial design and methodology 125 .</s><s xml:id="_K2xHrHz">Rather than classical trials, novel experimental and quasi-experimental methods to evaluate DL have been proposed and should be evaluated <ref type="bibr">126</ref> .</s><s xml:id="_bQKubhC">This may include ongoing evaluation of algorithms once in clinical practice, as they continue to learn and adapt to the population that they are implemented in.</s></p><p xml:id="_6DQXu7t"><s xml:id="_V6dj6Tu">Creation of AI-specific reporting standards.</s><s xml:id="_gddbz2c">A major reason for the difficulties encountered in evaluating the performance of DL on medical imaging are largely due to inconsistent and haphazard reporting.</s><s xml:id="_B9mSDZb">Although DL is widely considered as a 'predictive' model (where TRIPOD may be applied) the majority of AI interventions close to translation currently published are predominantly in the field of diagnostics (with specifics on index tests, reference standards and true/false positive/negatives and summary diagnostic scores, centred directly in the domain of STARD).</s><s xml:id="_8nhkBQt">Existing reporting guidelines for diagnostic accuracy studies (STARD) 114 , prediction models (TRIPOD) 127 , randomised trials (CONSORT) 128 and interventional trial protocols (SPIRIT) <ref type="bibr">129</ref> do not fully cover DL research due to specific considerations in methodology, data and interpretation required for these studies.</s><s xml:id="_wc8EJs3">As such, we applaud the recent publication of the CONSORT-AI 117 and SPIRIT-AI 130 guidelines, and await AI-specific amendments of the TRIPOD-AI 131 and STARD-AI 115 statements (which we are convening).</s><s xml:id="_gP9jZaD">We trust that when these are published, studies being conducted will have a framework that enables higher quality and more consistent reporting.</s></p><p xml:id="_keCGGMV"><s xml:id="_PwBE9Dd">Development of specific tools for determining the risk of study bias and applicability.</s><s xml:id="_6euAnm4">An update to the QUADAS-2 tool taking into account the nuances of DL diagnostic accuracy research should be considered.</s></p><p xml:id="_QzXyYyK"><s xml:id="_mMwRXsT">Updated specific ethical and legal framework.</s><s xml:id="_9EjDjBB">Outdated policies need to be updated and key questions answered in terms of liability in cases of medical error, doctor and patient understanding, control over algorithms and protection of medical data 132 .</s><s xml:id="_EydzQdw">The World Health Organisation 133 and others have started to develop guidelines and principles to regulate the use of AI.</s><s xml:id="_SFu34E5">These regulations will need to be adapted by each country to fit their own political and healthcare context 134 .</s><s xml:id="_rdvG4QH">Furthermore, these guidelines will need to proactively and objectively evaluate technology to ensure best practices are developed and implemented in an evidence-based manner <ref type="bibr">135</ref> .</s><s xml:id="_gHjbZNj">CONCLUSION DL is a rapidly developing field that has great potential in all aspects of healthcare, particularly radiology.</s><s xml:id="_fcdPRUy">This systematic review and meta-analysis appraised the quality of the literature and provided pooled diagnostic accuracy for DL techniques in three medical specialities.</s><s xml:id="_ejramSf">While the results demonstrate that DL currently has a high diagnostic accuracy, it is important that these findings are assumed in presence poor design, conduct and reporting of studies, which can lead to bias and overestimating the power of these algorithms.</s><s xml:id="_XMgwTs6">The application of DL can only be improved with standardised guidance around study design and reporting, which could help clarify clinical utility in the future.</s><s xml:id="_pj8eYxX">There is an immediate need for the development of AI-specific STARD and TRIPOD statements to provide robust guidance around key issues in this field before the potential of DL in diagnostic healthcare is truly realised in clinical practice.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8rtETqS">METHODS</head><p xml:id="_WxfPGk7"><s xml:id="_AheqRqN">This systematic review was conducted in accordance with the guidelines for the 'Preferred Reporting Items for Systematic Reviews and Meta-Analyses' extension for diagnostic accuracy studies statement (PRISMA-DTA) <ref type="bibr">136</ref> .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CcEsmMm">Eligibility criteria</head><p xml:id="_tecBj2E"><s xml:id="_AhfWYMj">Studies that report upon the diagnostic accuracy of DL algorithms to investigate pathology or disease on medical imaging were sought.</s><s xml:id="_ZW9Xssz">The primary outcome was various diagnostic accuracy metrics.</s><s xml:id="_eKkdBkv">Secondary outcomes were study design and quality of reporting.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_yCamyah">Data sources and searches</head><p xml:id="_duK45Fj"><s xml:id="_xujP7sR">Electronic bibliographic searches were conducted in Medline and EMBASE up to 3rd January 2020.</s><s xml:id="_P7Dgx8A">MESH terms and all-field search terms were searched for 'neural networks' (DL or convolutional or cnn) and 'imaging' (magnetic resonance or computed tomography or OCT or ultrasound or X-ray) and 'diagnostic accuracy metrics' (sensitivity or specificity or AUC).</s><s xml:id="_8GSTsCV">For the full search strategy, please see Supplementary Methods 1.</s><s xml:id="_m3aSvVh">The search included all study designs.</s><s xml:id="_x6fSQh8">Further studies were identified through manual searches of bibliographies and citations until no further relevant studies were identified.</s><s xml:id="_xh8KMBH">Two investigators (R.A. and V.S.) independently screened titles and abstracts, and selected all relevant citations for full-text review.</s><s xml:id="_bAYeUkk">Disagreement regarding study inclusion was resolved by discussion with a third investigator (H.A.).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Ywdb7Vk">Inclusion criteria</head><p xml:id="_AqDYTRZ"><s xml:id="_NN5NuPr">Studies that comprised a diagnostic accuracy assessment of a DL algorithm on medical imaging in human populations were eligible.</s><s xml:id="_54sHXtj">Only studies that stated either diagnostic accuracy raw data, or sensitivity, specificity, AUC, NPV, PPV or accuracy data were included in the meta-analysis.</s><s xml:id="_3Ug7Bfj">No limitations were placed on the date range and the last search was performed in January 2020.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SFP26tt">Exclusion criteria</head><p xml:id="_nV5yySw"><s xml:id="_z69AkzK">Articles were excluded if the article was not written in English.</s><s xml:id="_KYSYRGR">Abstracts, conference articles, pre-prints, reviews and metaanalyses were not considered because an aim of this review was to appraise the methodology, reporting standards and quality of primary research studies being published in peer-reviewed journals.</s><s xml:id="_tvc5CFy">Studies that investigated the accuracy of image segmentation or predicting disease rather than identification or classification were excluded.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cJ9jJmx">Data extraction and quality assessment</head><p xml:id="_HE7vpqW"><s xml:id="_Xwr7A4G">Two investigators (R.A. and V.S.) independently extracted demographic and diagnostic accuracy data from the studies, using a predefined electronic data extraction spreadsheet.</s><s xml:id="_6dpkZnq">The data fields were chosen subsequent to an initial scoping review and were, in the opinion of the investigators, sufficient to fulfil the aims of this review.</s><s xml:id="_BP7Jq4p">Data were extracted on (i) first author, (ii) year of publication, (iii) type of neural network, (iv) population, (v) dataset-split into training, validation and test sets, (vi) imaging modality, (vii) body system/disease, (viii) internal/external validation methods, (ix) reference standard, (x) diagnostic accuracy raw data-true and false positives and negatives, (xi) percentages of AUC, accuracy, sensitivity, specificity, PPV, NPV and other metrics reported.</s></p><p xml:id="_AYmAmHX"><s xml:id="_zF7SyEt">Three investigators (R.A., V.S. and GM) assessed study methodology using the QUADAS-2 checklist to evaluate the risk of bias and any applicability concerns of the studies 118 .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TdzvhE8">Data synthesis and analysis</head><p xml:id="_pXUrT8b"><s xml:id="_WN3VzfH">A bivariate model for diagnostic meta-analysis was used to calculate summary estimates of sensitivity, specificity and AUC data <ref type="bibr">137</ref> .</s><s xml:id="_ZdAKmDx">Independent proportion and their differences were calculated and pooled through DerSimonian and Laird randomeffects modelling <ref type="bibr">138</ref> .</s><s xml:id="_tQCcp2C">This considered both between-study and within-study variances that contributed to study weighting.</s><s xml:id="_GQwkd2W">Study-specific estimates and 95% CIs were computed and represented on forest plots.</s><s xml:id="_FZxJakj">Heterogeneity between studies was assessed using I 2 (25-49% was considered to be low heterogeneity, 50-74% was moderate and &gt;75% was high heterogeneity).</s><s xml:id="_Z6asNmk">Where raw diagnostic accuracy data were available, the SROC model was used to evaluate the relationship between sensitivity and specificity 139 .</s><s xml:id="_DSmQ6BX">We utilised Stata version 15 (Stata Corp LP, College Station, TX, USA) for all statistical analyses.</s></p><p xml:id="_QtWbqEt"><s xml:id="_HpdgTBY">We chose to appraise the performance of DL algorithms to identify individual disease or pathology patterns on different imaging modalities in isolation, e.g., identifying lung nodules on a thoracic CT scan.</s><s xml:id="_EFhDNQE">We felt that combining imaging modalities and diagnoses would add heterogeneity and variation to the analysis.</s><s xml:id="_Ymze3eN">Meta-analysis was only performed where there were greater than or equal to three patient cohorts, reporting for each specific pathology and imaging modality.</s><s xml:id="_HTTSwwg">This study is registered with PROSPERO, CRD42020167503.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc><div><p xml:id="_QegPbja"><s xml:id="_nq3apcy">Fig. 1 PRISMA flow diagram of included studies.</s><s xml:id="_zCXCTdz">PRISMA (preferred reporting items for systematic reviews and meta-analyses) flow diagram of included studies.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><figDesc><div><p xml:id="_QmchvSF"><s xml:id="_WMM7GsA">et al. 2020 (a) ChestNet; (b) VGG-16;(c</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><figDesc><div><p xml:id="_kgUREbx"><s xml:id="_y2CvXKz">varying modality and quality represent important areas of research in DL 121 .</s><s xml:id="_xYx53E8">Study methodology.</s><s xml:id="_3QhH7yq">Many studies did not undertake external validation of the algorithm in a separate test set and relied upon results from the internal validation data; the same dataset used to train the algorithm initially.</s><s xml:id="_SSEbnp3">This may lead to an overestimation of the diagnostic accuracy of the algorithm.</s><s xml:id="_k6trr9r">The problem of overfitting has been well described in relation to machine learning algorithms 122 .</s><s xml:id="_afARYWg">True demonstration of the performance of these algorithms can only be assumed if they are externally validated on separate test sets with previously unseen data that are representative of the target population.</s><s xml:id="_rX6YVQc">Surprisingly, few studies compared the diagnostic accuracy of a) Ophthalmic Imaging b) Respiratory Imaging c) Breast Imaging</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2</head><label>2</label><figDesc><div><p xml:id="_JYJQ6Jg"><s xml:id="_STmvKCr">Fig. 2 QUADAS-2 summary plots.</s><s xml:id="_vrbJdpr">Risk of bias and applicability concerns summary about each QUADAS-2 domain presented as percentages across the 82 included studies in ophthalmic imaging (a), 115 in respiratory imaging (b) and 82 in breast imaging (c).</s></p></div></figDesc><graphic coords="18,116.02,76.73,387.64,477.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc><div><p xml:id="_y99h6FT"><s xml:id="_25fwSa2">Summary estimates of pooled speciality and imaging modality specific diagnostic accuracy metrics.</s></p></div></figDesc><table><row><cell>Imaging modality Diagnosis AUC 95% CI I 2 Sensitivity 95% CI I 2 Specificity 95% CI I 2 PPV 95% CI I 2 NPV 95% CI I 2 Accuracy 95% CI I 2 F1 I 2 score 95% CI</cell><cell>Ophthalmology imaging</cell><cell>RFP DR 0.939 0.920-0.958 99.9 0.976 0.975-0.977 99.9 0.902 0.889-0.916 99.7 0.389 0.166-0.612 99.7 1 1 90.6 0.927 0.899-0.955 96.3</cell><cell>RFP AMD 0.963 0.948-0.979 99.3 0.973 0.971-0.974 99.9 0.924 0.896-0.952 99.6 0.797 0.719-0.875 99.9</cell><cell>RFP Glaucoma 0.933 0.924-0.942 99.6 0.883 0.862-0.904 99.9 0.918 0.898-0.938 99.7 0.881 0.847-0.915 97.7</cell><cell>RFP ROP 0.96 0.913-1.008 99.5 0.907 0.749-1.066 99.8</cell><cell>OCT DR 1 0.999-1.0 98.1 0.954 0.937-0.972 98.9 0.993 0.991-0.994 98.2 0.97 0.959-0.981 97.5</cell><cell>OCT AMD 0.969 0.955-0.983 99.4 0.997 0.996-0.997 99.7 0.932 0.914-0.950 98.9 0.936 0.906-0.965 99.6</cell><cell>OCT Glaucoma 0.964 0.941-0.986 77.7</cell><cell>Respiratory imaging</cell><cell>CT Lung nodules 0.937 0.924-0.949 97 0.86 0.831-0.890 99.7 0.896 0.871-0.921 99.2 0.785 0.711-0.858 99.2 0.889 0.747-0.834 97.9 0.870-0.908 98.4 0.79</cell><cell>CT Lung cancer 0.887 0.847-0.928 95.9 0.837 0.780-0.894 94.6 0.826 0.735-0.918 98.1 0.827 0.784-0.870 81.7</cell><cell>X-ray Nodules 0.884 0.842-0.925 99.6 0.75 0.634-0.866 99 0.944 0.912-0.976 98.4 0.86 0.736-0.984 99.8 0.842-0.945 81.4 0.894</cell><cell>X-ray Mass 0.864 0.827-0.901 99.7 0.801 0.683-0.919 99.7</cell><cell>X-ray Abnormal 0.917 0.869-0.966 99.9 0.873 0.762-0.985 99.9 0.894 0.860-0.929 98.7 0.85 0.567-1.133 100 0.859 0.558-0.962 99.7 0.736-0.983 99 0.76</cell><cell>X-ray Atelectasis 0.824 0.783-0.866 99.7</cell><cell>X-ray Cardiomegaly 0.905 0.871-0.938 99.7</cell><cell>X-ray Consolidation 0.875 0.800-0.949 99.9 0.914 0.816-1.013 99.5 0.751 0.637-0.866 98.6 0.897 0.828-0.966 96.4</cell><cell>X-ray Pulmonary oedema 0.893 0.843-0.944 99.9</cell><cell>X-ray Effusion 0.906 0.862-0.950 99.8</cell><cell>X-ray Emphysema 0.885 0.855-0.916 99.7</cell><cell>X-ray Fibrosis 0.834 0.796-0.872 99.7</cell><cell>X-ray Hiatus hernia 0.894 0.858-0.930 99.8</cell><cell>X-ray Infiltration 0.724 0.682-0.767 99.6</cell><cell>X-ray Pleural thickening 0.816 0.762-0.870 99.8</cell><cell>X-ray Pneumonia 0.845 0.782-0.907 99.9 0.951 0.936-0.965 96.3 0.716 0.480-0.953 100 0.681 0.367-0.995 100 0.763 0.838-0.941 97.6 0.559-0.968 100 0.889</cell><cell>X-ray Pneumothorax 0.91 0.863-0.957 99.9 0.718 0.433-1.004 100 0.918 0.870-0.965 99.9 0.496 0.369-0.623 100</cell><cell>X-ray Tuberculosis 0.979 0.978-0.981 99.6 0.998 0.997-0.999 99.6 1 0.999-1.000 95.3 0.94 0.921-0.959 84.6</cell><cell>Breast imaging</cell><cell>MMG Breast cancer 0.873 0.853-0.894 98.8 0.851 0.779-0.923 99.9 0.882 0.859-0.905 97.2 0.905 0.880-0.930 97.9</cell><cell>Ultrasound Breast cancer 0.909 0.881-0.936 91.7 0.853 0.815-0.891 93.9 0.901 0.870-0.931 96.6 0.804 0.727-0.880 93.7 0.922 0.851-0.992 97.2 0.873 0.803-0.906 87.9 0.841-0.906 87.5 0.855</cell><cell>MRI Breast cancer 0.868 0.850-0.886 27.8 0.786 0.710-0.861 80.5 0.788 0.697-0.880 86.2</cell><cell>DBT Breast cancer 0.908 0.880-0.937 63.2 0.831 0.675-0.988 97.6 0.918 0.905-0.930 0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc><div><p xml:id="_eCrm6ak"><s xml:id="_6SVKp55">Characteristics of ophthalmic imaging studies.</s></p></div></figDesc><table><row><cell>Type of internal</cell><cell>validation</cell></row><row><cell>Test datasets</cell><cell></cell></row><row><cell>Population</cell><cell></cell></row><row><cell>Test set</cell><cell></cell></row><row><cell>Prospective?</cell><cell></cell></row><row><cell>Model</cell><cell></cell></row><row><cell>Study</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc><div><p xml:id="_eK7VgzK"><s xml:id="_xGgYsTM">Characteristics of respiratory imaging studies.</s></p></div></figDesc><table><row><cell>Body system/disease</cell><cell></cell><cell>Abnormal X-ray</cell><cell>Nodules</cell><cell>Lung cancer</cell><cell></cell><cell>Nodules</cell><cell>(a) Critical radiographs;</cell><cell>(b) normal radiographs</cell><cell>Lung cancer</cell><cell></cell><cell></cell><cell>(a) Abnormal chest X-ray;</cell><cell>(b) normal chest X-ray; (c)</cell><cell>atelectasis; (d)</cell><cell>cardiomegaly; (e)</cell><cell>effusion; (f) infiltration; (g)</cell><cell>mass; (h) nodule (i)</cell><cell>pneumonia; (j)</cell><cell>pneumothorax; (k)</cell><cell>consolidation; (l) oedema;</cell><cell>(m) emphysema; (n)</cell><cell>fibrosis; (o) pleural</cell><cell>thickening; (p) hernia</cell><cell>(a) Abnormal X-ray; (b)</cell><cell>cardiomegaly</cell></row><row><cell>Imaging</cell><cell>modality</cell><cell>X-ray</cell><cell>CT</cell><cell>CT</cell><cell></cell><cell>CT</cell><cell>X-ray</cell><cell></cell><cell>CT</cell><cell></cell><cell></cell><cell>X-ray</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X-ray</cell></row><row><cell>AI vs</cell><cell>clinician</cell><cell>No</cell><cell>No</cell><cell>No</cell><cell></cell><cell>No</cell><cell>No</cell><cell></cell><cell>Yes</cell><cell></cell><cell></cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>No</cell></row><row><cell>Reference standard</cell><cell></cell><cell>Routine clinical reports</cell><cell>Expert readers</cell><cell>Expert reader, existing</cell><cell>labels in dataset</cell><cell>Expert readers</cell><cell>Routine clinical reports</cell><cell></cell><cell>Histopathology,</cell><cell>follow up</cell><cell></cell><cell>Routine clinical reports</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Expert readers</cell></row><row><cell>Type of internal External</cell><cell>validation validation</cell><cell>Random split No</cell><cell>NR No</cell><cell>Random split No</cell><cell></cell><cell>Random split No</cell><cell>Hold-out method No</cell><cell></cell><cell>Random split Yes</cell><cell></cell><cell></cell><cell>Random split No</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Random split No</cell></row><row><cell>Test datasets</cell><cell></cell><cell>Chest X-ray14</cell><cell>LIDC-IDRI</cell><cell>Kaggle Data Science Bowl</cell><cell></cell><cell>LIDC-IDRI</cell><cell>Kings College London</cell><cell></cell><cell>(a) National Lung Cancer</cell><cell>Screening Trial; (b)</cell><cell>Northwestern Medicine</cell><cell>Chest X-ray14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Diagnostic Imaging</cell><cell>Department of Sheba</cell><cell>Medical Centre, Tel</cell><cell>Hashomer, Israel</cell></row><row><cell>Population</cell><cell></cell><cell>Images</cell><cell>Nodules</cell><cell>Scans</cell><cell></cell><cell>Nodules</cell><cell>Images</cell><cell></cell><cell>Scans</cell><cell></cell><cell></cell><cell>X-rays</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Images</cell></row><row><cell>Test set</cell><cell></cell><cell>380</cell><cell>848</cell><cell>419</cell><cell></cell><cell>668</cell><cell>15,887</cell><cell></cell><cell>(a) 6716;</cell><cell>(b) 1139</cell><cell></cell><cell>22,424</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>194</cell></row><row><cell>Prospective?</cell><cell></cell><cell>No</cell><cell>No</cell><cell>No</cell><cell></cell><cell>No</cell><cell>No</cell><cell></cell><cell>No</cell><cell></cell><cell></cell><cell>No</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>No</cell></row><row><cell>Study Model</cell><cell></cell><cell>Abiyev et al. 2018 CNN</cell><cell>Al-Shabi et al. 2019 Local-Global</cell><cell>Alakwaa et al. 2017 U-Net</cell><cell></cell><cell>Ali et al. 2018 3D CNN</cell><cell cols="2">Annarumma et al. 2019 110 CNN</cell><cell>Ardila et al. 2019 64 Inception-v1</cell><cell></cell><cell></cell><cell>Baltruschat et al. 2019 ResNet50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN Bar et al. 2018</cell><cell>Becker et al. 2018 62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc><div><p xml:id="_EBfBvrB"><s xml:id="_s4J4PrZ">Characteristics of breast imaging studies.</s></p></div></figDesc><table><row><cell>Body system/disease</cell><cell></cell><cell></cell></row><row><cell>Imaging modality</cell><cell></cell><cell></cell></row><row><cell>AI vs</cell><cell>clinician?</cell><cell></cell></row><row><cell>Reference standard</cell><cell></cell><cell></cell></row><row><cell>External</cell><cell>validation</cell><cell></cell></row><row><cell>Type of internal</cell><cell>validation</cell><cell></cell></row><row><cell>Test datasets</cell><cell></cell><cell></cell></row><row><cell>Test Set</cell><cell></cell><cell></cell></row><row><cell>Prospective?</cell><cell>No</cell><cell></cell></row><row><cell>Model</cell><cell>CNN</cell><cell></cell></row><row><cell>Study</cell><cell>Abdelsamea</cell><cell>et al. 2019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc><div><p><s xml:id="_Wf2PZh9">Variation in DL imaging studies.DataImage pre-processing, augmentation and preparation Are data augmentation techniques such as cropping, padding and flipping used?Is there quality control of the images being used to train the algorithm?I.e., were poor quality images excluded.Were relevant images manually selected?</s></p></div></figDesc><table><row><cell>Study design</cell><cell>Retrospective or prospective data collection.</cell></row><row><cell>Image eligibility</cell><cell>How are images chosen for inclusion in the study?</cell></row><row><cell></cell><cell>Were the data from private or open-access repositories?</cell></row><row><cell>Training, validation, test sets</cell><cell>Are each of the three sets independent of each other, without overlap?</cell></row><row><cell></cell><cell>Does data from the same patient appear in multiple datasets?</cell></row><row><cell>Datasets</cell><cell>Are the datasets used single or multicentre?</cell></row><row><cell></cell><cell>Is a public or open-source dataset used?</cell></row><row><cell>Size of datasets</cell><cell>Wide variation in size of datasets for training and testing.</cell></row><row><cell></cell><cell>Is the size of the datasets justified?</cell></row><row><cell></cell><cell>Are sample size statistical considerations applied for the test set?</cell></row><row><cell>Use of 'external' test sets for final reporting</cell><cell>Is an independent test set used for 'external validation'?</cell></row><row><cell></cell><cell>Is the independent test set constructed using an unenriched representative sample?</cell></row><row><cell>Multi-vendor images</cell><cell>Are images from different scanners and vendors included in the datasets to enhance</cell></row><row><cell></cell><cell>generalisability?</cell></row><row><cell></cell><cell>Are imaging acquisition parameters described?</cell></row><row><cell>Algorithm</cell><cell></cell></row><row><cell>Index test</cell><cell>Was sufficient detail given on the algorithm to allow replication and independent</cell></row><row><cell></cell><cell>validation?</cell></row><row><cell></cell><cell>What type of algorithm was used? E.g., CNN, Autoencoder, SVM.</cell></row><row><cell></cell><cell>Was the algorithm made publicly or commercially available?</cell></row><row><cell></cell><cell>Was the construct or architecture of the algorithm made available?</cell></row><row><cell>Additional AI algorithmic information</cell><cell>Is the algorithm a static model or is it continuously evolving?</cell></row><row><cell>Demonstrate how algorithm makes decisions</cell><cell>Is there a specific design for end-user interpretability, e.g., saliency or probability maps</cell></row><row><cell>Methods</cell><cell></cell></row><row><cell>Transfer learning</cell><cell>Was transfer learning used for training and validation?</cell></row><row><cell>Cross validation</cell><cell>Was k-fold cross validation used during training to reduce the effects of randomness in</cell></row><row><cell></cell><cell>dataset splits?</cell></row><row><cell>Reference standard</cell><cell>Is the reference standard used of high quality and widely accepted in the field?</cell></row><row><cell></cell><cell>What was the rationale for choosing the reference standard?</cell></row><row><cell>Additional clinical information</cell><cell>Was additional clinical information given to healthcare professionals to simulate normal</cell></row><row><cell></cell><cell>clinical process?</cell></row><row><cell>Performance benchmarking</cell><cell>What was performance of algorithm benchmarked to?</cell></row><row><cell></cell><cell>What is expertise level and level of consensus of healthcare professionals if used?</cell></row><row><cell>Results</cell><cell></cell></row><row><cell>Raw diagnostic accuracy data</cell><cell>Are raw diagnostic accuracy data reported in a contingency table demonstrating TP, FP,</cell></row><row><cell></cell><cell>FN, TN?</cell></row><row><cell>Metrics for estimating diagnostic accuracy</cell><cell>Which diagnostic accuracy metrics reported? Sensitivity, Sensitivity, PPV, NPV, Accuracy,</cell></row><row><cell>performance</cell><cell>AUROC</cell></row><row><cell>Unit of assessment</cell><cell>Which unit of assessment reported, e.g., per patient, per scan or per lesion.</cell></row><row><cell>Rows in bold are part of STARD-2015 criteria.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_mXSdCXZ"><s xml:id="_hJDZJqT">npj Digital Medicine (2021) 65 Published in partnership with Seoul National University Bundang Hospital 1234567890():,;</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_S9v7Ax2"><s xml:id="_cCsSGjA">Published in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2021) 65</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_PBxdvqm"><s xml:id="_ThEnVRt">npj Digital Medicine (2021)<ref type="bibr" target="#b64">65</ref> Published in partnership with Seoul National University Bundang Hospital</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_xNaDMNX">ACKNOWLEDGEMENTS</head><p xml:id="_yaK5V7b"><s xml:id="_GwB9jzS"><rs type="institution" subtype="infrastructure">Infrastructure</rs> support for this research was provided by the <rs type="funder">NIHR Imperial Biomedical Research Centre (BRC)</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">Infrastructure</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_8fGMjjH">DATA AVAILABILITY</head><p xml:id="_Ndswdq5"><s xml:id="_z3K3btx">The authors declare that all the data included in this study are available within the paper and its Supplementary Information files.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2RPg2FW">ADDITIONAL INFORMATION Supplementary information</head><p xml:id="_4eQgEfh"><s xml:id="_dVRa4DT">The online version contains supplementary material available at <ref type="url" target="https://doi.org/10.1038/s41746-021-00438-z">https://doi.org/10.1038/s41746-021-00438-z</ref>.</s></p><p xml:id="_3zCwrZB"><s xml:id="_fPDf5RX">Correspondence and requests for materials should be addressed to H.A.</s></p><p xml:id="_bCtyMYn"><s xml:id="_6CAEEkd">Reprints and permission information is available at <ref type="url" target="http://www.nature.com/reprints">http://www.nature.com/  reprints</ref></s></p><p xml:id="_Rxdns5u"><s xml:id="_TyE4Hbw">Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_Nv74BKX">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14539</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7WQw8uC">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">LeCun, Y., Bengio, Y. &amp; Hinton, G. Deep learning. Nature 521, 436-444 (2015).</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_7fH3z6x">Predicting the futurebig data, machine learning, and clinical medicine</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Emanuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ndZn9y7">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<biblScope unit="page" from="1216" to="1219" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Obermeyer, Z. &amp; Emanuel, E. J. Predicting the future -big data, machine learning, and clinical medicine. N. Engl. J. Med. 375, 1216-1219 (2016).</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_ydGgdNr">A guide to deep learning in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-018-0316-z</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6QKwGRV">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Esteva, A. et al. A guide to deep learning in healthcare. Nat. Med. 25, 24-29 (2019).</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_YbUME3D">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2017.07.005</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RHaseyJ">Med. Image Anal</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Litjens, G. et al. A survey on deep learning in medical image analysis. Med. Image Anal. 42, 60-88 (2017).</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_czWHCVt">Assessing radiology research on artificial intelligence: a brief guide for authors, reviewers, and readers-from the radiology editorial board</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bluemke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PHWj5xQ">Radiology</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="page" from="487" to="489" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bluemke, D. A. et al. Assessing radiology research on artificial intelligence: a brief guide for authors, reviewers, and readers-from the radiology editorial board. Radiology 294, 487-489 (2020).</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_TfjmGEj">Artificial intelligence (AI) and global health: how can AI contribute to health in resource-poor settings?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cossy-Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Schwalbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kGX6qNa">BMJ Glob. Health</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="798" to="e798" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wahl, B., Cossy-Gantner, A., Germann, S. &amp; Schwalbe, N. R. Artificial intelligence (AI) and global health: how can AI contribute to health in resource-poor set- tings? BMJ Glob. Health 3, e000798-e000798 (2018).</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_YbNCWGR">Big data and medical research in China</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-M</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fNKRdaT">BMJ</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page">5910</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, L., Wang, H., Li, Q., Zhao, M.-H. &amp; Zhan, Q.-M. Big data and medical research in China. BMJ 360, j5910 (2018).</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_5dkKA4r">Radiologist supply and workload: international</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11604-008-0259-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qUTXDwp">Radiat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="455" to="465" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nakajima, Y., Yamada, K., Imamura, K. &amp; Kobayashi, K. Radiologist supply and workload: international Radiat. Med. 26, 455-465 (2008).</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_RqWRgWd">Key challenges for delivering clinical impact with artificial intelligence</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vSF4GnY">BMC Med</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">195</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kelly, C. J., Karthikesalingam, A., Suleyman, M., Corrado, G. &amp; King, D. Key challenges for delivering clinical impact with artificial intelligence. BMC Med. 17, 195 (2019).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_jjAqX4S">High-performance medicine: the convergence of human and artificial intelligence</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_e4wUaTj">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="44" to="56" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. Nat. Med. 25, 44-56 (2019).</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_Wqxa7tc">The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benjamens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhunnoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Meskó</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-020-00324-0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3uxeuT5">npj Digital Med</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Benjamens, S., Dhunnoo, P. &amp; Meskó, B. The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database. npj Digital Med. 3, 118 (2020).</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_5sUHX6U">Big data and machine learning in health care</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Beam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2017.18391</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WBs6u39">JAMA</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page" from="1317" to="1318" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beam, A. L. &amp; Kohane, I. S. Big data and machine learning in health care. JAMA 319, 1317-1318 (2018).</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_HcNqBqg">A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/s2589-7500(19)30123-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PJrRA6j">Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="271" to="e297" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, X. et al. A comparison of deep learning performance against health-care professionals in detecting diseases from medical imaging: a systematic review and meta-analysis. Lancet Digital Health 1, e271-e297 (2019).</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_abwVWRn">Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Folk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hYKntN2">npj Digital Med</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abràmoff, M. D., Lavin, P. T., Birch, M., Shah, N. &amp; Folk, J. C. Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices. npj Digital Med. 1, 39 (2018).</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_uuaSp9u">Artificial intelligence using deep learning to screen for referable and vision-threatening diabetic retinopathy in Africa: a clinical validation study</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bellemo</surname></persName>
		</author>
		<idno type="DOI">10.1016/s2589-7500(19)30004-4</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hWDAZAQ">Lancet Digital Health</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="35" to="e44" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bellemo, V. et al. Artificial intelligence using deep learning to screen for refer- able and vision-threatening diabetic retinopathy in Africa: a clinical validation study. Lancet Digital Health 1, e35-e44 (2019).</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_9E898Mp">Performance of deep learning architectures and transfer learning for detecting glaucomatous optic neuropathy in fundus photographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xmBTVee">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">16685</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Christopher, M. et al. Performance of deep learning architectures and transfer learning for detecting glaucomatous optic neuropathy in fundus photographs. Sci. Rep. 8, 16685 (2018).</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_NYNrtgB">Performance of a deep-learning algorithm vs manual grading for detecting diabetic retinopathy in India</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2019.2004</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7e2tzU5">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="987" to="993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gulshan, V. et al. Performance of a deep-learning algorithm vs manual grading for detecting diabetic retinopathy in India. JAMA Ophthalmol 137, 987-993 (2019).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_hzfxqtK">Visualizing deep learning models for the detection of referable diabetic retinopathy and glaucoma</title>
		<author>
			<persName><forename type="first">S</forename><surname>Keel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scheetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2018.6035</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BeVWhPb">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="288" to="292" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Keel, S., Wu, J., Lee, P. Y., Scheetz, J. &amp; He, M. Visualizing deep learning models for the detection of referable diabetic retinopathy and glaucoma. JAMA Oph- thalmol. 137, 288-292 (2019).</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_DSRnM7k">Automated diagnosis and grading of diabetic retinopathy using optical coherence tomography</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Sandhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TJzh9TZ">Investig. Ophthalmol. Vis. Sci</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="3155" to="3160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sandhu, H. S. et al. Automated diagnosis and grading of diabetic retinopathy using optical coherence tomography. Investig. Ophthalmol. Vis. Sci. 59, 3155-3160 (2018).</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_Pxy9UpB">Detecting glaucoma based on spectral domain optical coherence tomography imaging of peripapillary retinal nerve fiber layer: a comparison study between hand-crafted features and deep learning model</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Da3fNEF">Graefes Arch. Clin. Exp. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">258</biblScope>
			<biblScope unit="page" from="577" to="585" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zheng, C. et al. Detecting glaucoma based on spectral domain optical coher- ence tomography imaging of peripapillary retinal nerve fiber layer: a compar- ison study between hand-crafted features and deep learning model. Graefes Arch. Clin. Exp. Ophthalmol. 258, 577-585 (2020).</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_PX2MCJh">Evaluation of artificial intelligence-based grading of diabetic retinopathy in primary care</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kanagasingam</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamanetworkopen.2018.2665</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vdBAvfY">JAMA Netw. Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="182665" to="e182665" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kanagasingam, Y. et al. Evaluation of artificial intelligence-based grading of diabetic retinopathy in primary care. JAMA Netw. Open 1, e182665-e182665 (2018).</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_PKWHndj">NET: a convolutional network automated classification of multiclass retinal diseases using spectral-domain optical coherence tomography images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alqudah</surname></persName>
		</author>
		<author>
			<persName><surname>Aoct</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11517-019-02066-y</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KFqrTvC">Med. Biol. Eng. Comput</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="41" to="53" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alqudah, A. M. AOCT-NET: a convolutional network automated classification of multiclass retinal diseases using spectral-domain optical coherence tomography images. Med. Biol. Eng. Comput. 58, 41-53 (2020).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_sqdH3mP">Validation of a deep learning model to screen for glaucoma using images from different fundus cameras and data augmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Asaoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5xhm8kb">Ophthalmol. Glaucoma</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="231" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Asaoka, R. et al. Validation of a deep learning model to screen for glaucoma using images from different fundus cameras and data augmentation. Oph- thalmol. Glaucoma 2, 224-231 (2019).</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_UM335sr">Disease classification of macular optical coherence tomography scans using deep learning software: validation on independent, multicenter data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<idno type="DOI">10.1097/iae.0000000000002640</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rGbh8vT">Retina</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1549" to="1557" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bhatia, K. K. et al. Disease classification of macular optical coherence tomo- graphy scans using deep learning software: validation on independent, multi- center data. Retina 40, 1549-1557 (2020).</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_UbPkysW">Fusing results of several deep learning architectures for automatic classification of normal and diabetic macular edema in optical coherence tomography</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pmeTh4e">Conference proceedings: Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="670" to="673" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chan, G. C. Y. et al. Fusing results of several deep learning architectures for automatic classification of normal and diabetic macular edema in optical coherence tomography. In Conference proceedings: Annual International Con- ference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference, Vol. 2018, 670-673 (IEEE, 2018).</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_VdGbK9A">Automated identification of diabetic retinopathy using deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gargeya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ophtha.2017.02.008</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nNJ6hjA">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="962" to="969" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gargeya, R. &amp; Leng, T. Automated identification of diabetic retinopathy using deep learning. Ophthalmology 124, 962-969 (2017).</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_r6zT7eh">A deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grassmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ophtha.2018.02.037</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yBKqpfm">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="1410" to="1420" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Grassmann, F. et al. A deep learning algorithm for prediction of age-related eye disease study severity scale for age-related macular degeneration from color fundus photography. Ophthalmology 125, 1410-1420 (2018).</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_JHcGMgy">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2016.17216</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qKPENGQ">JAMA</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gulshan, V. et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. JAMA 316, 2402-2410 (2016).</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_hWEUBfV">Artificial intelligence-based decision-making for age-related macular degeneration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yrZCBSZ">Theranostics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="232" to="245" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hwang, D. K. et al. Artificial intelligence-based decision-making for age-related macular degeneration. Theranostics 9, 232-245 (2019).</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_6HDQamS">Development and validation of a deep-learning algorithm for the detection of neovascular age-related macular degeneration from colour fundus photographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Keel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4dNs9pR">Clin. Exp. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1009" to="1018" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Keel, S. et al. Development and validation of a deep-learning algorithm for the detection of neovascular age-related macular degeneration from colour fundus photographs. Clin. Exp. Ophthalmol. 47, 1009-1018 (2019).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_rTetKAR">Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ophtha.2018.01.034</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pTd3m8v">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="1264" to="1272" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krause, J. et al. Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy. Ophthalmology 125, 1264-1272 (2018).</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_2QTHaA4">Automatic detection of diabetic retinopathy in retinal fundus photographs based on deep learning algorithm</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JXRDymJ">Transl. Vis. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, F. et al. Automatic detection of diabetic retinopathy in retinal fundus pho- tographs based on deep learning algorithm. Transl. Vis. Sci. Technol. 8, 4 (2019).</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_4f5eArM">An automated grading system for detection of vision-threatening referable diabetic retinopathy on the basis of color fundus photographs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.2337/dc18-0147</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Yut6N6A">Diabetes Care</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2509" to="2516" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, Z. et al. An automated grading system for detection of vision-threatening referable diabetic retinopathy on the basis of color fundus photographs. Dia- betes Care 41, 2509-2516 (2018).</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_qGTpwv5">Development and validation of a deep learning system to detect glaucomatous optic neuropathy using fundus photographs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2019.3501</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zme24ce">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="1353" to="1360" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, H. et al. Development and validation of a deep learning system to detect glaucomatous optic neuropathy using fundus photographs. JAMA Ophthalmol. 137, 1353-1360 (2019).</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_3xzGuCt">A deep learning-based algorithm identifies glaucomatous discs using monoscopic fundus photographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ogla.2018.04.002</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5DCnmeh">Ophthalmol. Glaucoma</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, S. et al. A deep learning-based algorithm identifies glaucomatous discs using monoscopic fundus photographs. Ophthalmol. Glaucoma 1, 15-22 (2018).</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_HSEnRAA">Accurate, fast, data efficient and interpretable glaucoma diagnosis with automated spatial analysis of the whole cup to disc profile</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J C</forename><surname>Maccormick</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0209409</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wVRYBXj">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">209409</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">MacCormick, I. J. C. et al. Accurate, fast, data efficient and interpretable glau- coma diagnosis with automated spatial analysis of the whole cup to disc profile. PLoS ONE 14, e0209409 (2019).</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_ZnaTYTH">Deep learning and glaucoma specialists: the relative importance of optic disc features to predict glaucoma referral in fundus photographs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Phene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Pah3DtK">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1627" to="1639" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Phene, S. et al. Deep learning and glaucoma specialists: the relative importance of optic disc features to predict glaucoma referral in fundus photographs. Ophthalmology 126, 1627-1639 (2019).</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_qRvBDZQ">Diabetic retinopathy screening using deep neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Sime</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1111/ceo.13056</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UAXPncv">Clin. Exp. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="412" to="416" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ramachandran, N., Hong, S. C., Sime, M. J. &amp; Wilson, G. A. Diabetic retinopathy screening using deep neural network. Clin. Exp. Ophthalmol. 46, 412-416 (2018).</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_j26YcYK">Deep learning versus human graders for classifying diabetic retinopathy severity in a nationwide screening program</title>
		<author>
			<persName><forename type="first">P</forename><surname>Raumviboonsuk</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-019-0099-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kGPKB8T">npj Digital Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Raumviboonsuk, P. et al. Deep learning versus human graders for classifying diabetic retinopathy severity in a nationwide screening program. npj Digital Med. 2, 25 (2019).</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_bf65Rfa">Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sayres</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ophtha.2018.11.016</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fK5c9N5">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="552" to="564" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sayres, R. et al. Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic retinopathy. Ophthalmology 126, 552-564 (2019).</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_ed5F6XE">Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S W</forename><surname>Ting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hJXMdnQ">JAMA</title>
		<imprint>
			<biblScope unit="volume">318</biblScope>
			<biblScope unit="page" from="2211" to="2223" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ting, D. S. W. et al. Development and validation of a deep learning system for diabetic retinopathy and related eye diseases using retinal images from multi- ethnic populations with diabetes. JAMA 318, 2211-2223 (2017).</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_DTVjvMM">Deep learning in estimating prevalence and systemic risk factors for diabetic retinopathy: a multi-ethnic study</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S W</forename><surname>Ting</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-019-0097-x</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kSCF9Jm">npj Digital Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ting, D. S. W. et al. Deep learning in estimating prevalence and systemic risk factors for diabetic retinopathy: a multi-ethnic study. npj Digital Med. 2, 24 (2019).</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_UqhXQ4r">Diagnostic accuracy of a device for the automated detection of diabetic retinopathy in a primary care setting</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Verbraak</surname></persName>
		</author>
		<idno type="DOI">10.2337/dc18-0148</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6jXgvZe">Diabetes Care</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">651</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Verbraak, F. D. et al. Diagnostic accuracy of a device for the automated detec- tion of diabetic retinopathy in a primary care setting. Diabetes Care 42, 651 (2019).</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_hp3Sa6J">Fast convolutional neural network training using selective data sampling: application to hemorrhage detection in color fundus images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Grinsven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Hoyng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8ewYjqc">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1273" to="1284" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Van Grinsven, M. J., van Ginneken, B., Hoyng, C. B., Theelen, T. &amp; Sánchez, C. I. Fast convolutional neural network training using selective data sampling: application to hemorrhage detection in color fundus images. IEEE Trans. Med. Imaging 35, 1273-1284 (2016).</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_W7cjspC">Evaluation of an AI system for the automated detection of glaucoma from stereoscopic optic disc photographs: the European Optic Disc Assessment Study</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PXSZZJp">Eye</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1791" to="1797" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rogers, T. W. et al. Evaluation of an AI system for the automated detection of glaucoma from stereoscopic optic disc photographs: the European Optic Disc Assessment Study. Eye 33, 1791-1797 (2019).</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_nxBy6CG">Evaluation of a deep learning system for identifying glaucomatous optic neuropathy based on color fundus photographs</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Al-Aswad</surname></persName>
		</author>
		<idno type="DOI">10.1097/ijg.0000000000001319</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wRz366m">J. Glaucoma</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1029" to="1034" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Al-Aswad, L. A. et al. Evaluation of a deep learning system for identifying glaucomatous optic neuropathy based on color fundus photographs. J. Glau- coma 28, 1029-1034 (2019).</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_TKmM7nH">Automated diagnosis of plus disease in retinopathy of prematurity using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hde5D47">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="803" to="810" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brown, J. M. et al. Automated diagnosis of plus disease in retinopathy of pre- maturity using deep convolutional neural networks. JAMA Ophthalmol. 136, 803-810 (2018).</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_AmAUkgn">Utility of deep learning methods for referability classification of age-related macular degeneration</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2018.3799</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NqbaC6Y">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="1305" to="1307" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Burlina, P. et al. Utility of deep learning methods for referability classification of age-related macular degeneration. JAMA Ophthalmol. 136, 1305-1307 (2018).</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_kbUfUv3">Automated grading of age-related macular degeneration from color fundus images using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Burlina</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2017.3782</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2kKDyJ7">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="1170" to="1176" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Burlina, P. M. et al. Automated grading of age-related macular degeneration from color fundus images using deep convolutional neural networks. JAMA Ophthalmol. 135, 1170-1176 (2017).</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_PUZmtef">Comparing humans and deep learning performance for grading AMD: a study in using universal deep features and transfer learning for automated AMD analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Bressler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_edWRMcY">Computers Biol. Med</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="80" to="86" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Burlina, P., Pacheco, K. D., Joshi, N., Freund, D. E. &amp; Bressler, N. M. Comparing humans and deep learning performance for grading AMD: a study in using universal deep features and transfer learning for automated AMD analysis. Computers Biol. Med. 82, 80-86 (2017).</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_2ue4x3B">Clinically applicable deep learning for diagnosis and referral in retinal disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Fauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Y4pY6BG">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1342" to="1350" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">De Fauw, J. et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat. Med. 24, 1342-1350 (2018).</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_622YBAT">Automatic glaucoma classification using color fundus images based on convolutional neural networks and transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gómez-Valverde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UvQWhXQ">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="892" to="913" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gómez-Valverde, J. J. et al. Automatic glaucoma classification using color fundus images based on convolutional neural networks and transfer learning. Biomed. Opt. Express 10, 892-913 (2019).</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_jG5R7wz">Human versus machine: comparing a deep learning algorithm to human gradings for detecting glaucoma on fundus photographs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yArrb8h">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page" from="123" to="131" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jammal, A. A. et al. Human versus machine: comparing a deep learning algo- rithm to human gradings for detecting glaucoma on fundus photographs. Am. J. Ophthalmol. 211, 123-131 (2019).</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_ZPjceUx">Identifying medical diagnoses and treatable diseases by image-based deep learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kermany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_s3AmJh7">Cell</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page">1129</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kermany, D. S. et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell 172, 1122-1131.e1129 (2018).</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_s97pPRa">Deep learning-based automated detection of retinal diseases using optical coherence tomography images</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Gwjt3eX">Biomed. Opt. Express</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="6204" to="6226" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, F. et al. Deep learning-based automated detection of retinal diseases using optical coherence tomography images. Biomed. Opt. Express 10, 6204-6226 (2019).</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_AfFJPbU">An artificial intelligence platform for the multihospital collaborative management of congenital cataracts</title>
		<author>
			<persName><forename type="first">E</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DN7CaMj">Nat. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Long, E. et al. An artificial intelligence platform for the multihospital colla- borative management of congenital cataracts. Nat. Biomed. Eng. 1, 0024 (2017).</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_sDXwCXy">Accuracy of ultra-wide-field fundus ophthalmoscopy-assisted deep learning, a machine-learning technology, for detecting age-related macular degeneration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Matsuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ECMBDJM">Int. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1269" to="1275" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matsuba, S. et al. Accuracy of ultra-wide-field fundus ophthalmoscopy-assisted deep learning, a machine-learning technology, for detecting age-related macular degeneration. Int. Ophthalmol. 39, 1269-1275 (2019).</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_uAvVcEr">Automated detection of a nonperfusion area caused by retinal vein occlusion in optical coherence tomography angiography images using deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nagasato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZY7BqeV">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">223965</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nagasato, D. et al. Automated detection of a nonperfusion area caused by retinal vein occlusion in optical coherence tomography angiography images using deep learning. PLoS ONE 14, e0223965 (2019).</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_zVVyqK9">DeepSeeNet: a deep learning model for automated classification of patient-based age-related macular degeneration severity from color fundus photographs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eeYdsmv">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="565" to="575" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peng, Y. et al. DeepSeeNet: a deep learning model for automated classification of patient-based age-related macular degeneration severity from color fundus photographs. Ophthalmology 126, 565-575 (2019).</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_3EP8x3a">Development of a deep residual learning algorithm to screen for glaucoma from fundus photography</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shibata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TQqERZp">Sci. Rep</title>
		<imprint>
			<biblScope unit="page">14665</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Shibata, N. et al. Development of a deep residual learning algorithm to screen for glaucoma from fundus photography. Sci. Rep. 14665</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_9wjNqe6">Development of an automated screening system for retinopathy of prematurity using a deep neural network for wide-angle retinal images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QsUSkt7">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10232" to="10241" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, Y. et al. Development of an automated screening system for retinopathy of prematurity using a deep neural network for wide-angle retinal images. IEEE Access 7, 10232-10241 (2019).</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_VsHT5TB">Classification of breast cancer in ultrasound imaging using a generic deep learning analysis software: a pilot study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zN9GEMU">Br. J. Radio</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page">20170576</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Becker, A. S. et al. Classification of breast cancer in ultrasound imaging using a generic deep learning analysis software: a pilot study. Br. J. Radio. 91, 20170576 (2018).</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_rY25MaF">Toward an expert level of lung cancer detection and classification using a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YW5qwH5">Oncologist</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1159" to="1165" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, C. et al. Toward an expert level of lung cancer detection and classifi- cation using a deep convolutional neural network. Oncologist 24, 1159-1165 (2019).</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_eskU8pf">End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7tTVnY5">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="954" to="961" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ardila, D. et al. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. Nat. Med. 25, 954-961 (2019).</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_sG3hdsW">Deep learning for chest radiograph diagnosis in the emergency department</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GgHuRPk">Radiology</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="573" to="580" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hwang, E. J. et al. Deep learning for chest radiograph diagnosis in the emer- gency department. Radiology 293, 573-580 (2019).</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_W6PHrkg">Development and validation of a deep learning-based automated detection algorithm for major thoracic diseases on chest radiographs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_47u2NqC">JAMA Netw. Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="191095" to="e191095" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hwang, E. J. et al. Development and validation of a deep learning-based automated detection algorithm for major thoracic diseases on chest radio- graphs. JAMA Netw. Open 2, e191095-e191095 (2019).</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_yryQmDV">Development and validation of a deep learning-based automatic detection algorithm for active pulmonary tuberculosis on chest radiographs</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.1093/cid/ciy967</idno>
		<ptr target="https://doi.org/10.1093/cid/ciy967" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_dQJSMZp">Clin. Infect. Dis</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hwang, E. J. et al. Development and validation of a deep learning-based automatic detection algorithm for active pulmonary tuberculosis on chest radiographs. Clin. Infect. Dis. https://doi.org/10.1093/cid/ciy967 (2018).</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_UGbzCGr">Identifying pulmonary nodules or masses on chest radiography using deep learning: external validation and strategies to improve clinical practice</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Yf8aJGp">Clin. Radiol</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liang, C. H. et al. Identifying pulmonary nodules or masses on chest radiography using deep learning: external validation and strategies to improve clinical practice. Clin. Radiol. 75, 38-45 (2020).</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_UznxwbY">Development and validation of deep learning-based automatic detection algorithm for malignant pulmonary nodules on chest radiographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eAdVuW7">Radiology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="218" to="228" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nam, J. G. et al. Development and validation of deep learning-based automatic detection algorithm for malignant pulmonary nodules on chest radiographs. Radiology 290, 218-228 (2018).</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_Ap8EmAj">Using artificial intelligence to read chest radiographs for tuberculosis detection: A multi-site evaluation of the diagnostic accuracy of three deep learning systems</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ppEcWdf">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">15000</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qin, Z. Z. et al. Using artificial intelligence to read chest radiographs for tuberculosis detection: A multi-site evaluation of the diagnostic accuracy of three deep learning systems. Sci. Rep. 9, 15000 (2019).</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_V6rgkpF">Pulmonary nodule detection in CT images: false positive reduction using multi-view convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DvUXxwh">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1160" to="1169" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Setio, A. A. A. et al. Pulmonary nodule detection in CT images: false positive reduction using multi-view convolutional networks. IEEE Trans. Med. Imaging 35, 1160-1169 (2016).</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main" xml:id="_BHmq88t">Deep convolutional neural network-based software improves radiologist detection of malignant lung nodules on chest radiographs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_A2K5ZGp">Radiology</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="page" from="199" to="209" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sim, Y. et al. Deep convolutional neural network-based software improves radiologist detection of malignant lung nodules on chest radiographs. Radiology 294, 199-209 (2020).</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_6aqGE6T">Automated detection of moderate and large pneumothorax on frontal chest X-rays using deep convolutional neural networks: a retrospective study</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mongan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_u5x7sG3">PLOS Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1002697</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Taylor, A. G., Mielke, C. &amp; Mongan, J. Automated detection of moderate and large pneumothorax on frontal chest X-rays using deep convolutional neural networks: a retrospective study. PLOS Med. 15, e1002697 (2018).</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_cXnBJHB">Machine learning approach for distinguishing malignant and benign lung nodules utilizing standardized perinodular parenchymal features from CT</title>
		<author>
			<persName><forename type="first">J</forename><surname>Uthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VzT8M8p">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="3207" to="3216" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Uthoff, J. et al. Machine learning approach for distinguishing malignant and benign lung nodules utilizing standardized perinodular parenchymal features from CT. Med. Phys. 46, 3207-3216 (2019).</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main" xml:id="_qKj7YSJ">Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Zech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4UnXHCQ">PLOS Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1002683</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zech, J. R. et al. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLOS Med. 15, e1002683 (2018).</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_TgtxdMY">Performance of deep learning model in detecting operable lung cancer with chest radiographs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9gTE6Kp">J. Thorac. Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="86" to="91" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cha, M. J., Chung, M. J., Lee, J. H. &amp; Lee, K. S. Performance of deep learning model in detecting operable lung cancer with chest radiographs. J. Thorac. Imaging 34, 86-91 (2019).</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main" xml:id="_2kqur8D">Deep learning for the classification of small (≤2 cm) pulmonary nodules on ct imaging: a preliminary study</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Chae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vE7J7nM">Acad. Radiol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="55" to="E63" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chae, K. J. et al. Deep learning for the classification of small (≤2 cm) pul- monary nodules on ct imaging: a preliminary study. Acad. Radiol. 27, E55-E63 (2020).</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_sHBP9x8">Towards automatic pulmonary nodule management in lung cancer screening with deep learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PhTXXtW">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">46479</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ciompi, F. et al. Towards automatic pulmonary nodule management in lung cancer screening with deep learning. Sci. Rep. 7, 46479 (2017).</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_sDzePCZ">Assessment of convolutional neural networks for automated classification of chest radiographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Dunnmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_v96X2bz">Radiology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="537" to="544" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dunnmon, J. A. et al. Assessment of convolutional neural networks for auto- mated classification of chest radiographs. Radiology 290, 537-544 (2018).</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_qXT3Z6H">Deep learning-enabled system for rapid pneumothorax screening on chest CT</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pb9g5dx">Eur. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page">108692</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, X. et al. Deep learning-enabled system for rapid pneumothorax screening on chest CT. Eur. J. Radiol. 120, 108692 (2019).</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main" xml:id="_WpQ2f6N">Evaluating the performance of a deep learning-based computer-aided diagnosis (DL-CAD) system for detecting and characterizing lung nodules: comparison with the performance of double reading by radiologists</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_epnfrUu">Thorac. Cancer</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, L., Liu, Z., Huang, H., Lin, M. &amp; Luo, D. Evaluating the performance of a deep learning-based computer-aided diagnosis (DL-CAD) system for detecting and characterizing lung nodules: comparison with the performance of double reading by radiologists. Thorac. Cancer 10, 183-192 (2019).</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main" xml:id="_qW55yjB">Chest radiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and populationadjusted evaluation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Majkowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8pUjEv5">Radiology</title>
		<imprint>
			<biblScope unit="volume">294</biblScope>
			<biblScope unit="page" from="421" to="431" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Majkowska, A. et al. Chest radiograph interpretation with deep learning models: assessment with radiologist-adjudicated reference standards and population- adjusted evaluation. Radiology 294, 421-431 (2019).</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_q4yVRmv">Deep learning-based detection system for multiclass lesions on chest radiographs: comparison with observer readings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cx3mB7C">Eur. Radiol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1359" to="1368" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Park, S. et al. Deep learning-based detection system for multiclass lesions on chest radiographs: comparison with observer readings. Eur. Radiol. 30, 1359-1368 (2019).</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_zGYWgUC">Human-machine partnership with artificial intelligence for chest radiograph diagnosis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3ndPUW3">npj Digital Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Patel, B. N. et al. Human-machine partnership with artificial intelligence for chest radiograph diagnosis. npj Digital Med. 2, 111 (2019).</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main" xml:id="_u9rrEqq">Deep learning for chest radiograph diagnosis: a retrospective comparison of the CheXNeXt algorithm to practicing radiologists</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Jdv77kA">PLOS Med</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1002686</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rajpurkar, P. et al. Deep learning for chest radiograph diagnosis: a retrospective comparison of the CheXNeXt algorithm to practicing radiologists. PLOS Med. 15, e1002686 (2018).</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main" xml:id="_p2WZ6bW">Deep learning in chest radiography: detection of findings and presence of change</title>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZpRV8CK">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">204155</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Singh, R. et al. Deep learning in chest radiography: detection of findings and presence of change. PLoS ONE 13, e0204155 (2018).</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main" xml:id="_MhXDx6C">Deep learning for classifying fibrotic lung disease on high-resolution computed tomography: a casecohort study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L F</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Calandriello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sverzellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eYbvfss">Lancet Respir. Med</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="837" to="845" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Walsh, S. L. F., Calandriello, L., Silva, M. &amp; Sverzellati, N. Deep learning for clas- sifying fibrotic lung disease on high-resolution computed tomography: a case- cohort study. Lancet Respir. Med. 6, 837-845 (2018).</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main" xml:id="_gBtz64K">3D convolutional neural network for differentiating preinvasive lesions from invasive adenocarcinomas appearing as ground-glass nodules with diameters ≤3 cm using HRCT</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qKsaepn">Quant. Imaging Med. Surg</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="491" to="499" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S. et al. 3D convolutional neural network for differentiating pre- invasive lesions from invasive adenocarcinomas appearing as ground-glass nodules with diameters ≤3 cm using HRCT. Quant. Imaging Med. Surg. 8, 491-499 (2018).</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main" xml:id="_Xg4m5Yj">Application of deep learning-based computer-aided detection system: detecting pneumothorax on chest radiograph after biopsy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nfxEynZ">Eur. Radio</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5341" to="5348" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Park, S. et al. Application of deep learning-based computer-aided detection system: detecting pneumothorax on chest radiograph after biopsy. Eur. Radio. 29, 5341-5348 (2019).</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main" xml:id="_kkJ2vDG">Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lakhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dsUZhJ8">Radiology</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="574" to="582" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lakhani, P. &amp; Sundaram, B. Deep learning at chest radiography: automated classification of pulmonary tuberculosis by using convolutional neural networks. Radiology 284, 574-582 (2017).</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main" xml:id="_vZ6BMHz">Deep learning in mammography: diagnostic accuracy of a multipurpose image analysis software in the detection of breast cancer</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UUVW3Bt">Investig. Radio</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="434" to="440" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Becker, A. S. et al. Deep learning in mammography: diagnostic accuracy of a multipurpose image analysis software in the detection of breast cancer. Investig. Radio. 52, 434-440 (2017).</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main" xml:id="_XjGxMfp">Automatic classification of ultrasound breast lesions using a deep convolutional neural network mimicking human decision-making</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ciritsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FCAESTX">Eur. Radio</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5458" to="5468" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ciritsis, A. et al. Automatic classification of ultrasound breast lesions using a deep convolutional neural network mimicking human decision-making. Eur. Radio. 29, 5458-5468 (2019).</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main" xml:id="_SDGYKW6">RAMS: remote and automatic mammogram screening</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tamil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x3w5p3q">Comput. Biol. Med</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cogan, T., Cogan, M. &amp; Tamil, L. RAMS: remote and automatic mammogram screening. Comput. Biol. Med. 107, 18-29 (2019).</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main" xml:id="_fcQU4fy">International evaluation of an AI system for breast cancer screening</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mckinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x7aeWuk">Nature</title>
		<imprint>
			<biblScope unit="volume">577</biblScope>
			<biblScope unit="page" from="89" to="94" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McKinney, S. M. et al. International evaluation of an AI system for breast cancer screening. Nature 577, 89-94 (2020).</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main" xml:id="_sPSUEGX">An automated confirmatory system for analysis of mammograms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Mayorga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M A</forename><surname>Hussein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dERxCbX">Comput. Methods Prog. Biomed</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="134" to="144" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peng, W., Mayorga, R. V. &amp; Hussein, E. M. A. An automated confirmatory system for analysis of mammograms. Comput. Methods Prog. Biomed. 125, 134-144 (2016).</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main" xml:id="_cqA5cRV">Detecting and classifying lesions in mammograms with deep learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ribli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horváth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pollner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Csabai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xk6B7th">Sci. Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">4165</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ribli, D., Horváth, A., Unger, Z., Pollner, P. &amp; Csabai, I. Detecting and classifying lesions in mammograms with deep learning. Sci. Rep. 8, 4165 (2018).</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main" xml:id="_tG2WvYg">Detection of Breast cancer with mammography: effect of an artificial intelligence support system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodríguez-Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ty5EwDZ">Radiology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rodríguez-Ruiz, A. et al. Detection of Breast cancer with mammography: effect of an artificial intelligence support system. Radiology 290, 305-314 (2018).</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main" xml:id="_m73vBqt">Stand-alone artificial intelligence for breast cancer detection in mammography: comparison with 101 radiologists</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Ruiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vyJzzDw">J. Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="916" to="922" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rodriguez-Ruiz, A. et al. Stand-alone artificial intelligence for breast cancer detection in mammography: comparison with 101 radiologists. J. Natl Cancer Inst. 111, 916-922 (2019).</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main" xml:id="_BkKKRpU">Breast mass classification in sonography with transfer learning using a deep convolutional neural network and color conversion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Byra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yZhJdU5">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">100</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Byra, M. et al. Breast mass classification in sonography with transfer learning using a deep convolutional neural network and color conversion. Med. Phys. 46, 746-755 (2019). 100</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main" xml:id="_7h5EftN">Effect of a deep learning framework-based computer-aided diagnosis system on the diagnostic performance of radiologists in differentiating between malignant and benign masses on breast ultrasonography</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YCC5mR4">Korean J. Radio</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">101</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Choi, J. S. et al. Effect of a deep learning framework-based computer-aided diagnosis system on the diagnostic performance of radiologists in differ- entiating between malignant and benign masses on breast ultrasonography. Korean J. Radio. 20, 749-758 (2019). 101</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main" xml:id="_DdbNqBE">Artificial intelligence-based classification of breast lesions imaged with a multiparametric breast mri protocol with ultrafast DCE-MRI, T2, and DWI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Dalmis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hFvTvQg">Investig. Radiol</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">102</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dalmis, M. U. et al. Artificial intelligence-based classification of breast lesions imaged with a multiparametric breast mri protocol with ultrafast DCE-MRI, T2, and DWI. Investig. Radiol. 54, 325-332 (2019). 102</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main" xml:id="_4KeYxm3">Distinction between benign and malignant breast masses at breast ultrasound using deep learning method with convolutional neural network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hkR45YC">Jpn J. Radio</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">103</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fujioka, T. et al. Distinction between benign and malignant breast masses at breast ultrasound using deep learning method with convolutional neural net- work. Jpn J. Radio. 37, 466-472 (2019). 103</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main" xml:id="_TkhT82q">A comparison of logistic regression analysis and an artificial neural network using the BI-RADS Lexicon for ultrasonography in conjunction with introbserver variability</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fWqsSee">J. Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">104</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kim, S. M. et al. A comparison of logistic regression analysis and an artificial neural network using the BI-RADS Lexicon for ultrasonography in conjunction with introbserver variability. J. Digital Imaging 25, 599-606 (2012). 104</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main" xml:id="_AuSuutX">Radiomic versus convolutional neural networks analysis for classification of contrast-enhancing lesions at multiparametric breast MRI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Truhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QkWG9c4">Radiology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Truhn, D. et al. Radiomic versus convolutional neural networks analysis for classification of contrast-enhancing lesions at multiparametric breast MRI. Radiology 290, 290-297 (2019). 105</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main" xml:id="_xQ8Tpv2">Deep neural networks improve radiologists&apos; performance in breast cancer screening</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AjrvBJq">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wu, N. et al. Deep neural networks improve radiologists&apos; performance in breast cancer screening. IEEE Trans. Med. Imaging 39, 1184-1194 (2020). 106</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main" xml:id="_thfsba8">A deep learning model to triage screening mammograms: a simulation study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XV23Z7y">Radiology</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yala, A., Schuster, T., Miles, R., Barzilay, R. &amp; Lehman, C. A deep learning model to triage screening mammograms: a simulation study. Radiology 293, 38-46 (2019). 107</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main" xml:id="_VbNCX8U">Weakly supervised 3D deep learning for breast cancer classification and localization of the lesions in MR images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BAEVSaY">J. Magn. Reson. Imaging</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">108</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou, J. et al. Weakly supervised 3D deep learning for breast cancer classifica- tion and localization of the lesions in MR images. J. Magn. Reson. Imaging 50, 1144-1151 (2019). 108</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main" xml:id="_DRQuZrN">Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YEWUFby">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li, Z. et al. Efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs. Ophthalmology 125, 1199-1206 (2018). 109</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main" xml:id="_dsXhmQY">Diagnostic efficacy and therapeutic decision-making capacity of an artificial intelligence platform for childhood cataracts in eye clinics: a multicentre randomized controlled trial</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5mXWYue">EClinicalMedicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lin, H. et al. Diagnostic efficacy and therapeutic decision-making capacity of an artificial intelligence platform for childhood cataracts in eye clinics: a multi- centre randomized controlled trial. EClinicalMedicine 9, 52-59 (2019). 110</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main" xml:id="_hKXcmbB">Automated triaging of adult chest radiographs with deep artificial neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Annarumma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7cqMjAa">Radiology</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="page">111</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Annarumma, M. et al. Automated triaging of adult chest radiographs with deep artificial neural networks. Radiology 291, 196-202 (2019). 111</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main" xml:id="_Fgdgf4H">A deep learning mammography-based model for improved breast cancer risk prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Portnoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sqVkxAF">Radiology</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="page">112</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yala, A., Lehman, C., Schuster, T., Portnoi, T. &amp; Barzilay, R. A deep learning mammography-based model for improved breast cancer risk prediction. Radi- ology 292, 60-66 (2019). 112</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main" xml:id="_78Rm7y3">Meta-analyses: how to read a funnel plot</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sedgwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_S535Eck">BMJ</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page">113</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sedgwick, P. Meta-analyses: how to read a funnel plot. BMJ 346, f1342 (2013). 113</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main" xml:id="_5wqfwYU">Detection and characterization of MRI breast lesions using deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Herent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_H66FtQY">Diagn. Inter. Imaging</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">114</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Herent, P. et al. Detection and characterization of MRI breast lesions using deep learning. Diagn. Inter. Imaging 100, 219-225 (2019). 114</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main" xml:id="_D3tsbtz">an updated list of essential items for reporting diagnostic studies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Bossuyt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7ZFaXXd">BMJ</title>
		<imprint>
			<biblScope unit="volume">351</biblScope>
			<biblScope unit="page">5527</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bossuyt, P. M. et al. STARD 2015: an updated list of essential items for reporting diagnostic studies. BMJ 351, h5527 (2015).</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main" xml:id="_NUPu9bc">Developing specific reporting guidelines for diagnostic accuracy studies assessing AI interventions: the STARD-AI Steering Group</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sounderajah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2jFEp9P">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sounderajah, V. et al. Developing specific reporting guidelines for diagnostic accuracy studies assessing AI interventions: the STARD-AI Steering Group. Nat. Med. 26, 807-808 (2020). 116</note>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main" xml:id="_NtFREmw">Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vollmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_czR37ZH">BMJ</title>
		<imprint>
			<biblScope unit="volume">368</biblScope>
			<biblScope unit="page">117</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vollmer, S. et al. Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effec- tiveness. BMJ 368, l6927 (2020). 117</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main" xml:id="_5x2d64M">Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wPKHaZX">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu, X. et al. Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension. Nat. Med. 26, 1364-1374 (2020). 118</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main" xml:id="_GdPUyw6">QUADAS-2: a revised tool for the quality assessment of diagnostic accuracy studies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Whiting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xTYFaU8">Ann. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Whiting, P. F. et al. QUADAS-2: a revised tool for the quality assessment of diagnostic accuracy studies. Ann. Intern. Med. 155, 529-536 (2011). 119</note>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><surname>Food</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Administration</surname></persName>
		</author>
		<title level="m" xml:id="_vKYfVJa">Artificial Intelligence and Machine Learning in Software as a Medical Device</title>
		<imprint>
			<publisher>US Food and Drug Administratio</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">120</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Food, U. &amp; Administration, D. Artificial Intelligence and Machine Learning in Software as a Medical Device (US Food and Drug Administratio, 2019). 120.</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main" xml:id="_uDXMhSH">Automated deep-neural-network surveillance of cranial images for acute neurologic events</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Titano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mrtCAdP">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">121</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Titano, J. J. et al. Automated deep-neural-network surveillance of cranial images for acute neurologic events. Nat. Med. 24, 1337-1341 (2018). 121</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main" xml:id="_Dvb58GP">Reliability of supervised machine learning using synthetic data in health care: Model to preserve privacy for data sharing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rankin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UxQ5pdY">JMIR Med. Inform</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">122</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rankin, D. et al. Reliability of supervised machine learning using synthetic data in health care: Model to preserve privacy for data sharing. JMIR Med. Inform. 8, e18910 (2020). 122</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main" xml:id="_ZkqM5QN">On over-fitting in model selection and subsequent selection bias in performance evaluation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UfeZZZs">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cawley, G. C. &amp; Talbot, N. L. On over-fitting in model selection and subsequent selection bias in performance evaluation. J. Mach. Learn. Res. 11, 2079-2107 (2010). 123</note>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main" xml:id="_eY74wYh">What is the state of neural network pruning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
		<ptr target=").124" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
	<note type="raw_reference">Blalock, D., Ortiz, J., Frankle, J. &amp; Guttag, J. What is the state of neural network pruning? Preprint at https://arxiv.org/abs/2003.03033 (2020). 124</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main" xml:id="_pe6SNbx">Challenges to the reproducibility of machine learning models in health care</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Beam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Manrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x33A8N3">JAMA</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">125</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Beam, A. L., Manrai, A. K. &amp; Ghassemi, M. Challenges to the reproducibility of machine learning models in health care. JAMA 323, 305-306 (2020). 125</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main" xml:id="_mBuBwJU">Bridging the health data divide</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yqjG8by">J. Med. Internet Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">126</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Celi, L. A. et al. Bridging the health data divide. J. Med. Internet Res. 18, e325 (2016). 126</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main" xml:id="_d9Aw56Q">Artificial intelligence and machine learning in clinical development: a translational perspective</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_r5nT2Qk">npj Digital Med</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">127</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shah, P. et al. Artificial intelligence and machine learning in clinical develop- ment: a translational perspective. npj Digital Med. 2, 69 (2019). 127</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main" xml:id="_8Rq6jzp">Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD statement</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Moons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_d3Ummh7">BMJ</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Collins, G. S., Reitsma, J. B., Altman, D. G. &amp; Moons, K. G. Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD statement. BMJ 350, g7594 (2015). 128</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main" xml:id="_YCn5xNq">Statement: updated guidelines for reporting parallel group randomised trials</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moher</surname></persName>
		</author>
		<author>
			<persName><surname>Consort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Abtg8K3">BMJ</title>
		<imprint>
			<biblScope unit="volume">340</biblScope>
			<biblScope unit="page">129</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Schulz, K. F., Altman, D. G. &amp; Moher, D. CONSORT 2010 Statement: updated guidelines for reporting parallel group randomised trials. BMJ 340, c332 (2010). 129</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main" xml:id="_X9RK2bC">SPIRIT 2013 statement: defining standard protocol items for clinical trials</title>
		<author>
			<persName><forename type="first">A.-W</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bGFeZQe">Ann. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page">130</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chan, A.-W. et al. SPIRIT 2013 statement: defining standard protocol items for clinical trials. Ann. Intern. Med. 158, 200-207 (2013). 130</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main" xml:id="_Y2RmWyD">Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension</title>
		<author>
			<persName><forename type="first">Cruz</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_EhpfGD8">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">131</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cruz Rivera, S. et al. Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension. Nat. Med. 26, 1351-1363 (2020). 131</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main" xml:id="_KCcCgqS">Reporting of artificial intelligence prediction models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Moons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_J6zAagd">Lancet</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page">132</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Collins, G. S. &amp; Moons, K. G. Reporting of artificial intelligence prediction models. Lancet 393, 1577-1579 (2019). 132</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main" xml:id="_Q6jyMhe">Big data and machine learning algorithms for healthcare delivery</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Khor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9ZKnQeG">Lancet Oncol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">133</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ngiam, K. Y. &amp; Khor, I. W. Big data and machine learning algorithms for health- care delivery. Lancet Oncol. 20, e262-e273 (2019). 133</note>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main" xml:id="_xdZ7tsK">Big Data and Artificial Intelligence for Achieving Universal Health Coverage: an International Consultation on Ethics: Meeting Report</title>
		<imprint>
			<date type="published" when="2017-10">October 2017. 2018</date>
			<publisher>World Health Organization</publisher>
			<biblScope unit="page">134</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization. Big Data and Artificial Intelligence for Achieving Universal Health Coverage: an International Consultation on Ethics: Meeting Report, 12-13 October 2017 (World Health Organization, 2018). 134</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main" xml:id="_SANrUCM">Artificial Intelligence and the &apos;Good Society&apos;: the US, EU, and UK approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taddeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Floridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NgJEMYC">Sci. Eng. Ethics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">135</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cath, C., Wachter, S., Mittelstadt, B., Taddeo, M. &amp; Floridi, L. Artificial Intelligence and the &apos;Good Society&apos;: the US, EU, and UK approach. Sci. Eng. Ethics 24, 505-528 (2018). 135</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main" xml:id="_WQB5DJq">The ethics of biomedical &apos;Big Data&apos; analytics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mYXARMv">Philos. Technol</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">136</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mittelstadt, B. The ethics of biomedical &apos;Big Data&apos; analytics. Philos. Technol. 32, 17-21 (2019). 136</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main" xml:id="_w442Zb8">Preferred reporting items for a systematic review and meta-analysis of diagnostic test accuracy studies: the PRISMA-DTA statement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D F</forename><surname>Mcinnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qaaUwwZ">JAMA</title>
		<imprint>
			<biblScope unit="volume">319</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McInnes, M. D. F. et al. Preferred reporting items for a systematic review and meta-analysis of diagnostic test accuracy studies: the PRISMA-DTA statement. JAMA 319, 388-396 (2018). 137</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main" xml:id="_89W9TYe">Bivariate analysis of sensitivity and specificity produces informative summary measures in diagnostic reviews</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Reitsma</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jclinepi.2005.02.022</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CPD9ahc">J. Clin. Epidemiol</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">138</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Reitsma, J. B. et al. Bivariate analysis of sensitivity and specificity produces informative summary measures in diagnostic reviews. J. Clin. Epidemiol. 58, 982-990 (2005). 138</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main" xml:id="_zN7NFrN">Meta-analysis in clinical trials</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dersimonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_h6bY84k">Controlled Clin. Trials</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
	<note type="raw_reference">DerSimonian, R. &amp; Laird, N. Meta-analysis in clinical trials. Controlled Clin. Trials 7, 177-188 (1986). 139</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main" xml:id="_P629mjT">Guidelines for diagnostic tests and diagnostic accuracy in surgical research</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ashrafian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Darzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Athanasiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wfDcsXX">J. Investig. Surg</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="57" to="65" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jones, C. M., Ashrafian, H., Darzi, A. &amp; Athanasiou, T. Guidelines for diagnostic tests and diagnostic accuracy in surgical research. J. Investig. Surg. 23, 57-65 (2010).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
