<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_t9kWb9m">Considerations for addressing bias in artificial intelligence for health equity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Abr√†moff</surname></persName>
							<email>michael-abramoff@uiowa.edu</email>
							<idno type="ORCID">0000-0002-3490-0037</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation , Washington , D.C.</note>
								<orgName type="department">Collaborative Community for Ophthalmic Imaging Foundation</orgName>
								<orgName type="laboratory">Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>D.C</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michelle</forename><forename type="middle">E</forename><surname>Tarver</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA.</note>
								<orgName type="department">Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA.</note>
								<orgName type="department">Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nilsa</forename><surname>Loyo-Berrios</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA.</note>
								<orgName type="department">Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sylvia</forename><surname>Trujillo</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Center for Devices and Radiological Health , US Food and Drug Administration , Silver Spring , MD , USA.</note>
								<orgName type="department">Center for Devices and Radiological Health</orgName>
								<orgName type="institution">US Food and Drug Administration</orgName>
								<address>
									<settlement>Silver Spring</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danton</forename><surname>Char</surname></persName>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> OCHIN , Portland , OR , USA. Center for Biomedical Ethics ,</note>
								<orgName type="department">Center for Biomedical Ethics</orgName>
								<orgName type="institution">OCHIN</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<note type="raw_affiliation"><label>5</label> Stanford University School of Medicine , San Francisco , CA , USA. Department of Anesthesiology ,</note>
								<orgName type="department">Department of Anesthesiology</orgName>
								<orgName type="institution">Stanford University School of Medicine</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> OCHIN , Portland , OR , USA. Center for Biomedical Ethics ,</note>
								<orgName type="department">Center for Biomedical Ethics</orgName>
								<orgName type="institution">OCHIN</orgName>
								<address>
									<settlement>Portland</settlement>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<note type="raw_affiliation"><label>5</label> Stanford University School of Medicine , San Francisco , CA , USA. Department of Anesthesiology ,</note>
								<orgName type="department">Department of Anesthesiology</orgName>
								<orgName type="institution">Stanford University School of Medicine</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziad</forename><surname>Obermeyer</surname></persName>
							<affiliation key="aff5">
								<note type="raw_affiliation"><label>6</label> Stanford University School of Medicine , Division of Pediatric Cardiac Anesthesia , San Francisco , CA , USA.</note>
								<orgName type="department">Division of Pediatric Cardiac Anesthesia</orgName>
								<orgName type="institution">Stanford University School of Medicine</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<note type="raw_affiliation"><label>6</label> Stanford University School of Medicine , Division of Pediatric Cardiac Anesthesia , San Francisco , CA , USA.</note>
								<orgName type="department">Division of Pediatric Cardiac Anesthesia</orgName>
								<orgName type="institution">Stanford University School of Medicine</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Malvina</forename><forename type="middle">B</forename><surname>Eydelman</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA.</note>
								<orgName type="department">Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA.</note>
								<orgName type="department">Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Maisel</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA.</note>
								<orgName type="department">Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Iowa</orgName>
								<address>
									<settlement>Iowa City</settlement>
									<region>IA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Washington</surname></persName>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Michael</surname></persName>
						</author>
						<author>
							<persName><surname>Abr√†moff</surname></persName>
							<idno type="ORCID">0000-0002-3490-0037</idno>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation , Washington , D.C.</note>
								<orgName type="department">Collaborative Community for Ophthalmic Imaging Foundation</orgName>
								<orgName type="laboratory">Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>D.C</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<note type="raw_affiliation">School of Public Health , University of California , Berkeley , CA , USA.</note>
								<orgName type="department">School of Public Health</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_9Z3Te8k">Considerations for addressing bias in artificial intelligence for health equity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FFD91C7289F2DD6F09A96660B67B0B14</idno>
					<idno type="DOI">10.1038/s41746-023-00913-9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T06:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_EEhjQUk"><p xml:id="_r5fK9pg"><s xml:id="_RCCcJfJ">Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices.</s><s xml:id="_yMQpSYP">Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed.</s><s xml:id="_dw3aKhE">We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated.</s><s xml:id="_QWR857S">The goal of these "Considerations" is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NNyqBWs">INTRODUCTION</head><p xml:id="_bhqbUG2"><s xml:id="_xtRFuWZ">The US Department of Health and Human Services defines health equity 1 as the absence of avoidable disparities or differences among socioeconomic and demographic groups or geographic areas in health status and health outcomes such as disease, disability, or mortality <ref type="bibr" target="#b0">2</ref> .</s><s xml:id="_wUWqwMW">While there are multiple reasons for avoidable health inequities <ref type="bibr" target="#b0">2</ref> , lack of equitable access to diagnosis and treatment are prominent in diseases ranging from breast cancer, depression, to diabetic eye disease <ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref><ref type="bibr" target="#b6">[8]</ref> .</s><s xml:id="_W8jW2qX">Fostering health equity has been a goal of healthcare stakeholders: patients and their organizations, providers, ethicists, payors, regulators, legislators, and AI creators.</s><s xml:id="_x5NanS9">With the exponential growth in new digital health technologies and the rise of artificial intelligence/machine learning (AI/ML)-enabled medical devices, innovators may potentiate existing disparities or instead, leverage opportunities to mitigate health inequities <ref type="bibr" target="#b7">9</ref> .</s></p><p xml:id="_DdSR3Db"><s xml:id="_VtnZTw9">Artificial Intelligence (AI) systems can perform tasks that mimic human cognitive capabilities <ref type="bibr" target="#b8">10</ref> , or may perform new functions that humans are unable to do <ref type="bibr" target="#b9">11</ref> .</s><s xml:id="_sQsB8Cr">Such AI systems are typically not explicitly programmed, the systems learn from data that reflect highly cognitive tasks that may otherwise be performed by trained healthcare professionals.</s><s xml:id="_xUUa5q6">In many cases, AI systems are intended to aid healthcare professionals (HCPs) in managing or treating patients; there are also AI systems intended to be used directly by patients to help manage a disease or condition <ref type="bibr" target="#b10">12</ref> .</s><s xml:id="_jpMwz5H">Healthcare AI systems have the potential to foster access to healthcare for underserved populations, while improving care quality at both the level of the individual patient and the population, at reduced cost for patients, payors, and society <ref type="bibr" target="#b0">[2]</ref><ref type="bibr" target="#b1">[3]</ref><ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref><ref type="bibr" target="#b5">[7]</ref><ref type="bibr" target="#b6">[8]</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b10">[12]</ref><ref type="bibr" target="#b11">[13]</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref><ref type="bibr" target="#b14">[16]</ref> .</s></p><p xml:id="_YjaAUef"><s xml:id="_yw2md2f">Some healthcare AI-enabled devices have been authorized by FDA and have been in clinical use for over a decade, with more devices being currently developed.</s><s xml:id="_d76B3cv">While the vast majority of AI systems intended to be used by HCPs serve to aid those HCPs, there are also AI/ML-enabled devices that make a clinical decision without human oversight, including the first point-of-care autonomous AI system on the US market <ref type="bibr" target="#b15">17</ref> , which received national coverage and reimbursement thereby allowing widespread deployment <ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b17">19</ref> .</s><s xml:id="_SzTBNGU">Thus, AI systems are increasingly in a position to help improve patient and population health outcomes and drive down cost, increase physician job satisfaction, and address health disparities <ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b1">3,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b21">23</ref> .</s></p><p xml:id="_8BsgysD"><s xml:id="_pgXYyxZ">However, adding AI to healthcare processes may unintentionally have undesired effects.</s><s xml:id="_Ua7Jk7B">Multiple studies have shown examples of the use of AI in healthcare (not evaluated by regulatory agencies) exacerbating, rather than mitigating, health disparities <ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25</ref> .</s><s xml:id="_Ab7fQ7k">This is especially the case where the systems that utilize AI do not adhere to rapidly emerging evidence-based standards <ref type="bibr" target="#b24">26</ref> , or where these may be designed for non-marketed use but ultimately are used more broadly.</s><s xml:id="_HdgpZPy">One study of a widely-used AI system showed that while its stated goal was to identify patients who needed extra help with their complex health needs, its actual objective function (its "achieved goal") was to predict healthcare costs.</s><s xml:id="_uqc6jfy">This use of the AI system out of context resulted in sicker, Black patients receiving similar care to healthier, White patients despite needing higher acuity care.</s><s xml:id="_khwfHQG">Thus, the inherent bias in the algorithm appeared to contribute to worse outcomes for Black patients by influencing the likelihood they would receive the appropriate level of care <ref type="bibr" target="#b23">25</ref> .</s></p><p xml:id="_9TAfgJu"><s xml:id="_Tzdn2FG">These ethical and other concerns with AI in healthcare have been shown by a number of research studies <ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref> .</s><s xml:id="_UGSvP2R">Abramoff et al. proposed ethical frameworks for AI <ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref><ref type="bibr" target="#b29">[31]</ref> to help proactively address the issue of undesirable algorithmic bias as well as other concerns with AI.</s><s xml:id="_A4GBVF3">More recently, the Foundational Considerations on Algorithmic Interpretation (FPOAI) workgroup of the Collaborative Community on Ophthalmic Imaging published their "Foundational Considerations" <ref type="bibr" target="#b30">32</ref> on AI as a start to developing metrics for ethics, including metrics for "Equity" <ref type="bibr" target="#b31">33</ref> , in order to be able to evaluate how specific AI systems adhere to various bioethical principles.</s></p><p xml:id="_JJhQxAh"><s xml:id="_cpkCrQ7">Bias in the healthcare process Undesirable bias ("bias" in short) in the conceptualization, development and application of AI-ML-enabled medical devices that is not acknowledged or addressed has the potential to exacerbate existing health inequities or create new disparities.</s><s xml:id="_4VKDMzS">In its recent Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan 10 , FDA articulated the importance of addressing bias in the development and use of AI/ML-enabled medical devices.</s></p><p xml:id="_DVxbtXT"><s xml:id="_AxEeycy">Healthcare is the prevention, treatment, and management of medical conditions and the preservation of mental and physical well-being <ref type="bibr" target="#b32">34</ref> .</s><s xml:id="_SkwBKRx">Through a series of processes and medical products implemented or delivered by healthcare providers, improved outcomes can be realized for patients and populations.</s><s xml:id="_mzEPHBQ">Opportunities exist for AI alone or in combination with healthcare providers, to deliver healthcare solutions.</s><s xml:id="_kcdsV64">The use of AI is rapidly expanding, and examples of AI that have been implemented include assistive AI for breast cancer screening, hypertension management, stroke management, and autonomous AI for diabetic eye exams.</s><s xml:id="_tJSYw3E">Ethical frameworks that consider the potential negative and positive implications of widespread collection, analysis and use of large datasets can be used to determine whether a given healthcare process meets the goal of achieving good health outcomes for all patients.</s><s xml:id="_xDrBY3R">Typically, a Pareto optimum is sought between multiple bioethical principles, such as beneficence, autonomy and equity ("Justics") <ref type="bibr" target="#b30">32</ref> .</s><s xml:id="_gacKpFh">How much a process meets a specific bioethical principles, can be quantified using 'metrics for ethics', and these may affect benefit-risk determinations.</s><s xml:id="_QdY24u9">While it is beyond the scope of these "Considerations" to exhaustively list these metrics, we give some examples to illustrate the principle.</s><s xml:id="_UwgKXvR">For the principle of beneficence, it can be a common metric such as sensitivity, specificity, or clinical outcome, while for the principle of equity, it can be sensitivity disaggregated by demographic subgroup, differential clinical outcome across subgroups, or even population achieved sensitivity or specificity to measure the impact of access to the process, as we have defined previously <ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b33">35</ref> .</s></p><p xml:id="_gtxsFHE"><s xml:id="_GXfJ4BS">The focus of these "Considerations" is to determine how much a given healthcare process (that may include AI) meets the equity principle.</s><s xml:id="_RxVmFGq">To illustrate, a given process may improve healthcare outcomes for a patient, or a population, on average.</s><s xml:id="_xhztbT5">However, when we consider outcomes across the population in more detail, this assumed improvement through integration of the AI system may not be evenly distributed across the population, though disease characteristics including prevalence, severity and prognosis may be otherwise equally distributed.</s><s xml:id="_2wCPghB">The AI may affect a large variance in outcome improvement for some groups compared to others, so that some groups may have substantially worse outcomes than others, such as in the example mentioned above <ref type="bibr" target="#b23">25</ref> .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KGxePmK">Measuring bias in AI systems</head><p xml:id="_9wDZSES"><s xml:id="_ktnggRN">Bias in any part of the healthcare process can lead to differential impacts on different groups <ref type="bibr" target="#b34">36</ref> , and historically has resulted in poorer health outcomes for underrepresented, underserved, and under-resourced groups 1 .</s><s xml:id="_NwY2xCy">Examples of such groups are groups that are defined by racial, ethnic, age, sex, gender, national origin, disability, religion, political, or genetic information characteristics 1 .</s><s xml:id="_Vg6HPwV">Thus, such bias reduces the bioethical principle of "Justice," as Char et al. <ref type="bibr" target="#b26">28</ref> and Abramoff et al. <ref type="bibr" target="#b30">32</ref> described.</s><s xml:id="_fG5ApKV">On the basis of such ethical frameworks, and the continuing development of "metrics for ethics," bias can be quantified as differential impact of a healthcare process on a particular group.</s><s xml:id="_mQm8YYJ">Humans delivering healthcare can also exhibit bias; for example, a recent study showed provider bias, where providers' charts documented Black patients' symptoms and signs in a more pejorative manner, with the potential to exacerbate health disparities <ref type="bibr" target="#b35">37</ref> .</s><s xml:id="_UMcq56e">Other studies suggest physician bias in caring for other populations as well <ref type="bibr" target="#b36">[38]</ref><ref type="bibr" target="#b37">[39]</ref><ref type="bibr" target="#b38">[40]</ref> .</s></p><p xml:id="_3WmxZrw"><s xml:id="_68B2VXd">How much any process, whether partially delivered or aided by AI, or fully delivered by humans, meets the "equity" bioethical principle can be quantified in various ways.</s><s xml:id="_MKur7qz">Such measurements are necessarily specific to the use case and patients' risk of harm being considered, but an emerging set of studies draws on new data to measure algorithmic performance.</s><s xml:id="_rXRwcTn">In fact, such measurements have shown that AI systems can counter bias by human decision makers <ref type="bibr" target="#b9">11</ref> .</s><s xml:id="_FtqgPsa">Similarly, in a diagnostic process, there may be concern about equity in accuracy.</s><s xml:id="_PMkkr8B">Subgroup statistical testing for the presence or absence of an effect on diagnostic accuracy ("accuracy disaggregation") could be used to determine how well the equity principle is met <ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b39">41</ref> .</s><s xml:id="_2hzJJNX">As an example, where there is concern about access to a diagnostic test, population-achieved sensitivity and specificity, which measures the impact of both access and sensitivity, has been proposed as a way to understand the impact of bias on population health, when including so-called invisible populations <ref type="bibr" target="#b30">32</ref> .</s><s xml:id="_kzBCPFv">By allowing optimization of populationachieved sensitivity and specificity, this metric can aid in improving population outcomes through diagnostic assessment, including those performed by AI.</s></p><p xml:id="_zkvKAsN"><s xml:id="_2HyQUZb">Ultimately, understanding and mitigation of AI bias starts with assessment and quantification of possible sources of bias along the entire lifecycle of an AI device.</s><s xml:id="_wc9spRC">Identification of bias is only part of mitigation, and stakeholders will have to decide, based on the AI context and perceived benefit/burden ratios, the extent to which identified biases can and should be mitigated.</s></p><p xml:id="_mF5yYqV"><s xml:id="_U4hS2Ss">The AI total product lifecycle Bioethical analysis of the AI lifecycle by Char et al. <ref type="bibr" target="#b26">28</ref> highlighted the pipeline, ranging from conception over development to deployment ("access") of AI systems, and the parallel pipeline of evaluation and oversight activities at each stage.</s><s xml:id="_yVH3tVS">On top of this model, we analyzed the key factors associated with ethical considerations, from the existing literature as well as newly identified.</s><s xml:id="_kxzNvBT">This pipeline model framework is useful for systematic ethical appraisals of AI systems from development through deployment, and for interdisciplinary collaboration of diverse stakeholders that will be required to understand and subsequently manage the ethical implications of AI in healthcare <ref type="bibr" target="#b26">28</ref> .</s><s xml:id="_xE5UpzA">Abramoff et al. subsequently linked this model to specific metrics for the conception, design, development, training, validation and implementation phases of AI technologies in healthcare <ref type="bibr" target="#b30">32</ref> .</s></p><p xml:id="_HQNCeMY"><s xml:id="_hbEmNAd">Another approach to decompose AI bias into different components has been proposed <ref type="bibr" target="#b40">42</ref> .</s><s xml:id="_8DQ9hvt">The approach divides sources of algorithmic bias into three main components: (direct) model bias, training data variance and training data noise.</s><s xml:id="_E2cRKTT">However, this approach, which focuses on AI built exclusively from retrospectively collected data, incorrectly assumes that the reference standard (sometimes referred to as ground truth) to compare the AI system to is perfectly correct <ref type="bibr" target="#b30">32</ref> , and focuses solely on the potential for bias in the AI/ML algorithm.</s><s xml:id="_daUkvwV">It does not consider the other 'pipeline phases' as set forward by Char et al. nor the integration of the AI into the care process.</s><s xml:id="_ttqv5tp">In other words, it does not consider what matters most to patients and other stakeholders: whether or not the addition of an AI system to the care process results in a favorable change in carei.e., improved clinical outcome <ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b30">32</ref> .</s><s xml:id="_5WBW7u6">Gianfrancesco et al. similarly limited their analysis to bias derived from characteristics of retrospective training sets derived from existing Electronic Health Record (EHR) data <ref type="bibr" target="#b42">44</ref> .</s></p><p xml:id="_uBFEnTs"><s xml:id="_BpQsvNr">The framework developed by Char et al., on the other hand, recognizes specific AI system pipeline phases: conception, development, calibration, implementation, evaluation, deployment, and oversight, and the ethical considerations, including "equity" along each of these phases.</s></p><p xml:id="_W3VCXFW"><s xml:id="_qmnVC3F">In 2019, FDA illustrated how the Total Product Lifecycle (TPLC) approach to the regulation of medical devices similarly applied to AI systems that meet the definition of a medical device, in its Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning <ref type="bibr" target="#b43">45</ref> .</s><s xml:id="_29s9u93">TPLC describes the different phases of a device, including software such as AI systems, from conceptualization to its impact once on the (US) market as the following:</s></p><formula xml:id="formula_0">‚Ä¢ Conception, ‚Ä¢ Design, ‚Ä¢ Development, ‚Ä¢ Validation,</formula><p xml:id="_RqqSQCS"><s xml:id="_reCd7vt">‚Ä¢ Access and marketing, and</s></p><formula xml:id="formula_1">‚Ä¢ Monitoring.</formula><p xml:id="_FJCJadB"><s xml:id="_yVq46dJ">These TPLC phases map in a straightforward manner to the 'pipeline phases' as defined in Char et al. <ref type="bibr" target="#b26">28</ref> and as operationalized in Abramoff et al. <ref type="bibr" target="#b30">32</ref> , see also ref. <ref type="bibr" target="#b44">46</ref> .</s><s xml:id="_C73s8nC">Thus, our intent is to extend TPLC to ethical analysis, by considering the potential impact on equity at each of these phases, as well as the potential for mitigation of AI bias, as defined above.</s><s xml:id="_f69zyVF">From an ethical perspective, the equity principle can be analyzed and optimized within each phase of the existing TPLC framework.</s><s xml:id="_asGq8PU">Depending on which TPLC phase is considered, standard equity metrics can be added, such as for the development phase, key performance indicators in software development, quality systems and risk of population harm analysis, or, for the validation phase of diagnostic medical devices, and absence of racial or ethnic effects on sensitivity and efficacy <ref type="bibr" target="#b30">32</ref> .</s></p><p xml:id="_dBVWSHj"><s xml:id="_zFa4NjN">The proposed TPLC framework in Fig. <ref type="figure" target="#fig_0">1</ref> adds equity considerations for AI systems, including the wider context of where the AI system is used in healthcare, with the goal of net benefit for the entire target population <ref type="bibr" target="#b33">35</ref> .</s><s xml:id="_2truwJ8">This framework (Fig. <ref type="figure" target="#fig_0">1</ref>) is not intended to be comprehensive for all bias risks and mitigations, however it does initiate a discussion on AI and healthcare equity along the TPLC.</s><s xml:id="_H2ejArD">The present framework is intended to complement the principles outlined in the International Medical Device Regulators Forum's (IMDRF) <ref type="bibr" target="#b45">47</ref> Software as a Medical Device Clinical Evaluation, FDA's Good Machine Learning Practice 48 the aforementioned documents, but specifically hone in on ways to identify and mitigate bias in the development and evaluation of AI-ML-enabled software.</s><s xml:id="_ggMWpVG">It helps illustrate the importance of proactively developing an analytical framework to aid in identifying sources of impactful bias along the TPLC before a proposed AI tool propagates health disparities.</s></p><p xml:id="_eAmQTss"><s xml:id="_yF5TdTm">As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, there is the potential to favorably, or unfavorably, impact health equity at every phase of the TPLC.</s><s xml:id="_2pcgvuM">Phases differ in the nature of potential bias, as well as it and its effects on health equity can be quantified and mitigated.</s><s xml:id="_SPm4Xak">Importantly, the equity impact at each phase is independent of all other phases, in other words, even when all potential bias has been mitigated in earlier phases, the next phase can still introduce undesirable equity effects.</s><s xml:id="_7Xxe5pe">While Fig. <ref type="figure" target="#fig_0">1</ref> is not exhaustive, it highlights the opportunities to consider equity and bias along the TPLC.</s><s xml:id="_VzJWFk6">Thus, equity, considered upstream in the development process, has potential ripple effects on the downstream health outcomes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YXyzM9A">Conception phase</head><p xml:id="_XvdjJPW"><s xml:id="_4PGtpZe">At conceptualization of the AI/ML-enabled medical device, it is critical to think through the TPLC development paradigm and identify opportunities to address and mitigate bias.</s><s xml:id="_AhmxQNh">During the conception phase, consider the health conditions and the care process in which the AI system will be used.</s><s xml:id="_Hty7MrT">Determining which health condition(s) will be the focus of the AI/ML-enabled medical device, may at the outset be directed at fostering health equity.</s><s xml:id="_hRcxcNP">Technologies that target conditions where the burden of disease is shouldered by a specific segment of the population may lead to more opportunities for improving health outcomes in that population.</s><s xml:id="_qVtEbTh">Examples might include developing an AI/ML-enabled device that helps diagnose narrow angle glaucoma or open angle glaucoma, conditions with higher prevalence in Asian, or Black and Latino populations, respectively.</s><s xml:id="_N89acAF">In addition, it may be important to determine the setting in which the AI/ML-enabled device will be used at the outset to help identify and mitigate bias.</s><s xml:id="_wzQ8KrD">This requires optimization between generalizability and scalability of an AI/ML algorithm on the one hand, and on the other hand, optimizing its development, training and validation for the populations in which it will be used.</s><s xml:id="_cDPfjbz">For example, if an AI system for a diagnostic process is developed, trained, and validated only on those with ready access to healthcare services, but intended and deployed as a screening tool for an entire community or population, some of who lack routine healthcare access, the differential healthcare access may be a major source of inequity and AI-induced bias.</s><s xml:id="_KawbF8Q">The impact of such differential access can be measured for example through population-achieved sensitivity 32 compared to overall sensitivity, i.e., the fraction of correctly identified disease cases in a sample, without regard to representativeness of that sample (or lack thereof).</s></p><p xml:id="_XGf57ya"><s xml:id="_KnYtgbd">Additionally, historical data used in the development of AI/MLenabled devices may be fraught with miscategorized, mislabeled or mis-tagged, and missingness that differentially impacts different segments of the population.</s><s xml:id="_DDFVST2">For example, historically reported similarities in disease phenotype, prevalence, or severity across groups may not reflect the actual differences in disease phenotype, prevalence or disease severity across groups.</s><s xml:id="_nnn2HTm">This bias may result from historical differences in access to care, differential treatment and quality of care offered in the healthcare system <ref type="bibr" target="#b47">49</ref> , as well as differing group concerns about data usage and ownership <ref type="bibr" target="#b48">50</ref> .</s><s xml:id="_HSQ8svR">Such incorrect assumptions about the disease under study may lead to incorrect, biased AI systems from conceptualization <ref type="bibr" target="#b49">51</ref> .</s><s xml:id="_xdUA3mN">Abramoff et al. asserts such bias may also be the logical result of 'vernacular medicine' which are regional biases in care that may not expand to broader communities <ref type="bibr" target="#b30">32</ref> .</s><s xml:id="_5q62UFE">The inclusion of various viewpoints, backgrounds, experience and expertise on the creator team (including engineers, data scientists, clinicians, and other AI creators) may be an additional opportunity to avoid or mitigate the continuation of such biases into "vernacular AI" during each phase of the TPLC.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_tA3Up4B">Design phase</head><p xml:id="_MydJJH4"><s xml:id="_8UkffSj">During this phase, consider the equity implications related to intended use of the medical device.</s><s xml:id="_mF5AWy7">In addition to the health condition for which the device will be used, other aspects of intended use including the operators and needed skills to use the device (e.g., human factors/usability engineering); the ways in which the device will integrate into the clinical workflow, the length of time needed to effectively use the device and the associated burden on patients and providers; the target patient population; and disease spectrum can all impact utilization and broad access to the technology.</s><s xml:id="_aQrGDx9">Not addressing the ethical and clinical constraints that were described in the conception stage may result in solidifying bias in the AI design.</s><s xml:id="_n5HA6Dc">AI validity, explainability and transparency all help assess the equity implications of the Software as a Medical Device's (SaMD)'s algorithm design <ref type="bibr" target="#b30">32</ref> .</s><s xml:id="_4d677w9">The introduction of using racially invariant priors instead of fully the algorithm from training data may be one approach to prevent the introduction of AI bias <ref type="bibr" target="#b39">41</ref> .</s><s xml:id="_VYur6qs">The TPLC model is foundational to how FDA regulates medical devices, and Design and Development phases are typically rapidly iterated: we emphasize that biases introduced during each of these two phases will propagate to the other, if not mitigated before the next iteration.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_QXXAcZY">Development phase</head><p xml:id="_6MZ5ztT"><s xml:id="_dpFDuev">In the development phase of the AI algorithm, training dataset selection is another opportunity to proactively include equity considerations.</s><s xml:id="_vsUns49">Initial considerations for the training sets used in AI/ML-enabled devices were published by FDA in 2021 <ref type="bibr" target="#b8">10</ref> , and further expanded in the Good Machine Learning Practice (GMLP) document <ref type="bibr" target="#b46">48</ref> .</s><s xml:id="_bFJwYeK">It is important to consider whether the relevant characteristics of the intended patient population (such as age, gender, sex, race, and ethnicity), use, and measurement inputs are sufficiently represented in training and test datasets, to maximize generalization to the intended population in which the AI system will be used <ref type="bibr" target="#b46">48</ref> .</s><s xml:id="_K4ps9f4">Bias issues may arise around a) retrospective use of historical datasets b) more or less inclusive contemporary or prospectively collected datasets, and c) clinical study verification (covered in Validation section).</s><s xml:id="_f99E5Qe">For example, use of historical datasets may reflect differential access to care and differential quality of care due to sociocultural forces may lead to skewed distribution in the training data <ref type="bibr" target="#b48">50</ref> .</s><s xml:id="_nvARxTD">Prospective collection of data for training datasets is not exempt from potential biases.</s><s xml:id="_c2XDzJn">The eligibility criteria or other aspects of the recruitment and enrollment process, such as the reward or time commitment for data collection (e.g., need to miss work) could potentially be a constraint for people with limited financial resourcesthe socalled "invisible populations".</s><s xml:id="_xm5cdDk">Finally, bias in the 'reference standard', for the training dataset, may be caused by using inadequate proxies for clinical outcomes as reference standard <ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b50">52</ref> .</s><s xml:id="_HWrPhWh">For considerations around which reference standard to use, see Abramoff et al. 2021 <ref type="bibr" target="#b30">32</ref> As an example, if clinicians are used as the reference standard, their potential bias in their diagnosis may lead to bias in the training data, ultimately persisting as bias in the AI system <ref type="bibr" target="#b35">37</ref> .</s><s xml:id="_zaYwBtH">Similarly, outcomes or proxies thereof used as reference standards may historical inequities for subgroups, so that access and bias in delivery of care for subgroups may result in differential outcomes for the same disease phenotype.</s><s xml:id="_GWRcZpx">Metrics for such training set bias may be assessed through subgroup analysis and stratification of characteristics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4c2sWCZ">Validation phase</head><p xml:id="_6q9KJpE"><s xml:id="_nmsGHCF">Important factors to consider in the validation phase for AI/MLenabled devices have been included in documents, such as the International Medical Device Regulators Forum (IMDRF's) Software as a Medical Device (SaMD): Clinical Evaluation <ref type="bibr" target="#b51">53</ref> , and more recently in FDA's Guiding Principles for Good Machine Learning Practice (GMLP) <ref type="bibr" target="#b46">48</ref> .</s><s xml:id="_EYa2FwG">We use the term validation consistent with how it is used within the context of medical device development, i.e., 'confirmation by examination and provision of objective evidence that the particular requirements for a specific intended use can be consistently fulfilled' <ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b52">54</ref> .</s><s xml:id="_ZSAhsxv">When considering bias in validation, it is critical to evaluate how well clinical study subjects are mirrored in the data sets on which the AI system was conceptualized, designed, and developed.</s><s xml:id="_J9Tfv4s">Ensuring that the relevant characteristics of the intended patient population including age, gender, sex, race and ethnicity, are appropriately represented in a sample of adequate size in a clinical study, allows results to be reasonably generalized to the intended use population.</s><s xml:id="_vJXAgr4">Thoughtful evaluation will expose bias and enhance appropriate and generalizable performance across the intended patient population.</s><s xml:id="_axVQMmp">In addition, diversity in clinical sites where the studies are conducted will be an important consideration to generate diverse validation studies.</s><s xml:id="_FvJtZSv">Historically disadvantaged groups may be more likely to receive care in clinics that may lack the resources for the trained operators necessary to be a study site <ref type="bibr" target="#b53">55</ref> .</s><s xml:id="_puMyypB">By considering metrics for how similar to real-world use the trial is (e.g., metric for operator expertise and diagnosability), there may be an opportunity to expand inclusion of more diverse clinical sites in the trial.</s><s xml:id="_CY76XKy">These approaches can be assessed for their impact on replicability of findings in other samples of patients such as whether preregistration and arm's length protocols are followed <ref type="bibr" target="#b30">32</ref> .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2SScctF">Access and monitoring phases</head><p xml:id="_JrWPZtK"><s xml:id="_texJ8Nj">The access and monitoring phase includes deployment, monitoring and surveillance of the AI/ML-enabled device's performance and may also be subject to bias in implementation.</s><s xml:id="_XufREer">This phase is where we have an opportunity to more comprehensively consider and measure the cumulative effects of potential biases at all phases of the TPLC, with real world evidence.</s><s xml:id="_hmrFGPC">In other words, we can estimate whether the 'real-world realization' of the AI system as it was originally conceptualized, designed and developed, measurably impacts health equity.</s><s xml:id="_yH9HBAA">During this phase, creators can assess the a priori vision of how well the AI-enabled device fits into the clinical workflow, and is usable with the prespecified staff skills, usability, cost and other resource use <ref type="bibr" target="#b30">32</ref> .</s><s xml:id="_MWZPmVT">For example, if monitoring shows that low resourced patients are unable to access the device because the clinics in those locations cannot afford the high cost of the device, such as in under-resourced, or rural clinics, then the goals of the AI-enabled device to impact health outcomes in this population may not be realized; this can be quantified by a metric like population achieved sensitivity <ref type="bibr" target="#b8">10</ref> .</s><s xml:id="_sEJGjWh">This monitoring information may thus lead to re-conceptualization of the device, for example with lower cost hardware, and more sophisticated ML algorithms to increase accessibility of the device in these populations <ref type="bibr" target="#b54">56</ref> .</s><s xml:id="_4mMFJhJ">Receiving care with the AI system may also impart higher cost or higher copay for the patient which may impact patients' access differentially.</s><s xml:id="_NfKzgee">AI-induced bias can be introduced here through mismatch or shifts in process completion.</s><s xml:id="_Afb2JyS">For example, a process that combines identifying and treating true cases of diabetic retinopathy in people with diabetes may be skewed towards negative outcomes if there is differential follow-through for treatment.</s><s xml:id="_m2afZZN">This follow-through for treatment is also subject to the same social determinants of health that can lead to inequitable utilization of healthcare services, and thereby lead to biased assessments of the device's performance.</s><s xml:id="_ydVkGxT">While these factors are best considered in the concept phase, effects on equity can be quantified and mitigations implemented here.</s><s xml:id="_WxxQeAV">Metrics such as population achieved sensitivity and specificity, device underperformance in certain groups, and other metrics of equitable access and outcomes can be assessed across subgroups longitudinally, and may help determine at what stage of the TPLC there may be opportunities to mitigate inequities.</s><s xml:id="_FEas784">The above shows the importance of monitoring AI's impact in the real world, and the limitations of current frameworks for how to think about monitoring and surveillance in such a real-world setting: discussion among all stakeholders is crucial.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TjaMaEZ">DISCUSSION</head><p xml:id="_bjd5BTa"><s xml:id="_fqkN2Vb">We describe the sources and impacts of bias in AI systems on health equity, and propose approaches for potential mitigation across the AI's Total Product Lifecycle (TPLC).</s><s xml:id="_CY9EDFg">These Considerations are the start of a discussion with all stakeholders, including bioethicists, AI creators, regulatory agencies, patient advocacy groups, clinicians and their professional societies, other provider groups, and payors and value-based care organizations.</s><s xml:id="_Z7Pmqap">Equity analysis and bias mitigation consistent with the present, expanded TPLC, will allow AI creators, regulators, payors and healthcare practitioners to better understand how potential bias may impact healthcare decisions and outcomes.</s><s xml:id="_4ad4EA5">The many potential sources of bias that can be introduced or addressed along the different phases of the TPLC can be assessed using appropriate metrics and mitigated using tailored approaches.</s><s xml:id="_y6DqP8D">By focusing on the goal of ensuring health equity along the TPLC framework, stakeholders can collectively identify and mitigate inequities, leading to better health outcomes for all.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc><div><p xml:id="_UjPGY8Z"><s xml:id="_z8MRBvZ">Fig. 1 Total Product LifeCycle (TPLC) equity expanded framework with examples for each phase.</s></p></div></figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_kdvHZKa"><s xml:id="_YqaXJnq">npj Digital Medicine (2023) 170 Published in partnership with Seoul National University Bundang Hospital 1234567890():,;</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_v7Nw8zr"><s xml:id="_RU47b9E">Published in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023) 170</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_y5zNxx5"><s xml:id="_C99eKkm">npj Digital Medicine (2023) 170Published in partnership with Seoul National University Bundang Hospital</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_xNqzzev">ACKNOWLEDGEMENTS</head><p xml:id="_989gc7Y"><s xml:id="_RUCYD3v"><rs type="institution">FDA</rs> participates in the <rs type="institution">Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group</rs> as a member of the <rs type="institution">Collaborative Community on Ophthalmic Imaging Foundation</rs>.</s><s xml:id="_CenbbRx">Any opinions, positions, or policies in the manuscript are those of the authors and do not necessarily represent those of the <rs type="institution">FDA</rs>.</s></p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jKjrDWm">Reporting summary</head><p xml:id="_ErrjJH5"><s xml:id="_Byc54CS">Further information on research design is available in the Nature Research Reporting Summary linked to this article.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rfDTX5H">ADDITIONAL INFORMATION Supplementary information</head><p xml:id="_AMYbMap"><s xml:id="_xehwKQ6">The online version contains supplementary material available at <ref type="url" target="https://doi.org/10.1038/s41746-023-00913-9">https://doi.org/10.1038/s41746-023-00913-9</ref>.</s></p><p xml:id="_adTGhXD"><s xml:id="_Czye9ZG">Correspondence and requests for materials should be addressed to Michael D. Abr√†moff.</s></p><p xml:id="_7Q5qWwP"><s xml:id="_c3YDZNW">Reprints and permission information is available at <ref type="url" target="http://www.nature.com/reprints">http://www.nature.com/  reprints</ref> Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_CKhKF2A">Inequalities in health: definitions, concepts, and theories</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Arcaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Arcaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Subramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3xJCmDz">Glob. Health Action</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">27106</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Arcaya, M. C., Arcaya, A. L. &amp; Subramanian, S. V. Inequalities in health: definitions, concepts, and theories. Glob. Health Action 8, 27106 (2015).</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_rSJPf4K">Racial/ethnic disparities and barriers to diabetic retinopathy screening in youths</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2021.1551</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8T2PzJp">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="791" to="795" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thomas, C. G. et al. Racial/ethnic disparities and barriers to diabetic retinopathy screening in youths. JAMA Ophthalmol. 139, 791-795 (2021)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_t7ACFzD">Identifying key barriers to effective breast cancer control in rural settings</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Sprague</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ypmed.2021.106741</idno>
		<ptr target="https://doi.org/10.1016/j.ypmed.2021.106741" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_ETEtnVV">Prev Med</title>
		<imprint>
			<biblScope unit="page">106741</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sprague, B. L. et al. Identifying key barriers to effective breast cancer control in rural settings. Prev Med. 106741, https://doi.org/10.1016/j.ypmed.2021.106741 (2021).</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_KEffCNH">Health and racial disparity in breast cancer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Yedjou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VXjJ6Z5">Adv. Exp. Med. Biol</title>
		<imprint>
			<biblScope unit="volume">1152</biblScope>
			<biblScope unit="page" from="31" to="49" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yedjou, C. G. et al. Health and racial disparity in breast cancer. Adv. Exp. Med. Biol. 1152, 31-49 (2019).</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_4vCv4QB">Disparities in diabetic retinopathy screening and disease for racial and ethnic minority populations-a literature review</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nsiah-Kumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Ortmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0027-9684(15)30929-9</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VnQds42">J. Natl Med Assoc</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="430" to="437" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nsiah-Kumi, P., Ortmeier, S. R. &amp; Brown, A. E. Disparities in diabetic retinopathy screening and disease for racial and ethnic minority populations-a literature review. J. Natl Med Assoc. 101, 430-437 (2009).</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_y5Sah9H">Black-white differences in risk of developing retinopathy among individuals with type 2 diabetes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Georgopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wqjvnn2">Diabetes Care</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="779" to="783" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Harris, E. L., Sherman, S. H. &amp; Georgopoulos, A. Black-white differences in risk of developing retinopathy among individuals with type 2 diabetes. Diabetes Care 22, 779-83 (1999).</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_GXqJKVe">Diabetes and diabetic retinopathy in a Mexican-American population: proyecto VER</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>West</surname></persName>
		</author>
		<idno type="DOI">10.2337/diacare.24.7.1204</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MD78FMd">Diabetes Care</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1204" to="1209" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">West, S. K. et al. Diabetes and diabetic retinopathy in a Mexican-American population: proyecto VER. Diabetes Care 24, 1204-9 (2001).</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_NREh3a9">Technologies and health inequities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Timmermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaufman</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-soc-121919-054802</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9ScRtYj">Ann. Rev. Sociol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="583" to="602" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Timmermans, S., Kaufman, R. Technologies and health inequities. Ann. Rev. Sociol. 46, 583-602 (2020).</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D A</forename></persName>
		</author>
		<idno type="DOI">10.31032/ijbpas/2025/14.8.9309</idno>
		<ptr target="https://www.fda.gov/media/145022/download" />
		<title level="m" xml:id="_dTqVhzJ">Digital Health Center of Excellence C. Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Food &amp; Drug Administration</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">U. S. Food &amp; Drug Administration (F. D. A.) Digital Health Center of Excellence C. Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan. https://www.fda.gov/media/145022/download (2021).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_E7q9enB">An algorithmic approach to reducing unexplained pain disparities in underserved populations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-020-01192-7</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cw33BGF">Nat. Med</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="136" to="140" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pierson, E., Cutler, D. M., Leskovec, J., Mullainathan, S. &amp; Obermeyer, Z. An algorithmic approach to reducing unexplained pain disparities in underserved populations. Nat. Med 27, 136-140 (2021).</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_UdneHrP">Board of trustees policy summary</title>
		<ptr target="https://www.ama-assn.org/system/files/2019-08/ai-2018-board-policy-summary.pdf" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_sZpX6hf">Augmented Intelligence In Healthcare</title>
		<imprint>
			<publisher>American Medical Association</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">American Medical Association. Board of trustees policy summary. Augmented Intelligence In Healthcare. https://www.ama-assn.org/system/files/2019-08/ai- 2018-board-policy-summary.pdf (2019).</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_JQDEmZC">Artificial intelligence in health care: will the value match the hype</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Emanuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Wachter</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2019.4914</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cWBDVvW">JAMA</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="page" from="2281" to="2282" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Emanuel, E. J. &amp; Wachter, R. M. Artificial intelligence in health care: will the value match the hype. JAMA 321, 2281-2282 (2019).</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Autonomous</surname></persName>
		</author>
		<ptr target="https://www.forbes.com/sites/oraclecloud/2020/01/16/autonomous-in-action-self-driving-cars-get-all-the-publicity-but-other-industries-are-already-getting-exceptional-value-from-ai-based-systems/#1ecc65d86e94" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Autonomous A. I. in Action. https://www.forbes.com/sites/oraclecloud/2020/01/ 16/autonomous-in-action-self-driving-cars-get-all-the-publicity-but-other- industries-are-already-getting-exceptional-value-from-ai-based-systems/ #1ecc65d86e94 (2020).</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_yKEHtd9">What are health disparities and health equity? We need to be clear</title>
		<author>
			<persName><forename type="first">P</forename><surname>Braveman</surname></persName>
		</author>
		<idno type="DOI">10.1177/00333549141291s203</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tny7Gks">Public Health Rep</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="5" to="8" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Braveman, P. What are health disparities and health equity? We need to be clear. Public Health Rep. 129, 5-8 (2014).</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_G5RQbZr">Healthy people: the role of law and policy in the Nation&apos;s Public Health Agenda</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mcgowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Teitelbaum</surname></persName>
		</author>
		<idno type="DOI">10.1177/1073110519857320</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_u7kmWPm">J. Law Med. Ethics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="63" to="67" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">McGowan, A. K., Kramer, K. T. &amp; Teitelbaum, J. B. Healthy people: the role of law and policy in the Nation&apos;s Public Health Agenda. J. Law Med. Ethics 47, 63-67 (2019).</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.31525/fda2-ucm604357.htm</idno>
		<ptr target="https://www.fda.gov/newsevents/newsroom/pressannouncements/ucm604357.htm" />
		<title level="m" xml:id="_K2v3A2w">FDA permits marketing of artificial intelligencebased device to detect certain diabetes-related eye problems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Food &amp; Drug Administration</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">U. S. Food &amp; Drug Administration. FDA permits marketing of artificial intelligence- based device to detect certain diabetes-related eye problems. https://www.fda.gov/ newsevents/newsroom/pressannouncements/ucm604357.htm (2018).</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_sBWhgdN">Proposal to Establish Values for Remote Retinal Imaging (CPT code 92229)</title>
		<author>
			<persName><forename type="first">Medicaid</forename><surname>Centers For Medicare</surname></persName>
		</author>
		<author>
			<persName><surname>Services</surname></persName>
		</author>
		<idno type="DOI">10.1377/forefront.20210129.937900</idno>
		<ptr target="https://public-inspection.federalregister.gov/2021-14973.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">56</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Centers for Medicare &amp; Medicaid Services. e. Proposal to Establish Values for Remote Retinal Imaging (CPT code 92229) (Pages 56ff). https://public- inspection.federalregister.gov/2021-14973.pdf (2021).</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_ANVXvyD">A reimbursement framework for artificial intelligence in healthcare</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-022-00621-w</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AArwsE5">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abramoff, M. D. et al. A reimbursement framework for artificial intelligence in healthcare. NPJ Digit Med. 5, 72 (2022).</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_DtRGV6Y">Ensuring fairness in machine learning to advance health equity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajkomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Chin</surname></persName>
		</author>
		<idno type="DOI">10.7326/m18-1990</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RsxtMUw">Ann. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="866" to="872" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rajkomar, A., Hardt, M., Howell, M. D., Corrado, G. &amp; Chin, M. H. Ensuring fairness in machine learning to advance health equity. Ann. Intern. Med. 169, 866-872 (2018).</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_hd5FK27">Machine learning, health disparities, and causal reasoning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Cullen</surname></persName>
		</author>
		<idno type="DOI">10.7326/m18-3297</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ety5KNr">Ann. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="883" to="884" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Goodman, S. N., Goel, S. &amp; Cullen, M. R. Machine learning, health disparities, and causal reasoning. Ann. Intern. Med. 169, 883-884 (2018).</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_h7XkJtF">The SEE study: safety, efficacy, and equity of implementing autonomous artificial intelligence for diagnosing diabetic retinopathy in youth</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.2337/dc20-1671</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eE5qvPb">Diabetes Care</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="781" to="787" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wolf, R. M. et al. The SEE study: safety, efficacy, and equity of implementing autonomous artificial intelligence for diagnosing diabetic retinopathy in youth. Diabetes Care 44, 781-787 (2021).</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_qUfeK6V">Cost-effectiveness of autonomous point-of-care diabetic retinopathy screening for pediatric patients with diabetes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Channa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamaophthalmol.2020.3190</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gYdDbER">JAMA Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1063" to="1069" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wolf, R. M., Channa, R., Abramoff, M. D. &amp; Lehmann, H. P. Cost-effectiveness of autonomous point-of-care diabetic retinopathy screening for pediatric patients with diabetes. JAMA Ophthalmol. 138, 1063-1069 (2020).</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_RQFqGsA">stand for augmenting inequality in the era of covid-19 healthcare?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leslie</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.n304</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_damKaCU">BMJ</title>
		<imprint>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page">304</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Leslie, D. et al. stand for augmenting inequality in the era of covid-19 healthcare? BMJ 372, n304 (2021).</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_BTyYYkg">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aax2342</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dumcqY7">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Obermeyer, Z., Powers, B., Vogeli, C. &amp; Mullainathan, S. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 447-453 (2019).</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main" xml:id="_wxV26Ud">Algorithmic bias playbook</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nissan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<ptr target="https://www.chicagobooth.edu/-/media/project/chicago-booth/centers/caai/docs/algorithmic-bias-playbook-june-2021.pdf" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Center for Applied Artificial Intelligence, University of Chicago Booth</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Obermeyer, Z., Nissan, R., Stern, M. Algorithmic bias playbook. Center for Applied Artificial Intelligence, University of Chicago Booth. https:// www.chicagobooth.edu/-/media/project/chicago-booth/centers/caai/docs/ algorithmic-bias-playbook-june-2021.pdf (2021).</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_JevQzyZ">Lessons learned about autonomous AI: finding a safe, efficacious, and ethical path through the development process</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tobey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Char</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XPvKJ8r">Am. J. Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="134" to="142" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abramoff, M. D., Tobey, D. &amp; Char, D. S. Lessons learned about autonomous AI: finding a safe, efficacious, and ethical path through the development process. Am. J. Ophthalmol. 214, 134-142 (2020).</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_SBns978">Identifying ethical considerations for machine learning healthcare applications</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Char</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abr√†moff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feudtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2gtPPtd">Am. J. Bioeth</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="7" to="17" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Char, D. S., Abr√†moff, M. D. &amp; Feudtner, C. Identifying ethical considerations for machine learning healthcare applications. Am. J. Bioeth. 20, 7-17 (2020).</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_qAj2QsC">Implementing machine learning in health care -addressing ethical challenges</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Char</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Magnus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XZk3dfE">N. Engl. J. Med</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="981" to="983" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Char, D. S., Shah, N. H. &amp; Magnus, D. Implementing machine learning in health care -addressing ethical challenges. N. Engl. J. Med. 378, 981-983 (2018).</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_RyXtWsS">Diagnosing diabetic retinopathy with artificial intelligence: what information should be included to ensure ethical informed consent? Original research</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ursin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Timmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orzechowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Steger</surname></persName>
		</author>
		<idno type="DOI">10.3389/fmed.2021.695217</idno>
		<ptr target="https://doi.org/10.3389/fmed.2021.695217" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_dUeuJGa">Front. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ursin, F., Timmermann, C., Orzechowski, M., Steger, F. Diagnosing diabetic reti- nopathy with artificial intelligence: what information should be included to ensure ethical informed consent? Original research. Front. Med. 8 https://doi.org/ 10.3389/fmed.2021.695217 (2021).</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_DVzNwkg">Commentary: diagnosing diabetic retinopathy with artificial intelligence: what information should be included to ensure ethical informed consent</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tava</surname></persName>
		</author>
		<idno type="DOI">10.3389/fmed.2021.765936</idno>
		<ptr target="https://doi.org/10.3389/fmed.2021.765936" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_T9sQGh6">Front. Med</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">765936</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abramoff, M. D., Mortensen, Z. &amp; Tava, C. Commentary: diagnosing diabetic retinopathy with artificial intelligence: what information should be included to ensure ethical informed consent. Front. Med. 8, 765936, https://doi.org/10.3389/ fmed.2021.765936 (2021).</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_N9C4zsC">Foundational considerations for artificial intelligence using ophthalmic images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abramoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NNsukQQ">Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="14" to="e32" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abramoff, M. D. et al. Foundational considerations for artificial intelligence using ophthalmic images. Ophthalmology 129, e14-e32 (2022).</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main" xml:id="_guf4dxN">Fairness in machine learning: a survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Caton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haas</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2010.04053</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2010.04053" />
		<imprint>
			<date type="published" when="2020-10-01">2020. 01 October 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Caton, S., Haas, C. Fairness in machine learning: a survey. arxiv, https://doi.org/ 10.48550/arXiv.2010.04053 (2020), Accessed 01 October 2020.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_8Fmwf4u">Medicare: a strategy for quality assurance</title>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Tk3cVWN">Medicare: A Strategy for Quality Assurance</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Lohr</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Institute of Medicine (IOM ; Institute of Medicine</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Institute of Medicine (IOM). Medicare: a strategy for quality assurance. In: Lohr, K. N. (ed) Medicare: A Strategy for Quality Assurance: Volume 1. (Institute of Medicine, 1990).</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" xml:id="_fbHAKXR">Factors to Consider When Making Benefit-Risk Determinations in Medical Device Premarket Approval and De Novo Classifications</title>
		<imprint>
			<publisher>U.S. Food &amp; Drug Administration</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Food &amp; Drug Administration (FDA) CDRH</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">U.S. Food &amp; Drug Administration (FDA) CDRH. Factors to Consider When Making Benefit-Risk Determinations in Medical Device Premarket Approval and De Novo Classifications (U.S. Food &amp; Drug Administration, 2019).</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_Hk3BgSQ">Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakeshimana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Olubeko</surname></persName>
		</author>
		<idno type="DOI">10.3389/frai.2020.561802</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kR4yAUk">Front Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">561802</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fletcher, R. R., Nakeshimana, A. &amp; Olubeko, O. Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health. Front Artif. Intell. 3, 561802 (2020).</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_R8BKJr2">Negative patient descriptors: documenting racial bias in the electronic health record</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oliwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Tung</surname></persName>
		</author>
		<idno type="DOI">10.1377/hlthaff.2021.01423</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Tkq56eF">Health Aff</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="203" to="211" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sun, M., Oliwa, T., Peek, M. E. &amp; Tung, E. L. Negative patient descriptors: doc- umenting racial bias in the electronic health record. Health Aff. 41, 203-211 (2022).</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_69myJrt">What factors lead to racial disparities in outcomes after total knee arthroplasty?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s40615-021-01168-4</idno>
		<ptr target="https://doi.org/10.1007/s40615-021-01168-4" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_YGgcZ7z">J. Racial Ethn. Health Disparities</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hu, D. A. et al. What factors lead to racial disparities in outcomes after total knee arthroplasty? J. Racial Ethn. Health Disparities. https://doi.org/10.1007/s40615- 021-01168-4 (2021).</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_P5Ck9zu">Racial and socioeconomic differences in eye care utilization among Medicare beneficiaries with glaucoma</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Halawa</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ophtha.2021.09.022</idno>
		<ptr target="https://doi.org/10.1016/j.ophtha.2021.09.022" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_7EETqss">Ophthalmology</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Halawa, O. A. et al. Racial and socioeconomic differences in eye care utilization among Medicare beneficiaries with glaucoma. Ophthalmology. https://doi.org/ 10.1016/j.ophtha.2021.09.022 (2021).</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_XPqvpvg">Prisoner health care</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Goldfrank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ebzjcy8">Urban Health</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="26" to="28" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gage, D. &amp; Goldfrank, L. Prisoner health care. Urban Health. 14, 26-8 (1985).</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_2JWs9xr">Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Abr√†moff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Folk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6v4eJNq">Nat. Digital Med</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abr√†moff, M. D., Lavin, P. T., Birch, M., Shah, N. &amp; Folk, J. C. Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices. Nat. Digital Med. 39, 1 (2018).</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main" xml:id="_eDhMu99">Why is my classifier discriminatory?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno>arXiv 1805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">12002</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen, I., Johansson, F. D. &amp; Sontag, D. Why is my classifier discriminatory? arXiv 1805, 12002 (2018).</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_cvtRCY4">Making machine learning models clinically useful</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bagley Ph</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2019.10306</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zbXwufd">JAMA</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="1351" to="1352" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shah, N. H., Milstein, A. &amp; Bagley Ph, D. S. Making machine learning models clinically useful. JAMA 322, 1351-1352 (2019).</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_N2sEaV6">Potential biases in machine learning algorithms using electronic health record data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gianfrancesco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tamang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yazdany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schmajuk</surname></persName>
		</author>
		<idno type="DOI">10.1001/jamainternmed.2018.3763</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bUSu5eR">JAMA Intern. Med</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="1544" to="1547" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gianfrancesco, M. A., Tamang, S., Yazdany, J. &amp; Schmajuk, G. Potential biases in machine learning algorithms using electronic health record data. JAMA Intern. Med. 178, 1544-1547 (2018).</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main" xml:id="_xexxyKj">Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD)</title>
		<ptr target="https://www.fda.gov/media/122535/download" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Food &amp; Drug Administration (FDA) CDRH</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">U.S. Food &amp; Drug Administration (FDA) CDRH. Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD), https://www.fda.gov/media/122535/download (2019).</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main" xml:id="_NhBxWWb">Towards a Standard for Identifying and Managing Bias in Artificial Intelligence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<idno type="DOI">10.6028/NIST.SP.1270</idno>
		<ptr target="https://doi.org/10.6028/NIST.SP.1270" />
		<imprint>
			<date type="published" when="1270">1270. 2023</date>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">NIST Special</note>
	<note type="raw_reference">Schwartz, R. et al. Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. NIST Special Publication 1270. National Institute of Standards of Technology. https://doi.org/10.6028/NIST.SP.1270 (2023).</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<ptr target="http://www.imdrf.org/docs/imdrf/final/technical/imdrf-tech-140918-samd-framework-risk-categorization-141013.pdf" />
		<title level="m" xml:id="_sJW92Z3">International Medical Device Regulators Forum -Software as a Medical Device</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>SaMD) Working Group</orgName>
		</respStmt>
	</monogr>
	<note>Software as a Medical Device Possible Framework for Risk Categorization and Corresponding Considerations</note>
	<note type="raw_reference">International Medical Device Regulators Forum -Software as a Medical Device (SaMD) Working Group. &quot;Software as a Medical Device&quot;: Possible Framework for Risk Categorization and Corresponding Considerations. http://www.imdrf.org/docs/ imdrf/final/technical/imdrf-tech-140918-samd-framework-risk-categorization- 141013.pdf (2014).</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D A</forename><surname>Cdrh</surname></persName>
		</author>
		<ptr target="https://www.fda.gov/medical-devices/software-medical-device-samd/good-machine-learning-practice-medical-device-development-guiding-principles" />
		<title level="m" xml:id="_UkVRqjm">Good Machine Learning Practice for Medical Device Development: Guiding Principles (GMLP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>U. S. Food &amp; Drug Administration</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">U. S. Food &amp; Drug Administration (F. D. A.) CDRH. Good Machine Learning Practice for Medical Device Development: Guiding Principles (GMLP). https://www.fda.gov/ medical-devices/software-medical-device-samd/good-machine-learning- practice-medical-device-development-guiding-principles (2021).</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_4hrvRC5">Racial and ethnic disparities in the quality of health care</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fiscella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Sanders</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-publhealth-032315-021439</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_V96zxYC">Annu. Rev. Public Health</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="375" to="394" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fiscella, K. &amp; Sanders, M. R. Racial and ethnic disparities in the quality of health care. Annu. Rev. Public Health 37, 375-94 (2016).</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_6W6JaUj">Patient apprehensions about the use of artificial intelligence in healthcare</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-021-00509-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KKYPX9X">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">140</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Richardson, J. P. et al. Patient apprehensions about the use of artificial intelli- gence in healthcare. NPJ Digit Med. 4, 140 (2021).</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_UKfajZx">Skin of color: biology, structure, function, and implications for dermatologic disease</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_c5VbSpS">J. Am. Acad. Dermatol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="41" to="62" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Taylor, S. C. Skin of color: biology, structure, function, and implications for der- matologic disease. J. Am. Acad. Dermatol. 46, S41-62 (2002).</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<idno type="DOI">10.1109/etis64005.2025.10961721</idno>
		<ptr target="https://www.imdrf.org/sites/default/files/2022-05/IMDRF%20AIMD%20WG%20Final%20Document%20N67.pdf" />
		<title level="m" xml:id="_YhamWkB">International Medical Device Regulators Forum (IMDRF) Artificial Intelligence Medical Devices (AIMD) Working Group. Machine Learning-enabled Medical Devices: Key Terms and Definitions</title>
		<imprint/>
	</monogr>
	<note type="raw_reference">International Medical Device Regulators Forum (IMDRF) Artificial Intelligence Medical Devices (AIMD) Working Group. Machine Learning-enabled Medical Devices: Key Terms and Definitions. https://www.imdrf.org/sites/default/files/2022- 05/IMDRF%20AIMD%20WG%20Final%20Document%20N67.pdf.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" xml:id="_eXrSKup">International Medical Device Regulators Forum. Software as a Medical Device (SAMD): Clinical Evaluation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Food &amp; Drug Administration (FDA</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">U.S. Food &amp; Drug Administration (FDA); International Medical Device Regulators Forum. Software as a Medical Device (SAMD): Clinical Evaluation (2016).</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<idno>ISO/IEC/IEEE 90003:</idno>
		<title level="m" xml:id="_5ZGrP45">2018 Software Engineering -Guidelines for the Application of ISO 9001:2015 to Computer Software</title>
		<imprint>
			<publisher>International Organization for Standardization</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">International Organization for Standardization (ISO). ISO/IEC/IEEE 90003:2018 Software Engineering -Guidelines for the Application of ISO 9001:2015 to Com- puter Software. (International Organization for Standardization, 2018).</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_z8f2mgD">Separate and unequal: clinics where minority and nonminority patients receive primary care</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Varkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_888HdBv">Arch. Intern. Med</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="page" from="243" to="250" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Varkey, A. B. et al. Separate and unequal: clinics where minority and nonminority patients receive primary care. Arch. Intern. Med. 169, 243-250 (2009).</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">)</forename><surname>Datacc</surname></persName>
		</author>
		<author>
			<persName><surname>Dhmcc</surname></persName>
		</author>
		<ptr target="https://datacc.dimesociety.org/development/" />
		<title level="m" xml:id="_T79dHJZ">DATAcc Inclusivity Toolkit for Digital Health Measurement Product Development</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">DATAcc) DHMCC. DATAcc Inclusivity Toolkit for Digital Health Measurement Pro- duct Development. https://datacc.dimesociety.org/development/ (2022).</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
