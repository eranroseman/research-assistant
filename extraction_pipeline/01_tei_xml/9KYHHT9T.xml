<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_DJdFp4u">A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emma</forename><surname>Beede</surname></persName>
							<email>embeede@google.com</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Google Health Google Health Google Health Palo Alto , CA Palo Alto , CA Singapore</note>
								<orgName type="institution">Google Health Google Health Google Health Palo Alto</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA CA</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elizabeth</forename><surname>Baylor</surname></persName>
							<email>ebaylor@google.com</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Google Health Google Health Google Health Palo Alto , CA Palo Alto , CA Singapore</note>
								<orgName type="institution">Google Health Google Health Google Health Palo Alto</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA CA</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fred</forename><surname>Hersch</surname></persName>
							<email>fredhersch@google.com</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">Google Health Google Health Google Health Palo Alto , CA Palo Alto , CA Singapore</note>
								<orgName type="institution">Google Health Google Health Google Health Palo Alto</orgName>
								<address>
									<settlement>Palo Alto</settlement>
									<region>CA CA</region>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Iurchenko</surname></persName>
							<email>annaiu@google.com</email>
							<affiliation key="aff1">
								<note type="raw_affiliation">Google Health Google Health Rajavithi Hospital Palo Alto , CA Palo Alto , CA Bangkok , Thailand</note>
								<orgName type="institution">Google Health Google Health Rajavithi Hospital Palo Alto</orgName>
								<address>
									<settlement>Palo Alto Bangkok</settlement>
									<region>CA CA</region>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
							<email>lwilcox@google.com</email>
							<affiliation key="aff1">
								<note type="raw_affiliation">Google Health Google Health Rajavithi Hospital Palo Alto , CA Palo Alto , CA Bangkok , Thailand</note>
								<orgName type="institution">Google Health Google Health Rajavithi Hospital Palo Alto</orgName>
								<address>
									<settlement>Palo Alto Bangkok</settlement>
									<region>CA CA</region>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paisan</forename><surname>Ruamviboonsuk</surname></persName>
							<email>paisan.trs@gmail.com</email>
							<affiliation key="aff1">
								<note type="raw_affiliation">Google Health Google Health Rajavithi Hospital Palo Alto , CA Palo Alto , CA Bangkok , Thailand</note>
								<orgName type="institution">Google Health Google Health Rajavithi Hospital Palo Alto</orgName>
								<address>
									<settlement>Palo Alto Bangkok</settlement>
									<region>CA CA</region>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Vardoulakis</surname></persName>
							<email>lauravar@google.com</email>
							<affiliation key="aff2">
								<note type="raw_affiliation">Google Health Palo Alto , CA</note>
								<orgName type="institution">Google Health Palo Alto</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_y7pf3RU">A Human-Centered Evaluation of a Deep Learning System Deployed in Clinics for the Detection of Diabetic Retinopathy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A6B59434BAB025FC03F9071D16219800</idno>
					<idno type="DOI">10.1145/3313831.3376718</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T07:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_HQZaMkn">human-centered AI</term>
					<term xml:id="_c3kseUS">health</term>
					<term xml:id="_GHjFWNr">deep learning</term>
					<term xml:id="_EwYybsQ">diabetes CCS Concepts</term>
					<term xml:id="_3j9JF4E">Human-centered computing â†’ Empirical studies in HCI;</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_qQVnecb"><p xml:id="_434e8aW"><s xml:id="_WDChEXH">Deep learning algorithms promise to improve clinician workflows and patient outcomes.</s><s xml:id="_5MPFuWC">However, these gains have yet to be fully demonstrated in real world clinical settings.</s><s xml:id="_48VY6KV">In this paper, we describe a human-centered study of a deep learning system used in clinics for the detection of diabetic eye disease.</s><s xml:id="_epb4uYm">From interviews and observation across eleven clinics in Thailand, we characterize current eye-screening workflows, user expectations for an AI-assisted screening process, and post-deployment experiences.</s><s xml:id="_4WDXNEj">Our findings indicate that several socio-environmental factors impact model performance, nursing workflows, and the patient experience.</s><s xml:id="_UNQcdqG">We draw on these findings to reflect on the value of conducting humancentered evaluative research alongside prospective evaluations of model accuracy.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wPCVHsv">INTRODUCTION</head><p xml:id="_JrGJyCg"><s xml:id="_9XfXKrg">Diabetes is a growing problem around the world, including in Southeast Asia <ref type="bibr" target="#b39">[39]</ref>.</s><s xml:id="_prGhnqP">As of 2016, 9.6% of Thailand's population was living with diabetes, comparable to 9.1% of the population in the United States <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b40">40]</ref>.</s><s xml:id="_SdSsuFK">With diabetes comes complications, including diabetic retinopathy (DR), a condition caused by chronically high blood sugar that damages blood vessels in the retina, the thin layer at the back of the eye responsible for sensing light and sending signals to the brain.</s><s xml:id="_hEjpZ4E">These blood vessels can leak or hemorrhage, causing vision distortion or loss.</s><s xml:id="_4Qh3cA3">DR is one of the leading causes of vision impairment in the world <ref type="bibr" target="#b29">[29]</ref>, and causes 5% of cases of blindness worldwide, excluding refractive errors <ref type="bibr" target="#b38">[38]</ref>.</s><s xml:id="_pabh3fS">In Thailand, 34% of patients with diabetes have low vision or blindness in either eye <ref type="bibr" target="#b22">[23]</ref>.</s></p><p xml:id="_JuJf4gR"><s xml:id="_CrBp7Nx">In early stages of DR, a patient often has no symptoms, making it important for people living with diabetes to be screened regularly, as this is the stage in which damage can be reversedprogression of DR can be stopped or significantly reduced by blood sugar control.</s><s xml:id="_hKdAcq5">Early detection is key to initiate timely treatment and mitigate the risk of blindness.</s></p><p xml:id="_DKKCkBE"><s xml:id="_byUavCf">Since 2013, the Ministry of Health in Thailand has set a goal to screen 60% of its diabetic population for diabetic retinopathy (DR).</s><s xml:id="_bZ2FMtU">However, reaching this goal is a challenge due to a shortage of clinical specialists.</s><s xml:id="_gzJaXNa">In Thailand, there are 1500 ophthalmologists, including 200 retinal specialists, who provide ophthalmic care to approximately 4.5 million patients with diabetes <ref type="bibr" target="#b22">[23]</ref>-a ratio of about 1:3000, about double of what it is in the United States <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>.</s><s xml:id="_zbkM47N">The shortage of doctors limits the ability to screen patients and also creates a treatment backlog for those found to have DR.</s><s xml:id="_ey4rvsX">As a result, nurses CHI 2020 Paper CHI 2020, April 25-30, 2020, Honolulu, HI, USA conduct DR screenings when patients come in for diabetes check-ups, by taking photos of the retina and sending them to an ophthalmologist for review.</s></p><p xml:id="_TVRuFzE"><s xml:id="_KMj8S6a">Our team has developed a deep learning algorithm that can provide an assessment of diabetic retinopathy, bypassing the need to wait weeks for an ophthalmologist to review the retinal images <ref type="bibr" target="#b19">[20]</ref>.</s><s xml:id="_Q6YhZJK">This algorithm has been shown to have specialistlevel accuracy (&gt;90% sensitivity and specificity) for the detection of referable cases of diabetic retinopathy.</s><s xml:id="_5hpr4PF">Through a large-scale, retrospective study comparing the algorithm to human graders, the deep learning algorithm shows significant reduction in the false negative rate (by 23%) at the cost of slightly higher false positive rates (2%) <ref type="bibr" target="#b33">[33]</ref>.</s></p><p xml:id="_WkHsxxE"><s xml:id="_DSme7hF">Currently, there are no requirements for AI systems to be evaluated through observational clinical studies, nor is it common practice.</s><s xml:id="_gAct9Jw">This is a problem because the success of a deep learning model does not rest solely on its accuracy, but also on its ability to improve patient care <ref type="bibr" target="#b35">[35]</ref>.</s></p><p xml:id="_BJcnPvW"><s xml:id="_HtaEa4h">Prospective studies that involve evaluations of deep learning models within a clinical environment are beginning to emerge <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_Ka8zSWr">These studies are designed to provide additional evidence of model accuracy (sensitivity and specificity), but are not sufficient to evaluate true clinical effectiveness -that is, impact on patient care <ref type="bibr" target="#b25">[25]</ref>, nor do they explore socio-environmental factors that impact model performance in the wild.</s><s xml:id="_deczxEg">Furthermore, as Yang and colleagues note, when HCI researchers attempt to study AI systems in a hospital or clinic, they are often prevented from fully embedding into clinical workflows and from evaluating systems using authentic patient data <ref type="bibr" target="#b42">[42]</ref>.</s><s xml:id="_YBxjZgA">This paper contributes the first human-centered observational study of a deep learning system deployed directly in clinical care with patients.</s><s xml:id="_YHR86J6">Through field observations and interviews at eleven clinics across Thailand, we explored the expectations and realities that nurses encounter in bringing a deep learning model into their clinical practices.</s><s xml:id="_TpbEZaM">First, we outline typical eye-screening workflows and challenges that nurses experience when screening hundreds of patients.</s><s xml:id="_mv3ynvP">Then, we explore the expectations nurses have for an AI-assisted eye screening process.</s><s xml:id="_2Z6HHpG">Next, we present a human-centered, observational study of the deep learning system used in clinical care, examining nurses' experiences with the system, and the socioenvironmental factors that impacted system performance.</s><s xml:id="_dhJv8jF">Finally, we conclude with a discussion around applications of HCI methods to the evaluation of deep learning algorithms in clinical environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_eP7ugMv">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4VkxhYd">Design research in clinical settings</head><p xml:id="_2UeJRRy"><s xml:id="_YzExdHF">Researchers have shown that paper prototyping and wizardof-oz methods can be used with real patient data to study contextual fit of prototypes in clinical settings <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b36">36]</ref>.</s><s xml:id="_bNxwBxu">Additional human-centered evaluative research has occurred with more robust systems deployed within a hospital, and that operate on patient data during the deployment period for the purposes of conducting small-scale pilot studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b21">22]</ref>.</s><s xml:id="_UD37qaN">However, the artifacts and systems used in these studies did not incorporate deep learning models, which add challenges to traditional formative design approaches.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FjfwSyM">Challenges evaluating AI in clinical settings</head><p xml:id="_55kXwCK"><s xml:id="_FACdYmx">Past work on systems using artificial intelligence, such as computer-aided detection (CAD) <ref type="bibr" target="#b9">[10]</ref> or Decision Support Tools (DSTs) <ref type="bibr" target="#b3">[4]</ref> has highlighted several obstacles in going from research and development environments to hospital or clinical settings.</s><s xml:id="_vPvaCpT">These obstacles include frequent lack of utility to clinicians and logistical hurdles that slow or block deployment <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">27]</ref>.</s><s xml:id="_ZUZxKtJ">Even systems with widespread adoption, such as CAD in Mammography, have been shown to require a radiologist to do more work, not less, <ref type="bibr" target="#b17">[18]</ref> and generally do not improve a radiologist's diagnostic accuracy <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">26]</ref>.</s></p><p xml:id="_eut4qFn"><s xml:id="_Veawqxg">Human-centered evaluation of interactive, deep learning systems-within clinical environments-is an open area of research <ref type="bibr" target="#b30">[30]</ref>.</s><s xml:id="_vGVVKmh">Cai and colleagues created interactive techniques that can lead to increased diagnostic utility and increased user trust in the predictions of a deep learning system, used by pathologists in a lab setting <ref type="bibr" target="#b7">[8]</ref>.</s><s xml:id="_eZxDCq4">In subsequent work, <ref type="bibr">Cai et al.</ref> examined what information pathologists (in a lab setting) found to be important when being introduced to AI assistants, before integrating these assistants into routine prostate cancer screening practice <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_hfBfK4F">Clinicians in their study emphasized a need to gain an initial "global" impression of a model (e.g. its limitations, medical point-of-view, idiosyncrasies, and overall objective).</s><s xml:id="_RG5fnnH">While these studies bring us closer to understanding the needs of clinicians as they interact with deep-learningbased systems, they do not account for the highly situated nature of activities in clinical environments <ref type="bibr" target="#b15">[16]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_EWy5yp5">Observational studies of the introduction of diagnostic and information systems</head><p xml:id="_utsFsjY"><s xml:id="_rCpVqYJ">A large body of ethnographic and ethnomethodologicallyinformed studies have focused on the effects of workflow when new diagnostic and information systems are introduced into clinical environments.</s><s xml:id="_yAkFMXY">In the medical imaging domain, researchers have long known that clinical trials fail to grasp the situated social and collaborative dimensions of medical work <ref type="bibr" target="#b20">[21]</ref>.</s><s xml:id="_5kgXESy">Alberdi et al.'s ethnographic study of the introduction of a CAD tool in breast screening found a range of effects on experts' decisions, both beneficial and detrimental <ref type="bibr" target="#b1">[2]</ref>.</s><s xml:id="_g5NetPH">Yang, et al. conducted a design and field assessment of a DST for cardiologists, and found that the more an AI system is unobtrusively embedded into current workflows, the more likely a clinician will embrace such a system <ref type="bibr" target="#b42">[42]</ref>.</s><s xml:id="_8jY6B8N">Unfortunately, Yang and colleagues faced challenges evaluating their predictive model using authentic patient data, and were only able to evaluate their tool using one synthetic patient case.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_N4prwCd">Jirotka et al. studied work practices in digital mammography</head><p xml:id="_CUWdauv"><s xml:id="_WztadmK">screening, finding that system design affected both trust in the screening system and humans' trust in each other, when their work was mediated by the system <ref type="bibr" target="#b24">[24]</ref>.</s><s xml:id="_EhXJUns">Their study found that clinicians developed their own tolerance and workarounds for system policies, in order to trust the system results.</s><s xml:id="_CNEzKkh">Their study highlighted the importance of understanding differences in data acquisition and analysis practices that tended to emerge in local contexts (e.g.</s><s xml:id="_Xvje9kV">different characteristic appearances of images if taken at one site or another)-differences that clinicians naturally acclimated to and incorporated into their human decision-making processes <ref type="bibr" target="#b24">[24]</ref>.</s></p><p xml:id="_M4C7mxJ"><s xml:id="_WgEcPj7">In a recent study on computerized medication order entry, DSTs improved pharmacists' workflow but increased communication load and impaired aspects of human decision-making <ref type="bibr" target="#b32">[32]</ref>.</s><s xml:id="_qHBXpEY">Reddy and colleagues examined the effects that a new wireless notification system had on surgical ICU clinicians, finding a number of disruptions to existing work practices and information flows.</s><s xml:id="_xYNhyCw">In their study, the new system prevented residents and fellows from managing critical events before they were relayed to the attending physician, disrupting their ability to manage problems that did not require escalation <ref type="bibr" target="#b34">[34]</ref>.</s></p><p xml:id="_pS2RnHE"><s xml:id="_up4SeDK">Our research builds upon the long tradition of studies that examine environmental and contextual factors surrounding systems designed for a clinical environment.</s><s xml:id="_qpf7GsV">To our knowledge, this paper presents the first human-centered evaluation of a production-level deep learning model for diagnostic prediction, being used across several clinics for patient care.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_xHrBBeK">DR SCREENING IN THAILAND</head><p xml:id="_GPJvFVs"><s xml:id="_HQAKkWx">In 2013, the Thailand Ministry of Public Health set up a national screening program where patients can be screened for diabetic retinopathy when visiting their local clinic for a diabetes check-up on designated eye screening days (available year round in some clinics, or during a period of two months for clinics that share the equipment).</s><s xml:id="_t9dfxVt">The Ministry of Public Health set an initial target of screening 60% of diabetic patients in each region.</s><s xml:id="_KnWNBpp">However, even with ample opportunity to receive an eye screen, Ministry data indicates that less than 50% of the diabetic patients are screened annually since the inception of the program.</s></p><p xml:id="_TuzpQ2Z"><s xml:id="_n8wF2YX">During the eye screening portion of a diabetes check-up, nurses are tasked with taking photos of a patient's retina, called a fundus photo (Figure <ref type="figure" target="#fig_0">1</ref>).</s><s xml:id="_cna6UYW">These images are sent, either by email or on a compact disc in postal mail, to an ophthalmologist and are often reviewed weeks to months after a patient's fundus photo was taken.</s><s xml:id="_NpP2UbH">However, nurses often perform initial grading themselves, checking for apparent abnormalities that clearly warrant a referral.</s></p><p xml:id="_K2kw7jx"><s xml:id="_CSQnv4d">When received, the ophthalmologist evaluates the images for the presence and severity of DR.</s><s xml:id="_gMsVxH8">DR generally has 4 levels of severity: Mild, Moderate, Severe Non-Proliferative, and Proliferative <ref type="bibr" target="#b28">[28]</ref>.</s><s xml:id="_nSywDBF">In addition, vision can be threatened by the development of Diabetic Macular Edema (DME), an accumulation of fluid or lipid in the macula due to leaking blood vessels that can occur at any stage of diabetic retinopathy.</s><s xml:id="_CRpsqz6">Depending on the severity of DR, presence of DME, and the visual acuity of the patient when screened, patients may be referred to an ophthalmologist or told to come back for more frequent screening exams.</s><s xml:id="_F5URzSK">After results are available -up to 10 weeks later -patients with no DR are advised to come back for screening at approximately one year; patients with Mild DR are advised to come back at approximately six months, and patients with moderate, severe, or proliferative DR, as well as DME, get referred to an ophthalmologist for a comprehensive exam and possible treatment.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_yJ3s4Rw">AI-ASSISTED EYE SCREENING</head><p xml:id="_Gz6aFmK"><s xml:id="_gnUfnhg">Using a system employing a deep learning algorithm to assess DR could reduce the requirement for nurses to perform grading in the moment, and eliminate the need for nurses to send fundus images to an ophthalmologist for review (Figure <ref type="figure" target="#fig_1">2</ref>).</s><s xml:id="_4d8XxqV">Removing this bottleneck could provide patients with immediate results, give nurses the ability to make a referral recommendation in the moment, facilitate faster treatment of patients, and allow more cases to be screened and reviewed.</s><s xml:id="_cj5agmv">Furthermore, it could allow nurses to spend additional time with patients on diabetes management and education, which is what ultimately leads to improved patient outcomes <ref type="bibr" target="#b13">[14]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6GeuPTd">Context of use</head><p xml:id="_xKfWRMT"><s xml:id="_pJwDNYs">In partnership with Rajavithi Hospital under the Ministry of Public Health, the deep learning system is being deployed in multiple clinics throughout Thailand, in a large-scale prospective study with 7600 patients.</s><s xml:id="_VugYUjB">The purpose of the prospective study is to evaluate the feasibility and performance of the algorithm in a real-world clinical setting.</s><s xml:id="_YgCtWJQ">The research presented in this paper was conducted in parallel with this prospective study, across two regions in Thailand: Pathum Thani and Chang Mai, in order to evaluate the socio-environmental factors that influence the algorithm's success, and how use of the system affects nursing workflows and the patient experience.</s></p><p xml:id="_ES5etvH"><s xml:id="_fdhhZnB">As part of the prospective study, the deep learning system was initially deployed in three clinics within the province of Pathum Thani: Klong Luang, Nongsue, and Lamlukka.</s><s xml:id="_SDGcyGU">The three initial sites screened patients from December 2018 to May 2019, at which point their eye screening season for the year concluded, to be resumed in October 2019.</s><s xml:id="_YAQt3gQ">Each clinic offered screening between two and eight days a month, and on each screening day, would see between 30-200 patients.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Xg3q6tN">Prospective Study Protocol</head><p xml:id="_gHhj4xs"><s xml:id="_85ESKny">On a designated screening day, patients due for an eye exam were identified as they registered at the clinic and provided information about the study.</s><s xml:id="_79cgREk">Prior to their exam, all patients were called by a nurse or nursing assistant and assessed for study eligibility, based on age and comorbidities.</s><s xml:id="_HW4ws5A">If eligible, Paper 589 the nurse invited them to participate in the study, explained how their data would be used, reviewed the written consent form, and answered any questions.</s><s xml:id="_JXKnwGh">If the patient consented to participate, a pre-generated case identifier was assigned.</s><s xml:id="_Cuq6mE5">The patient's name, date of birth, and case identifier were entered into a paper register maintained by the local clinic staff, should any future follow-up be needed.</s><s xml:id="_gwMTj7X">Patients that did not sign the consent continued with their exam as per the typical local process, having their images sent to an ophthalmologist to be graded without use of the deep learning system.</s></p><p xml:id="_zTCanFQ"><s xml:id="_XMEGFEe">Once a patient was called for their eye exam, the camera operator verified consent, took photos of each eye using the clinic's current fundus camera, and uploaded them to the deep learning system, using the case number as the sole identifier.</s><s xml:id="_5pvX8zC">The images were sent to the algorithm in the cloud, and an assessment of the presence and severity of DR, as well as the presence or absence of Diabetic Macular Edema (DME) was returned in real-time, including a recommendation for whether or not the patient should be referred to an ophthalmologist (Figure <ref type="figure" target="#fig_2">3</ref>).</s></p><p xml:id="_zFa9uVa"><s xml:id="_js6E7ad">If the system recommended referral (due to either DR severity or an ungradable image), the nurse (often the camera operator) gave the patient two referral notes.</s><s xml:id="_5hW6z2q">The first was the standard referral letter given by the doctor and required for the appointment at the referral center.</s><s xml:id="_b9y54kk">The second was provided to study participants to allow the referral centers to easily track referral adherence.</s><s xml:id="_uuUWCJP">The patient was to take both notes to the specialist.</s></p><p xml:id="_trpQf3x"><s xml:id="_yGgfSVF">Each week, images and corresponding predictions were reviewed by a member of the study staff, an ophthalmologist, referred to as an overreader, to ensure the system had not missed any patients that needed to be referred.</s><s xml:id="_S6FmpxV">If the overreader determined that a patient missed by the system should be seen by a specialist, a nurse followed up with the patient and issued a referral.</s><s xml:id="_6fkRQZd">If the system referred the patient but it should not have, no action was taken.</s></p><p xml:id="_AQyFRvH"><s xml:id="_wmUpSKF">At the completion of the prospective study, all collected images will go through an adjudication panel to determine the deep learning system's accuracy.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RBrmX5A">HUMAN-CENTERED CLINICAL FIELD STUDY</head><p xml:id="_EzCR68m"><s xml:id="_ak7zDXc">Our work complements the prospective study, by adding an additional focus on the human aspects to deploying and using a deep learning system within a clinical environment.</s><s xml:id="_WNNxKTF">Through a multi-round study, both before and after deploying the deep learning system, we explore nurses' expectations of the sys- tem, clarity of the patient consent process, experiences of the operation of the technology, and shifts in workflow to accommodate the new system.</s><s xml:id="_nH3Ebmc">This human-centered evaluation is the primary focus of this paper.</s></p><p xml:id="_4HW6Vrx"><s xml:id="_hx4hNPQ">We conducted fieldwork at 11 clinics within the provinces of Pathum Thani and Chiang Mai during November 2018, April 2019, and August 2019.</s><s xml:id="_4nNqnFG">We timed our visits to observe and understand the eye screening process across multiple sites (five clinics within Pathum Thani and six within Chiang Mai) prior to the deployment of the deep learning system.</s><s xml:id="_sfgxpFD">After the system was deployed at three clinics in Pathum Thani, and used for patient screenings for at least one month, we returned to the deployment sites and conducted observations and interviews with all primary users of the system.</s><s xml:id="_4sTxqU4">We also held weekly feedback phone calls with the local study coordinator, who was in close contact with the nurse users on a day-to-day basis.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qtG4ZcY">Data Collection</head><p xml:id="_wyxyKu3"><s xml:id="_8CRGgek">Pre-Deployment Fieldwork Before any system deployments, we conducted 26 hours of observation (adopting a non-member role <ref type="bibr" target="#b12">[13]</ref>) across 11 clinics.</s><s xml:id="_GgtYzV2">We observed the eye-screening process of more than 100 patients, along with general clinical activity.</s><s xml:id="_xtMsPdT">Additionally, we conducted 15 hours of interviews with the 13 nurses who lead eye screening at eight clinics.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sPgGKc5">Post-Deployment Fieldwork</head><p xml:id="_cFe8SpA"><s xml:id="_dm5uE6k">After deployment of the deep learning system at three clinics in Pathum Thani, we spent 12 hours observing the eye screening process of about 50 patients, and conducted seven hours of interviews among five nurses and one camera technician (all primary users of the system).</s></p><p xml:id="_HRpeJr6"><s xml:id="_sPupEdD">In all interviews (pre-and post-deployment), one researcher led the discussion with the aid of a Thai interpreter who translated the researcher's questions and the participant's responses.</s><s xml:id="_jbsVPhd">An additional researcher acted as a note-taker during the interviews and wrote down the interpreter's translation of the participant's responses.</s><s xml:id="_pEEcDSW">Interviews were approximately 90 minutes long and took place in the nurse's office.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FGtktuG">Data Analysis</head><p xml:id="_duJaFE2"><s xml:id="_KGxx2qJ">Our analysis, for both pre-and post-deployment studies, included observational notes, interview recordings, and system logs, including model predictions and usage data from 1838 image uploads.</s><s xml:id="_KDqdpPs">To analyze our observations and interviews, we created affinity notes at the end of each day, which included 1-2 clinic visits.</s><s xml:id="_69SZ2ce">At the conclusion of our clinic visits for a given trip (each lasting about a week), the site researchers gathered and performed an affinity mapping of the interview and observation data <ref type="bibr" target="#b4">[5]</ref>.</s><s xml:id="_xS9PzgC">Themes that emerged included: gradability, referrals, design implications, satisfaction, consent considerations, eye-screening workflows, and patient education.</s></p><p xml:id="_SN5sgA4"><s xml:id="_NEpRAWs">Upon returning from each trip, recordings of participants' responses (interpreter quotes) were machine-transcribed and used as a backup to written verbatim notes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_C9vyd5W">Participants</head><p xml:id="_TpM4ByY"><s xml:id="_JBcQzPu">Several roles make up the clinician team in charge of eye screening at the clinic.</s><s xml:id="_GebCNVp">During our visits, we focused on primary users of the system: non-communicable disease (NCD) nurses, who lead patient care for general diabetes cases and may also perform eye screening; ophthalmic nurses, who have completed a four-month specialty course to learn how to capture and assess fundus photos and discuss eye diseases with patients, including DR; and camera technicians, who manage the process of taking fundus photos before passing them to a nurse for initial assessment and patient consultation.</s></p><p xml:id="_fSEvabM"><s xml:id="_NgrQBvZ">All nurses were female, and ranged in nursing experience from 2 to 30 years (mean=17).</s><s xml:id="_JZTAvFC">Eight of the nurses had completed a four-month course in ophthalmic nursing, which they considered to be their specialty, with an average of five years experience in the specialty.</s><s xml:id="_P32KGCG">The remaining nurses considered themselves non-communicable disease nurses and had taken abbreviated 2-10 day courses in screening patients for DR.</s><s xml:id="_NXNbMc9">The camera technician was also female and had a tenure of three months taking fundus photos (Table <ref type="table" target="#tab_0">1</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_r7ajnMu">FINDINGS</head><p xml:id="_ttnsvFq"><s xml:id="_sdJ2HM3">We begin by providing an overview of the eye screening workflows and processes that we observed, prior to deployment of the deep learning system.</s><s xml:id="_kArmuFb">We describe the different workflows and processes for conducting eye screenings among clinical sites, and how nurses often needed to make initial DR referral determinations themselves, given the high-volume of patients being screened each day.</s><s xml:id="_YuYvHcU">We review nurses' expectations of an AI-tool, and their desire to have it improve their ability to read images.</s></p><p xml:id="_2vTUvYt"><s xml:id="_kmvPH96">Next, we describe results from our post-deployment research, evaluating the deep-learning system from a human-centered perspective.</s><s xml:id="_T4xFSSP">We found that several contextual factors affected model performance in the clinical setting, which in turn affected nurses' attitudes towards the system and their willingness to consent patients into the prospective study of the system.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gnHUvuf">Paper 589 CHI 2020 Paper</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_QPqE5aK">Pre-deployment Findings</head><p xml:id="_5e7FBWD"><s xml:id="_rVszKEt">As a typical eye-screening workflow, diabetic patients fasted overnight and presented to the clinic in the morning for blood tests.</s><s xml:id="_cqYYpan">Next, they went to a general waiting area to measure their blood pressure and weight (often self-measured with machines in the waiting area).</s><s xml:id="_e2NYdF3">If the patient was due for an eye screening, they waited (anywhere from five minutes to three hours, depending on their queue number) to have their fundus photo taken.</s><s xml:id="_HT2aa5d">The nurse would take an image of the patient's right and left eyes, sometimes re-taking an image if it was determined to be too dark-typically caused by a source of light in the room or from an age-related cataract, making the pupil too small or presenting an ocular media too cloudy for the camera to image the retina.</s><s xml:id="_5wy6Uxp">This process was repeated for all patients waiting in the queue, and occured each screening day at the clinic, usually once or twice a week.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_7JK8UxA">Observational findings</head><p xml:id="_xzRqpd9"><s xml:id="_9mq4Epq">We observed a high degree of variation in the eye-screening process across the 11 clinics in our study.</s><s xml:id="_mJu6XX8">The processes of capturing and grading images were consistent across clinics, but nurses had a large degree of autonomy on how they organized the screening workflow, and different resources were available at each clinic.</s><s xml:id="_8W97pCQ">For instance, one of the clinics in Chiang Mai was part of a larger hospital and had an ophthalmologist on staff, while the others relied exclusively on on-site nurses to read the image.</s><s xml:id="_SNM2NaG">Some clinics in Chiang Mai had the ability to use dilation drops at their discretion, but none in Pathum Thani did.</s><s xml:id="_eVGyqfv">At a clinic in Pathum Thani, eye screening was organized in an assembly-line fashion, with a camera technician taking the fundus photos and a nurse leading the consultation with the patient.</s><s xml:id="_DNJYnbr">In another Chiang Mai clinic, the nurse utilized part of the screening day for patient education, gathering all the patients together at the beginning of the day, to watch a YouTube video on a weight loss success story.</s><s xml:id="_tczC9Es">She also used her time with patients to coach them, and would pull patients with high blood sugar aside to "interview" them.</s><s xml:id="_PFpv34e">P11 told us, "I ask patients to reflect on their lifestyle, it's better than me telling them what to do, they don't listen to that."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4FjW7gF">Clinic Screening Conditions</head><p xml:id="_MDurW8p"><s xml:id="_qwQryay">The setting and locations where eye screenings took place were also highly varied across clinics.</s><s xml:id="_CGUUDzs">Only two clinics had a dedicated screening room that could be darkened to ensure patients' pupils were large enough to take a high-quality fundus photo.</s><s xml:id="_ytqJ5zh">In other clinics, eye screening took place in the nurses' offices, or where additional patients received a foot sensitivity screening or nutritional counseling.</s><s xml:id="_5ZGGEHA">As a result, the lights were rarely turned off in these settings while capturing a fundus photo, even when a fluorescent light was situated directly above the camera.</s><s xml:id="_SkrjH3D">We were interested to see how these real-world conditions would affect our model performance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_NXHMZ6F">Volume</head><p xml:id="_ffpw9TC"><s xml:id="_HPpsM96">Through our observations and interviews, we saw, first-hand, the challenges required to meet the national screening goal target.</s><s xml:id="_MQkvha7">Some clinics attempted to screen two hundred patients within five hours, allotting only ninety seconds to screen each patient, which often resulted in extended clinic hours or patients being rescheduled for another day (Figure <ref type="figure" target="#fig_4">4</ref>).</s><s xml:id="_fFrvFUk">[The screening process], it's okay, but that's if there aren't 300 people putting pressure on you.</s><s xml:id="_XxzEJs7">With 300, it's not okay, there's not enough time in that crowded hospital setting."</s><s xml:id="_jqJY2Cu">-P2</s></p><p xml:id="_Hg75Qfn"><s xml:id="_axc7TpH">Patients come in, around 100 at a time, and it's not just eye screening that needs to happen...There are many things to do.</s><s xml:id="_EmAvuPT">After the medical history, the patient goes to the waiting area, and if there are a lot of people I have to pitch in and help out everywhere in the clinic.</s><s xml:id="_PcmUeUf">That's a lot of work... Things would be better if I could just be focused."</s><s xml:id="_jRVcsK2">-P7</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3NrMvZw">Referral Determinations</head><p xml:id="_HWzsnhH"><s xml:id="_7ZUDEFE">All images were initially assessed by a nurse then sent to an ophthalmologist for review.</s><s xml:id="_eBUs4yH">The ability to assess fundus photos for DR varied from nurse to nurse.</s><s xml:id="_MefyYUq">While most nurses told us they felt comfortable assessing for the presence of DR, they didn't know how to determine the severity if present.</s><s xml:id="_JEWtHTe">P4 told us, "I know if it's not normal, but I don't know what to call it."</s><s xml:id="_qMA47Ed">To make the ultimate decision of whether a patient needs to be referred to an ophthalmologist for an exam and potentially for treatment, the nurse turned to the ophthalmologist or retinal specialist, who are most often remote.</s></p><p xml:id="_9D24aJE"><s xml:id="_fJ5543D">Images that appeared normal to the nurses were typically sent via email to the ophthalmologist in batches that include several weeks worth of images.</s><s xml:id="_nyHKF25">The ophthalmologist then determined whether or not the patient needs to be referred for an exam, and typically sent the results back to the nurses within 1-2 weeks.</s><s xml:id="_sMqfxMy">For images that seemed abnormal, some nurses sent the image to the ophthalmologist via instant messaging in hopes of getting a recommendation for the patient quickly.</s><s xml:id="_tMsWCGz">These recommendations usually took days, but were sometimes returned within hours.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_tapfrqT">Expectations for AI-assisted screening</head><p xml:id="_pvF6ZjU"><s xml:id="_YrPgpE8">During our pre-deployment interviews with nurses, we sought feedback on potential designs for the visual presentation of the DR and DME prediction.</s><s xml:id="_jsKaHwE">We found that fundus images were a deep part of the nurses' practice, and it became clear that the images need to be prominently displayed alongside the DR prediction.</s><s xml:id="_b6R9QVF">Displaying the fundus images would not only provide confidence to the nurse that the correct image</s></p><p xml:id="_Ay8ATCQ"><s xml:id="_rs2xE5E">Paper 589 CHI 2020 Paper CHI 2020, April 25-30, 2020, Honolulu, HI, USA</s></p><p xml:id="_YFRKxJf"><s xml:id="_E3bPkCf">was being used for the assessment, it also provided nurses with information they could use to convince patients to seek treatment.</s><s xml:id="_NukBFQ3">If a patient needed urgent treatment, but wasn't experiencing any symptoms, nurses wanted to be able to show the fundus images directly to the patient, and point out the area of concern.</s><s xml:id="_CT5jVkS">They were excited about a combination of images with the prediction as a way to aid in those conversations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_snCsaQQ">Potential benefits</head><p xml:id="_uXkbKDE"><s xml:id="_94TVCDk">Nurses foresaw two potential benefits of having an AI-assisted screening process.</s><s xml:id="_3aSa23s">The first was using the system as a learning opportunity-improving their ability to make accurate DR assessments themselves.</s><s xml:id="_PxeBMBa">In their typical practice, nurses enjoyed fundus photo reading as an educational experience and an opportunity to apply their training.</s></p><p xml:id="_rXD9sG3"><s xml:id="_SGpwxZs">I like to study and learn things-and this is where you learn, the way you become more knowledgeable.</s><s xml:id="_UCZscBE">-P4</s></p><p xml:id="_CBhT5UH"><s xml:id="_4cjwjSr">In the past, I got some things wrong.</s><s xml:id="_fqhnyUy">I made referrals when I shouldn't have.</s><s xml:id="_n4NtSjD">The doctor told me it was something other than DR, and I learned from this.</s><s xml:id="_BKQc9tS">Now my readings are better.</s><s xml:id="_U9hmRc8">-P6 I went to training to learn how to read images.</s><s xml:id="_hHsFAdw">I'm very interested in it.</s><s xml:id="_45Ps87x">I asked for images from the ophthalmologist-he sent them to me, I read them, and then I asked him to check my work.</s><s xml:id="_U5Ktngk">I got 850/1000 right... right now no one can replace me.</s><s xml:id="_7zVgjz5">-P11</s></p><p xml:id="_ckMceyh"><s xml:id="_dAFSD6w">The second benefit was the potential to use the deep learning system's results to prove their own readings to on-site doctors.</s></p><p xml:id="_88vgPjm"><s xml:id="_AAakHv8">Several nurses expressed frustration with their assessments being undervalued or dismissed by physicians, and they were excited about the potential to demonstrate their own expertise to more senior clinicians.</s><s xml:id="_uQFDDu7">As P7 explained, "They don't believe us."</s><s xml:id="_RgVfTTe">P11 stated, "It could confirm what we already know."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_th7BCK6">Potential challenges</head><p xml:id="_vaV8n2d"><s xml:id="_GqVSJhP">Nurses also anticipated challenges using the deep learning system within clinical care.</s><s xml:id="_nAQwfYZ">With patient volume already a burden, nurses were concerned that following the study protocol (including uploading images) would add to their workload and ability to screen all patients arriving each day.</s><s xml:id="_bhreMCy">Nurses were also concerned about the consequences for patients if the algorithm produced a false positive, including the additional travel burden to follow up on a referral, the cost of missing work associated with travel, and the emotional strain a positive result could place on them.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Tzp7yf8">Post-deployment findings</head><p xml:id="_JTAPT4m"><s xml:id="_xrz4Uce">With the addition of the deep learning system, patients generally followed the same journey through the clinic as normal, with the exception of now being able to receive an immediate determination of whether or not a referral is needed (Figure <ref type="figure" target="#fig_1">2</ref>).</s><s xml:id="_K5sr4kJ">In this section, we present findings that we were only able to uncover as a result of studying the deep learning system in the context of actual clinical care.</s><s xml:id="_DeJBBsJ">We discuss several challenges embedded in the intended journey of AI-assisted eye screening: consenting patients, factors influencing system performance and the clinician/patient experience, and resulting system workarounds.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_5KXdxWY">Consenting patients</head><p xml:id="_qYXsfDN"><s xml:id="_RPEq97d">Given that the deep learning system was deployed in an observational, prospective study, it was critical for nurses to obtain patient consent prior to using the system.</s><s xml:id="_bHUCGk3">The informed consent process was the first challenge we observed, and was made more complicated by the need to explain the deep learning system.</s></p><p xml:id="_j7fH4V2"><s xml:id="_9dvEspZ">To participate, patients provided both written and verbal consent before having their eyes photographed.</s><s xml:id="_S8SPJDz">As the system provides an immediate referral recommendation, nurses knew that they may need to convince a patient to visit a specialist, depending upon the results.</s><s xml:id="_zeh6mYC">This was a large change from their previous workflow, where results may not be available for up to 10 weeks, long after a patient has left the clinic.</s><s xml:id="_5KZHhCR">As a result of the prospective study protocol design, and potentially needing to make on-the-spot plans to visit the referral hospital, we observed nurses at clinics 4 and 5 dissuading patients from participating in the prospective study, for fear that it would cause unnecessary hardship.</s></p><p xml:id="_WAXawHc"><s xml:id="_6xWsZnx">At one clinic, we observed nurses mentioning to patients that if they received a positive DR result they will be referred to Pathum Thani Hospital (an hour drive away).</s><s xml:id="_Tv32qM7">Not all patients have cars, and transportation for them is uncertain.</s><s xml:id="_4VY4yd2">While patients at all sites were given the same information via written consent forms, some nurses felt the need to "warn" patients that they would need to travel should a referral be given.</s><s xml:id="_ffVdHa9">Given the far distance and inconvenience of getting to Pathum Thani Hospital, 50% of patients at clinic 4 opted out of participating in the study, even though it was unlikely that they would be referred.</s></p><p xml:id="_NzPMSdH"><s xml:id="_dyjScxV">[Patients] are not concerned with accuracy, but how the experience will be-will it waste my time if I have to go to the hospital?</s><s xml:id="_MdFZNjf">I assure them they don't have to go to the hospital.</s><s xml:id="_wjurfhp">They ask, 'does it take more time?',</s><s xml:id="_JDSb2gZ">'Do I go somewhere else?' Some people aren't ready to go so won't join the research.</s><s xml:id="_JbcQpP9">40-50% don't join because they think they have to go to the hospital.</s><s xml:id="_FhnsKgV">-P6</s></p><p xml:id="_gRKu6vJ"><s xml:id="_VhBvKXR">Through observation and interviews, we found a tension between the ability to know the results immediately and risk the need to travel, versus receiving a delayed referral notification and risk not receiving prompt treatment.</s><s xml:id="_saEkB5p">Patients had to consider their means and desire to be potentially referred to a far-away hospital.</s><s xml:id="_Cyvbkh3">Nurses had to consider their willingness to follow the study protocol, their trust in the deep learning system's results, and whether or not they felt the system's referral recommendations would unnecessarily burden the patient.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sDwPW6B">Clinical Factors Influencing System Performance</head><p xml:id="_MyA44BG"><s xml:id="_RTvkMWm">We suspected, even using a high-quality dataset to train the deep learning model, that environmental and contextual factors would impact the system's performance in a clinical setting.</s></p><p xml:id="_c54vhHz"><s xml:id="_W54hBkZ">Through our field research, we indeed found this to be true and observed several factors that affected the system's performance, as well as the clinician and patient experience.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pWm3qeY">Gradability</head><p xml:id="_m8tHxWE"><s xml:id="_WZfHsbW">For an image to be gradable by a human or an algorithm, it needs to capture the retinal field clearly.</s><s xml:id="_dgpjhuP">To achieve a clear Paper 589 CHI 2020 Paper image, enough light from the camera needs to get through to the back of the eye.</s><s xml:id="_WmaTqYK">For that light to get through, a patient's pupil needs to be large, helped by either a dark environment, dilation drops, or both.</s></p><p xml:id="_s7HHWhk"><s xml:id="_DEksFQN">The deep learning system has stringent guidelines regarding the images it will assess.</s><s xml:id="_HJq4aeh">For patient safety reasons, it was designed to decrease the chance that it would make an incorrect assessment, and therefore only assesses the highest-quality images.</s><s xml:id="_C9pyQAf">If an image has a bit of blur or a dark area, for instance, the system will reject it, even if it could make a strong prediction.</s><s xml:id="_gc46CEy">Because the deep learning system cannot guarantee that it hasn't missed something, these images are deemed ungradable.</s></p><p xml:id="_4SAZT2w"><s xml:id="_RverwBH">After clinics 2, 4, and 5 all reported issues with gradability, we reviewed system logs to determine how many images were rejected by the algorithm.</s><s xml:id="_m3rf48r">Out of 1838 images that were put through the system (in the first six months of usage), 393 (21%) didn't meet the system's high standards for grading.</s><s xml:id="_62mNFRd">Through our observations and interviews, we found that lowquality images were caused by fundus photos being taken in a non-darkened environment, as observed in our pre-deployment findings, or from a camera that needed repair.</s><s xml:id="_KVpCHDn">In addition, clinics in Patham Thani were not using dilation drops on patients, which could have aided in capturing a quality image.</s></p><p xml:id="_pYqyBD6"><s xml:id="_Swag2ew">In the case of an ungradable image, the system notifies the nurse and recommends the patient be referred to an ophthalmologist, as part of the prospective study protocol.</s><s xml:id="_NRszMzM">This immediate gradability feedback is something that the nurses did not have before, and turned out to be frustrating as images they felt were human-readable were rejected by the system.</s><s xml:id="_MYxf5nQ">Because of this, nurses somewhat questioned the power of the deep learning system.</s><s xml:id="_c37QWAB">P6 said, "It gives guaranteed results, but it has some limitations.</s><s xml:id="_mfc7kH3">Some images are blurry, and I can still read it, but [the system] can't."</s><s xml:id="_gQKGPUR">P3 shared the same sentiment, "It's good but I think it's not as accurate.</s><s xml:id="_j7GFgME">If [the eye] is a little obscured, it can't grade it."</s><s xml:id="_nDXsjPU">The system's high standards for image quality is at odds with the consistency and quality of images that the nurses were routinely capturing under the constraints of the clinic, and this mismatch caused frustration and added work.</s></p><p xml:id="_xF5P3yG"><s xml:id="_qfkjv3w">This in-the-moment feedback caused the nurses to take more photos, in an attempt to achieve an image the system will grade.</s></p><p xml:id="_yYEwcDj"><s xml:id="_6NUSECQ">In doing this, they noticed that they can create a semi-complete image from two ungradable photos.</s><s xml:id="_4qzWdgq">If in one photo she was able to see the top half of the image field, the nurse would take a second photo where she could capture the bottom half from the same eye (Figure <ref type="figure" target="#fig_6">5</ref>).</s><s xml:id="_r8njDgA">She would try to assess for DR using the top half of one image and the bottom half of another.</s><s xml:id="_3sSBsV4">They expected the deep learning system to perform this workaround as well; however, it couldn't, because it requires one highquality image per eye, and cannot make assessments based on a composite from two images.</s><s xml:id="_yfjRppn">"I want to be able to upload a second image with the focus on the macula.</s><s xml:id="_zDaKh2A">I want to focus on one area and crop it so I don't keep getting an ungradable result," said P7.</s><s xml:id="_3Dp5TT2">With poor lighting conditions commonly causing low-quality images, and many patients waiting in the queue, these ungradable images frustrated both patients and nurses.</s><s xml:id="_W8q8c5T">We observed nurses spending 2-4 minutes with each patient, retrying taking the photos a second time if the first ones were unable to be assessed, but never retrying for a third due to the discomfort the camera's bright flash causes to patients.</s><s xml:id="_m8ErVqq">P6 expressed this concern, "I'll do two tries.</s><s xml:id="_sWYbqp9">The patients can't take more than that."</s><s xml:id="_sJtXfCK">In the event that a nurse cannot capture a gradable image, the patient is to be referred to a specialist (potentially unnecessarily).</s></p><p xml:id="_8sWfUhG"><s xml:id="_KM5kDfq">Once we understood the rate of images that could not be assessed by the deep learning system, we explored solutions to improve lighting conditions that will lead to higher-quality images, such as darkening the room as much as possible, using a cloth as a hood or veil to limit light further, and waiting 60 seconds between photographing the right and left eyes, allowing the pupils to re-adjust, as they constrict from the flash of the camera.</s><s xml:id="_SZ4E943">However, these solutions are often difficult to implement in practice.</s><s xml:id="_NxhbzUa">For instance, turning a light off is difficult when the eye screening takes place in the same room where another patient is discussing their results with a nurse, receiving a foot sensitivity screening, or receiving nutritional counseling.</s><s xml:id="_pDEY8ne">Waiting 60 seconds before imaging the second eye is nearly impossible when a nurse has 150 patients in the queue waiting to be screened.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kdakGU4">Internet speed and connectivity</head><p xml:id="_tCKZF8V"><s xml:id="_PR9xC5C">One key difference in the eye screening workflow before and after the implementation of the deep learning system is that images are now uploaded to the cloud to get an assessment while the patient waits for results.</s><s xml:id="_FjaQEUN">On a strong internet connection, these results appear within a few seconds.</s><s xml:id="_kFmwyM9">However, the clinics in our study often experienced slower and less reliable connections.</s><s xml:id="_rFPwwFn">This causes some images to take 60-90 seconds to upload, slowing down the screening queue and limiting the number of patients that can be screened in a day.</s><s xml:id="_8r65yMy">In one clinic, the internet went out for a period of two hours during eye screening, reducing the number of patients screened from 200 to only 100.</s></p><p xml:id="_pWWf336"><s xml:id="_hK7Jn9j">Patients like the instant results but the internet is slow and patients complain.</s><s xml:id="_ptgAT2s">They've been waiting here since 6 a.m. and for the first two hours we could only screen 10 patients.</s><s xml:id="_jFcnN8m">-P8</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jzaHT7G">Workarounds to the prospective study protocol</head><p xml:id="_c9fnc5j"><s xml:id="_HbKhHqs">To the nurses, ensuring quality care for patients was just as important as not overly-inconveniencing them.</s></p><p xml:id="_g3jZdQd"><s xml:id="_JMXHtGZ">In the case of Paper 589 CHI 2020 Paper CHI 2020, April 25-30, 2020, Honolulu, HI, USA an ungradable image, the prospective study protocol originally stated that the patient should be referred to Pathum Thani Hospital, a larger provincial hospital with an in-house ophthalmologist that can examine the patient for DR and treat if needed.</s><s xml:id="_dbpmbpB">However, it is unlikely that the patient has severe or proliferative DR.</s><s xml:id="_W67zEcW">Additionally, Pathum Thani Hospital is far away and inconvenient for many patients.</s><s xml:id="_vfJEJXN">Patients either need to take off work, or for older patients that cannot drive on their own, a child of theirs needs to take off work to drive them.</s><s xml:id="_KHgMmm7">We observed P8 urging a patient to go to the hospital when he was diagnosed with pterygium, another eye disease: P8: You should go to the hospital, there's pterygium in both eyes on the corneas.</s><s xml:id="_hhHqEdP">They will check for DR too.</s><s xml:id="_2XxpBCp">Patient: My child doesn't have time to take me.</s><s xml:id="_deMmFTK">P8: The problem will get bigger.</s><s xml:id="_YvAZhC3">If it covers your eyes, you won't be able to see.</s><s xml:id="_FAwEE57">If your child doesn't understand, have her call me.</s><s xml:id="_jVgdfYE">This nurse was willing to go above and beyond to ensure patients got the treatment they need.</s><s xml:id="_f3XsGc4">But because of the large inconvenience it would cause patients to go to Pathum Thani Hospital, nurses were generally hesitant to refer patients there as a result of ungradable images alone, given the low likelihood that they have severe or proliferative DR.</s><s xml:id="_4qZs2QP">Instead, we observed nurses working around the prospective study protocol; relying on previous workflows and criteria for determining whether or not to refer the patient to a specialist.</s><s xml:id="_TXABETU">I look at results and then determine the blood sugar result, the HbA1c.</s><s xml:id="_59EUXwe">[Sometimes] people go to the hospital and it's a waste of time.</s><s xml:id="_VJ7ktgb">You should always look at the blood result and if one eye is okay [and the other is ungradable], then I don't recommend it.</s><s xml:id="_DFEdt3A">But every time I ask if they want to go.</s><s xml:id="_9vRYnrz">-P6</s></p><p xml:id="_gqvGN6f"><s xml:id="_KWY64Sr">We found that nurses took this approach across the three sites: in the case where an image was ungradable by the system, but the patient had no history of diabetic retinopathy and their blood sugar was well-controlled at the time of the visit, the nurse would make her own judgement call to send the patient home without a referral.</s><s xml:id="_kWBGTe8">P8 described what she does in the case of an ungradable image at her clinic, "I look at their history.</s><s xml:id="_njY3AqT">If it was bad last year, I refer.</s><s xml:id="_7aAZKrs">If it's okay, I send [the photo] to the doctor."</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ENVnvNe">DISCUSSION</head><p xml:id="_ccpeyn4"><s xml:id="_uxt62VT">Through our field research before and after deployment of the deep learning system, we discovered several factors that influenced model use and performance.</s><s xml:id="_h22FVb7">Poor lighting conditions had always been a factor for nurses taking photos, but only through using the deep learning system did it present a real problem, leading to ungradable images and user frustration.</s><s xml:id="_VTa7xeq">Despite being designed to reduce the time needed for patients to receive care, deployment of the system occasionally caused unnecessary delays for patients.</s><s xml:id="_zxCxDs5">When network connectivity issues occurred, all patients present for a screening experienced delays or worse, rescheduled appointments.</s><s xml:id="_8nBaZ64">Finally, concerns for potential patient hardship (time, cost, and travel) as a result of on-the-spot referral recommendations from the system, led some nurses to discourage patient participation in the prospective study altogether.</s></p><p xml:id="_ne9BRJA"><s xml:id="_Msb5M8e">Of these factors, ungradability had the largest impact on model performance and the patient experience, and this finding generalizes past system evaluation within a prospective study.</s><s xml:id="_FK4B2fk">The findings illuminate the tension between designing a threshold for the quality of images that the system will use to make an assessment, and the quality of images that arise from an imperfect, resource-constrained environment.</s></p><p xml:id="_DHDAruk"><s xml:id="_nJYuxAx">System thresholds that impact accuracy and safety (e.g., sensitivity and specificity) will always be an important consideration when deploying a deep learning system within a clinical environment.</s><s xml:id="_BKeStAU">These thresholds are also something that need to be reevaluated as a system moves from prospective evaluation to widespread use.</s><s xml:id="_CN8mFfw">Appropriate thresholds during evaluation may be different from thresholds set once a system's accuracy is better understood.</s><s xml:id="_UQWCgD6">Researchers will always need to consider: what is the appropriate threshold to ensure accuracy and safety, and what is the risk of deferring a prediction based on imperfect data?</s></p><p xml:id="_Bw7eAmH"><s xml:id="_wtDrWV2">Through observations during a prospective study, we saw ungradable images increased wait times for patients at the clinic, and caused nurses to think twice before making referrals to ophthalmologists.</s><s xml:id="_HGkXT6z">As a result of conducting this research, we were able to inform an iteration of the study design: the study protocol now delays the patient referral for an ungradable image until an ophthalmologist, the overreader, has made a determination.</s><s xml:id="_bK6KEny">This will presumably lower the rate of referrals, as the human overreader is willing to make a calculated judgment on the likelihood of disease, even if the system is not.</s></p><p xml:id="_Qe5FqVK"><s xml:id="_5r3KYTZ">Our research highlights that end-users and their environment determine how a new system will be implemented; that implementation is of equal importance to the accuracy of the algorithm itself, and cannot always be controlled through careful planning.</s><s xml:id="_BGmqxXf">A recent longitudinal ethnography explored theoretical and empirical factors that lead to non-adoption of technology in healthcare <ref type="bibr" target="#b18">[19]</ref>.</s><s xml:id="_xSGqjrk">Complexity across factors (e.g., medical conditions treated, organizational structure) increased the likelihood of non-adoption.</s><s xml:id="_3aqZ9j2">When introducing new technologies, planners, policy makers, and technology designers did not account for the dynamic and emergent nature of issues arising in complex healthcare programs.</s><s xml:id="_Zsu3WGQ">The authors argue that attending to people-their motivations, values, professional identities, and the current norms and routines that shape their work-is vital when planning deployments <ref type="bibr" target="#b18">[19]</ref>.</s></p><p xml:id="_m9ZSyHe"><s xml:id="_6VutuEj">Our findings suggest that even when a deep learning system performs a relatively straightforward task (e.g., focuses on retinal images and does not cross into multiple domains, organizational implementation challenges, or policy challenges), socio-environmental factors are likely to impact system performance.</s><s xml:id="_a2zjUKv">Our research also suggests that many environmental factors that negatively impact model performance in the real world have the potential to be significantly reduced or eliminated by tactical means (in our case, through lighting adjustments and camera repairs).</s><s xml:id="_4zpXGDY">However, such adjustments could</s></p><p xml:id="_emeYh5n"><s xml:id="_6ukfeGu">Paper 589 CHI 2020 Paper CHI 2020, April 25-30, 2020, Honolulu, HI, USA</s></p><p xml:id="_g8mAauX"><s xml:id="_pd7RhmY">be costly and even infeasible in low-resource settings, making it even more important to deeply engage with contextual phenomena early on.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_aA9Jh3h">HCI Research on Deep Learning Systems</head><p xml:id="_qxvHbkA"><s xml:id="_WTVJCdX">Thus far, most HCI studies of clinical systems have focused on interaction and presentation techniques for clinical data, or observations of the introduction of diagnostic and information systems that are not deep-learning based.</s><s xml:id="_Bs4GftG">Logistically, researchers have faced hurdles conducting studies within clinical environments <ref type="bibr" target="#b42">[42]</ref>, and often are limited to lab-based studies where clinicians use retrospective patient data <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, or synthetic patient data in order to complete simulated tasks.</s></p><p xml:id="_GrvsG2j"><s xml:id="_JfjKxkc">With machine learning, and in particular, deep learning, showing great promise to advance the field of medicine and improve care, there is a need for the HCI community to develop approaches for designing and evaluating machine learning systems in clinical settings.</s></p><p xml:id="_H4zqCK3"><s xml:id="_J7wJTV6">In many ways, classic user-centered approaches still apply when conducting research in the age of deep learning.</s><s xml:id="_NGUVmS7">Formative work including clinical ethnography, observational studies, user interviews, and participatory design, have allowed researchers to build a foundational understanding of clinical problems and potential design solutions, prior to system development and evaluation.</s><s xml:id="_mamU39w">These methods are still critical to the success of deep learning-based systems, and can and should be conducted prior to, and in parallel with, system development.</s></p><p xml:id="_EWKgNzw"><s xml:id="_G65Sn4P">Similarly, paper-and digital-prototyping still hold as strong methods to help system designers understand user reactions to tools that make clinical predictions.</s></p><p xml:id="_xXQ7fkm"><s xml:id="_mSH5fqq">The problem occurs once researchers need to conduct studies using live clinical data.</s><s xml:id="_VNVjURg">Once a deep learning-based system has been built, evaluations of model accuracy (sensitivity and specificity) have historically used only retrospective datasets, which may or may not be ecologically valid.</s><s xml:id="_R3wVJgU">With observational, prospective studies becoming an emerging approach to validate real-world clinical accuracy of deep learning models <ref type="bibr" target="#b0">[1]</ref>, we argue that conducting HCI research alongside prospective evaluations has many advantages.</s><s xml:id="_fr3R2Mh">This approach enables researchers to evaluate using live data, with target clinicians, in a contextual environment.</s><s xml:id="_C5NYbBt">Furthermore, it provides opportunities to identify vital socio-environmental factors ahead of widespread deployment, such as issues that arise from system use or misuse, perceived utility, workflow integration, and experiential measures for both clinicians and patients.</s></p><p xml:id="_eKfEAtr"><s xml:id="_Gw4GpVX">As we observed, the design of the prospective study protocol itself needs careful consideration and is a ripe opportunity for participatory design <ref type="bibr" target="#b6">[7]</ref> and service design <ref type="bibr" target="#b16">[17]</ref>.</s><s xml:id="_AyuN7kn">The closer a protocol mirrors an ideal deployment post-evaluation, the more researchers will be able to understand the likelihood of future system use.</s></p><p xml:id="_aS4BxtM"><s xml:id="_YjJw6Nq">Practically speaking, prospective studies are a late-stage evaluation, meaning that the opportunity for multiple cycles of design iteration are significantly reduced.</s><s xml:id="_UHmHXWU">As such, teams developing deep learning models for clinical care should have a high level of confidence in a potential system and its deployment approach, prior to running a prospective study that evaluates model accuracy.</s><s xml:id="_kFnt9TU">For system developers, this means that formative research that provides a strong understanding of clinical users and their context is critically important to the success of such a system.</s><s xml:id="_M6Nh2Mv">By incorporating human-centered evaluations into deep learning model evaluations, and studying model performance on live data generated at the clinical site, we can reduce the risk that deep learning systems will fail in the wild, and increase the likelihood for meaningful improvements to patients and clinicians.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WhJNJEN">LIMITATIONS AND FUTURE WORK</head><p xml:id="_8eyDSFQ"><s xml:id="_99YUWCA">This research is not without limitations.</s><s xml:id="_ehAkazh">Although we evaluated a deep learning system in the wild, our study focused on primary users of the system: nurses and camera technicians.</s><s xml:id="_3NbCBj3">Further research is needed to understand how the system affects the patient experience; in particular, patients' trust of the result, and likelihood to act on the result.</s><s xml:id="_8gjvxTm">Additional research is also needed to understand how the system may alter the practices of ophthalmologists who evaluate patients that have received a prediction from the deep learning system.</s><s xml:id="_cCwEbDc">Lastly, as more systems are evaluated in clinical environments, an important area of future work includes the design of study protocols for conducting human-centered prospective studies and studies on end-to-end service design of AI-based clinical products.</s></p><p xml:id="_Gpu36eR"><s xml:id="_htyNEVt">Since this research, we have begun to hold participatory design workshops with nurses, potential camera operators, and retinal specialists (the doctor who would receive referred patients from the system) at future deployment sites.</s><s xml:id="_F9XAJYs">Clinicians are designing new workflows that involve the system and are proactively identifying potential barriers to implementation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_v2zHmFE">CONCLUSION</head><p xml:id="_qcWFUTD"><s xml:id="_Mw59YN9">We are in a critical time in healthcare and technology, with deep learning algorithms poised to advance the field of medicine and improve patient outcomes.</s><s xml:id="_yWqFqfA">In this paper, we describe a sociotechnical study of a deep learning system for the detection of diabetic eye disease, used with patients in clinical care.</s><s xml:id="_BbW33n2">Through observation and interviews with nurses and technicians in parallel with a prospective study to evaluate the deep learning system's accuracy, we discover several socio-environmental factors that impact model performance, nursing workflows, and patient experience.</s><s xml:id="_dvwBJTX">By conducting human-centered evaluative research prior to, and alongside, prospective evaluations of model accuracy, we were able to understand contextual needs of clinicians and patients prior to widespread deployment, and recommend system and environmental changes that would lead to an improved experience.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc><div><p xml:id="_5jJdR7N"><s xml:id="_Y8xTYBT">Figure 1.</s><s xml:id="_8UvFVHe">A nurse operates the fundus camera, taking images of a patient's retina.</s></p></div></figDesc><graphic coords="3,322.31,62.36,240.62,155.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc><div><p xml:id="_dHGTWnz"><s xml:id="_r7Bj49Q">Figure 2. Eye screening process before and after deployment of the deep learning system.</s></p></div></figDesc><graphic coords="4,65.98,62.36,486.14,87.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc><div><p xml:id="_Tagggfv"><s xml:id="_J4HUamS">Figure 3. Web application displaying the deep learning model's predictions for diabetic retinopathy and diabetic macular edema, along with the fundus photos.</s></p></div></figDesc><graphic coords="4,322.31,191.98,240.63,186.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><figDesc><div><p xml:id="_kPAQnhY"><s xml:id="_XDk2gsv">This study was approved by the Ethical Review Committee for Research in Human Subjects of Rajavithi Hospital and the Ethical Committees of hospitals or health centers from which retinal images of patients with diabetes were used.</s><s xml:id="_JgsMhAJ">Patients gave informed consent allowing their retinal images to be used in the prospective study evaluating the accuracy of the deep learning system: registered in the Thai Clinical Trials Reg-April 25-30, 2020, Honolulu, HI, USA istry, Registration Number TCTR20190902002.</s><s xml:id="_P6z7cGM">Clinicians gave informed consent for participating in the interviews and observations reported in this paper.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc><div><p xml:id="_P2WfpkQ"><s xml:id="_Sbs8kCW">Figure 4. Busy screening site at a clinic in Pathum Thani.</s></p></div></figDesc><graphic coords="6,322.31,62.36,240.64,137.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc><div><p xml:id="_5JEX46R"><s xml:id="_3srQqwt">Figure 5.</s><s xml:id="_bZxdhqw">A nurse attempts to form a composite image of one eye by taking two images of the same eye, with varied lighting.</s></p></div></figDesc><graphic coords="8,339.32,62.36,206.61,70.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc><div><p xml:id="_dY3QnDG"><s xml:id="_KxkR93Z">).</s></p></div></figDesc><table><row><cell>Nursing Screening</cell></row></table><note xml:id="_2tHk3qm"><p><s xml:id="_zDcKUYU">. Clinic and Interview participant details.</s><s xml:id="_skG8XyP">Provinces: Pathum Thani (PT) and Chiang Mai (CM).</s><s xml:id="_MMyT9yc">Rows in bold represent postdeployment participation.</s><s xml:id="_BhWjGWN">Clinics 9-11 involved pre-deployment observations only.</s><s xml:id="_aPxCyG3">Clinics 1-4 were visited, pre-deployment, in November 2018.</s><s xml:id="_aQu3Etu">In April 2019, clinics 2, 4, and 5 were visited post-deployment, and clinics 9-11 were visited pre-deployment.</s><s xml:id="_EDHaazQ">In August 2019, clinics 6-8 were visited pre-deployment.</s><s xml:id="_WudBHsv">Participant #'s are not included to protect their anonymity.</s></p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_CjtWwFc">ACKNOWLEDGMENTS</head><p xml:id="_zjE5bgQ"><s xml:id="_hT9Hz7q">We are especially thankful to the clinicians, staff, and patients at all participating clinics.</s><s xml:id="_eewAjvp">Thanks to <rs type="person">Kasumi Widner</rs>, <rs type="person">Richa Tiwari</rs>, <rs type="person">Nanlaphat Kuntee</rs>, and <rs type="person">Variya Nganthavee</rs> for their research support.</s><s xml:id="_CFbdqZ6">Thanks to <rs type="person">Sara Gabriele</rs>, <rs type="person">T Saensuksopa</rs>, <rs type="person">Rebecca Shapley</rs>, and <rs type="person">Brian Levinstein</rs> for their contributions to the system design.</s><s xml:id="_3nW3rbH">Thanks to <rs type="person">Tayyeba Ali</rs>, <rs type="person">Wing</rs> (<rs type="person">Eric) Li</rs>, <rs type="person">Ilana Traynis</rs>, <rs type="person">Jonathan Krause</rs>, <rs type="person">Rory Sayres</rs>, <rs type="person">Elin Pederson</rs>, <rs type="person">Greg Wolff</rs>, <rs type="person">Lily Peng</rs>, and <rs type="person">Greg Corrado</rs> for their helpful comments on this paper.</s></p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_AwWCskh"><p xml:id="_Ydebgru"><s xml:id="_aVkDkUc">Paper 589</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_zTbByR3">Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices</title>
		<author>
			<persName><surname>Michael D AbrÃ moff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Philip T Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilay</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><surname>Folk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aq7vNuV">NPJ Digit Med</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2018-08">2018. Aug. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael D AbrÃ moff, Philip T Lavin, Michele Birch, Nilay Shah, and James C Folk. 2018. Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices. NPJ Digit Med 1 (Aug. 2018), 39.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_PrukGQ5">Use of computer-aided detection (CAD) tools in screening mammography: a multidisciplinary investigation</title>
		<author>
			<persName><forename type="first">A A</forename><surname>Alberdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Povyakalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Strigini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartswood</surname></persName>
		</author>
		<author>
			<persName><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><surname>Slack</surname></persName>
		</author>
		<idno type="DOI">10.1259/bjr/37646417</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_v5wAcYS">Br. J. Radiol</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="40" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E Alberdi, A A Povyakalo, L Strigini, P Ayton, M Hartswood, R Procter, and R Slack. 2005. Use of computer-aided detection (CAD) tools in screening mammography: a multidisciplinary investigation. Br. J. Radiol. 78 Spec No 1 (2005), S31-40.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<idno type="DOI">10.4135/9781412963855.n51</idno>
		<ptr target="https://www.aao.org/newsroom/eye-health-statistics#_edn25" />
		<title level="m" xml:id="_67mBHtn">Eye Health Statistics</title>
		<imprint>
			<publisher>American Academy of Ophthalmology</publisher>
			<date type="published" when="2015">2015. 2015. 2019-9-7</date>
		</imprint>
	</monogr>
	<note type="raw_reference">American Academy of Ophthalmology. 2015. Eye Health Statistics. https: //www.aao.org/newsroom/eye-health-statistics#_edn25. (2015). Accessed: 2019-9-7.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_ZkcP8XE">Clinical Decision Support Systems: Theory and Practice</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eta</surname></persName>
		</author>
		<author>
			<persName><surname>Berner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Eta S Berner. 2007. Clinical Decision Support Systems: Theory and Practice. Springer Science &amp; Business Media.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main" xml:id="_uFeAwVU">Contextual design: defining customer-centered systems</title>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Holtzblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Hugh Beyer and Karen Holtzblatt. 1997. Contextual design: defining customer-centered systems. Elsevier.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_Qy8vXH2">Taking the Time to Care: Empowering Low Health Literacy Hospital Patients with Virtual Nurse Agents</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">W</forename><surname>Bickmore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">W</forename><surname>Jack</surname></persName>
		</author>
		<idno type="DOI">10.1145/1518701.1518891</idno>
		<ptr target="http://dx.doi.org/10.1145/1518701.1518891" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_6HpGyE6">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;09)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;09)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
	<note type="raw_reference">Timothy W. Bickmore, Laura M. Pfeifer, and Brian W. Jack. 2009. Taking the Time to Care: Empowering Low Health Literacy Hospital Patients with Virtual Nurse Agents. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI &apos;09). ACM, New York, NY, USA, 1265-1274. DOI: http://dx.doi.org/10.1145/1518701.1518891</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main" xml:id="_xVnfYSD">Participatory IT design: designing for business and workplace realities</title>
		<author>
			<persName><forename type="first">Keld</forename><surname>BÃ¸dker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finn</forename><surname>Kensing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesper</forename><surname>Simonsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Keld BÃ¸dker, Finn Kensing, and Jesper Simonsen. 2009. Participatory IT design: designing for business and workplace realities. MIT press.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_bfMvvXr">Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making</title>
		<author>
			<persName><forename type="first">Carrie</forename><forename type="middle">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayan</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300234</idno>
		<ptr target="http://dx.doi.org/10.1145/3290605.3300234" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_PFSgHCn">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note>Article 4</note>
	<note type="raw_reference">Carrie J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S. Corrado, Martin C. Stumpe, and Michael Terry. 2019a. Human-Centered Tools for Coping with Imperfect Algorithms During Medical Decision-Making. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19). ACM, New York, NY, USA, Article 4, 14 pages. DOI:http://dx.doi.org/10.1145/3290605.3300234</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_GWMbdnV">Hello AI&quot;: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making</title>
		<author>
			<persName><forename type="first">Carrie Jun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_36qqgvC">CSCW Conf Comput Support Coop Work</title>
		<imprint>
			<date type="published" when="2019">2019. 2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Carrie Jun Cai, Samantha Winter, David Steiner, Lauren Wilcox, and Michael Terry. 2019b. &quot;Hello AI&quot;: Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making. CSCW Conf Comput Support Coop Work 2019 (2019).</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_RUdauaF">Computer aided detection (CAD): an overview</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><surname>Castellino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WJMrj94">Cancer Imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ronald A Castellino. 2005. Computer aided detection (CAD): an overview. Cancer Imaging 5, 1 (2005), 17.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno type="DOI">10.1046/j.1523-5394.2001.96005.x</idno>
		<ptr target="https://www.cdc.gov/media/releases/2017/p0718-diabetes-report.html" />
		<title level="m" xml:id="_J9dPZtP">More than 100 million Americans have diabetes or prediabetes</title>
		<imprint>
			<publisher>CDC</publisher>
			<date type="published" when="2019-02">2019. February 2019</date>
		</imprint>
	</monogr>
	<note>Press Release</note>
	<note type="raw_reference">CDC. 2019. More than 100 million Americans have diabetes or prediabetes. Press Release. (February 2019). https://www.cdc.gov/media/releases/2017/ p0718-diabetes-report.html.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_jCJqAE7">Impact of Computer-Aided Detection Systems on Radiologist Accuracy With Digital Mammography</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Elodia B Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helga</forename><forename type="middle">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Hendrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etta</forename><forename type="middle">D</forename><surname>Yaffe</surname></persName>
		</author>
		<author>
			<persName><surname>Pisano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NkDxBNj">AJR Am. J. Roentgenol</title>
		<imprint>
			<biblScope unit="volume">203</biblScope>
			<biblScope unit="page">909</biblScope>
			<date type="published" when="2014-10">2014. Oct. 2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Elodia B Cole, Zheng Zhang, Helga S Marques, R Edward Hendrick, Martin J Yaffe, and Etta D Pisano. 2014. Impact of Computer-Aided Detection Systems on Radiologist Accuracy With Digital Mammography. AJR Am. J. Roentgenol. 203, 4 (Oct. 2014), 909.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_guydRQc">Participant Observation: A Guide for Fieldworkers</title>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Musante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dewalt</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Billie</forename><forename type="middle">R</forename><surname>Dewalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Rowman Altamira</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Kathleen Musante DeWalt and Billie R DeWalt. 2002. Participant Observation: A Guide for Fieldworkers. Rowman Altamira.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_9N6xRjD">Diabetes patient education: a meta-analysis and meta-regression</title>
		<author>
			<persName><forename type="first">Theodore</forename><surname>Shelley E Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Speroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Dittus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">A</forename><surname>Pichert</surname></persName>
		</author>
		<author>
			<persName><surname>Elasy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uzG2GZz">Patient Educ. Couns</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="105" />
			<date type="published" when="2004-01">2004. Jan. 2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shelley E Ellis, Theodore Speroff, Robert S Dittus, Anne Brown, James W Pichert, and Tom A Elasy. 2004. Diabetes patient education: a meta-analysis and meta-regression. Patient Educ. Couns. 52, 1 (Jan. 2004), 97-105.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_MHN9DRG">Many miles to go...&quot;: a systematic review of the implementation of patient decision support interventions into routine clinical practice</title>
		<author>
			<persName><forename type="first">Glyn</forename><surname>Elwyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Scholl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Tietbohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mala</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">Gk</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catharine</forename><surname>Clay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">France</forename><surname>LÃ©garÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trudy</forename><surname>Van Der Weijden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmen</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<idno type="DOI">10.1186/1472-6947-13-s2-s14</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SexyKNA">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2013-11">2013. Nov. 2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Glyn Elwyn, Isabelle Scholl, Caroline Tietbohl, Mala Mann, Adrian GK Edwards, Catharine Clay, France LÃ©garÃ©, Trudy van der Weijden, Carmen L Lewis, Richard M Wexler, and others. 2013. &quot;Many miles to go...&quot;: a systematic review of the implementation of patient decision support interventions into routine clinical practice. BMC medical informatics and decision making 13, 2 (Nov. 2013), 1-10.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_gr86Zbz">A review of 25 years of CSCW research in healthcare: contributions, challenges and future agendas</title>
		<author>
			<persName><forename type="first">Geraldine</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Ellingsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_j5wZbwp">Computer Supported Cooperative Work (CSCW)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="609" to="665" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Geraldine Fitzpatrick and Gunnar Ellingsen. 2013. A review of 25 years of CSCW research in healthcare: contributions, challenges and future agendas. Computer Supported Cooperative Work (CSCW) 22, 4-6 (2013), 609-665.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_FvN58eG">Promoting service design as a core practice in interaction design</title>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">Jodi Forlizzi and John Zimmerman. Promoting service design as a core practice in interaction design.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_vURy7gP">Anniversary Paper: History and status of CAD and quantitative image analysis: The role of Medical Physics and AAPM</title>
		<author>
			<persName><forename type="first">Maryellen</forename><forename type="middle">L</forename><surname>Giger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heang-Ping</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Boone</surname></persName>
		</author>
		<idno type="DOI">10.1118/1.3013555</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x7vmefn">Med. Phys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">5799</biblScope>
			<date type="published" when="2008-12">2008. Dec. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Maryellen L Giger, Heang-Ping Chan, and John Boone. 2008. Anniversary Paper: History and status of CAD and quantitative image analysis: The role of Medical Physics and AAPM. Med. Phys. 35, 12 (Dec. 2008), 5799.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_mDQ4Fbu">Analysing the role of complexity in explaining the fortunes of technology programmes: empirical application of the NASSS framework</title>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Wherton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chrysanthi</forename><surname>Papoutsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenni</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Hinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12916-018-1050-6</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vsJ3vEG">BMC medicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Trisha Greenhalgh, Joe Wherton, Chrysanthi Papoutsi, Jenni Lynch, Gemma Hughes, Sue Hinder, Rob Procter, Sara Shaw, and others. 2018. Analysing the role of complexity in explaining the fortunes of technology programmes: empirical application of the NASSS framework. BMC medicine 16, 1 (2018), 66.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_YEy8RPP">Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs</title>
		<author>
			<persName><forename type="first">Lily</forename><surname>Varun Gulshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">C</forename><surname>Coram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arunachalam</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhashini</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasumi</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Widner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Madams</surname></persName>
		</author>
		<author>
			<persName><surname>Cuadros</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uTteccf">Jama</title>
		<imprint>
			<biblScope unit="volume">316</biblScope>
			<biblScope unit="page" from="2402" to="2410" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, and others. 2016. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama 316, 22 (2016), 2402-2410.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_D3guX4Z">Repairing&apos; the Machine: A Case Study of the Evaluation of Computer-Aided Detection Tools in Breast Screening</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hartswood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Rouncefield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Slack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Soutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-94-010-0068-0_20</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_X4FVquv">ECSCW 2003</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="375" to="394" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mark Hartswood, Rob Procter, Mark Rouncefield, Roger Slack, James Soutter, and Alex Voss. 2003. &apos;Repairing&apos; the Machine: A Case Study of the Evaluation of Computer-Aided Detection Tools in Breast Screening. In ECSCW 2003. Springer, 375-394.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_EKJsZ6Z">Supporting families in reviewing and communicating about radiology imaging studies</title>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Matthew K Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meeshu</forename><surname>Feustel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Agnihotri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">F</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Simoneaux</surname></persName>
		</author>
		<author>
			<persName><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6UR3Xp2">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2017 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">589</biblScope>
		</imprint>
	</monogr>
	<note>Paper</note>
	<note type="raw_reference">Matthew K Hong, Clayton Feustel, Meeshu Agnihotri, Max Silverman, Stephen F Simoneaux, and Lauren Wilcox. 2017. Supporting families in reviewing and communicating about radiology imaging studies. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. ACM, 5245-5256. Paper 589</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main" xml:id="_nJfN7Dq">The national survey of blindness low vision and visual impairment in thailand 2006</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jenchitr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hanutsaha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iamsirithaworn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Parnrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Choosri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jenchitr W, Hanutsaha P, Iamsirithaworn S, Parnrat U, Choosri P. 2007. The national survey of blindness low vision and visual impairment in thailand 2006-2007.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<idno type="DOI">10.61508/refl.v10i0.114263</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mBGEGuA">Thai J Pub Hlth Ophthalmol</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thai J Pub Hlth Ophthalmol 21, 1 (2007), 10.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_tGVQCtK">Collaboration and trust in healthcare innovation: The eDiaMoND case study</title>
		<author>
			<persName><forename type="first">Marina</forename><surname>Jirotka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hartswood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Slack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catelijne</forename><surname>Coopmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hinds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10606-005-9001-0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rAA4ubf">Computer Supported Cooperative Work (CSCW)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="369" to="398" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marina Jirotka, Rob Procter, Mark Hartswood, Roger Slack, Andrew Simpson, Catelijne Coopmans, Chris Hinds, and Alex Voss. 2005. Collaboration and trust in healthcare innovation: The eDiaMoND case study. Computer Supported Cooperative Work (CSCW) 14, 4 (2005), 369-398.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_KVDRAS8">With an eye to AI and autonomous diagnosis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pearse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FRSY9CN">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2018-08">2018. Aug. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pearse A Keane and Eric J Topol. 2018. With an eye to AI and autonomous diagnosis. npj Digital Medicine 1, 1 (Aug. 2018), 1-3.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_UaTZHgC">Why CAD failed in mammography</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Jha</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jacr.2017.12.029</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3fCJr8x">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="537" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ajay Kohli and Saurabh Jha. 2018. Why CAD failed in mammography. Journal of the American College of Radiology 15, 3 (2018), 535-537.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_TKTegc6">Clinical Decision-Support Systems</title>
		<author>
			<persName><forename type="first">Blackford</forename><surname>Mark A Musen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Middleton</surname></persName>
		</author>
		<author>
			<persName><surname>Greenes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pAZpUZh">Biomedical Informatics</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="643" to="674" />
		</imprint>
	</monogr>
	<note type="raw_reference">Mark A Musen, Blackford Middleton, and Robert A Greenes. 2014. Clinical Decision-Support Systems. In Biomedical Informatics. Springer, London, 643-674.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<ptr target="http://www.icoph.org/dynamic/attachments/resources/diabetic-retinopathy-detail.pdf" />
		<title level="m" xml:id="_hGuyFHa">International Clinical Diabetic Retinopathy Disease Severity Scale</title>
		<imprint>
			<publisher>American Academy of Ophthalmology</publisher>
			<date type="published" when="2002-10">2002. Oct. 2002. 2019-12-17</date>
		</imprint>
	</monogr>
	<note type="raw_reference">American Academy of Ophthalmology. 2002. International Clinical Diabetic Retinopathy Disease Severity Scale. http://www.icoph.org/dynamic/attachments/resources/ diabetic-retinopathy-detail.pdf. (Oct. 2002). Accessed: 2019-12-17.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<idno type="DOI">10.1177/0145482x9208600103</idno>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/blindness-and-visual-impairment." />
		<title level="m" xml:id="_cq9EdWG">Vision impairment and blindness</title>
		<imprint>
			<publisher>World Health Organization</publisher>
			<date type="published" when="2018">2018. 2018. 2019-9-13</date>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization. 2018. Vision impairment and blindness. https://www.who.int/news-room/ fact-sheets/detail/blindness-and-visual-impairment. (2018). Accessed: 2019-9-13.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main" xml:id="_mTyAJaD">Identifying Challenges and Opportunities in Human-AI Collaboration in Healthcare</title>
		<author>
			<persName><forename type="first">Young</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Yi</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barbarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Astrid</forename><surname>Kaziunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karandeep</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Lasecki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sun Young Park, Pei-Yi Kuo, Andrea Barbarin, Elizabeth Kaziunas, Astrid Chow, Karandeep Singh, Lauren Wilcox, and Walter Lasecki. 2019. Identifying Challenges and Opportunities in Human-AI Collaboration in Healthcare. (2019).</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_avAAuuA">Using mobile phones to present medical information to hospital patients</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Pfeifer Vardoulakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Karlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gatewood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desney</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2207676.2208601</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jzc3DwG">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
	<note type="raw_reference">Laura Pfeifer Vardoulakis, Amy Karlson, Dan Morris, Greg Smith, Justin Gatewood, and Desney Tan. 2012. Using mobile phones to present medical information to hospital patients. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1411-1420.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_a79Nrfe">The impact of computerised physician order entry and clinical decision support on pharmacist-physician communication in the hospital setting: A qualitative study</title>
		<author>
			<persName><forename type="first">Jamie</forename><forename type="middle">J</forename><surname>Sarah K Pontefract</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><forename type="middle">K</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><forename type="middle">A</forename><surname>Vallance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabi</forename><surname>Marriott</surname></persName>
		</author>
		<author>
			<persName><surname>Redwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Vy4u5Cm">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">207450</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sarah K Pontefract, Jamie J Coleman, Hannah K Vallance, Christine A Hirsch, Sonal Shah, John F Marriott, and Sabi Redwood. 2018. The impact of computerised physician order entry and clinical decision support on pharmacist-physician communication in the hospital setting: A qualitative study. PloS one 13, 11 (2018), e0207450.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_96WdMyT">Deep learning versus human graders for classifying diabetic retinopathy severity in a nationwide screening program</title>
		<author>
			<persName><forename type="first">Paisan</forename><surname>Raumviboonsuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peranut</forename><surname>Chotcomwongse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rory</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kasumi</forename><surname>Widner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonia</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornwipa</forename><surname>Phene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mongkol</forename><surname>Hemarat</surname></persName>
		</author>
		<author>
			<persName><surname>Tadarati</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aUmxRyg">npj Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Paisan Raumviboonsuk, Jonathan Krause, Peranut Chotcomwongse, Rory Sayres, Rajiv Raman, Kasumi Widner, Bilson JL Campana, Sonia Phene, Kornwipa Hemarat, Mongkol Tadarati, and others. 2019. Deep learning versus human graders for classifying diabetic retinopathy severity in a nationwide screening program. npj Digital Medicine 2, 1 (2019), 25.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_SZMyKYk">Technology, work, and information flows: Lessons from the implementation of a wireless alert pager system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Madhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanda</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shabot</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_T4P2Cdr">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Madhu C Reddy, David W McDonald, Wanda Pratt, and M Michael Shabot. 2005. Technology, work, and information flows: Lessons from the implementation of a wireless alert pager system. Journal of biomedical informatics 38, 3 (2005), 229-238.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_ST5mnjy">Making Machine Learning Models Clinically Useful</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">C</forename><surname>Milstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phd</forename><surname>Bagley</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2019.10306</idno>
		<ptr target="http://dx.doi.org/10.1001/jama.2019.10306" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_K2zP3SA">JAMA</title>
		<imprint>
			<biblScope unit="volume">08</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nigam H. Shah, Arnold Milstein, and Steven C. Bagley, PhD. 2019. Making Machine Learning Models Clinically Useful. JAMA (08 2019). DOI: http://dx.doi.org/10.1001/jama.2019.10306</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_Y6QTfVw">Designing patient-centric information displays for hospitals</title>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desney</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gatewood</surname></persName>
		</author>
		<idno type="DOI">10.1145/1753326.1753650</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Urq3Vme">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2123" to="2132" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lauren Wilcox, Dan Morris, Desney Tan, and Justin Gatewood. 2010. Designing patient-centric information displays for hospitals. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 2123-2132.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_5GNWxMP">Interactive tools for inpatient medication tracking: a multi-phase study with cardiothoracic surgery patients</title>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Woollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Prey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Restaino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><surname>Bakken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sackeim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Vawdrey</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocv160</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FnnVxsr">J. Am. Med. Inform. Assoc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="144" to="158" />
			<date type="published" when="2016-01">2016. Jan. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lauren Wilcox, Janet Woollen, Jennifer Prey, Susan Restaino, Suzanne Bakken, Steven Feiner, Alexander Sackeim, and David K Vawdrey. 2016. Interactive tools for inpatient medication tracking: a multi-phase study with cardiothoracic surgery patients. J. Am. Med. Inform. Assoc. 23, 1 (Jan. 2016), 144-158.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_xya9U7K">Global Initiative for the Elimination of Avoidable Blindness : action plan 2006-2011</title>
		<idno type="DOI">10.1016/s0002-9394(99)80239-6</idno>
		<ptr target="https://www.who.int/blindness/Vision2020_report.pdf" />
		<imprint>
			<date type="published" when="2007">2007. 2007. 2019-9-7</date>
			<publisher>World Health Organization</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization. 2007. Global Initiative for the Elimination of Avoidable Blindness : action plan 2006-2011. https://www.who.int/blindness/Vision2020_report.pdf. (2007). Accessed: 2019-9-7.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<idno type="DOI">10.1016/b978-0-12-822521-9.60022-9</idno>
		<ptr target="https://www.who.int/news-room/fact-sheets/detail/diabetes." />
		<title level="m" xml:id="_F9jd5uv">WHO: Diabetes factsheet</title>
		<imprint>
			<publisher>World Health Organization</publisher>
			<date type="published" when="2014">2014. 2014. 2019-9-13</date>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization. 2014. WHO: Diabetes factsheet. https: //www.who.int/news-room/fact-sheets/detail/diabetes. (2014). Accessed: 2019-9-13.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<idno type="DOI">10.30875/2ce42796-en</idno>
		<ptr target="https://www.who.int/diabetes/country-profiles/tha_en.pdf?ua=1." />
		<title level="m" xml:id="_Ud67k4J">Diabetes country profiles</title>
		<meeting><address><addrLine>Thailand</addrLine></address></meeting>
		<imprint>
			<publisher>World Health Organization</publisher>
			<date type="published" when="2016">2016. 2016. 2016. 2019-9-7</date>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization. 2016a. Diabetes country profiles 2016 : Thailand. https://www.who.int/diabetes/ country-profiles/tha_en.pdf?ua=1. (2016). Accessed: 2019-9-7.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="https://www.who.int/diabetes/country-profiles/usa_en.pdf" />
		<title level="m" xml:id="_mshXMvp">Diabetes country profiles</title>
		<imprint>
			<publisher>USA</publisher>
			<date type="published" when="2016">2016. 2016. 2016. 2019-9-7</date>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization. 2016b. Diabetes country profiles 2016 : USA. https: //www.who.int/diabetes/country-profiles/usa_en.pdf. (2016). Accessed: 2019-9-7.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_YqEAWsw">Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Zimmerman</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300468</idno>
		<ptr target="http://dx.doi.org/10.1145/3290605.3300468Paper589" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_nENzPB8">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">238</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Qian Yang, Aaron Steinfeld, and John Zimmerman. 2019. Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI &apos;19). ACM, New York, NY, USA, Article 238, 11 pages. DOI: http://dx.doi.org/10.1145/3290605.3300468 Paper 589</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
