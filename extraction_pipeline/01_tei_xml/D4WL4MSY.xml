<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_wcKCbcG">From Ads to Interventions: Contextual Bandits in Mobile Health</title>
				<funder ref="#_mQdMuMa">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
					<idno type="DOI" subtype="crossref">https://doi.org/10.13039/100000002</idno>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_mcUXF2v">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_8GyQNqB">
					<orgName type="full">CAREER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
							<email>tewaria@umich.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan , Ann Arbor , MI , USA</note>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan , Ann Arbor , MI , USA</note>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
							<email>samurphy@umich.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan , Ann Arbor , MI , USA</note>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<note type="raw_affiliation">University of Michigan , Ann Arbor , MI , USA</note>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_nKWnb2A">From Ads to Interventions: Contextual Bandits in Mobile Health</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8FD2F400A74D4343661509AFBB26C5D4</idno>
					<idno type="DOI">10.1007/978-3-319-51394-2_25</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T08:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_PuAqrTH"><p xml:id="_6kSxHb9"><s xml:id="_hCkkACm">The first paper on contextual bandits was written by Michael Woodroofe in 1979 (Journal of the American Statistical Association, 74(</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="439.36" lry="666.15"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Pey9rf9">Introduction</head><p xml:id="_XuQnvM9"><s xml:id="_wpQqSa6">The classic multi-armed bandit problem (see, e.g., <ref type="bibr" target="#b0">[1]</ref>) is perhaps the simplest model of a sequential decision making problem where one wishes to maximize the cumulative sum of rewards received over some time horizon.</s><s xml:id="_f289MuD">Faced with a finite number of alternatives, called actions or arms, the decision maker must choose between them at every time point.</s><s xml:id="_TWR2nNH">One has to balance the exploration of actions that have hitherto yielded low rewards, with exploitation of current knowledge about actions have yielded high rewards so far.</s></p><p xml:id="_25uJbrz"><s xml:id="_T6c2TSQ">Woodroofe <ref type="bibr" target="#b1">[2]</ref> noted that, in most sequential decision making scenarios, there is likely to be some additional information available that can be useful for decision making.</s><s xml:id="_8ya7DXR">For example, in a clinical trial with two drugs, we might have people's genetic or demographic information available as features.</s><s xml:id="_MThHjEX">If so, then rather than thinking about a two-armed bandit problem, one should think about the clinical trial as a contextual bandit problem where we want to learn how to map user features into one of the available actions, i.e., one of the two drugs in this case.</s><s xml:id="_5Gca9Ee">Woodroofe defined this problem, albeit in the case of just one feature, but he did not call it a "contextual bandit" problem.</s><s xml:id="_Ygbmvda">Instead he called it a "bandit problem with a concomitant variable".</s></p><p xml:id="_FExWN46"><s xml:id="_meyzyqs">As it sometimes the case with broadly useful problems, contextual bandit problems have been considered by many different communities by many different names.</s><s xml:id="_k5xQrHB">They have been called "bandit problems with side observations" <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, "bandit problems with side information" <ref type="bibr" target="#b4">[5]</ref>, "associative reinforcement learning" <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, "reinforcement learning with immediate reward" <ref type="bibr" target="#b8">[9]</ref>, "associative bandit problems" <ref type="bibr" target="#b9">[10]</ref>, and "bandit problems with covariates" <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_Dz2H7Qn">The term "contextual bandits" was coined by Langford and Zhang <ref type="bibr" target="#b14">[15]</ref> and we stick to it because it is descriptive yet short.</s></p><p xml:id="_dqreF4s"><s xml:id="_Fpjh66p">Recent interest in contextual bandits has been driven to a large extent by personalization problems arising on the web.</s><s xml:id="_kJzkWRG">How to use user and webpage features to select the best ad to show to the user on a given webpage <ref type="bibr" target="#b15">[16]</ref>?</s><s xml:id="_TxAcXq6">How to show personalized news articles to web users based on their interests <ref type="bibr" target="#b16">[17]</ref>?</s><s xml:id="_RaYCarb">With the emergence of mobile health, we expect that many ideas developed to show personalized ads to users on the web will be found useful in personalizing mobile health interventions to a specific person in a particular context.</s></p><p xml:id="_xWWedqX"><s xml:id="_ugsRNH6">The framework of Just-In-Time Adaptive Interventions <ref type="bibr" target="#b17">[18]</ref> has recently been put forward to unify a number of decision making problems that arise in mobile health across a variety of behavior change domains including alcohol abuse, depression, obesity, and substance abuse.</s><s xml:id="_JgcEznY">There are five keys components of JITAIs: decision points, decision rules, tailoring variables, intervention options, and proximal outcomes.</s><s xml:id="_mbhxJdW">Contextual bandit algorithms can be used for personalizing JITAIs.</s><s xml:id="_222bx7a">The tailoring variables, such as GPS location, calendar busyness, and heartrate, form the context.</s><s xml:id="_vBEJDvA">The intervention options are the actions.</s><s xml:id="_kGpxs6g">For simplicity, we assume throughout this chapter, that there are only two intervention options: whether to intervene or not.</s><s xml:id="_BXQ29HP">For example, in a physical activity JITAI, the two intervention options might be whether or not to send an activity encouraging message.</s><s xml:id="_aDG7DJS">Once an intervention option is chosen, a proximal outcome (i.e., reward) is obtained.</s><s xml:id="_Gk2ec8a">Again, to use the example of the physical activity JITAI, our proximal outcome might be the number of steps the person walked in the 1 h following the decision point.</s><s xml:id="_8JVakFF">In JITAIs, the fundamental pattern that repeats over time is the following.</s><s xml:id="_x6NUV9M">mobile phone records the proximal outcome (interpreted as a reward, so higher is better) 5: done</s></p><p xml:id="_3pgNH6B"><s xml:id="_Dc6Y6HC">In the rest of this chapter, we will see how the contextual bandit problem is a good way to think about the problem of personalizing JITAIs in a mobile health setting.</s><s xml:id="_4RNqWgd">We will look at online learning algorithms that learn good decision rules (policies) over time by interacting with the environment using a protocol very similar to the fundamental temporally-repeating pattern described above.</s><s xml:id="_fUrAKuX">We will first survey existing contextual bandit frameworks and algorithms to give the reader a sense of the breadth of work that has occurred in this area across several different fields including computer science, electrical engineering, operations research, and statistics.</s><s xml:id="_kQGrwK8">Then we will highlight the unique challenges that arise in mobile health and discuss how existing contextual bandit algorithms will need to be modified before they can be used successfully in mobile health.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CxUpV3F">Online Learning in Contextual Bandits</head><p xml:id="_UKtmtyN"><s xml:id="_wXSewSw">In this section we will review the online learning literature on contextual bandit problems.</s><s xml:id="_xAxfNWM">The focus will be on algorithms that minimize their regret.</s><s xml:id="_KdUV6Q9">Regret measures the difference between the reward that could have been accumulated with prior knowledge of the problem, and the reward accumulated by the learning algorithm.</s><s xml:id="_eTTRTgp">The precise definition depends on the setting in which one is analyzing the learning algorithm.</s><s xml:id="_Nf3V8Kh">We will consider three settings that make increasingly weaker assumptions about the data generating process.</s><s xml:id="_Gc7yk8Y">In the first setting, contexts and rewards are all stochastically generated from an iid process.</s><s xml:id="_ZdCve7X">In the second setting, contexts are arbitrary but rewards are stochastic.</s><s xml:id="_da7WN5m">Finally, in the third setting, contexts and rewards are all arbitrary.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qF89VYQ">Stochastic Contextual Bandits</head><p xml:id="_sVRNSzj"><s xml:id="_kENyMu3">In the stochastic setting, we assume that the context and reward triples f.X t ; R 0 t ; R 1 t /g T tD1 are generated by sampling independently from an underlying distribution D. The following online learning protocol is followed.</s></p><p xml:id="_2kX4vHJ"><s xml:id="_62nWV9H">1: for t D 1 to T do 2: receive context X t 3: algorithm takes action A t 4: receive reward R t D R A t t 5: end for</s></p><p xml:id="_wdZW3Fr"><s xml:id="_bMVeFbG">The contexts X t are drawn from some context space X .</s><s xml:id="_KNYYq6y">Unless otherwise specified, we will assume that the context X t 2 R p is a vector with p components so that X Â R p .</s><s xml:id="_pyTTJGs">The literature has considered situations both less general (e.g., finite context space <ref type="bibr" target="#b18">[19]</ref>) and more general (e.g., contexts in a general metric space <ref type="bibr" target="#b19">[20]</ref>).</s><s xml:id="_zC5tQ9p">The actions A t lie an action space A which we will assume, unless indicated otherwise, to be f0; 1g with 1 corresponding to the option of providing an intervention and 0 to not providing.</s></p><p xml:id="_ZVG7fDB"><s xml:id="_fp9s2bh">A policy or decision rule W X !</s><s xml:id="_aM8wjMT">A decides which action gets taken in which contexts.</s><s xml:id="_Y4jaR78">The value of a policy is defined as the expected reward obtained when actions are chosen according to :</s></p><formula xml:id="formula_0">V. / D E .X;R 0 ;R 1 / D R .X/ :</formula><p xml:id="_Z4dN9g9"><s xml:id="_RYWYQxS">The value of a policy, in turn, depends on the expected reward functions Á a ; a 2 A , defined as:</s></p><formula xml:id="formula_1">Á a .x/ D E .X;R 0 ;R 1 / D OER a jX D x:</formula><p xml:id="_emhRZx6"><s xml:id="_VeCX6Bb">Note that the value of a policy and the expected reward functions are related to each other as follows:</s></p><formula xml:id="formula_2">V. / D E X D X Á .X/ .X/ :</formula><p xml:id="_qUJPxK5"><s xml:id="_8Bgy97Q">Here D X is the marginal distribution of contexts.</s><s xml:id="_9rJNkTH">The optimal policy ?</s><s xml:id="_rsUMvbW">, among all possible policies, is given by</s></p><formula xml:id="formula_3">? .x/ D argmax a2A Á a .x/:<label>(1)</label></formula><p xml:id="_wndBgc5"><s xml:id="_MZD6GV5">An online learning algorithm L is a sequence of maps L t ; 1 Ä t Ä T, where L t maps the history just prior to time t, f.X s ; A s ; R s /g t 1 sD1 , along with the current context X t to an action A t 2 A .</s><s xml:id="_rX473UX">If any of the maps L t are stochastic, i.e., the algorithm uses some internal randomization, then we call it a randomized online learning algorithm.</s><s xml:id="_vQm32kR">Otherwise, we call it a deterministic online learning algorithm.</s></p><p xml:id="_MYfwePu"><s xml:id="_qENRjKH">We will look at several different notions of regret.</s><s xml:id="_3U4aeJS">All of them will be of the form:</s></p><p xml:id="_jnGzYMS"><s xml:id="_C7u9xng">"best expected cumulative reward in a comparison class"</s></p><formula xml:id="formula_4">T X tD1 EOER t</formula><p xml:id="_YF3m8MT"><s xml:id="_MaPd9zX">where the first term, referred to as the "benchmark" or "comparator" term measures the total expected reward that would have been obtained with advanced knowledge of the distributions (in the stochastic case) or nature's moves (in the adversarial case).</s><s xml:id="_V5sC6EP">The second term is the expected reward accumulated by the online learning algorithm.</s><s xml:id="_Ysa5xD3">Note that this expectation is taken with respect to any randomness in nature's generation of contexts and rewards, as well as any randomness used by the algorithm (if it is a randomized online learning algorithm).</s><s xml:id="_Hsn5N6K">Contextual bandit problems can be approached through several perspectives.</s><s xml:id="_zCBAABT">We can adopt a regression perspective and view the problem as one of estimating the expected reward functions Á a .x/.</s><s xml:id="_bvmaDNq">Given estimates b Á a , we can choose the corresponding "greedy" policy GREEDY.b</s><s xml:id="_TcDMxEZ">Á a / defined as GREEDY.b</s><s xml:id="_VZkG2FA">Á a /.x/ D argmax a2A b Á a .x/:</s></p><p xml:id="_WsQwsJp"><s xml:id="_HbUDh35">Note that the optimal policy defined in (1) is nothing but GREEDY.Á a /.</s></p><p xml:id="_dS3rNTm"><s xml:id="_kzUvSks">In the case of two actions, one can also adopt a binary classification perspective and fix a set ˘of policies that can also be thought of as a set of classifiers.</s><s xml:id="_7gR6Vk2">The best policy in this class is In the rest of this section, we first review approaches, both parametric and nonparametric, based on the regression perspective.</s><s xml:id="_vK5SJDr">Then we will consider classification based approaches that search for good policies in a restricted class.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_DnSk3RG">Parametric Estimation of Expected Reward Functions</head><p xml:id="_mN5KjwU"><s xml:id="_Ds4wZg6">In addition to assuming that the triples .X t ; R 0 t ; R 1 t / are iid, let us also assume that</s></p><formula xml:id="formula_5">R a t D ˇ&gt; a X t C a t ;<label>(2)</label></formula><p xml:id="_rXWevKj"><s xml:id="_UYR74qJ">where X t ; ˇ&gt; a 2 R p and a t are iid mean-zero random variables.</s><s xml:id="_er9Vf45">This implies that the expected reward functions Á a .x/</s><s xml:id="_s6CunzE">D ˇ&gt; a x are linear in the context x.</s><s xml:id="_FHXEEju">Under this assumption, the best policy takes the form ? .x/</s><s xml:id="_JVre9k9">D GREEDY.ˇa/.x/</s><s xml:id="_zq8PdDs">D argmax a2A ˇ&gt; a x:</s></p><p xml:id="_pvFz99B"><s xml:id="_5MRZ66V">Expected reward of this optimal policy over T time steps is T V. ?</s><s xml:id="_xeVMMqq">/.</s><s xml:id="_NCMBxFa">The expected regret of a learning algorithm is defined as</s></p><formula xml:id="formula_6">T V. ? / T X tD1 EOER t :<label>(3)</label></formula><p xml:id="_MeJnq5c"><s xml:id="_j9w9TkP">A simple approach for online learning in this setting is to adopt what has been called a "certainty equivalence with forcing" strategy in the adaptive control literature <ref type="bibr" target="#b20">[21]</ref>.</s></p><p xml:id="_vRxWVyB"><s xml:id="_3fQEAPh">The idea is to choose a predetermined sequence of time points when the learning algorithm simply explores different actions.</s><s xml:id="_RKnWu9J">On rounds other than the exploration rounds, the algorithm "exploits" the current knowledge.</s><s xml:id="_ZWKVhtw">"Greedy" or "certainty equivalent" exploitation means that the algorithm believes its current estimates of the expected reward function and takes the optimal action according to those estimates.</s></p><p xml:id="_ArgQzjr"><s xml:id="_gXJK46y">Algorithm 1 Linear Response Bandit Algorithm <ref type="bibr" target="#b21">[22]</ref> Inputs: n 0 (initial exploration length), T a (exploration times for action a), h (localization parameter to decide which estimates to use)</s></p><formula xml:id="formula_7">for</formula><p xml:id="_R27khet"><s xml:id="_Dyfuetd">t D 1 to 2n 0 do Take action A t D 0 or A t D 1 depending on whether t is odd or even end for for t D 2n 0 C 1 to T do if t 2 T a then =? Exploration round ?=</s><s xml:id="_neH6hCc">Take action A t D a Update b ˇa using least squares on previous rounds when action a was taken Update e ˇa using least squares on previous exploration rounds when action a was taken else =? Exploitation round ?= if j. e ˇ1 e ˇ0/ &gt; X t j &gt; h=2 then Take action A t D argmax a .</s><s xml:id="_HdNchB3">e ˇa/ &gt; X t else Take action A t D argmax a .</s><s xml:id="_Dx7WD8x">b ˇa/ &gt; X t end if end if end for</s></p><p xml:id="_4VVN48c"><s xml:id="_mzZQgzG">The algorithm of Goldenshluger and Zeevi <ref type="bibr" target="#b21">[22]</ref> (Algorithm 1) adopts such an approach with a slight twist: it maintains two sets of estimates for the expected reward functions.</s><s xml:id="_Gj2hp7H">The first set of estimates, e ˇa, are computed from data obtained during forced exploration rounds and the second set of estimates, b ˇa, are computed from data obtained in all previous rounds.</s><s xml:id="_87hFMqG">At an exploitation round, the algorithm checks to see if there is enough gap between the quality of the two actions according to e ˇa.</s><s xml:id="_CJgQFxT">If there is enough gap, then it selects an action using the policy GREEDY.</s><s xml:id="_N4W98Pf">e ˇa/, otherwise it uses the policy GREEDY.</s><s xml:id="_XX7xmAG">b ˇa/.</s><s xml:id="_hT7akj3">Goldenshluger and Zeevi establish an O.p 3 log T/ regret bound for Algorithm 1 under several assumptions including the assumption that a t are normally distributed and that a "margin" condition holds.</s><s xml:id="_skyq3ax">Goldenshluger and Zeevi had earlier brought the margin condition from the classification literature into the contextual bandit literature <ref type="bibr" target="#b22">[23]</ref>.</s><s xml:id="_YEswAmj">The margin condition ensures that the contexts X t are distributed such that, with high probability, the treatment effect magnitude j.ˇ1 ˇ0/ &gt; X t j is large enough.</s><s xml:id="_zfhKtJq">A margin assumption is problematic in a mobile health setting where treatment effects are often expected to be small.</s></p><p xml:id="_AuGXZdb"><s xml:id="_nFP232d">Recently, Bastani and Bayati <ref type="bibr" target="#b23">[24]</ref> have extended Algorithm 1 to the high dimensional case where the vectors ˇa are sparse, i.e., the number kˇak 0 of nonzero elements in ˇa satisfies kˇak 0 D s p.</s><s xml:id="_2hb9mdg">They improve the O.p 3 log T/ regret rate to O.s 2 log 2 T C s 2 log T log p/ after making assumptions similar to those made by Goldenshluger and Zeevi.</s></p><p xml:id="_3gZJ5GH"><s xml:id="_TcFFVSH">Linearity of the expected reward function is not the only case that been considered for modeling the expected reward.</s><s xml:id="_GYbaqvE">Agarwal et al. <ref type="bibr" target="#b24">[25]</ref> consider a setting where the expected reward function is assumed to lie in a general class with finitely many members.</s><s xml:id="_6axfndE">However, extending their results to general, finite dimensional, expected reward function classes is an open problem.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_93hS5WF">Nonparametric Estimation of Expected Reward Functions</head><p xml:id="_zmgApUA"><s xml:id="_Hj4eQsj">Instead of assuming the linear model ( <ref type="formula" target="#formula_5">2</ref>), we can consider the model</s></p><formula xml:id="formula_8">R a t D f a .X t / C a t ;<label>(4)</label></formula><p xml:id="_82czKJr"><s xml:id="_xw4KY5p">where f a are functions chosen from a non-parametric class of functions, say, those satisfying certain smoothness conditions, and a t are iid mean-zero random variables.</s><s xml:id="_4Jk3AQz">Assume that the contexts are normalized such that X t 2 OE0; 1 p .</s><s xml:id="_QeUjRCF">Algorithm 2 Randomized Allocation with Nonparametric Estimation <ref type="bibr" target="#b12">[13]</ref> Inputs: n 0 (initial exploration length), NPR (nonparametric regression procedure such as nearest neighbor regression), t (sequence of exploration probabilities) for t D 1 to 2n 0 do Take action A t D 0 or A t D 1 depending on whether t is odd or even end for Get initial estimates b f a by feeding data from previous rounds to NPR</s></p><formula xml:id="formula_9">for t D 2n 0 C 1 to T do Let G t D argmax a b f a .X t / //</formula><p xml:id="_pkAucfp"><s xml:id="_DXdph5Y">greedy action Let E t D action selected at random // random exploration With probability .1 t / take action A t D G t , else A t D E t // -greedy Collect reward R t and feed into NPR to get updated estimate b f a for a D A t end for</s></p><p xml:id="_dUbZTuE"><s xml:id="_zdJNsV6">Yang and Zhu <ref type="bibr" target="#b12">[13]</ref> initiated the study of contextual bandits in this non-parametric setting and looked at the "competitive ratio":</s></p><formula xml:id="formula_10">P T tD1 f A t .X t / P T tD1 max a2A f a .X t / :</formula><p xml:id="_nZfUVrS"><s xml:id="_TGkXBxY">Their algorithm, given as Algorithm 2, estimates the functions f a using some non-parametric procedure such as the histogram method or the nearest neighbor method.</s><s xml:id="_8yq76F5">It selects actions using the so-called -greedy strategy.</s><s xml:id="_6r5qg9M">That is, with some small probability a random action is selected.</s><s xml:id="_mN7XnFf">Otherwise, the action that looks best according to the current estimates b f a is taken.</s></p><p xml:id="_Y6GPG8s"><s xml:id="_f2rjuaW">Assuming that f a is non-negative and continuous on OE0; 1 p and that D X has a density bounded away from zero, Yang and Zhu show that the competitive ratio of their contextual bandit algorithm converges to 1 almost surely, for both the histogram and nearest neighbor methods provided that the width of histograms and number of nearest neighbors are chosen in an appropriate manner as T ! 1.</s></p><p xml:id="_jtPew9D"><s xml:id="_Ws4vjRx">The results of Yang and Zhu are asymptotic and assume only continuity of the function f a .</s><s xml:id="_erePMcF">Assuming a smoothness condition of the form 8x; x 0 ; a; kf a .x/</s><s xml:id="_GhUMHNh">f a .x 0</s><s xml:id="_A8JKTk2">/j Ä L kx x 0 k ˇ;</s></p><p xml:id="_7VnPEpr"><s xml:id="_HMBJdJy">Rigollet and Zeevi <ref type="bibr" target="#b13">[14]</ref> gave finite sample expected regret bounds where the expected regret is still defined as in <ref type="bibr" target="#b2">(3)</ref> except that now</s></p><formula xml:id="formula_11">? .x/ D GREEDY.f a /.x/ D argmax a2A f a .x/:</formula><p xml:id="_arWyTQ8"><s xml:id="_QUYz2Rw">They also assumed a margin condition that controls the probability of observing a context where the treatment effect is non-zero but too small: there exists ı 0 2 .0;</s><s xml:id="_MUbEh3j">1/ such that 8ı 2 OE0; ı 0 /; 9C ı s:t:</s></p><formula xml:id="formula_12">P X D X OE0 &lt; jf 0 .X/ f 1 .X/j &lt; ı Ä C ı ı ˛:</formula><p xml:id="_8rbPZGN"><s xml:id="_A2ywCQ6">If ˛ˇ&gt; 1 then the optimal policy ?</s><s xml:id="_NK3R38d">does not depend on x and always pulls the same arm.</s><s xml:id="_CqqEZvu">Therefore, to ensure a non-trivial optimal policy, they assume that ˛ˇÄ 1.</s><s xml:id="_MuM3dtE">Their expected regret guarantees are polynomial in T where the exponent depends on the dimension p of the contexts, the margin parameter ˛and the smoothness parameter ˇ.</s><s xml:id="_jfPsx5s">They also provide almost matching lower bounds.</s><s xml:id="_m3EdVRq">Note these polynomial in T regret rates are much worse than the logarithmic rates in T achievable in the parametric case under margin assumptions.</s></p><p xml:id="_5d8MsYe"><s xml:id="_huy4RCc">Perchet and Rigollet <ref type="bibr" target="#b25">[26]</ref> extend the work of Rigollet and Zeevi to the case when the number of arms might be (much) larger than 2. They also extend the range of the margin parameter where the bounds hold and eliminate logarithmic gaps between upper and lower bounds.</s><s xml:id="_DU8bvk8">However, their algorithm requires knowledge of the smoothness parameter ˇ.</s><s xml:id="_AfztbWq">In practice, the smoothness parameter is not known.</s><s xml:id="_HVMCVAR">Qian and Yang <ref type="bibr" target="#b26">[27]</ref> show how to use "Lepski-type" procedures from the nonparametric function estimation literature to select the smoothness parameter ˇin a data-dependent way and still achieve (near) minimax regret bounds that would be obtained assuming that the smoothness is known in advance.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_C3bauWz">Competing Against a Policy Class</head><p xml:id="_9TuuJjZ"><s xml:id="_UZ8KaVH">In this section, we consider approaches that dispense entirely with the task of estimating the expected reward function.</s><s xml:id="_PBjtWtU">Instead they fix a class ˘of policies and aim to minimize the expected regret relative to the class ˘, which is defined as</s></p><formula xml:id="formula_13">T V. ? ˘/ T X tD1 EOER t ;<label>(5)</label></formula><p xml:id="_rQp3myr"><s xml:id="_9rpp5m7">where ?</s><s xml:id="_yvuPnJ2">˘is the best policy in ˘.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sDCV5Fw">Algorithm 3 Epoch Greedy Algorithm [15]</head><p xml:id="_Pg5UfRg"><s xml:id="_K38F7MW">Inputs: Function `.D/ that given a data set D, outputs the number of exploitation rounds to do next</s></p><formula xml:id="formula_14">D 0 D fg; t 1 D</formula><p xml:id="_kCBwhaP"><s xml:id="_8rUrCVf">1 for Epoch j D 1; 2; : : : do t D t j =? Single exploration step ?=</s><s xml:id="_VJ6N3dB">Select A t uniformly at random from A D j D D j 1 [ f.X t ; A t ; R t /g =? Update policy ?=</s><s xml:id="_pByEvFE">Compute b j D argmax 2˘P.x;a;r/2Dj r1 OE .x/</s><s xml:id="_pBdAx4k">D a =? Exploitation phase ?= t jC1 D t j C s.D j / C 1 for t D t j C 1 to t jC1 1 do Take action A t D b j .X t / end for end for</s></p><p xml:id="_kEuATcY"><s xml:id="_Uhwnghy">If the policy class ˘is finite (j˘j &lt; 1) and small enough that one enumerate all the policies at every time step, then the Exp4 algorithm, given later in section "Competing Against a Fixed Class of Policies", can be used.</s><s xml:id="_HzAzJTW">With two actions, it enjoys an expected regret bound of O.</s></p><p xml:id="_sSCPdTM"><s xml:id="_X4QhKcf">p T log j˘j/ in the fully adversarial setting where the context and reward triples are assumed to be completely arbitrary.</s><s xml:id="_b8Rz8Fp">If an algorithm enjoys an regret bound in the adversarial setting, it can be shown that it will also satisfy the same bound when the stochastic setting, i.e., when the contexts and rewards are generated by an iid process and regret is measured as in <ref type="bibr" target="#b4">(5)</ref> above.</s></p><p xml:id="_whpQWNh"><s xml:id="_TWeZBXN">If the policy class is huge or infinite, then enumeration of all policies is infeasible and Exp4 cannot be applied.</s><s xml:id="_Dt3t3cK">However, in the stochastic setting, one can use the "certainty equivalence with forcing" idea described in section "Parametric Estimation of Expected Reward Functions" above.</s><s xml:id="_ymNSJZB">Langford and Zhang's <ref type="bibr" target="#b14">[15]</ref> Epoch-Greedy algorithm (Algorithm 3) does just that.</s><s xml:id="_bdKKATa">On an exploration round, it takes one of the two actions at random with probability 1=2.</s><s xml:id="_R9rWF6R">After an exploration round, it builds an unbiased estimator of the value of any policy as:</s></p><formula xml:id="formula_15">b V. jD/ D 1 jDj X .x;a;r/2D 2r1 OE .x/ D a</formula><p xml:id="_Pb4ruUR"><s xml:id="_ukgAW9m">where D is the dataset consisting of context, action, reward triples from exploration rounds so far.</s><s xml:id="_r9PSAQg">Since each action is selected at random with probability 1=2 on exploration rounds, it is easy to see that E</s></p><formula xml:id="formula_16">h b V. jD/ i D V.</formula><p xml:id="_6UUwyCC"><s xml:id="_AUKaZwr">/ where the expectation is taken over the distribution of contexts and rewards as well as with respect to the algorithm's uniform randomization to select the actions on exploration rounds.</s><s xml:id="_p8D2fT7">The policy selected for the next exploitation phase is then simply</s></p><formula xml:id="formula_17">argmax 2˘b V. jD/ D argmax 2˘X .x;a;r/2D r1 OE .x/ D a :<label>(6)</label></formula><p xml:id="_dvDvwav"><s xml:id="_4tWdP7X">This is where the computational advantage of Epoch-Greedy comes in.</s><s xml:id="_bcExjxR">It never accesses the policies in ˘except via the operation above.</s><s xml:id="_HE2fba9">All we need is a computational blackbox or "oracle" that can answer the "argmax" queries above.</s></p><p xml:id="_KazaWXy"><s xml:id="_RdjQy65">Let us call such an oracle an AMO (for Arg Max Oracle).</s><s xml:id="_2REg67C">If a cost-sensitive classifier implementation exists for the class ˘then it can serve as an AMO.</s><s xml:id="_YErSRRT">Therefore, can even be infinite as long as an efficient AMO is available for it.</s><s xml:id="_Ku6EXdk">The regret bound of Epoch-Greedy, with a finite class ˘, is O.T 2=3 .log</s><s xml:id="_dwGPuKq">j˘j/ 1=3 /.</s><s xml:id="_BQrSjBv">This is obtained by having O.T 2=3 .log</s><s xml:id="_uqZg6SQ">j˘j/ 1=3 / epochs till time T resulting in the same number of AMO calls since exactly one AMO call is made per epoch.</s><s xml:id="_gzQVkPg">Langford and Zhang note that ˘need not be finite and that a similar regret bound can be shown for an infinite class with finite VC (Vapnik-Chervonenkis) dimension.</s><s xml:id="_GkXuP5y">Note that for such policy classes, the regret bound of any algorithm that depends on the cardinality of the policy class (such as the one for the Exp4 algorithm in section "Competing Against a Fixed Class of Policies" below) becomes vacuous.</s></p><p xml:id="_UKuUuka"><s xml:id="_rTavwc8">Epoch-Greedy's regret guarantee of O.T 2=3 .log</s><s xml:id="_yhzMuYy">j˘j/ 1=3 / might appear to be much worse than logarithmic regret guarantees presented in section "Parametric Estimation of Expected Reward Functions" above.</s><s xml:id="_8Mmz6Su">Recall that those guarantees were under additional assumptions such as margin conditions and the constants hidden in the O. / notation depend on distribution dependent parameters such as the margin parameter.</s><s xml:id="_xmGYbrW">Logarithmic regret guarantees for Epoch-Greedy are possible if one is willing to make additional assumptions and allow distribution dependent constants to appear in the regret guarantee.</s><s xml:id="_CxsWwqu">For instance, consider a finite policy class ˘such that there is a unique maximizer ? of the value V. / over in ˘.</s><s xml:id="_G9wfxVA">Let &gt; 0 denote the gap between the value of ? and that of the second-best policy:</s></p><formula xml:id="formula_18">D V. ? / max ¤ ? V. /:</formula><p xml:id="_hB8bNjG"><s xml:id="_tR2zfjm">Langford and Zhang show that Epoch-Greedy also enjoys a regret bound of O..log j˘j C log T/= 2 /.</s><s xml:id="_BjPfgg4">Note that this bound is logarithmic in T but blows up as !</s><s xml:id="_AJkZTfp">0.</s></p><p xml:id="_Xja8fyt"><s xml:id="_4GW3rgs">Dudik et al. <ref type="bibr" target="#b27">[28]</ref> gave an algorithm called RandomizedUCB that achieves</s></p><formula xml:id="formula_19">O. p T log.Tj˘j=ı/ C log.j˘j=ı//</formula><p xml:id="_YU4KyB2"><s xml:id="_szzrTdD">regret with probability at least 1 ı.</s><s xml:id="_4kUw76q">Moreover, it requires only polynomially many calls to the AMO at every round.</s><s xml:id="_QErYpmb">However, its practical utility is still limited as the polynomial involved is of moderately high degree (it invokes the AMO Q O.T 5 / times per round where Q O hides logarithmic factors).</s><s xml:id="_KtQFgzr">More recent work of Agarwal et al. <ref type="bibr" target="#b28">[29]</ref> has managed to bring down the total number of AMO calls to just O.</s></p><p xml:id="_XK2sVV4"><s xml:id="_pD3jFKX">p T= log.j˘j=ı// over all T rounds, with probability at least 1 ı, while still preserving the regret bound of RandomizedUCB.</s></p><p xml:id="_UhBnZ3d"><s xml:id="_d6cUqB3">The bandit algorithms discussed above appear quite attractive for use in mobile health due to the fast rate at which the regret decreases to 0. That is, user aggravation and disruption due to inappropriately timed delivery of intervention options would be minimized due to the fast rate at which the algorithm learns the best action for a given context.</s><s xml:id="_gJtnkeM">This is a critical point due to the high levels of app abandonment present in mobile health <ref type="bibr" target="#b29">[30]</ref>.</s><s xml:id="_jSwEWqZ">However these algorithms achieve these high learning rates under the assumption that the contexts and rewards are all generated from an iid process.</s><s xml:id="_bteqzmk">Suppose the context includes the user's stress level; user stress at different time points are clearly not independent.</s><s xml:id="_k7HBEHU">That is, a user who was stressed during the morning is more likely to be stressed in the afternoon than a user who was not stressed during the morning.</s><s xml:id="_aw52rqu">Also, stress at different time points are unlikely to be identically distributed.</s><s xml:id="_fC4eNvD">For example, the probability that a smoker is stressed on the day before she quits smoking is probably quite different from the probability that the same smoker is stressed on the day after she has quit smoking.</s><s xml:id="_MJwyaWQ">However, it may be that the noise level in the dynamics of the context will be sufficiently high so that a model assuming iid contexts and rewards provides a good approximation.</s><s xml:id="_uFvYQKD">Indeed Lei <ref type="bibr" target="#b30">[31]</ref> found that in simulated experiments mimicking mobile health studies, the regret of a bandit algorithm similar to those above is robust to dependence between contexts at different times.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fS7SVeE">Adversarial Contexts with Stochastic Rewards</head><p xml:id="_xzxaUdh"><s xml:id="_ArtrcR4">The assumption that the contexts are drawn iid from a fixed distribution is quite unrealistic in a lot of practical settings, including mobile health.</s><s xml:id="_jnwfRbP">Researchers have therefore considered a model where the contexts are arbitrary but the reward given context and action is still stochastic in the following sense.</s><s xml:id="_9BZ28qk">Let fD a .</s><s xml:id="_RBwMPJF">jx/ W x 2 X g, for a 2 f0; 1g, be two families of distributions over rewards indexed by the context x.</s><s xml:id="_hRZKXrR">Note that we are considering the case of two actions, i.e., A D f0; 1g.</s><s xml:id="_kvkFGPW">The following online protocol is followed.</s><s xml:id="_96WeMVG">The contexts are denoted by lower case letters to emphasize that they are not random variables but from an arbitrary deterministic sequence.</s></p><p xml:id="_pEcrFPt"><s xml:id="_3MWKYvu">1: nature generates fx t g T tD1 in advance 2: for t D 1 to T do 3: receive context x t 4: algorithm takes action A t 5:</s></p><p xml:id="_wKWXVB5"><s xml:id="_k6AmQ89">receive reward R t which is drawn from D A t .</s><s xml:id="_vsxyeeQ">jx t / 6: end for Let Á a .x/</s><s xml:id="_4N4FzGx">be the expected value of the distribution D a .</s><s xml:id="_SVv8xmJ">jx/.</s><s xml:id="_7n6Tk7r">The optimal policy is given by: ?</s><s xml:id="_N8zeakM">.x/</s><s xml:id="_QwkVxsB">D argmax a2A Á a .x/;</s><s xml:id="_CqZ8r7H">and we define the expected regret of an online learning algorithm as:</s></p><formula xml:id="formula_20">T X tD1 Á ? .x t / .x t / T X tD1 EOER t :</formula><p xml:id="_aVSQm8m"><s xml:id="_zW3SB4a">All regret bounds mentioned in this subsection hold uniformly over all possible sequences fx t g T tD1 of contexts (with some mild restrictions like boundedness of the contexts).</s></p><p xml:id="_NpwzYuj"><s xml:id="_TBAkAYF">Li et al. <ref type="bibr" target="#b16">[17]</ref> gave an algorithm called LinUCB that is based on the following linearity assumption: Á a .x/</s><s xml:id="_j2pANtM">D ˇ&gt; a x; <ref type="bibr" target="#b6">(7)</ref> where x; ˇa 2 R p .</s><s xml:id="_EecHRdn">LinUCB is here presented as Algorithm 4. It follows a long line of work in bandit algorithms that use upper confidence bounds for action selection.</s></p><p xml:id="_HPvW3R8"><s xml:id="_PyarePj">To each action's current estimate, it adds a confidence term which reflects the algorithm's current uncertainty about that estimate.</s><s xml:id="_PnTWmsm">The action selected is the one that maximizes the sum of the estimated reward and the confidence bound.</s></p><p xml:id="_8y6Pf9H"><s xml:id="_kAp7cY4">Algorithm 4 LinUCB Algorithm <ref type="bibr" target="#b14">[15]</ref> Inputs: ˛(tuning parameter used in computing upper confidence bounds)</s></p><formula xml:id="formula_21">A</formula><p xml:id="_aAmvYvG"><s xml:id="_ppXmskF">a D I p p ; b a D 0 p 1 for all a for t D 1 to T do Compute O ˇa D .A a / 1 b a for all a // ridge regression Compute U a D .</s><s xml:id="_HXdv5kz">O ˇa/ &gt; x t C ˛px &gt; t .A a / 1 x t for all a // upper confidence bound Take action A t D argmax a U a and observe reward R t For a D A t , update A a D A a C x t x &gt; t , b a D b a C R t x t end for</s></p><p xml:id="_pQjeyTN"><s xml:id="_bzPPWrs">LinUCB performs well empirically as demonstrated by Li et al. in the context of personalized news article recommendations on the web.</s><s xml:id="_vxSSygP">However, its theoretical analysis is complicated by the fact that its estimates are not based on iid samples (recall the reward depends on the action and the action is selected using data on prior rewards and prior actions) and there are no known regret bounds.</s><s xml:id="_rjCZJUK">Chu et al. <ref type="bibr" target="#b31">[32]</ref> provide an algorithm called SupLinUCB that calls BaseLinUCB as a subroutine and show that it enjoys a regret bound of O p Tp log 3 .T log T=ı/ Á with probability at least 1 ı.</s><s xml:id="_3KvPgdm">The idea of taking a basic procedure like BaseLinUCB, whose statistical analysis is simplified by assuming independence among the samples, and then using a master algorithm SupLinUCB to ensure the assumption holds, goes back to the work of Auer <ref type="bibr" target="#b32">[33]</ref>.</s><s xml:id="_Fyy3p73">His work also considered arbitrary context vectors with linear expected reward functions as in <ref type="bibr" target="#b6">(7)</ref> and followed some early line of work in the computer science literature <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b33">34]</ref>.</s><s xml:id="_WnVx3sd">His basic and master algorithms were called LinRel and SupLinRel.</s><s xml:id="_jJjuBq6">SupLinRel was also shown to enjoy a regret bound of O.</s></p><p xml:id="_Jd3KJwp"><s xml:id="_qPR2FDS">p Tp log 3 .T log T=ı// with probability at least 1 ı.</s><s xml:id="_3bpkf6J">However, LinUCB has practical advantages over LinRel.</s><s xml:id="_pQrpDdz">It is easier to implement and numerically more stable as it relies on ridge regression as its computational core and not on full eigendecompositions like LinRel.</s><s xml:id="_H428zUe">We would like to also point out that LinUCB has been generalized from the standard linear setting as in <ref type="bibr" target="#b6">(7)</ref> to the generalized linear setting <ref type="bibr" target="#b34">[35]</ref> for use with non-continuous rewards such as binary rewards.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Zc49juB">Nonlinear Expected Reward Functions</head><p xml:id="_efRca5Q"><s xml:id="_sZQaK8J">Readers familiar with the literature on kernel methods and support vector machines in machine learning will recall that these methods deal with non-linearity by embedding the contexts x t into a high, or even infinite, dimensional space via a feature mapping .x</s><s xml:id="_yMutuUn">t / 2 H K , where H K is a reproducing kernel Hilbert space (RKHS) corresponding to the kernel K.x; x 0 / D h .x/;</s><s xml:id="_3kfkxyR">.x 0</s><s xml:id="_pWFmcWS">/i.</s><s xml:id="_EsRPqsy">The kernel K thus measures similarity between contexts using the inner product in a higher dimensional space.</s><s xml:id="_8BPv2Cx">LinUCB has been extended to the RKHS setting by Valko et al. <ref type="bibr" target="#b35">[36]</ref>.</s><s xml:id="_JmDj5aq">They also provided regret bounds that depend on the "effective dimension" which is, roughly speaking, the number of principal dimensions in which the embedded data points in the RKHS are mostly contained.</s></p><p xml:id="_9nzRx3V"><s xml:id="_cVHsDmD">Other work on contextual bandits with arbitrary contexts and non-linear expected reward functions includes the Query-ad clustering algorithm of Lu et al. <ref type="bibr" target="#b36">[37]</ref> and the RELEAF algorithm of Tekin and van der Schaar <ref type="bibr" target="#b37">[38]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wmpvE2z">Thompson Sampling</head><p xml:id="_pXeatJa"><s xml:id="_2aYs2TG">Thompson sampling, also called "posterior sampling" <ref type="bibr" target="#b38">[39]</ref> or "probability matching" <ref type="bibr" target="#b39">[40]</ref>, is a Bayesian approach to designing online learning algorithms for bandit problems.</s><s xml:id="_YjHhjHC">In the linear setup as in <ref type="bibr" target="#b6">(7)</ref> above, it involves choosing prior distributions for the unknown reward parameters ˇa and choosing conditional distributions for the rewards given context and action.</s><s xml:id="_4BkYrVb">Algorithm 5 chooses the prior to be a multivariate normal distribution with mean zero and covariance matrix 2 I p p .</s><s xml:id="_erC8qmm">It also assumes that the reward given context x and action a is drawn from a normal distribution with mean ˇ&gt; a x and variance 2 .</s><s xml:id="_GHbWw57">At every time step, it draws samples e ˇa from the posterior distribution for ˇa and chooses the action with the highest mean e ˇ&gt; a x t according to the drawn posterior samples.</s><s xml:id="_ZFrrcWR">Once the action is taken and the corresponding reward observed, it updates the posterior distribution for the corresponding reward parameter.</s></p><p xml:id="_hfRgASU"><s xml:id="_9qybJ8r">Agrawal and Goyal <ref type="bibr" target="#b40">[41]</ref> analyze Algorithm 5 and prove a regret bound of O Â p q T 1C .log</s><s xml:id="_CYfQueE">T log.1=ı//</s><s xml:id="_4saU2X9">Ã with probability 1 ı.</s><s xml:id="_K2TA4MB">Here 2 .0;</s><s xml:id="_jdVh7YP">1/ is a tuning parameter.</s><s xml:id="_MT8XHmx">Thompson sampling had been applied to contextual bandits [42] before Agrawal and Goyal's work but finite time regret bounds were not available.</s><s xml:id="_FHxQa3Y">Agrawal and Goyal's regret analysis holds under much weaker assumptions that made to derive the Thompson Sampling algorithm itself.</s><s xml:id="_w5MMsF4">First, the regret analysis itself Algorithm 5 Thompson Sampling Algorithm [15] Inputs: 2 (variance parameter used in the prior and in the reward linear model) A a D I p p ; b a D 0 p 1 for all a for t D 1 to T do Compute O ˇa D .A a / 1 b a for all a Sample e ˇa from NORMAL.</s><s xml:id="_mnCDEfA">O ˇa; 2 .A a / 1 / for all a // Sample from the posterior Take action A t D argmax a .</s><s xml:id="_RwgS22P">e ˇa/ &gt; x t and observe reward R t For a D A t , update A a D A a C x t x &gt; t , b a D b a C R t x t end for</s></p><p xml:id="_tt6MnWK"><s xml:id="_mzPMmTf">makes no use of the prior.</s><s xml:id="_ga3JqVe">It holds for every ˇa choice as long as it is bounded.</s><s xml:id="_YWCqWb8">Second, it does not assume that the rewards are actually drawn from a normal distribution.</s><s xml:id="_CwXcMBn">It does require the linearity assumption in <ref type="bibr" target="#b6">(7)</ref> to hold but the rewards are only assumed to be sub-gaussian.</s></p><p xml:id="_qgH7CCk"><s xml:id="_mk5ajWB">As discussed above, this section does not require that the contexts are iid.</s><s xml:id="_VkTmFcC">Thus the bandit algorithms considered here can accommodate settings in which the contexts can have arbitrary relationships one with another.</s><s xml:id="_ggrVWzx">Despite this, as discussed above, for some algorithms one can guarantee how fast the algorithm learns with time.</s><s xml:id="_MVKaENz">This may be useful in mobile health particularly in areas of science where the dynamic evolution of the contexts over time are not yet well understood, for example when the context includes craving for substances or alternately physiological and perceived stress.</s><s xml:id="_sDnm8j9">However, this setting continues to be potentially problematic in that how users respond to interventions (e.g., reward distribution given context) can change with time.</s><s xml:id="_D8rQ4XY">For example, the relationship between self-efficacy and relapse to smoking appears to change as time increases from the quit date <ref type="bibr" target="#b42">[43]</ref>; this is likely to mean that the distribution of the reward as a function of an intervention option and a context involving self-efficacy is likely to change with time as well.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jQsnqN2">Fully Adversarial Contextual Bandits</head><p xml:id="_qE7yGee"><s xml:id="_QRjQXaa">In this section, we further relax our assumptions on how the contexts and rewards are generated.</s><s xml:id="_mwW5xaW">First, we consider a setting where the adversary chooses a sequence of contexts and reward distributions.</s><s xml:id="_zkCg3WE">In this setting, the aim is do well with respect to a policy that knows the sequence of distributions in advance.</s><s xml:id="_2ZUaGfn">Second, we consider a setting where the adversary chooses a sequence of contexts and reward values.</s><s xml:id="_B6d55qJ">In this setting, the aim is do well with respect to a pre-defined class ˘of policies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_aG8Yma5">Competing Against Greedy Policies with Changing Reward Distributions</head><p xml:id="_wza6VAE"><s xml:id="_smNUzK7">Here the context sequence as well as the sequence of reward distributions given context and action are chosen arbitrarily.</s><s xml:id="_2shYT2n">Denote the choice of the action a's reward distribution given context x at time t by D a t .</s><s xml:id="_UGtDbbE">jx/.</s><s xml:id="_dcctDPX">Denote the expected reward under this distribution by Á a t .x/.</s><s xml:id="_Z7XDjnC">Consider the following online protocol.</s></p><p xml:id="_YgQVveH"><s xml:id="_e2xbb4B">1: nature generates f.x t ; D 0 t .</s><s xml:id="_UWDBPDB">jx/; D 1 t .</s><s xml:id="_dbtpC5f">jx//g T tD1 in advance 2: for t D 1 to T do 3: receive context x t 4: algorithm takes action A t 5:</s></p><p xml:id="_2VHYDbx"><s xml:id="_xHcetAD">receive reward R t drawn from the distribution D A t .</s><s xml:id="_wfTCc7V">jx t / with expectation Á A t t .x</s><s xml:id="_FynEHZc">t / 6: end for At the end of T rounds, the time-average of the expected reward functions for action a played by nature is</s></p><formula xml:id="formula_22">N Á a .x/ D 1 T P T tD1 Á a t .</formula><p xml:id="_gnCuw2J"><s xml:id="_hsU9Q8V">x/.</s><s xml:id="_rGnRHec">The regret definition below compares the learning algorithm's expected reward to that of the greedy policy with respect to N Á a :</s></p><formula xml:id="formula_23">? .x/ D GREEDY. N Á a /.x/ D argmax a2A T X tD1 Á a t .</formula><p xml:id="_xFhPs5F"><s xml:id="_HnEq8uq">x/:</s></p><p xml:id="_qvJ9vJa"><s xml:id="_YcdQKMw">Regret is now defined as</s></p><formula xml:id="formula_24">T X tD1 Á ? .x t / t .x t / T X tD1 EOER t :</formula><p xml:id="_SqUWzwk"><s xml:id="_MCUzAdk">Note that in the protocol above, there are two sources of randomness.</s><s xml:id="_qHCkeme">First, there is randomness in nature's realization of the rewards unless the distributions D a t .</s><s xml:id="_cSYYvPE">jx/ are point masses.</s><s xml:id="_um4jNwg">Second, the online learning algorithm may be a randomized one and could be using additional randomization to select its actions A t .</s><s xml:id="_cws3DJk">The expectation above is with respect both possible sources of randomness.</s></p><p xml:id="_jTdBVGW"><s xml:id="_BxpYzPe">We know of only one paper that gives bandit algorithms in this setting.</s><s xml:id="_AQSbPQM">Slivkins <ref type="bibr" target="#b19">[20]</ref> considers this setting in Section 7 of his paper.</s><s xml:id="_m87yY6h">In this chapter, we have mostly focused on the case of two actions, i.e., A D f0; 1g and our context space X has often been a subset of R p .</s><s xml:id="_NKdT46v">He considers a much more general case when A ; X are metric spaces.</s><s xml:id="_tTtNVYY">In our specific setting, his assumptions on the expected reward functions is that they are Lipschitz with respect to some norm k k defined on R p : 8t; x; x 0 ; a; a 0 ; jÁ a t .x/</s><s xml:id="_rzZPSK7">Á a t .x 0</s><s xml:id="_33Htkan">/j Ä kx x 0 k:</s></p><p xml:id="_ukWPsDJ"><s xml:id="_hFVxba8">His algorithm achieves a regret bound of O.T 1 1=.2Cd</s><s xml:id="_Pqtr65P">X / .log</s><s xml:id="_QWWNvex">T// where d X is the covering dimension of the context space X under the metric kx x 0 k.</s><s xml:id="_HCVt4Yb">Note that the covering dimension of a metric space is defined as the smallest integer d such that the number of balls of radius r required to cover the space is O.r d /.</s><s xml:id="_2b74ZkF">When X Â R p is a bounded set, we always have d X Ä p.</s></p><p xml:id="_CeACFAc"><s xml:id="_UjGYGUx">1: nature generates f.x t ; r 0 t ; r 1 t /g T tD1 in advance 2: for t D 1 to T do 3: receive context x t 4: algorithm takes action A t 5: receive reward R t D r A t t 6: end for Regret is now defined as max 2˘T X tD1 r .x</s><s xml:id="_Uf6Rxk9">t / t T X tD1 EOER t :</s></p><p xml:id="_4qhVrUD"><s xml:id="_hdHmzuk">Regret bounds in the adversarial setting hold uniformly over all choices of the context, reward sequence f.x t ; r 0 t ; r 1 t /g T tD1 .</s><s xml:id="_YV5j737">A special case of the above setup when there is only one unchanging context, x t D x, reduces to the adversarial multi-armed bandit problem with K arms (we have focused on the K D 2 case in this chapter).</s><s xml:id="_uhZEF7h">This problem was first considered by Auer et al. <ref type="bibr" target="#b43">[44]</ref>.</s><s xml:id="_FP2XUMQ">Their Exp3 algorithm obtains an expected regret bound of O.</s></p><p xml:id="_qfHk9Aa"><s xml:id="_RNf6BdF">p KT log K/ which can be improved to O. p KT/ using a different algorithm <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref>.</s><s xml:id="_B3atPsN">They also present a variant Exp3.P that enjoys a bound on the regret not just in expectation but with high probability.</s><s xml:id="_vMHrKtV">More interesting in the contextual bandit setting is their Exp4 algorithm.</s><s xml:id="_BffFwJE">Exp4 applies in the case when there are a finite number of "experts" each suggesting an action to take at a given round.</s><s xml:id="_Es6W2fu">We can identify their experts with policies in the set ˘if the set is finite.</s><s xml:id="_kxBYgUv">We present the Exp4 algorithm as Algorithm 6.</s><s xml:id="_uzZPnCK">They prove an expected regret bound of O.</s></p><p xml:id="_ff9ptKp"><s xml:id="_mjXMAMf">p KT log.j˘j// for Exp4 which reduces to O. p T log j˘j/ when K D 2. Even though the regret bound can tolerate very large policy classes, the implementation of the algorithm itself is practical only for very small policy classes since Exp4 maintains a weight for each policy in the class.</s></p><p xml:id="_ejgwSnd"><s xml:id="_pzBsskV">High probability guarantees matching those of Exp4 have been obtained by Beygelzimer et al. <ref type="bibr" target="#b46">[47]</ref> using their Exp4.P algorithm.</s><s xml:id="_hSwHdq6">Note that the same paper also presents an algorithm VE for the stochastic setting when the context and reward tuples are drawn iid from a fixed distribution.</s><s xml:id="_zzCZkhX">Even if ˘is an infinite class but the VC dimension of ˘is d &lt; 1, VE enjoys a regret bound of O. p dT log.T=.dı/// with probability at least 1 ı.</s></p><p xml:id="_wNJHSYN"><s xml:id="_HqkrQ4s">At least conceptually, the Exp family algorithms provided in this section appear rather promising because they require the least restrictive assumptions on the rewards in order to learn.</s><s xml:id="_cPcy7Cv">However these algorithms, because they are designed to Algorithm 6 Exp4 Algorithm <ref type="bibr" target="#b14">[15]</ref> Inputs: 2 .0; 1 (learning rate/step size; also used an exploration parameter)</s></p><formula xml:id="formula_25">w</formula><p xml:id="_tEdpPVC"><s xml:id="_kPE4xKj">D 1 for all 2 ˘// set equal weights for all policies initially for t D 1 to T do Compute W D P 2˘w =? convert policy weight into action probabilities ?=</s><s xml:id="_dh4PhWy">For all a, compute p a D .1 / 1 W P 2˘w 1 OE .x</s><s xml:id="_PMjSXhY">t / D a C =2 Choose A t D a with probability p a and observe reward R t Set O r a D R t =p a if A t D a and 0 otherwise // estimate rewards for both actions For all 2 ˘, set w D w exp O r .xt/</s><s xml:id="_KwybwE8">=2 // update policy weights end for</s></p><p xml:id="_jCEvT3G"><s xml:id="_mNNuWsm">work in the worst cases, may learn too slowly for a large subset of a particular population such as the population of smokers who are trying to quit.</s><s xml:id="_W6cuMaT">At this time, we do not have good rules of thumb for selecting the type of algorithm to employ for optimizing mobile health interventions depending on the type of populations and behavior change problem.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wEYxcUP">Challenges in Mobile Health Applications</head><p xml:id="_CXHMu7y"><s xml:id="_sh9vvXv">We have seen that a wide variety of contextual bandit algorithms and theoretical frameworks to analyze them already exist in the literature.</s><s xml:id="_KKnhhWt">These ideas serve as useful starting points for the design of online learning algorithms in mobile health.</s><s xml:id="_scncq3K">However, to truly make an impact in mobile health, significant work needs to be done to deal with challenges that arise in the mobile health setting.</s><s xml:id="_kWFVsE3">In this section, we consider some of these challenges and explore ways to start addressing them.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_dRWdpwd">Finding a Good Initial Policy</head><p xml:id="_6tZNXhY"><s xml:id="_eF889Br">Good initialization of the learning process is very important.</s><s xml:id="_DgS3tET">If the algorithm chooses very bad actions in the beginning, it can have a negative impact on health outcomes and user engagement.</s><s xml:id="_k86JBNQ">One possibility is to consult domain experts and use an expert derived policy at the start.</s><s xml:id="_NwR5kDQ">However, it might turn out to be difficult to turn intuitive judgements of domain experts into a precisely stated policy.</s><s xml:id="_9jvCYTc">Moreover, mobile health is a relatively new area and often domain experts lack sufficient knowledge of what works and what does not when interventions are delivered through mobile devices and wearables.</s></p><p xml:id="_ZvrXzsP"><s xml:id="_nqcXGeh">We think that it is much better to proceed in an evidence-based manner and initialize the policy using data previously gathered, say in a microrandomized trial <ref type="bibr" target="#b47">[48]</ref>.</s><s xml:id="_ahQEfVZ">Data from a microrandomized trial can be used for a variety of purposes including estimation of the value of a policy in question.</s><s xml:id="_ShgMvg7">If candidate policies can be evaluated then a good one can be selected from a set of policies.</s><s xml:id="_9xarhtp">Microrandomized trials offer very high-quality data.</s><s xml:id="_RbA4dB2">But even less high quality data can be useful.</s><s xml:id="_TDWnUyY">For example, if the policy that generated data in a mobile health study is exactly or partially known then one can still form reasonable estimates of the value of a given policy.</s><s xml:id="_uXGchWT">The problem of using an existing batch of data gathered under one policy to reason about the value of another policy is called the problem of "offline learning" or "offline evaluation".</s><s xml:id="_BgR8Ujw">There is work in both the computer science <ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref>, as well as the statistics literature on this problem <ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref><ref type="bibr" target="#b55">[56]</ref><ref type="bibr" target="#b56">[57]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cCTFRNy">Interpretability of the Learned Policy</head><p xml:id="_C8wekz7"><s xml:id="_fK2Ps6z">Progress in mobile health will occur when human-computer interface researchers, machine learning researchers and statisticians work in close collaboration with domain scientists such as behavioral scientists.</s><s xml:id="_UUZP4V4">On the one hand, we need guidance from theories of behavior change to guide the development of mobile health interventions.</s><s xml:id="_Z9dNqa8">On the other hand, the policy learned using online learning algorithms needs to be communicated back to behavioral scientists so that they can interpret it in light of their theories or use it to change and refine existing theories.</s><s xml:id="_v7eaBMc">This communication is facilitated by learning interpretable policies.</s><s xml:id="_f6X593D">Using policies represented by large decision trees, deep neural networks or kernel methods may not lend themselves easily to interpretation.</s></p><p xml:id="_mBT3hpU"><s xml:id="_YPhCUad">Lei <ref type="bibr" target="#b30">[31]</ref> has explored the use of actor-critic methods from the reinforcement learning literature in setting of contextual bandits.</s><s xml:id="_SwPJ8jd">The critic part is responsible for estimating the expected reward function and can use very flexible non-parametric and non-linear regression methods.</s><s xml:id="_qaVrMvS">The actor part is responsible for generating a policy using the estimates provided by the critic.</s><s xml:id="_Zv8qabc">Since only the policy needs to be communicated to the domain scientist, we just need to keep the actor architecture simple by choosing a low-dimensional interpretable policy parameterization.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PZuqnm9">Assessing Usefulness of Contextual Variables</head><p xml:id="_xFKqZsa"><s xml:id="_t27KwDQ">Contextual variables in mobile health are often costly to acquire.</s><s xml:id="_Ake3YUX">If they are passively sensed by the phone (e.g., GPS location) or a wearable (e.g., heartrate), acquiring them drains the battery.</s><s xml:id="_trzFXRY">If they are actively acquired by asking the user a self-report question (e.g., about their mood), acquiring them incurs user burden.</s><s xml:id="_T7FDNzr">Therefore, it is important to develop methods that enable researchers to decide whether or not a contextual variable is useful for deciding which intervention to deliver.</s><s xml:id="_zZpSENS">For example, suppose we use the following interpretable parameterization for a stochastic policy .x/</s><s xml:id="_gRfsFHZ">D exp.ˇ&gt;x/ 1 C exp.ˇ&gt;x/ :</s></p><p xml:id="_DEnENZH"><s xml:id="_WKhgJfN">Note that maps the context to OE0; 1 instead of f0; 1g and should be interpreted as the probability to taking action 1.</s><s xml:id="_43hd7dU">This is called the "Gibbs" or the "expit" parameterization.</s><s xml:id="_QwjPj6S">If we simply output an estimate b ˇat the end of the learning process, it is not very useful for assessing usefulness of variables.</s><s xml:id="_PHMfVKA">We need to provide confidence intervals for these estimates.</s><s xml:id="_v9DW6qA">Then, we can see whether a 95% confidence interval for, say, b ˇ1, contains zero or not.</s><s xml:id="_GCCMUqp">This will provide researchers with an evidence-based method to decide whether the first contextual variable in the context x is useful or not.</s><s xml:id="_TbvGnGA">We have not seen many tools to enable such reasoning in existing contextual bandit algorithms.</s><s xml:id="_2t8kWQB">An exception is the work of Lei <ref type="bibr" target="#b30">[31]</ref> mentioned above that does construct confidence intervals for the policy parameters estimated using their actor-critic online learning algorithm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GNnuUPV">Computational Considerations</head><p xml:id="_qTcFu9d"><s xml:id="_6rhGsbe">Computation on mobile phones consumes resources.</s><s xml:id="_vYYGvNN">If we perform computations on the phone we need to think about implementing the learning algorithms very efficiently in order to not put an undue burden on the phone's performance and battery life.</s><s xml:id="_hcKYPUQ">If we perform computations on the cloud, we need to minimize data transfer between the phone and the cloud to save the phone's resources.</s><s xml:id="_K5RVbC3">We also need to take into account occasional failures, due to a bad network reception or drained battery.</s><s xml:id="_YBsRtNX">These failures can cause the learning algorithm to not be able to push fresh data to the cloud or pull the latest policy or action recommendation from the cloud.</s><s xml:id="_9aDtJb5">There is little work on designing and proving guarantees about contextual bandit algorithms that are resilient to such failures.</s></p><p xml:id="_QApgmXp"><s xml:id="_bTecUYJ">Another question that needs work is how to tradeoff the frequency of learning with the noise level in the data.</s><s xml:id="_FWrpV2U">All algorithms presented above make an update whenever an action is selected and a reward is observed.</s><s xml:id="_RBd2dFF">If the data is very noisy then we might have the learning algorithm update its policy at larger time intervals so as to acquire more information.</s><s xml:id="_4JxPB9U">What should be the time intervals at which our learning algorithm updates the policy?</s><s xml:id="_UbuNWu4">To answer this question, one will have to consider the computational complexity of the update as well the amount (governed perhaps by a step size parameter) by which a single update changes the policy.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RANqFGj">Robustness to Failure of Assumptions</head><p xml:id="_4R4zYVB"><s xml:id="_NCkB9CR">Algorithms designed for the worst-case adversarial framework can perform suboptimally when data is actually generated stochastically.</s><s xml:id="_ZE999s5">Algorithms that have guarantees under stochastic assumptions can behave badly when the specific stochastic assumptions underlying their analysis are not met.</s><s xml:id="_RGXeDbz">In mobile health, where the consequence of such non-robustness is worse health outcomes for people, we need to pay serious attention to such issues.</s><s xml:id="_VqepnQR">Three assumptions that make repeated appearance in the theoretical analyses of contextual bandit algorithms are independence, stationarity, and absence of the impact of actions on the user's future contexts.</s><s xml:id="_GwXvYPK">Any candidate online learning algorithm needs to be tested for reasonable departures from these ideal assumptions in simulations before being deployed in a real study with users.</s><s xml:id="_5rnJ6EB">Existing algorithms need to be analyzed under weaker assumptions, if possible.</s><s xml:id="_maeVvtz">Otherwise, attempts should be made to quantify the degradation in performance in non-ideal settings.</s><s xml:id="_fk9b4Mm">New algorithms that are more robust to failure of assumptions need to be designed and associated guarantees provided.</s></p><p xml:id="_bB9ubTS"><s xml:id="_UWWrpYd">Some contextual bandit algorithms enjoy regret guarantees only in expectation.</s><s xml:id="_CHXeVHT">But an algorithm whose regret is small in expectation, but has high variance, can have very serious consequences in mobile health.</s><s xml:id="_k2Xn2Z2">High variance in regret means that occasionally, the algorithm performs very poorly and its regret is much larger than the provided guarantee.</s><s xml:id="_tJd33WB">The will translate into adverse health outcomes for some people in the cohort being studied.</s><s xml:id="_cUNADDx">High probability guarantees on the regret are better than guarantees in expectation but they are simply the first step in the direction of designing learning algorithms that better manage the risk of hurting people's health outcomes.</s><s xml:id="_SpM6Wzj">There is some work on risk-aversion in multi-armed bandit problems <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59]</ref>.</s><s xml:id="_wxgZRTh">It is possible that some of the techniques developed there can be useful for contextual bandit learning algorithms too.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_twZKAk5">Costly to Acquire or Missing Contexts and Rewards</head><p xml:id="_rmqj6WQ"><s xml:id="_pqpGB9u">As noted above, contextual variables can be costly to acquire in a mobile health setting.</s><s xml:id="_pBSnDbQ">Even rewards can be costly to acquire especially if they cannot be passively sensed and we have to rely on user self-reports.</s><s xml:id="_rubPqVd">If a variable is indeed useful for decision-making then choosing not to acquire it will lead to sub-optimal decisions.</s><s xml:id="_xAvRu9U">Similarly, we cannot simply decide to not acquire the reward variable because doing so will hamper the ability of the learning algorithm to learn from observed rewards.</s><s xml:id="_wDC5jpB">The key is to acquire costly variables judiciously.</s><s xml:id="_EgHRDRV">We can maintain predictions of such variables and acquire them only when uncertainty about them increases beyond a threshold.</s><s xml:id="_gJ635x6">If the costs associated with acquisition can be quantified then it can be formally included in the definition of regret.</s><s xml:id="_8YfRUrS">Currently, we do not have much guidance on how to deal with costly to acquire contexts and rewards.</s></p><p xml:id="_8h2g8MY"><s xml:id="_HNcpegv">Another aspect not treated properly in the existing literature is missingness of contextual variables and rewards.</s><s xml:id="_kDzZSea">Maintaining predictions of variables that can be potentially missing, of course, helps.</s><s xml:id="_TjrDgrT">However, missingness of self-reported data can also indicate one or more of the following: high user stress, high user busyness and low user engagement.</s><s xml:id="_sbTxmhQ">Thus, missingness can itself be used as a contextual variable to use in decision-making.</s><s xml:id="_DUmB2kX">More research is needed to fully integrate support for missing data in existing contextual bandit algorithms.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1: at a given decision point do 2 :</head><label>2</label><figDesc><div><p xml:id="_wfYFF7R"><s xml:id="_XcwbsQ8">mobile phone collects tailoring variables (the context) 3: a decision rule (or policy) maps the tailoring variables into an intervention option (the action) 4:</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc><div><p xml:id="_gbJxgsd"><s xml:id="_AMpmbpb">Instead of estimating the underlying expected reward function, one can instead simply try to compete with ?</s><s xml:id="_SxM9sGU">˘.</s></p></div></figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xml:id="_qBRCPtj"><p xml:id="_gN7CU9R"><s xml:id="_taX4faJ">Acknowledgements This work was supported by awards <rs type="grantNumber">R01 AA023187</rs> and <rs type="grantNumber">R01 HL125440</rs> from the <rs type="funder">National Institutes of Health</rs>, and <rs type="funder">CAREER</rs> award <rs type="grantNumber">IIS-1452099</rs> from the <rs type="funder">National Science Foundation</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mcUXF2v">
					<idno type="grant-number">R01 AA023187</idno>
				</org>
				<org type="funding" xml:id="_mQdMuMa">
					<idno type="grant-number">R01 HL125440</idno>
				</org>
				<org type="funding" xml:id="_8GyQNqB">
					<idno type="grant-number">IIS-1452099</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_235nHDZ">Competing Against a Fixed Class of Policies</head><p xml:id="_JarmpDm"><s xml:id="_HSETtr7">In the previous section, the policy we compete against was indirectly defined by the expected reward functions played by nature.</s><s xml:id="_PV4KYtn">Here we fix a class ˘of policies in advance and try to compete with the best policy in ˘.</s><s xml:id="_EJkw93Q">The protocol is now as follows.</s><s xml:id="_GTW9aZ2">Note that now nature generates arbitrary contexts and reward values for the two actions.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_94cTAJB">Multi-armed bandit allocation indices</title>
		<author>
			<persName><forename type="first">John</forename><surname>Gittins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Glazebrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470980033</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">John Gittins, Kevin Glazebrook, and Richard Weber. Multi-armed bandit allocation indices. John Wiley &amp; Sons, 2011.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_Th4n9bM">A one-armed bandit problem with a concomitant variable</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Woodroofe</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1979.10481033</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZwgnaA4">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">368</biblScope>
			<biblScope unit="page" from="799" to="806" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the American Statistical Association, 74(368):799-806, 1979.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_7VCStW7">Bandit problems with side observations</title>
		<author>
			<persName><forename type="first">Chih-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vincent Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UN3vZeU">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="355" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>Automatic Control</note>
	<note type="raw_reference">Chih-Chun Wang, Sanjeev R. Kulkarni, and H. Vincent Poor. Bandit problems with side observations. Automatic Control, IEEE Transactions on, 50(3):338-355, 2005.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_jVXSe9x">Arbitrary side observations in bandit problems</title>
		<author>
			<persName><forename type="first">Chih-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vincent Poor</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.aam.2004.10.004</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_y7jGZyp">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="903" to="938" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chih-Chun Wang, Sanjeev R. Kulkarni, and H. Vincent Poor. Arbitrary side observations in bandit problems. Advances in Applied Mathematics, 34(4):903-938, 2005.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_rQCecms">A note on performance limitations in bandit problems with side information</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Goldenshluger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Zeevi</surname></persName>
		</author>
		<idno type="DOI">10.1109/tit.2011.2104450</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_u8bwGDY">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1707" to="1713" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Information Theory</note>
	<note type="raw_reference">Alexander Goldenshluger and Assaf Zeevi. A note on performance limitations in bandit problems with side information. Information Theory, IEEE Transactions on, 57(3):1707-1713, 2011.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_fhwKP59">Associative reinforcement learning using linear probabilistic concepts</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_A57vBRr">Proceedings of the Sixteenth International Conference on Machine Learning</title>
		<meeting>the Sixteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
	<note type="raw_reference">Naoki Abe and Philip M. Long. Associative reinforcement learning using linear probabilistic concepts. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 3-11, 1999.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_XuSUrsz">Associative reinforcement learning: A generate and test algorithm</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1022642026684</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JCw9Cvn">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="319" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Leslie P. Kaelbling. Associative reinforcement learning: A generate and test algorithm. Machine Learning, 15(3):299-319, 1994.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_fzksYqa">Associative reinforcement learning: Functions in k-DNF</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1022689909846</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QhpPqS9">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="298" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Leslie P. Kaelbling. Associative reinforcement learning: Functions in k-DNF. Machine Learning, 15(3):279-298, 1994.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_EtnAVVk">Reinforcement learning with immediate rewards and linear hypotheses</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Biermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00453-003-1038-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wg6HbuQ">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="263" to="293" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Naoki Abe, Alan W. Biermann, and Philip M. Long. Reinforcement learning with immediate rewards and linear hypotheses. Algorithmica, 37(4):263-293, 2003.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_Et7C4ne">Experienceefficient learning in associative bandit problems</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Mesterharm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haym</forename><surname>Hirsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tCefXNY">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="889" to="896" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander L. Strehl, Chris Mesterharm, Michael L. Littman, and Haym Hirsh. Experience- efficient learning in associative bandit problems. In Proceedings of the 23rd international conference on Machine learning, pages 889-896. ACM, 2006.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_zhFSCpa">Covariate models for Bernoulli bandits</title>
		<author>
			<persName><forename type="first">Murray</forename><forename type="middle">K</forename><surname>Clayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_B8VRTXE">Sequential Analysis</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="426" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Murray K. Clayton. Covariate models for Bernoulli bandits. Sequential Analysis, 8(4):405-426, 1989.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_gSjyQJB">One-armed bandit problems with covariates</title>
		<author>
			<persName><forename type="first">Jyotirmoy</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4ez6gqY">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1978" to="2002" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jyotirmoy Sarkar. One-armed bandit problems with covariates. The Annals of Statistics, pages 1978-2002, 1991.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_mn52dAn">Randomized allocation with nonparametric estimation for a multiarmed bandit problem with covariates</title>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1015362186</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_czFTGQc">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="121" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yuhong Yang and Dan Zhu. Randomized allocation with nonparametric estimation for a multi- armed bandit problem with covariates. The Annals of Statistics, 30(1):100-121, 2002.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_U6Frxjc">Nonparametric bandits with covariates</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Rigollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wTrWW9p">Proceedings of the 23rd Conference on Learning Theory</title>
		<editor>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kalai</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</editor>
		<meeting>the 23rd Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="54" to="66" />
		</imprint>
	</monogr>
	<note type="raw_reference">Philippe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. In Adam Tauman Kalai and Mehryar Mohri, editors, Proceedings of the 23rd Conference on Learning Theory, pages 54-66, 2010.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_8SNjJgN">The epoch-greedy algorithm for multi-armed bandits with side information</title>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pVK7Xqe">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="817" to="824" />
		</imprint>
	</monogr>
	<note type="raw_reference">John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In Advances in neural information processing systems, pages 817-824, 2008.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_gJmaTJN">Learning to optimally schedule internet banner advertisements</title>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsuyoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PkrHNxP">Proceedings of the Sixteenth International Conference on Machine Learning</title>
		<meeting>the Sixteenth International Conference on Machine Learning</meeting>
		<imprint>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
	<note type="raw_reference">Naoki Abe and Atsuyoshi Nakamura. Learning to optimally schedule internet banner adver- tisements. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 12-21.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_nbZxdAN">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772758</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rN8wDGM">Proceedings of the 19th International Conference on World Wide Web</title>
		<meeting>the 19th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web, pages 661-670. ACM, 2010.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_5E9KmT9">Just-in-time adaptive interventions (JITAIs) in mobile health: Key components and design principles for ongoing health behavior support</title>
		<author>
			<persName><forename type="first">Inbal</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawna</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><forename type="middle">J</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Uy4cyzW">Annals of Behavioral Medicine</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>accepted subject to revisions</note>
	<note type="raw_reference">Inbal Nahum-Shani, Shawna N. Smith, Bonnie J. Spring, Linda M. Collins, Katie Witkiewitz, Ambuj Tewari, and Susan A. Murphy. Just-in-time adaptive interventions (JITAIs) in mobile health: Key components and design principles for ongoing health behavior support. Annals of Behavioral Medicine, 2016. accepted subject to revisions.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_UV47SeQ">PAC-Bayesian analysis of contextual bandits</title>
		<author>
			<persName><forename type="first">Yevgeny</forename><surname>Seldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Ortner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Laviolette</surname></persName>
		</author>
		<idno type="DOI">10.1109/tit.2012.2211334</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KEvK6D8">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1683" to="1691" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yevgeny Seldin, Peter Auer, John S. Shawe-Taylor, Ronald Ortner, and François Laviolette. PAC-Bayesian analysis of contextual bandits. In Advances in Neural Information Processing Systems, pages 1683-1691, 2011.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_y7k95YH">Contextual bandits with similarity information</title>
		<author>
			<persName><forename type="first">Aleksandrs</forename><surname>Slivkins</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000068</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_r7JnYKJ">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2533" to="2568" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Aleksandrs Slivkins. Contextual bandits with similarity information. The Journal of Machine Learning Research, 15(1):2533-2568, 2014.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_Xz5MpxV">Certainty equivalence control with forcing: revisited</title>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demosthenis</forename><surname>Teneketzis</surname></persName>
		</author>
		<idno type="DOI">10.1016/0167-6911(89)90107-2</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9HwJp8X">Systems &amp; control letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="405" to="412" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rajeev Agrawal and Demosthenis Teneketzis. Certainty equivalence control with forcing: revisited. Systems &amp; control letters, 13(5):405-412, 1989.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_Pw3Vvcq">A linear response bandit problem</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Goldenshluger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Zeevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_THVAK69">Stochastic Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="230" to="261" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Goldenshluger and Assaf Zeevi. A linear response bandit problem. Stochastic Systems, 3(1):230-261, 2013.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_XYDNW6x">Woodroofe&apos;s one-armed bandit problem revisited</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Goldenshluger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Assaf</forename><surname>Zeevi</surname></persName>
		</author>
		<idno type="DOI">10.1214/08-aap589</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qadgzZw">The Annals of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1603" to="1633" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alexander Goldenshluger and Assaf Zeevi. Woodroofe&apos;s one-armed bandit problem revisited. The Annals of Applied Probability, 19(4):1603-1633, 2009.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main" xml:id="_QAwVcnM">Online decision-making with high-dimensional covariates</title>
		<author>
			<persName><forename type="first">Hamsa</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohsen</forename><surname>Bayati</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.2661896</idno>
		<idno>SSRN 2661896</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hamsa Bastani and Mohsen Bayati. Online decision-making with high-dimensional covariates. Available at SSRN 2661896, 2015.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_KKPcRtp">Contextual bandit learning with predictable rewards</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NqTrGpg">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alekh Agarwal, Miroslav Dudík, Satyen Kale, John Langford, and Robert E. Schapire. Contextual bandit learning with predictable rewards. In International Conference on Artificial Intelligence and Statistics, pages 19-26, 2012.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_M7bgyGx">The multi-armed bandit problem with covariates</title>
		<author>
			<persName><forename type="first">Vianney</forename><surname>Perchet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Rigollet</surname></persName>
		</author>
		<idno type="DOI">10.1214/13-aos1101</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ejVd25M">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="693" to="721" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. The Annals of Statistics, 41(2):693-721, 2013.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_Z2wc8qY">Randomized allocation with arm elimination in a bandit problem with covariates</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tXnACGf">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="242" to="270" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wei Qian and Yuhong Yang. Randomized allocation with arm elimination in a bandit problem with covariates. Electronic Journal of Statistics, 10(1):242-270, 2016.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_dJR4DzJ">Efficient optimal learning for contextual bandits</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Karampatziakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qwzPmzQ">Proceedings of the Twenty-Seventh Conference Annual Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Seventh Conference Annual Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
	<note type="raw_reference">Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In Proceedings of the Twenty-Seventh Conference Annual Conference on Uncertainty in Artificial Intelligence, pages 169-178. AUAI Press, 2011.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_EvvKMt9">Taming the monster: A fast and simple algorithm for contextual bandits</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kFbg3bF">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1638" to="1646" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In Proceedings of the 31st International Conference on Machine Learning, pages 1638-1646, 2014.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="http://www.consumer-health.com/motivating-patients-to-use-smartphone-health-apps/" />
		<title level="m" xml:id="_bvWQ6MW">Motivating patients to use smartphone health apps</title>
		<imprint>
			<date type="published" when="2011-06-30">2011. June 30, 2016</date>
		</imprint>
		<respStmt>
			<orgName>Consumer Health Information Corporation</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Consumer Health Information Corporation. Motivating patients to use smartphone health apps, 2011. URL: http://www.consumer-health.com/motivating-patients-to-use-smartphone-health- apps/, accessed: June 30, 2016.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main" xml:id="_3Rnb4bs">An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention</title>
		<author>
			<persName><forename type="first">Huitian</forename><surname>Lei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">Huitian Lei. An Online Actor Critic Algorithm and a Statistical Decision Procedure for Personalizing Intervention. PhD thesis, University of Michigan, 2016.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_uvZZQFd">Contextual bandits with linear payoff functions</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3TSpuAY">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="208" to="214" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payoff functions. In International Conference on Artificial Intelligence and Statistics, pages 208-214, 2011.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_vXx4wPt">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hpuE8Ru">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. The Journal of Machine Learning Research, 3:397-422, 2003.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_kGUhSPk">On-line evaluation and prediction using linear functions</title>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<idno type="DOI">10.1145/267460.267471</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TQ6GadP">Proceedings of the tenth annual conference on Computational learning theory</title>
		<meeting>the tenth annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
	<note type="raw_reference">Philip M. Long. On-line evaluation and prediction using linear functions. In Proceedings of the tenth annual conference on Computational learning theory, pages 21-31. ACM, 1997.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_zHgz9vN">Parametric bandits: The generalized linear case</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Filippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Cappe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Uh4rKPE">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. Parametric bandits: The generalized linear case. In Advances in Neural Information Processing Systems, pages 586-594, 2010.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_dGdZuPj">Finite-time analysis of kernelised contextual bandits</title>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Korda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Flaounas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GJuSZpW">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">654</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Michal Valko, Nathan Korda, Rémi Munos, Ilias Flaounas, and Nello Cristianini. Finite-time analysis of kernelised contextual bandits. In Uncertainty in Artificial Intelligence, page 654, 2013.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_Ahg5geb">Contextual multi-armed bandits</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dávid</forename><surname>Pál</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pál</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8bXU3Kr">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tyler Lu, Dávid Pál, and Martin Pál. Contextual multi-armed bandits. In International Conference on Artificial Intelligence and Statistics, pages 485-492, 2010.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_zKe9ysS">RELEAF: An algorithm for learning and exploiting relevance</title>
		<author>
			<persName><forename type="first">Cem</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="DOI">10.1109/jstsp.2015.2402646</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RwbBN4g">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cem Tekin and Mihaela van der Schaar. RELEAF: An algorithm for learning and exploiting relevance. IEEE Journal of Selected Topics in Signal Processing, 9(4):716-727, June 2015.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_eUpabvZ">Learning to optimize via posterior sampling</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
		<idno type="DOI">10.1287/moor.2014.0650</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8Yzw79a">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1221" to="1243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathe- matics of Operations Research, 39(4):1221-1243, 2014.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_3F5tBnw">A modern Bayesian look at the multi-armed bandit</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vRZmPb3">Applied Stochastic Models in Business and Industry</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="639" to="658" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Steven L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry, 26(6):639-658, 2010.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_3vHWBaf">Thompson sampling for contextual bandits with linear payoffs</title>
		<author>
			<persName><forename type="first">Shipra</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_VbbaSt9">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
	<note type="raw_reference">Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In Proceedings of the 30th International Conference on Machine Learning (ICML- 13), pages 127-135, 2013.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_6ek8eTS">Optimistic Bayesian sampling in contextual-bandit problems</title>
		<author>
			<persName><forename type="first">Benedict</forename><forename type="middle">C</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Korda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Leslie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fB8Wtc5">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2069" to="2106" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Benedict C. May, Nathan Korda, Anthony Lee, and David S. Leslie. Optimistic Bayesian sampling in contextual-bandit problems. The Journal of Machine Learning Research, 13(1):2069-2106, 2012.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_AVT4kkB">Dynamic influences on smoking relapse process</title>
		<author>
			<persName><forename type="first">Saul</forename><surname>Shiffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PgY8QEY">Journal of Personality</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1715" to="1748" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Saul Shiffman. Dynamic influences on smoking relapse process. Journal of Personality, 73(6):1715-1748, 2005.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_5jNUhCB">The nonstochastic multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6znXmC4">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="77" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_D9zHAD2">Minimax policies for adversarial and stochastic bandits</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2phvXxz">Proceedings of the 22nd Annual Conference on Learning Theory</title>
		<meeting>the 22nd Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic bandits. In Proceedings of the 22nd Annual Conference on Learning Theory, 2004.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_uSxTrNq">Fighting bandits with a new kind of smoothness</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chansoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_EnrSpZm">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2188" to="2196" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Abernethy, Chansoo Lee, and Ambuj Tewari. Fighting bandits with a new kind of smoothness. In Advances in Neural Information Processing Systems 28, pages 2188-2196, 2015.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_vRgSsfT">Contextual bandit algorithms with supervised learning guarantees</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wEUTK6j">Proceedings of the 14th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 14th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
	<note type="raw_reference">Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandit algorithms with supervised learning guarantees. In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics, volume 15 of JMLR Workshop and Conference Proceedings, pages 19-26, 2011.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_guTP9cN">Microrandomized trials: An experimental design for developing just-in-time adaptive interventions</title>
		<author>
			<persName><forename type="first">Predrag</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">B</forename><surname>Hekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saul</forename><surname>Shiffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><surname>Boruvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7wCkec4">Health Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1220" to="1228" />
			<date type="published" when="2015-12">Dec 2015</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
	<note type="raw_reference">Predrag Klasnja, Eric B. Hekler, Saul Shiffman, Audrey Boruvka, Daniel Almirall, Ambuj Tewari, and Susan A. Murphy. Microrandomized trials: An experimental design for developing just-in-time adaptive interventions. Health Psychology, 34(Suppl):1220-1228, Dec 2015.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_WdJE3CT">Exploration scavenging</title>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390223</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Qx2YThf">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="528" to="535" />
		</imprint>
	</monogr>
	<note type="raw_reference">John Langford, Alexander Strehl, and Jennifer Wortman. Exploration scavenging. In Proceedings of the 25th international conference on Machine learning, pages 528-535. ACM, 2008.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_y49nbyF">Learning from logged implicit exploration data</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_f9ARWxq">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2217" to="2225" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alex Strehl, John Langford, Lihong Li, and Sham M. Kakade. Learning from logged implicit exploration data. In Advances in Neural Information Processing Systems, pages 2217-2225, 2010.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_52qtZXy">Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GQpmdhY">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
	<note type="raw_reference">Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 297-306. ACM, 2011.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_TrRnBZx">An unbiased offline evaluation of contextual bandit algorithms with generalized linear models</title>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesup</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YxQtBhd">Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2</title>
		<meeting>the Workshop on On-line Trading of Exploration and Exploitation 2<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-02">July 2, 2011. 2012</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="19" to="36" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
	<note type="raw_reference">Lihong Li, Wei Chu, John Langford, Taesup Moon, and Xuanhui Wang. An unbiased offline evaluation of contextual bandit algorithms with generalized linear models. In Proceedings of the Workshop on On-line Trading of Exploration and Exploitation 2 July 2, 2011, Bellevue, Washington, USA, volume 26 of JMLR Workshop and Conference Proceedings, pages 19-36, 2012.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_2GyWwEz">Doubly robust policy evaluation and learning</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PqGptYu">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1097" to="1104" />
		</imprint>
	</monogr>
	<note type="raw_reference">Miroslav Dudík, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In Proceedings of the 28th International Conference on Machine Learning, pages 1097-1104, 2011.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_cgMt6up">Performance guarantees for individualized treatment rules</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mA7ZVtZ">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1180</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Min Qian and Susan A. Murphy. Performance guarantees for individualized treatment rules. Annals of Statistics, 39(2):1180, 2011.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_3wB8j9g">Estimating individualized treatment rules using outcome weighted learning</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">John</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.2012.695674</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zdN2n4Y">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">499</biblScope>
			<biblScope unit="page" from="1106" to="1118" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yingqi Zhao, Donglin Zeng, A. John Rush, and Michael R. Kosorok. Estimating individualized treatment rules using outcome weighted learning. Journal of the American Statistical Association, 107(499):1106-1118, 2012.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_9VeF7j8">A robust method for estimating optimal treatment regimes</title>
		<author>
			<persName><forename type="first">Baqun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><forename type="middle">A</forename><surname>Tsiatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Davidian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Vsxz2Yf">Biometrics</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1010" to="1018" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Baqun Zhang, Anastasios A. Tsiatis, Eric B. Laber, and Marie Davidian. A robust method for estimating optimal treatment regimes. Biometrics, 68(4):1010-1018, 2012.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_FpaXvxt">Estimating optimal treatment regimes from a classification perspective</title>
		<author>
			<persName><forename type="first">Baqun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><forename type="middle">A</forename><surname>Tsiatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Davidian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Laber</surname></persName>
		</author>
		<idno type="DOI">10.1002/sta.411</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jzcS65h">Stat</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="114" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Baqun Zhang, Anastasios A. Tsiatis, Marie Davidian, Min Zhang, and Eric Laber. Estimating optimal treatment regimes from a classification perspective. Stat, 1(1):103-114, 2012.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_s2gxVNf">Risk-aversion in multi-armed bandits</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jTnmNwe">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3275" to="3283" />
		</imprint>
	</monogr>
	<note type="raw_reference">Amir Sani, Alessandro Lazaric, and Rémi Munos. Risk-aversion in multi-armed bandits. In Advances in Neural Information Processing Systems, pages 3275-3283, 2012.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_GVfsuBY">Mean-variance and value at risk in multi-armed bandit problems</title>
		<author>
			<persName><forename type="first">Sattar</forename><surname>Vakili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9fbb6Ab">2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1330" to="1335" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sattar Vakili and Qing Zhao. Mean-variance and value at risk in multi-armed bandit problems. In 2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 1330-1335. IEEE, 2015.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
