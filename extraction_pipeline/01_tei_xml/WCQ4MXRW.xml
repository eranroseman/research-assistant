<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_qWtJPzy">Optimizing mHealth Interventions with a Bandit</title>
				<funder ref="#_ADyGAd7">
					<orgName type="full">NIAAA</orgName>
				</funder>
				<funder ref="#_NzFvaEV">
					<orgName type="full">NHLBI/</orgName>
				</funder>
				<funder ref="#_Fe8YMUG">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_ByRvygq">
					<orgName type="full">NIDA</orgName>
				</funder>
				<funder ref="#_SuPYr92">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">PI Linda Collins</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Mashfiqui</forename><surname>Rabbi</surname></persName>
							<email>mrabbi@fas.harvard.edu</email>
						</author>
						<author>
							<persName><forename type="first">Predrag</forename><surname>Klasnja</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tanzeem</forename><surname>Choudhury</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ambuj</forename><surname>Tewari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Susan</forename><surname>Murphy</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>B</label> ( ) Department of Statistics , Harvard University , 1 Oxford St. #316 , Cambridge , MA 02138 , USA</note>
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<addrLine>1 Oxford St. #316</addrLine>
									<postCode>02138</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">School of Information , University of Michigan , Ann Arbor , USA</note>
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">Department of Information Science , Cornell University , Ithaca , USA</note>
								<orgName type="department">Department of Information Science</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">Department of Statistics , University of Michigan , Ann Arbor , USA</note>
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<note type="raw_affiliation">Department of Statistics and Department of Computer Science , Harvard University , Cambridge , USA</note>
								<orgName type="department" key="dep1">Department of Statistics</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_EsqukBk">Optimizing mHealth Interventions with a Bandit</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A0FB5B8E5D6E75194727290B30EA4EB3</idno>
					<idno type="DOI">10.1007/978-3-030-98546-2_21</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T13:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_RAEusGG"><p xml:id="_TVXpr2k"><s xml:id="_ySHtSkj">Mobile health (mHealth) interventions can improve health outcomes by intervening in the moment of need or in the right life circumstance.</s><s xml:id="_gzDDu62">mHealth interventions are now technologically feasible because current off-the-shelf mobile phones can acquire and process data in real time to deliver relevant interventions in the moment.</s><s xml:id="_U5erVUN">Learning which intervention to provide in the moment, however, is an optimization problem.</s><s xml:id="_KDCxJzk">This book chapter describes one algorithmic approach, a "bandit algorithm," to optimize mHealth interventions.</s><s xml:id="_b9fUa7c">Bandit algorithms are well-studied and are commonly used in online recommendations (e.g., Google's ad placement, or news recommendations).</s><s xml:id="_2KhzK2h">Below, we walk through simulated and real-world examples to demonstrate how bandit algorithms can be used to personalize and contextualize mHealth interventions.</s><s xml:id="_TzZQyCz">We conclude by discussing challenges in developing bandit-based mhealth interventions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.1" xml:id="_pmTWhAq">Introduction</head><p xml:id="_uHBr7cq"><s xml:id="_49HCk9t">Before mHealth, the standard of care was periodic visits to a clinician's office, interspersed with little to no patient support in between visits.</s><s xml:id="_uNNKJPn">At the clinician's office,</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="439.37" lry="666.142"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_T2XW5Y4"><p xml:id="_NDm3TCq"><s xml:id="_bun7enT">data is collected to describe the patient's state at that visit time and self-report data about the patient's state prior to the current visit time is collected through an errorprone mechanism of recalling past events.</s><s xml:id="_QvUP6Xp">The mHealth model has enabled significant progress in situ data collection between clinic visits; phone sensors can now capture personal data at a millisecond level, and improvement in user interfaces has reduced the burden of self-report information <ref type="bibr" target="#b9">(Kubiak and Smyth 2019)</ref>.</s><s xml:id="_xrg3wQP">mHealth interventions using persuasive design features are promising approaches for improving patients health <ref type="bibr" target="#b2">(Baumeister et al. 2019;</ref><ref type="bibr" target="#b12">Messner et al. 2019)</ref>.</s><s xml:id="_GJFkZqq">However providing effective interventions personalized to the patient between patient visits remains challenging.</s></p><p xml:id="_88RkG7w"><s xml:id="_pEGKBfq">Two key components of intervening at the right time are personalization and contextualization.</s><s xml:id="_kY7mdpA">Personalization is the process of matching an individual's preferences and lifestyle.</s><s xml:id="_GbqpNNd">e.g., a physical activity intervention can say, "You walked 10 times in the last week near your office.</s><s xml:id="_ZuFyYw3">Don't forget to take small walks near your office today."</s><s xml:id="_jqr7UNN">Such personalization can lower barriers to acting on the suggestion <ref type="bibr" target="#b8">(Hochbaum et al. 1952)</ref>.</s><s xml:id="_S5gUWAq">Contextualization takes personalization one step further by delivering interventions at moments of need or at an opportune moment when the intervention is easy to follow <ref type="bibr" target="#b7">(Fogg 2009</ref>).</s><s xml:id="_a497nN7">e.g., when a participant reaches the office, a push notification with the earlier walking suggestion can be sent, or, just after a high risk teen reports high stress, a SMS can be sent with ideas to reduce stress.</s></p><p xml:id="_DGzsCjA"><s xml:id="_P3na5y2">Contextualization and personalization are complex problems because different people may prefer different interventions and these preferences may vary by context.</s><s xml:id="_tdMVKS2">Fortunately, similar problems have been solved before.</s><s xml:id="_wemc8Qh">When Google places ads or Netflix suggests movies, they adapt their recommendation based on user preferences and characteristics, utilizing bandit algorithms.</s><s xml:id="_YAkQeST">Here we describe how to repurpose bandit algorithms to personalize and contextualize mHealth interventions.</s><s xml:id="_pV6Cyeq">We will start with a simple example, where we personalize a daily list of physical activity suggestions to an individual.</s><s xml:id="_Zhn9KcC">We will then extend this simple example to account for contextual factors (e.g., weather).</s><s xml:id="_JdAEHhr">We conclude with a real-world example and discuss future challenges in developing personalized/contextualized interventions with bandit algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.2" xml:id="_NGWapN4">Background</head><p xml:id="_DVNuktG"><s xml:id="_JWeeGKB">Bandit algorithms: "Bandit algorithms" are so called because they were first devised for the situation of a gambler playing one-armed bandits (slot machines with a long arm on the side instead of a push button).</s><s xml:id="_Pna5pnv">Each time the gambler picks a slot machine, he/she receives a reward.</s><s xml:id="_4EkEdPK">The bandit problem is to learn how to best sequentially select slot machines so as to maximize total rewards.</s><s xml:id="_5qQesGw">The fundamental issue of bandit problems is the exploitation-exploration tradeoff; here exploitation means re-using highly rewarding slot machines from the past and exploration means trying new or less-used slot machines to gather more information.</s><s xml:id="_yNkP5ng">While exploration may yield less short-term payoff, an exploitation-only approach may miss a highly rewarding slot machine.</s><s xml:id="_84JeuMM">Researchers have proposed solutions to the bandit's exploit-explore tradeoff across many areas.</s><s xml:id="_hpAc5Gk">In particular, once the relevance of bandit algorithms to internet advertising was understood, there was a flurry of work <ref type="bibr" target="#b4">(Bubeck and Cesa-Bianchi 2012)</ref>.</s><s xml:id="_42Nnq5J">Nowadays, bandit algorithms are theoretically well understood, and their benefits have been empirically demonstrated <ref type="bibr" target="#b4">(Bubeck and Cesa-Bianchi 2012;</ref><ref type="bibr" target="#b6">Chapelle et al. 2012</ref>).</s></p><p xml:id="_EB6tWdJ"><s xml:id="_paSswHK">An important class of bandit problems is the contextual bandit problem that considers additional contextual information in selecting the slot machine <ref type="bibr" target="#b18">(Woodroofe 1979)</ref>.</s><s xml:id="_jTduaMu">Contextual bandit problems provide a natural model for developing mobile health interventions.</s><s xml:id="_zXcRc55">In this model, the context is the information about the individual's current circumstances, the slot machines correspond to the different intervention options, and the rewards are near-time, proximal, outcomes <ref type="bibr" target="#b13">(Nahum-Shani et al. 2017)</ref>.</s><s xml:id="_wW32xMs">In this setup, optimizing mHealth intervention delivery is the act of learning the intervention option that will result in the best proximal outcome in a given circumstance.</s><s xml:id="_UZctTWD">This is same as solving the contextual bandit problem.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.3" xml:id="_8qZSAaw">Optimizing Intervention with a Bandit Algorithm</head><p xml:id="_5Q7WhJE"><s xml:id="_EtctNSW">We will use two simulated examples to explain how bandits can be used to optimize an mHealth intervention for an individual.</s><s xml:id="_yvg3bJm">In Sect.</s><s xml:id="_FqjvMTq">21.4, we will discuss another realworld mobile application that builds on the ideas introduced in the first two simple examples.</s></p><p xml:id="_ZYtzWJU"><s xml:id="_AccpJKC">In our first example, the bandit algorithm will be used to select an optimal set of five physical activity suggestions, for an individual, from a set of ten suggestions.</s><s xml:id="_MF9K6bb">A set of five suggestions is optimal if the set leads to the highest level of daily activity for that individual.</s><s xml:id="_UGMzQfg">The second example extends the first by finding a set of five suggestions for each of several contexts.</s><s xml:id="_XNDZgmj">Contextualizing suggestions can be helpful because the same suggestion may be more actionable in certain contexts (e.g., good weather or day of the week).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.3.1" xml:id="_WBwamJC">Personalizing Suggestions for an Individual</head><p xml:id="_srVDsbc"><s xml:id="_98v8p6b">Consider a scenario in which Jane's health plan gives her a physical activity tracker and a smartphone app.</s><s xml:id="_zHKuuuR">Jane's health plan has found that the ten activity suggestions from Table <ref type="table" target="#tab_1">21</ref>.1 often work for many less-active people to increase their activity.</s><s xml:id="_FrBPMc3">Note that the order of suggestions in Table <ref type="table" target="#tab_1">21</ref>.1 does not imply any specific ranking.</s><s xml:id="_VNTqGYV">It is unlikely, however, that every individual will be able to follow or prefer to follow all the 10 suggestions equally and there will be inter-personal variability in which suggestions are followed and to what degree.</s><s xml:id="_UttUa6t">Thus, we set the goal of learning the five suggestions with the highest chance of maximizing Jane's activity.</s><s xml:id="_F4JFnjP">We use the bandit algorithm, which is running as part of Jane's smartphone app, to achieve this</s></p><p xml:id="_2PV6uaE"><s xml:id="_JEeyAS3">Table 21.1 List of 10 suggestions 1. Walk 30 min 2. Add intervals: walk 5 min, walk very fast for 5 min, repeat 3 times 3. Take the stairs instead of the elevator whenever possible 4. Go for a walk with a friend or your dog 5. Swim a lap, rest 1 min, repeat 10 times 6.</s><s xml:id="_4YYSrHn">Attend a fitness class at your gym 7. Try some of the strength training and bodyweight exercises illustrated by the fitness app on your phone 8. Do yoga 9. Park at the far end of the parking lot to walk farther 10.</s><s xml:id="_bSD5t6s">Do yardwork for at least 10 min</s></p><p xml:id="_x4YeAyG"><s xml:id="_Yqh59Uv">goal.</s><s xml:id="_KPb3dqC">Each morning, the app issues a set of 5 suggestions.</s><s xml:id="_hSBbqfP">The app then monitors Jane's activities throughout the day and uses that information to choose 5 suggestions for the following day.</s></p><p xml:id="_wysCYbE"><s xml:id="_8AKKPsS">Formally, we will refer to each set of five activity suggestions as an intervention option or action.</s><s xml:id="_nw6TGbg">This intervention option or action is the particular choice of the five suggestions.</s><s xml:id="_F9pEMKr">On the morning of day t, the app suggests to Jane the action A t , where A t = [S t1 , S t2 , S t3 , . . .</s><s xml:id="_qazpswe">, S t10 ] T is a 10 × 1 vector of binary variables.</s><s xml:id="_PMjatqk">S ti has a value of 1 if the i-th suggestion from Table <ref type="table" target="#tab_1">21</ref>.1 is shown to Jane on day t, and 0 otherwise.</s><s xml:id="_N8cH3gT">Thus A t will have 5 entries equal to 1 and 5 entries equal to 0. Further, let Y t denote the number of active minutes for Jane on day t, which might be called the proximal outcome or reward of action A t .</s></p><p xml:id="_7zcpdPx"><s xml:id="_2cjXnCu">Consider the following linear regression model for the mean of the daily active minutes Y t on day t in terms of the suggestions:</s></p><formula xml:id="formula_0">E[Y t |A t ] = 10 i=1 β i S ti = β T A t (21.1)</formula><p xml:id="_AGBZgB4"><s xml:id="_C6rPBxr">where the second equality is written more compactly by using vector notation, β = [β 1 , β 2 , . . .</s><s xml:id="_Uy8YENG">, β 10 ] T .</s><s xml:id="_QNFWsdC">Here β 1 , β 2 , β 3 , . . .</s><s xml:id="_tWfGcaT">, β 10 respectively represent suggestion 1, 2, 3, . . .</s><s xml:id="_EdBb5pb">, 10 s contribution to Jane's number of active minutes.</s><s xml:id="_aPXPheA">Therefore, Eq. 21.1.</s><s xml:id="_Nxg2Hdh">has the following simple interpretation: Y t , the number of daily active minutes, is the sum of the effects of the 5 activity suggestions provided on day t (i.e., suggestions for which S ti = 1).</s><s xml:id="_XkXxXAE">Formally, our goal is to discover the best action A t = a * that is, the set of 5 suggestions that makes Jane most active (that results in the highest mean daily active minutes).</s><s xml:id="_CV2bRNb">We can formally write this goal as: given β, determine the action a * for which</s></p><formula xml:id="formula_1">β T a * ≥ β T a (21.2)</formula><p xml:id="_ZDyymrc"><s xml:id="_FZhPPU5">where a is a combination of 5 suggestions from Table <ref type="table" target="#tab_1">21</ref>.1.</s><s xml:id="_BKbdZuy">β is, however, unknown.</s><s xml:id="_A3yrgPE">We can estimate Jane's a * by running experiments in the following way: at the start of a day t, the app selects action A t (in other words, it delivers to Jane a combination of 5 suggestions from Table <ref type="table" target="#tab_1">21</ref>.1).</s><s xml:id="_m3NxqYh">The tracker then counts the number of minutes Jane is active on the day (note that this number is the proximal outcome Y t ).</s><s xml:id="_rEKhn3P">If the 5 suggestions are useful, then Jane will be more active that day and Y t will be high compared to other days with a different set of 5 suggestions.</s><s xml:id="_eWRn7Yc">Now, the question is: how to select the 5 suggestions each day?</s><s xml:id="_xU45jGn">One simple approach is to select 5 suggestions out of 10 with equal probability.</s><s xml:id="_m42zz8z">But such a uniform selection strategy will select more useful and less useful suggestions equally.</s><s xml:id="_rynND3y">A more sophisticated approach is to use the information already available from the past experiments to select future suggestions that will both yield additional information about a * and give as few less useful suggestions as possible.</s><s xml:id="_HQk4M82">Note that here we face the same exploit-explore tradeoff faced by the classic bandit setting's gambler-i.e., how to balance exploiting suggestions that seemed useful in the past with exploring less frequently issued suggestions.</s></p><p xml:id="_CyefHQW"><s xml:id="_UC3MJjH">An effective approach to delivering less useful suggestions as little as possible is "optimism in the face of uncertainty" epitomized by the Upper Confidence Bound (UCB) technique <ref type="bibr" target="#b1">(Auer et al. 2002;</ref><ref type="bibr" target="#b11">Li et al. 2010)</ref>.</s><s xml:id="_S6mBG8b">Bandit algorithms based on the UCB have been well studied and possess guarantees of minimizing the number of less useful suggestions.</s><s xml:id="_FMdYEDj">The key intuition behind the UCB idea is the following: First, for each choice of action a t , a confidence interval is constructed for the linear combination β T a t .</s><s xml:id="_jZuWQRb">Recall this linear combination represents E[Y t |A t = a t ], the expected proximal outcome after receiving action, a t .</s><s xml:id="_ZVYjb9p">Then the UCB bandit algorithm selects the action with the highest upper confidence limit.</s><s xml:id="_kAQtezt">Note that the upper confidence limit for β T a t can be high for either of two reasons: (1) either β T a t is large and thus a t is a good action to make Jane active, or (2) the confidence interval is very wide with a high upper limit, indicating that there is much uncertainty about the value of β T a t .</s><s xml:id="_CqJWs8K">Using the upper confidence limit represents UCB's optimism; UCB is optimistic that actions with high upper confidence limits will be the best actions, even though a larger upper confidence limit can mean more uncertainty.</s><s xml:id="_4fCeTKS">However, if an action with high upper confidence is indeed not the optimal action, then selecting the action will reduce the uncertainty about the effect of this action.</s><s xml:id="_nXrCYPf">This will help UCB realize that the action is indeed not useful.</s></p><p xml:id="_BMXBFCt"><s xml:id="_E52sksd">How does UCB choose an action using the upper confidence interval?</s><s xml:id="_SdFgNCP">By following these two steps.</s><s xml:id="_3vmS4Q8">The first step involves using Eq.</s><s xml:id="_mcTaqYn">21.1 to estimate β assuming homogeneous error variance.</s><s xml:id="_kbdKNQZ">We might use ridge regression to estimate β because ridge regression regularizes to avoid overfitting, especially when Jane has just begun to use the app and we have less data <ref type="bibr" target="#b11">(Li et al. 2010;</ref><ref type="bibr" target="#b3">Bishop 2007)</ref>.</s><s xml:id="_qwXrQYC">In this case the estimator of β, denoted by β t , after t days of using the bandit algorithm is:</s></p><formula xml:id="formula_2">β t = -1 t t u=1 A u Y u (21.3)</formula><p xml:id="_eU2EV7m"><s xml:id="_mnRBZ38">where -1 t = t u=1 A u A T u + I 10 and I 10 is an 10×10 identity matrix.</s><s xml:id="_H772rMx">Equation <ref type="formula">21</ref>.3 is the standard solution for ridge regression.</s><s xml:id="_rGvHq9G">The second step is to construct an upper confidence limit for β T a for each possible action a; the upper confidence limit on day t for action a is given by β T t a + α a T -1 t a, where α is an appropriate critical value.</s><s xml:id="_rdwv5Bp">Note, since we assumed homogeneous error variance, -1 t is proportional to the covariance for β t , and a T -1 t a is the covariance of β T a. Thus, a T -1 t a represents standard deviation of β T a and the upper confidence limit of β T a has an interpretable form, which is simply the current estimate, β T t a, plus its standard deviation multiplied up to a constant factor α.</s><s xml:id="_cJZHZAE">Then, to choose the UCB action for day t + 1, we calculate the a t+1 for which</s></p><formula xml:id="formula_3">β T t a t+1 + α α T t+1 -1 t α t+1 ≥ β T t a + α α T -1 t a (21.4)</formula><p xml:id="_frpFP4J"><s xml:id="_UGHnYN7">for all actions a. i.e., a t+1 is selected to maximize the upper confidence limit on the mean of Y t+1 .</s><s xml:id="_kjzxVE6">This approach possesses strong guarantees to minimize the number of less useful suggestions <ref type="bibr" target="#b11">(Li et al. 2010;</ref><ref type="bibr" target="#b0">Auer 2002</ref>).</s></p><p xml:id="_QRFmnav"><s xml:id="_rjapZWV">Here we summarize how the UCB bandit algorithm works on Jane's smartphone.</s><s xml:id="_UTsefgv">First there is an "exploration phase" to allow the UCB algorithm to form preliminary estimates of β.</s><s xml:id="_pSjSTmp">This phase lasts for a number of days, say t 0 days, during which each morning the UCB bandit algorithm randomly selects an action, that is, uniformly selects five activity suggestions from the 10, and delivers these suggestions to Jane in the application.</s><s xml:id="_56TBqD8">Then at the end of day t 0 , the UCB bandit uses an incremental calculation to form β t 0 and t 0 based on the selected action, Jane's activity minutes, Y t 0 , for that day and the prior day's β t 0 -1 and t 0 -1 .</s><s xml:id="_JhGrM3E">Next the UCB algorithm calculates the upper confidence limit for each action and selects the action a t 0 +1 with the highest upper confidence limit.</s><s xml:id="_HUeBr8r">On the next morning, Jane is provided the five suggestions as specified by a t 0 +1 .</s><s xml:id="_uUgXGgs">The UCB algorithm repeats the process by estimating new β t 0 +1 t 0 +1 and an updated set of 5 suggestions are chosen for the next day and so on.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.3.1.1" xml:id="_Kna9eS3">A Simulation Example</head><p xml:id="_xNrcNvk"><s xml:id="_wjZ7U95">In this section, we use a simulated example to demonstrate how a UCB bandit algorithm can personalize suggestions for Jane.</s><s xml:id="_dbTaM5n">We assume the following simple model of how Jane responds to the suggestions: When Jane sees a suggestion, she follows it with probability p or does not follow it with probability 1p.</s><s xml:id="_SgRV2Kr">If Jane follows the suggestion, she spends D minutes following it on a particular day.</s><s xml:id="_8RV7bCC">We assume D is random and normally distributed, because Jane may not spend the same amount of time each time she follows the same suggestion.</s><s xml:id="_ytumpm4">In Table <ref type="table" target="#tab_1">21</ref>.2, we created an artificial example scenario with p and D values for different suggestions.</s><s xml:id="_jTVqRtn">The D values are written as mean ± standard deviation.</s><s xml:id="_mH4UHuR">We also show the expected number Table 21.2</s><s xml:id="_bNnKMfe">A simulated scenario for Jane where p represents the probability of following a suggestion when Jane sees it, and if the suggestion is followed, "Duration" represents the number of daily minutes spent following the suggestion.</s><s xml:id="_rP5T9dd">Finally, p and "Duration" are used to compute the expected value pE <ref type="bibr">[D]</ref> With the above setup, we run the simulation in two stages.</s><s xml:id="_gQvkrMH">In the first stage, suggestions are included with equal probability in the five suggestions on each of the first fourteen days.</s><s xml:id="_BWFcAsF">This initial "exploration phase" helps to form an initial estimate of β.</s><s xml:id="_zMTqWDv">In the second stage, we run the UCB bandit algorithm: on each day, we compute β t , according to Eq. 21.3, and choose an action using Eq.</s><s xml:id="_kQ7MZrc">21.4.</s><s xml:id="_hzzm5uq">We run these simulation for 56 days, or 8 weeks.</s><s xml:id="_49JRjHg">We run 200 instances of the simulation to account for randomness in the problem.</s><s xml:id="_sgx2mKY">One source of this randomness comes from the exploration phase, where the app generates non-identical sequences of random suggestions based on when Jane starts using the app.</s><s xml:id="_uBU9Grh">We deal with this randomness by resetting the randomization seed after each simulation run.</s><s xml:id="_c8f9etR">Another source of randomness comes from the within-person variability of how Jane responds to the suggestions.</s><s xml:id="_zMKDPUc">We create a second stream of random numbers to simulate how Jane responds to the suggestions.</s><s xml:id="_k5gvs6r">The seed of this second stream remains unchanged after each simulation run; we do not reset this seed because, doing so will add the randomness of resetting the seeds to the within-person variability.</s><s xml:id="_s3GgGaN">Table <ref type="table" target="#tab_1">21</ref>.3 shows the results, where we report the mean of the β estimates.</s><s xml:id="_eRrfhsv">At the top, we list the actual β values.</s><s xml:id="_6Xkj4m6">We then list in each row how many times a suggestion is issued by UCB over a two week period.</s><s xml:id="_phY7MzK">We use boldface for the top five suggestions (1st, 3rd, 4th, 8th, 9th in Table <ref type="table" target="#tab_1">21</ref>.1).</s><s xml:id="_Tz79nKD">The simulation shows that after the two-week exploration phase, UCB chooses the top (boldfaced) suggestions more times than the less useful ones.</s><s xml:id="_NrbGy5w">Since a suggestion can be picked only once a day, the top suggestions 1, 2, and 8 from Table <ref type="table" target="#tab_1">21</ref>.3 are picked nearly every day after the exploration phase (11-14 days between week 3-4, 5-6, and 7-8).</s><s xml:id="_UfKajEC">However, suggestions 3, 9, and 10 all have similar β values.</s><s xml:id="_79dySx4">As a result, UCB is often uncertain among them and chooses the 10th suggestion sometimes wrongly, since it is not in the top five suggestions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.3.2" xml:id="_DZdS4fu">Optimizing Interventions for Different Contexts</head><p xml:id="_bFu764G"><s xml:id="_XSybXhh">In the earlier section, we discussed an example of personalizing suggestions with the UCB algorithm.</s><s xml:id="_DY4b8aE">Our goal was to demonstrate the inner workings of a bandit algorithm in a simple setting.</s><s xml:id="_gYH6WPm">Here we discuss extending the prior example to a more realistic setting where we tailor suggestion based on users' context.</s><s xml:id="_zJy7CKA">Indeed, context can determine whether, and the degree to which, certain suggestions are actionable.</s><s xml:id="_GkAwrNn">For example, Jane may only be able to act on the yardwork suggestion on the weekend, or she may appreciate and act on the reminder to take her dog for a walk when the weather is good.</s><s xml:id="_6RvmdUy">By adapting suggestions to different contexts, we hope to enhance her activity level.</s><s xml:id="_6e3QgSy">Fortunately, we can contextualize suggestions by re-purposing the bandit technique described already.</s><s xml:id="_Pwxaqjw">We briefly describe one way to do so below.</s></p><p xml:id="_Tjdrxvh"><s xml:id="_t5tCTfY">For clarity, we will first consider a very simple context involving only the weather and day of the week.</s><s xml:id="_grX3ugP">For these two contexts, there are two states (i) weekend or weekday, (ii) good or bad weather, where we consider the whole day as bad weather if only part is.</s><s xml:id="_7HtDNrU">Thus, each day belongs to one of four different context combinations</s></p><p xml:id="_DkVBe4N"><s xml:id="_66M8eFa">Table 21.4</s><s xml:id="_sta4Pfc">Different types of contexts Context 1 Bad weather, weekend 2 Bad weather, weekday 3 Good weather, weekend 4 Good weather, weekday</s></p><p xml:id="_Sve3CZ7"><s xml:id="_mq4qpU2">(see Table <ref type="table" target="#tab_1">21</ref>.4).</s><s xml:id="_ZW4QQa4">Note this simple characterization of only 4 contexts is to convey the idea of contextualization rather than actually to realistically handle a large number of contexts.</s></p><p xml:id="_Vts88W6"><s xml:id="_8dQM4jV">For these four context combinations, the task of contextualizing suggestions boils down to optimizing the suggestions for each of the four.</s><s xml:id="_SbZR65y">An intuitive approach is to use 4 different bandit algorithms, one for each context combination.</s><s xml:id="_EQ5gC2j">Depending on the context on day t, the corresponding bandit would be activated for optimizing suggestions for that context.</s><s xml:id="_mmq6kQS">Recall that an action is a set of five activity suggestions from the 10 in Table <ref type="table" target="#tab_1">21</ref>.1.</s><s xml:id="_7WUd5AZ">Each of the four different bandit algorithms uses a model such as Eq.</s><s xml:id="_RcvmguQ">21.1.</s><s xml:id="_HU2dTC8">but with different βs due to the different contexts.</s><s xml:id="_WkQ4ZWX">We represent this difference by sub-scripting β as β k for the k-th (k = 1, 2, 3, 4) context.</s><s xml:id="_WH4Sc7Q">So, the goal is to learn the optimal action a * k that maximizes the average number of minutes active for Jane in context k.</s><s xml:id="_QChwgjB">That is, for k = 1, 2, 3, 4 the goal is to learn the action a * k which satisfies</s></p><formula xml:id="formula_4">β T k a * k ≥ β T k a k</formula><p xml:id="_ngdrt39"><s xml:id="_NUT3mgT">Again, one UCB bandit algorithm can be run per context to learn the optimal five suggestions for that context.</s></p><p xml:id="_JX4QAqE"><s xml:id="_jYWmuey">Note that using a separate bandit algorithm for each context is not a feasible approach in a real-world setting; there are too many possible contexts.</s><s xml:id="_9AE7fS3">It would take the bandit algorithm many days to obtain good estimates of the β k parameters.</s><s xml:id="_pJuqWfS">However, we can use a few tricks to handle large number of contexts.</s><s xml:id="_B3cahTt">First, we may know a priori that some suggestions are equally actionable across different contexts and some suggestions are not at all actionable in certain contexts.</s><s xml:id="_As2f4Vy">If the suggestions are equally actionable across contexts, we can use the same β k parameter values for these contexts.</s><s xml:id="_zJ34SMZ">And if a suggestion is not actionable in a given context we can set its parameter in β k to zero.</s><s xml:id="_gTs6tsa">Second, we can pool information across people.</s><s xml:id="_GQAXeDK">For example, some suggestions, such as yardwork, are more actionable on weekends for most people.</s><s xml:id="_wMgpm7G">Thus, we don't need to find β k for each user individually.</s><s xml:id="_3FzVE5C">Pooling information, however, requires a Bayesian approach where for a new user, initially β k is pooled from prior users and once some data from the user is available, β k is then adapted to make more user-specific changes.</s><s xml:id="_XxyaYes">Bayesian approaches to bandit algorithms are beyond the scope of this chapter; but the techniques are along the same lines as UCB <ref type="bibr" target="#b5">(Chapelle and Li 2011)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.4" xml:id="_U4nfnwZ">A Real-World Example</head><p xml:id="_Y4hqHgy"><s xml:id="_8a9C7RV">Earlier, we gave two simple examples of how the UCB bandit algorithm can personalize and contextualize mobile health interventions.</s><s xml:id="_qkskGG3">Real-world examples, however, are more complicated, with many potential suggestions and many contexts.</s><s xml:id="_7CvcM7M">Below we discuss an mHealth app called MyBehavior that has been deployed multiple times in real world studies <ref type="bibr" target="#b16">(Rabbi et al. 2018;</ref><ref type="bibr" target="#b14">Rabbi et al. 2015)</ref>.</s><s xml:id="_PcVEj8z">MyBehavior utilizes phone sensor data to design unique suggestions for an individual and subsequently uses a bandit algorithm to find the activity suggestions that maximize chances of daily calorie burns.</s><s xml:id="_Kgzwpcq">Like the example in Sect.</s><s xml:id="_VjYmWr8">21.3, MyBehavior issues the suggestions once each morning.</s><s xml:id="_vVqNjXa">The number of suggestions, however, is higher than in Table <ref type="table" target="#tab_1">21</ref>.1 because the suggestions in MyBehavior closely match an individual's routine behaviors, and routine behaviors are dynamic.</s><s xml:id="_5JEyRdu">In the following, we briefly discuss how MyBehavior uses the bandit algorithm.</s><s xml:id="_rCuBQbj">More information on this can be found in <ref type="bibr" target="#b15">(Rabbi et al. 2017)</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.4.1" xml:id="_jeaXX6M">MyBehavior: Optimizing Individualized Suggestions to Promote More Physical Activity</head><p xml:id="_bgherZX"><s xml:id="_UVZ5YfF">The following discussion of MyBehavior first covers how unique suggestions are created for each individual.</s><s xml:id="_jsRteQf">We then briefly discuss how a bandit algorithm is used to find optimal activity suggestions that have the highest chance of maximizing an individual's daily calorie burn.</s></p><p xml:id="_E5F348J"><s xml:id="_5N5rPcM">The MyBehavior app tracks an individual's physical activity and location every minute.</s><s xml:id="_d8QZXty">The detected physical activities include walking, running, driving, and being stationary.</s><s xml:id="_b8dXwZe">The app then analyzes the location-tagged activity data to find patterns that are representative of the user's behaviors.</s><s xml:id="_Ydk8xaN">Figure <ref type="figure">21</ref>.1 shows several examples of behaviors found by MyBehavior. Figure <ref type="figure">21</ref>.1a and b respectively contain places where a user stayed stationary and a location where the user frequently walked.</s><s xml:id="_xxbjMzH">Figure <ref type="figure">21</ref>.1c shows similar walking behaviors from another user.</s><s xml:id="_m5hFX3m">MyBehavior uses these behavioral patterns to generate suggestions that are unique to each individual.</s><s xml:id="_AACzT2g">For example, one intervention may suggest an activity goal at specific locations that the user regularly goes to.</s><s xml:id="_hgtjhAD">Such tailoring makes feedback more compelling, since a user's familiarity with the location enhances adherence <ref type="bibr" target="#b7">(Fogg 2009)</ref>.</s></p><p xml:id="_3vC7hAp"><s xml:id="_4DztP93">Specifically, MyBehavior creates three kinds of uniquely individualized suggestions: (i) for stationary behaviors, MyBehavior pinpoints the locations where the user tends to be stationary and suggests taking small walking breaks every hour in these locations.</s><s xml:id="_4fAJvNt">(ii) for walking behaviors, MyBehavior locates the different places the user usually walks and suggests continuing to walk in those locations (iii) for other behaviors, e.g., participation in yoga class or gym exercises, MyBehavior simply reminds the user to keep up the good work.</s><s xml:id="_5Dy5dTJ">Fig. 21.2 MyBehavior app screenshots for three different users.</s><s xml:id="_tNcYftJ">Figures 21.1 and 21.2 have been reproduced from Rabbi et al. (2015) with appropriate permission from the authors</s></p><p xml:id="_WPn69Ne"><s xml:id="_GKQgTRS">Since MyBehavior suggestions are tailored to the user, the first suggestion at the top of each screen shot is to walk, but the locations are different.</s><s xml:id="_Hf9u3hw">Also, the first and third users receive a gym weight training exercise suggestion that the second user does not.</s><s xml:id="_jn78UKw">Now, how does MyBehavior decide which suggestions to give?</s><s xml:id="_j9WdKMr">MyBehavior uses a bandit algorithm like that in Sect.</s><s xml:id="_fgmBXnT">21.3's first example, where suggestions are issued once a day.</s><s xml:id="_KJtpg4E">But MyBehavior can offer many more suggestions than Table <ref type="table" target="#tab_1">21</ref>.1 contains, depending on the variety of locations in which a user might be sedentary or active, etc.</s><s xml:id="_8veAKQt">Fortunately, the bandit algorithm can still efficiently adapt to these high numbers of tailored suggestions.</s><s xml:id="_spskaZk"><ref type="bibr" target="#b15">Rabbi et al. (2017)</ref> details how this optimization works, but the key intuitions are the following: (i) Most human behaviors are highly repetitive and routine and occur in the same locations.</s><s xml:id="_M3Hh665">Routine behaviors and locations will be detected early and thus included soon in the individual's list of suggestions.</s><s xml:id="_WVBKZWE">(ii) The suggestions relating to routine behaviors and locations are more likely to be followed than suggestions of non-routine behaviors in non-routine locations.</s><s xml:id="_5ExfGwH">Thus, the bandit will learn about the effects of these suggestions more quickly and these suggestions will likely remain effective if the user's routine does not change.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="21.5" xml:id="_Xcpw8Sn">Discussion</head><p xml:id="_DCY3eVv"><s xml:id="_qcDhRTF">In the last two sections, we discussed several examples of how bandit algorithms can optimize mobile health interventions.</s><s xml:id="_q9SAKEq">The bandit algorithm balances experimenting with different activity suggestions and selecting activity suggestions that currently appear most useful.</s><s xml:id="_g7NesMa">This balancing act ensures that the algorithm acquires necessary information while maintaining an engaging user experience by providing as few less-useful suggestions as possible.</s><s xml:id="_VEm25nE">While we showed that bandit algorithms can be useful to personalize and contextualize suggestions, there are additional real-world complexities that pose new challenges for bandit algorithms to address:</s></p><p xml:id="_9zBdG7c"><s xml:id="_6wFdzWs">Ignoring delayed effects: In bandit algorithms, the optimal action is the action that maximizes the immediate reward (proximal outcome).</s><s xml:id="_9Hjjg25">In other words, bandit algorithms ignore the potential impact of the action on future context and future proximal outcomes.</s><s xml:id="_abktq67">Some actions, however, can have long-term negative effects even if the short-term effect is positive.</s><s xml:id="_WQDfajn">e.g., delivering an office walking suggestion may increase a user's current activity level, but the user might become bored after repeating the office walk several days, thus future suggestions may be less effective.</s><s xml:id="_v8Dvq43">In these cases, other algorithms that explicitly allow past actions to impact future outcomes <ref type="bibr" target="#b17">(Sutton and Barto 1998</ref>) might be used.</s><s xml:id="_qzEhyFS">Precisely, the outcome of these algorithms are Y t + V (X t+1 ), where V (X t+1 ) is the prediction of the impact of the actions on future proximal outcomes given the context X t+1 at the time t +1 (a bandit algorithm acts as if V (X t+1 ) = 0).</s><s xml:id="_usV9SmF">These algorithms tend to learn more slowly than bandit algorithms, since we need additional data to form the prediction V (X t+1 ).</s></p><p xml:id="_VSyxqVh"><s xml:id="_jPGq3aS">We conjecture that the noisier the data is, the harder it will be to form high quality predictions of V (X t+1 ) and thus as a result, bandit algorithms may still be preferable.</s></p><p xml:id="_DZaEEUQ"><s xml:id="_5zZd4eE">Non-stationarity: Most bandit algorithms assume "stationary" settings; i.e., the responsivity of a user in a given context to an action does not change with time.</s><s xml:id="_y9vKtVh">This assumption can be violated in real-word settings; in MyBehavior, for example, we observed that many suggestions become ineffective when people switched job and moved from one location to another.</s><s xml:id="_nXaa8Rh">Such changes over time are often referred to as "non-stationarity."</s><s xml:id="_47QKntT">Other types of non-stationarity can be caused by life events such as a significant other's illness or aging.</s><s xml:id="_u7SddC9">Bandit algorithms are typically slow to adapt to non-stationarity.</s><s xml:id="_NPzZbZB">Speeding up this process is a critical direction for future bandit research.</s></p><p xml:id="_PVVgJY4"><s xml:id="_ZCsCDUu">Dealing with less data: In real world applications, where the number of contexts and actions are many, bandit algorithms will need a lot of burdensome experimentation to find the optimal action for a given context.</s><s xml:id="_rusZUSG">One way around this is to use a "warm start."</s><s xml:id="_UZd7aFT">A warm start set of decision rules that link the context to the action can be constructed using data from micro-randomized trials <ref type="bibr" target="#b8">(Klasnja et al. 2015)</ref> involving similar individuals.</s><s xml:id="_YRd72uE">Recently <ref type="bibr" target="#b10">Lei et al. (2014)</ref> developed a bandit algorithm that can employ a warm start.</s><s xml:id="_dnRrcx2">However, we still need to test whether, and in which settings, warm starts will sufficiently speed up learning.</s></p><p xml:id="_cfr39Ra"><s xml:id="_WfmwUym">Adverse effects: Since mHealth interventions are generally behavioral, the risk of personal harm is often minimal.</s><s xml:id="_Uj8pn8r">Nonetheless, there could be potential iatrogenic effect because phones cannot capture every piece of contextual information and bandit algorithms ignore the long-term effects of interventions.</s><s xml:id="_GMzEe5w">Since bandit algorithms don't take interventions' long-term effects into account, the algorithm may notify or otherwise deliver interventions too much and thus cause annoyance and reduce app engagement.</s><s xml:id="_zXnHPY9">Future work needs to investigate how to account for such long-term adverse effects.</s><s xml:id="_MDFGuh8">Furthermore, current phone sensors cannot automatically capture critical contextual information such as a user's health risks, preferences, barriers, emotional states, etc. Incomplete information may cause the algorithm to provide less appealing (e.g., not suggesting an activity that a user likes but didn't do often in the past) and inappropriate suggestions (e.g., asking someone who is injured to walk).</s><s xml:id="_G7QR4Fx">Providing human control over the suggestion generation process can mitigate these problems; e.g., a user can delete inappropriate suggestions and prioritize the suggestions that are more appealing <ref type="bibr" target="#b14">(Rabbi et al. 2015)</ref>.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_8DT9TY3"><s xml:id="_GtmD43Q">, which also represents β values for the suggestionSuggestionsp Duration, D (min) Expected duration pE[D]  that Jane spends following a suggestion when she sees it.</s><s xml:id="_t73y9PM">This expected number is p × E[D] + (1p) × 0 = pE[D].</s><s xml:id="_SXd3yND">These expected minutes are also β values in Eq. 21.1.</s><s xml:id="_2bGFgJm">Note that β values are unknown in real world setting.</s><s xml:id="_Etbbtzz">We use known β values in a simulated example to show how the UCB algorithm finds the suggestions with higher β values.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><figDesc><div><p xml:id="_VjePTTh"><s xml:id="_qEx6AyY">Figure 21.2 shows several screen shots of the MyBehavior app, where Fig. 21.2a-c are suggestions for three separate users.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 21 . 1</head><label>211</label><figDesc><div><p xml:id="_rcM3JU2"><s xml:id="_xCuQshn">Fig. 21.1 Visualization of a user's movements over a week a heatmap showing the locations where the user is stationary everyday b location traces of frequent walks for the user c location traces of frequent walks for another user</s></p></div></figDesc><graphic coords="11,56.40,56.90,326.37,174.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 21 .3</head><label>21</label><figDesc><div><p xml:id="_SS4kK4h"><s xml:id="_EAXQKGF">Number of times suggestions are picked by the app within each of the two-week intervals.</s><s xml:id="_Rq86xjw">N denotes the number of days the app selects a suggestion in the time frame mentioned within parenthesis.</s><s xml:id="_QSXpPXa">Note, the number of times a suggestion can be selected during a two-week period is at most 14 (i.e., N ≤ 14)</s></p></div></figDesc><table><row><cell>Suggestions</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell></row><row><cell>β</cell><cell>15.0</cell><cell>0.4</cell><cell>5.2</cell><cell>18.9</cell><cell>0.0</cell><cell>2.2</cell><cell>0.0</cell><cell>10.3</cell><cell>6.3</cell><cell>5.1</cell></row><row><cell>N (week 1-2)</cell><cell>7.1</cell><cell>7.2</cell><cell>7.0</cell><cell>7.0</cell><cell>6.8</cell><cell>6.9</cell><cell>6.9</cell><cell>6.8</cell><cell>7.1</cell><cell>7.1</cell></row><row><cell>N (week 3-4)</cell><cell>12.4</cell><cell>3.9</cell><cell>6.3</cell><cell>13.4</cell><cell>2.8</cell><cell>4.5</cell><cell>2.5</cell><cell>9.6</cell><cell>7.8</cell><cell>6.5</cell></row><row><cell>N (week 5-6)</cell><cell>12.8</cell><cell>3.5</cell><cell>6.3</cell><cell>13.7</cell><cell>2.6</cell><cell>4.3</cell><cell>1.7</cell><cell>10.1</cell><cell>8.1</cell><cell>6.7</cell></row><row><cell>N (week 7-8)</cell><cell>13.1</cell><cell>3.4</cell><cell>6.4</cell><cell>13.8</cell><cell>2.4</cell><cell>4.3</cell><cell>1.6</cell><cell>10.1</cell><cell>7.8</cell><cell>6.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_T5EZHhN">Acknowledgements</head><p xml:id="_n5vkGcS"><s xml:id="_CgVYkB9">This work has been supported by <rs type="funder">NIDA</rs> <rs type="grantNumber">P50 DA039838</rs> (<rs type="funder">PI Linda Collins</rs>), <rs type="funder">NIAAA</rs> <rs type="grantNumber">R01 AA023187</rs> (PI S. Murphy), <rs type="funder">NHLBI/</rs><rs type="grantNumber">NIA R01 HL125440</rs> (PI: PK), <rs type="grantNumber">NIBIB U54EB020404</rs> (PI: SK). <rs type="person">A. Tewari</rs> acknowledges the support of a <rs type="grantName">Sloan Research Fellowship</rs> and an <rs type="funder">NSF</rs> <rs type="grantName">CAREER grant</rs> <rs type="grantNumber">IIS-1452099</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ByRvygq">
					<idno type="grant-number">P50 DA039838</idno>
				</org>
				<org type="funding" xml:id="_ADyGAd7">
					<idno type="grant-number">R01 AA023187</idno>
				</org>
				<org type="funding" xml:id="_NzFvaEV">
					<idno type="grant-number">NIA R01 HL125440</idno>
				</org>
				<org type="funding" xml:id="_Fe8YMUG">
					<idno type="grant-number">NIBIB U54EB020404</idno>
					<orgName type="grant-name">Sloan Research Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_SuPYr92">
					<idno type="grant-number">IIS-1452099</idno>
					<orgName type="grant-name">CAREER grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_r5VHAhh">Using confidence bounds for exploitation-exploration trade-offs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_X27CKKY">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="2002-11">2002. Nov</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Auer P (2002) Using confidence bounds for exploitation-exploration trade-offs. J Mach Learn Res 3(Nov):397-422</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_YSYkhGK">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1013689704352</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VVZz77K">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Auer P, Cesa-Bianchi N, Fischer P (2002) Finite-time analysis of the multiarmed bandit problem. Mach Learn 47(2-3):235-256</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_8HBqwDT">Persuasive e-health design for behavior change</title>
		<author>
			<persName><forename type="first">H</forename><surname>Baumeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baumel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pryss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E-M</forename><surname>Messner</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-31620-4_17</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_puWacva">Mobile sensing and digital phenotyping: new developments in psychoinformatics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Baumeister</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Montag</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="x" to="x" />
		</imprint>
	</monogr>
	<note type="raw_reference">Baumeister H, Kraft R, Baumel A, Pryss R, Messner E-M (2019) Persuasive e-health design for behavior change. In: Baumeister H, Montag C (eds) Mobile sensing and digital phenotyping: new developments in psychoinformatics. Springer, Berlin, pp x-x</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main" xml:id="_DCktc3E">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<idno type="DOI">10.1108/03684920710743466</idno>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Bishop CM (2007) Pattern recognition and machine learning. Springer</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_njqvHzU">Regret analysis of stochastic and nonstochastic multi-armed bandit problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesa-</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename></persName>
		</author>
		<idno type="DOI">10.1561/9781601986276</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XGBZ84J">Found Trends® Mach Learn</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Bubeck S, Cesa-Bianchi N (2012) Regret analysis of stochastic and nonstochastic multi-armed bandit problems. Found Trends® Mach Learn 5(1):1-122</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_uaRV9JM">An empirical evaluation of thompson sampling</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/1120.003.0083</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qTm594N">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chapelle O, Li L (2011) An empirical evaluation of thompson sampling. In: Advances in neural information processing systems, pp 2249-2257</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_VrRyZy5">Large-scale validation and analysis of interleaved search evaluation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<idno type="DOI">10.1145/2094072.2094078</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5VJN8g8">ACM Trans Inf Syst (TOIS)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chapelle O, Joachims T, Radlinski F, Yue Y (2012) Large-scale validation and analysis of interleaved search evaluation. ACM Trans Inf Syst (TOIS) 30(1):6</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_ZXStTQV">A behavior model for persuasive design</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Fogg</surname></persName>
		</author>
		<idno type="DOI">10.1145/1541948.1541999</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tKZnZp5">Proceedings of the 4th international conference on persuasive technology</title>
		<meeting>the 4th international conference on persuasive technology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">40</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Fogg BJ (2009) A behavior model for persuasive design. In: Proceedings of the 4th international conference on persuasive technology, ACM, vol 40</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_q72FDup">Microrandomized trials: an experimental design for developing just-in-time adaptive interventions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rosenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S ;</forename><surname>Kegels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Hekler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shiffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Boruvka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QYvfyzs">Health Psychol</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">1220</biblScope>
			<date type="published" when="1952">1952. 2015</date>
		</imprint>
	</monogr>
	<note>Health belief model. United States Public Health Service</note>
	<note type="raw_reference">Hochbaum G, Rosenstock I, Kegels S (1952) Health belief model. United States Public Health Service Klasnja P, Hekler EB, Shiffman S, Boruvka A, Almirall D, Tewari A, Murphy SA (2015) Microran- domized trials: an experimental design for developing just-in-time adaptive interventions. Health Psychol 34(S):1220</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_ZRM3jmG">Connecting domains-ecological momentary assessment in a mobile sensing framework</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kubiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Smyth</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-31620-4_12</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tTuMrEW">Mobile sensing and digital phenotyping: new developments in psychoinformatics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Baumeister</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Montag</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="x" to="x" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kubiak T, Smyth JM (2019) Connecting domains-ecological momentary assessment in a mobile sensing framework. In: Baumeister H, Montag C (eds) Mobile sensing and digital phenotyping: new developments in psychoinformatics. Springer, Berlin, pp x-x</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_msEv4JD">An actor-critic contextual bandit algorithm for personalized interventions using mobile devices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kWMGJaZ">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lei, H., Tewari, A., &amp; Murphy, S. (2014) An actor-critic contextual bandit algorithm for personalized interventions using mobile devices. Advances in Neural Information Processing Systems, 27</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_MsDQuQS">A contextual-bandit approach to personalized news article recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="DOI">10.1145/1772690.1772758</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FnKNAme">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">Li L, Chu W, Langford J, Schapire RE (2010) A contextual-bandit approach to personalized news article recommendation. In: Proceedings of the 19th international conference on World wide web, ACM, pp 661-670</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_6CWVm2f">mHealth applications: potentials, limitations, current quality and future directions</title>
		<author>
			<persName><forename type="first">E-M</forename><surname>Messner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>'rourke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baumeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-31620-4_15</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nvVTYr2">Mobile sensing and digital phenotyping: new developments in psychoinformatics</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Baumeister</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Montag</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="x" to="x" />
		</imprint>
	</monogr>
	<note type="raw_reference">Messner E-M, Probst T, O&apos;Rourke T, Baumeister H., Stoyanov S (2019) mHealth applications: potentials, limitations, current quality and future directions. In: Baumeister H, Montag C (eds) Mobile sensing and digital phenotyping: new developments in psychoinformatics. Springer, Berlin, pp x-x</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_jgyMuj5">Justin-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support</title>
		<author>
			<persName><forename type="first">Nahum-Shani I</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DUtWRHf">Ann Behav Med</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="446" to="462" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nahum-Shani I, Smith SN, Spring BJ, Collins LM, Witkiewitz K, Tewari A, Murphy SA (2017) Just- in-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support. Ann Behav Med 52(6):446-462</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_wzGpKDY">MyBehavior: automatic personalized health feedback from user behaviors and preferences using smartphones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.1145/2750858.2805840</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vhqHjVF">Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing</title>
		<meeting>the 2015 ACM international joint conference on pervasive and ubiquitous computing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="707" to="718" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi M, Aung MH, Zhang M, Choudhury T (2015) MyBehavior: automatic personalized health feedback from user behaviors and preferences using smartphones. In: Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing, pp 707-718</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_xMdvNVc">Towards health recommendation systems: an approach for providing automated personalized health feedback from mobile data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-51394-2_26</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aTpSHG9">Mobile health</title>
		<imprint>
			<biblScope unit="page" from="519" to="542" />
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi M, Aung MH, Choudhury T (2017) Towards health recommendation systems: an approach for providing automated personalized health feedback from mobile data. Mobile health. Springer, Cham, pp 519-542</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_ZTJf2tC">Feasibility and acceptability of mobile phone-based auto-personalized physical activity recommendations for chronic pain self-management: pilot study on adults</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.2196/10147</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kZxwKDB">J Med Internet Res</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10147</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi M, Aung MS, Gay G, Reid MC, Choudhury T (2018) Feasibility and acceptability of mobile phone-based auto-personalized physical activity recommendations for chronic pain self-management: pilot study on adults. J Med Internet Res 20(10):e10147</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_87753E2">Reinforcement learning: an introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton RS, Barto AG (1998) Reinforcement learning: an introduction. MIT Press</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_TvkANdy">A one-armed bandit problem with a concomitant variable</title>
		<author>
			<persName><forename type="first">M</forename><surname>Woodroofe</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1979.10481033</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bATUPrp">J Am Stat Assoc</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">368</biblScope>
			<biblScope unit="page" from="799" to="806" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Woodroofe M (1979) A one-armed bandit problem with a concomitant variable. J Am Stat Assoc 74(368):799-806</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
