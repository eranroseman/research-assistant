<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_cefbfNc">Reinforcement Learning in Healthcare: A Survey</title>
				<funder ref="#_hsuvQC7">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-04-24">24 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Jiming</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shamim</forename><surname>Nemati</surname></persName>
						</author>
						<title level="a" type="main" xml:id="_cAcFmN7">Reinforcement Learning in Healthcare: A Survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-24">24 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">F69FBD1F661247EDD970820F9CF6FC6F</idno>
					<idno type="arXiv">arXiv:1908.08796v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T11:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_E2WBke7">Reinforcement Learning</term>
					<term xml:id="_VkwpFym">Healthcare</term>
					<term xml:id="_ssjhdbJ">Dynamic Treatment Regimes</term>
					<term xml:id="_Te5xHUP">Critical Care</term>
					<term xml:id="_pJZvhWB">Chronic Disease</term>
					<term xml:id="_pUGajd6">Automated Diagnosis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_8wg8U77"><p xml:id="_YZKGuuT"><s xml:id="_RcA6jrX">As a subfield of machine learning, reinforcement learning (RL) aims at empowering one's capabilities in behavioural decision making by using interaction experience with the world and an evaluative feedback.</s><s xml:id="_8sVrJmv">Unlike traditional supervised learning methods that usually rely on one-shot, exhaustive and supervised reward signals, RL tackles with sequential decision making problems with sampled, evaluative and delayed feedback simultaneously.</s><s xml:id="_8ceAqFt">Such distinctive features make RL technique a suitable candidate for developing powerful solutions in a variety of healthcare domains, where diagnosing decisions or treatment regimes are usually characterized by a prolonged and sequential procedure.</s><s xml:id="_UEQwZ9U">This survey discusses the broad applications of RL techniques in healthcare domains, in order to provide the research community with systematic understanding of theoretical foundations, enabling methods and techniques, existing challenges, and new insights of this emerging paradigm.</s><s xml:id="_hxYtwGA">By first briefly examining theoretical foundations and key techniques in RL research from efficient and representational directions, we then provide an overview of RL applications in healthcare domains ranging from dynamic treatment regimes in chronic diseases and critical care, automated medical diagnosis from both unstructured and structured clinical data, as well as many other control or scheduling domains that have infiltrated many aspects of a healthcare system.</s><s xml:id="_vbrPwPR">Finally, we summarize the challenges and open issues in current research, and point out some potential solutions and directions for future research.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_VWUdR9Q">I. INTRODUCTION</head><p xml:id="_BjnEvMU"><s xml:id="_G3ErjFU">Driven by the increasing availability of massive multimodality data, and developed computational models and algorithms, the role of AI techniques in healthcare has grown rapidly in the past decade <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_5X8nCmZ">This emerging trend has promoted increasing interests in the proposal of advanced data analytical methods and machine learning approaches in a variety of healthcare applications <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_mfPACRg">As as a subfield in machine learning, reinforcement learning (RL) has achieved tremendous theoretical and technical achievements in generalization, representation and efficiency in recent years, leading to its increasing applicability to real-life problems in playing games, robotics control, financial and business management, autonomous driving, natural language processing, computer vision, biological data analysis, and art creation, just to name a few <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</s></p><p xml:id="_hHmAnrg"><s xml:id="_RZRyjSA">In RL problems, an agent chooses an action at each time step based on its current state, and receives an evaluative Chao Yu is with the School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China.</s><s xml:id="_TrpdUKx">(Email: yuchao3@mail.sysu.edu.cn).</s><s xml:id="_hEbwyqx">Jiming Liu is with the Computer Science Department, Hong Kong Baptist University, Kowloon Tong, Hong Kong.</s><s xml:id="_sg6dFYe">(Email: jiming@Comp.HKBU.Edu.HK).</s><s xml:id="_ScMknvr">Shamim Nemati is with the Department of Biomedical Informatics, UC San Diego, La Jolla, CA, USA.</s><s xml:id="_m2zPRvC">(Email: snemati@health.ucsd.edu).</s></p><p xml:id="_Pu3pXKt"><s xml:id="_XPbamq4">feedback and the new state from the environment.</s><s xml:id="_vd6PePD">The goal of the agent is to learn an optimal policy (i.e., a mapping from the states to the actions) that maximizes the accumulated reward it receives over time.</s><s xml:id="_KxAMCma">Therefore, agents in RL do not receive direct instructions regarding which action they should take, instead they must learn which actions are the best through trial-and-error interactions with the environment.</s><s xml:id="_M9QvyPD">This adaptive closed-loop feature renders RL distinct from traditional supervised learning methods for regression or classification, in which a list of correct labels must be provided, or from unsupervised learning approaches to dimensionality reduction or density estimation, which aim at finding hidden structures in a collection of example data <ref type="bibr" target="#b10">[11]</ref>.</s><s xml:id="_5JrYWuA">Moreover, in comparison with other traditional control-based methods, RL does not require a well-represented mathematical model of the environment, but develops a control policy directly from experience to predict states and rewards during a learning procedure.</s><s xml:id="_88tNtu5">Since the design of RL is letting an agent controller interact with the system, unknown and time-varying dynamics as well as changing performance requirements can be naturally accounted for by the controller <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_VfmN6pt">Lastly, RL is uniquely suited to systems with inherent time delays, in which decisions are performed without immediate knowledge of effectiveness, but evaluated by a long-term future reward.</s></p><p xml:id="_tGt4vjH"><s xml:id="_Z4H9P6W">The above features naturally make RL an attractive solution to constructing efficient policies in various healthcare domains, where the decision making process is usually characterized by a prolonged period or sequential procedure <ref type="bibr" target="#b15">[16]</ref>.</s><s xml:id="_5nmKVkR">Typically, a medical or clinical treatment regime is composed of a sequence of decision to determine the course of decisions such as treatment type, drug dosage, or re-examination timing at a time point according to the current health status and prior treatment history of an individual patient, with a goal of promoting the patient's long-term benefits.</s><s xml:id="_FeWxRQ7">Unlike the common procedure in traditional randomized controlled trials that derive treatment regimes from the average population response, RL can be tailored for achieving precise treatment for individual patients who may possess high heterogeneity in response to the treatment due to variety in disease severity, personal characteristics and drug sensitivity.</s><s xml:id="_54KEVuE">Moreover, RL is able to find optimal policies using only previous experiences, without requiring any prior knowledge about the mathematical model of the biological systems.</s><s xml:id="_MAKPNvK">This makes RL more appealing than many existing control-based approaches in healthcare domains since it could be usually difficult or even impossible to build an accurate model for the complex human body system and the responses to administered treatments, due to nonlinear, varying and delayed interaction between treatments and human bodies.</s></p><p xml:id="_gG7ZxfV"><s xml:id="_hq6XXHq">Thus far, a plethora of theoretical or experimental studies have applied RL techniques and models in a variety of heathcare domains, achieving performance exceeding that of</s></p><p xml:id="_vpX7MZh"><s xml:id="_B8SWMqG">TABLE I SUMMARY OF ABBREVIATIONS IN RL Acronym Description AC Actor-Critic A3C Asynchronous Advantage Actor Critic BRL Batch Reinforcement Learning DDPG Deep Deterministic Policy Gradient DRL Deep Reinforcement Learning DP Dynamic programming DQN Deep Q Network DDQN Dueling DQN DDDQN Double Dueling DQN FQI-SVG/ERT Fitted Q Iteration with Support Vector Regression/Extremely Randomized Trees GAN Generative Adversarial Net HRL Hierarchical Reinforcement Learning IRL Inverse Reinforcement Learning LSPI Least-Squares Policy Iteration MDP Markov Decision Process MC Monte Carlo NAC Natural Actor Critic PAC Probably Approximately Correct PI Policy Iteration PS Policy Search POMDP Partially Observed Markov Decision Process PORL Partially Observed Reinforcement Learning PPO Proximal Policy Optimization PRL Preference-based Reinforcement Learning RRL Relational Reinforcement Learning TD Temporal Difference TRL Transfer Reinforcement Learning TRPO Trust Region Policy Optimization VI Value Iteration</s></p><p xml:id="_AtXZGGS"><s xml:id="_ywzEk2P">alternative techniques in many cases.</s><s xml:id="_2e8S2Ny">This survey aims at providing an overview of such successful RL applications, covering adaptive treatment regimes in chronic diseases and critical care, automated clinical diagnosis, as well as many other healthcare domains such as clinical resource allocation/scheduling and optimal process control.</s><s xml:id="_VtQTPCz">We also discuss the challenges, open issues and future directions of research necessary to advance further successful applications of RL in healthcare.</s><s xml:id="_JCcFWWH">By this, we hope this survey can provide the research community with systematic understanding of foundations, enabling methods and techniques, challenges, and new insights of this emerging paradigm.</s><s xml:id="_yu8dMHM">Section II provides a structured summarization of the theoretical foundations and key techniques in RL research from two main directions: efficient directions that mainly aim at improving learning efficiency by making best use of past experience or knowledge, and representational directions that focus on constructive or relational representation problems in RL.</s><s xml:id="_ADdzcun">Then, Section III gives a global picture of application domains of RL in healthcare, each of which is discussed in more detail in the following sections.</s><s xml:id="_ujaYXDT">Section IV discusses dynamic treatment regimes in both chronic disease and critical care, and Section V describes automated medical diagnosis using either structured or unstructured medical data.</s><s xml:id="_KNbbkY2">In addition, VI talks about other more broad application domains including health resources allocation and scheduling, optimal process control, drug discovery and development, as well as health management.</s><s xml:id="_SSzy4VJ">Section VII describes several challenges and open issues in current research.</s><s xml:id="_EkQRnKr">Finally, Section VIII discusses potential directions that are necessary in the future research.</s></p><p xml:id="_weJ38tX"><s xml:id="_kewwcvf">For convenience, Tables <ref type="table">I</ref> and <ref type="table">II</ref> summarize the main acronyms in RL and healthcare domains, respectively.</s><s xml:id="_jpJaX7n">the agent after taking action a in state s; and γ ∈ [0, 1] is a discount factor.</s><s xml:id="_4dPdAMR">An agent's policy π : S × A → [0, 1] is a probability distribution that maps an action a ∈ A to a state s ∈ S. When given an MDP and a policy π, the expected reward of following this policy when starting in state s, V π (s), can be defined as follows:</s></p><formula xml:id="formula_0">V π (s) E π ∞ t=0 γ t R(s t , π(s t ))|s 0 = s<label>(1)</label></formula><p xml:id="_cSxg2eD"><s xml:id="_sQevq4Z">The value function can also be defined recursively using the Bellman operator B π :</s></p><formula xml:id="formula_1">B π V π (s) R(s, π(s)) + γ s ∈S P(s, a, s )V π (s )<label>(2)</label></formula><p xml:id="_adCENfA"><s xml:id="_K7MQG7Z">Since the Bellman operator B π is a contraction mapping of value function V , there exists a fixed point of value V π such that B π V π = V π in the limit.</s><s xml:id="_yj4Bryn">The goal of an MDP problem is to compute an optimal policy π * such that V π * (s) ≥ V π (s) for every policy π and every state s ∈ S. To involve the action information, Q-value is used to represent the optimal value of each state-action pair by Equation <ref type="formula">3</ref>.</s></p><formula xml:id="formula_2">Q * (s, a) = R(s, a) + γ s ∈S P(s, a, s ) max a ∈A Q(s , a ) (3)</formula><p xml:id="_bsqGMC9"><s xml:id="_VWaRCUA">2) Basic Solutions and Challenging Issues: Many solution techniques are available to compute an optimal policy for a given MDP.</s><s xml:id="_fnVGvYw">Broadly, these techniques can be categorized as model-based or model-free methods, based on whether a complete knowledge of the MDP model can be specified a priori.</s><s xml:id="_C6CTps4">Model-based methods, also referred to as planning methods, require a complete description of the model in terms of the transition and reward functions, while model-free methods, also referred to as learning methods, learn an optimal policy simply based on received observations and rewards.</s></p><p xml:id="_NX4AqWY"><s xml:id="_ENcj7xK">Dynamic programming (DP) <ref type="bibr" target="#b16">[17]</ref> is a collection of modelbased techniques to compute an optimal policy given a complete description of an MDP model.</s><s xml:id="_MZfvgg9">DP includes two main different approaches: Value Iteration (VI) and Policy Iteration (PI).</s><s xml:id="_Tz2NpRv">VI specifies the optimal policy in terms of value function Q * (s, a) by iterating the Bellman updating as follows:</s></p><formula xml:id="formula_3">Q t+1 (s, a) = R(s, a) + γ s ∈S P(s, a, s ) max a ∈A Q t (s , a ) (4)</formula><p xml:id="_7d4XX7e"><s xml:id="_rtExRxN">For each iteration, the value function of every state s is updated one step further into the future based on the current estimate.</s><s xml:id="_b5CYDn3">The concept of updating an estimate based on the basis of other estimates is often referred to as bootstrapping.</s><s xml:id="_UP9NEHQ">The value function is updated until the difference between two iterations, Q t and Q t+1 , is less than a small threshold.</s><s xml:id="_tpKvg54">The optimal policy is then derived using π * (s) = arg max a∈A Q * .</s><s xml:id="_9QBgYBg">Unlike VI, PI learns the policy directly.</s><s xml:id="_PdhtHR3">It starts with an initial random policy π, and iteratively updates the policy by first computing the associated value function Q π (policy evaluation or prediction) and then improving the policy using π(s) = arg max a∈A Q(s, a) (policy improvement or control).</s></p><p xml:id="_dsRbC3P"><s xml:id="_8YxTyud">Despite being mathematically sound, DP methods require a complete and accurate description of the environment model, which is unrealistic in most applications.</s><s xml:id="_kVxhS77">When a model of the problem is not available, the problem can then be solved by using direct RL methods, in which an agent learns its optimal policy while interacting with the environment.</s><s xml:id="_F4MtTW3">Monte Carlo (MC) methods and Temporal difference (TD) methods are two main such methods, with the difference of using episode-byepisode update in MC or step-by-step update in TD.</s><s xml:id="_hc45wGs">Denote R</s></p><p xml:id="_geVB7j2"><s xml:id="_qYDQjj5">(n) t = R t+1 + γR t+2 + ... + γ n-1 R t+n + γ n V t (s t+n ) nstep return at time t, then the general n-step update rule in TD methods is defined by ∆V t (s t ) = α[R (n) t -V t (s t )], in which α ∈ (0, 1] is an appropriate learning rate controlling the contribution of the new experience to the current estimate.</s><s xml:id="_yzGmMzP">MC methods then can be considered as an extreme case of TD methods when the update is conducted after the whole episode of steps.</s><s xml:id="_vRdwmrT">In spite of having higher complexity in analyzing the efficiency and speed of convergence, TD methods usually require less memory for estimates and less computation, thus are easier to implement.</s></p><p xml:id="_Jze5TAf"><s xml:id="_vttF8VP">If the value function of a policy π is estimated by using samples that are generated by strictly following this policy, the RL algorithm is called on-policy, while off-policy algorithms can learn the value of a policy that is different from the one being followed.</s><s xml:id="_wVkVBAE">One of the most important and widely used RL approach is Q-learning <ref type="bibr" target="#b17">[18]</ref>, which is an off-policy TD algorithm.</s><s xml:id="_qy6mdpH">Its one-step updating rule is given by Equation <ref type="formula">5</ref>,</s></p><formula xml:id="formula_4">Q t+1 (s, a) = Q t (s, a)+α t [R(s, a)+γ max a Q t (s , a )-Q t (s, a)]</formula><p xml:id="_JcGKnbc"><s xml:id="_94fZSsv">(5) where α ∈ (0, 1] is an appropriate learning rate which controls the contribution of the new experience to the current estimate.</s></p><p xml:id="_wn94Kpz"><s xml:id="_euBSFnc">Likewise, the SARSA algorithm <ref type="bibr" target="#b18">[19]</ref> is an representation for on-policy TD approaches given by Equation <ref type="formula" target="#formula_5">6</ref>:</s></p><formula xml:id="formula_5">Q t+1 (s, a) = Q t (s, a)+α t [R(s, a)+γQ t (s , π(s ))-Q t (s, a)]<label>(6)</label></formula><p xml:id="_4SjxxNz"><s xml:id="_yrU9zg8">The idea is that each experienced sample brings the current estimate Q(s, a) closer to the optimal value Q * (s, a).</s><s xml:id="_2XuBS7W">Qlearning starts with an initial estimate for each state-action pair.</s><s xml:id="_KfvBFDE">When an action a is taken in state s, resulting in the next state s , the corresponding Q-value Q(s, a) is updated with a combination of its current value and the TD error ( R(s, a)</s></p><formula xml:id="formula_6">+ γ max a Q t (s , a ) -Q t (s, a) for Q-learning, or R(s, a)+γQ t (s , π(s ))-Q t (s, a) for SARSA).</formula><p xml:id="_ACNyaAV"><s xml:id="_nrNGagc">The TD error is the difference between the current estimate Q(s, a) and the expected discounted return based on the experienced sample.</s><s xml:id="_VZ7Ufps">The Q value of each state-action pair is stored in a table for a discrete state-action space.</s><s xml:id="_9HMzSCb">It has been proved that this tabular Q-learning converges to the optimal Q * (s, a) w.p.1 when all state-action pairs are visited infinitely often and an appropriate exploration strategy and learning rate are chosen <ref type="bibr" target="#b17">[18]</ref>.</s></p><p xml:id="_tdhAWE5"><s xml:id="_BbWdJqQ">Besides the above value-function based methods that maintain a value function whereby a policy can be derived, direct policy-search (PS) algorithms <ref type="bibr" target="#b19">[20]</ref> try to estimate the policy directly without representing a value function explicitly, whereas the actor-critic (AC) methods <ref type="bibr" target="#b20">[21]</ref> keep separate, explicit representations of both value functions and policies.</s></p><p xml:id="_458GWgf"><s xml:id="_PchJHqz">In AC methods, the actor is the policy to select actions, and the critic is an estimated value function to criticize the actions chosen by the actor.</s><s xml:id="_XKq6jjC">After each action execution, the critic evaluates the performance of action using the TD error.</s><s xml:id="_jMDgf2a">The advantages of AC methods include that they are more appealing in dealing with large scale or even continuous actions and learning stochastic policies, and more easier in integrating domain specific constraints on policies.</s></p><p xml:id="_xbBdVKW"><s xml:id="_NTKvg85">In order to learn optimal policies, an RL agent should make a balance between exploiting the knowledge obtained so far by acting optimally, and exploring the unknown space in order to find new efficient actions.</s><s xml:id="_2UxygVK">Such an explorationexploitation trade-off dilemma is one of the most fundamental theoretical issues in RL, since an effective exploration strategy enables the agent to make an elegant balance between these two processes by choosing explorative actions only when this behavior can potentially bring a higher expected return.</s><s xml:id="_6zUcHuH">A large amount of effort has been devoted to this issue in the traditional RL community, proposing a wealth of exploration strategies including simple heuristics such as ε-greedy and Boltzmann exploration, Bayesian learning <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, countbased methods with Probably Approximately Correct (PAC) guarantees <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, as well as more expressive methods of intrinsic motivation such as novelty, curiosity and surprise <ref type="bibr" target="#b25">[26]</ref>.</s><s xml:id="_yPEMjZ5">For example, the ε-greedy strategy selects the greedy action, arg max a Q t (s, a), with a high probability, and, occasionally, with a small probability selects an action uniformly at random.</s><s xml:id="_TYNZw9w">This ensures that all actions and their effects are experienced.</s><s xml:id="_4NVBHDC">The ε-greedy exploration policy can be given by Equation <ref type="formula" target="#formula_7">7</ref>.</s></p><formula xml:id="formula_7">π(a ) = 1 -ε if a = arg max a Q(s, a), ε otherwise. (<label>7</label></formula><formula xml:id="formula_8">)</formula><p xml:id="_92z6KxD"><s xml:id="_6ucCX93">where ε ∈ [0, 1] is an exploration rate.</s></p><p xml:id="_MtdgvQS"><s xml:id="_Wedbd9d">Other fundamental issues in RL research include but are not limited to the credit assignment problem <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b26">[27]</ref>, the sampel/space/time complexity <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, function approximation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, safety <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, robustness <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, and interpretability <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>.</s><s xml:id="_vrZEDec">A more comprehensive and in-depth review on these issues can be found in <ref type="bibr" target="#b37">[38]</ref>, and more recently in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qXm6B86">B. Key Techniques in RL</head><p xml:id="_9y2MruK"><s xml:id="_bbAtA3w">This section discusses some key techniques used in contemporary RL, most of which can be understood in the light of the framework and solutions defined in the section ahead, yet these new techniques emphasize more sophisticated use of samples, models of the world and learned knowledge of previous tasks for efficiency purpose, as well as what should be represented and how things should be represented during an RL problem.</s><s xml:id="_FFajRPz">Note that the classification of these two kinds of techniques are not mutually exclusive, which means that some representation techniques are also used for improving the learning efficiency, and vice versa.</s></p><p xml:id="_rf8M6Wn"><s xml:id="_dUrAjyn">1) Efficient Techniques: The purpose of using efficient techniques is to improve the learning performance in terms of, for example, convergence ratio, sample efficient, computation cost or generalization capabilities of an RL method.</s><s xml:id="_tpnxXtq">This improvement can be achieved by using different levels of knowledge: the Experience-level techniques focus on utilizing the past experience for more stable and data-efficient learning; the Model-level techniques focus on building and planning over a model of the environment in order to improve sample efficiency; while the Task-level techniques aim at generalizing the learning experience from past tasks to new relevant ones.</s></p><p xml:id="_kDv7jMB"><s xml:id="_tKUmYrw">a) Experience-level: In traditional pure on-line TD learning methods such as Q-learning and SARSA, an agent immediately conducts a DP-like update of the value functions every step interacting with the environment and then disregards the experienced state transition tuple afterwards.</s><s xml:id="_UdJaJgT">In spite of guaranteed convergence and great success in solving simple toy problems, this kind of local updates poses several severe performance problems when applied to more realistic systems with larger and possibly continuous settings.</s><s xml:id="_6XHP3aE">Since each experience tuple is used only for one update and then forgotten immediately, a larger number of samples are required to enable an optimal solution, causing the so called exploration overhead problem.</s><s xml:id="_2fpSFGK">Moreover, it has been shown that directly combining function approximation methods with pure on-line TD methods can cause instable or even diverged performance <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>.</s><s xml:id="_H7uZ4ym">These inefficiency and instability problems become even more pronounced in real environments, particularly in healthcare systems, where physical interactions between patients and environments call for more efficient sampling and stable learning methods.</s></p><p xml:id="_8ZrTzTG"><s xml:id="_qQHcnAH">The Experience-level techniques focus on how to make the best of the past learning experience for more stable and efficient learning, and are the major driving force behind the proposal of modern Batch RL (BRL) <ref type="bibr" target="#b38">[39]</ref>.</s><s xml:id="_5bZdtw3">In BRL, two basic techniques are used: storing the experience in a buffer and reusing it as if it were new (the idea of experience replay for addressing the inefficiency problem), and separating the DP step from the function approximation step by using a supervised learning to fit the function approximator over the sampled experience (the idea of fitting for addressing the instability problem).</s><s xml:id="_WEdBGBs">There are several famous BRL approaches in the literature, such as the non-linear approximator cases of Neural Fitted Q Iteration (NFQI <ref type="bibr" target="#b39">[40]</ref>), the Tree-based FQI <ref type="bibr" target="#b40">[41]</ref>, and robust linear approximation techniques for policy learning such as Least-Squares Policy Iteration (LSPI <ref type="bibr" target="#b41">[42]</ref>).</s><s xml:id="_Wgvgar8">As will be discovered later, these BRL methods have enjoyed wide and successful applications in clinical decision makings, due to their promise in greatly improving learning speed and approximation accuracy, particularly from limited amounts of clinical data.</s></p><p xml:id="_ndVzcyR"><s xml:id="_gXBqzvE">b) Model-level: Unlike Experience-level techniques that emphasize the efficient use of experience tuples, the Modellevel techniques try to build a model of the environment (in terms of the transition and reward functions) and then derive optimal policies from the environment model when it is approximately correct.</s><s xml:id="_TkKNRvt">This kind of model-based RL (MRL) approaches is rather different from the model-free RL methods such as TD methods or MC methods that directly estimate value functions without building a model of the environment <ref type="bibr" target="#b42">[43]</ref>.</s><s xml:id="_AGv7pP4">Using some advanced exploration strategies and planning methods such as DP or Monte Carlo Tree Search (MCTS) <ref type="bibr" target="#b43">[44]</ref>, MRL methods are usually able to learn an accurate model quickly and then use this model to plan multi-step actions.</s><s xml:id="_JcHbRQh">Therefore, MRL methods normally have better sample efficiency than model-free methods <ref type="bibr" target="#b27">[28]</ref>.</s></p><p xml:id="_9f6XhEY"><s xml:id="_A5yY7p7">c) Task-level: A higher task-level of efficient approaches focuses on the development of methods to transfer knowledge from a set of source tasks to a target task.</s><s xml:id="_sbdD7AX">Transfer RL (TRL) uses the transferred knowledge to significantly improve the learning performance in the target task, e.g., by reducing the samples needed for a nearly optimal performance, or increasing the final convergence level <ref type="bibr" target="#b44">[45]</ref>.</s><s xml:id="_DCHE3TF">Taylor and Stone <ref type="bibr" target="#b45">[46]</ref> provided a thorough review on TRL approaches by five transfer dimensions: how the source task and target task may differ (e.g., in terms of action, state, reward or transition functions), how to select the source task (e.g., all previously seen tasks, or only one task specified by human or modified automatically), how to define task mappings (e.g., specified by human or learned from experience), what knowledge to transferred (from experience instances to higher level of models or rules), and allowed RL methods (e.g., MRL, PS, or BRL).</s></p><p xml:id="_Zbu6MF5"><s xml:id="_rUQRZCK">2) Representational Techniques: Unlike traditional machine learning research that simply focuses on feature engineering for function approximation, representational techniques in RL can be in a broader perspective, paying attention to constructive or relational representation problems relevant not only to function approximation for state/action, polices and value functions, but also to more exogenous aspects regarding agents, tasks or models <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_WQ8ApBa">a) Representation for Value Functions or Policies: Many traditional RL algorithms have been mainly designed for problems with small discrete state and action spaces, which can be explicitly stored in tables.</s><s xml:id="_uNqbqQn">Despite the inherent challenges, applying these RL algorithms to continuous or highly dimensional domains would cause extra difficulties.</s><s xml:id="_sQvSDeC">A major aspect of representational techniques is to represent structures of policies and value functions in a more compact form for an efficient approximation of solutions and thus scaling up to larger domains.</s><s xml:id="_rp2Yhjj">Broadly, three categories of approximation methods can be clarified <ref type="bibr" target="#b30">[31]</ref>: model-approximation methods that approximate the model and compute the desired policy on this approximated model; value-approximation methods that approximate a value function whereby a policy can be inferred, and policy-approximation methods that search in policy space directly and update this policy to approximate the optimal policy, or keep separate, explicit representations of both value functions and policies.</s></p><p xml:id="_Sk979Tw"><s xml:id="_H9Pjnnc">The value functions or policies can be parameterized using either linear or non-linear function approximation presentations.</s><s xml:id="_ac4Bk5M">Whereas the linear function approximation is better understood, simple to implement and usually has better convergence guarantees, it needs explicit knowledge about domain features, and also prohibits the representation of interactions between features.</s><s xml:id="_5awzRaA">On the contrary, non-linear function approximation methods do not need for good informative features and usually obtain better accuracy and performance in practice, but with less convergence guarantees.</s></p><p xml:id="_NAR64v3"><s xml:id="_apqHDDH">A notable success of RL in addressing real world complex problems is the recent integration of deep neural networks into RL <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, fostering a new flourishing research area of Deep RL (DRL) <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_cuKVNsP">A key factor in this success is that deep learning can automatically abstract and extract high-level features and semantic interpretation directly from the input data, avoiding complex feature engineering or delicate feature hand-crafting and selection for an individual task <ref type="bibr" target="#b48">[49]</ref>.</s><s xml:id="_nHBJa67">b) Representation for Reward Functions: In a general RL setting, the reward function is represented in the form of an evaluative scalar signal, which encodes a single objective for the learning agent.</s><s xml:id="_2J77P5y">In spite of its wide applicability, this kind of quantifying reward functions has its limits inevitably.</s><s xml:id="_THAvSrg">For example, real life problems usually involve two or more objectives at the same time, each with its own associated reward signal.</s><s xml:id="_fjh24sr">This has motivated the emerging research topic of multi-objective RL (MORL) <ref type="bibr" target="#b49">[50]</ref>, in which a policy must try to make a trade-off between distinct objectives in order to achieve a Pareto optimal solution.</s><s xml:id="_Q6JccHP">Moreover, it is often difficult or even impossible to obtain feedback signals that can be expressed in numerical rewards in some real-world domains.</s><s xml:id="_9xwWkDk">Instead, qualitative reward signals such as being better or higher may be readily available and thus can be directly used by the learner.</s><s xml:id="_eXqHK2A">Preference-based RL (PRL) <ref type="bibr" target="#b50">[51]</ref> is a novel research direction combining RL and preference learning <ref type="bibr" target="#b51">[52]</ref> to equip an RL agent with a capability to learn desired policies from qualitative feedback that is expressed by various ranking functions.</s><s xml:id="_6Npss34">Last but not the least, all the existing RL methods are grounded on an available feedback function, either in an explicitly numerical or a qualitative form.</s><s xml:id="_EgN4m7f">However, when such feedback information is not readily available or the reward function is difficult to specify manually, it is then necessary to consider an approach to RL whereby the reward function can be learned from a set of presumably optimal trajectories so that the reward is consistent with the observed behaviors.</s><s xml:id="_GqZbqp4">The problem of deriving a reward function from observed behavior is referred to as Inverse RL (IRL) <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, which has received an increasingly high interest by researchers in the past few years.</s><s xml:id="_A3wycBj">Numerous IRL methods have been proposed, including the Maximum Entropy IRL <ref type="bibr" target="#b54">[55]</ref>, the Apprenticeship Learning <ref type="bibr" target="#b55">[56]</ref>, nonlinear representations of the reward function using Gaussian processes <ref type="bibr" target="#b56">[57]</ref>, and Bayesian IRL <ref type="bibr" target="#b57">[58]</ref>.</s></p><p xml:id="_sHP3fBu"><s xml:id="_qJnrzKd">c) Representation for Tasks or Models: Much recent research on RL has focused on representing the tasks or models in a compact way to facilitate construction of an efficient policy.</s><s xml:id="_6xVjG4u">Factored MDPs <ref type="bibr" target="#b58">[59]</ref> are one of such approaches to representing large structured MDPs compactly, by using a dynamic Bayesian network (DBN) to represent the transition model among states that involve only some set of state variables, and the decomposition of global task reward to individual variables or small clusters of variables.</s><s xml:id="_Czr8E2m">This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs also grows exponentially in the representation size.</s><s xml:id="_ftvZ3Bn">A large number of methods has been proposed to employ factored representation of MDP models for improving learning efficiency for either model-based <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> or model-free RL problems <ref type="bibr" target="#b61">[62]</ref>.</s><s xml:id="_Z3WQvmK">A more challenging issues is how to learn this compact structure dynamically during on-line learning <ref type="bibr" target="#b62">[63]</ref>.</s></p><p xml:id="_34VN5Ss"><s xml:id="_5KwpYY9">Besides the factored representation of states, a more general method is to decompose large complex tasks into smaller sets of sub-tasks, which can be solved separatively.</s><s xml:id="_7MMqJJe">Hierarchical RL (HRL) <ref type="bibr" target="#b63">[64]</ref> formalizes hierarchical methods that use abstract states or actions over a hierarchy of subtasks to decompose the original problem, potentially reducing its computational complexity.</s><s xml:id="_6erARFU">Hengst <ref type="bibr" target="#b64">[65]</ref> discussed the various concepts and approaches in HRL, including algorithms that can automatically learn the hierarchical structure from interactions with the domain.</s><s xml:id="_dVum3wB">Unlike HRL that focuses on hierarchical decomposition of tasks, Relational RL (RRL) <ref type="bibr" target="#b65">[66]</ref> provides a new representational paradigm to RL in worlds explicitly modeled in terms of objects and their relations.</s><s xml:id="_UA9hNz4">Using expressive data structures that represent the objects and relations in an explicit way, RRL aims at generalizing or facilitating learning over worlds with the same or different objects and relations.</s><s xml:id="_hjq9jrX">The main representation methods and techniques in RRL have been surveyed in detail in <ref type="bibr" target="#b65">[66]</ref>.</s></p><p xml:id="_mjmzXUs"><s xml:id="_sM96hNr">Last but not the least, Partially Observable MDP (POMDP) is widely adopted to represent models when the states are not fully observable, or the observations are noisy.</s><s xml:id="_WPf45PR">Learning in POMDP, denoted as Partially Observable RL (PORL), can be rather difficult due to extra uncertainties caused by the mappings from observations to hidden states <ref type="bibr" target="#b66">[67]</ref>.</s><s xml:id="_MY8AGac">Since environmental states in many real life applications, notably in healthcare systems, are only partially observable, PORL then becomes a suitable technique to derive a meaningful policy in such realistic environments.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cGeFthY">III. APPLICATIONS OF RL IN HEALTHCARE</head><p xml:id="_zfGWn5Y"><s xml:id="_T6zuass">On account of its unique features against traditional machine learning, statistic learning and control-based methods, RLrelated models and approaches have been widely applied in healthcare domains since decades ago.</s><s xml:id="_Grj83E3">The early days of focus has been devoted to the application of DP methods in various pharmacotherapeutic decision making problems using pharmacokinetic/pharmacodynamic (PK/PD) models <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>.</s><s xml:id="_yhFHprg">Hu et al., <ref type="bibr" target="#b69">[70]</ref> used POMDP to model drug infusion problem for the administration of anesthesia, and proposed efficient heuristics to compute suboptimal though useful treatment strategies.</s><s xml:id="_mvpWP83">Schaeffer et al. <ref type="bibr" target="#b70">[71]</ref> discussed the benefits and associated challenges of MDP modeling in the context of medical treatment, and reviewed several instances of medical applications of MDPs, such as spherocytosis treatment and breast cancer screening and treatment.</s></p><p xml:id="_S5U5RQp"><s xml:id="_c5u8GMu">With the tremendous theoretical and technical achievements in generalization, representation and efficiency in recent years, RL approaches have been successfully applied in a number of healthcare domains to date.</s><s xml:id="_6HgWmHX">Broadly, these application domains can be categorized into three main types: dynamic treatment regimes in chronic disease or critical care, automated medical diagnosis, and other general domains such as health resources allocation and scheduling, optimal process control, drug discovery and development, as well as health management.</s><s xml:id="_f5hH3Sy">Figure <ref type="figure" target="#fig_1">2</ref> provides a diagram outlining the application domains, illustrating how this survey is organized along the lines of the three broad domains in the field.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_UE7wuCm">IV. DYNAMIC TREATMENT REGIMES</head><p xml:id="_mNzG7Km"><s xml:id="_PbyEcyt">One goal of healthcare decision-making is to develop effective treatment regimes that can dynamically adapt to the varying clinical states and improve the long-term benefits of patients.</s><s xml:id="_D869E4Q">Dynamic treatment regimes (DTRs) <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, alternatively named as dynamic treatment policies <ref type="bibr" target="#b73">[74]</ref>, adaptive interventions <ref type="bibr" target="#b74">[75]</ref>, or adaptive treatment strategies <ref type="bibr" target="#b75">[76]</ref>, provide a new paradigm to automate the process of developing new effective treatment regimes for individual patients with long-term care <ref type="bibr" target="#b76">[77]</ref>.</s><s xml:id="_Vfy9Yc5">A DTR is composed of a sequence of decision rules to determine the course of actions (e.g., treatment type, drug dosage, or reexamination timing) at a time point according to the current health status and prior treatment history of an individual patient.</s><s xml:id="_yMymPaU">Unlike traditional randomized controlled trials that are mainly used as an evaluative tool for confirming the efficacy of a newly developed treatment, DTRs are tailored for generating new scientific hypotheses and developing optimal treatments across or within groups of patients <ref type="bibr" target="#b76">[77]</ref>.</s><s xml:id="_YkwBhKW">Utilizing valid data generated, for instance, from the Sequential Multiple Assignment Randomized Trial (SMART) <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b78">[79]</ref>, an optimal DTR that is capable of optimizing the final clinical outcome of particular interest can be derived.</s></p><p xml:id="_3w9B7P8"><s xml:id="_taFueY7">The design of DTRs can be viewed as a sequential decision making problem that fits into the RL framework well.</s><s xml:id="_Z8NkFcp">The series of decision rules in DTRs are equivalent to the policies in RL, while the treatment outcomes are expressed by the reward functions.</s><s xml:id="_xjQNtxh">The inputs in DTRs are a set of clinical observations and assessments of patients, and the outputs are the treatments options at each stage, equivalent to the states and actions in RL, respectively.</s><s xml:id="_tv6u4YV">Apparently, applying RL methods to solve DTR problems demonstrates several benefits.</s><s xml:id="_GR7HGk2">RL is capable of achieving time-dependent decisions on the best treatment for each patient at each decision time, thus accounting for heterogeneity across patients.</s><s xml:id="_5BrmFSK">This precise treatment can be achieved even without relying on the identification of any accurate mathematical models or explicit relationship between treatments and outcomes.</s><s xml:id="_wKceUDn">Furthermore, RL driven solutions enable to improve long-term outcomes by considering delayed effect of treatments, which is the major characteristic of medical treatment.</s><s xml:id="_sjfHAkc">Finally, by careful engineering the reward function using expert or domain knowledge, RL provides an elegant way to multi-objective optimization of treatment between efficacy and the raised side effect.</s></p><p xml:id="_BkCT8Mc"><s xml:id="_dWRrVzz">Due to these benefits, RL naturally becomes an appealing tool for constructing optimal DTRs in healthcare.</s><s xml:id="_XtWNSb2">In fact, solving DTR problems accounts for a large proportion of RL studies in healthcare applications, which can be supported by the dominantly large volume of references in this area.</s><s xml:id="_FUMBCE3">The domains of applying RL in DTRs can be classified into two main categories: chronic diseases and critical care.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_bPaUxjf">A. Chronic Diseases</head><p xml:id="_eMPMwEc"><s xml:id="_STPMug2">Chronic diseases are now becoming the most pressing public health issue worldwide, constituting a considerable portion of death every year <ref type="bibr" target="#b79">[80]</ref>.</s><s xml:id="_hgNtPZh">Chronic diseases normally feature a long period lasting three months or more, expected to require continuous clinical observation and medical care.</s><s xml:id="_Xn27sau">The widely prevailing chronic diseases include endocrine diseases (e.g., diabetes and hyperthyroidism), cardiovascular diseases (e.g., heart attacks and hypertension), various mental illnesses (e.g., depression and schizophrenia), cancer, HIV infection, obesity, and other oral health problems <ref type="bibr" target="#b80">[81]</ref>.</s><s xml:id="_cctAGmh">Long-term treatment of these illnesses is often made up of a sequence of medical intervention that must take into account the changing health status of a patient and adverse effects occurring from previous treatment.</s><s xml:id="_wZvjwcp">In general, the relationship of treatment duration, dosage and type against the patient's response is too complex to be be explicitly specified.</s><s xml:id="_QVgVspX">As such, practitioners usually resort to some protocols following the Chronic Care Model (CCM) <ref type="bibr" target="#b81">[82]</ref> to facilitate decision making in chronic disease conditions.</s><s xml:id="_RhqMYyG">Since such protocols are derived from average responses to treatment in populations of patients, selecting the best sequence of treatments for an individual patient poses significant challenges due to the diversity across or whithin the population.</s><s xml:id="_bPc7F8V">RL has been utilized to automate the discovery and generation of optimal DTRs in a variety of chronic diseases including caner, diabetes, anemia, HIV and several common mental illnesses.</s></p><p xml:id="_Gc8pAJN"><s xml:id="_xzfUAtX">1) Cancer: Cancer is one of the main chronic diseases that causes death.</s><s xml:id="_VqvnXup">About 90.5 million people had cancer in 2015 and approximately 14 million new cases are occurring each year, causing about 8.8 million annual deaths that account for</s></p><p xml:id="_tguFwzX"><s xml:id="_56SMbpj">TABLE III SUMMARY OF RL APPLICATION EXAMPLES IN THE DEVELOPMENT OF DTRS IN CANCER Applications References Base Methods Efficient Techniques Representational Techniques Data Acquisition Highlights or Limits Optimal chemotherapy drug dosage for cancer treatment Zhao et al. [83] Q-learning BRL N/A ODE model Using SVR or ERT to fit Q values; simplistic reward function structure with integer values to assess the tradeoff between efficacy and toxicity.</s><s xml:id="_NNgvHnv">Hassani et al. [84] Q-learning N/A N/A ODE model Naive discrete formulation of states and actions.</s><s xml:id="_e6RJHDr">Ahn &amp; Park [85] NAC N/A N/A ODE model Discovering the strategy of performing continuous treatment from the beginning.</s><s xml:id="_kCpt6NG">Humphrey [86] Q-learning BRL N/A ODE model proposed in [83] Using three machine learning methods to fit Q values, in high dimensional and subgroup scenarios.</s><s xml:id="_zQYfgJ9">Padmanabhan [87] Q-learning N/A N/A ODE model Using different reward functions to model different constraints in cancer treatment.</s><s xml:id="_Kq3aRB9">Zhao et al. [88] Q-learning BRL (FQI-SVR) N/A ODE model driven by real NSCLC data Considering censoring problem in multiple lines of treatment in advanced NSCLC; using overall survival time as the net reward.</s><s xml:id="_s9SC3eP">Fürnkranz et al. [52], Cheng et al. [89] PI N/A PRL ODE model proposed in [83] Combining preference learning and RL for optimal therapy design in cancer treatment, but only in model-based DP settings.</s><s xml:id="_TrdRu2F">Akrour et al. [90], Busa-Fekete et al. [91] PS N/A PRL ODE model proposed in [83] Using active ranking mechanism to reduce the number of needed ranking queries to the expert to yield a satisfactory policy without a generated model.</s><s xml:id="_rWdDMbJ">Optimal fractionation scheduling of radiation therapy for cancer treatment Vincent [92] Q-learning, SARSA(λ), TD(λ), PS BRL (FQI-ERT) N/A Linear model, ODE model Extended ODE model for radiation therapy; using hard constraints in the reward function and simple exploration strategy.</s><s xml:id="_SRAbQWy">Tseng et al. [93] Q-learning N/A DRL (DQN) Data from 114 NSCLC patients Addressing limited sample size problem using GAN and approximating the transition probability using DNN.</s><s xml:id="_A3xt2u4">Jalalimanesh et al.[94] Q-learning N/A N/A Agent-based model Using agent-based simulation to model the dynamics of tumor growth.</s><s xml:id="_bGXbEMJ">Jalalimanesh et al.[95] Q-learning N/A MORL Agent-based model Formulated as a multi-objective problem by considering conflicting objective of minimising tumour therapy period and unavoidable side effects.</s><s xml:id="_kTTeHep">Hypothetical or generic cancer clinical trial Goldberg &amp; Kosorok [96], Soliman [97] Q-learning N/A N/A Linear model Addressing problems with censored data and a flexible number of stages.</s><s xml:id="_KrTeWB8">Yauney &amp; Shah [98] Q-learning N/A DRL (DDQN) ODE model Addressing the problem of unstructured outcome rewards using action-driven rewards.</s></p><p xml:id="_cP7kGRm"><s xml:id="_P8eg7hV">15.7% of total deaths worldwide <ref type="bibr" target="#b98">[99]</ref>.</s><s xml:id="_9BvbkuV">The primary treatment options for cancer include surgery, chemotherapy, and radiation therapy.</s><s xml:id="_4d2neED">To analyze the dynamics between tumor and immune systems, numerous computational models for spatiotemporal or non-spatial tumor-immune dynamics have been proposed and analyzed by researchers over the past decades <ref type="bibr" target="#b99">[100]</ref>.</s><s xml:id="_d4yjdg9">Building on these models, control policies have been put forward to obtain efficient drug administration (see <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b100">[101]</ref> and references therein).</s></p><p xml:id="_ZJxGV4F"><s xml:id="_ps8VbdC">Being a sequential evolutionary process by nature, cancer treatment is a major objective of RL in DTR applications <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b102">[103]</ref>.</s><s xml:id="_BPMKDFY">Table <ref type="table">III</ref> summaries the major studies of applying RL in various aspects of cancer treatment, from the perspectives of application scenarios (chemotherapy, radiotherapy or generic cancer treatment simulation), basic RL methods, the efficient and representational techniques applied (if applicable), the learning data (retrospective clinical data, or generated from simulation models or computational models), and the main highlights and limits of the study.</s></p><p xml:id="_9HDAdRa"><s xml:id="_MCnUjT3">RL methods have been extensively studied in deriving efficient treatment strategies for cancer chemotherapy.</s><s xml:id="_kUNpqCg">Zhao et al. <ref type="bibr" target="#b82">[83]</ref> first applied model-free TD method, Q-learning, for decision making of agent dosage in chemotherapy.</s><s xml:id="_QueBurF">Drawing on the chemotherapy mathematical model expressed by several Ordinary Difference Equations (ODE), virtual clinical trial data from in vivo tumor growth patterns was quantitatively generated.</s><s xml:id="_nFfSyR4">Two explicit machine learning approaches, support vector regression (SVG) <ref type="bibr" target="#b103">[104]</ref> and extremely randomized trees (ERT) <ref type="bibr" target="#b40">[41]</ref>, were applied to fit the approximated Q-functions to the generated trial data.</s><s xml:id="_EV2UsFR">Using this kind of batch learning methods, it was demonstrated that optimal strategies could be extracted directly from clinical trial data in simulation.</s><s xml:id="_tpedHdC">Ahn and Park <ref type="bibr" target="#b84">[85]</ref> studied the applicability of the Natural AC (NAC) approach <ref type="bibr" target="#b20">[21]</ref> to the drug scheduling of cancer chemotherapy based on an ODE-based tumor growth model proposed by de Pillis and Radunskaya <ref type="bibr" target="#b104">[105]</ref>.</s><s xml:id="_SZU6aMM">Targeting at minimizing the tumor cell population and the drug amount while maximizing the populations of normal and immune cells, the NAC approach could discover an effective drug scheduling policy by injecting drug continuously from the beginning until an appropriate time.</s><s xml:id="_4kg86sK">This policy showed better performance than traditional pulsed chemotherapy protocol that administers the drug in a periodical manner, typically on an order of several hours.</s><s xml:id="_enAwvjm">The superiority of using continuous dosing treatment over a burst of dosing treatment was also supported by the work <ref type="bibr" target="#b83">[84]</ref>, where naive discrete Q-learning was applied.</s><s xml:id="_hvVsCd9">More recently, Padmanabhan et al. <ref type="bibr" target="#b86">[87]</ref> proposed different formulations of reward function in Q-learning to generate effective drug dosing policies for patient groups with different characteristics.</s><s xml:id="_9f92d8Z">Humphrey <ref type="bibr" target="#b85">[86]</ref> investigated several supervised learning approaches (Classification And Regression Trees (CART), random forests, and modified version of Multivariate Adaptive Regression Splines (MARS)) to estimate Q values in a simulation of an advanced generic cancer trial.</s></p><p xml:id="_PWkzf9P"><s xml:id="_VxkkNXa">Radiotherapy is another major option of treating cancer, and a number of studies have applied RL approaches for developing automated radiation adaptation protocols <ref type="bibr" target="#b105">[106]</ref>.</s><s xml:id="_ed27PeK">Jalalimanesh et al. <ref type="bibr" target="#b93">[94]</ref> proposed an agent-based simulation model and Q-learning algorithm to optimize dose calculation in radiotherapy by varying the fraction size during the treatment.</s><s xml:id="_hjx53st">Vincent <ref type="bibr" target="#b91">[92]</ref> described preliminary efforts in investigating a variety of RL methods to find optimal scheduling algorithms for radiation therapy, including the exhaustive PS <ref type="bibr" target="#b19">[20]</ref>, FQI <ref type="bibr" target="#b39">[40]</ref>, SARSA(λ) <ref type="bibr" target="#b18">[19]</ref> and K-Nearest Neighbors-TD(λ) <ref type="bibr" target="#b106">[107]</ref>.</s><s xml:id="_UP9cPQs">The preliminary findings suggest that there may be an advantage in using non-uniform fractionation schedules for some tissue types.</s></p><p xml:id="_7kAgzwA"><s xml:id="_2e9MRBY">As the goal of radiotherapy is in essence a multi-objective problem to erase the tumour with radiation while not impacting normal cells as much as possible, Jalalimanesh et al. <ref type="bibr" target="#b94">[95]</ref> proposed a multi-objective distributed Q-learning algorithm to find the Pareto-optimal solutions for calculating radiotherapy dose.</s><s xml:id="_t6BDHv8">Each objective was optimized by an individual learning agent and all the agents compromised their individual solutions in order to derive a Pareto-optimal solution.</s><s xml:id="_mTa3gqu">Under the multiobjective formulation, three different clinical behaviors could be properly modeled (i.e., aggressive, conservative or moderate), by paying different degree of attention to eliminating cancer cells or taking care of normal cells.</s></p><p xml:id="_aTRZQuq"><s xml:id="_x9etkjd">A recent study <ref type="bibr" target="#b92">[93]</ref> proposed a multi-component DRL framework to automate adaptive radiotherapy decision making for non-small cell lung cancer (NSCLC) patients.</s><s xml:id="_F8hks9Q">Aiming at reproducing or mimicking the decisions that have been previously made by clinicians, three neural network components, namely Generative Adversarial Net (GAN), transition Deep Neural Networks (DNN) and Deep Q Network (DQN), were applied: the GAN component was used to generate sufficiently large synthetic patient data from historical small-sized real clinical data; the transition DNN component was employed to learn how states would transit under different actions of dose fractions, based on the data synthesized from the GAN and available real clinical data; once the whole MDP model has been provided, the DQN component was then responsible for mapping the state into possible dose strategies, in order to optimize future radiotherapy outcomes.</s><s xml:id="_9uRm5vJ">The whole framework was evaluated in a retrospective dataset of 114 NSCLC patients who received radiotherapy under a successful dose escalation protocol.</s><s xml:id="_fuE8MBB">It was demonstrated that the DRL framework was able to learn effective dose adaptation policies between 1.5 and 3.8 Gy, which complied with the original dose range used by the clinicians.</s></p><p xml:id="_kkwH9gK"><s xml:id="_J74rWtT">The treatment of cancer poses several significant theoretical problems for applying existing RL approaches.</s><s xml:id="_DzPuDKY">Patients may drop out the treatment anytime due to various uncontrolled reasons, causing the final treatment outcome (e.g., survival time in cancer treatment) unobserved.</s><s xml:id="_MWMwc38">This data censoring problem <ref type="bibr" target="#b95">[96]</ref> complicates the practical use of RL in discovering individualized optimal regimens.</s><s xml:id="_ezqvH2y">Moreover, in general cancer treatment, the initiation and timing of the next line of therapy depend on the disease progression, and thus the number of treatment stage can be flexible.</s><s xml:id="_nADjcnE">For instance, NSCLC patients usually receive one to three treatment lines, and the necessity and timing of the second and third lines of treatment vary from person to person.</s><s xml:id="_RABSNM8">Developing valid methodology for computing optimal DTRs in such a flexible setting is currently a premier challenge.</s><s xml:id="_mSqKvhs">Zhao et al. <ref type="bibr" target="#b87">[88]</ref> presented an adaptive Q-learning approach to discover optimal DTRs for the first and second lines of treatment in Stage IIIB/IV NSCLC.</s><s xml:id="_X2mAUje">The trial was conducted by randomizing the different compounds for first and second-line treatments, as well as the timing of initiating the second-line therapy.</s><s xml:id="_6RcxHhB">In order to successfully handle the complex censored survival data, a modification of SVG approach, -SV R-C, was proposed to estimate the optimal Q values.</s><s xml:id="_ZJXsGTZ">A simulation study showed that the approach could select optimal compounds for two lines of treatment directly from clinical data, and the best initial time for second-line therapy could be derived while taking into account the heterogeneity across patients.</s><s xml:id="_jT9fPxU">Other studies <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b96">[97]</ref> presented the novel censored-Q-learning algorithm that is adjusted for a multi-stage decision problem with a flexible number of stages in which the rewards are survival times that are subject to censoring.</s></p><p xml:id="_MrMnjGR"><s xml:id="_MCjZpeE">To tackle the problem that a numerical reward function should be specified beforehand in standard RL techniques, several studies investigated the possibility of formulating rewards using qualitative preference or simply based on past actions in the treatment of cancer <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b97">[98]</ref>.</s><s xml:id="_BbAqtMF">Akrour et al. <ref type="bibr" target="#b89">[90]</ref> proposed a PRL method combined with active ranking in order to decrease the number of ranking queries to the expert needed to yield a satisfactory policy.</s><s xml:id="_C2YeKgS">Experiments on the cancer treatment testbeds showed that a very limited external information in terms of expert's ranking feedbacks might be sufficient to reach state-of-the-art results.</s><s xml:id="_uaVSYFf">Busa-Fekete et al.</s></p><p xml:id="_MjFu6ET"><s xml:id="_kf4gXMt">[91] introduced a preference-based variant of a direct PS method in the medical treatment design for cancer clinical trials.</s><s xml:id="_kTmCVwC">A novel approach based on action-driven rewards was first proposed in <ref type="bibr" target="#b97">[98]</ref>.</s><s xml:id="_Q5pyy5U">It was showed that new dosing regimes in cancer chemotherapy could be learned using action-derived penalties, suggesting the possibility of using RL methods in situations when final outcomes are not available, but priors on beneficial actions can be more easily specified.</s></p><p xml:id="_MqU7uru"><s xml:id="_nSMwVkk">2) Diabetes: Diabetes mellitus, or simply called diabetes, is one of the most serious chronic diseases in the world.</s><s xml:id="_8KKhXSm">According to a recent report released by International Diabetes Federation (IDF), there are 451 million people living with diabetes in 2017, causing approximately 5 million deaths worldwide and USD 850 billion global healthcare expenditure <ref type="bibr" target="#b107">[108]</ref>.</s><s xml:id="_KnaSEEj">It is expected that by 2045, the total number of adults with diabetes would increase to near 700 million, accounting for 9.9% of the adult population.</s><s xml:id="_4YZ6xHD">Since the high prevalence of diabetes presents significant social influence and financial burdens, there has been an increasing urgency to ensure effective treatment to diabetes across the world.</s></p><p xml:id="_sN955Zf"><s xml:id="_XC8bJ3K">Intensive research concern has been devoted to the development of effective blood glucose control strategies in treatment of insulin-dependent diabetes (i.e., type 1 diabetes).</s><s xml:id="_uMJhja2">Since its first proposal in the 1970s <ref type="bibr" target="#b108">[109]</ref>, artificial pancreas (AP) have been widely used in the blood glucose control process to compute and administrate a precise insulin dose, by using a continuous glucose monitoring system (CGMS) and a closed-loop controller <ref type="bibr" target="#b109">[110]</ref>.</s><s xml:id="_T9UHFUv">Tremendous progress has been made towards insulin infusion rate automation in AP using traditional control strategies such as Proportional-Integral-Derivative (PID), Model Predictive Control (MPC), and Fuzzy Logic (FL) <ref type="bibr" target="#b110">[111]</ref>, <ref type="bibr" target="#b111">[112]</ref>.</s><s xml:id="_ztUM9jn">A major concern is the inter-and intra-variability of the diabetic population which raises the demand for a personalized, patient specific approach of the glucose regulation.</s><s xml:id="_Ms3G4HM">Moreover, the complexity of the physiolog-ical system, the variety of disturbances such as meal, exercise, stress and sickness, along with the difficulty in modelling accurately the glucose-insulin regulation system all raise the need in the development of more advanced adaptive algorithms for the glucose regulation.</s></p><p xml:id="_SWcNeRp"><s xml:id="_UnMAvMP">RL approaches have attracted increasingly high attention in personalized, patient specific glucose regulation in AP systems <ref type="bibr" target="#b112">[113]</ref>.</s><s xml:id="_3yFVrBv">Yasini et al. <ref type="bibr" target="#b113">[114]</ref> made an initial study on using RL to control an AP to maintain normoglycemic around 80 mg/dl.</s><s xml:id="_3D4SvNk">Specifically, model-free TD Q-learning algorithm was applied to compute the insulin delivery rate, without relying on an explicit model of the glucose-insulin dynamics.</s><s xml:id="_FwPacms">Daskalaki et al. <ref type="bibr" target="#b114">[115]</ref> presented an AC controller for the estimation of insulin infusion rate in silico trial based on the University of Virginia/Padova type 1 diabetes simulator <ref type="bibr" target="#b115">[116]</ref>.</s><s xml:id="_c4uv9Gq">In an evaluation of 12 day meal scenario for 10 adults, results showed that the approach could prevent hypoglycaemia well, but hyperglycaemia could not be properly solved due to the static behaviors of the Actor component.</s><s xml:id="_jX28K99">The authors then proposed using daily updates of the average basal rate (BR) and the insulin-to-carbohydrate (IC) ratio in order to optimize glucose regulation <ref type="bibr" target="#b116">[117]</ref>, and using estimation of information transfer (IT) from insulin to glucose for automatic and personalized tuning of the AC approach <ref type="bibr" target="#b117">[118]</ref>.</s><s xml:id="_pYNYntf">This idea was motivated by the fact that small adaptation of insulin in the Actor component may be sufficient in case of large amount of IT from insulin to glucose, whereas more dramatic updates may be required for low IT.</s><s xml:id="_qNKn994">The results from the Control Variability Grid Analysis (CVGA) showed that the approach could achieve higher performance in all three groups of patients, with 100% percentages in the A+B zones for adults, and 93% for both adolescents and children, compared to approaches with random initialization and zero initial values.</s><s xml:id="_JhtkWbJ">The AC approach was significantly extended to directly link to patient-specific characteristics, and evaluated more extensively under a complex meal protocol, meal uncertainty and insulin sensitivity variation [119], <ref type="bibr" target="#b119">[120]</ref>.</s></p><p xml:id="_rRj3aYx"><s xml:id="_SDgaWwz">A number of studies used certain mathematical models to simulate the glucose-insulin dynamic system in patients.</s><s xml:id="_jQMGFBR">Based on the Palumbo mathematical model <ref type="bibr" target="#b120">[121]</ref>, the onpolicy SARSA was used for insulin delivery rate <ref type="bibr" target="#b121">[122]</ref>.</s><s xml:id="_WR3KXm9">Ngo et al. applied model-based VI method <ref type="bibr" target="#b122">[123]</ref> and AC method <ref type="bibr" target="#b123">[124]</ref> to reduce the fluctuation of the blood glucose in both fasting and post-meal scenarios, drawing on the Bergman's minimal insulin-glucose kinetics model <ref type="bibr" target="#b124">[125]</ref> and the Hovorka model <ref type="bibr" target="#b125">[126]</ref> to simulate a patient.</s><s xml:id="_cgzVDWB">De Paula et al. <ref type="bibr" target="#b126">[127]</ref>, <ref type="bibr" target="#b127">[128]</ref> proposed policy learning algorithms that integrates RL with Gaussian processes to take into account glycemic variability under uncertainty, using the Ito's stochastic model of the glucose-insulin dynamics <ref type="bibr" target="#b128">[129]</ref>.</s></p><p xml:id="_FxpWBHv"><s xml:id="_34WGyPy">There are also several data-driven studies carried out to analyze RL in diabetes treatment based on real data from diabetes patients.</s><s xml:id="_eWDuVwK">Utilizing the data extracted from the medical records of over 10,000 patients in the University of Tokyo Hospital, Asoh et al. <ref type="bibr" target="#b129">[130]</ref> estimated the MDP model underlying the progression of patient state and evaluated the value of treatment using the VI method.</s><s xml:id="_RjRtH9X">The opinions of a doctor were used to define the reward for each treatment.</s><s xml:id="_URbZppH">The preassumption of this predefined reward function then motivated the application of IRL approach to reveal the reward function that doctors were using during their treatments <ref type="bibr" target="#b130">[131]</ref>.</s><s xml:id="_YuveKGV">Using observational data on the effect of food intake and physical activity in an outpatient setting using mobile technology, Luckett et al. <ref type="bibr" target="#b131">[132]</ref> proposed the V-learning method that directly estimates a policy which maximizes the value over a class of policies and requires minimal assumptions on the data-generating process.</s><s xml:id="_jTkyeD8">The method has been applied to estimate treatment regimes to reduce the number of hypo and hyperglycemic episodes in patients with type 1 diabetes.</s></p><p xml:id="_kHDXs7n"><s xml:id="_FdZHqce">3) Anemia: Anemia is a common comorbidity in chronic renal failure that occurs in more than 90% of patients with endstage renal disease (ESRD) who are undertaking hemodialysis.</s><s xml:id="_ZvJMhXT">Caused by a failure of adequately producing endogenous erythropoietin (EPO) and thus red blood cells, anemia can have significant impact on organ functions, giving rise to a number of severe consequences such as heart disease or even increased mortality.</s><s xml:id="_qMSAX2s">Currently, anemia can be successfully treated by administering erythropoiesis-stimulating agents (ESAs), in order to maintain the hemoglobin (HGB) level within a narrow range of 11-12 g/dL.</s><s xml:id="_DxR5Mxz">To achieve this, professional clinicians must carry out a labor intensive process of dosing ESAs to assess monthly HGB and iron levels before making adjustments accordingly.</s><s xml:id="_QzJwjGK">However, since the existing Anemia Management Protocol (AMP) does not account for the high inter-and intraindividual variability in the patient's response, the HGB level of some patients usually oscillates around the target range, causing several risks and side-effects.</s></p><p xml:id="_regsJ34"><s xml:id="_jVMbVfZ">As early as in 2005, Gaweda et al. <ref type="bibr" target="#b132">[133]</ref> first proposed using RL to perform individualized treatment in the management of renal anemia.</s><s xml:id="_bTcUp46">The target under control is the HGB, whereas the control input is the amount of EPO administered by the physician.</s><s xml:id="_ffRpUfs">As the iron storage in the patient, determined by Transferrin Saturation (TSAT), also has an impact on the process of red blood cell creation, it is considered as a state component together with HGB.</s><s xml:id="_rzGAHq9">To model distinct dose-response relationship within a patient population, a fuzzy model was estimated first by using real records of 186 hemodialysis patients from the Division of Nephrology, University of Louisville.</s><s xml:id="_ryafFxt">On-policy TD method, SARSA, was then performed on the sample trajectories generated by the model.</s><s xml:id="_YxY9h6W">Results show that the proposed approach generates adequate dosing strategies for representative individuals from different response groups.</s><s xml:id="_fWuddcT">The authors then proposed a combination of MPC approach with SARSA for decision support in anemia management <ref type="bibr" target="#b133">[134]</ref>, with the MPC component used for simulation of patient response and SARSA for optimization of the dosing strategy.</s><s xml:id="_SQcY4HG">However, the automated RL approaches in these studies could only achieve a policy with a comparable outcome against the existing AMP.</s><s xml:id="_hwp7Puj">Other studies applied various kinds of Q-learning, such as Q-learning with function approximation, or directly based on state-aggregation <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b136">[137]</ref>, in providing effective treatment regimes in anemia.</s></p><p xml:id="_jSvYvFX"><s xml:id="_D49Gw9R">Several studies resorted to BRL methods to derive optimal ESA dosing strategies for anemia treatment.</s><s xml:id="_hUyn4sa">By performing a retrospective study of a cohort of 209 hemodialysis patients, Malof and Gaweda <ref type="bibr" target="#b137">[138]</ref> adopted the batch FQI method to achieve dosing strategies that were superior to a standard AMP.</s><s xml:id="_Pdtudsz">The FQI method was also applied by Escandell et al. <ref type="bibr" target="#b138">[139]</ref> for discovering efficient dosing strategies based on the historical treatment data of 195 patients in nephrology centers allocated around Italy and Portugal.</s><s xml:id="_8KcxYgn">An evaluation of the FQI method on a computational model that describes the effect of ESAs on the hemoglobin level showed that FQI could achieve an increment of 27.6% in the proportion of patients that are within the targeted range of hemoglobin during the period of treatment.</s><s xml:id="_TdF6CDM">In addition, the quantity of drug needed is reduced by 5.13%, which indicates a more efficient use of ESAs <ref type="bibr" target="#b139">[140]</ref>.</s></p><p xml:id="_wydDUVz"><s xml:id="_5UA7Pyj">4) HIV: Discovering effective treatment strategies for HIVinfected individuals remains one of the most significant challenges in medical research.</s><s xml:id="_aUn7Q7B">To date, the effective way to treat HIV makes use of a combination of anti-HIV drugs (i.e., antiretrovirals) in the form of Highly Active Antiretroviral Therapy (HAART) to inhibit the development of drug-resistant HIV strains <ref type="bibr" target="#b140">[141]</ref>.</s><s xml:id="_9TvQ5Fz">Patients suffering from HIV are typically prescribed a series of treatments over time in order to maximize the long-term positive outcomes of reducing patients' treatment burden and improving adherence to medication.</s><s xml:id="_ebh9Vfk">However, due to the differences between individuals in their immune responses to treatment, discovering the optimal drug combinations and scheduling strategy is still a difficult task in both medical research and clinical trials.</s></p><p xml:id="_cy435Ed"><s xml:id="_skav84M">Ernst et al. <ref type="bibr" target="#b141">[142]</ref> first introduced RL techniques in computing Structured Treatment Interruption (STI) strategies for HIV infected patients.</s><s xml:id="_Nqe3A3R">Using a mathematical model <ref type="bibr" target="#b140">[141]</ref> to artificially generate the clinical data, the BRL method FIQ-ERT was applied to learn an optimal drug prescription strategy in an off-line manner.</s><s xml:id="_NMcazTd">The derived STI strategy is featured with a cycling between the two main anti-HIV drugs: Reverse Transcriptase Inhibitors (RTI) and Protease Inhibitors (PI), before bringing the patient to the healthy drug-free steadystate.</s><s xml:id="_tH5BQqr">Using the same mathematical model, Parbhoo <ref type="bibr" target="#b142">[143]</ref> further implemented three kinds of BRL methods, FQI-ERT, neural FQI and LSPI, to the problem of HIV treatment, indicating that each learning technique had its own advantages and disadvantages.</s><s xml:id="_j7xQ3ev">Moreover, a testing based on a ten-year period of real clinical data from 250 HIV-infected patients in Charlotte Maxeke Johannesburg Academic Hospital, South Africa verified that the RL methods were capable of suggesting treatments that were reasonably compliant with those suggested by clinicians.</s></p><p xml:id="_MVNrghR"><s xml:id="_MdBgq3h">A mixture-of-experts approach was proposed in <ref type="bibr" target="#b143">[144]</ref> to combine the strengths of both kernel-based regression methods (i.e., history-alignment model) and RL (i.e., model-based Bayesian PORL) for HIV therapy selection.</s><s xml:id="_SW3WYNQ">Since kernelbased regression methods are more suitable for modeling more related patients in history, while model-based RL methods are more suitable for reasoning about the future outcomes, automatically selecting an appropriate model for a particular patient between these two methods thus tends to provide simpler yet more robust patterns of response to the treatment.</s><s xml:id="_vQGe3Jp">Making use of a subset of the EuResist database consisting of HIV genotype and treatment response data for 32,960 patients, together with the 312 most common drug combinations in the cohort, the treatment therapy derived by the mixture-of-experts approach outperformed those derived by each method alone.</s></p><p xml:id="_brbUwK4"><s xml:id="_uaUTeqN">Since the treatment of HIV highly depends the patient's immune system that varies from person to person, it is thus necessary to derive efficient learning strategies that can address and identify the variations across subpopulations.</s><s xml:id="_yhfk3By">Marivate et al. <ref type="bibr" target="#b144">[145]</ref> formalized a routine to accommodate multiple sources of uncertainty in BRL methods to better evaluate the effectiveness of treatments across a subpopulations of patients.</s><s xml:id="_WTq2yrK">Other approaches applied various kinds of TRL techniques so as to take advantage of the prior information from previously learned transition models <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b146">[147]</ref> or learned policy <ref type="bibr" target="#b147">[148]</ref>.</s><s xml:id="_zUuwsaN">More recently, Yu et al. <ref type="bibr" target="#b148">[149]</ref> proposed a causal policy gradient algorithm and evaluated it in the treatment of HIV in order to facilitate the final learning performance and increase explanations of learned strategies.</s></p><p xml:id="_Ya64ARb"><s xml:id="_cS6XCFn">The treatment of HIV provides a well-known testbed for evaluation of exploration mechanisms in RL research.</s><s xml:id="_BPAUe5x">Simulations show that the basin of attraction of the healthy steady-state is rather small compared to that of the nonhealthy steady state <ref type="bibr" target="#b140">[141]</ref>.</s><s xml:id="_G3FhqSV">Thus, general exploration methods are unable to yield meaningful performance improvement as they can only obtain samples in the vicinity of the "nonhealthy" steady state.</s><s xml:id="_NAjhnT9">To solve this issue, several studies have proposed more advanced exploration strategies in order to increase the learning performance in HIV treatment.</s><s xml:id="_UnkGJYU">Pazis et al. <ref type="bibr" target="#b149">[150]</ref> introduced an algorithm for PAC optimal exploration in continuous state spaces.</s><s xml:id="_XvFhWzZ">Kawaguchi considered the time bound in a PAC exploration process <ref type="bibr" target="#b150">[151]</ref>.</s><s xml:id="_TqvtnbN">Results in both studies showed that the exploration algorithm could achieve far better strategies than other existing exploration strategies in HIV treatment.</s></p><p xml:id="_janxEGt"><s xml:id="_KPSQnNR">5) Mental Disease: Mental diseases are characterized by a long-term period of clinical treatments that usually require adaptation in the duration, dose, or type of treatment over time <ref type="bibr" target="#b151">[152]</ref>.</s><s xml:id="_eBVbeZU">Given that the brain is a complex system and thus extremely challenging to model, applying traditional controlbased methods that rely on accurate brain models in mental disease treatment is proved infeasible.</s><s xml:id="_TwRxDU6">Well suited to the problem at hand, RL has been widely applied to DTRs in a wide range of mental illness including epilepsy, depression, schizophrenia and various kinds of substance addiction.</s><s xml:id="_KSTNapT">a) Epilepsy: Epilepsy is one of the most common severe neurological disorders, affecting around 1% of the world population.</s><s xml:id="_5HVGy4e">When happening, epilepsy is manifested in the form of intermittent and intense seizures that are recognized as abnormal synchronized firing of neural populations.</s><s xml:id="_Hb4AesK">Implantable electrical deep-brain stimulation devices are now an important treatment option for drug-resistant epileptic patients.</s><s xml:id="_GcUwmmb">Researchers from nonlinear dynamic systems analysis and control have proposed promising prediction and detection algorithms to suppress the frequency, duration and amplitude of seizures <ref type="bibr" target="#b152">[153]</ref>.</s><s xml:id="_S4p8amb">However, due to lack of full understanding of seizure and its associated neural dynamics, designing optimal seizure suppression algorithms via minimal electrical stimulation has been for a long time a challenging task in treatment of epilepsy.</s></p><p xml:id="_mHjV7MW"><s xml:id="_zzRSGxp">RL enables direct closed-loop optimizations of deep-brain stimulation strategies by adapting control policies to patients' unique neural dynamics, without necessarily relying on having accurate prediction or detection of seizures.</s><s xml:id="_ZYbcQYw">The goal is to explicitly maximize the effectiveness of stimulation, while simultaneously minimizing the overall amount of stimulation applied thus reducing cell damage and preserving cognitive and neurological functions <ref type="bibr" target="#b153">[154]</ref>.</s><s xml:id="_F8Upbrm">Guez et al. <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b156">[157]</ref> applied the BRL method, FQI-ERT, to optimize a deepbrain stimulation strategy for the treatment of epilepsy.</s><s xml:id="_FbGQkuj">Encoding the observed Electroencephalograph (EEG) signal as a 114-dimensional continuous feature vector, and four different simulation frequencies as the actions, the RL approach was applied to learn an optimal stimulation policy using data from an in vitro animal model of epilepsy (i.e., field potential recordings of seizure-like activity in slices of rat brains).</s><s xml:id="_x8QyXSj">Results showed that RL strategies substantially outperformed the current best stimulation strategies in the literature, reducing the incidence of seizures by 25% and total amount of electrical stimulation to the brain by a factor of about 10. Subsequent validation work <ref type="bibr" target="#b157">[158]</ref> showed generally similar results that RL-based policy could prevent epilepsy with a significant reduced amount of stimulation, compared to fixed-frequency stimulation strategies.</s><s xml:id="_97ymU8q">Bush and Pineau <ref type="bibr" target="#b158">[159]</ref> applied manifold embeddings to reconstruct the observable state space in MRL, and applied the proposed approach to tackle the high complexity of nonlinearity and partially observability in real-life systems.</s><s xml:id="_2qRCyRS">The learned neurostimulation policy was evaluated to suppress epileptic seizures on animal brain slices and results showed that seizures could be effectively suppressed after a short transient period.</s></p><p xml:id="_BDmJK2j"><s xml:id="_HnZdZwE">While the above in vitro biological models of epilepsy are useful for research, they are nonetheless time-consuming and associated with high cost.</s><s xml:id="_fcvRTwD">In contrast, computational models can provide large amounts of reproducible and cheap data that may permit precise manipulations and deeper investigations.</s><s xml:id="_nHCx3Fz">Vincent <ref type="bibr" target="#b91">[92]</ref> proposed an in silico computational model of epileptiform behavior in brain slices, which was verified by using biological data from rat brain slices in vitro.</s><s xml:id="_xHgRZZe">Nagaraj et al. <ref type="bibr" target="#b159">[160]</ref> proposed the first computational model that captures the transition from inter-ictal to ictal activity, and applied naive Q-learning method to optimize stimulation frequency for controlling seizures with minimum stimulations.</s><s xml:id="_ZgXjdJZ">It was shown that even such simple RL methods could converge on the optimal solution in simulation with slow and fast interseizure intervals.</s></p><p xml:id="_vm5w9Uy"><s xml:id="_sc4YSGj">b) Depression: Major depressive disorder (MDD), also known simply as depression, is a mental disorder characterized by at least two weeks of low mood that is present across most situations.</s><s xml:id="_t9ChB4a">Using data from the Sequenced Treatment Alternatives to Relieve Depression (STAR*D) trial <ref type="bibr" target="#b160">[161]</ref>, which is a sequenced four-stage randomized clinical trial of patients with MDD, Pineau et al. <ref type="bibr" target="#b161">[162]</ref> first applied Kernelbased BRL <ref type="bibr" target="#b162">[163]</ref> for constructing useful DTRs for patients with MDD.</s><s xml:id="_QqWWR4z">Other work tries to address the problem of nonsmooth of decision rules as well as nonregularity of the parameter estimations in traditional RL methods by proposing various extensions over default Q-learning procedure in order to increase the robustness of learning <ref type="bibr" target="#b163">[164]</ref>.</s><s xml:id="_A2nHpyH">Laber et al. <ref type="bibr" target="#b164">[165]</ref> proposed a new version of Q-learning, interactive Q-learning (IQ-learning), by interchanging the order of certain steps in traditional Q-learning, and showed that IQ-learning improved on Q-learning in terms of integrated mean squared error in a study of MDD.</s><s xml:id="_vZ2c2nm">The IQ-learning framework was then extended to optimize functionals of the outcome distribution other than the expected value <ref type="bibr" target="#b165">[166]</ref>, <ref type="bibr" target="#b166">[167]</ref>.</s><s xml:id="_DeyM962">Schulte et al. <ref type="bibr" target="#b167">[168]</ref> provided systematic empirical studies of Q-learning and Advantage-learning (A-learning) <ref type="bibr" target="#b168">[169]</ref> methods and illustrated their performance using data from an MDD study.</s><s xml:id="_sZsMYKM">Other approaches include the penalized Q-learning <ref type="bibr" target="#b169">[170]</ref>, the Augmented Multistage Outcome-Weighted Learning (AMOL) <ref type="bibr" target="#b170">[171]</ref>, the budgeted learning algorithm <ref type="bibr" target="#b171">[172]</ref>, and the Censored Q-learning algorithm <ref type="bibr" target="#b96">[97]</ref>.</s></p><p xml:id="_tACkYvx"><s xml:id="_K3U2mHd">c) Schizophrenia: RL methods have been also used to derive optimal DTRs in treatment of schizophrenia, using data from the Clinical Antipsychotic Trials of Intervention Effectiveness (CATIE) study <ref type="bibr" target="#b172">[173]</ref>, which was an 18-month study divided into two main phases of treatment.</s><s xml:id="_SaG2j97">An in-depth case study of using BRL, FQI, to optimize treatment choices for patients with schizophrenia using data from CATIE was given by <ref type="bibr" target="#b173">[174]</ref>.</s><s xml:id="_jFE9UKY">Key technical challenges of applying RL in typically continuous, highly variable, and high-dimensional clinical trials with missing data were outlined.</s><s xml:id="_HPxxGTy">To address these issues, the authors proposed the use of multiple imputation to overcome the missing data problem, and then presented two methods, bootstrap voting and adaptive confidence intervals, for quantifying the evidence in the data for the choices made by the learned optimal policy.</s><s xml:id="_PFfrjzr">Ertefaie et al. <ref type="bibr" target="#b174">[175]</ref> accommodated residual analyses into Q-learning in order to increase the accuracy of model fit and demonstrated its superiority over standard Q-learning using data from CATIE.</s></p><p xml:id="_TvRcPFE"><s xml:id="_WFyRYPN">Some studies have focused on optimizing multiple treatment objectives in dealing with schizophrenia.</s><s xml:id="_sHtDEjF">Lizotte et al. <ref type="bibr" target="#b175">[176]</ref> extended the FQI algorithm by considering multiple rewards of symptom reduction, side-effects and quality of life simultaneously in sequential treatments for schizophrenia.</s><s xml:id="_3a4Xvp9">However, it was assumed that end-users had a true reward function that was linear in the objectives and all future actions could be chosen optimally with respect to the same true reward function over time.</s><s xml:id="_zYByFzb">To solve these issues, the authors then proposed the non-deterministic multi-objective FIQ algorithm, which computed policies for all preference functions simultaneously from continuous-state, finite-horizon data <ref type="bibr" target="#b176">[177]</ref>.</s><s xml:id="_w63KSAh">When patients do not know or cannot communicate their preferences, and there is heterogeneity across patient preferences for these outcomes, formation of a single composite outcome that correctly balances the competing outcomes for all patients is not possible.</s><s xml:id="_hxuR6yz">Laber et al. <ref type="bibr" target="#b177">[178]</ref> then proposed a method for constructing DTRs for schizophrenia that accommodates competing outcomes and preference heterogeneity across both patients and time by recommending sets of treatments at each decision point.</s><s xml:id="_HGebffX">Butler et al. <ref type="bibr" target="#b178">[179]</ref> derived a preference sensitive optimal DTR for schizophrenia patient by directly eliciting patients' preferences overtime.</s></p><p xml:id="_HsePuTV"><s xml:id="_27pdCHD">d) Substance Addiction: Substance addiction, or substance use disorder (SUD), often involves a chronic course of repeated cycles of cessation followed by relapse <ref type="bibr" target="#b179">[180]</ref>, <ref type="bibr" target="#b74">[75]</ref>.</s><s xml:id="_YbqU5xs">There has been great interest in the development of DTRs by investigators to deliver in-time interventions or preventions to end-users using RL methods, guiding them to lead healthier lives.</s><s xml:id="_CDvGvn6">For example, Murphy et al. <ref type="bibr" target="#b180">[181]</ref> applied AC algorithm to reduce heavy drinking and smoking for university students.</s><s xml:id="_ZfAGuAN">Chakraborty et al. <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b181">[182]</ref>, <ref type="bibr" target="#b182">[183]</ref> used Q-learning with linear models to identify DTRs for smoking cessation treatment regimes.</s><s xml:id="_xfTbYRa">Tao et al. <ref type="bibr" target="#b183">[184]</ref> proposed a tree-based RL method to directly estimate optimal DTRs, and identify dynamic SUD treatment regimes for adolescents.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wuvJCRw">B. Critical Care</head><p xml:id="_tdqT8Yx"><s xml:id="_z3ZRqfk">Unlike the treatment of chronic diseases, which usually requires a long period of constant monitoring and medication, critical care is dedicated to more seriously ill or injured patients that are in need of special medical treatments and nursing care.</s><s xml:id="_UXWVEmU">Usually, such patients are provided with separate geographical area, or formally named the intensive care unit (ICU), for intensive monitoring and close attention, so as to improve the treatment outcomes <ref type="bibr" target="#b184">[185]</ref>.</s><s xml:id="_RawudZe">ICUs will play a major role in the new era of healthcare systems.</s><s xml:id="_gdwxQDm">It is estimated that the ratio of ICU beds to hospital beds would increase from 3-5% in the past to 20-30% in the future <ref type="bibr" target="#b185">[186]</ref>.</s></p><p xml:id="_jpavXsA"><s xml:id="_73GK22V">Significant attempts have been devoted to the development of clearer guidelines and standardizing approaches to various aspects of interventions in ICUs, such as sedation, nutrition, administration of blood products, fluid and vasoactive drug therapy, haemodynamic endpoints, glucose control, and mechanical ventilation <ref type="bibr" target="#b184">[185]</ref>.</s><s xml:id="_XQ3JrRD">Unfortunately, only a few of these interventions could be supported by high quality evidence from randomised controlled trials or meta-analyses <ref type="bibr" target="#b186">[187]</ref>, especially when it comes to development of potentially new therapies for complex ICU syndromes, such as sepsis <ref type="bibr" target="#b187">[188]</ref> and acute respiratory distress syndrome <ref type="bibr" target="#b188">[189]</ref>.</s></p><p xml:id="_hyeyMus"><s xml:id="_shmMyNs">Thanks to the development in ubiquitous monitoring and censoring techniques, it is now possible to generate rich ICU data in a variety of formats such as free-text clinical notes, images, physiological waveforms, and vital sign time series, suggesting a great deal of opportunities for the applications of machine learning and particularly RL techniques in critical care <ref type="bibr" target="#b189">[190]</ref>, <ref type="bibr" target="#b190">[191]</ref>.</s><s xml:id="_UqkAPsn">However, the inherent 3C (Compartmentalization, Corruption, and Complexity) features indicate that critical care data are usually noisy, biased and incomplete <ref type="bibr" target="#b4">[5]</ref>.</s><s xml:id="_ncMq9A7">Properly processing and interpreting this data in a way that can be used by existing machine learning methods is the premier challenge of data analysis in critical care.</s><s xml:id="_VyPxv7A">To date, RL has been widely applied in the treatment of sepsis (Section IV-B1), regulation of sedation (Section IV-B2), and some other decision making problems in ICUs such as mechanical ventilation and heparin dosing (Section IV-B3).</s><s xml:id="_QHwN6gb">Table <ref type="table">IV</ref> summarizes these applications according to the applied RL techniques and the sources of data acquired during learning.</s></p><p xml:id="_kUUAUuC"><s xml:id="_PRSVUaU">1) Sepsis: Sepsis, which is defined as severe infection causing life-threatening acute organ failure, is a leading cause of mortality and associated healthcare costs in critical care <ref type="bibr" target="#b225">[226]</ref>.</s><s xml:id="_TZEZPRd">While numbers of international organizations have devoted significant efforts to provide general guidance for treating sepsis over the past 20 years, physicians at practice still lack universally agreed-upon decision support for sepsis <ref type="bibr" target="#b187">[188]</ref>.</s><s xml:id="_CGZSn8U">With the available data obtained from freely accessible critical care databases such as the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC) <ref type="bibr" target="#b226">[227]</ref>, recent years have seen an increasing number of studies that applied RL techniques to the problem of deducing optimal treatment policies for patients with sepsis <ref type="bibr" target="#b227">[228]</ref>.</s></p><p xml:id="_zQ85R9n"><s xml:id="_QqpAbsS">The administration of intravenous (IV) and maximum vasopressor (VP) is a key research and clinical challenge in sepsis.</s><s xml:id="_AUgAUPM">A number of studies have been carried out to tackle this issue in the past years.</s><s xml:id="_eejSTqp">Komorowski et al. <ref type="bibr" target="#b191">[192]</ref>, <ref type="bibr" target="#b192">[193]</ref> directly applied the on-policy SARSA algorithm and modelbased PI method in a discretized state and action-space.</s><s xml:id="_AWmrJpS">Raghu et al. <ref type="bibr" target="#b193">[194]</ref>, <ref type="bibr" target="#b194">[195]</ref> examined fully continuous state and action space, where policies are learned directly from the physiological state data.</s><s xml:id="_uq6WZVU">To this end, the authors proposed the fully-connected Dueling Double DQN to learn an approximation for the optimal action-value function, which combines three state-of-the-art efficiency and stability boosting techniques in DRL, i.e., Double DQN <ref type="bibr" target="#b228">[229]</ref>, Dueling DQN <ref type="bibr" target="#b229">[230]</ref> and Prioritized Experience Replay (PER) <ref type="bibr" target="#b230">[231]</ref>.</s><s xml:id="_3fJH9cD">Experimental results demonstrated that using continuous state-space modeling could identify interpretable policies with improved patient outcomes, potentially reducing patient mortality in the hospital by 1.8 -3.6%.</s><s xml:id="_jVcnQfJ">The authors also directly estimated the transition model in continuous state-space, and applied two PS methods, the direct policy gradient and Proximal Policy Optimization PPO <ref type="bibr" target="#b231">[232]</ref>, to derive a treatment strategy <ref type="bibr" target="#b195">[196]</ref>.</s><s xml:id="_dgrTRzA">Utomo et al. <ref type="bibr" target="#b196">[197]</ref> proposed a graphical model that was able to show transitions of patient health conditions and treatments for better explanability, and applied MC to generate a realtime treatment recommendation.</s><s xml:id="_fThHZeU">Li et al. <ref type="bibr" target="#b200">[201]</ref> provided an online POMDP solution to take into account uncertainty and history information in sepsis clinical applications.</s><s xml:id="_wmydBA8">Futoma et al. <ref type="bibr" target="#b198">[199]</ref> used multi-output Gaussian processes and DRL to directly learn from sparsely sampled and frequently missing multivariate time series ICU data.</s><s xml:id="_sJ4GaFy">Peng et al. <ref type="bibr" target="#b197">[198]</ref> applied the mixture-of-experts framework <ref type="bibr" target="#b143">[144]</ref> in sepsis treatment by automatically switching between kernel learning and DRL depending on patient's current history.</s><s xml:id="_Qxj8uhv">Results showed that this kind of mixed learning could achieve better performance than the strategies by physicians, Kernel learning and DQN learning alone.</s><s xml:id="_cBhfTS8">Most recently, Yu et al. <ref type="bibr" target="#b199">[200]</ref> addressed IRL problems in sepsis treatment.</s></p><p xml:id="_g69tDXr"><s xml:id="_DXvrsUH">Targeting at glycemic regulation problems for severely ill septic patients, Weng et al. <ref type="bibr" target="#b201">[202]</ref> applied PI to learn the optimal targeted blood glucose levels from real data trajectories.</s><s xml:id="_dM8F3uq">Petersen et al. <ref type="bibr" target="#b202">[203]</ref> investigated the cytokine mediation problem in sepsis treatment, using the DRL method, Deep Deterministic Policy Gradient (DDPG) <ref type="bibr" target="#b232">[233]</ref>, to tackle the hidimensional continuous states and actions, and potential-based reward shaping <ref type="bibr" target="#b233">[234]</ref> to facilitate the learning efficiency.</s><s xml:id="_EkUgG22">The proposed approach was evaluated using an agent-based model, the Innate Immune Response Agent-Based Model (IIRABM), that simulates the immune response to infection.</s><s xml:id="_QpAxfPk">The learned treatment strategy was showed to achieve 0.8% mortality over 500 randomly selected patient parameterizations with mortalities average of 49%, suggesting that adaptive, person-</s></p><p xml:id="_rcUJvhe"><s xml:id="_Nds7KkZ">TABLE IV SUMMARY OF RL APPLICATION EXAMPLES IN THE DEVELOPMENT OF DTRS IN CRITICAL CARE Domain Application Reference Base method Efficient Techniques Representational Techniques Data Acquisition Highlights and Limits Sepsis Administration of IV fluid and maximum VP Komorowski et al. [192], [193] SARSA,PI N/A N/A MIMIC-III Naive application of SARSA and PI in a discrete state and action-space.</s><s xml:id="_yWe5NYc">Raghu et al. [194], [195] Q-learning N/A DRL (DDDQN) MIMIC-III Application of DRL in a fully continuous state but discrete action space.</s><s xml:id="_hySvtnu">Raghu et al. [196] PS MRL N/A MIMIC-III Model-based learning with continuous state-space; integrating clinician's policies into RL policies.</s><s xml:id="_5QvMZE2">Utomo et al. [197] MC N/A N/A MIMIC-III Estimating transitions of patient health conditions and treatments to increase its explainability.</s><s xml:id="_sXQhS72">Peng et al. [198] Q-learning N/A DRL (DDDQN) MIMIC-III Adaptive switching between kernel learning and DRL.</s><s xml:id="_K2DsCvS">Futoma et al. [199] Q-learning N/A DRL Clinical data at university hospital Tackling sparsely sampled and frequently missing multivariate time series data.</s><s xml:id="_HKSzpHK">Yu et al. [200] Q-learning BRL(FQI) DRL, IRL MIMIC-III Inferring the best reward functions using deep IRL.</s><s xml:id="_bSW8HNE">Li et al. [201] AC N/A PORL MIMIC-III Taking into account uncertainty and history information of sepsis patients.</s><s xml:id="_VSwrdF8">Targeted blood glucose regulation Weng et al. [202] PI N/A N/A MIMIC-III Learning the optimal targeted blood glucose levels for sepsis patients Cytokine mediation Petersen et al. [203] AC N/A DRL (DDPG) Agent-based model Using reward shaping to facilitate the learning efficiency; significantly reducing mortality from 49% to 0.8%.</s><s xml:id="_W6Exf3P">Anesthesia Regulation and automation of sedation and analgesia to maintain physiological stability and lowering pains of patients Moore et al. [204], [205] Q(λ) N/A N/A PK/PD model Achieving superior stability compared to a well-tuned PID controller.</s><s xml:id="_6835sCG">Moore et al. [206], [207] Q-learning N/A N/A PK/PD model Using the change of BIS as the state representation.</s><s xml:id="_5kFqMRn">Moore et al. [208], [209] Q-learning N/A N/A In vivo study First clinical trial for anesthesia administration using RL on human volunteers.</s><s xml:id="_ASJV62z">Sadati et al. [210] Unclear N/A N/A PK/PD model Expert knowledge can be used to realize reasonable initial dosage and keep drug inputs in safe values.</s><s xml:id="_BDd3tsg">Borera et al. [211] Q-learning N/A N/A PK/PD model Using an adaptive filter to eliminate the delays when estimating patient state.</s><s xml:id="_uXrV5qw">Lowery &amp; Faisal [212] AC N/A N/A PK/PD model Considering the continuous state and action spaces.</s><s xml:id="_WCmM49p">Padmanabhan et al. [213] Q-learning N/A N/A PK/PD model Regulating sedation and hemodynamic parameters simultaneously.</s><s xml:id="_XFKKZXY">Humbert et al. [214] N/A N/A POMDP, IRL Clinical data Training an RL agent to mimic decisions by expert anesthesiologists.</s><s xml:id="_avhNw8n">Others Heparin Dosing Nemati et al. [215] Q-learning BRL PORL MIMIC II End-to-end learning with hidden states of patients.</s><s xml:id="_53u8Rm5">Lin et al. [216] AC N/A DRL(DDPG) MIMIC, Emory Healthcare data Addressing dosing problems in continuous state-action spaces.</s><s xml:id="_CKTQ4us">General medication recommendation Wang et al. [217] AC N/A DRL (DDPG) MIMIC-III Combining supervised and reinforcement learning for medication dosing covering a large number of diseases.</s><s xml:id="_FXGAMJt">Mechanical ventilation and sedative dosing Prasad et al. [218] Q-learning BRL(FQI) N/A MIMIC-III Optimal decision making for the weaning time of mechanical ventilation and personalized sedation dosage.</s><s xml:id="_cfnVUFV">Yu et al. [219] Q-learning BRL(FQI) IRL MIMIC-III Applying IRL in inferring the reward functions.</s><s xml:id="_NVVTEMg">Yu et al. [220] AC N/A N/A MIMIC-III Combing supervised learning and AC for more efficient decision making.</s><s xml:id="_t4DnMv4">Jagannatha et al. [221] Q-learning, PS BRL(FQI) N/A MIMIC-III Analyzing limitations of off-policy policy evaluation methods in ICU settings.</s><s xml:id="_dyC5YPT">Ordering of lab tests Cheng et al. [222] Q-learning BRL(FQI) MORL MIMIC III Designing a multi-objective reward function that reflects clinical considerations when ordering labs.</s><s xml:id="_dQA2GzX">Chang et al. [223] Q-learning N/A DRL (Dueling DQN)</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_bhvG5B6">MIMIC III</head><p xml:id="_pUa9h27"><s xml:id="_ZgxdtfH">The first RL application on multi-measurement scheduling problem in the clinical setting.</s><s xml:id="_CaFA56s">Prevention and treatments for GVHD Krakow et al. <ref type="bibr" target="#b223">[224]</ref> Q-learning N/A N/A CIBMTR data First proposal of DTRs for acute GVHD prophylaxis and treatment.</s><s xml:id="_Jp7kt7r">Liu et al. <ref type="bibr" target="#b224">[225]</ref> Q-learning N/A DRL (DQN) CIBMTR data Incorporation of a supervised learning step into RL.</s></p><p xml:id="_7D5Y6CR"><s xml:id="_CBAd8e5">alized multi-cytokine mediation therapy could be promising for treating sepsis.</s></p><p xml:id="_WmtrYj2"><s xml:id="_FqUdCBn">2) Anesthesia: Another major drug dosing problem in ICUs is the regulation and automation of sedation and analgesia, which is essential in maintaining physiological stability and lowering pains of patients.</s><s xml:id="_H56r4n9">Whereas surgical patients typically require deep sedation over a short duration of time, sedation for ICU patients, especially when using mechanical ventilation, can be more challenging <ref type="bibr" target="#b217">[218]</ref>.</s><s xml:id="_vACnrv6">Critically ill patients who are supported by mechanical ventilation require adequate sedation for several days to guarantee safe treatment in the ICU <ref type="bibr" target="#b234">[235]</ref>.</s><s xml:id="_YpJSaPB">A misdosing of sedation or under sedation is not acceptable since over sedation can cause hypotension, prolonged recovery time, delayed weaning from mechanical ventilation, and other related negative outcomes, whereas under sedation can cause symptoms such as anxiety, agitation and hyperoxia <ref type="bibr" target="#b212">[213]</ref>.</s></p><p xml:id="_snMcPcb"><s xml:id="_YuTeJWC">The regulation of sedation in ICUs using RL methods has attracted attention of researcher for decades.</s><s xml:id="_W7tDam5">As early as in 1994, Hu et al. <ref type="bibr" target="#b69">[70]</ref> studied the problem of anesthesia control by applying some of the founding principles of RL (the MDP formulation and its planning solutions).</s><s xml:id="_RMqeAju">More recently, RL-based control methods, using surrogate measures of anesthetic effect, e.g., the bispectral (BIS) index, as the controlled variable, has enhanced individualized anesthetic management, resulting in the overall improvement of patient outcomes when compared with traditional controlled administration.</s><s xml:id="_v4DYFHF">Moore et al. <ref type="bibr" target="#b203">[204]</ref>, <ref type="bibr" target="#b204">[205]</ref> applied TD Q(λ) in administration of intravenous propofol in ICU settings, using the well-studied Marsh-Schnider pharmacokinetic model to estimate the distribution of drug within the patient, and a pharmacodynamic model for estimating drug effect.</s><s xml:id="_nSpAqAH">The RL method adopted the error of BIS and estimation of the four compartmental propofol concentrations as the input state, different propofol dose as control actions, and the BIS error as the reward.</s><s xml:id="_TxAMa8R">The method demonstrated superior stability and responsiveness when compared to a well-tuned PID controller.</s><s xml:id="_QaT55EM">The authors then modeled the drug disposition system as three states corresponding to the change of BIS, and applied basic Q-learning method to solving this problem <ref type="bibr" target="#b205">[206]</ref>, <ref type="bibr" target="#b206">[207]</ref>.</s><s xml:id="_TZqcjMx">They also presented the first clinical in vivo trial for closed-loop control of anesthesia administration using RL on 15 human volunteers <ref type="bibr" target="#b207">[208]</ref>, <ref type="bibr" target="#b208">[209]</ref>.</s><s xml:id="_RkxMD9c">It was demonstrated that patient specific control of anesthesia administration with improved control accuracy as compared to other studies in the literature could be achieved both in simulation and the clinical study.</s></p><p xml:id="_EmqZzWg"><s xml:id="_EZFpFyH">Targeting at both muscle relaxation (paralysis) and Mean Arterial Pressure (MAP), Sadati et al. <ref type="bibr" target="#b209">[210]</ref> proposed an RL-based fuzzy controllers architecture in automation of the clinical anesthesia.</s><s xml:id="_8wh5PxS">A multivariable anesthetic mathematical model was presented to achieve an anesthetic state using two anesthetic drugs of Atracurium and Isoflurane.</s><s xml:id="_4HmYUBH">The highlight was that the physician's clinical experience could be incorporated into the design and implementation of the architecture, to realize reasonable initial dosage and keep drug inputs in safe values.</s><s xml:id="_EXPv2v9">Padmanabhan et al. <ref type="bibr" target="#b212">[213]</ref> used a closed-loop anesthesia controller to regulate the BIS and MAP within a desired range.</s><s xml:id="_uPQEnwv">Specifically, a weighted combination of the error of the BIS and MAP signals is considered in the proposed RL algorithm.</s><s xml:id="_ntnUTtq">This reduces the computational complexity of the RL algorithm and consequently the controller processing time.</s><s xml:id="_rJnpeEs">Borera et al. <ref type="bibr" target="#b210">[211]</ref> proposed an Adaptive Neural Network Filter (ANNF) to improve RL control of propofol hypnosis.</s></p><p xml:id="_EP7hVEQ"><s xml:id="_YXTSpPE">Lowery and Faisal <ref type="bibr" target="#b211">[212]</ref> used a continuous AC method to first learn a generic effective control strategy based on average patient data and then fine-tune itself to individual patients in a personalization stage.</s><s xml:id="_JxuSMEH">The results showed that the reinforcement learner could reduce the dose of administered anesthetic agent by 9.4% as compared to a fixed controller, and keep the BIS error within a narrow, clinically acceptable range 93.9% of the time.</s><s xml:id="_eR5GKtU">More recently, an IRL method has been proposed that used expert trajectories provided by anesthesiologists to train an RL agent for controlling the concentration of drugs during a global anesthesia <ref type="bibr" target="#b213">[214]</ref>.</s></p><p xml:id="_embQtwN"><s xml:id="_ktABS6Z">3) Other Applications in Critical Care: While the previous sections are devoted to two topic-specific applications of RL methods in critical care domains, there are many other more general medical problems that perhaps have received less attention by researchers.</s><s xml:id="_4FRmrEH">One such problem is regarding the medication dosing, particulary, heparin dosing, in ICUs.</s><s xml:id="_2CDYGgJ">A recent study by Ghassemi et al. <ref type="bibr" target="#b235">[236]</ref> highlighted that the misdosing of medications in the ICU is both problematic and preventable, e.g., up to two-thirds of patients at the study institution received a non-optimal initial dose of heparin, due to the highly personal and complex factors that affect the dose-response relationship.</s><s xml:id="_fnK57Mp">To address this issue, Nemati et al.</s></p><p xml:id="_s8jhS5V"><s xml:id="_hExqSx2">[215] inferred hidden states of patients via discriminative hidden Markov model and applied neural FQI to learn optimal heparin dosages.</s><s xml:id="_52jGJP2">Lin et al. <ref type="bibr" target="#b215">[216]</ref> applied DDPG in continuous state-action spaces to learn a better policy for heparin dosing from observational data in MIMIC and the Emory University clinical data.</s><s xml:id="_JJ8bW4R">Wang et al. <ref type="bibr" target="#b216">[217]</ref> combined supervised signals and reinforcement signals to learn recommendations for medication dosing involving a large number of diseases and medications in ICUs.</s></p><p xml:id="_ZKd4A8h"><s xml:id="_9GSR86Z">Another typical application of RL in ICUs is to develop a decision support tool for automating the process of airway and mechanical ventilation.</s><s xml:id="_x7xmCfn">The need for mechanical ventilation is required when patients in ICUs suffer from acute respiratory failure (ARF) caused by various conditions such as cardiogenic pulmonary edema, sepsis or weakness after abdominal surgery <ref type="bibr" target="#b236">[237]</ref>.</s><s xml:id="_J6ExtW5">The management of mechanical ventilation is particularly challenging in ICUs.</s><s xml:id="_zfYsbBr">One one hand, higher costs occur if unnecessary ventilation is still taking effect, while premature extubation can give rise to increased risk of morbidity and mortality.</s><s xml:id="_D942mnN">Optimal decision making regarding when to wean patients off of a ventilator thus becomes nontrivial since there is currently no consistent clinical opinion on the best protocol for weaning of ventilation <ref type="bibr" target="#b237">[238]</ref>.</s><s xml:id="_FecFQpT">Prasad et al. <ref type="bibr" target="#b217">[218]</ref> applied off-policy RL algorithms, FQI-ERT and with feed forward neural networks, to determine the best weaning time of invasive mechanical ventilation, and the associated personalized sedation dosage.</s><s xml:id="_QgepjTZ">The policies learned showed promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.</s><s xml:id="_6gxg7JW">Targeting at the same problem as <ref type="bibr" target="#b217">[218]</ref>, Jagannatha et al. <ref type="bibr" target="#b220">[221]</ref> analyzed the properties and limitations of standard off-policy evaluation methods in RL and discussed possible extensions to them in order to improve their utility in clinical domains.</s><s xml:id="_pPEVCkc">More recently, Yu et al. applied Bayesian inverse RL <ref type="bibr" target="#b218">[219]</ref> and Supervised-actorcritic <ref type="bibr" target="#b219">[220]</ref> to learn a suitable ventilator weaning policy from real trajectories in retrospective ICU data.</s><s xml:id="_A98xGzf">RL has been also used in the development of optimal policy for the ordering of lab tests in ICUs [222], <ref type="bibr" target="#b222">[223]</ref>, and prevention and treatments for graft versus host disease (GVHD) <ref type="bibr" target="#b223">[224]</ref>, <ref type="bibr" target="#b224">[225]</ref> using data set from the Center for International Bone Marrow Transplant Research (CIBMTR) registry database.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_j3TcPjz">V. AUTOMATED MEDICAL DIAGNOSIS</head><p xml:id="_queHDtH"><s xml:id="_V3bc5wE">Medical diagnosis is a mapping process from a patient's information such as treatment history, current signs and symptoms to an accurate clarification of a disease.</s><s xml:id="_hERHXsU">Being a complex task, medical diagnosis often requires ample medical investigation on the clinical situations, causing significant cognitive burden for clinicians to assimilate valuable information from complex and diverse clinical reports.</s><s xml:id="_3dpH9rE">It has been reported that diagnostic error accounts for as high as 10% of deaths and 17% of adverse events in hospitals <ref type="bibr" target="#b238">[239]</ref>.</s><s xml:id="_dq6d3Cc">The error-prone process in diagnosis and the necessity to assisting the clinicians for a better and more efficient decision making urgently call for a significant revolution of the diagnostic process, leading to the advent of automated diagnostic era that is fueled by advanced big data analysis and machine learning techniques <ref type="bibr" target="#b239">[240]</ref>, <ref type="bibr" target="#b240">[241]</ref>, <ref type="bibr" target="#b241">[242]</ref>.</s></p><p xml:id="_pghXDZ2"><s xml:id="_qdGD2C3">Normally formulated as a supervised classification problem, existing machining learning methods on clinical diagnosis heavily rely on a large number of annotated samples in order to infer and predict the possible diagnoses <ref type="bibr" target="#b242">[243]</ref>, <ref type="bibr" target="#b243">[244]</ref>, <ref type="bibr" target="#b244">[245]</ref>.</s><s xml:id="_XEmEj96">Moreover, these methods have limits in terms of capturing the underlying dynamics and uncertainties in the diagnosing process and considering only a limited number of prediction labels <ref type="bibr" target="#b245">[246]</ref>.</s><s xml:id="_JjdpurP">To overcome these issues, researchers are increasingly interested in formulating the diagnostic inferencing problem as a sequential decision making process and using RL to leverage a small amount of labeled data with appropriate evidence generated from relevant external resources <ref type="bibr" target="#b245">[246]</ref>.</s><s xml:id="_S35f3eZ">The existing research can be classified into two main categories, according to the type of clinical data input into the learning process: the structured medical data such as physiological signals, images, vital signs and lab tests, and the unstructured data of free narrative text such as laboratory reports, clinical notes and summaries.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qanP7X9">A. Structured Medical Data</head><p xml:id="_S4YsZcS"><s xml:id="_RZ5D6uH">The most successful application of RL in diagnosis using structured data pertains to various processing and analysis tasks in medical image examination, such as feature extracting, image segmentation, and object detection/localization/tracing <ref type="bibr" target="#b246">[247]</ref>, <ref type="bibr" target="#b247">[248]</ref>.</s><s xml:id="_ADRAZk8">Sahba et al. <ref type="bibr" target="#b248">[249]</ref>, <ref type="bibr" target="#b249">[250]</ref>, <ref type="bibr" target="#b250">[251]</ref>, <ref type="bibr" target="#b251">[252]</ref> applied basic Q-learning to the segmentation of the prostate in transrectal ultrasound images (UI).</s><s xml:id="_q7gv7eW">Liu and Jiang <ref type="bibr" target="#b252">[253]</ref> used a DRL method, Trust Region Policy Optimization (TRPO), for joint surgical gesture segmentation and classification.</s><s xml:id="_xnmrwYf">Ghesu et al. <ref type="bibr" target="#b253">[254]</ref> applied basic DQN to automatic landmark detection problems, and achieved more efficient, accurate and robust performance than state-of-the-art machine learning and deep learning approaches on 2D Magnetic Resonance Images (MRI), UI and 3D Computed Tomography (CT) images.</s><s xml:id="_ZdW3EbU">This approach was later extended to exploit multi-scale image representations for large 3D CT scans <ref type="bibr" target="#b254">[255]</ref>, and consider incomplete data <ref type="bibr" target="#b255">[256]</ref> or nonlinear multi-dimensional parametric space in MRI scans of the brain region <ref type="bibr" target="#b256">[257]</ref>.</s></p><p xml:id="_cm59dCB"><s xml:id="_pQFZVQ2">Alansary et al. evaluated different kinds of DRL methods (DQN, Double DQN (DDQN), Duel DQN, and Duel DDQN) <ref type="bibr" target="#b11">[12]</ref> for anatomical landmark localization in 3D fetal UI <ref type="bibr" target="#b257">[258]</ref>, and automatic standard view plane detection <ref type="bibr" target="#b258">[259]</ref>.</s><s xml:id="_p69ACNA">Al and Yun <ref type="bibr" target="#b259">[260]</ref> applied AC based direct PS method for aortic valve landmarks localization and left atrial appendage seed localization in 3D CT images.</s><s xml:id="_e9ZK8as">Several researchers also applied DQN methods in 3D medical image registration problems <ref type="bibr" target="#b260">[261]</ref>, <ref type="bibr" target="#b261">[262]</ref>, <ref type="bibr" target="#b262">[263]</ref>, active breast lesion detection from dynamic contrast-enhanced MRI <ref type="bibr" target="#b263">[264]</ref>, and robust vessel centerline tracing problems in multi-modality 3D medical volumes <ref type="bibr" target="#b264">[265]</ref>.</s></p><p xml:id="_qUJu5mC"><s xml:id="_VPMkRnK">Netto et al. <ref type="bibr" target="#b265">[266]</ref> presented an overview of work applying RL in medical image applications, providing a detailed illustration of particular use of RL for lung nodules classification.</s><s xml:id="_5DmJ9bV">The problem of classification is modeled as a sequential decision making problem, in which each state is defined as the combination of five 3D geometric measurements, the actions are random transitions between states, and the final goal is to discover the shortest path from the pattern presented to a known target of a malignant or a benign pattern.</s><s xml:id="_cYsHp5v">Preliminary results demonstrated that the Q-learning method can effectively classify lung nodules from benign and malignant directly based on lung lesions CT images.</s></p><p xml:id="_QnGt2TJ"><s xml:id="_G8b8qkH">Fakih and Das <ref type="bibr" target="#b266">[267]</ref> developed a novel RL-based approach, which is capable of suggesting proper diagnostic tests that optimize a multi-objective performance criterion accounting for issues of costs, morbidity, mortality and time expense.</s><s xml:id="_DbZasRB">To this end, some diagnostic decision rules are first extracted from current medical databases, and then the set of possible testing choices can be identified by comparing the state of patient with the attributes in the decision rules.</s><s xml:id="_8yDeMMH">The testing choices and the combined overall performance criterion then serve as inputs to the core RL module and the VI algorithm is applied for obtaining optimized diagnostic strategies.</s><s xml:id="_rzu4Aug">The approach was evaluated on a sample diagnosing problem of solitary pulmonary nodule (SPN) and results verified its success in improving testing strategies in diagnosis, compared with several other fixed testing strategies.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_f8E4trq">B. Unstructured Medical Data</head><p xml:id="_twdPk2d"><s xml:id="_nr6Cabc">Unlike the formally structured data that are directly machine understandable, large proportions of clinical information are stored in a format of unstructured free text that contains a relatively more complete picture of associated clinical events <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_crPv4Vk">Given their expressive and explanatory power, there is great potential for clinical notes and narratives to play a vital role in assisting diagnosis inference in an underlying clinical scenario.</s><s xml:id="_ErmFsxv">Moreover, limitations such as knowledge incompleteness, sparsity and fixed schema in structured knowledge have motivated researchers to use various kinds of unstructured external resources such as online websites for related medical diagnosing tasks <ref type="bibr" target="#b245">[246]</ref>.</s></p><p xml:id="_HtcfcPf"><s xml:id="_txDkQ82">Motivated by the Text REtrieval Conference-Clinical Decision Support (TREC-CDS) track dataset <ref type="bibr" target="#b267">[268]</ref>, diagnosis inferencing from unstructured clinical text has gained much attention among AI researchers recently.</s><s xml:id="_T2p9kE2">Utilizing particular natural language processing techniques to extract useful information from clinical text, RL has been used to optimize the diagnosis inference procedure in several studies.</s><s xml:id="_vt5syzZ">Ling et al. <ref type="bibr" target="#b245">[246]</ref>, <ref type="bibr" target="#b268">[269]</ref> proposed a novel clinical diagnosis inferencing approach that applied DQN to incrementally learn about the most appropriate clinical concepts that best describe the correct diagnosis by using evidences gathered from relevant external resources (from Wikipedia and MayoClinic).</s><s xml:id="_P3tqEkG">Experiments on the TREC-CDS datasets demonstrated the effectiveness of the proposed approach over several non RL-based systems.</s></p><p xml:id="_ryTz7gS"><s xml:id="_qjGvstc">Exploiting real datasets from the Breast Cancer Surveillance Consortium (BCSC) <ref type="bibr" target="#b269">[270]</ref>, Chu et al. <ref type="bibr" target="#b270">[271]</ref> presented an adaptive online learning framework for supporting clinical breast cancer diagnosis.</s><s xml:id="_J2m5S8h">The framework integrates both supervised learning models for breast cancer risk assessment and RL models for decision-making of clinical measurements.</s><s xml:id="_a2n3UJQ">The framework can quickly update relevant model parameters based on current diagnosis information during the training process.</s><s xml:id="_hKpa9Mp">Additionally, it can build flexible fitted models by integrating different model structures and plugging in the corresponding parameters during the prediction process.</s><s xml:id="_kQARNvr">The authors demonstrated that the RL models could achieve accurate breast cancer risk assessment from sequential data and incremental features.</s></p><p xml:id="_SwhMH68"><s xml:id="_YPyCMhn">In order to facilitate self-diagnosis while maintaining reasonable accuracy, the concept of symptom checking (SC) has been proposed recently.</s><s xml:id="_uS2dbcH">SC first inquires a patient with a series of questions about their symptoms, and then attempts to diagnose some potential diseases <ref type="bibr" target="#b244">[245]</ref>.</s><s xml:id="_J7Xw7Ht">Tang et al. <ref type="bibr" target="#b271">[272]</ref> formulated inquiry and diagnosis policies as an MDP, and adopted DQN to learn to inquire and diagnose based on limited patient data.</s><s xml:id="_RgSB9JC">Kao et al. <ref type="bibr" target="#b272">[273]</ref> applied context-aware HRL scheme to improve accuracy of SC over traditional systems making a limited number of inquiries.</s><s xml:id="_x3WEKbg">Empirical studies on a simulated dataset showed that the proposed model drastically improved disease prediction accuracy by a significant margin.</s><s xml:id="_knbG7Dg">The SC system was successfully employed in the DeepQ Tricorder which won the second prize in the Qualcomm Tricorder XPRIZE competition in year 2017 <ref type="bibr" target="#b273">[274]</ref>, <ref type="bibr" target="#b274">[275]</ref>.</s></p><p xml:id="_mkbeVdm"><s xml:id="_vBPsF8y">A dialogue system was proposed in <ref type="bibr" target="#b275">[276]</ref> for automatic diagnosis, in which the medical dataset was built from a pediatric department in a Chinese online healthcare community.</s><s xml:id="_kY7MPcS">The dataset consists of self-reports from patients and conversational data between patients and doctors.</s><s xml:id="_rSWY54A">A DQN approach was then used to train the dialogue policy.</s><s xml:id="_RtGGRUS">Experiment results showed that the RL-based dialogue system was able to collect symptoms from patients via conversation and improve the accuracy for automatic diagnosis.</s><s xml:id="_YtEzQuP">In order to increase the efficiency of the dialogue systems, Tang et al. <ref type="bibr" target="#b276">[277]</ref> applied DQN framework to train an efficient dialogue agent to sketch disease-specific lexical probability distribution, and thus to converse in a way that maximizes the diagnosis accuracy and minimizes the number of conversation turns.</s><s xml:id="_aVjcjcy">The dialogue system was evaluated on the mild cognitive impairment diagnosis from a real clinical trial, and results showed that the RL-driven framework could significantly outperform state-ofthe-art supervised learning approaches using only a few turns of conversation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_e42Kdtx">VI. OTHER HEALTHCARE DOMAINS</head><p xml:id="_dqhG6WP"><s xml:id="_3kYebXV">Besides the above applications of RL in DTR design and automated medical diagnosis, there are many other case applications in broader healthcare domains that focus on problems specifically in health resource scheduling and allocation, optimal process control, drug discovery and development, as well as health management.</s></p><p xml:id="_AdzRMPJ"><s xml:id="_WxSdQPg">(1) Health Resource Scheduling and Allocation.</s><s xml:id="_Rx3bWPa">The healthcare system is a typical service-oriented system where customers (e.g., patients) are provided with service using limited resources, e.g. the time slots, nursing resources or diagnostic devices <ref type="bibr" target="#b277">[278]</ref>.</s><s xml:id="_XZPz8eR">Business process management (BPM) plays a key role in such systems as the objective of the service provider is to maximize profit overtime, considering various customer classes and service types with dynamics or uncertainties such as cancellations or no-shows of patients <ref type="bibr" target="#b278">[279]</ref>, <ref type="bibr" target="#b279">[280]</ref>.</s><s xml:id="_xrkBXfE">Since the optimal resource allocation problem in BPM can be seen as a sequential decision making problem, RL is then naturally suitable for offering reasonable solutions.</s><s xml:id="_8HjUreU">Huang et al. <ref type="bibr" target="#b278">[279]</ref> formulated the allocation optimization problems in BPM as an MDP and used basic Q-learning algorithm to derive an optimal solution.</s><s xml:id="_w8y4zgX">The RL-based approach was then applied to address the problem of optimizing resource allocation in radiology CT-scan examination process.</s><s xml:id="_f6tcCFg">A heuristic simulationbased approximate DP approach was proposed in <ref type="bibr" target="#b277">[278]</ref>, which considered both stochastic service times and uncertain future arrival of clients.</s><s xml:id="_J9tdJA7">The experimental investigation using data from the radiological department of a hospital indicated an increases of 6.9% in the average profit of the hospital and 9% in the number of examinations.</s><s xml:id="_EwJxWtG">Gomes <ref type="bibr" target="#b280">[281]</ref> applied a DRL method, Asynchronous Advantage Actor Critic (A3C) <ref type="bibr" target="#b281">[282]</ref>, to schedule appointments in a set of increasingly challenging environments in primary care systems.</s></p><p xml:id="_MsDPsVt"><s xml:id="_HyaQy2v">(2) Optimal Process Control.</s><s xml:id="_WQdB7BF">RL has also been widely applied in deriving an optimal control policy in a variety of healthcare situations, ranging from surgical robot operation <ref type="bibr" target="#b282">[283]</ref>, <ref type="bibr" target="#b283">[284]</ref>, <ref type="bibr" target="#b284">[285]</ref>, <ref type="bibr" target="#b285">[286]</ref>, <ref type="bibr" target="#b286">[287]</ref>, functional electrical stimulation (FES) <ref type="bibr" target="#b287">[288]</ref>, <ref type="bibr" target="#b288">[289]</ref>, and adaptive rate control for medical video streaming <ref type="bibr" target="#b289">[290]</ref>, <ref type="bibr" target="#b290">[291]</ref>.</s><s xml:id="_y6uH5ux">Li and Burdick <ref type="bibr" target="#b282">[283]</ref> applied RL to learn a control policy for a surgical robot such that the robot can conduct some basic clinical operations automatically.</s><s xml:id="_NGAnFuy">A function approximation based IRL method was used to derive an optimal policy from experts' demonstrations in high dimensional sensory state space.</s><s xml:id="_UvXQSz2">The method was applied to the evaluation of surgical robot operators in three clinical tasks of knot tying, needling passing and suturing.</s><s xml:id="_mPbqFhV">Thananjeyan et al. <ref type="bibr" target="#b283">[284]</ref> and Nguyen et al. <ref type="bibr" target="#b284">[285]</ref> applied DRL algorithm, TRPO, in learning tensioning policies effectively for surgical gauze cutting.</s><s xml:id="_ZK2WDEZ">Chen et al. <ref type="bibr" target="#b285">[286]</ref> combined programming by demonstration and RL for motion control of flexible manipulators in minimally invasive surgical performance, while Baek et al. <ref type="bibr" target="#b286">[287]</ref> proposed the use of RL to perform resection automation of cholecystectomy by planning a path that avoids collisions in a laparoscopic surgical robot system.</s></p><p xml:id="_r7mNH9V"><s xml:id="_9smzJ3A">FES employs neuroprosthesis controllers to apply electrical current to the nerves and muscles of individuals with spinal cord injuries for rehabilitative movement <ref type="bibr" target="#b291">[292]</ref>.</s><s xml:id="_uwRa9XJ">RL has been used to calculate stimulation patterns to efficiently adapt the control strategy to a wide range of time varying situations in patients' preferences and reaching dynamics.</s><s xml:id="_SnsEzTZ">AC-based control strategies <ref type="bibr" target="#b292">[293]</ref>, <ref type="bibr" target="#b293">[294]</ref>, <ref type="bibr" target="#b288">[289]</ref> were proposed to evaluate targetoriented task performed using a planar musculoskeletal human arm in FES.</s><s xml:id="_NTm8EFR">To solve the reward learning problem in large state spaces, an IRL approach was proposed in <ref type="bibr" target="#b287">[288]</ref> to evaluate the effect of rehabilitative stimulations on patients with spinal cord injuries based on the observed patient motions.</s></p><p xml:id="_jJmDFax"><s xml:id="_TuMZwQM">RL-based methods have also been widely applied in adaptive control in mobile health medical video communication systems.</s><s xml:id="_mGfNSqH">For example, Istepanian et al. <ref type="bibr" target="#b289">[290]</ref> proposed a new rate control algorithm based on Q-learning that satisfies medical quality of service requirements in bandwidth demanding situations of ultrasound video streaming.</s><s xml:id="_jf76aY5">Alinejad <ref type="bibr" target="#b290">[291]</ref> applied Q-learning for cross-layer optimization in real-time medical video streaming.</s></p><p xml:id="_WXcNqBc"><s xml:id="_buyZnNZ">(3) Drug Discovery and Development.</s><s xml:id="_hCuT7Da">Drug discovery and development is a time-consuming and costly process that usually lasts for 10-17 years, but with as low as around 10% overall probability of success <ref type="bibr" target="#b294">[295]</ref>.</s><s xml:id="_jkudKdz">To search an effective molecule that meets the multiple criteria such as bioactivity and synthetic accessibility in a prohibitively huge synthetically feasible molecule space is extremely difficult.</s><s xml:id="_893zJ4T">By using computational methods to virtually design and test molecules, de novo design offers ways to facilitate cycle of drug development <ref type="bibr" target="#b295">[296]</ref>.</s><s xml:id="_szUC6cQ">It is until recent years that RL methods have been applied in various aspects of de novo design for drug discovery and development.</s><s xml:id="_ar6gUSu">Olivecrona <ref type="bibr" target="#b296">[297]</ref> used RL to fine tune the recurrent neural network in order to generate molecules with certain desirable properties through augmented episodic likelihood.</s><s xml:id="_dnkWy4E">Serrano et al. <ref type="bibr" target="#b297">[298]</ref> applied DQN to solve the proteinligand docking prediction problem, while Neil et al.</s></p><p xml:id="_Ew9u6SZ"><s xml:id="_bpVGrgN">[299] investigated the PPO method in molecular generation.</s></p><p xml:id="_Z5tAT2X"><s xml:id="_DV4kyBQ">More recently, Popova et al. <ref type="bibr" target="#b299">[300]</ref> applied DRL methods to generate novel targeted chemical libraries with desired properties.</s></p><p xml:id="_DKp7mrU"><s xml:id="_KGunZYW">(4) Health Management.</s><s xml:id="_E3pGsRX">As a typical application domain, RL has also been used in adaptive interventions to support health management such as promoting physical activities for diabetic patients <ref type="bibr" target="#b300">[301]</ref>, <ref type="bibr" target="#b301">[302]</ref>, or weight management for obesity patients <ref type="bibr" target="#b302">[303]</ref>, <ref type="bibr" target="#b303">[304]</ref>.</s><s xml:id="_5kF7JF6">In these applications, throughout continuous monitoring and communication of mobile health, personalized intervention policies can be derived to input the monitored measures and output when, how and which plan to deliver.</s><s xml:id="_QXsu8pv">A notable work was by Yom et al. <ref type="bibr" target="#b300">[301]</ref>, who applied RL to optimize messages sent to the users, in order to improve their compliance with the activity plan.</s><s xml:id="_cd6XRNU">A study of 27 sedentary diabetes type 2 patients showed that participants who received messages generated by the RL algorithm increased the amount of activity and pace of walking, while the patients using static policy did not.</s><s xml:id="_6JmWpns">Patients assigned to the RL algorithm group experienced a superior reduction in blood glucose levels compared to the static control policies, and longer participation caused greater reductions in blood glucose levels.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_5NxKvPM">VII. CHALLENGES AND OPEN ISSUES</head><p xml:id="_PZfn6Nc"><s xml:id="_XHRe6Xp">The content above has summarized the early endeavors and continuous progress of applying RL in healthcare over the past decades.</s><s xml:id="_XHXuv5S">Focus has been given to the vast variety of application domains in healthcare.</s><s xml:id="_jc56GWg">While notable success has been obtained, the majority of these studies simply applied existing naive RL approaches in solving healthcare problems in a relatively simplified setting, thus exhibiting some common shortcomings and practical limitations.</s><s xml:id="_9yCsBPB">This section discusses several challenges and open issues that have not been properly addressed by the current research, from perspectives of how to deal with the basic components in RL (i.e., formulation of states, actions and rewards, learning with world models, and evaluation of policies), and fundamental theoretical issues in traditional RL research (i.e., the exploration-exploitation tradeoff and credit assignment problem).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GgRpYPW">A. State/Action Engineering</head><p xml:id="_PK7NX3G"><s xml:id="_g9VywWT">The first step in applying RL to a healthcare problem is determining how to collect and pre-process proper medical data, and summarize such data into some manageable state representations in a way that sufficient information can be retained for the task at hand.</s><s xml:id="_hAqh7UH">Selecting the appropriate level of descriptive information contained in the states is extremely important.</s><s xml:id="_KvgDU6a">On one hand, it would be better to contain as detailed information as possible in the states, since this complete information can provide a greater distinction among patients.</s><s xml:id="_H3gkC6H">On the other hand, however, increasing the state space makes the model become more difficult to solve.</s><s xml:id="_u3YrnTd">It is thus essential that a good state representation include any compulsory factors or variables that causally affect both treatment decisions and the outcomes.</s><s xml:id="_WeRuwwF">Previous studies have showed that, to learn an effective policy through observational medical data, the states should be defined in a way that to the most approximates the behavior policy that has generated such data <ref type="bibr" target="#b304">[305]</ref>, <ref type="bibr" target="#b305">[306]</ref>.</s></p><p xml:id="_YY6u2J9"><s xml:id="_FQmkq85">However, data in medical domains often exhibit notable biases or noises that are presumably varying among different clinicians, devices, or even medical institutes, reflecting comparable inter-patient variability <ref type="bibr" target="#b91">[92]</ref>.</s><s xml:id="_2FTk3Tt">For some complex diseases, clinicians still face inconsistent guides in selecting exact data as the state in a given case <ref type="bibr" target="#b190">[191]</ref>.</s><s xml:id="_Jj4kpFw">In addition, the notorious issue of missing or incomplete data can further exaggerate the problem of data collection and state representation in medical settings, where the data can be collected from patients who may fail to complete the whole trial, or the number of treatment stages or timing of initializing the next line of therapy is flexible.</s><s xml:id="_f7mDrR5">This missing or censoring data will tend to increase the variance of estimates of the value function and thus the policy in an RL setting.</s><s xml:id="_kXuc3km">While the missing data problem can be generally solved using various imputation methods that sample several possible values from the estimated distribution to fill in missing values, the censoring data problem is far more challenging, calling for more sophisticated techniques for state representation and value estimation in such flexible settings <ref type="bibr" target="#b95">[96]</ref>, <ref type="bibr" target="#b96">[97]</ref>.</s></p><p xml:id="_vZN8cYg"><s xml:id="_p6QbkyH">Most existing work defines the states over the processed medical data with raw physiological, pathological, and demographics information, either using simple discretization methods to enable storage of value function in tabular form, or using some kinds of function approximation models (e.g., linear models or deep neural models).</s><s xml:id="_uQ3nsWg">While this kind of state representation is simple and easy to implement, the rich temporal dependence or causal information, which is the key feature of medical data, can be largely neglected <ref type="bibr" target="#b306">[307]</ref>.</s><s xml:id="_DejTATf">To solve this issue, various probabilistic graphical models <ref type="bibr" target="#b307">[308]</ref> can be used to allow temporal modeling of time series medical data, such as dynamic Bayesian networks (DBNs), in which nodes correspond to the random variables of interest, edges indicate the relationship between these random variables, and additional edges model the time dependency.</s><s xml:id="_6ghdjCu">These kinds of graphical models have the desirable property that allows for interpretation of interactions between state variables or between states and actions, which is not the case for other methods such as SVMs and neural networks.</s></p><p xml:id="_e26sgdp"><s xml:id="_cMKME7b">Coupled with the state representation in RL is the formulation of actions.</s><s xml:id="_6aZWqNk">The majority of existing work has mainly focused on discretization of the action space into limited bins of actions.</s><s xml:id="_FeUranB">Although this formulation is quite reasonable in some medical settings, such as choices in between turning on ventilation or weaning off it, there are many other situations where actions are by themselves continuous/multidimensional variables.</s><s xml:id="_DbyTcF7">While the simplification of discretizing medicine dosage is necessary in the early proof-of-concept stage, realizing fully continuous dosing in the original action space is imperative in order to meet the commitments of precision medicine <ref type="bibr" target="#b308">[309]</ref>.</s><s xml:id="_r8EWb3r">There has been a significant achievement in the continuous control using AC methods and PS methods in the past years, particularly from the area of robotic control <ref type="bibr" target="#b19">[20]</ref> and DRL <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_EX8sQPX">While this achievement can provide direct solutions to this problem, selecting the action over large/infinite space is still non-trivial, especially when dealing with any sample complexity guarantees (PAC).</s><s xml:id="_dTVTkv8">An effective method for efficient action selection in continuous and high dimensional action spaces, while at the same time maintaining low exploration complexity of PAC guarantees would extend the applicability of current methods to more sample-critical medical problems.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZuERJ8H">B. Reward Formulation</head><p xml:id="_pEnE8YX"><s xml:id="_8EfBgj2">Among all the basic components, the reward may be at the core of an RL process.</s><s xml:id="_5PKUDPs">Since it encodes the goal information of a learning task, a proper formulation of reward functions plays the most crucial role in the success of RL.</s><s xml:id="_mA7GtH9">However, the majority of current RL applications in healthcare domains are still grounded on simple numerical reward functions that must be explicitly defined beforehand to indicate the goal of treatments by clinicians.</s><s xml:id="_ZXv6z6f">It is true that in some medical settings, the outcomes of treatments can be naturally generated and explicitly represented in a numerical form, for example, the time elapsed, the vitals monitored, or the mortality reduced.</s><s xml:id="_udqYtkG">In general, however, specifying such a reward function precisely is not only difficult but sometimes even misleading.</s><s xml:id="_Cq6WnNx">For instance, in treatment of cancers <ref type="bibr" target="#b82">[83]</ref>, the reward function was usually decomposed into several independent or contradictory components based on some prior domain knowledge, each of which was mapped into some integer numbers, e.g., -60 as a high penalty for patient death and +15 as a bonus for a cured patient.</s><s xml:id="_zKbvcGg">Several threshold and weighting parameters were needed to provide a way for trading-off efficacy and toxicity, which heavily rely on clinicians' personal experience that varies from one to another.</s><s xml:id="_XRtbJvB">This kind of somewhat arbitrary quantifications might have significant influence on the final learned therapeutic strategies and it is unclear how changing these numbers can affect the resulting strategies.</s></p><p xml:id="_Yy4Pgsx"><s xml:id="_CSS34Uy">To conquer the above limitations, one alternative is to provide the learning agent with more qualitative evaluations for actions, turning the learning into a PRL problem <ref type="bibr">[310]</ref>.</s><s xml:id="_vaS3Asv">Unlike the standard RL approaches that are restricted to numerical and quantitative feedback, the agent's preferences instead can be represented by more general types of preference models such as ranking functions that sort states, actions, trajectories or even policies from most to least promising <ref type="bibr" target="#b50">[51]</ref>.</s><s xml:id="_XbHVeQb">Using such kind of ranking functions has a number of advantages as they are more natural and easier to acquire in many applications in clinical practice, particularly, when it is easier to require comparisons between several, possibly suboptimal actions or trajectories than to explicitly specify their performance.</s><s xml:id="_8wRfW5t">Moreover, considering that the medical decisions always involve two or more related or contradictory aspects during treatments such as benefits versus associated cost, efficacy versus toxicity, and efficiency versus risk, it is natural to shape the learning problem into a multi-objective optimization problem.</s><s xml:id="_x9cHYAZ">MORL techniques <ref type="bibr" target="#b49">[50]</ref> can be applied to derive a policy that makes a trade-off between distinct objectives in order to achieve a Pareto optimal solution.</s><s xml:id="_6SMAExJ">Currently, there are only very limited studies in the literature that applied PRL <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b177">[178]</ref> and MORL <ref type="bibr" target="#b176">[177]</ref>, <ref type="bibr" target="#b310">[311]</ref>, <ref type="bibr" target="#b175">[176]</ref>, <ref type="bibr">[222]</ref> in medical settings, for optimal therapy design in treatment of cancer, schizophrenia or lab tests ordering in ICUs.</s><s xml:id="_BtBJWn7">However, all these studies still focus on very limited application scenarios where only static preferences or fixed objectives were considered.</s><s xml:id="_Wznd7RU">In a medical context, the reward function is usually not a fixed term but subject to changing with regard to a variety of factors such as the time, the varying clinical situations and the evolving physiopsychic conditions of the patients.</s><s xml:id="_dQHv7MW">Applying PRL and MORL related principles to broader domains and considering the dynamic and evolving process of patients' preferences and treatment objectives is still a challenging issue that needs to be further explored.</s></p><p xml:id="_7S48wPs"><s xml:id="_zGWd7jx">A more challenging issue is regarding the inference of reward functions directly from observed behaviors or clinical data.</s><s xml:id="_3RBs452">While it is straightforward to formulate a reward function, either quantitatively or qualitatively, and then compute the optimal policy using this function, it is sometimes preferable to directly estimate the reward function of experts from a set of presumably optimal treatment trajectories in retrospective medical data.</s><s xml:id="_NrGUFg8">Imitation learning, particularly, IRL <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, is one of the most feasible approaches to infer reward functions given observations of optimal behaviour.</s><s xml:id="_JxwgWeb">However, applying IRL in clinical settings is not straightforward, due to the inherent complexity of clinical data and its associated uncertainties during learning.</s><s xml:id="_ngzGvc9">The variance during the policy learning and reward learning can amplify the bias in each learning process, potentially leading to divergent solutions that can be of little use in practical clinical applications <ref type="bibr" target="#b311">[312]</ref>, <ref type="bibr" target="#b130">[131]</ref>.</s></p><p xml:id="_CkzNwC5"><s xml:id="_rTWScnQ">Last but not the least, while it is possible to define a short-term reward function at each decision step using prior human knowledge, it would be more reasonable to provide a long-term reward only at the end of a learning episode.</s><s xml:id="_yga6EFz">This is especially the case in healthcare domains where the real evaluation outcomes (e.g., decease of patients, duration of treatment) can only be observed at the end of treatment.</s><s xml:id="_cNnwPeW">Learning with sparse rewards is a challenging issue that has attracted much attention in recent RL research.</s><s xml:id="_sVsN89k">A number of effective approaches have been proposed, such as the hindsight experience replay <ref type="bibr" target="#b312">[313]</ref>, the unsupervised auxiliary learning <ref type="bibr" target="#b313">[314]</ref>, the imagination-augmented learning <ref type="bibr" target="#b314">[315]</ref>, and the reward shaping <ref type="bibr" target="#b233">[234]</ref>.</s><s xml:id="_F2VJa8g">While there have been several studies that address the sparse reward problem in healthcare domains, most of these studies only focus on DTRs with a rather short horizon (typically three or four steps).</s><s xml:id="_ngXqEPS">Moreover, previous work has showed that entirely ignoring short-term rewards (e.g.</s><s xml:id="_y9WwaCm">maintaining hourly physiologic blood pressure for sepsis patients) could prevent from learning crucial relationships between certain states and actions <ref type="bibr" target="#b306">[307]</ref>.</s><s xml:id="_KPbm8rJ">How to tackle sparse reward learning with a long horizon in highly dynamic clinical environments is still a challenging issue in both theoretical and practical investigations of RL in healthcare.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pWkdc8v">C. Policy Evaluation</head><p xml:id="_fwan9XH"><s xml:id="_NMGbavF">The process of estimating the value of a policy (i.e., target policy) with data collected by another policy (i.e., behavior policy) is called off-policy evaluation problem <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_ZSB866F">This problem is critical in healthcare domains because it is usually infeasible to estimate policy value by running the policy directly on the target populations (i.e., patients) due to high cost of experiments, uncontrolled risks of treatments, or simply unethical/illegal humanistic concerns.</s><s xml:id="_q7eZRyz">Thus, it is needed to estimate how the learned policies might perform on retrospective data before testing them in real clinical environments.</s><s xml:id="_9vZVYHP">While there is a large volume of work in RL community that focuses on importance sampling (IS) techniques and how to trade off between bias and variance in IS-based offpolicy evaluation estimators (e.g., <ref type="bibr" target="#b315">[316]</ref>), simply adopting these estimators in healthcare settings might be unreliable due to issues of sparse rewards or large policy discrepancy between RL learners and physicians.</s><s xml:id="_CKbfmgC">Using sepsis management as a running example, Gottesman et al., <ref type="bibr" target="#b304">[305]</ref> discussed in detail why evaluation of polices using retrospective health data is a fundamentally challenging issue.</s><s xml:id="_tuBa6rF">They argued that any inappropriate handling of state representation, variance of IS-based statistical estimators, and confounders in more adhoc measures would result in unreliable or even misleading estimates of the quality of a treatment policy.</s><s xml:id="_rDXuJbg">The estimation quality of the off-policy evaluation is critically dependent on how precisely the behaviour policy is estimated from the data, and whether the probabilities of actions under the approximated behaviour policy model represent the true probabilities <ref type="bibr" target="#b305">[306]</ref>.</s><s xml:id="_uVwjAxT">While the main reasons have been largely unveiled, there is still little work on effective policy evaluation methods in healthcare domains.</s><s xml:id="_86qUmm5">One recent work is by Li et al. <ref type="bibr" target="#b200">[201]</ref>, who provided an off-policy POMDP learning method to take into account uncertainty and history information in clinical applications.</s><s xml:id="_aQC4XYN">Trained on real ICU data, the proposed policy was capable of dictating near-optimal dosages in terms of vasopressor and intravenous fluid in a continuous action space for sepsis patients.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rYumCRV">D. Model Learning</head><p xml:id="_cYxu6ya"><s xml:id="_TWF5rKu">In the efficient techniques described in Section II-B, modelbased methods enable improved sample efficiency over modelfree methods by learning a model of the transition and reward functions of the domain on-line and then planing a policy using this model <ref type="bibr" target="#b42">[43]</ref>.</s><s xml:id="_TyyutMr">It is surprising that there are quite limited model-based RL methods applied in healthcare in the current literature <ref type="bibr" target="#b195">[196]</ref>, <ref type="bibr" target="#b192">[193]</ref>, <ref type="bibr" target="#b196">[197]</ref>.</s><s xml:id="_sghkTua">While a number of modelbased RL algorithms have been proposed and investigated in the RL community (e.g., R-max <ref type="bibr" target="#b24">[25]</ref>, E 3 <ref type="bibr" target="#b316">[317]</ref>), most of these algorithms assume that the agent operates in small domains with a discrete state space, which is contradictory to the healthcare domains usually involving multi-dimensional continuously valued states and actions.</s><s xml:id="_J3swtDe">Learning and planning over such large scale continuous models would cause additional challenges for existing model-based methods <ref type="bibr" target="#b42">[43]</ref>.</s><s xml:id="_cWXPCsR">A more difficult problem is to develop efficient exploration strategies in continuous action/state space <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_4MB7Eg2">By deriving a finite representation of the system that both allows efficient planning and intelligent exploration, it is potential to solve the challenging model learning tasks in healthcare systems more efficiently than contemporary RL algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_aFDjfPq">E. Exploration Strategies</head><p xml:id="_d3rvV6C"><s xml:id="_eeMASRT">Exploration plays a core role in RL, and a large amount of effort has been devoted to this issue in the RL community.</s><s xml:id="_UN9XXTj">A wealth of exploration strategies have been proposed in the past decades.</s><s xml:id="_qVJZvvw">Surprisingly, the majority of existing RL applications in healthcare domains simply adopt simple heuristic-based exploration strategies (i.e., ε-greedy strategy).</s><s xml:id="_AHxWE6N">While this kind of handling exploration dilemmas has made notable success, it becomes infeasible in dealing with more complicated dynamics and larger state/action spaces in medical settings, causing either a large sample complexity or an asymptotic performance far from the optimum.</s><s xml:id="_GF3yj5y">Particularly, in cases of an environment where only a rather small percentage of the state space is reachable, naive exploration from the entire space would be quite inefficient.</s><s xml:id="_4z6sRQb">This problem is getting more challenging in continuous state/action space, for instance, in the setting of HIV treatment <ref type="bibr" target="#b141">[142]</ref>, where the basin of attraction of the healthy state is rather small compared to that of the unhealthy state.</s><s xml:id="_JEWKYkb">It has been shown that traditional exploration methods are unable to obtain obvious performance improvement and generate any meaningful treatment strategy even after a long period of search in the whole space <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b150">[151]</ref>.</s><s xml:id="_b7tv5Wr">Therefore, there is a justifiable need for strategies that can identify dynamics during learning or utilize a performance measure to explore smartly in high dimensional spaces.</s><s xml:id="_gNsN3rv">In recent years, several more advanced exploration strategies have been proposed, such as PAC guaranteed exploration methods targeting at continuous spaces <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b150">[151]</ref>, concurrent exploration mechanisms <ref type="bibr" target="#b317">[318]</ref>, <ref type="bibr" target="#b318">[319]</ref>, <ref type="bibr" target="#b319">[320]</ref> and exploration in deep RL <ref type="bibr" target="#b320">[321]</ref>, <ref type="bibr" target="#b321">[322]</ref>, <ref type="bibr" target="#b322">[323]</ref>.</s><s xml:id="_gTq7BfU">It is thus imperative to incorporate such exploration strategies in more challenging medical settings, not only to decrease the sample complexity significantly, but more importantly to seek out new treatment strategies that have not been discovered before.</s></p><p xml:id="_Buj2Khk"><s xml:id="_z9eHcgt">Another aspect of applying exploration strategies in healthcare domains is the consideration of true cost of exploration.</s><s xml:id="_epjTbG6">Within the vanilla RL framework, whenever an agent explores an inappropriate action, the consequent penalty acts as a negative reinforcement in order to discourage the wrong action.</s><s xml:id="_MdQY4yB">Although this procedure is appropriate for most situations, it may be problematic in some environments where the consequences of wrong actions are not limited to bad performance, but can result in unrecoverable effects.</s><s xml:id="_6PrneU9">This is obviously true when dealing with patients in healthcare domains: although we can reset a robot when it has fallen down, we cannot bring back to life when a patience has been given a fatal medical treatment.</s><s xml:id="_KzW8EPD">Consequently, methods for safe exploration are of great real world interest in medical settings, in order to preclude unwanted, unsafe actions <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b323">[324]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GCyRWSu">F. Credit Assignment</head><p xml:id="_esSgA9w"><s xml:id="_tM4kM2x">Another important aspect of RL is the credit assignment problem that decides when an action or which actions is responsible for the learning outcome after a sequence of decisions.</s><s xml:id="_cTGAcbB">This problem is critical as the evaluation of whether an action being "good" or "bad" usually cannot be decided upon right away, but until the final goal has been achieved by the agent.</s><s xml:id="_xT3xEF5">As each action at each step contributes more or less to the final performance of success or failure, it is thus necessary to give distinct credit to the actions along the whole path, giving rise to the difficult problem of temporal credit assignment problem.</s><s xml:id="_eptTHFw">A related problem is the structural credit assignment problem, in which the problem is to distribute feedback over the multiple candidates (e.g., multiple concurrently learning agents, action choices, or structure representations of the agent's policy).</s></p><p xml:id="_SmtWmWH"><s xml:id="_Hc4Q7zh">The temporal credit assignment problem is more prominent in healthcare domains as the effect of treatments can be much varied or delayed.</s><s xml:id="_tYkeHE2">Traditional RL research tackles the credit assignment problem using simple heuristics such as eligibility traces that weigh the past actions according to how far the time has elapsed (i.e., the backward view), or discount factors that weigh the future events according to how far away they will happen (i.e., the forward view) <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_T6RTRSk">These kinds of fixed and simplified heuristics are incapable of modelling more complex interaction modes in a medical situation.</s><s xml:id="_2xVh98J">As a running example of explaining changes in blood glucose of a person with type 1 diabetes mellitus <ref type="bibr" target="#b324">[325]</ref>, it is difficult to give credit to the two actions of doing exercise in the morning or taking insulin after lunch, both of which can potentially cause hypoglycemia in the afternoon.</s><s xml:id="_N9v2e83">Since there are many factors to affect blood glucose and the effect can take place after many hours, e.g., moderate exercise can lead to heightened insulin sensitivity for up to 22 hours, simply assigning an eligibility trace that decays with time elapsed is thus unreasonable, misleading or even incorrect.</s><s xml:id="_dG7QHrx">How to model the time-varying causal relationships in healthcare and incorporate them into the learning process is therefore a challenging issue that requires more investigations.</s><s xml:id="_YtSPdZA">The abundant literature in causal explanation <ref type="bibr" target="#b325">[326]</ref> and inference <ref type="bibr" target="#b326">[327]</ref> can be introduced to provide a more powerful causal reasoning tool to the learning algorithm.</s><s xml:id="_W8TCznJ">By producing hypothesized sequences of causal mechanisms that seek to explain or predict a set of real or counterfactual events which have been observed or manipulated <ref type="bibr" target="#b327">[328]</ref>, not only can the learning performance be potentially improved, but also more explainable learned strategies can be derived, which is ultimately important in healthcare domains.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_V6rgxsF">VIII. FUTURE PERSPECTIVES</head><p xml:id="_6Nua5xa"><s xml:id="_BphZCxr">We have discussed a number of major challenges and open issues raised in the current applications of RL techniques in healthcare domains.</s><s xml:id="_dCGX3Pz">Properly addressing these issues are of great importance in facilitating the adoption of any medical procedure or clinical strategy using RL.</s><s xml:id="_akXVgDQ">Looking into the future, there is an urgent need in bringing recent development in both theories and techniques of RL together with the emerging clinical requirements in practice so as to generate novel solutions that are more interpretable, robust, safe, practical and efficient.</s><s xml:id="_pbBHsBy">In this section, we briefly discuss some of the future perspectives that we envision the most critical towards realizing such ambitions.</s><s xml:id="_fcPpzc3">We mainly focus on three theoretical directions: the interpretability of learned strategies, the integration of human or prior domain knowledge, and the capability of learning from small data.</s><s xml:id="_6beruBh">Healthcare under ambient intelligence and real-life applications are advocated as two main practical directions for RL applications in the coming age of intelligent healthcare.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_AeUCUvn">A. Interpretable Strategy Learning</head><p xml:id="_ke739v9"><s xml:id="_X6tZAWg">Perhaps one of the most profound issues with modern machine learning methods, including RL, is the lack of clear interpretability <ref type="bibr" target="#b328">[329]</ref>.</s><s xml:id="_4JEw823">Usually functioning as a black box expressed by, for instance, deep neural networks, models using RL methods receive a set of data as input and directly output a policy which is difficult to interpret.</s><s xml:id="_HPz4GsH">Although impressive success has been made in solving challenging problems such as learning to play Go and Atari games, the lack of interpretability renders the policies unable to reveal the real correlation between features in the data and specific actions, and to impose and verify certain desirable policy properties, such as worst-case guarantees or safety constraints, for further policy debugging and improvement <ref type="bibr" target="#b329">[330]</ref>.</s><s xml:id="_MZ4Jg6r">These limits therefore greatly hinder the successful adoption of RL policies for safety-critical applications such as in medical domains as clinicians are unlikely to try new treatments without rigorous validation for safety, correctness and robustness <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b330">[331]</ref>.</s></p><p xml:id="_AT9yfhS"><s xml:id="_NH6ryqh">Recently, there has been growing interest in attempting to address the problem of interpretability in RL algorithms.</s><s xml:id="_mJzCyp9">There are a variety of ways to realize interpretability of learned policy, by either using small, closed-form formulas to compute index-based policies <ref type="bibr" target="#b331">[332]</ref>, using program synthesis to learn higher-level symbolic interpretable representations of learned policies <ref type="bibr" target="#b332">[333]</ref>, utilizing genetic programming for interpretable policies represented by compact algebraic equations <ref type="bibr" target="#b35">[36]</ref>, or using program verification techniques to verify certain properties of the programs which are represented as decision trees <ref type="bibr" target="#b36">[37]</ref>.</s><s xml:id="_5RJWMsa">Also, there has been growing attention not only on developing interpretable representations, but also on generating explicit explanations for sequential decision making problems <ref type="bibr" target="#b333">[334]</ref>.</s><s xml:id="_khD5bY3">While several works specifically focused on interpretability of deep models in healthcare settings <ref type="bibr" target="#b334">[335]</ref>, <ref type="bibr" target="#b335">[336]</ref>, how to develop interpretable RL solutions in order to increase the robustness, safety and correctness of learned strategies in healthcare domains is still an unsolved issue that calls for further investigations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ENEYwrt">B. Integration of Prior Knowledge</head><p xml:id="_zVgBHMg"><s xml:id="_pDxjH48">There is a wealth of prior knowledge in healthcare domains that can be used for learning performance improvement.</s><s xml:id="_CWaXBM4">The integration of such prior knowledge can be conducted in different manners, either through configuration or presentation of learning parameters, components or models <ref type="bibr" target="#b336">[337]</ref>, <ref type="bibr" target="#b134">[135]</ref>, knowledge transfer from different individual patients, subtypes/sub-populations or clinical domains <ref type="bibr" target="#b146">[147]</ref>, or enabling human-in-the-loop interactive learning <ref type="bibr" target="#b337">[338]</ref>.</s></p><p xml:id="_cKjHVrj"><s xml:id="_PGCCFRs">Gaweda et al. <ref type="bibr" target="#b336">[337]</ref>, <ref type="bibr" target="#b134">[135]</ref> presented an approach to management of anemia that incorporates a critical prior knowledge about the doseresponse characteristic into the learning approach, that is, for all patients, it is known that the dose-response curve of HGB vs. EPO is monotonically nonincreasing.</s><s xml:id="_MQXHBCM">Thus, if a patient's response is evaluated as insufficient for a particular dose at a particular state, then the physician knows that the optimal dose for that state is definitely higher than the administered one.</s><s xml:id="_kF6j3yb">Consequently, there is no need to explore the benefit of lower doses at further stages of treatment.</s><s xml:id="_ZxGyrmC">To capture this feature, the authors introduced an additional mechanism to the original Q-learning algorithm so that the information about monotonically increasing character of the HGB vs. EPO curve can be incorporated in the update procedure.</s><s xml:id="_r6bzJBT">This modification has been shown to make the EPO dosing faster and more efficiently.</s></p><p xml:id="_kNK7N5S"><s xml:id="_pWr9duh">While transfer learning has been extensively studied in the agent learning community <ref type="bibr" target="#b45">[46]</ref>, there is quite limited work on applying TRL techniques in healthcare settings.</s><s xml:id="_gjtsy4G">The learning performance in the target task can be potentially facilitated by using latent variable models, pre-trained model parameters from past tasks, or directly learning a mapping between past and target tasks, thus extending personalized care to groups of patients with similar diagnoses.</s><s xml:id="_4KDH8BW">Marivate et al. <ref type="bibr" target="#b144">[145]</ref> highlighted the potential benefit of taking into account individual variability and data limitations when performing batch policy evaluation for new individuals in HIV treatment.</s><s xml:id="_fj6UgUN">A recent approach on TRL using latent variable models was proposed by Killian et al. <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b146">[147]</ref>, who used a Gaussian Process latent variable model for HIV treatment by both inferring the transition dynamics within a task instance and also in the transfer between task instances.</s></p><p xml:id="_E4J95XA"><s xml:id="_vr9xV2K">Another way of integrating prior knowledge into an RL process can be making use of the human cognitive abilities or domain expertise to guide, shape, evaluate or validate the agent's learning process, making the traditional RL into a human-in-the-loop interactive RL problem <ref type="bibr" target="#b338">[339]</ref>.</s><s xml:id="_xXf5Swm">Human knowledge-driven RL methods can be of great interest to problems in healthcare domains, where traditional learning algorithms would possibly fail due to issues such as insufficient training samples, complex and incomplete data or unexplainable learning process <ref type="bibr" target="#b337">[338]</ref>.</s><s xml:id="_cujzBrY">Consequently, the integration of the humans (i.e., doctors) into the learning process, and the interaction of an expert's knowledge with the automatic learning data would greatly enhance the knowledge discovery process <ref type="bibr" target="#b339">[340]</ref>.</s><s xml:id="_M5p62zq">While there is some previous work from other domains, particularly in training of robots <ref type="bibr" target="#b340">[341]</ref>, <ref type="bibr" target="#b341">[342]</ref>, human-in-theloop interactive RL is not yet well established in the healthcare domain.</s><s xml:id="_fxyjFpZ">It remains open for future research to transfer the insights from existing studies into the healthcare domain to ensure successful applications of existing RL methods.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_E3nrmbq">C. Learning from Small Data</head><p xml:id="_zbFeDeH"><s xml:id="_NKPmHEm">There is no doubt that the most recent progresses of RL, particularly DRL, are highly dependent on the premise of large number of training samples.</s><s xml:id="_4HqJCx6">While this is quite reasonable conceptually, that is, we cannot learn new things that we have not tried sufficiently enough, there still exist many domains lacking sufficient available training samples, specifically, in some healthcare domains <ref type="bibr" target="#b342">[343]</ref>.</s><s xml:id="_sJ9MYJw">For example, in diagnose settings, medical images are much more difficult to be annotated with certain lesions in high-quality without specific expertise compared to general images with simple categories.</s><s xml:id="_RW4Meeu">In addition, there are usually few historical data or cases for new diseases and rare illness, making it impossible to obtain sufficient training samples with accurate labels.</s><s xml:id="_49nuamr">In such circumstances, directly applying existing RL methods on limited data may result in overly optimistic, or in other extreme, pessimistic about treatments that are rarely performed in practice.</s></p><p xml:id="_Rm59e2F"><s xml:id="_gAjVYBr">Broadly, there are two different ways of dealing with a small sample learning problem <ref type="bibr" target="#b343">[344]</ref>.</s><s xml:id="_bYUv8US">The direct solution can be using data augmentation strategies such as deformations <ref type="bibr" target="#b344">[345]</ref> or GANs <ref type="bibr" target="#b345">[346]</ref> to increase samples and then employ conventional learning methods.</s><s xml:id="_mk87Mkq">The other type of solutions can be applying various model modification or domain adaptation methods such as knowledge distillation <ref type="bibr" target="#b346">[347]</ref> or meta-learning <ref type="bibr" target="#b347">[348]</ref> to enable efficient learning that overcomes the problem of data scarcity.</s><s xml:id="_efHCw9p">While still in its early stage, significant progress has been made in small sample learning research in recent years <ref type="bibr" target="#b343">[344]</ref>.</s><s xml:id="_Qbtqju7">How to build on these achievements and tackle the small data RL problems in healthcare domains thus calls for new methods of future investigations.</s><s xml:id="_BgEKN6Y">One initial work is by Tseng et al. <ref type="bibr" target="#b92">[93]</ref> who developed automated radiation adaptation protocols for NSCLC patients by using GAN to generate synthetic patient data and DQN to learn dose decisions with the synthesized data and the available real clinical data.</s><s xml:id="_ej3t3qb">Results showed that the learned dose strategies by DQN were capable of achieving similar results to those chosen by clinicians, yielding feasible and quite promising solutions for automatic treatment designs with limited data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZfgdESm">D. Healthcare under Ambient Intelligence</head><p xml:id="_mTnHceG"><s xml:id="_kPN8vQH">The recent development in sensor networks and wearable devices has facilitated the advent of new era of healthcare systems that are characterized by low-cost mobile sensing and pervasive monitoring within the home and outdoor environments <ref type="bibr" target="#b348">[349]</ref>.</s><s xml:id="_B6RKYRY">The Ambient Intelligence (AmI) technology, which enables innovative human-machine interactions through unobtrusive and anticipatory communications, has the potential to enhance the healthcare domain dramatically by learning from user interaction, reasoning reasoning about users' goals and intensions, and planning activities and future interactions <ref type="bibr" target="#b349">[350]</ref>.</s><s xml:id="_3WGjBa8">By using various kinds of sensing devices, such as smart phones, GPS and body sensors monitoring motions and activities, it is now possible to remotely and continuously collect patients' health information such that proper treatment or intervention decisions can be made anytime and anywhere.</s></p><p xml:id="_b23GWaz"><s xml:id="_kTMEAur">As an instance of online decision making in a possibly infinite horizon setting involving many stages of interventions, RL plays a key role in achieving the future vision of AmI in healthcare systems through continuous interaction with the environment and adaption to the user needs in a transparent and optimal manner.</s><s xml:id="_26EYVPt">In fact, the high level of monitoring and sensing provides ample opportunity for RL methods that can fuse estimates of a given physiologic parameter from multiple sources to provide a single measurement, and derive optimal strategies using these data.</s><s xml:id="_SdcxZHY">Currently, there are several studies that have applied RL to achieve AmI in healthcare domains.</s><s xml:id="_gcG8HNq">For example, RL has been used to adapt the intervention strategies of smart phones in order to recommend regular physical activity to people who suffer from diabetes type 2 <ref type="bibr" target="#b300">[301]</ref>, <ref type="bibr" target="#b301">[302]</ref>, or who have experienced a cardiac event and been in cardiac rehab <ref type="bibr" target="#b350">[351]</ref>, <ref type="bibr" target="#b351">[352]</ref>, <ref type="bibr" target="#b352">[353]</ref>.</s><s xml:id="_87hUBM9">It has been also been used for mobile health intervention for college students who drink heavily and smoke cigarettes <ref type="bibr" target="#b180">[181]</ref>.</s></p><p xml:id="_yGr2Tw3"><s xml:id="_nCtnbd7">Despite the successes, healthcare under AmI poses some unique challenges that preclude direct application of existing RL methodologies for DTRs.</s><s xml:id="_f3wKUVd">For example, it typically involves a large number of time points or infinite time horizon for each individual; the momentary signal may be weak and may not directly measure the outcome of interest; and estimation of optimal treatment strategies must be done online as data accumulate.</s><s xml:id="_Dq75SdA">How to tackle these issues is of great importance in the successful applications of RL methods in the advent of healthcare systems under AmI.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cWDkXXs">E. Future in-vivo Studies</head><p xml:id="_4hsYZeY"><s xml:id="_JydhWy8">To date, the vast volume of research reporting the development of RL techniques in healthcare is built upon certain computational models that leverages mathematical representation of how a patient responds to given treatment policies, or upon retrospective clinical data to directly derive appropriate treatment strategies.</s><s xml:id="_dggZYKX">While this kind of in silico study is essential as a tool for early stage exploration or direct derivation of adaptive treatment strategies by providing approximate or highly simplified models, future in vivo studies of closed-loop RL approaches are urgently required to reliably assess the performance and personalization of the proposed approaches in real-life implementations.</s><s xml:id="_DBYJ79U">However, a number of major issues still remain related to, in particular, data collection and preprocessing in real clinical settings, and high inter-individual differences of the physiological responses, thus calling for careful consideration of safety, efficiency and robustness of RL methods in real-life healthcare applications.</s></p><p xml:id="_WTxqkFd"><s xml:id="_bGUDa8f">First and foremost, safety is of paramount importance in medical settings, thus it is imperative to ensure that the actions during learning be safe enough when dealing with in vivo subjects.</s><s xml:id="_EVA7FEE">In some healthcare domains, the consequences of wrong actions are not merely limited to bad performance, but may include long-term effects that cannot be compensated by more profitable exploitation later on.</s><s xml:id="_uA82wMV">As one wrong action can result in unrecoverable effects, learning in healthcare domains poses a safety exploration dilemma <ref type="bibr" target="#b323">[324]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</s><s xml:id="_pvv8sMc">It is worth noting that there are substantial ongoing efforts in the computer science community to address precisely these problems, namely in developing risk-directed exploration algorithms that can efficiently learn with formal guarantees regarding the safety (or worst-case performance) of the system <ref type="bibr" target="#b32">[33]</ref>.</s><s xml:id="_GfawHZm">With this consideration, the agent's choice of actions is aided by an appropriate risk metric acting as an exploration bonus toward safer regions of the search space.</s><s xml:id="_UHr2mKJ">How to draw on these achievements and develop safe exploration strategies is thus urgently required to implement RL methods in real-life healthcare applications.</s></p><p xml:id="_BwRhvTs"><s xml:id="_Ez94urc">Another issue is regarding the sample efficiency of RL methods in in vivo studies <ref type="bibr" target="#b82">[83]</ref>.</s><s xml:id="_hmtasA7">While it is possible for the RL algorithms to collect large numbers of samples in simulations, it is unrealistic for sample-critical domains where collecting samples would cause significant cost.</s><s xml:id="_Tj8HKvg">This is obviously true when dealing with real patients who would possibly not survive the long-term repeated trail-and-error treatment.</s><s xml:id="_u8EVWKY">Luckily, the wide range of efficient techniques reviewed in Section II-B can provide promising solutions to this problem.</s><s xml:id="_eV7FN3U">Specifically, the sample-level batch learning methods can be applied for more efficient use of past samples, while modelbased methods enable better use of samples by building the model of the environment.</s><s xml:id="_SxUcxRj">Another appealing solution is using task-level transfer methods that can reuse the past treatment or patient information to facilitate learning in new cases, or directly transfer the learned policies in simulations to real environments.</s><s xml:id="_Cc7Rs2p">To enable efficient transfer, RL algorithms can be provided with initial knowledge that can direct the learning in its initial stage toward more profitable and safer regions of the state space, or with demonstrations and teacher advice from an external expert that can interrupt exploration and provide expert knowledge when the agent is confronted with unexpected situations.</s></p><p xml:id="_N8pauSa"><s xml:id="_nUQqQwN">The last issue in the real-life implementation of RL approaches is regarding the robustness of derived solutions.</s><s xml:id="_cZ6VutA">Despite inherently being suitable for optimizing outcomes in stochastic processes with uncertainty, existing RL methods are still facing difficulties in handling incomplete or noisy state variables in partially observable real healthcare environments, and in providing measures of confidence (e.g. standard errors, confidence sets, hypothesis tests).</s><s xml:id="_7XYDaxK">Uncertainty can also be caused by the MDP parameters themselves, which leads to significant increases in the difficulty of the problem, in terms of both computational complexity and data requirements.</s><s xml:id="_4REUscN">While there has been some recent work on robust MDP solutions which accounts for this issue <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, a more general and sound theoretical and empirical evaluation is still lacking.</s><s xml:id="_A5aTYVX">Moreover, most current studies are built upon predefined functions to map states and actions into some integer numbers.</s><s xml:id="_EEmbp4q">It is unclear how changing these numbers would affect the resulting optimal solutions.</s><s xml:id="_FZ53xaM">Understanding the robustness of RL methods in uncertain healthcare settings is the subject of ongoing critical investigations by the statistics, computer science and healthcare communities.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MD72hSz">IX. CONCLUSIONS</head><p xml:id="_bmpggZN"><s xml:id="_HNC7Jh7">RL presents a mathematically solid and technically sound solution to optimal decision making in various healthcare tasks challenged with noisy, multi-dimensional and incomplete data, nonlinear and complex dynamics, and particularly, sequential decision precedures with delayed evaluation feedback.</s><s xml:id="_SDzQKNJ">This paper aims to provide a state-of-the-art comprehensive survey of RL applications to a variety of decision making problems in the area of healthcare.</s><s xml:id="_fUwfV6C">We have provided a structured summarization of the theoretical foundations and key techniques in the RL research from traditional machine learning perspective, and surveyed the broad-ranging applications of RL methods in solving problems affecting manifold areas of healthcare, from DTRs in chronic diseases and critical care, automated clinical diagnosis, to other healthcare domains such as clinical resource allocation and scheduling.</s><s xml:id="_FZ6SPa6">The challenges and open issues in the current research have been discussed in detail from the perspectives of basic components constituting an RL process (i.e., states, actions, rewards, policies and models), and fundamental issues in RL research (i.e., the exploration-exploitation dilemma and credit assignment).</s><s xml:id="_NbrZp7q">It should be emphasized that, although each of these challenging issues has been investigated extensively in the RL community for a long time, achieving remarkably successful solutions, it might be problematic to directly apply these solutions in the healthcare settings due to the inherent complexity in processes of medical data processing and policy learning.</s><s xml:id="_5kbSd7t">In fact, the unique features embodied in the clinical or medical decision making process urgently call for development of more advanced RL methods that are really suitable for real-life healthcare problems.</s><s xml:id="_4RdhPb5">Apart from the enumerated challenges, we have also pointed out several perspectives that remain comparatively less addressed by the current literature.</s><s xml:id="_zVhNSWp">Interpretable learning, transfer learning as well as small-data learning are the three theoretical directions that require more effort in order to make substantial progress.</s><s xml:id="_fbXemVS">Moreover, how to tailor the existing RL methods to deal with the pervasive data in the new era of AmI healthcare systems and take into consideration safety, robustness and efficiency caused by real-life applications are two main paradigms that need to be carefully handled in practice.</s></p><p xml:id="_eRB4QAT"><s xml:id="_eUhM8wE">The application of RL in healthcare is at the intersection of computer science and medicine.</s><s xml:id="_8X4EB2c">Such cross-disciplinary research requires a concerted effort from machine learning researchers and clinicians who are directly involved in patient care and medical decision makings.</s><s xml:id="_mxb5tms">While notable success has been obtained, RL has still received far less attention by researchers, either from computer science or from medicine, compared to other research paradigms in healthcare domains, such as traditional machine learning, deep learning, statistical learning and control-driven methods.</s><s xml:id="_KDJ5HyN">Driven by both substantial progress in theories and techniques in the RL research, as well as practical demands from healthcare practitioners and managers, this situation is now changing rapidly and recent years have witnessed a surge of interest in the paradigm of applying RL in healthcare, which can be supported by the dramatic increase in the number of publications on this topic in the past few years.</s><s xml:id="_RZmyX7p">Serving as the first comprehensive survey of RL applications in healthcare, this paper aims at providing the research community with systematic understanding of foundations, broad palette of methods and techniques available, existing challenges, and new insights of this emerging paradigm.</s><s xml:id="_Wnes3Uh">By this, we hope that more researchers from various disciplines can utilize their expertise in their own area and work collaboratively to generate more applicable solutions to optimal decision makings in healthcare.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc><div><p xml:id="_7KsaWM6"><s xml:id="_GDUa4fP">Fig. 1.</s><s xml:id="_PB5kH32">The summarization of theoretical foundations, basic solutions, challenging issues and advanced techniques in RL.</s></p></div></figDesc><graphic coords="3,100.37,53.14,411.26,290.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc><div><p xml:id="_XYX6NTJ"><s xml:id="_DRWp3C7">Fig. 2. The outline of application domains of RL in healthcare.</s></p></div></figDesc><graphic coords="7,56.25,53.14,236.48,330.70" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_maRq8cz">ACKNOWLEDGMENT</head><p xml:id="_kb36KgH"><s xml:id="_zeMzJUT">This work is supported by the <rs type="programName">Hongkong Scholar Program</rs> under Grant <rs type="grantNumber">XJ2017028</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hsuvQC7">
					<idno type="grant-number">XJ2017028</idno>
					<orgName type="program" subtype="full">Hongkong Scholar Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_8zSevDM">The coming of age of artificial intelligence in medicine</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Shortliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stefanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bellazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abu-Hanna</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2008.07.017</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VPEPraf">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. L. Patel, E. H. Shortliffe, M. Stefanelli, P. Szolovits, M. R. Berthold, R. Bellazzi, and A. Abu-Hanna, &quot;The coming of age of artificial intelligence in medicine,&quot; Artificial Intelligence in Medicine, vol. 46, no. 1, pp. 5-17, 2009.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_5E2QfRV">Artificial intelligence in medicine and cardiac imaging: harnessing big data and advanced computing to provide personalized medical diagnosis and treatment</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Dilsizian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BVSdJaG">Current Cardiology Reports</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">441</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. E. Dilsizian and E. L. Siegel, &quot;Artificial intelligence in medicine and cardiac imaging: harnessing big data and advanced computing to provide personalized medical diagnosis and treatment,&quot; Current Cardiology Reports, vol. 16, no. 1, p. 441, 2014.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_DrsANFZ">Artificial intelligence in healthcare: past, present and future</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kddp6qw">Stroke and Vascular Neurology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="230" to="243" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Jiang, Y. Jiang, H. Zhi, Y. Dong, H. Li, S. Ma, Y. Wang, Q. Dong, H. Shen, and Y. Wang, &quot;Artificial intelligence in healthcare: past, present and future,&quot; Stroke and Vascular Neurology, vol. 2, no. 4, pp. 230-243, 2017.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_bbFcPgA">The practical implementation of artificial intelligence technologies in medicine</title>
		<author>
			<persName><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cMNaFzB">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">30</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. He, S. L. Baxter, J. Xu, J. Xu, X. Zhou, and K. Zhang, &quot;The practical implementation of artificial intelligence technologies in medicine,&quot; Nature Medicine, vol. 25, no. 1, p. 30, 2019.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_pYsxN6c">Machine learning and decision support in critical care</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Niehaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_D3xQQnE">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="444" to="466" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. E. Johnson, M. M. Ghassemi, S. Nemati, K. E. Niehaus, D. A. Clifton, and G. D. Clifford, &quot;Machine learning and decision support in critical care,&quot; Proceedings of the IEEE, vol. 104, no. 2, pp. 444-466, 2016.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_eEV7VYY">Deep learning for health informatics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ravì</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deligianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Andreu-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/jbhi.2016.2636665</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XUrqa9a">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ravì, C. Wong, F. Deligianni, M. Berthelot, J. Andreu-Perez, B. Lo, and G.-Z. Yang, &quot;Deep learning for health informatics,&quot; IEEE Journal of Biomedical and Health Informatics, vol. 21, no. 1, pp. 4-21, 2017.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_7UrzeBT">Opportunities and obstacles for deep learning in biology and medicine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Himmelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Beaulieu-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ferrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Agapow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_G2kmBBF">bioRxiv</title>
		<imprint>
			<biblScope unit="page">142760</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Ching, D. S. Himmelstein, B. K. Beaulieu-Jones, A. A. Kalinin, B. T. Do, G. P. Way, E. Ferrero, P.-M. Agapow, M. Zietz, M. M. Hoffman et al., &quot;Opportunities and obstacles for deep learning in biology and medicine,&quot; bioRxiv, p. 142760, 2018.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_uyBW4Kk">Big data application in biomedical research and health care: a literature review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gopukumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.4137/bii.s31559</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pb3xj9F">Biomedical Informatics Insights</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="S31" to="559" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Luo, M. Wu, D. Gopukumar, and Y. Zhao, &quot;Big data application in biomedical research and health care: a literature review,&quot; Biomedical Informatics Insights, vol. 8, pp. BII-S31 559, 2016.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_DVtrgWu">A guide to deep learning in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HzBmkHk">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Esteva, A. Robicquet, B. Ramsundar, V. Kuleshov, M. DePristo, K. Chou, C. Cui, G. Corrado, S. Thrun, and J. Dean, &quot;A guide to deep learning in healthcare,&quot; Nature Medicine, vol. 25, no. 1, p. 24, 2019.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_B9xjjfG">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PRnNHr2">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., &quot;Human-level control through deep reinforcement learning,&quot; Nature, vol. 518, no. 7540, p. 529, 2015.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_NzXd25k">Reinforcement learning improves behaviour from evaluative feedback</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_S4g4b4e">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">445</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. L. Littman, &quot;Reinforcement learning improves behaviour from evaluative feedback,&quot; Nature, vol. 521, no. 7553, p. 445, 2015.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main" xml:id="_qSdWGHM">Deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06339</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Y. Li, &quot;Deep reinforcement learning,&quot; arXiv preprint arXiv:1810.06339, 2018.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_NJ9aw7Y">Applications of deep learning and reinforcement learning to biological data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassanelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/tnnls.2018.2790388</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2xwkSAK">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2063" to="2079" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Mahmud, M. S. Kaiser, A. Hussain, and S. Vassanelli, &quot;Applications of deep learning and reinforcement learning to biological data,&quot; IEEE transactions on neural networks and learning systems, vol. 29, no. 6, pp. 2063-2079, 2018.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_xKP242P">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_WRnq9wF">Reinforcement learning for control: Performance, stability, and deep approximators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bus ¸oniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Bruin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tolić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Palunko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rjTDf78">Annual Reviews in Control</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Bus ¸oniu, T. de Bruin, D. Tolić, J. Kober, and I. Palunko, &quot;Re- inforcement learning for control: Performance, stability, and deep approximators,&quot; Annual Reviews in Control, 2018.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_468skR3">Guidelines for reinforcement learning in healthcare</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TDaYXUK">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">O. Gottesman, F. Johansson, M. Komorowski, A. Faisal, D. Sontag, F. Doshi-Velez, and L. A. Celi, &quot;Guidelines for reinforcement learning in healthcare.&quot; Nature medicine, vol. 25, no. 1, p. 16, 2019.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_ZKVecSD">Dynamic programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<idno type="DOI">10.2307/j.ctv1nxcw0f</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Bellman, Dynamic programming. Courier Corporation, 2013.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_HyvuySY">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1023/a:1022676722315</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yZeNfxV">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. J. Watkins and P. Dayan, &quot;Q-learning,&quot; Machine Learning, vol. 8, no. 3-4, pp. 279-292, 1992.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main" xml:id="_nZK4g4d">On-line Q-learning using connectionist systems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<idno type="DOI">10.59972/yzxn5vgx</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">37</biblScope>
			<pubPlace>England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge, Department of Engineering Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">G. A. Rummery and M. Niranjan, On-line Q-learning using connec- tionist systems. University of Cambridge, Department of Engineering Cambridge, England, 1994, vol. 37.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_wSTZVjM">Policy search for motor primitives in robotics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GnC5umA">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Kober and J. R. Peters, &quot;Policy search for motor primitives in robotics,&quot; in Advances in Neural Information Processing Systems, 2009, pp. 849-856.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_YP2Qvd4">Natural actor-critic</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fRHEbuV">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="1180" to="1190" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Peters and S. Schaal, &quot;Natural actor-critic,&quot; Neurocomputing, vol. 71, no. 7-9, pp. 1180-1190, 2008.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_NZzGXT2">Bayesian reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Poupart</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-27645-3_11</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GcV25zs">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="359" to="386" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Vlassis, M. Ghavamzadeh, S. Mannor, and P. Poupart, &quot;Bayesian reinforcement learning,&quot; in Reinforcement Learning. Springer, 2012, pp. 359-386.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_k2zQS6V">Bayesian reinforcement learning: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kxUuuss">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="359" to="483" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Ghavamzadeh, S. Mannor, J. Pineau, A. Tamar et al., &quot;Bayesian reinforcement learning: A survey,&quot; Foundations and Trends R in Ma- chine Learning, vol. 8, no. 5-6, pp. 359-483, 2015.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_69bFHRs">Reinforcement learning in finite mdps: Pac analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<idno type="DOI">10.1145/1143844.1143955</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_t45yxG5">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2413" to="2444" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. L. Strehl, L. Li, and M. L. Littman, &quot;Reinforcement learning in finite mdps: Pac analysis,&quot; Journal of Machine Learning Research, vol. 10, no. Nov, pp. 2413-2444, 2009.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_XHYyQEV">R-max-a general polynomial time algorithm for near-optimal reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vWUnJDv">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="213" to="231" />
			<date type="published" when="2002-10">Oct. 2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. I. Brafman and M. Tennenholtz, &quot;R-max-a general polynomial time algorithm for near-optimal reinforcement learning,&quot; Journal of Machine Learning Research, vol. 3, no. Oct, pp. 213-231, 2002.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_hgJ2ndJ">Intrinsic motivation and reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kFrFtUV">Intrinsically motivated learning in natural and artificial systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="17" to="47" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. G. Barto, &quot;Intrinsic motivation and reinforcement learning,&quot; in Intrinsically motivated learning in natural and artificial systems. Springer, 2013, pp. 17-47.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_xJVYwjj">A comprehensive survey of multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
		<idno type="DOI">10.1109/tsmcc.2007.913919</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NpQJdwQ">IEEE Transactions on Systems, Man, And Cybernetics-Part C: Applications and Reviews</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Busoniu, R. Babuska, and B. De Schutter, &quot;A comprehensive survey of multiagent reinforcement learning,&quot; IEEE Transactions on Systems, Man, And Cybernetics-Part C: Applications and Reviews, 38 (2), 2008, 2008.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_bs6cADR">On the sample complexity of reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>London London, England</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note type="raw_reference">S. M. Kakade et al., &quot;On the sample complexity of reinforcement learning,&quot; Ph.D. dissertation, University of London London, England, 2003.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_mapbpPx">Sample complexity bounds of exploration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-27645-3_6</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_dgTVFWk">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="175" to="204" />
		</imprint>
	</monogr>
	<note type="raw_reference">L. Li, &quot;Sample complexity bounds of exploration,&quot; in Reinforcement Learning. Springer, 2012, pp. 175-204.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main" xml:id="_PE2UFP6">Reinforcement learning and dynamic programming using function approximators</title>
		<author>
			<persName><forename type="first">L</forename><surname>Busoniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">L. Busoniu, R. Babuska, B. De Schutter, and D. Ernst, Reinforcement learning and dynamic programming using function approximators. CRC press, 2010.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_VXbgEnb">Reinforcement learning in continuous state and action spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_n4HzUA5">Reinforcement learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="207" to="251" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Van Hasselt, &quot;Reinforcement learning in continuous state and action spaces,&quot; in Reinforcement learning. Springer, 2012, pp. 207-251.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_HHjfQsk">Safe exploration in markov decision processes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_h5hCD3c">Proceedings of the 29th International Coference on International Conference on Machine Learning</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1451" to="1458" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. M. Moldovan and P. Abbeel, &quot;Safe exploration in markov decision processes,&quot; in Proceedings of the 29th International Coference on International Conference on Machine Learning. Omnipress, 2012, pp. 1451-1458.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_9EHtxek">A comprehensive survey on safe reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Garcıa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kk7stfG">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1437" to="1480" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Garcıa and F. Fernández, &quot;A comprehensive survey on safe rein- forcement learning,&quot; Journal of Machine Learning Research, vol. 16, no. 1, pp. 1437-1480, 2015.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_DqGSE5H">Robust markov decision processes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rustem</surname></persName>
		</author>
		<idno type="DOI">10.1287/moor.1120.0566</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zpHUVU8">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="183" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Wiesemann, D. Kuhn, and B. Rustem, &quot;Robust markov decision processes,&quot; Mathematics of Operations Research, vol. 38, no. 1, pp. 153-183, 2013.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main" xml:id="_Wrs24sp">Distributionally robust markov decision processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7503.003.0197</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ePUrWwV">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2505" to="2513" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Xu and S. Mannor, &quot;Distributionally robust markov decision pro- cesses,&quot; in Advances in Neural Information Processing Systems, 2010, pp. 2505-2513.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main" xml:id="_kQZbzE8">Interpretable policies for reinforcement learning by genetic programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Udluft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Runkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TX8s3dD">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="158" to="169" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Hein, S. Udluft, and T. A. Runkler, &quot;Interpretable policies for rein- forcement learning by genetic programming,&quot; Engineering Applications of Artificial Intelligence, vol. 76, pp. 158-169, 2018.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main" xml:id="_enmjTGw">Verifiable reinforcement learning via policy extraction</title>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-04083-2_11</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Ye7gvG5">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2499" to="2509" />
		</imprint>
	</monogr>
	<note type="raw_reference">O. Bastani, Y. Pu, and A. Solar-Lezama, &quot;Verifiable reinforcement learning via policy extraction,&quot; in Advances in Neural Information Processing Systems, 2018, pp. 2499-2509.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_qfEVDzD">Reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Otterlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sXEMuJs">Adaptation, learning, and optimization</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Wiering and M. Van Otterlo, &quot;Reinforcement learning,&quot; Adaptation, learning, and optimization, vol. 12, 2012.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main" xml:id="_NuuaBz7">Batch reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-27645-3_2</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GAnjnkU">Reinforcement learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="45" to="73" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Lange, T. Gabel, and M. Riedmiller, &quot;Batch reinforcement learning,&quot; in Reinforcement learning. Springer, 2012, pp. 45-73.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_R553N8J">Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uYhJMtu">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Riedmiller, &quot;Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method,&quot; in European Confer- ence on Machine Learning. Springer, 2005, pp. 317-328.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_5j4cvkC">Tree-based batch mode reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Geurts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PUQeEYc">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="503" to="556" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ernst, P. Geurts, and L. Wehenkel, &quot;Tree-based batch mode rein- forcement learning,&quot; Journal of Machine Learning Research, vol. 6, no. Apr, pp. 503-556, 2005.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_x9ansAE">Least-squares policy iteration</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Lagoudakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-30164-8_468</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pZUuQsh">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1107" to="1149" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. G. Lagoudakis and R. Parr, &quot;Least-squares policy iteration,&quot; Journal of Machine Learning Research, vol. 4, no. Dec, pp. 1107-1149, 2003.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_FqnsrBu">Learning and using models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-27645-3_4</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bUJPTmv">Reinforcement learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="111" to="141" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Hester and P. Stone, &quot;Learning and using models,&quot; in Reinforcement learning. Springer, 2012, pp. 111-141.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main" xml:id="_4eDfaUF">A survey of monte carlo tree search methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">I</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XZJV9BM">IEEE Transactions on Computational Intelligence and AI in games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton, &quot;A survey of monte carlo tree search methods,&quot; IEEE Transactions on Computational Intelligence and AI in games, vol. 4, no. 1, pp. 1-43, 2012.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_9YjxKrF">Transfer in reinforcement learning: a framework and a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazaric</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-27645-3_5</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8NgcJFj">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="143" to="173" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Lazaric, &quot;Transfer in reinforcement learning: a framework and a survey,&quot; in Reinforcement Learning. Springer, 2012, pp. 143-173.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_gP7UdWz">Transfer learning for reinforcement learning domains: A survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-01882-4_7</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3QhDbwy">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1633" to="1685" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. E. Taylor and P. Stone, &quot;Transfer learning for reinforcement learning domains: A survey,&quot; Journal of Machine Learning Research, vol. 10, no. Jul, pp. 1633-1685, 2009.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_NAZnc9S">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xVe4u83">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., &quot;Mastering the game of go with deep neural networks and tree search,&quot; Nature, vol. 529, no. 7587, p. 484, 2016.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main" xml:id="_srsvz4R">A survey of deep neural network architectures and their applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Alsaadi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2016.12.038</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2XhgSMh">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">234</biblScope>
			<biblScope unit="page" from="11" to="26" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi, &quot;A survey of deep neural network architectures and their applications,&quot; Neurocomputing, vol. 234, pp. 11-26, 2017.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_wX8zmmE">Efficient processing of deep neural networks: A tutorial and survey</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<idno type="DOI">10.1109/jproc.2017.2761740</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zaaDsKj">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
		</imprint>
	</monogr>
	<note type="raw_reference">V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, &quot;Efficient processing of deep neural networks: A tutorial and survey,&quot; Proceedings of the IEEE, vol. 105, no. 12, pp. 2295-2329, 2017.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_Qf7QTaz">Multiobjective reinforcement learning: A comprehensive overview</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PGtsxcv">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="398" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Liu, X. Xu, and D. Hu, &quot;Multiobjective reinforcement learning: A comprehensive overview,&quot; IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 45, no. 3, pp. 385-398, 2015.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_CHHaxqE">A survey of preference-based reinforcement learning methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Akrour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yps79EF">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4945" to="4990" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Wirth, R. Akrour, G. Neumann, and J. Fürnkranz, &quot;A survey of preference-based reinforcement learning methods,&quot; The Journal of Machine Learning Research, vol. 18, no. 1, pp. 4945-4990, 2017.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_KWn6GYq">Preferencebased reinforcement learning: a formal framework and a policy iteration algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-012-5313-8</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7gWyw3p">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="123" to="156" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Fürnkranz, E. Hüllermeier, W. Cheng, and S.-H. Park, &quot;Preference- based reinforcement learning: a formal framework and a policy iteration algorithm,&quot; Machine Learning, vol. 89, no. 1-2, pp. 123-156, 2012.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_RGXmCbd">Algorithms for inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5XWGXN3">ICML</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="663" to="670" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Y. Ng, S. J. Russell et al., &quot;Algorithms for inverse reinforcement learning.&quot; in ICML, 2000, pp. 663-670.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_RTBxwqR">A survey of inverse reinforcement learning techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhifei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Meng</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sfXCuYr">International Journal of Intelligent Computing and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="311" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Zhifei and E. Meng Joo, &quot;A survey of inverse reinforcement learning techniques,&quot; International Journal of Intelligent Computing and Cybernetics, vol. 5, no. 3, pp. 293-311, 2012.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_kpTV2D2">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
		<idno type="DOI">10.1145/1409635.1409678</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zz8gm2N">AAAI</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, &quot;Maximum entropy inverse reinforcement learning.&quot; in AAAI, vol. 8. Chicago, IL, USA, 2008, pp. 1433-1438.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_eZRSr3G">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6e7xa3V">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Abbeel and A. Y. Ng, &quot;Apprenticeship learning via inverse rein- forcement learning,&quot; in Proceedings of the twenty-first international conference on Machine learning. ACM, 2004, p. 1.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_7aTJM6t">Nonlinear inverse reinforcement learning with gaussian processes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_JTPEV93">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Levine, Z. Popovic, and V. Koltun, &quot;Nonlinear inverse reinforcement learning with gaussian processes,&quot; in Advances in Neural Information Processing Systems, 2011, pp. 19-27.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_haK7PRf">Bayesian inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Amir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6PXvgAU">Urbana</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">61801</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ramachandran and E. Amir, &quot;Bayesian inverse reinforcement learn- ing,&quot; Urbana, vol. 51, no. 61801, pp. 1-4, 2007.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_ThvbCN7">Efficient solution algorithms for factored mdps</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.1000</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Qp6d9j4">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="399" to="468" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Guestrin, D. Koller, R. Parr, and S. Venkataraman, &quot;Efficient solution algorithms for factored mdps,&quot; Journal of Artificial Intelligence Research, vol. 19, pp. 399-468, 2003.</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_YUfg6cZ">Efficient reinforcement learning in factored mdps</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CUhgdf7">IJCAI</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="740" to="747" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Kearns and D. Koller, &quot;Efficient reinforcement learning in factored mdps,&quot; in IJCAI, vol. 16, 1999, pp. 740-747.</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_ahyHAKr">Algorithm-directed exploration for model-based reinforcement learning in factored mdps</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Patrascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mKFVMZg">ICML</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. Guestrin, R. Patrascu, and D. Schuurmans, &quot;Algorithm-directed exploration for model-based reinforcement learning in factored mdps,&quot; in ICML, 2002, pp. 235-242.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_R6uxnXs">Near-optimal reinforcement learning in factored mdps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_m5B8Z5a">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="604" to="612" />
		</imprint>
	</monogr>
	<note type="raw_reference">I. Osband and B. Van Roy, &quot;Near-optimal reinforcement learning in factored mdps,&quot; in Advances in Neural Information Processing Systems, 2014, pp. 604-612.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_Vbq5mc7">Efficient structure learning in factored-state mdps</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Diuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WKsbHP6">AAAI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="645" to="650" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. L. Strehl, C. Diuk, and M. L. Littman, &quot;Efficient structure learning in factored-state mdps,&quot; in AAAI, vol. 7, 2007, pp. 645-650.</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_n3DjKDs">Recent advances in hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fdKyWcs">Discrete Event Dynamic Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="41" to="77" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. G. Barto and S. Mahadevan, &quot;Recent advances in hierarchical reinforcement learning,&quot; Discrete Event Dynamic Systems, vol. 13, no. 1-2, pp. 41-77, 2003.</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_72fQAE8">Hierarchical approaches</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hengst</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-27645-3_9</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_y4xdNDu">Reinforcement learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="293" to="323" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. Hengst, &quot;Hierarchical approaches,&quot; in Reinforcement learning. Springer, 2012, pp. 293-323.</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_Vu4Ngwd">Solving relational and first-order logical markov decision processes: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Otterlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KEywrBV">Reinforcement Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="253" to="292" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. van Otterlo, &quot;Solving relational and first-order logical markov decision processes: A survey,&quot; in Reinforcement Learning. Springer, 2012, pp. 253-292.</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_NCNhepX">Reinforcement learning algorithm for partially observable markov decision problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MQChgKU">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Jaakkola, S. P. Singh, and M. I. Jordan, &quot;Reinforcement learning algorithm for partially observable markov decision problems,&quot; in Ad- vances in Neural Information Processing Systems, 1995, pp. 345-352.</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_JG6NzGw">A computer program for digitalis dosage regimens</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Jelliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rockwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ycqXaXe">Mathematical Biosciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="179" to="193" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. W. Jelliffe, J. Buell, R. Kalaba, R. Sridhar, and R. Rockwell, &quot;A computer program for digitalis dosage regimens,&quot; Mathematical Biosciences, vol. 9, pp. 179-193, 1970.</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<title level="m" xml:id="_dnzwVVM">Mathematical methods in medicine</title>
		<imprint>
			<publisher>World Scientific Publishing Co., Inc</publisher>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. E. Bellman, Mathematical methods in medicine. World Scientific Publishing Co., Inc., 1983.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_hdGetKG">Comparison of some control strategies for three-compartment pk/pd models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lovejoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Shafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GbjynWF">Journal of Pharmacokinetics and Biopharmaceutics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="525" to="550" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Hu, W. S. Lovejoy, and S. L. Shafer, &quot;Comparison of some control strategies for three-compartment pk/pd models,&quot; Journal of Pharmacokinetics and Biopharmaceutics, vol. 22, no. 6, pp. 525-550, 1994.</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main" xml:id="_a9zBJ8g">Modeling medical treatment using markov decision processes</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shechter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SeWbZgz">Operations Research and Health Care</title>
		<imprint>
			<biblScope unit="page" from="593" to="612" />
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. J. Schaefer, M. D. Bailey, S. M. Shechter, and M. S. Roberts, &quot;Modeling medical treatment using markov decision processes,&quot; in Operations Research and Health Care. Springer, 2005, pp. 593-612.</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main" xml:id="_DktMaCP">Dynamic treatment regimes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vcNYmUq">Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="447" to="464" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Chakraborty and S. A. Murphy, &quot;Dynamic treatment regimes,&quot; Annual Review of Statistics and Its Application, vol. 1, pp. 447-464, 2014.</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_ru7YMn2">Dynamic treatment regimes: Technical challenges and applications</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Pelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kHQmsyq">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1225</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. B. Laber, D. J. Lizotte, M. Qian, W. E. Pelham, and S. A. Murphy, &quot;Dynamic treatment regimes: Technical challenges and applications,&quot; Electronic Journal of Statistics, vol. 8, no. 1, p. 1225, 2014.</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_QU2yES2">Estimation of survival distributions of treatment policies in two-stage randomization designs in clinical trials</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lunceford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davidian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Tsiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_h7SexRA">Biometrics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="57" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. K. Lunceford, M. Davidian, and A. A. Tsiatis, &quot;Estimation of survival distributions of treatment policies in two-stage randomization designs in clinical trials,&quot; Biometrics, vol. 58, no. 1, pp. 48-57, 2002.</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main" xml:id="_xyPyFrj">Adaptive interventions in child and adolescent mental health</title>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chronis-Tuscano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_S5CYzSw">Journal of Clinical Child &amp; Adolescent Psychology</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="383" to="395" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Almirall and A. Chronis-Tuscano, &quot;Adaptive interventions in child and adolescent mental health,&quot; Journal of Clinical Child &amp; Adolescent Psychology, vol. 45, no. 4, pp. 383-395, 2016.</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main" xml:id="_sMxAHdE">Adaptive treatment strategies in chronic disease</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Lavori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_D68aPTM">Annu. Rev. Med</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="443" to="453" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. W. Lavori and R. Dawson, &quot;Adaptive treatment strategies in chronic disease,&quot; Annu. Rev. Med., vol. 59, pp. 443-453, 2008.</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E M</forename><surname>Moodie</surname></persName>
		</author>
		<title level="m" xml:id="_4DsG7Fb">Statistical Reinforcement Learning</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Chakraborty and E. E. M. Moodie, Statistical Reinforcement Learn- ing. Springer New York, 2013.</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_Z6QpV4e">An experimental design for the development of adaptive treatment strategies</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_K7Hf8YS">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1455" to="1481" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Murphy, &quot;An experimental design for the development of adaptive treatment strategies,&quot; Statistics in Medicine, vol. 24, no. 10, pp. 1455- 1481, 2005.</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_We4Bp7T">Developing adaptive treatment strategies in substance abuse research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tenhave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pqH72pM">Drug &amp; Alcohol Dependence</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="24" to="S30" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Murphy, K. G. Lynch, D. Oslin, J. R. McKay, and T. TenHave, &quot;Developing adaptive treatment strategies in substance abuse research,&quot; Drug &amp; Alcohol Dependence, vol. 88, pp. S24-S30, 2007.</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main" xml:id="_Uc57t4B">Preventing chronic diseases: a vital investment</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Organization</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>World Health Organization</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">W. H. Organization, Preventing chronic diseases: a vital investment. World Health Organization, 2005.</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moodie</surname></persName>
		</author>
		<title level="m" xml:id="_8bmezzE">Statistical methods for dynamic treatment regimes</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Chakraborty and E. Moodie, Statistical methods for dynamic treat- ment regimes. Springer, 2013.</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main" xml:id="_2GTNdcP">Improving chronic illness care: translating evidence into action</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hindmarsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ApDnZdS">Health Affairs</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="64" to="78" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. H. Wagner, B. T. Austin, C. Davis, M. Hindmarsh, J. Schaefer, and A. Bonomi, &quot;Improving chronic illness care: translating evidence into action,&quot; Health Affairs, vol. 20, no. 6, pp. 64-78, 2001.</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_UfTUjE8">Reinforcement learning design for cancer clinical trials</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eheZTe5">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="3294" to="3315" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Zhao, M. R. Kosorok, and D. Zeng, &quot;Reinforcement learning design for cancer clinical trials,&quot; Statistics in Medicine, vol. 28, no. 26, pp. 3294-3315, 2009.</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_QStqESf">Reinforcement learning based control of tumor growth with chemotherapy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hassani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ryJpDwG">2010 International Conference on System Science and Engineering (ICSSE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="185" to="189" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Hassani et al., &quot;Reinforcement learning based control of tumor growth with chemotherapy,&quot; in 2010 International Conference on System Science and Engineering (ICSSE). IEEE, 2010, pp. 185-189.</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main" xml:id="_5rbwvAm">Drug scheduling of cancer chemotherapy based on natural actor-critic approach</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.biosystems.2011.07.005</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x89nn5k">BioSystems</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Ahn and J. Park, &quot;Drug scheduling of cancer chemotherapy based on natural actor-critic approach,&quot; BioSystems, vol. 106, no. 2-3, pp. 121-129, 2011.</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main" xml:id="_ybEjUwh">Using reinforcement learning to personalize dosing strategies in a simulated cancer trial with high dimensional data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Humphrey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Humphrey, &quot;Using reinforcement learning to personalize dosing strategies in a simulated cancer trial with high dimensional data,&quot; 2017.</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main" xml:id="_A4X3UQx">Reinforcement learning-based control of drug dosing for cancer chemotherapy treatment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Haddad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_z6e4CTK">Mathematical biosciences</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="11" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Padmanabhan, N. Meskin, and W. M. Haddad, &quot;Reinforcement learning-based control of drug dosing for cancer chemotherapy treat- ment,&quot; Mathematical biosciences, vol. 293, pp. 11-20, 2017.</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main" xml:id="_6P4tu6k">Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Socinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FQTHBYZ">Biometrics</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1422" to="1433" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Zhao, D. Zeng, M. A. Socinski, and M. R. Kosorok, &quot;Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer,&quot; Biometrics, vol. 67, no. 4, pp. 1422-1433, 2011.</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main" xml:id="_EJQaJV4">Preferencebased policy iteration: Leveraging preference learning for reinforcement learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-23780-5_30</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fymGSxh">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="312" to="327" />
		</imprint>
	</monogr>
	<note type="raw_reference">W. Cheng, J. Fürnkranz, E. Hüllermeier, and S.-H. Park, &quot;Preference- based policy iteration: Leveraging preference learning for reinforce- ment learning,&quot; in Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2011, pp. 312- 327.</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main" xml:id="_8WeWf83">April: Active preference learning-based reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Akrour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schoenauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-33486-3_8</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_k2pkGyB">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Akrour, M. Schoenauer, and M. Sebag, &quot;April: Active preference learning-based reinforcement learning,&quot; in Joint European Confer- ence on Machine Learning and Knowledge Discovery in Databases. Springer, 2012, pp. 116-131.</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main" xml:id="_g4bWtC3">Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm</title>
		<author>
			<persName><forename type="first">R</forename><surname>Busa-Fekete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Szörényi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mgAWVGs">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="327" to="351" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Busa-Fekete, B. Szörényi, P. Weng, W. Cheng, and E. Hüllermeier, &quot;Preference-based reinforcement learning: evolutionary direct policy search using a preference-based racing algorithm,&quot; Machine Learning, vol. 97, no. 3, pp. 327-351, 2014.</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main" xml:id="_CB8s7Yy">Reinforcement learning in models of adaptive medical treatment strategies</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>McGill University Libraries</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note type="raw_reference">R. Vincent, &quot;Reinforcement learning in models of adaptive medical treatment strategies,&quot; Ph.D. dissertation, McGill University Libraries, 2014.</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main" xml:id="_q7xJ3J6">Deep reinforcement learning for automated radiation adaptation in lung cancer</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ten Haken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename><surname>Naqa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KRQkrEM">Medical Physics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6690" to="6705" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. H. Tseng, Y. Luo, S. Cui, J. T. Chien, R. K. Ten Haken, and I. E. Naqa, &quot;Deep reinforcement learning for automated radiation adaptation in lung cancer,&quot; Medical Physics, vol. 44, no. 12, pp. 6690-6705, 2017.</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main" xml:id="_55YPw6F">Simulation-based optimization of radiotherapy: Agent-based modeling and reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalalimanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soltani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_msdsYJh">Mathematics and Computers in Simulation</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="235" to="248" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Jalalimanesh, H. S. Haghighi, A. Ahmadi, and M. Soltani, &quot;Simulation-based optimization of radiotherapy: Agent-based modeling and reinforcement learning,&quot; Mathematics and Computers in Simula- tion, vol. 133, pp. 235-248, 2017.</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main" xml:id="_N7vCf6d">Multi-objective optimization of radiotherapy: distributed q-learning and agent-based simulation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jalalimanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hejazian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soltani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HS3TWzr">Journal of Experimental &amp; Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Jalalimanesh, H. S. Haghighi, A. Ahmadi, H. Hejazian, and M. Soltani, &quot;Multi-objective optimization of radiotherapy: distributed q-learning and agent-based simulation,&quot; Journal of Experimental &amp; Theoretical Artificial Intelligence, pp. 1-16, 2017.</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main" xml:id="_SxXrDwm">Q-learning with censored data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_vnUReK7">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Goldberg and M. R. Kosorok, &quot;Q-learning with censored data,&quot; Annals of Statistics, vol. 40, no. 1, p. 529, 2012.</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main" xml:id="_APPeXyy">Personalized medical treatments using novel reinforcement learning algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Soliman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.3922</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Y. M. Soliman, &quot;Personalized medical treatments using novel reinforce- ment learning algorithms,&quot; arXiv preprint arXiv:1406.3922, 2014.</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main" xml:id="_6SarGQW">Reinforcement learning with action-derived rewards for chemotherapy and clinical trial dosing regimen selection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yauney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5pSPdkJ">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="161" to="226" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Yauney and P. Shah, &quot;Reinforcement learning with action-derived rewards for chemotherapy and clinical trial dosing regimen selection,&quot; in Machine Learning for Healthcare Conference, 2018, pp. 161-226.</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main" xml:id="_aQTAq23">World cancer report 2014</title>
		<author>
			<persName><forename type="first">B</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rv8uhwE">Health</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Stewart, C. P. Wild et al., &quot;World cancer report 2014,&quot; Health, 2017.</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main" xml:id="_pWB7TZF">Interactions between the immune system and cancer: a brief review of non-spatial mathematical models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eftimie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Bramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Earn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tcNYkMX">Bulletin of Mathematical Biology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="32" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Eftimie, J. L. Bramson, and D. J. Earn, &quot;Interactions between the immune system and cancer: a brief review of non-spatial mathematical models,&quot; Bulletin of Mathematical Biology, vol. 73, no. 1, pp. 2-32, 2011.</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main" xml:id="_sQdE7hg">A survey of optimization models on cancer chemotherapy treatment planning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Alagoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Erenay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UDUAnUq">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="331" to="356" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Shi, O. Alagoz, F. S. Erenay, and Q. Su, &quot;A survey of optimiza- tion models on cancer chemotherapy treatment planning,&quot; Annals of Operations Research, vol. 221, no. 1, pp. 331-356, 2014.</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main" xml:id="_e4pANBS">Cancer evolution: mathematical models and computational inference</title>
		<author>
			<persName><forename type="first">N</forename><surname>Beerenwinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerstung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Markowetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QJVk85H">Systematic Biology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="e25" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Beerenwinkel, R. F. Schwarz, M. Gerstung, and F. Markowetz, &quot;Cancer evolution: mathematical models and computational inference,&quot; Systematic Biology, vol. 64, no. 1, pp. e1-e25, 2014.</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main" xml:id="_UCFuth3">Personalizing cancer therapy via machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manasinghka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shrager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tadepalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tbUkE8P">Workshops of NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Tenenbaum, A. Fern, L. Getoor, M. Littman, V. Manasinghka, S. Natarajan, D. Page, J. Shrager, Y. Singer, and P. Tadepalli, &quot;Person- alizing cancer therapy via machine learning,&quot; in Workshops of NIPS, 2010.</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main" xml:id="_Vsq84VZ">Support vector method for function approximation, regression estimation and signal processing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XVzW7Sm">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="281" to="287" />
		</imprint>
	</monogr>
	<note type="raw_reference">V. Vapnik, S. E. Golowich, and A. J. Smola, &quot;Support vector method for function approximation, regression estimation and signal processing,&quot; in Advances in Neural Information Processing Systems, 1997, pp. 281- 287.</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main" xml:id="_gxuaqYv">The dynamics of an optimally controlled tumor model: A case study</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>De Pillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radunskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BbqudM6">Mathematical and Computer Modelling</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1221" to="1244" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">L. G. De Pillis and A. Radunskaya, &quot;The dynamics of an optimally controlled tumor model: A case study,&quot; Mathematical and Computer Modelling, vol. 37, no. 11, pp. 1221-1244, 2003.</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main" xml:id="_mjBka2q">Machine learning in radiation oncology: Opportunities, requirements, and needs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Valdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Solberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9kK5RfC">Frontiers in Oncology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Feng, G. Valdes, N. Dixit, and T. D. Solberg, &quot;Machine learning in radiation oncology: Opportunities, requirements, and needs,&quot; Frontiers in Oncology, vol. 8, 2018.</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main" xml:id="_FM8xKJS">Robust high performance reinforcement learning through weighted k-nearest neighbors</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Lope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maravall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uKEDgG4">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1251" to="1259" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. de Lope, D. Maravall et al., &quot;Robust high performance reinforce- ment learning through weighted k-nearest neighbors,&quot; Neurocomputing, vol. 74, no. 8, pp. 1251-1259, 2011.</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main" xml:id="_mFuqwM5">Idf diabetes atlas: Global estimates of diabetes prevalence for 2017 and projections for 2045</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karuranga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Da Rocha Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ohlrogge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Malanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gMXEnbG">Diabetes Research and Clinical Practice</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="271" to="281" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Cho, J. Shaw, S. Karuranga, Y. Huang, J. da Rocha Fernandes, A. Ohlrogge, and B. Malanda, &quot;Idf diabetes atlas: Global estimates of diabetes prevalence for 2017 and projections for 2045,&quot; Diabetes Research and Clinical Practice, vol. 138, pp. 271-281, 2018.</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main" xml:id="_ahQXfFy">Clinical control of diabetes by the artificial pancreas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Albisser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Davidovac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Botz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zingg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schipper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JATnhNz">Diabetes</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="397" to="404" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. M. Albisser, B. Leibel, T. Ewart, Z. Davidovac, C. Botz, W. Zingg, H. Schipper, and R. Gander, &quot;Clinical control of diabetes by the artificial pancreas,&quot; Diabetes, vol. 23, no. 5, pp. 397-404, 1974.</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main" xml:id="_xwfPzNZ">Artificial pancreas: past, present, future</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cobelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kovatchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VG9D7XH">Diabetes</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2672" to="2682" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Cobelli, E. Renard, and B. Kovatchev, &quot;Artificial pancreas: past, present, future,&quot; Diabetes, vol. 60, no. 11, pp. 2672-2682, 2011.</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main" xml:id="_ynVmbqv">A critical assessment of algorithms and challenges in the development of a closed-loop artificial pancreas</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bequette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VMEkBmk">Diabetes Technology &amp; Therapeutics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="47" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. W. Bequette, &quot;A critical assessment of algorithms and challenges in the development of a closed-loop artificial pancreas,&quot; Diabetes Technology &amp; Therapeutics, vol. 7, no. 1, pp. 28-47, 2005.</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main" xml:id="_tKybcPE">The artificial pancreas: current status and future prospects in the management of diabetes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Peyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dassau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Skyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZUJD7am">Annals of the New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1311</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="102" to="123" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Peyser, E. Dassau, M. Breton, and J. S. Skyler, &quot;The artificial pancreas: current status and future prospects in the management of diabetes,&quot; Annals of the New York Academy of Sciences, vol. 1311, no. 1, pp. 102-123, 2014.</note>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main" xml:id="_jAH9crs">The use of reinforcement learning algorithms to meet the challenges of an artificial pancreas</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Bothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dickens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Reichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ellger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="DOI">10.1586/17434440.2013.827515</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_C297gDQ">Expert Review of Medical Devices</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="661" to="673" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. K. Bothe, L. Dickens, K. Reichel, A. Tellmann, B. Ellger, M. West- phal, and A. A. Faisal, &quot;The use of reinforcement learning algorithms to meet the challenges of an artificial pancreas,&quot; Expert Review of Medical Devices, vol. 10, no. 5, pp. 661-673, 2013.</note>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main" xml:id="_YYeUNx8">Agent-based simulation for blood glucose</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yasini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Naghibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sistani</surname></persName>
		</author>
		<author>
			<persName><surname>Karimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_b8rkZ8v">International Journal of Applied Science, Engineering and Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="89" to="95" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Yasini, M. B. Naghibi Sistani, and A. Karimpour, &quot;Agent-based sim- ulation for blood glucose,&quot; International Journal of Applied Science, Engineering and Technology, vol. 5, pp. 89-95, 2009.</note>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main" xml:id="_RvBM3yJ">Preliminary results of a novel approach for glucose regulation using an actor-critic learning based controller</title>
		<author>
			<persName><forename type="first">E</forename><surname>Daskalaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Scarnato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mougiakakou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. Daskalaki, L. Scarnato, P. Diem, and S. G. Mougiakakou, &quot;Pre- liminary results of a novel approach for glucose regulation using an actor-critic learning based controller,&quot; 2010.</note>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main" xml:id="_PjuXegu">In silico preclinical trials: a proof of concept in closed-loop control of type 1 diabetes</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Kovatchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dalla Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cobelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. P. Kovatchev, M. Breton, C. Dalla Man, and C. Cobelli, &quot;In silico preclinical trials: a proof of concept in closed-loop control of type 1 diabetes,&quot; 2009.</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main" xml:id="_2DmjyR8">An actor-critic based controller for glucose regulation in type 1 diabetes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Daskalaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mougiakakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RSYjUMk">Computer Methods and Programs in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="116" to="125" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. Daskalaki, P. Diem, and S. G. Mougiakakou, &quot;An actor-critic based controller for glucose regulation in type 1 diabetes,&quot; Computer Methods and Programs in Biomedicine, vol. 109, no. 2, pp. 116-125, 2013.</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main" xml:id="_E6xWJbh">Personalized tuning of a reinforcement learning control algorithm for glucose regulation</title>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CsFw4xk">2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3487" to="3490" />
		</imprint>
	</monogr>
	<note type="raw_reference">--, &quot;Personalized tuning of a reinforcement learning control al- gorithm for glucose regulation,&quot; in 2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). IEEE, 2013, pp. 3487-3490.</note>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main" xml:id="_ambtpy9">Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes</title>
		<idno type="DOI">10.1371/journal.pone.0158722</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tZa6ZDe">PloS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">158722</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">119--, &quot;Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes,&quot; PloS One, vol. 11, no. 7, p. e0158722, 2016.</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main" xml:id="_sFyFTuh">A dual mode adaptive basal-bolus advisor based on reinforcement learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jankovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Budzinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mougiakakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_3dPwsje">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Q. Sun, M. Jankovic, J. Budzinski, B. Moore, P. Diem, C. Stettler, and S. G. Mougiakakou, &quot;A dual mode adaptive basal-bolus advisor based on reinforcement learning,&quot; IEEE journal of biomedical and health informatics, 2018.</note>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main" xml:id="_jHF2jP9">Qualitative behavior of a family of delay-differential models of the glucose-insulin system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Palumbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panunzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>Gaetano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4Y4qWb7">Discrete and Continuous Dynamical Systems Series B</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">399</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Palumbo, S. Panunzi, and A. De Gaetano, &quot;Qualitative behavior of a family of delay-differential models of the glucose-insulin system,&quot; Discrete and Continuous Dynamical Systems Series B, vol. 7, no. 2, p. 399, 2007.</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main" xml:id="_yMzha54">Glucose level control using temporal difference methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadrnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_48FtmP6">2017 Iranian Conference on Electrical Engineering (ICEE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="895" to="900" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Noori, M. A. Sadrnia et al., &quot;Glucose level control using temporal difference methods,&quot; in 2017 Iranian Conference on Electrical Engi- neering (ICEE). IEEE, 2017, pp. 895-900.</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main" xml:id="_fjNzFdJ">Reinforcement-learning optimal control for type-1 diabetes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holubová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Muzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Godtliebsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tt2uUtQ">2018 IEEE EMBS International Conference on Biomedical &amp; Health Informatics (BHI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="333" to="336" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. D. Ngo, S. Wei, A. Holubová, J. Muzik, and F. Godtliebsen, &quot;Reinforcement-learning optimal control for type-1 diabetes,&quot; in 2018 IEEE EMBS International Conference on Biomedical &amp; Health Infor- matics (BHI). IEEE, 2018, pp. 333-336.</note>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main" xml:id="_xbqjuvz">Control of blood glucose for type-1 diabetes by using reinforcement learning with feedforward algorithm</title>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bFrK8Jm">Computational and Mathematical Methods in Medicine</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">--, &quot;Control of blood glucose for type-1 diabetes by using rein- forcement learning with feedforward algorithm,&quot; Computational and Mathematical Methods in Medicine, vol. 2018, 2018.</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main" xml:id="_cCsZArm">Quantitative estimation of insulin sensitivity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Ider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cobelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qxFpXhb">American Journal of Physiology-Endocrinology And Metabolism</title>
		<imprint>
			<biblScope unit="volume">236</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">667</biblScope>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. N. Bergman, Y. Z. Ider, C. R. Bowden, and C. Cobelli, &quot;Quantitative estimation of insulin sensitivity.&quot; American Journal of Physiology- Endocrinology And Metabolism, vol. 236, no. 6, p. E667, 1979.</note>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main" xml:id="_UmTBCZ6">Nonlinear model predictive control of glucose concentration in subjects with type 1 diabetes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hovorka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Canonico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Chassin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Haueter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Massi-Benedetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Federici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Pieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Schaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schaupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7p7a4bh">Physiological Measurement</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">905</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Hovorka, V. Canonico, L. J. Chassin, U. Haueter, M. Massi- Benedetti, M. O. Federici, T. R. Pieber, H. C. Schaller, L. Schaupp, T. Vering et al., &quot;Nonlinear model predictive control of glucose concen- tration in subjects with type 1 diabetes,&quot; Physiological Measurement, vol. 25, no. 4, p. 905, 2004.</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main" xml:id="_WXa7Qbs">Controlling blood glucose variability under uncertainty using reinforcement learning and gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Paula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Ávila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2zkfeHA">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="310" to="332" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. De Paula, L. O. Ávila, and E. C. Martínez, &quot;Controlling blood glucose variability under uncertainty using reinforcement learning and gaussian processes,&quot; Applied Soft Computing, vol. 35, pp. 310-332, 2015.</note>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main" xml:id="_hHdJTjt">On-line policy learning and adaptation for real-time personalization of an artificial pancreas</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Paula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kP5bg9r">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2234" to="2255" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. De Paula, G. G. Acosta, and E. C. Martínez, &quot;On-line policy learning and adaptation for real-time personalization of an artificial pancreas,&quot; Expert Systems with Applications, vol. 42, no. 4, pp. 2234- 2255, 2015.</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main" xml:id="_Nh3KT6p">Blood glucose regulation with stochastic optimal control for insulin-dependent diabetic patients</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Acikgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Diwekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_FMCeAGj">Chemical Engineering Science</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1227" to="1236" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. U. Acikgoz and U. M. Diwekar, &quot;Blood glucose regulation with stochastic optimal control for insulin-dependent diabetic patients,&quot; Chemical Engineering Science, vol. 65, no. 3, pp. 1227-1236, 2010.</note>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main" xml:id="_KY8UjaF">Modeling medical records of diabetes using markov decision processes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hashida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_czQsMH8">Proceedings of ICML2013 Workshop on Role of Machine Learning in Transforming Healthcare</title>
		<meeting>ICML2013 Workshop on Role of Machine Learning in Transforming Healthcare</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Asoh, M. Shiro, S. Akaho, T. Kamishima, K. Hashida, E. Aramaki, and T. Kohro, &quot;Modeling medical records of diabetes using markov decision processes,&quot; in Proceedings of ICML2013 Workshop on Role of Machine Learning in Transforming Healthcare, 2013.</note>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main" xml:id="_6HY8bbM">An application of inverse reinforcement learning to medical records of diabetes treatment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S S</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hasida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zs5E4wG">ECMLPKDD2013 Workshop on Reinforcement Learning with Generalized Feedback</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Asoh, M. S. S. Akaho, T. Kamishima, K. Hasida, E. Aramaki, and T. Kohro, &quot;An application of inverse reinforcement learning to medical records of diabetes treatment,&quot; in ECMLPKDD2013 Workshop on Reinforcement Learning with Generalized Feedback, 2013.</note>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main" xml:id="_8SxXt7G">Estimating dynamic treatment regimes in mobile health using v-learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Luckett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Kahkoska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Maahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mayer-Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xMkuhGu">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>just-accepted</note>
	<note type="raw_reference">D. J. Luckett, E. B. Laber, A. R. Kahkoska, D. M. Maahs, E. Mayer- Davis, and M. R. Kosorok, &quot;Estimating dynamic treatment regimes in mobile health using v-learning,&quot; Journal of the American Statistical Association, no. just-accepted, pp. 1-39, 2018.</note>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main" xml:id="_87R3RHc">Reinforcement learning approach to individualization of chronic pharmacotherapy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gaweda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Muezzinoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Aronoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Zz2Qwu9">IJCNN&apos;05</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="3290" to="3295" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A. Jacobs, J. M. Zurada, and M. E. Brier, &quot;Reinforcement learning approach to individualization of chronic pharmacotherapy,&quot; in IJCNN&apos;05, vol. 5. IEEE, 2005, pp. 3290-3295.</note>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main" xml:id="_FT7kT5Z">Model predictive control with reinforcement learning for drug delivery in renal anemia management</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gaweda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Muezzinoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Aronoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mAUbJDn">IEEE EMBS&apos;06</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="5177" to="5180" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. E. Gaweda, M. K. Muezzinoglu, A. A. Jacobs, G. R. Aronoff, and M. E. Brier, &quot;Model predictive control with reinforcement learning for drug delivery in renal anemia management,&quot; in IEEE EMBS&apos;06. IEEE, 2006, pp. 5177-5180.</note>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main" xml:id="_Z4FEuUW">Individualization of pharmacological anemia management using reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gaweda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Muezzinoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Aronoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_byYpgbS">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="826" to="834" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A. Jacobs, J. M. Zurada, and M. E. Brier, &quot;Individualization of pharmacological anemia management using reinforcement learning,&quot; Neural Networks, vol. 18, no. 5-6, pp. 826-834, 2005.</note>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main" xml:id="_KmBkCYk">A reinforcement learning approach for individualizing erythropoietin dosages in hemodialysis patients</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Martín-Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soria-Olivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Climente-Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Jiménez-Torres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sV93uTS">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="9737" to="9742" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. D. Martín-Guerrero, F. Gomez, E. Soria-Olivas, J. Schmidhuber, M. Climente-Martí, and N. V. Jiménez-Torres, &quot;A reinforcement learn- ing approach for individualizing erythropoietin dosages in hemodialysis patients,&quot; Expert Systems with Applications, vol. 36, no. 6, pp. 9737- 9742, 2009.</note>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main" xml:id="_4kgftjP">Validation of a reinforcement learning policy for dosage optimization of erythropoietin</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Martín-Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soria-Olivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martínez-Sober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Climente-Martí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Diego-Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Jiménez-Torres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KHDjGMV">Australasian Joint Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="732" to="738" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. D. Martín-Guerrero, E. Soria-Olivas, M. Martínez-Sober, M. Climente-Martí, T. De Diego-Santos, and N. V. Jiménez- Torres, &quot;Validation of a reinforcement learning policy for dosage optimization of erythropoietin,&quot; in Australasian Joint Conference on Artificial Intelligence. Springer, 2007, pp. 732-738.</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main" xml:id="_9FDtBaf">Optimizing drug therapy with reinforcement learning: The case of anemia management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Malof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gaweda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_TQqgnvn">Neural Networks (IJCNN), The 2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2088" to="2092" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. M. Malof and A. E. Gaweda, &quot;Optimizing drug therapy with reinforcement learning: The case of anemia management,&quot; in Neural Networks (IJCNN), The 2011 International Joint Conference on. IEEE, 2011, pp. 2088-2092.</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main" xml:id="_Kh2n5MK">Adaptive treatment of anemia on hemodialysis patients: A reinforcement learning approach</title>
		<author>
			<persName><forename type="first">P</forename><surname>Escandell-Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martínez-Martínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Martín-Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soria-Olivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vila-Francés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Magdalena-Benedito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ce43qAZ">CIDM2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Escandell-Montero, J. M. Martínez-Martínez, J. D. Martín-Guerrero, E. Soria-Olivas, J. Vila-Francés, and R. Magdalena-Benedito, &quot;Adaptive treatment of anemia on hemodialysis patients: A reinforcement learning approach,&quot; in CIDM2011. IEEE, 2011, pp. 44-49.</note>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main" xml:id="_kUkggJa">Optimization of anemia treatment in hemodialysis patients via reinforcement learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Escandell-Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chermisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martinez-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomez-Sanchis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soria-Olivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vila-Francés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stopper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GCE9f4N">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. Escandell-Montero, M. Chermisi, J. M. Martinez-Martinez, J. Gomez-Sanchis, C. Barbieri, E. Soria-Olivas, F. Mari, J. Vila- Francés, A. Stopper, E. Gatti et al., &quot;Optimization of anemia treatment in hemodialysis patients via reinforcement learning,&quot; Artificial Intelli- gence in Medicine, vol. 62, no. 1, pp. 47-60, 2014.</note>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main" xml:id="_qj7fvkb">Dynamic multidrug therapies for hiv: Optimal and sti control approaches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RZxDAKN">Mathematical Biosciences and Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="241" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. M. Adams, H. T. Banks, H.-D. Kwon, and H. T. Tran, &quot;Dynamic multidrug therapies for hiv: Optimal and sti control approaches,&quot; Mathematical Biosciences and Engineering, vol. 1, no. 2, pp. 223-241, 2004.</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main" xml:id="_zjPuRrv">Clinical data based optimal sti strategies for hiv: a reinforcement learning approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_E7tAenw">45th IEEE Conference on Decision and Control</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="667" to="672" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ernst, G.-B. Stan, J. Goncalves, and L. Wehenkel, &quot;Clinical data based optimal sti strategies for hiv: a reinforcement learning approach,&quot; in 45th IEEE Conference on Decision and Control. IEEE, 2006, pp. 667-672.</note>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<title level="m" type="main" xml:id="_RXMYC35">A reinforcement learning design for hiv clinical trials</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parbhoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note type="raw_reference">S. Parbhoo, &quot;A reinforcement learning design for hiv clinical trials,&quot; Ph.D. dissertation, 2014.</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main" xml:id="_K2gNH8d">Combining kernel and model based learning for hiv therapy selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bogojeska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_m75vPJg">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">239</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Parbhoo, J. Bogojeska, M. Zazzi, V. Roth, and F. Doshi-Velez, &quot;Combining kernel and model based learning for hiv therapy selection,&quot; AMIA Summits on Translational Science Proceedings, vol. 2017, p. 239, 2017.</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main" xml:id="_rQnuYDG">Quantifying uncertainty in batch personalized sequential decision making</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Marivate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chemali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ceKU5xB">AAAI Workshop: Modern Artificial Intelligence for Health Analytics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. N. Marivate, J. Chemali, E. Brunskill, and M. L. Littman, &quot;Quanti- fying uncertainty in batch personalized sequential decision making.&quot; in AAAI Workshop: Modern Artificial Intelligence for Health Analytics, 2014.</note>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
		<title level="m" type="main" xml:id="_njVvhka">Transfer learning across patient variations with hidden parameter markov decision processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v31i1.11065</idno>
		<idno type="arXiv">arXiv:1612.00475</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">T. Killian, G. Konidaris, and F. Doshi-Velez, &quot;Transfer learning across patient variations with hidden parameter markov decision processes,&quot; arXiv preprint arXiv:1612.00475, 2016.</note>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main" xml:id="_y8STKhj">Robust and efficient transfer learning with hidden parameter markov decision processes</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FD5QjDU">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6250" to="6261" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. W. Killian, S. Daulton, G. Konidaris, and F. Doshi-Velez, &quot;Robust and efficient transfer learning with hidden parameter markov decision processes,&quot; in Advances in Neural Information Processing Systems, 2017, pp. 6250-6261.</note>
</biblStruct>

<biblStruct xml:id="b147">
	<monogr>
		<title level="m" type="main" xml:id="_V5a2B9M">Direct policy transfer via hidden parameter markov decision processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Yao, T. Killian, G. Konidaris, and F. Doshi-Velez, &quot;Direct policy transfer via hidden parameter markov decision processes,&quot; 2018.</note>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main" xml:id="_jJEVgUC">Incorporating causal factors into reinforcement learning for dynamic treatment regimes in hiv</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MtRZx4h">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Yu, Y. Dong, J. Liu, and G. Ren, &quot;Incorporating causal factors into reinforcement learning for dynamic treatment regimes in hiv,&quot; BMC medical informatics and decision making, vol. 19, no. 2, p. 60, 2019.</note>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main" xml:id="_Ka2vaT6">Pac optimal exploration in continuous space markov decision processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_adJXb2a">AAAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Pazis and R. Parr, &quot;Pac optimal exploration in continuous space markov decision processes.&quot; in AAAI, 2013.</note>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main" xml:id="_yeAVcpV">Bounded optimal exploration in mdp</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QHrKU8P">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1758" to="1764" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Kawaguchi, &quot;Bounded optimal exploration in mdp.&quot; in AAAI, 2016, pp. 1758-1764.</note>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main" xml:id="_yxyYnTW">Methodological challenges in constructing effective treatment sequences for chronic psychiatric disorders</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Oslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1038/sj.npp.1301241</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NdXqVDE">Neuropsychopharmacology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">257</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Murphy, D. W. Oslin, A. J. Rush, and J. Zhu, &quot;Methodological challenges in constructing effective treatment sequences for chronic psychiatric disorders,&quot; Neuropsychopharmacology, vol. 32, no. 2, p. 257, 2007.</note>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main" xml:id="_GNr2pTz">Eeg seizure detection and prediction algorithms: a survey</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Alotaiby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Alshebeili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E A</forename><surname>El-Samie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_s5khN6y">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">183</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. N. Alotaiby, S. A. Alshebeili, T. Alshawi, I. Ahmad, and F. E. A. El-Samie, &quot;Eeg seizure detection and prediction algorithms: a survey,&quot; EURASIP Journal on Advances in Signal Processing, vol. 2014, no. 1, p. 183, 2014.</note>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main" xml:id="_Z2uHnbh">Progress in neuroengineering for brain repair: New challenges and open issues</title>
		<author>
			<persName><forename type="first">G</forename><surname>Panuccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Semprini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Buccelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Colombi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chiappalone</surname></persName>
		</author>
		<idno type="DOI">10.1177/2398212818776475</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SqDPJ7U">Brain and Neuroscience Advances</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2398212818776475</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Panuccio, M. Semprini, L. Natale, S. Buccelli, I. Colombi, and M. Chiappalone, &quot;Progress in neuroengineering for brain repair: New challenges and open issues,&quot; Brain and Neuroscience Advances, vol. 2, p. 2398212818776475, 2018.</note>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main" xml:id="_hesQ9fu">Adaptive treatment of epilepsy via batch-mode reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_n8CJd5m">AAAI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1671" to="1678" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Guez, R. D. Vincent, M. Avoli, and J. Pineau, &quot;Adaptive treatment of epilepsy via batch-mode reinforcement learning.&quot; in AAAI, 2008, pp. 1671-1678.</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main" xml:id="_DSmZDp5">Treating epilepsy via adaptive neurostimulation: a reinforcement learning approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Panuccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Y6rm568">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="227" to="240" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Pineau, A. Guez, R. Vincent, G. Panuccio, and M. Avoli, &quot;Treating epilepsy via adaptive neurostimulation: a reinforcement learning ap- proach,&quot; International Journal of Neural Systems, vol. 19, no. 04, pp. 227-240, 2009.</note>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main" xml:id="_Avh9HCB">Adaptive control of epileptic seizures using reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>McGill University Library</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note type="raw_reference">A. Guez, &quot;Adaptive control of epileptic seizures using reinforcement learning,&quot; Ph.D. dissertation, McGill University Library, 2010.</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main" xml:id="_rPgx8k4">Adaptive control of epileptiform excitability in an in vitro model of limbic seizures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Panuccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Avoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.expneurol.2013.01.002</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CNa6cEx">Experimental Neurology</title>
		<imprint>
			<biblScope unit="volume">241</biblScope>
			<biblScope unit="page" from="179" to="183" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Panuccio, A. Guez, R. Vincent, M. Avoli, and J. Pineau, &quot;Adaptive control of epileptiform excitability in an in vitro model of limbic seizures,&quot; Experimental Neurology, vol. 241, pp. 179-183, 2013.</note>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main" xml:id="_bvxWcQF">Manifold embeddings for model-based reinforcement learning under partial observability</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ysEcCua">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Bush and J. Pineau, &quot;Manifold embeddings for model-based rein- forcement learning under partial observability,&quot; in Advances in Neural Information Processing Systems, 2009, pp. 189-197.</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main" xml:id="_5c3QaEe">Seizure control in a computational model using a reinforcement learning stimulation paradigm</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamperski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Netoff</surname></persName>
		</author>
		<idno type="DOI">10.1142/s0129065717500125</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VxJZydq">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">07</biblScope>
			<biblScope unit="page">1750012</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">V. Nagaraj, A. Lamperski, and T. I. Netoff, &quot;Seizure control in a computational model using a reinforcement learning stimulation paradigm,&quot; International Journal of Neural Systems, vol. 27, no. 07, p. 1750012, 2017.</note>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main" xml:id="_yUUkB5y">Sequenced treatment alternatives to relieve depression (star* d): rationale and design</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Lavori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Sackeim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Thase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Nierenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Quitkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kashner</surname></persName>
		</author>
		<idno type="DOI">10.1016/s0197-2456(03)00112-0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CN49mdY">Controlled clinical trials</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="142" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. J. Rush, M. Fava, S. R. Wisniewski, P. W. Lavori, M. H. Trivedi, H. A. Sackeim, M. E. Thase, A. A. Nierenberg, F. M. Quitkin, T. M. Kashner et al., &quot;Sequenced treatment alternatives to relieve depression (star* d): rationale and design,&quot; Controlled clinical trials, vol. 25, no. 1, pp. 119-142, 2004.</note>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main" xml:id="_Y9GSdAW">Constructing evidence-based treatment strategies using methods from computer science</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghizaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9CwXU6A">Drug &amp; Alcohol Dependence</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="52" to="S60" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Pineau, M. G. Bellemare, A. J. Rush, A. Ghizaru, and S. A. Murphy, &quot;Constructing evidence-based treatment strategies using methods from computer science,&quot; Drug &amp; Alcohol Dependence, vol. 88, pp. S52-S60, 2007.</note>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main" xml:id="_AbKp5de">Kernel-based reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ormoneit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ś</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Z9xdA5f">Machine learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="161" to="178" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Ormoneit and Ś. Sen, &quot;Kernel-based reinforcement learning,&quot; Ma- chine learning, vol. 49, no. 2-3, pp. 161-178, 2002.</note>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main" xml:id="_h89NyKq">Inference for optimal dynamic treatment regimes using an adaptive m-out-of-n bootstrap scheme</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YpXy5fy">Biometrics</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="723" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Chakraborty, E. B. Laber, and Y. Zhao, &quot;Inference for optimal dynamic treatment regimes using an adaptive m-out-of-n bootstrap scheme,&quot; Biometrics, vol. 69, no. 3, pp. 714-723, 2013.</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main" xml:id="_NU8CTRr">Interactive model building for q-learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Linn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Stefanski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rNcvQeN">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="831" to="847" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. B. Laber, K. A. Linn, and L. A. Stefanski, &quot;Interactive model building for q-learning,&quot; Biometrika, vol. 101, no. 4, pp. 831-847, 2014.</note>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<title level="m" type="main" xml:id="_5yPzubf">Interactive q-learning for probabilities and quantiles</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Linn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Stefanski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.3414</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">K. A. Linn, E. B. Laber, and L. A. Stefanski, &quot;Interactive q-learning for probabilities and quantiles,&quot; arXiv preprint arXiv:1407.3414, 2014.</note>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main" xml:id="_TsTbsqF">Interactive q-learning for quantiles</title>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aq68wNr">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="638" to="649" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">--, &quot;Interactive q-learning for quantiles,&quot; Journal of the American Statistical Association, vol. 112, no. 518, pp. 638-649, 2017.</note>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main" xml:id="_WAH3q5d">Q-and alearning methods for estimating optimal dynamic treatment regimes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Tsiatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davidian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cSdCyvk">Statistical science: a review journal of the Institute of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">640</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. J. Schulte, A. A. Tsiatis, E. B. Laber, and M. Davidian, &quot;Q-and a- learning methods for estimating optimal dynamic treatment regimes,&quot; Statistical science: a review journal of the Institute of Mathematical Statistics, vol. 29, no. 4, p. 640, 2014.</note>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main" xml:id="_fYzQ6cS">Optimal dynamic treatment regimes</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_STyraRg">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="331" to="355" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. A. Murphy, &quot;Optimal dynamic treatment regimes,&quot; Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 65, no. 2, pp. 331-355, 2003.</note>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main" xml:id="_qRr7hkJ">Penalized q-learning for dynamic treatment regimens</title>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HqQ3xWR">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">901</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Song, W. Wang, D. Zeng, and M. R. Kosorok, &quot;Penalized q-learning for dynamic treatment regimens,&quot; Statistica Sinica, vol. 25, no. 3, p. 901, 2015.</note>
</biblStruct>

<biblStruct xml:id="b170">
	<monogr>
		<title level="m" type="main" xml:id="_FbuY36a">Robust hybrid learning for estimating personalized dynamic treatment regimens</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02314</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Y. Liu, Y. Wang, M. R. Kosorok, Y. Zhao, and D. Zeng, &quot;Robust hybrid learning for estimating personalized dynamic treatment regimens,&quot; arXiv preprint arXiv:1611.02314, 2016.</note>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main" xml:id="_SMsawRs">Budgeted learning for developing personalized treatment</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UdT2tG7">ICMLA2014</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Deng, R. Greiner, and S. Murphy, &quot;Budgeted learning for developing personalized treatment,&quot; in ICMLA2014. IEEE, 2014, pp. 7-14.</note>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main" xml:id="_3QGRuY5">Neurocognitive effects of antipsychotic medications in patients with chronic schizophrenia in the catie trial</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Keefe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bilder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Meltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Capuano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Stroup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HRARcjr">Archives of General Psychiatry</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="633" to="647" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. S. Keefe, R. M. Bilder, S. M. Davis, P. D. Harvey, B. W. Palmer, J. M. Gold, H. Y. Meltzer, M. F. Green, G. Capuano, T. S. Stroup et al., &quot;Neurocognitive effects of antipsychotic medications in patients with chronic schizophrenia in the catie trial,&quot; Archives of General Psychiatry, vol. 64, no. 6, pp. 633-647, 2007.</note>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main" xml:id="_6RSSUmE">Informing sequential clinical decision-making through reinforcement learning: an empirical study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shortreed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Stroup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_JuQPaZk">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="109" to="136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. M. Shortreed, E. Laber, D. J. Lizotte, T. S. Stroup, J. Pineau, and S. A. Murphy, &quot;Informing sequential clinical decision-making through reinforcement learning: an empirical study,&quot; Machine Learning, vol. 84, no. 1-2, pp. 109-136, 2011.</note>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main" xml:id="_X6xyAPN">Q-learning residual analysis: application to the effectiveness of sequences of antipsychotic medications for patients with schizophrenia</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ertefaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shortreed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cZvzh8Y">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2221" to="2234" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Ertefaie, S. Shortreed, and B. Chakraborty, &quot;Q-learning residual analysis: application to the effectiveness of sequences of antipsychotic medications for patients with schizophrenia,&quot; Statistics in Medicine, vol. 35, no. 13, pp. 2221-2234, 2016.</note>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main" xml:id="_maDVqVS">Linear fitted-q iteration with multiple reward functions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1609/icaps.v23i1.13579</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pkRRzUH">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3253" to="3295" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. J. Lizotte, M. Bowling, and S. A. Murphy, &quot;Linear fitted-q iter- ation with multiple reward functions,&quot; Journal of Machine Learning Research, vol. 13, no. Nov, pp. 3253-3295, 2012.</note>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main" xml:id="_5Vn86M6">Multi-objective markov decision processes for data-driven decision support</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9DcWWAb">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7378" to="7405" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. J. Lizotte and E. B. Laber, &quot;Multi-objective markov decision processes for data-driven decision support,&quot; The Journal of Machine Learning Research, vol. 17, no. 1, pp. 7378-7405, 2016.</note>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main" xml:id="_EgChfJy">Set-valued dynamic treatment regimes for competing outcomes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_GpuCtzb">Biometrics</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="61" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. B. Laber, D. J. Lizotte, and B. Ferguson, &quot;Set-valued dynamic treatment regimes for competing outcomes,&quot; Biometrics, vol. 70, no. 1, pp. 53-61, 2014.</note>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main" xml:id="_ZS3rX6j">Incorporating patient preferences into estimation of optimal individualized treatment rules</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NVwWSsS">Biometrics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. L. Butler, E. B. Laber, S. M. Davis, and M. R. Kosorok, &quot;Incor- porating patient preferences into estimation of optimal individualized treatment rules,&quot; Biometrics, 2017.</note>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main" xml:id="_3d5k5HP">Managing addiction as a chronic condition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SkACEMC">Addiction Science &amp; Clinical Practice</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Dennis and C. K. Scott, &quot;Managing addiction as a chronic con- dition,&quot; Addiction Science &amp; Clinical Practice, vol. 4, no. 1, p. 45, 2007.</note>
</biblStruct>

<biblStruct xml:id="b180">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.05047</idno>
		<title level="m" xml:id="_thWQeWv">A batch, off-policy, actor-critic algorithm for optimizing the average reward</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">S. A. Murphy, Y. Deng, E. B. Laber, H. R. Maei, R. S. Sutton, and K. Witkiewitz, &quot;A batch, off-policy, actor-critic algorithm for optimizing the average reward,&quot; arXiv preprint arXiv:1607.05047, 2016.</note>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main" xml:id="_DdqPGYz">Inference for non-regular parameters in optimal dynamic treatment regimes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Strecher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TpeKbKm">Statistical Methods in Medical Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="317" to="343" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Chakraborty, S. Murphy, and V. Strecher, &quot;Inference for non-regular parameters in optimal dynamic treatment regimes,&quot; Statistical Methods in Medical Research, vol. 19, no. 3, pp. 317-343, 2010.</note>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main" xml:id="_CeGHZAx">Bias correction and confidence intervals for fitted q-iteration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Strecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5Yx9c7V">Workshop on Model Uncertainty and Risk in Reinforcement Learning</title>
		<meeting><address><addrLine>Whistler, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Chakraborty, V. Strecher, and S. Murphy, &quot;Bias correction and confidence intervals for fitted q-iteration,&quot; in Workshop on Model Uncertainty and Risk in Reinforcement Learning, NIPS, Whistler, Canada. Citeseer, 2008.</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main" xml:id="_2aYJ6Hu">Tree-based reinforcement learning for estimating optimal dynamic treatment regimes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almirall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_p8gQMRH">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1914" to="1938" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Tao, L. Wang, D. Almirall et al., &quot;Tree-based reinforcement learning for estimating optimal dynamic treatment regimes,&quot; The Annals of Applied Statistics, vol. 12, no. 3, pp. 1914-1938, 2018.</note>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main" xml:id="_jGCVnCm">Critical care-where have we been and where are we going?</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jaFsa8u">Critical Care</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J.-L. Vincent, &quot;Critical care-where have we been and where are we going?&quot; Critical Care, vol. 17, no. 1, p. S2, 2013.</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main" xml:id="_zWs3MF3">Critical care workforce</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Kb2gYPG">Critical Care Medicine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1350" to="1353" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Krell, &quot;Critical care workforce,&quot; Critical Care Medicine, vol. 36, no. 4, pp. 1350-1353, 2008.</note>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main" xml:id="_paFpS9A">State of the art review: the data revolution in critical care</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xuw5j7G">Critical Care</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Ghassemi, L. A. Celi, and D. J. Stone, &quot;State of the art review: the data revolution in critical care,&quot; Critical Care, vol. 19, no. 1, p. 118, 2015.</note>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main" xml:id="_xsutZXR">Surviving sepsis campaign: international guidelines for management of sepsis and septic shock: 2016</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Alhazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Sevransky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Sprung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Nunnally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9uQBSpR">Intensive Care Medicine</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="377" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Rhodes, L. E. Evans, W. Alhazzani, M. M. Levy, M. Antonelli, R. Ferrer, A. Kumar, J. E. Sevransky, C. L. Sprung, M. E. Nunnally et al., &quot;Surviving sepsis campaign: international guidelines for man- agement of sepsis and septic shock: 2016,&quot; Intensive Care Medicine, vol. 43, no. 3, pp. 304-377, 2017.</note>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main" xml:id="_pdcJsKh">Acute respiratory distress syndrome</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D T</forename><surname>Force</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ranieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rubenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TxcPkbv">Jama</title>
		<imprint>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="2526" to="2533" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. D. T. Force, V. Ranieri, G. Rubenfeld et al., &quot;Acute respiratory distress syndrome,&quot; Jama, vol. 307, no. 23, pp. 2526-2533, 2012.</note>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main" xml:id="_9Hdy9By">Use of machine-learning approaches to predict clinical deterioration in critically ill patients: A systematic review</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kamio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Masamune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_x4JuX4S">International Journal of Medical Research and Health Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Kamio, T. Van, and K. Masamune, &quot;Use of machine-learning approaches to predict clinical deterioration in critically ill patients: A systematic review,&quot; International Journal of Medical Research and Health Sciences, vol. 6, no. 6, pp. 1-7, 2017.</note>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main" xml:id="_XkAruzn">Machine learning in critical care: state-of-the-art and a sepsis case study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vellido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ribas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Sanmartín</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C R</forename><surname>Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YgsyE3G">Biomedical engineering online</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">135</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Vellido, V. Ribas, C. Morales, A. R. Sanmartín, and J. C. R. Rodríguez, &quot;Machine learning in critical care: state-of-the-art and a sepsis case study,&quot; Biomedical engineering online, vol. 17, no. 1, p. 135, 2018.</note>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main" xml:id="_QRqpPDB">A markov decision process to suggest optimal treatment of severe infections in intensive care</title>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faisal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vkkHTk5">Neural Information Processing Systems Workshop on Machine Learning for Health</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Komorowski, A. Gordon, L. Celi, and A. Faisal, &quot;A markov decision process to suggest optimal treatment of severe infections in intensive care,&quot; in Neural Information Processing Systems Workshop on Machine Learning for Health, 2016.</note>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main" xml:id="_Uuz9VpN">The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care</title>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Badawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rGrmZdg">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1716</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Komorowski, L. A. Celi, O. Badawi, A. C. Gordon, and A. A. Faisal, &quot;The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care,&quot; Nature Medicine, vol. 24, no. 11, p. 1716, 2018.</note>
</biblStruct>

<biblStruct xml:id="b193">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09602</idno>
		<title level="m" xml:id="_KYu5v8P">Deep reinforcement learning for sepsis treatment</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Raghu, M. Komorowski, I. Ahmed, L. Celi, P. Szolovits, and M. Ghassemi, &quot;Deep reinforcement learning for sepsis treatment,&quot; arXiv preprint arXiv:1711.09602, 2017.</note>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main" xml:id="_JMP7r6Q">Continuous state-space models for optimal sepsis treatment: a deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MFbgK2e">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="147" to="163" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Raghu, M. Komorowski, L. A. Celi, P. Szolovits, and M. Ghassemi, &quot;Continuous state-space models for optimal sepsis treatment: a deep reinforcement learning approach,&quot; in Machine Learning for Healthcare Conference, 2017, pp. 147-163.</note>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main" xml:id="_pRJmQdh">Model-based reinforcement learning for sepsis treatment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09602</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Raghu, M. Komorowski, and S. Singh, &quot;Model-based reinforcement learning for sepsis treatment,&quot; arXiv preprint arXiv:1811.09602, 2018.</note>
</biblStruct>

<biblStruct xml:id="b196">
	<monogr>
		<title level="m" type="main" xml:id="_ZdfdUSm">Treatment recommendation in critical care: A scalable and interpretable approach in partially observable health states</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Utomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. P. Utomo, X. Li, and W. Chen, &quot;Treatment recommendation in crit- ical care: A scalable and interpretable approach in partially observable health states,&quot; 2018.</note>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<title level="m" type="main" xml:id="_BnkCJmq">Improving sepsis treatment strategies by combining deep and kernel-based reinforcement learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wihl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04670</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">X. Peng, Y. Ding, D. Wihl, O. Gottesman, M. Komorowski, L.-w. H. Lehman, A. Ross, A. Faisal, and F. Doshi-Velez, &quot;Improving sepsis treatment strategies by combining deep and kernel-based reinforcement learning,&quot; arXiv preprint arXiv:1901.04670, 2019.</note>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main" xml:id="_WGyynD8">Learning to treat sepsis with multi-output gaussian process deep recurrent q-networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Futoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sendak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bedoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Heller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Futoma, A. Lin, M. Sendak, A. Bedoya, M. Clement, C. O&apos;Brien, and K. Heller, &quot;Learning to treat sepsis with multi-output gaussian process deep recurrent q-networks,&quot; 2018.</note>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main" xml:id="_NQfZCnM">Deep inverse reinforcement learning for sepsis treatment</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tDSRvuZ">IEEE ICHI</title>
		<imprint>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Yu, G. Ren, and J. Liu, &quot;Deep inverse reinforcement learning for sepsis treatment,&quot; in 2019 IEEE ICHI, 2019, pp. 1-3.</note>
</biblStruct>

<biblStruct xml:id="b200">
	<monogr>
		<title level="m" type="main" xml:id="_B3bYUJe">The actor search tree critic (astc) for off-policy pomdp learning in medical decision making</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11548</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">L. Li, M. Komorowski, and A. A. Faisal, &quot;The actor search tree critic (astc) for off-policy pomdp learning in medical decision making,&quot; arXiv preprint arXiv:1805.11548, 2018.</note>
</biblStruct>

<biblStruct xml:id="b201">
	<monogr>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00654</idno>
		<title level="m" xml:id="_wnUddnN">Representation and reinforcement learning for personalized glycemic control in septic patients</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">W.-H. Weng, M. Gao, Z. He, S. Yan, and P. Szolovits, &quot;Representation and reinforcement learning for personalized glycemic control in septic patients,&quot; arXiv preprint arXiv:1712.00654, 2017.</note>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<title level="m" type="main" xml:id="_nrenbJP">Precision medicine as a control problem: Using simulation and deep reinforcement learning to discover adaptive, personalized multi-cytokine therapy for sepsis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cockrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Faissol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10440</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. K. Petersen, J. Yang, W. S. Grathwohl, C. Cockrell, C. Santiago, G. An, and D. M. Faissol, &quot;Precision medicine as a control problem: Using simulation and deep reinforcement learning to discover adap- tive, personalized multi-cytokine therapy for sepsis,&quot; arXiv preprint arXiv:1802.10440, 2018.</note>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main" xml:id="_xzgKVTr">Intelligent control of closed-loop sedation in simulated icu patients</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Quasny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SwVJQmT">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="109" to="114" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. L. Moore, E. D. Sinzinger, T. M. Quasny, and L. D. Pyeatt, &quot;Intelligent control of closed-loop sedation in simulated icu patients.&quot; in FLAIRS Conference, 2004, pp. 109-114.</note>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main" xml:id="_kKA25q2">Sedation of simulated icu patients using reinforcement learning based control</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tVZ8RSw">International Journal on Artificial Intelligence Tools</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">01n02</biblScope>
			<biblScope unit="page" from="137" to="156" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. D. Sinzinger and B. Moore, &quot;Sedation of simulated icu patients using reinforcement learning based control,&quot; International Journal on Artificial Intelligence Tools, vol. 14, no. 01n02, pp. 137-156, 2005.</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main" xml:id="_pUS7GKP">Reinforcement learning: a novel method for optimal control of propofol-induced hypnosis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Doufas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KxHJPq3">Anesthesia &amp; Analgesia</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="360" to="367" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. L. Moore, A. G. Doufas, and L. D. Pyeatt, &quot;Reinforcement learning: a novel method for optimal control of propofol-induced hypnosis,&quot; Anesthesia &amp; Analgesia, vol. 112, no. 2, pp. 360-367, 2011.</note>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main" xml:id="_NYuTkee">Reinforcement learning versus proportional-integral-derivative control of hypnosis in a simulated intraoperative patient</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Quasny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Doufas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7ne4cGf">Anesthesia &amp; Analgesia</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="350" to="359" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. L. Moore, T. M. Quasny, and A. G. Doufas, &quot;Reinforcement learning versus proportional-integral-derivative control of hypnosis in a simulated intraoperative patient,&quot; Anesthesia &amp; Analgesia, vol. 112, no. 2, pp. 350-359, 2011.</note>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main" xml:id="_ShXdaf8">Reinforcement learning for closed-loop propofol anesthesia: a study in human volunteers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Padrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Doufas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zsQMUf8">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="655" to="696" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. L. Moore, L. D. Pyeatt, V. Kulkarni, P. Panousis, K. Padrez, and A. G. Doufas, &quot;Reinforcement learning for closed-loop propofol anesthesia: a study in human volunteers,&quot; The Journal of Machine Learning Research, vol. 15, no. 1, pp. 655-696, 2014.</note>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main" xml:id="_Fqg79Z8">Reinforcement learning for closed-loop propofol anesthesia: A human volunteer study</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Doufas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UPU9wWV">IAAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. L. Moore, P. Panousis, V. Kulkarni, L. D. Pyeatt, and A. G. Doufas, &quot;Reinforcement learning for closed-loop propofol anesthesia: A human volunteer study.&quot; in IAAI, 2010.</note>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main" xml:id="_rsduqQD">Multivariable anesthesia control using reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aflaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jahed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_m5sqHTP">IEEE SMC&apos;</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="4563" to="4568" />
			<date type="published" when="2006">2006</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">N. Sadati, A. Aflaki, and M. Jahed, &quot;Multivariable anesthesia control using reinforcement learning,&quot; in IEEE SMC&apos;06, vol. 6. IEEE, 2006, pp. 4563-4568.</note>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main" xml:id="_kbgYPD8">An adaptive neural network filter for improved patient state estimation in closedloop anesthesia control</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Borera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Doufas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Pyeatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ThKsFgP">IEEE ICTAI&apos;11. IEEE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="41" to="46" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. C. Borera, B. L. Moore, A. G. Doufas, and L. D. Pyeatt, &quot;An adaptive neural network filter for improved patient state estimation in closed- loop anesthesia control,&quot; in IEEE ICTAI&apos;11. IEEE, 2011, pp. 41-46.</note>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main" xml:id="_f3ZVexf">Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lowery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_eyvSuFH">IEEE/EMBS NER&apos;13</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1414" to="1417" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. Lowery and A. A. Faisal, &quot;Towards efficient, personalized anesthesia using continuous reinforcement learning for propofol infusion control,&quot; in IEEE/EMBS NER&apos;13. IEEE, 2013, pp. 1414-1417.</note>
</biblStruct>

<biblStruct xml:id="b212">
	<analytic>
		<title level="a" type="main" xml:id="_XB5Zb6N">Closed-loop control of anesthesia and mean arterial pressure using reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Haddad</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.bspc.2015.05.013</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rDSwK92">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="54" to="64" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Padmanabhan, N. Meskin, and W. M. Haddad, &quot;Closed-loop control of anesthesia and mean arterial pressure using reinforcement learning,&quot; Biomedical Signal Processing and Control, vol. 22, pp. 54-64, 2015.</note>
</biblStruct>

<biblStruct xml:id="b213">
	<monogr>
		<title level="m" type="main" xml:id="_aazhbMB">Learning from an expert</title>
		<author>
			<persName><forename type="first">P</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Audiffren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dubost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oudre</surname></persName>
		</author>
		<idno type="DOI">10.1109/tbme.2019.2954348</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">P. Humbert, J. Audiffren, C. Dubost, and L. Oudre, &quot;Learning from an expert.&quot;</note>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main" xml:id="_3F4EvyQ">Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XP5jKPh">IEEE 38th Annual International Conference of the Engineering in Medicine and Biology Society</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2978" to="2981" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Nemati, M. M. Ghassemi, and G. D. Clifford, &quot;Optimal medication dosing from suboptimal clinical examples: A deep reinforcement learning approach,&quot; in IEEE 38th Annual International Conference of the Engineering in Medicine and Biology Society. IEEE, 2016, pp. 2978-2981.</note>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main" xml:id="_fqWP2RT">A deep deterministic policy gradient approach to medication dosing and surveillance in the icu</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<idno type="DOI">10.1109/embc.2018.8513203</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rG3zEQJ">IEEE EMBC&apos;18</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4927" to="4931" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Lin, M. D. Stanley, M. M. Ghassemi, and S. Nemati, &quot;A deep deter- ministic policy gradient approach to medication dosing and surveillance in the icu,&quot; in IEEE EMBC&apos;18. IEEE, 2018, pp. 4927-4931.</note>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main" xml:id="_jrasXHM">Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219961</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cAEFkpM">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2447" to="2456" />
		</imprint>
	</monogr>
	<note type="raw_reference">L. Wang, W. Zhang, X. He, and H. Zha, &quot;Supervised reinforcement learning with recurrent neural network for dynamic treatment recom- mendation,&quot; in Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. ACM, 2018, pp. 2447-2456.</note>
</biblStruct>

<biblStruct xml:id="b217">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chivers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Draugelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Engelhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06300</idno>
		<title level="m" xml:id="_BrqdvKQ">A reinforcement learning approach to weaning of mechanical ventilation in intensive care units</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">N. Prasad, L.-F. Cheng, C. Chivers, M. Draugelis, and B. E. Engel- hardt, &quot;A reinforcement learning approach to weaning of mechanical ventilation in intensive care units,&quot; arXiv preprint arXiv:1704.06300, 2017.</note>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main" xml:id="_NekHU3G">Inverse reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12911-019-0763-6</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_R6HrK2m">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Yu, J. Liu, and H. Zhao, &quot;Inverse reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units,&quot; BMC medical informatics and decision making, vol. 19, no. 2, p. 57, 2019.</note>
</biblStruct>

<biblStruct xml:id="b219">
	<analytic>
		<title level="a" type="main" xml:id="_RuVtthv">Supervised-actor-critic reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_h3e56rU">BMC medical informatics and decision making</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">C. Yu, G. Ren, and Y. Dong, &quot;Supervised-actor-critic reinforcement learning for intelligent mechanical ventilation and sedative dosing in intensive care units,&quot; BMC medical informatics and decision making, 2020.</note>
</biblStruct>

<biblStruct xml:id="b220">
	<monogr>
		<title level="m" type="main" xml:id="_nF8C3Us">Towards high confidence offpolicy reinforcement learning for clinical applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jagannatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="raw_reference">A. Jagannatha, P. Thomas, and H. Yu, &quot;Towards high confidence off- policy reinforcement learning for clinical applications.&quot;</note>
</biblStruct>

<biblStruct xml:id="b221">
	<monogr>
		<title level="m" type="main" xml:id="_P3DDK9Z">An optimal policy for patient laboratory tests in intensive care units</title>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Engelhardt</surname></persName>
		</author>
		<idno type="DOI">10.1142/9789813279827_0029</idno>
		<idno type="arXiv">arXiv:1808.04679</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">L.-F. Cheng, N. Prasad, and B. E. Engelhardt, &quot;An optimal policy for patient laboratory tests in intensive care units,&quot; arXiv preprint arXiv:1808.04679, 2018.</note>
</biblStruct>

<biblStruct xml:id="b222">
	<monogr>
		<title level="m" type="main" xml:id="_QcaWj95">Dynamic measurement scheduling for adverse event forecasting using deep rl</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00268</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">C.-H. Chang, M. Mai, and A. Goldenberg, &quot;Dynamic measurement scheduling for adverse event forecasting using deep rl,&quot; arXiv preprint arXiv:1812.00268, 2018.</note>
</biblStruct>

<biblStruct xml:id="b223">
	<analytic>
		<title level="a" type="main" xml:id="_FwMVySf">Tools for the precision medicine era: How to develop highly personalized treatment recommendations from cohort and registry data using q-learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Krakow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Spellman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Couriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alousi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pidala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Last</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qknMnBt">American journal of epidemiology</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="160" to="172" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. F. Krakow, M. Hemmer, T. Wang, B. Logan, M. Arora, S. Spellman, D. Couriel, A. Alousi, J. Pidala, M. Last et al., &quot;Tools for the precision medicine era: How to develop highly personalized treatment recommendations from cohort and registry data using q-learning,&quot; American journal of epidemiology, vol. 186, no. 2, pp. 160-172, 2017.</note>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main" xml:id="_Z9n9BgH">Deep reinforcement learning for dynamic treatment regimes on medical registry data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ygXnqdv">IEEE ICHI&apos;17</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="380" to="385" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Liu, B. Logan, N. Liu, Z. Xu, J. Tang, and Y. Wang, &quot;Deep reinforcement learning for dynamic treatment regimes on medical registry data,&quot; in IEEE ICHI&apos;17. IEEE, 2017, pp. 380-385.</note>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main" xml:id="_P5ju8Ep">Sepsis: pathophysiology and clinical management</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gotts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Matthay</surname></persName>
		</author>
		<idno type="DOI">10.1136/bmj.i1585</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RWKSezz">Bmj</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page">1585</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. E. Gotts and M. A. Matthay, &quot;Sepsis: pathophysiology and clinical management,&quot; Bmj, vol. 353, p. i1585, 2016.</note>
</biblStruct>

<biblStruct xml:id="b226">
	<analytic>
		<title level="a" type="main" xml:id="_cjGG7Mp">Mimic-iii, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Li-Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BYaVjjb">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. E. Johnson, T. J. Pollard, L. Shen, H. L. Li-wei, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. A. Celi, and R. G. Mark, &quot;Mimic-iii, a freely accessible critical care database,&quot; Scientific Data, vol. 3, p. 160035, 2016.</note>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main" xml:id="_QNcUwup">Individualized sepsis treatment using reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Saria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SuChrR9">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1641</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Saria, &quot;Individualized sepsis treatment using reinforcement learn- ing,&quot; Nature medicine, vol. 24, no. 11, p. 1641, 2018.</note>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main" xml:id="_MxF8ASr">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v30i1.10295</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sNJ4PBm">AAAI</title>
		<meeting><address><addrLine>Phoenix, AZ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Van Hasselt, A. Guez, and D. Silver, &quot;Deep reinforcement learning with double q-learning.&quot; in AAAI, vol. 2. Phoenix, AZ, 2016, p. 5.</note>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main" xml:id="_Au3GgVm">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tkz8nGP">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas, &quot;Dueling network architectures for deep reinforcement learning,&quot; in International Conference on Machine Learning, 2016, pp. 1995-2003.</note>
</biblStruct>

<biblStruct xml:id="b230">
	<monogr>
		<title level="m" type="main" xml:id="_zNRvXDN">Prioritized experience replay</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">T. Schaul, J. Quan, I. Antonoglou, and D. Silver, &quot;Prioritized experience replay,&quot; arXiv preprint arXiv:1511.05952, 2015.</note>
</biblStruct>

<biblStruct xml:id="b231">
	<monogr>
		<title level="m" type="main" xml:id="_8ATDvkU">Proximal policy optimization algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, &quot;Prox- imal policy optimization algorithms,&quot; arXiv preprint arXiv:1707.06347, 2017.</note>
</biblStruct>

<biblStruct xml:id="b232">
	<monogr>
		<title level="m" type="main" xml:id="_hh3aPjD">Continuous control with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, &quot;Continuous control with deep reinforce- ment learning,&quot; arXiv preprint arXiv:1509.02971, 2015.</note>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main" xml:id="_fRJJwbx">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kSt4Xh7">ICML</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Y. Ng, D. Harada, and S. Russell, &quot;Policy invariance under reward transformations: Theory and application to reward shaping,&quot; in ICML, vol. 99, 1999, pp. 278-287.</note>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main" xml:id="_neTUbCh">Clinical decision support and closed-loop control for intensive care unit sedation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tannenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ecmUPEr">Asian Journal of Control</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1343" to="1350" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">W. M. Haddad, J. M. Bailey, B. Gholami, and A. R. Tannenbaum, &quot;Clinical decision support and closed-loop control for intensive care unit sedation,&quot; Asian Journal of Control, vol. 20, no. 5, pp. 1343-1350, 2012.</note>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main" xml:id="_GMY4Xan">A data-driven approach to optimized medication dosing: a focus on heparin</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Eche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Danziger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nkMYTWV">Intensive Care Medicine</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1332" to="1339" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. M. Ghassemi, S. E. Richter, I. M. Eche, T. W. Chen, J. Danziger, and L. A. Celi, &quot;A data-driven approach to optimized medication dosing: a focus on heparin,&quot; Intensive Care Medicine, vol. 40, no. 9, pp. 1332- 1339, 2014.</note>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main" xml:id="_WcADXbY">The intensive care medicine research agenda for airways, invasive and noninvasive mechanical ventilation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bellani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blanch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demoule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gattinoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guérin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Laffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Maggiore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_8ARWEB2">Intensive Care Medicine</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Jaber, G. Bellani, L. Blanch, A. Demoule, A. Esteban, L. Gattinoni, C. Guérin, N. Hill, J. G. Laffey, S. M. Maggiore et al., &quot;The intensive care medicine research agenda for airways, invasive and noninvasive mechanical ventilation,&quot; Intensive Care Medicine, vol. 43, no. 9, pp. 1352-1365, 2017.</note>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main" xml:id="_a6rTcfG">Focus on ventilation and airway management in the icu</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">De</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Citerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jaber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nDWpA86">Intensive Care Medicine</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1912" to="1915" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. De Jong, G. Citerio, and S. Jaber, &quot;Focus on ventilation and airway management in the icu,&quot; Intensive Care Medicine, vol. 43, no. 12, pp. 1912-1915, 2017.</note>
</biblStruct>

<biblStruct xml:id="b238">
	<monogr>
		<author>
			<orgName type="collaboration">E.</orgName>
		</author>
		<title level="m" xml:id="_eCCFR8F">Improving diagnosis in health care</title>
		<imprint>
			<publisher>National Academies Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>National Academies of Sciences, Medicine et al.</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">E. National Academies of Sciences, Medicine et al., Improving diag- nosis in health care. National Academies Press, 2016.</note>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main" xml:id="_aSQMj2B">A review on use of machine learning techniques in diagnostic health-care</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VT7fpvN">Artificial Intelligent Systems and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="102" to="107" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. K. Rai and K. Sowmya, &quot;A review on use of machine learning techniques in diagnostic health-care,&quot; Artificial Intelligent Systems and Machine Learning, vol. 10, no. 4, pp. 102-107, 2018.</note>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main" xml:id="_3gMMZMF">Survey of machine learning algorithms for disease diagnostic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fatima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_X96Yp63">Journal of Intelligent Learning Systems and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Fatima and M. Pasha, &quot;Survey of machine learning algorithms for disease diagnostic,&quot; Journal of Intelligent Learning Systems and Applications, vol. 9, no. 01, p. 1, 2017.</note>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main" xml:id="_tbH6DS9">Disease diagnosis in smart healthcare: Innovation, technologies and applications</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Chui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Alhalabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S H</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O D</forename><surname>Pablos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZaM63Ek">Sustainability</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">2309</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. T. Chui, W. Alhalabi, S. S. H. Pang, P. O. d. Pablos, R. W. Liu, and M. Zhao, &quot;Disease diagnosis in smart healthcare: Innovation, technologies and applications,&quot; Sustainability, vol. 9, no. 12, p. 2309, 2017.</note>
</biblStruct>

<biblStruct xml:id="b242">
	<monogr>
		<title level="m" type="main" xml:id="_zEAddAx">Learning to diagnose with lstm recurrent neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wetzel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03677</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Z. C. Lipton, D. C. Kale, C. Elkan, and R. Wetzel, &quot;Learning to diagnose with lstm recurrent neural networks,&quot; arXiv preprint arXiv:1511.03677, 2015.</note>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main" xml:id="_h8vPDV2">Retain: An interpretable predictive model for healthcare using reverse time attention mechanism</title>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SKEWee9">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3504" to="3512" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, and W. Stewart, &quot;Retain: An interpretable predictive model for healthcare using reverse time attention mechanism,&quot; in Advances in Neural Information Pro- cessing Systems, 2016, pp. 3504-3512.</note>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main" xml:id="_7cPwAnH">Medical question answering for clinical decision support</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Goodwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_HxpTxqZ">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. R. Goodwin and S. M. Harabagiu, &quot;Medical question answering for clinical decision support,&quot; in Proceedings of the 25th ACM Inter- national on Conference on Information and Knowledge Management. ACM, 2016, pp. 297-306.</note>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main" xml:id="_c8Kx76j">Diagnostic inferencing via improving clinical concept extraction with deep reinforcement learning: A preliminary study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3zSVgMc">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="271" to="285" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Ling, S. A. Hasan, V. Datla, A. Qadir, K. Lee, J. Liu, and O. Farri, &quot;Diagnostic inferencing via improving clinical concept extraction with deep reinforcement learning: A preliminary study,&quot; in Machine Learn- ing for Healthcare Conference, 2017, pp. 271-285.</note>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main" xml:id="_7sh6SJ7">Reinforcement learning in computer vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_fjJNCpC">CMV</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">106961S</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Bernstein and E. Burnaev, &quot;Reinforcement learning in computer vision,&quot; in CMV&apos;17, vol. 10696. International Society for Optics and Photonics, 2018, p. 106961S.</note>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main" xml:id="_JwMJZqX">A reinforcement learning framework for parameter control in computer vision applications</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5vvbrZF">Proceedings. First Canadian Conference on</title>
		<meeting>First Canadian Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
	<note>Computer and Robot Vision</note>
	<note type="raw_reference">G. W. Taylor, &quot;A reinforcement learning framework for parameter control in computer vision applications,&quot; in Computer and Robot Vision, 2004. Proceedings. First Canadian Conference on. IEEE, 2004, pp. 496-503.</note>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main" xml:id="_jKcR9FK">A reinforcement learning framework for medical image segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sahba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Tizhoosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Salama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_gVvUPgQ">IJCNN</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="511" to="517" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Sahba, H. R. Tizhoosh, and M. M. Salama, &quot;A reinforcement learning framework for medical image segmentation,&quot; in IJCNN, vol. 6, 2006, pp. 511-517.</note>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main" xml:id="_kXuKEzN">Application of opposition-based reinforcement learning in image segmentation</title>
	</analytic>
	<monogr>
		<title level="m" xml:id="_AWRsF3V">2007 IEEE Symposium on Computational Intelligence in Image and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="246" to="251" />
		</imprint>
	</monogr>
	<note type="raw_reference">--, &quot;Application of opposition-based reinforcement learning in image segmentation,&quot; in 2007 IEEE Symposium on Computational Intelligence in Image and Signal Processing. IEEE, 2007, pp. 246- 251.</note>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main" xml:id="_WQXFynu">Application of reinforcement learning for segmentation of transrectal ultrasound images</title>
	</analytic>
	<monogr>
		<title level="j" xml:id="_srNA7tQ">BMC Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">--, &quot;Application of reinforcement learning for segmentation of transrectal ultrasound images,&quot; BMC Medical Imaging, vol. 8, no. 1, p. 8, 2008.</note>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main" xml:id="_ZvkUqmP">Object segmentation in image sequences using reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BRVNJ4j">CSCI&apos;16. IEEE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1416" to="1417" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Sahba, &quot;Object segmentation in image sequences using reinforce- ment learning,&quot; in CSCI&apos;16. IEEE, 2016, pp. 1416-1417.</note>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main" xml:id="_vgHKv3P">Deep reinforcement learning for surgical gesture segmentation and classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_bZc9tn8">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="247" to="255" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Liu and T. Jiang, &quot;Deep reinforcement learning for surgical gesture segmentation and classification,&quot; in International Conference on Med- ical Image Computing and Computer-Assisted Intervention. Springer, 2018, pp. 247-255.</note>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main" xml:id="_NGYtrbf">An artificial agent for anatomical landmark detection in medical images</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ar6vuQG">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="229" to="237" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. C. Ghesu, B. Georgescu, T. Mansi, D. Neumann, J. Hornegger, and D. Comaniciu, &quot;An artificial agent for anatomical landmark detection in medical images,&quot; in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2016, pp. 229-237.</note>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main" xml:id="_gzASFDP">Multi-scale deep reinforcement learning for realtime 3d-landmark detection in ct scans</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9Nhv4HU">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. C. Ghesu, B. Georgescu, Y. Zheng, S. Grbic, A. Maier, J. Hornegger, and D. Comaniciu, &quot;Multi-scale deep reinforcement learning for real- time 3d-landmark detection in ct scans,&quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017.</note>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main" xml:id="_UEg88WF">Towards intelligent robust detection of anatomical structures in incomplete volumetric data</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zKz4ZNP">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="203" to="213" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. C. Ghesu, B. Georgescu, S. Grbic, A. Maier, J. Hornegger, and D. Comaniciu, &quot;Towards intelligent robust detection of anatomical structures in incomplete volumetric data,&quot; Medical Image Analysis, vol. 48, pp. 203-213, 2018.</note>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main" xml:id="_ZUPK6QV">Nonlinear adaptively learned optimization for object localization in 3d medical images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Etcheverry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Odry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mariappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_v7mVZVG">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="254" to="262" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Etcheverry, B. Georgescu, B. Odry, T. J. Re, S. Kaushik, B. Geiger, N. Mariappan, S. Grbic, and D. Comaniciu, &quot;Nonlinear adaptively learned optimization for object localization in 3d medical images,&quot; in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer, 2018, pp. 254-262.</note>
</biblStruct>

<biblStruct xml:id="b257">
	<monogr>
		<title level="m" type="main" xml:id="_jAc5zMs">Evaluating reinforcement learning agents for anatomical landmark detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alansary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Le Folgoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Alansary, O. Oktay, Y. Li, L. Le Folgoc, B. Hou, G. Vaillant, B. Glocker, B. Kainz, and D. Rueckert, &quot;Evaluating reinforcement learning agents for anatomical landmark detection,&quot; 2018.</note>
</biblStruct>

<biblStruct xml:id="b258">
	<monogr>
		<title level="m" type="main" xml:id="_EQpfxae">Automatic view planning with multi-scale deep reinforcement learning agents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alansary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Folgoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03228</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Alansary, L. L. Folgoc, G. Vaillant, O. Oktay, Y. Li, W. Bai, J. Passerat-Palmbach, R. Guerrero, K. Kamnitsas, B. Hou et al., &quot;Automatic view planning with multi-scale deep reinforcement learning agents,&quot; arXiv preprint arXiv:1806.03228, 2018.</note>
</biblStruct>

<biblStruct xml:id="b259">
	<monogr>
		<title level="m" type="main" xml:id="_ZJJCwSD">Partial policy-based reinforcement learning for anatomical landmark localization in 3d medical images</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">D</forename><surname>Yun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02908</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">W. A. Al and I. D. Yun, &quot;Partial policy-based reinforcement learning for anatomical landmark localization in 3d medical images,&quot; arXiv preprint arXiv:1807.02908, 2018.</note>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<title level="a" type="main" xml:id="_EDdP6w4">An artificial agent for robust image registration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Tournemire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kamen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Vng3tbt">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4168" to="4175" />
		</imprint>
	</monogr>
	<note type="raw_reference">R. Liao, S. Miao, P. de Tournemire, S. Grbic, A. Kamen, T. Mansi, and D. Comaniciu, &quot;An artificial agent for robust image registration.&quot; in AAAI, 2017, pp. 4168-4175.</note>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main" xml:id="_dX7FrVw">Multimodal image registration with deep context reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tamersoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_97jPyCa">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Ma, J. Wang, V. Singh, B. Tamersoy, Y.-J. Chang, A. Wimmer, and T. Chen, &quot;Multimodal image registration with deep context re- inforcement learning,&quot; in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2017, pp. 240-248.</note>
</biblStruct>

<biblStruct xml:id="b262">
	<analytic>
		<title level="a" type="main" xml:id="_Yngn9Ba">Robust non-rigid registration through agent-based action learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delingette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kamen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rdaSd3j">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="344" to="352" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Krebs, T. Mansi, H. Delingette, L. Zhang, F. C. Ghesu, S. Miao, A. K. Maier, N. Ayache, R. Liao, and A. Kamen, &quot;Robust non-rigid registra- tion through agent-based action learning,&quot; in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2017, pp. 344-352.</note>
</biblStruct>

<biblStruct xml:id="b263">
	<analytic>
		<title level="a" type="main" xml:id="_whqyQa5">Deep reinforcement learning for active breast lesion detection from dce-mri</title>
		<author>
			<persName><forename type="first">G</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_HnEycmp">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="665" to="673" />
		</imprint>
	</monogr>
	<note type="raw_reference">G. Maicas, G. Carneiro, A. P. Bradley, J. C. Nascimento, and I. Reid, &quot;Deep reinforcement learning for active breast lesion detection from dce-mri,&quot; in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2017, pp. 665-673.</note>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main" xml:id="_xqWetZQ">Deep reinforcement learning for vessel centerline tracing in multi-modality 3d volumes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_g8M77AR">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="755" to="763" />
		</imprint>
	</monogr>
	<note type="raw_reference">P. Zhang, F. Wang, and Y. Zheng, &quot;Deep reinforcement learning for vessel centerline tracing in multi-modality 3d volumes,&quot; in Inter- national Conference on Medical Image Computing and Computer- Assisted Intervention. Springer, 2018, pp. 755-763.</note>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main" xml:id="_PRnW6jJ">Application on reinforcement learning for diagnosis based on medical image</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M B</forename><surname>Netto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R C</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>De Paiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Almeida Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_k9rcMZr">Reinforcement Learning</title>
		<imprint>
			<publisher>InTech</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. M. B. Netto, V. R. C. Leite, A. C. Silva, A. C. de Paiva, and A. de Almeida Neto, &quot;Application on reinforcement learning for diag- nosis based on medical image,&quot; in Reinforcement Learning. InTech, 2008.</note>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main" xml:id="_4YCVvx8">Lead: a methodology for learning efficient approaches to medical diagnosis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Fakih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jVEp8Pb">IEEE Transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="220" to="228" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. J. Fakih and T. K. Das, &quot;Lead: a methodology for learning efficient approaches to medical diagnosis,&quot; IEEE Transactions on Information Technology in Biomedicine, vol. 10, no. 2, pp. 220-228, 2006.</note>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main" xml:id="_5eUE4Td">Overview of the trec 2016 clinical decision support track</title>
		<author>
			<persName><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Hersh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hxjURqP">TREC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Roberts, M. S. Simpson, E. M. Voorhees, and W. R. Hersh, &quot;Overview of the trec 2016 clinical decision support track.&quot; in TREC, 2016.</note>
</biblStruct>

<biblStruct xml:id="b268">
	<analytic>
		<title level="a" type="main" xml:id="_HnDxaVG">Learning to diagnose: Assimilating clinical narratives using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Datla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Farri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6eUYpTd">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<title level="s" xml:id="_bnYQnHR">Long Papers</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="895" to="905" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Ling, S. A. Hasan, V. Datla, A. Qadir, K. Lee, J. Liu, and O. Farri, &quot;Learning to diagnose: Assimilating clinical narratives using deep reinforcement learning,&quot; in Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), vol. 1, 2017, pp. 895-905.</note>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main" xml:id="_jYGbn4w">Breast cancer surveillance consortium: a national mammography screening and outcomes database</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ballard-Barbash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Taplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Yankaskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Ernster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Barlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Geller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kerlikowske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_k46wWaN">American Journal of Roentgenology</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1001" to="1008" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. Ballard-Barbash, S. H. Taplin, B. C. Yankaskas, V. L. Ernster, R. D. Rosenberg, P. A. Carney, W. E. Barlow, B. M. Geller, K. Kerlikowske, B. K. Edwards al., &quot;Breast cancer surveillance consortium: a national mammography screening and outcomes database.&quot; American Journal of Roentgenology, vol. 169, no. 4, pp. 1001-1008, 1997.</note>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main" xml:id="_6kTEHFQ">An adaptive online learning framework for practical breast cancer diagnosis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_f8RXPTP">Computer-Aided Diagnosis</title>
		<imprint>
			<biblScope unit="volume">9785</biblScope>
			<biblScope unit="page">978524</biblScope>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>International Society for Optics and Photonics</publisher>
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
	<note type="raw_reference">T. Chu, J. Wang, and J. Chen, &quot;An adaptive online learning framework for practical breast cancer diagnosis,&quot; in Medical Imaging 2016: Computer-Aided Diagnosis, vol. 9785. International Society for Optics and Photonics, 2016, p. 978524.</note>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main" xml:id="_qFEhcgD">Inquire and diagnose: Neural symptom checking ensemble using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5wxCwG9">Proceedings of NIPS Workshop on Deep Reinforcement Learning</title>
		<meeting>NIPS Workshop on Deep Reinforcement Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K.-F. Tang, H.-C. Kao, C.-N. Chou, and E. Y. Chang, &quot;Inquire and di- agnose: Neural symptom checking ensemble using deep reinforcement learning,&quot; in Proceedings of NIPS Workshop on Deep Reinforcement Learning, 2016.</note>
</biblStruct>

<biblStruct xml:id="b272">
	<monogr>
		<title level="m" type="main" xml:id="_rMpssQv">Context-aware symptom checking for disease diagnosis using hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H.-C. Kao, K.-F. Tang, and E. Y. Chang, &quot;Context-aware symptom checking for disease diagnosis using hierarchical reinforcement learn- ing,&quot; 2018.</note>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main" xml:id="_AKyM9MH">Artificial intelligence in xprize deepq tricorder</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><forename type="middle">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Chou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7xtEa6D">Proceedings of the 2nd International Workshop on Multimedia for Personal Health and Health Care</title>
		<meeting>the 2nd International Workshop on Multimedia for Personal Health and Health Care</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. Y. Chang, M.-H. Wu, K.-F. T. Tang, H.-C. Kao, and C.-N. Chou, &quot;Artificial intelligence in xprize deepq tricorder,&quot; in Proceedings of the 2nd International Workshop on Multimedia for Personal Health and Health Care. ACM, 2017, pp. 11-18.</note>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main" xml:id="_m9dhJd8">Deepq: Advancing healthcare through artificial intelligence and virtual reality</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XQrfN7J">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1068" to="1068" />
		</imprint>
	</monogr>
	<note type="raw_reference">E. Y. Chang, &quot;Deepq: Advancing healthcare through artificial intel- ligence and virtual reality,&quot; in Proceedings of the 2017 ACM on Multimedia Conference. ACM, 2017, pp. 1068-1068.</note>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main" xml:id="_V9tbmey">Task-oriented dialogue system for automatic diagnosis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sudf7rE">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s" xml:id="_RbJkS78">Short Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Wei, Q. Liu, B. Peng, H. Tou, T. Chen, X. Huang, K.-F. Wong, and X. Dai, &quot;Task-oriented dialogue system for automatic diagnosis,&quot; in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), vol. 2, 2018, pp. 201-207.</note>
</biblStruct>

<biblStruct xml:id="b276">
	<monogr>
		<title level="m" type="main" xml:id="_mpBs3wr">Improving mild cognitive impairment prediction via reinforcement learning and dialogue simulation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Uchendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06428</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">F. Tang, K. Lin, I. Uchendu, H. H. Dodge, and J. Zhou, &quot;Improving mild cognitive impairment prediction via reinforcement learning and dialogue simulation,&quot; arXiv preprint arXiv:1802.06428, 2018.</note>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main" xml:id="_Hr5gfPH">Approximate dynamic programming for capacity allocation in the service industry</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kolisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KVXJxGC">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">218</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="250" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H.-J. Schuetz and R. Kolisch, &quot;Approximate dynamic programming for capacity allocation in the service industry,&quot; European Journal of Operational Research, vol. 218, no. 1, pp. 239-250, 2012.</note>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main" xml:id="_R2wDWNg">Reinforcement learning based resource allocation in business process management</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_UZR8qhF">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="127" to="145" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Huang, W. M. van der Aalst, X. Lu, and H. Duan, &quot;Reinforcement learning based resource allocation in business process management,&quot; Data &amp; Knowledge Engineering, vol. 70, no. 1, pp. 127-145, 2011.</note>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main" xml:id="_eKCE7Vb">Clinic scheduling models with overbooking for patients with heterogeneous no-show probabilities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Turkcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6mmpmUK">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="144" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. Zeng, A. Turkcan, J. Lin, and M. Lawley, &quot;Clinic scheduling models with overbooking for patients with heterogeneous no-show probabilities,&quot; Annals of Operations Research, vol. 178, no. 1, pp. 121- 144, 2010.</note>
</biblStruct>

<biblStruct xml:id="b280">
	<monogr>
		<title level="m" type="main" xml:id="_BTwvgF2">Reinforcement learning for primary care e appointment scheduling</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S M T</forename><surname>Gomes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. S. M. T. Gomes, &quot;Reinforcement learning for primary care e appointment scheduling,&quot; 2017.</note>
</biblStruct>

<biblStruct xml:id="b281">
	<analytic>
		<title level="a" type="main" xml:id="_2tPMftp">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_dRkRDyQ">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
	<note type="raw_reference">V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, &quot;Asynchronous methods for deep reinforcement learning,&quot; in ICML, 2016, pp. 1928-1937.</note>
</biblStruct>

<biblStruct xml:id="b282">
	<monogr>
		<title level="m" type="main" xml:id="_NqpMqgD">A function approximation method for modelbased high-dimensional inverse reinforcement learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Burdick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07738</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">K. Li and J. W. Burdick, &quot;A function approximation method for model- based high-dimensional inverse reinforcement learning,&quot; arXiv preprint arXiv:1708.07738, 2017.</note>
</biblStruct>

<biblStruct xml:id="b283">
	<analytic>
		<title level="a" type="main" xml:id="_vchtBB2">Multilateral surgical pattern cutting in 2d orthotropic gauze with deep reinforcement learning policies for tensioning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Thananjeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SEU9RnX">IEEE ICRA&apos;17</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2371" to="2378" />
		</imprint>
	</monogr>
	<note type="raw_reference">B. Thananjeyan, A. Garg, S. Krishnan, C. Chen, L. Miller, and K. Goldberg, &quot;Multilateral surgical pattern cutting in 2d orthotropic gauze with deep reinforcement learning policies for tensioning,&quot; in IEEE ICRA&apos;17. IEEE, 2017, pp. 2371-2378.</note>
</biblStruct>

<biblStruct xml:id="b284">
	<monogr>
		<title level="m" type="main" xml:id="_bPME8dU">A new tensioning method using deep reinforcement learning for surgical pattern cutting</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03327</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">T. T. Nguyen, N. D. Nguyen, F. Bello, and S. Nahavandi, &quot;A new tensioning method using deep reinforcement learning for surgical pattern cutting,&quot; arXiv preprint arXiv:1901.03327, 2019.</note>
</biblStruct>

<biblStruct xml:id="b285">
	<analytic>
		<title level="a" type="main" xml:id="_gBaa34m">Towards transferring skills to flexible surgical robots with programming by demonstration and reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_QQW8NZB">ICACI&apos;16</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="378" to="384" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Chen, H. Y. Lau, W. Xu, and H. Ren, &quot;Towards transferring skills to flexible surgical robots with programming by demonstration and reinforcement learning,&quot; in ICACI&apos;16. IEEE, 2016, pp. 378-384.</note>
</biblStruct>

<biblStruct xml:id="b286">
	<analytic>
		<title level="a" type="main" xml:id="_6xmaNVJ">Path planning for automation of surgery robot based on probabilistic roadmap and reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_dfbaTf6">2018 15th International Conference on Ubiquitous Robots (UR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="342" to="347" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Baek, M. Hwang, H. Kim, and D.-S. Kwon, &quot;Path planning for automation of surgery robot based on probabilistic roadmap and reinforcement learning,&quot; in 2018 15th International Conference on Ubiquitous Robots (UR). IEEE, 2018, pp. 342-347.</note>
</biblStruct>

<biblStruct xml:id="b287">
	<analytic>
		<title level="a" type="main" xml:id="_Pu8qeP5">Inverse reinforcement learning via function approximation for clinical motion analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Burdick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jBfeNCf">IEEE ICRA&apos;18</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="610" to="617" />
		</imprint>
	</monogr>
	<note type="raw_reference">K. Li, M. Rath, and J. W. Burdick, &quot;Inverse reinforcement learning via function approximation for clinical motion analysis,&quot; in IEEE ICRA&apos;18. IEEE, 2018, pp. 610-617.</note>
</biblStruct>

<biblStruct xml:id="b288">
	<analytic>
		<title level="a" type="main" xml:id="_Khj5knP">Training an actor-critic reinforcement learning controller for arm movement using human-generated rewards</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Jagodnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Den Bogert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Branicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Kirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Jr6xR79">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1892" to="1905" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. M. Jagodnik, P. S. Thomas, A. J. van den Bogert, M. S. Bran- icky, and R. F. Kirsch, &quot;Training an actor-critic reinforcement learn- ing controller for arm movement using human-generated rewards,&quot; IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 25, no. 10, pp. 1892-1905, 2017.</note>
</biblStruct>

<biblStruct xml:id="b289">
	<analytic>
		<title level="a" type="main" xml:id="_4qr4m4k">Medical qos provision based on reinforcement learning in ultrasound streaming over 3.5 g wireless systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Istepanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Martini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_37PPgN3">IEEE Journal on Selected areas in Communications</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">R. S. Istepanian, N. Y. Philip, and M. G. Martini, &quot;Medical qos provision based on reinforcement learning in ultrasound streaming over 3.5 g wireless systems,&quot; IEEE Journal on Selected areas in Communications, vol. 27, no. 4, 2009.</note>
</biblStruct>

<biblStruct xml:id="b290">
	<analytic>
		<title level="a" type="main" xml:id="_bvsgY9E">Cross-layer ultrasound video streaming over mobile wimax and hsupa networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alinejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Istepanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4EEUVsV">IEEE transactions on Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="39" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Alinejad, N. Y. Philip, and R. S. Istepanian, &quot;Cross-layer ultrasound video streaming over mobile wimax and hsupa networks,&quot; IEEE transactions on Information Technology in Biomedicine, vol. 16, no. 1, pp. 31-39, 2012.</note>
</biblStruct>

<biblStruct xml:id="b291">
	<analytic>
		<title level="a" type="main" xml:id="_A5BP4qp">Functional electrical stimulation after spinal cord injury: current use, therapeutic effects and future directions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ragnarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xaC3nb5">Spinal cord</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">255</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">K. Ragnarsson, &quot;Functional electrical stimulation after spinal cord injury: current use, therapeutic effects and future directions,&quot; Spinal cord, vol. 46, no. 4, p. 255, 2008.</note>
</biblStruct>

<biblStruct xml:id="b292">
	<analytic>
		<title level="a" type="main" xml:id="_xK4XRsb">Creating a reinforcement learning controller for functional electrical stimulation of a human arm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Branicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bogert</surname></persName>
		</author>
		<author>
			<persName><surname>Jagodnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_x6Bsat9">The Yale Workshop on Adaptive and Learning Systems</title>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">49326</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">P. S. Thomas, M. Branicky, A. Van Den Bogert, and K. Jagodnik, &quot;Creating a reinforcement learning controller for functional electrical stimulation of a human arm,&quot; in The Yale Workshop on Adaptive and Learning Systems, vol. 49326. NIH Public Access, 2008, p. 1.</note>
</biblStruct>

<biblStruct xml:id="b293">
	<analytic>
		<title level="a" type="main" xml:id="_CfdBfmy">Application of the actor-critic architecture to functional electrical stimulation control of a human arm</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Den Bogert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Jagodnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Branicky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_V39DuXj">IAAI</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">P. S. Thomas, A. J. van den Bogert, K. M. Jagodnik, and M. S. Branicky, &quot;Application of the actor-critic architecture to functional electrical stimulation control of a human arm.&quot; in IAAI, 2009.</note>
</biblStruct>

<biblStruct xml:id="b294">
	<analytic>
		<title level="a" type="main" xml:id="_xz2pqUa">Can the pharmaceutical industry reduce attrition rates?</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Landis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_svusfrM">Nature reviews Drug discovery</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">711</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">I. Kola and J. Landis, &quot;Can the pharmaceutical industry reduce attrition rates?&quot; Nature reviews Drug discovery, vol. 3, no. 8, p. 711, 2004.</note>
</biblStruct>

<biblStruct xml:id="b295">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
		<title level="m" xml:id="_ruhVG94">De novo molecular design</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Schneider, De novo molecular design. John Wiley &amp; Sons, 2013.</note>
</biblStruct>

<biblStruct xml:id="b296">
	<analytic>
		<title level="a" type="main" xml:id="_mSmhUDn">Molecular de-novo design through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YbdsecY">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Olivecrona, T. Blaschke, O. Engkvist, and H. Chen, &quot;Molecular de-novo design through deep reinforcement learning,&quot; Journal of Cheminformatics, vol. 9, no. 1, p. 48, 2017.</note>
</biblStruct>

<biblStruct xml:id="b297">
	<analytic>
		<title level="a" type="main" xml:id="_Z4yXVTZ">Accelerating drugs discovery with deep reinforcement learning: An early approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Imbernón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pérez-Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cecilia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bueno-Crespo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Abellán</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4MwgfVE">Proceedings of the 47th International Conference on Parallel Processing Companion</title>
		<meeting>the 47th International Conference on Parallel Processing Companion</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Serrano, B. Imbernón, H. Pérez-Sánchez, J. M. Cecilia, A. Bueno- Crespo, and J. L. Abellán, &quot;Accelerating drugs discovery with deep reinforcement learning: An early approach,&quot; in Proceedings of the 47th International Conference on Parallel Processing Companion. ACM, 2018, p. 6.</note>
</biblStruct>

<biblStruct xml:id="b298">
	<monogr>
		<title level="m" type="main" xml:id="_qAQHwt7">Exploring deep recurrent models with reinforcement learning for molecule design</title>
		<author>
			<persName><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sellwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Neil, M. Segler, L. Guasch, M. Ahmed, D. Plumbley, M. Sellwood, and N. Brown, &quot;Exploring deep recurrent models with reinforcement learning for molecule design,&quot; 2018.</note>
</biblStruct>

<biblStruct xml:id="b299">
	<analytic>
		<title level="a" type="main" xml:id="_fjyvRRf">Deep reinforcement learning for de novo drug design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Popova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tropsha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_H7xqbCE">Science Advances</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7885</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Popova, O. Isayev, and A. Tropsha, &quot;Deep reinforcement learning for de novo drug design,&quot; Science Advances, vol. 4, no. 7, p. eaap7885, 2018.</note>
</biblStruct>

<biblStruct xml:id="b300">
	<analytic>
		<title level="a" type="main" xml:id="_FwjRPN6">Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozdoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qbKWPWz">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. Yom-Tov, G. Feraru, M. Kozdoba, S. Mannor, M. Tennenholtz, and I. Hochberg, &quot;Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system,&quot; Journal of Medical Internet Research, vol. 19, no. 10, 2017.</note>
</biblStruct>

<biblStruct xml:id="b301">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozdoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.04070</idno>
		<title level="m" xml:id="_hj9EAP6">A reinforcement learning system to encourage physical activity in diabetes patients</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">I. Hochberg, G. Feraru, M. Kozdoba, S. Mannor, M. Tennenholtz, and E. Yom-Tov, &quot;A reinforcement learning system to encourage physical activity in diabetes patients,&quot; arXiv preprint arXiv:1605.04070, 2016.</note>
</biblStruct>

<biblStruct xml:id="b302">
	<analytic>
		<title level="a" type="main" xml:id="_Xke9t3P">Adaptive interventions treatment modelling and regimen optimization using sequential multiple assignment randomized trials (smart) and q-learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baniya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5NyVkhD">Proceedings of IIE Annual Conference</title>
		<meeting>IIE Annual Conference</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1187" to="1192" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Baniya, S. Herrmann, Q. Qiao, and H. Lu, &quot;Adaptive interven- tions treatment modelling and regimen optimization using sequential multiple assignment randomized trials (smart) and q-learning,&quot; in Proceedings of IIE Annual Conference, 2017, pp. 1187-1192.</note>
</biblStruct>

<biblStruct xml:id="b303">
	<analytic>
		<title level="a" type="main" xml:id="_wqYaztJ">Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Forman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kerrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Butryn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Juarascio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Dallal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Crochiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5Gd4VYA">Journal of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. M. Forman, S. G. Kerrigan, M. L. Butryn, A. S. Juarascio, S. M. Manasse, S. Ontañón, D. H. Dallal, R. J. Crochiere, and D. Moskow, &quot;Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss?&quot; Journal of Behavioral Medicine, pp. 1-15, 2018.</note>
</biblStruct>

<biblStruct xml:id="b304">
	<monogr>
		<title level="m" type="main" xml:id="_ze63AWM">Evaluating reinforcement learning algorithms in observational health settings</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wihl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12298</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">O. Gottesman, F. Johansson, J. Meier, J. Dent, D. Lee, S. Srinivasan, L. Zhang, Y. Ding, D. Wihl, X. Peng et al., &quot;Evaluating reinforcement learning algorithms in observational health settings,&quot; arXiv preprint arXiv:1805.12298, 2018.</note>
</biblStruct>

<biblStruct xml:id="b305">
	<monogr>
		<title level="m" type="main" xml:id="_hZ5GPcr">Behaviour policy estimation in off-policy policy evaluation: Calibration matters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gottesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Komorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Faisal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01066</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">A. Raghu, O. Gottesman, Y. Liu, M. Komorowski, A. Faisal, F. Doshi- Velez, and E. Brunskill, &quot;Behaviour policy estimation in off-policy pol- icy evaluation: Calibration matters,&quot; arXiv preprint arXiv:1807.01066, 2018.</note>
</biblStruct>

<biblStruct xml:id="b306">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Jeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shashikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nemati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03271</idno>
		<title level="m" xml:id="_T4Vu2KV">Does the&quot; artificial intelligence clinician&quot; learn optimal treatment strategies for sepsis in intensive care?</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">R. Jeter, C. Josef, S. Shashikumar, and S. Nemati, &quot;Does the&quot; artificial intelligence clinician&quot; learn optimal treatment strategies for sepsis in intensive care?&quot; arXiv preprint arXiv:1902.03271, 2019.</note>
</biblStruct>

<biblStruct xml:id="b307">
	<monogr>
		<title level="m" type="main" xml:id="_XJYZFBE">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Koller, N. Friedman, and F. Bach, Probabilistic graphical models: principles and techniques. MIT press, 2009.</note>
</biblStruct>

<biblStruct xml:id="b308">
	<analytic>
		<title level="a" type="main" xml:id="_QS2mZTj">Precision medicinełpersonalized, problematic, and promising</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Jameson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jPxN493">Obstetrical &amp; Gynecological Survey</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="612" to="614" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. L. Jameson and D. L. Longo, &quot;Precision medicinełpersonalized, problematic, and promising,&quot; Obstetrical &amp; Gynecological Survey, vol. 70, no. 10, pp. 612-614, 2015.</note>
</biblStruct>

<biblStruct xml:id="b309">
	<analytic>
		<title level="a" type="main" xml:id="_FsPK6RN">Preference learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2jr3mYb">Encyclopedia of Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="789" to="795" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Fürnkranz and E. Hüllermeier, &quot;Preference learning,&quot; in Encyclope- dia of Machine Learning. Springer, 2011, pp. 789-795.</note>
</biblStruct>

<biblStruct xml:id="b310">
	<analytic>
		<title level="a" type="main" xml:id="_5JW2b9v">Efficient reinforcement learning with multiple reward functions for randomized controlled trial analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UQVA2m9">ICML&apos;10</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="695" to="702" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. J. Lizotte, M. H. Bowling, and S. A. Murphy, &quot;Efficient reinforce- ment learning with multiple reward functions for randomized controlled trial analysis,&quot; in ICML&apos;10. Citeseer, 2010, pp. 695-702.</note>
</biblStruct>

<biblStruct xml:id="b311">
	<analytic>
		<title level="a" type="main" xml:id="_U56Sb5Y">Inverse reinforcement learning with simultaneous estimation of rewards and dynamics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gindele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-50127-7_45</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_WH9Hh7w">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Herman, T. Gindele, J. Wagner, F. Schmitt, and W. Burgard, &quot;In- verse reinforcement learning with simultaneous estimation of rewards and dynamics,&quot; in Artificial Intelligence and Statistics, 2016, pp. 102- 110.</note>
</biblStruct>

<biblStruct xml:id="b312">
	<analytic>
		<title level="a" type="main" xml:id="_ktU8rY2">Hindsight experience replay</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ebaVdBQ">NIPS&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5048" to="5058" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Andrychowicz, F. Wolski, A. Ray, J. Schneider, R. Fong, P. Welin- der, B. McGrew, J. Tobin, O. P. Abbeel, and W. Zaremba, &quot;Hindsight experience replay,&quot; in NIPS&apos;17, 2017, pp. 5048-5058.</note>
</biblStruct>

<biblStruct xml:id="b313">
	<monogr>
		<title level="m" type="main" xml:id="_NtnzqhQ">Reinforcement learning with unsupervised auxiliary tasks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05397</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Sil- ver, and K. Kavukcuoglu, &quot;Reinforcement learning with unsupervised auxiliary tasks,&quot; arXiv preprint arXiv:1611.05397, 2016.</note>
</biblStruct>

<biblStruct xml:id="b314">
	<analytic>
		<title level="a" type="main" xml:id="_uNUjuwv">Imaginationaugmented agents for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tUsyrPK">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5690" to="5701" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Racanière, T. Weber, D. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li et al., &quot;Imagination- augmented agents for deep reinforcement learning,&quot; in Advances in neural information processing systems, 2017, pp. 5690-5701.</note>
</biblStruct>

<biblStruct xml:id="b315">
	<analytic>
		<title level="a" type="main" xml:id="_RT4fa2P">Doubly robust off-policy value evaluation for reinforcement learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nFHvkPd">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>JMLR. org</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="652" to="661" />
		</imprint>
	</monogr>
	<note type="raw_reference">N. Jiang and L. Li, &quot;Doubly robust off-policy value evaluation for reinforcement learning,&quot; in Proceedings of the 33rd International Conference on International Conference on Machine Learning-Volume 48. JMLR. org, 2016, pp. 652-661.</note>
</biblStruct>

<biblStruct xml:id="b316">
	<analytic>
		<title level="a" type="main" xml:id="_Zj8K6Ma">Near-optimal reinforcement learning in polynomial time</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_RD4Qx8D">Machine learning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="209" to="232" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Kearns and S. Singh, &quot;Near-optimal reinforcement learning in polynomial time,&quot; Machine learning, vol. 49, no. 2-3, pp. 209-232, 2002.</note>
</biblStruct>

<biblStruct xml:id="b317">
	<analytic>
		<title level="a" type="main" xml:id="_wEaD5mV">Efficient pac-optimal exploration in concurrent, continuous state mdps with delayed updates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vqqf9Qd">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1977" to="1985" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Pazis and R. Parr, &quot;Efficient pac-optimal exploration in concurrent, continuous state mdps with delayed updates.&quot; in AAAI, 2016, pp. 1977- 1985.</note>
</biblStruct>

<biblStruct xml:id="b318">
	<analytic>
		<title level="a" type="main" xml:id="_2CUWvet">Coordinated exploration in concurrent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dimakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_acrw9hU">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1270" to="1278" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Dimakopoulou and B. Van Roy, &quot;Coordinated exploration in concur- rent reinforcement learning,&quot; in International Conference on Machine Learning, 2018, pp. 1270-1278.</note>
</biblStruct>

<biblStruct xml:id="b319">
	<analytic>
		<title level="a" type="main" xml:id="_nusQZbW">Concurrent pac rl</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_quDju6W">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2624" to="2630" />
		</imprint>
	</monogr>
	<note type="raw_reference">Z. Guo and E. Brunskill, &quot;Concurrent pac rl.&quot; in AAAI, 2015, pp. 2624- 2630.</note>
</biblStruct>

<biblStruct xml:id="b320">
	<analytic>
		<title level="a" type="main" xml:id="_hhtCtRN">Ex2: Exploration with exemplar models for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8h5rZDx">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2574" to="2584" />
		</imprint>
	</monogr>
	<note type="raw_reference">J. Fu, J. Co-Reyes, and S. Levine, &quot;Ex2: Exploration with exemplar models for deep reinforcement learning,&quot; in Advances in Neural Information Processing Systems, 2017, pp. 2574-2584.</note>
</biblStruct>

<biblStruct xml:id="b321">
	<analytic>
		<title level="a" type="main" xml:id="_uba32pH"># exploration: A study of count-based exploration for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deturck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Xe98gnp">NIPS&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2750" to="2759" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Tang, R. Houthooft, D. Foote, A. Stooke, O. X. Chen, Y. Duan, J. Schulman, F. DeTurck, and P. Abbeel, &quot;# exploration: A study of count-based exploration for deep reinforcement learning,&quot; in NIPS&apos;17, 2017, pp. 2750-2759.</note>
</biblStruct>

<biblStruct xml:id="b322">
	<monogr>
		<title level="m" type="main" xml:id="_NFkEg5N">Incentivizing exploration in reinforcement learning with deep predictive models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00814</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">B. C. Stadie, S. Levine, and P. Abbeel, &quot;Incentivizing exploration in reinforcement learning with deep predictive models,&quot; arXiv preprint arXiv:1507.00814, 2015.</note>
</biblStruct>

<biblStruct xml:id="b323">
	<analytic>
		<title level="a" type="main" xml:id="_ap3DsTK">Safe exploration algorithms for reinforcement learning controllers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mannucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-J</forename><surname>Van Kampen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Visser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dQvgvBm">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1069" to="1081" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Mannucci, E.-J. van Kampen, C. de Visser, and Q. Chu, &quot;Safe exploration algorithms for reinforcement learning controllers,&quot; IEEE transactions on neural networks and learning systems, vol. 29, no. 4, pp. 1069-1081, 2018.</note>
</biblStruct>

<biblStruct xml:id="b324">
	<analytic>
		<title level="a" type="main" xml:id="_cKgP77Q">Causal explanation under indeterminism: A sampling approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Merck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fczsAMY">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1037" to="1043" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. A. Merck and S. Kleinberg, &quot;Causal explanation under indetermin- ism: A sampling approach.&quot; in AAAI, 2016, pp. 1037-1043.</note>
</biblStruct>

<biblStruct xml:id="b325">
	<monogr>
		<title level="m" type="main" xml:id="_pEgsgYg">Making things happen: A theory of causal explanation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Woodward</surname></persName>
		</author>
		<idno type="DOI">10.1093/0195155270.003.0005</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Woodward, Making things happen: A theory of causal explanation. Oxford university press, 2005.</note>
</biblStruct>

<biblStruct xml:id="b326">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winship</surname></persName>
		</author>
		<title level="m" xml:id="_ETZuV9w">Counterfactuals and causal inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. L. Morgan and C. Winship, Counterfactuals and causal inference. Cambridge University Press, 2015.</note>
</biblStruct>

<biblStruct xml:id="b327">
	<analytic>
		<title level="a" type="main" xml:id="_vWrkDZ7">Sequences of mechanisms for causal reasoning in artificial intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Voortman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">De</forename><surname>Jongh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2nPEATS">IJCAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="839" to="845" />
		</imprint>
	</monogr>
	<note type="raw_reference">D. Dash, M. Voortman, and M. De Jongh, &quot;Sequences of mechanisms for causal reasoning in artificial intelligence.&quot; in IJCAI, 2013, pp. 839- 845.</note>
</biblStruct>

<biblStruct xml:id="b328">
	<analytic>
		<title level="a" type="main" xml:id="_zh6ep6d">The mythos of model interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2edvrYG">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Z. C. Lipton, &quot;The mythos of model interpretability,&quot; Communications of the ACM, vol. 61, no. 10, pp. 36-43, 2018.</note>
</biblStruct>

<biblStruct xml:id="b329">
	<monogr>
		<title level="m" type="main" xml:id="_cGxqtsG">Towards mixed optimization for reinforcement learning with program synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00403</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">S. Bhupatiraju, K. K. Agrawal, and R. Singh, &quot;Towards mixed op- timization for reinforcement learning with program synthesis,&quot; arXiv preprint arXiv:1807.00403, 2018.</note>
</biblStruct>

<biblStruct xml:id="b330">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08037</idno>
		<title level="m" xml:id="_qvzJQG9">The doctor just won&apos;t accept that!</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Z. C. Lipton, &quot;The doctor just won&apos;t accept that!&quot; arXiv preprint arXiv:1711.08037, 2017.</note>
</biblStruct>

<biblStruct xml:id="b331">
	<analytic>
		<title level="a" type="main" xml:id="_YagCmf5">Policy search in a space of simple closed-form formulas: towards interpretability of reinforcement learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Maes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonteneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_v68hH8e">International Conference on Discovery Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="37" to="51" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Maes, R. Fonteneau, L. Wehenkel, and D. Ernst, &quot;Policy search in a space of simple closed-form formulas: towards interpretability of reinforcement learning,&quot; in International Conference on Discovery Science. Springer, 2012, pp. 37-51.</note>
</biblStruct>

<biblStruct xml:id="b332">
	<analytic>
		<title level="a" type="main" xml:id="_9CHuHfe">Programmatically interpretable reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4p3Tygq">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5052" to="5061" />
		</imprint>
	</monogr>
	<note type="raw_reference">A. Verma, V. Murali, R. Singh, P. Kohli, and S. Chaudhuri, &quot;Pro- grammatically interpretable reinforcement learning,&quot; in International Conference on Machine Learning, 2018, pp. 5052-5061.</note>
</biblStruct>

<biblStruct xml:id="b333">
	<analytic>
		<title level="a" type="main" xml:id="_3y7ZFfx">Generating explanations based on markov decision processes</title>
		<author>
			<persName><forename type="first">F</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sucar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Noguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reyes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tSDHxhU">Mexican International Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Elizalde, E. Sucar, J. Noguez, and A. Reyes, &quot;Generating explana- tions based on markov decision processes,&quot; in Mexican International Conference on Artificial Intelligence. Springer, 2009, pp. 51-62.</note>
</biblStruct>

<biblStruct xml:id="b334">
	<monogr>
		<title level="m" type="main" xml:id="_375DwRT">Distilling knowledge from deep networks with applications to healthcare domain</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03542</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Z. Che, S. Purushotham, R. Khemani, and Y. Liu, &quot;Distilling knowledge from deep networks with applications to healthcare domain,&quot; arXiv preprint arXiv:1512.03542, 2015.</note>
</biblStruct>

<biblStruct xml:id="b335">
	<analytic>
		<title level="a" type="main" xml:id="_zBrKgPQ">Beyond sparsity: Tree regularization of deep models for interpretability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XTnf3u8">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth, and F. Doshi- Velez, &quot;Beyond sparsity: Tree regularization of deep models for in- terpretability,&quot; in Thirty-Second AAAI Conference on Artificial Intelli- gence, 2018.</note>
</biblStruct>

<biblStruct xml:id="b336">
	<analytic>
		<title level="a" type="main" xml:id="_Vs33RuQ">Incorporating prior knowledge into q-learning for drug delivery individualization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Gaweda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Muezzinoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Aronoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Brier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XJVk7bd">Fourth International Conference on Machine Learning and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">A. E. Gaweda, M. K. Muezzinoglu, G. R. Aronoff, A. A. Jacobs, J. M. Zurada, and M. E. Brier, &quot;Incorporating prior knowledge into q-learning for drug delivery individualization,&quot; in Fourth International Conference on Machine Learning and Applications. IEEE, 2005, pp. 6-pp.</note>
</biblStruct>

<biblStruct xml:id="b337">
	<analytic>
		<title level="a" type="main" xml:id="_DNjSH9P">Interactive machine learning for health informatics: when do we need the human-in-the-loop?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZHQh6Uv">Brain Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="131" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Holzinger, &quot;Interactive machine learning for health informatics: when do we need the human-in-the-loop?&quot; Brain Informatics, vol. 3, no. 2, pp. 119-131, 2016.</note>
</biblStruct>

<biblStruct xml:id="b338">
	<monogr>
		<title level="m" type="main" xml:id="_evbPP2D">Agentagnostic human-in-the-loop reinforcement learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salvatier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04079</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">D. Abel, J. Salvatier, A. Stuhlmüller, and O. Evans, &quot;Agent- agnostic human-in-the-loop reinforcement learning,&quot; arXiv preprint arXiv:1701.04079, 2017.</note>
</biblStruct>

<biblStruct xml:id="b339">
	<analytic>
		<title level="a" type="main" xml:id="_5wEtGAH">High-performance medicine: the convergence of human and artificial intelligence</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jrYZpzg">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">E. J. Topol, &quot;High-performance medicine: the convergence of human and artificial intelligence,&quot; Nature medicine, vol. 25, no. 1, p. 44, 2019.</note>
</biblStruct>

<biblStruct xml:id="b340">
	<analytic>
		<title level="a" type="main" xml:id="_UamJcn3">Adaptively shaping reinforcement learning agents via human reward</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_8D5JrzM">Pacific Rim International Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="85" to="97" />
		</imprint>
	</monogr>
	<note type="raw_reference">C. Yu, D. Wang, T. Yang, W. Zhu, Y. Li, H. Ge, and J. Ren, &quot;Adaptively shaping reinforcement learning agents via human reward,&quot; in Pacific Rim International Conference on Artificial Intelligence. Springer, 2018, pp. 85-97.</note>
</biblStruct>

<biblStruct xml:id="b341">
	<analytic>
		<title level="a" type="main" xml:id="_3fqR2Rw">Policy shaping: Integrating human feedback with reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wQYsfsd">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2625" to="2633" />
		</imprint>
	</monogr>
	<note type="raw_reference">S. Griffith, K. Subramanian, J. Scholz, C. L. Isbell, and A. L. Thomaz, &quot;Policy shaping: Integrating human feedback with reinforcement learn- ing,&quot; in Advances in neural information processing systems, 2013, pp. 2625-2633.</note>
</biblStruct>

<biblStruct xml:id="b342">
	<monogr>
		<title level="m" type="main" xml:id="_GmrxdBc">Small sample learning in big data era</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04572</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">J. Shu, Z. Xu, and D. Meng, &quot;Small sample learning in big data era,&quot; arXiv preprint arXiv:1808.04572, 2018.</note>
</biblStruct>

<biblStruct xml:id="b343">
	<analytic>
		<title level="a" type="main" xml:id="_MMV63kk">Small-sample reinforcement learning: Improving policies using synthetic data 1</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Carden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Livsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xKmxSEn">Intelligent Decision Technologies</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. W. Carden and J. Livsey, &quot;Small-sample reinforcement learning: Improving policies using synthetic data 1,&quot; Intelligent Decision Tech- nologies, vol. 11, no. 2, pp. 167-175, 2017.</note>
</biblStruct>

<biblStruct xml:id="b344">
	<analytic>
		<title level="a" type="main" xml:id="_UMGZgPF">Deep convolutional neural networks and data augmentation for environmental sound classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_McGj2FZ">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="283" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">J. Salamon and J. P. Bello, &quot;Deep convolutional neural networks and data augmentation for environmental sound classification,&quot; IEEE Signal Processing Letters, vol. 24, no. 3, pp. 279-283, 2017.</note>
</biblStruct>

<biblStruct xml:id="b345">
	<analytic>
		<title level="a" type="main" xml:id="_WV9tCtM">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RKwdQqx">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
	<note type="raw_reference">I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, &quot;Generative adversarial nets,&quot; in Advances in neural information processing systems, 2014, pp. 2672- 2680.</note>
</biblStruct>

<biblStruct xml:id="b346">
	<analytic>
		<title level="a" type="main" xml:id="_qWpXqXX">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_nYV5PdT">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Hinton, O. Vinyals, and J. Dean, &quot;Distilling the knowledge in a neural network,&quot; stat, vol. 1050, p. 9, 2015.</note>
</biblStruct>

<biblStruct xml:id="b347">
	<analytic>
		<title level="a" type="main" xml:id="_YRVp8NR">Building machines that learn and think like people</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XyFdxs9">Behavioral and Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman, &quot;Building machines that learn and think like people,&quot; Behavioral and Brain Sciences, vol. 40, 2017.</note>
</biblStruct>

<biblStruct xml:id="b348">
	<analytic>
		<title level="a" type="main" xml:id="_Xy3fSvz">Unobtrusive sensing and wearable devices for health informatics</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C Y</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P L</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7YgcDT2">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1538" to="1554" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y.-L. Zheng, X.-R. Ding, C. C. Y. Poon, B. P. L. Lo, H. Zhang, X.-L. Zhou, G.-Z. Yang, N. Zhao, and Y.-T. Zhang, &quot;Unobtrusive sensing and wearable devices for health informatics,&quot; IEEE Transactions on Biomedical Engineering, vol. 61, no. 5, pp. 1538-1554, 2014.</note>
</biblStruct>

<biblStruct xml:id="b349">
	<analytic>
		<title level="a" type="main" xml:id="_nyXnYCd">A survey on ambient intelligence in healthcare</title>
		<author>
			<persName><forename type="first">G</forename><surname>Acampora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rashidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Vasilakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_F6z44NZ">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2470" to="2494" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">G. Acampora, D. J. Cook, P. Rashidi, and A. V. Vasilakos, &quot;A survey on ambient intelligence in healthcare,&quot; Proceedings of the IEEE, vol. 101, no. 12, pp. 2470-2494, 2013.</note>
</biblStruct>

<biblStruct xml:id="b350">
	<analytic>
		<title level="a" type="main" xml:id="_9CprZUA">Robust actor-critic contextual bandit for mobile health (mhealth) interventions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_3MD4ggV">Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<meeting>the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, J. Guo, R. Li, and J. Huang, &quot;Robust actor-critic contextual bandit for mobile health (mhealth) interventions,&quot; in Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. ACM, 2018, pp. 492-501.</note>
</biblStruct>

<biblStruct xml:id="b351">
	<analytic>
		<title level="a" type="main" xml:id="_qTd4wRn">Groupdriven reinforcement learning for personalized mhealth intervention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MEZH6qh">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="590" to="598" />
		</imprint>
	</monogr>
	<note type="raw_reference">F. Zhu, J. Guo, Z. Xu, P. Liao, L. Yang, and J. Huang, &quot;Group- driven reinforcement learning for personalized mhealth intervention,&quot; in International Conference on Medical Image Computing and Computer- Assisted Intervention. Springer, 2018, pp. 590-598.</note>
</biblStruct>

<biblStruct xml:id="b352">
	<analytic>
		<title level="a" type="main" xml:id="_Dx372nK">An actor-critic contextual bandit algorithm for personalized interventions using mobile devices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bRvCKuk">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">H. Lei, A. Tewari, and S. Murphy, &quot;An actor-critic contextual bandit al- gorithm for personalized interventions using mobile devices,&quot; Advances in Neural Information Processing Systems, vol. 27, 2014.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
