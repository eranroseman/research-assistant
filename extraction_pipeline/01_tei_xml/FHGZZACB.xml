<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_8bUnkgy">Uncovering Bias in Personal Informatics</title>
				<funder>
					<orgName type="full">Scientific Computing Office</orgName>
				</funder>
				<funder ref="#_nYVhhPr">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sofia</forename><surname>Yfantidou</surname></persName>
							<email>sofiayfantidou@csd.auth.gr</email>
						</author>
						<author>
							<persName><forename type="first">Pavlos</forename><surname>Sermpezis</surname></persName>
							<email>pavlossermpezis@csd.auth.gr</email>
						</author>
						<author>
							<persName><forename type="first">Athena</forename><surname>Vakali</surname></persName>
							<email>avakali@csd.auth.gr</email>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Baeza</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">Aristotle University of Thessa- loniki , Greece Institute for Experiential AI ,</note>
								<orgName type="department">Institute for Experiential AI</orgName>
								<orgName type="institution">Aristotle University of Thessa- loniki</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">Northeastern University , United States</note>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">Aristotle University of Thessaloniki , Thessaloniki , Greece , 54124 ; -Yates ,</note>
								<orgName type="department">-Yates</orgName>
								<orgName type="institution">Aristotle University of Thessaloniki</orgName>
								<address>
									<postCode>54124</postCode>
									<settlement>Thessaloniki</settlement>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">Institute for Experiential AI , Northeastern University , San Jose , United States , CA 95113 .</note>
								<orgName type="department">Institute for Experiential AI</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>95113</postCode>
									<settlement>San Jose</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_XACbXEW">Uncovering Bias in Personal Informatics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF54BCFE0D389F879EDD1AF90525ACE8</idno>
					<idno type="DOI">10.1145/3610914</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T08:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_UadF7gH">CCS Concepts:</term>
					<term xml:id="_D3dBDta">Human-centered computing ‚Üí Ubiquitous and mobile computing</term>
					<term xml:id="_7j9mHsW">‚Ä¢ Applied computing ‚Üí Consumer health</term>
					<term xml:id="_q36CRBX">‚Ä¢ Computing methodologies ‚Üí Artificial intelligence</term>
					<term xml:id="_p6tZ9R8">‚Ä¢ Social and professional topics ‚Üí Codes of ethics Personal informatics, machine learning, bias, fairness, ubiquitous computing, sensing data, digital biomarkers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_vgt9rDd"><p xml:id="_jFDcksP"><s xml:id="_hQzm58x">Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information.</s><s xml:id="_dmaXFnp">Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others.</s><s xml:id="_gFygg4n">Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications.</s><s xml:id="_MxbXUDM">In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle.</s><s xml:id="_ZWBzuFH">We use the most detailed framework to date for exploring the different sources of bias and find that biases exist both in the data generation and the model learning and implementation streams.</s><s xml:id="_EeTsETZ">According to our results, the most affected minority groups are users with health issues, such as diabetes, joint issues, and hypertension, and female users, whose data biases are propagated or even amplified by learning models, while intersectional biases can also be observed.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_BdNembH">INTRODUCTION</head><p xml:id="_XNg8nmn"><s xml:id="_7sqJjJq">Ubiquitous technologies, such as smartphones and wearables, are an integral part of our lives today <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b89">90]</ref>.</s><s xml:id="_WYcdgwn">Their proliferation has given rise to Personal Informatics (PI), namely a class of systems that "help people collect personally relevant information for the purpose of self-reflection and gaining self-knowledge" <ref type="bibr" target="#b65">[66]</ref>.</s><s xml:id="_SejKfvj">Such systems enable people to keep track of their productivity <ref type="bibr" target="#b61">[62]</ref>, finances <ref type="bibr" target="#b59">[60]</ref>, and learning <ref type="bibr" target="#b44">[45]</ref>.</s><s xml:id="_MxYpCqx">Yet, tracking various aspects of physical and mental health is particularly prevalent <ref type="bibr" target="#b32">[33]</ref>.</s><s xml:id="_9gbfzQK">PI systems can continuously and unobtrusively measure and collect physiological and behavioral data, namely, "digital biomarkers", from users through integrated sensors.</s><s xml:id="_4Qu6juN">Digital biomarkers contain an uncanny amount of personal information.</s><s xml:id="_VQGsZDY">Even the coarser behavioral biomarkers acquired from consumer wearables (e.g., steps, calories) strongly correlate to a person's gender, height, and weight <ref type="bibr" target="#b60">[61]</ref>, while signals of finer granularity (e.g., accelerometer and heart rate), can predict variables associated with an individual's physical health, fitness, and demographics <ref type="bibr" target="#b88">[89]</ref>.</s></p><p xml:id="_RJMTHJf"><s xml:id="_kyU5tS9">At the same time, consumer smartphones and wearables are now packed with an increasing number of advanced health tracking features, innovating in personal health, research, and care <ref type="bibr" target="#b6">[7]</ref>.</s><s xml:id="_RVh9mXv">Flagship consumer wearable algorithms -some approved by the US Food and Drug Administration-can now identify signs of atrial fibrillation (AFib) through electrocardiogram (ECG) or photoplethysmography (PPG) signals <ref type="bibr" target="#b36">[37]</ref>.</s><s xml:id="_BZq6sAV">On a different note, newly released watches introduce novel cycle-tracking functionality, including fertility prediction and notifications, using logged period data and temperature measurements <ref type="bibr" target="#b7">[8]</ref>.</s><s xml:id="_zK4cbcS">Mobility features include fall and crash detection through accelerometer and gyroscope measurements and notification of emergency services.</s><s xml:id="_DuQ2t7n">Similarly, newly integrated blood oxygen (SpO2) sensors on wearable rings can provide indicators and warn users about potential sleep apnea or lung diseases <ref type="bibr" target="#b93">[94]</ref>.</s><s xml:id="_ChXMJJf">It is evident that consumer smartphones and wearables have moved beyond step counts, marking a rapid transition towards mHealth .</s></p><p xml:id="_2X7D4m5"><s xml:id="_sdDnR2n">However, the prevalent PI adoption embeds important challenges due to the questionable transparency and unexplored biases in the systems' algorithms.</s><s xml:id="_xC9q9v2">Contrary to the common belief that algorithmic decisions are objective by definition, a machine learning (ML) model may be inherently unfair by learning, preserving, or even amplifying historical biases existent in the data <ref type="bibr" target="#b80">[81]</ref>.</s><s xml:id="_bHvBJrf">Unfortunately, real-world cases of unfair ML models are abundant even within the ubiquitous computing community.</s><s xml:id="_zUQmmXd">For example, neural network algorithms trained to classify skin lesions were found to exhibit lower diagnostic accuracy in black patients <ref type="bibr" target="#b58">[59]</ref>.</s><s xml:id="_neT3Mmc">Moreover, racial bias has been identified in health sensors such as oximeters, which were primarily tested on white populations, resulting in the misclassification of people of color <ref type="bibr" target="#b87">[88]</ref>.</s><s xml:id="_sgdjdHE">Given the growing potential of PI devices in mHealth, imagine the ethical and social implications if an AFib detection algorithm exhibited bias against a specific race or if a fertility prediction algorithm was biased against women in developing countries.</s></p><p xml:id="_YEpReqS"><s xml:id="_vjcnBqp">Despite this growing interest in ML fairness, a focused emphasis on the requirements of unbiased PI systems in mHealth settings is lacking <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b109">110]</ref>.</s><s xml:id="_QxFytyW">Yet, PI systems are deployed in high-stakes health applications, while their input data modality, i.e., personal sensitive data, makes them susceptible to propagating bias.</s><s xml:id="_6t45hVr">Thus, exploring biases within these systems is critical to raise awareness regarding mitigating and regulatory actions required to avert potential negative consequences.</s><s xml:id="_f9ZHTXx">This need is further highlighted by the differences between PI and other domains, such as facial or speech recognition:</s></p><p xml:id="_kTaxp95"><s xml:id="_wuJBK8a">‚Ä¢ The digital divide as a barrier of entry: To contribute data to an image or voice dataset, users do not need any technical knowledge or niche device.</s><s xml:id="_ExFHtjb">However, to contribute to a PI dataset, users face significant "entry barriers" in terms of digital capacity or device ownership, creating new-found representation biases.</s><s xml:id="_gfV85Hp">‚Ä¢ Emerging technologies accuracy: Facial or speech recognition devices, e.g., camera or voice recorder, are mature technologies with high accuracy.</s><s xml:id="_55FMabd">On the contrary, emerging PI devices' accuracy significantly varies across models, creating unexplored measurement biases and discrepancies across user segments.</s><s xml:id="_QANRBV8">‚Ä¢ Complex nature of data: It may be straightforward to identify biases in terms of skin tone, nationality and gender in facial or speech recognition.</s><s xml:id="_4CE2AJG">Yet, identifying biases in digital biomarkers, e.g., sensor data, is complicated.</s><s xml:id="_VCr2Whj">Biases in PI data can remain hidden and be further propagated or even amplified in ML models.</s><s xml:id="_J9fn3Pf">Motivated by these differences and research gap, we present the first comprehensive study on bias in PI: We adopt the most complete framework to date for understanding sources of harm in the ML life cycle <ref type="bibr" target="#b91">[92]</ref> ( ¬ß3), and explore biases in the data generation ( ¬ß4) and model building and implementation ( ¬ß5) streams.</s><s xml:id="_kpJxtFA">Ultimately, we apply our methodology in the largest real-world PI dataset to date, while providing preliminary indications of generalizability through differing datasets and use cases ( ¬ß6), and we offer recommendations for bias mitigation ( ¬ß7).</s><s xml:id="_Z6Pq9kF">Specifically, our research questions (RQs) and the respective contributions are as follows:</s></p><p xml:id="_jmgzwdg"><s xml:id="_gyPf57t">(1) Are PI data susceptible to biases?</s><s xml:id="_thqpfV2">We examine whether ubiquitous digital biomarkers are subject to historical, representation, and measurement biases.</s><s xml:id="_K6zjVjg">To demonstrate our point, we analyze the MyHeart Counts dataset <ref type="bibr" target="#b51">[52]</ref>, comprising physical activity, fitness, sleep, and cardiovascular health data for 50K participants across the United States (US).</s><s xml:id="_qn65enC">Our results reveal biases across all stages of the data generation stream, highlighting the need for careful usage of PI datasets, in general, and the MyHeart Counts data, in particular.</s><s xml:id="_3yhtVDA">(2) Do ML models inherit PI data biases?</s><s xml:id="_nQrAq7b">We examine whether biases inherent in PI data persist during modeling.</s><s xml:id="_x7pAAQp">Specifically, we assess various learning and personalization models for aggregation, learning, and deployment biases.</s><s xml:id="_bR2PArj">Consistent with prior research <ref type="bibr" target="#b78">[79]</ref>, our findings indicate that data biases are propagated to learning models, particularly for user groups with intersecting identities.</s><s xml:id="_ZuXYFnr">They are also significantly amplified in their personalized counterparts, prompting further exploration of personalization trade-offs.</s><s xml:id="_zGAvy2B">(3) Do synthetic benchmarks hide the imperfect nature of PI?</s><s xml:id="_5utyjVY">We explore whether "perfect" synthetic benchmark datasets can hide PI data and model "imperfections" and biases during evaluation.</s><s xml:id="_PgjK2NZ">Specifically, we compare a random benchmark, representative of our data, with one designed to achieve demographic parity for evaluation biases.</s><s xml:id="_fjHfpx3">Our findings highlight the importance of establishing PI benchmarks that are representative of the intended target populations to avoid deploying models with unidentified biases.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_3bQ8BzE">BACKGROUND AND RELATED WORK</head><p xml:id="_3bHvwcc"><s xml:id="_JRPPRzW">This section provides the necessary background and delimits the scope of this work, while discussing relevant literature on the conceptual space of fairness in PI for mHealth.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_xFCgxDH">Bias and Fairness in PI Definitions</head><p xml:id="_zqtWXXu"><s xml:id="_mNXWXTW">Bias in ML is a potential source of unfairness that can lead to harmful consequences, such as discrimination.</s><s xml:id="_JruSJUV">In terms of algorithms, bias can be defined as "a systematic error or an unexpected tendency to favor one outcome over another" <ref type="bibr" target="#b69">[70]</ref>.</s><s xml:id="_DzYdx6n">The term can also refer to an algorithm's undesired dependence on specific data attributes that may be linked to a demographic group, e.g., based on gender, race, or religion <ref type="bibr" target="#b38">[39]</ref>.</s><s xml:id="_5RTNgHF">While bias is related to fairness, it is important to note that algorithmic bias is distinct from ethics.</s><s xml:id="_VfpJMjQ">It is simply a mathematical and statistical consequence of an algorithm, including the data used, the logic itself and the user interaction feedback-loop, making it fully quantifiable <ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_67Q8BDZ">Unlike bias, the fairness of an ML model is judged against a set of legal or ethical principles, which are subject to the local government and culture.</s><s xml:id="_Q4N6pVJ">The Fairness, Accountability, and Transparency in Machine Learning (FAccT/ML) community defines fairness as a principle that "ensures that algorithmic decisions do not create discriminatory or unjust impacts when comparing across different demographics (e.g., race, sex, etc.)" <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_kfDksV3">Fairness is an inexorably subjective and context-dependent notion and incorporates different metrics for different definitions, some of which are even mutually incompatible <ref type="bibr" target="#b41">[42]</ref>.</s><s xml:id="_NGJNFYK">For example, drawing from our use case ( ¬ß3.2) and as per relevant literature <ref type="bibr" target="#b75">[76]</ref>, women tend to perform overall less physical activity compared to their male counterparts.</s><s xml:id="_Cafrp5n">Hence, a PI goal-setting algorithm could either give females lower goals because they historically perform less physical activity or give females equal high goals despite historic differences to encourage behavior change.</s><s xml:id="_AaJXfGb">Fairness in this context is subjective and dependent on the viewpoint.</s><s xml:id="_SMfg6S5">However, algorithmic bias is objective and can be identified regardless of fairness considerations.</s><s xml:id="_pwT65Nt">For instance, the first algorithm described would be biased against women since it assigns them fewer high-activity goals compared to men.</s></p><p xml:id="_tz35rrp"><s xml:id="_qBJSYgC">Bias can (but does not always) result in discrimination.</s><s xml:id="_C7mpYPM">We consider systems fairer if they are less biased, but building ML systems without bias is practically difficult and possibly infeasible.</s><s xml:id="_4T3Zmuj">However, quantifying and mitigating bias is an attainable and important step toward building fairer ML systems.</s><s xml:id="_GTbFF4n">Hence, our work aims to unveil and quantify biases in the PI life cycle without the subjective element of personal fairness perspectives.</s><s xml:id="_JRFUZtW">Note that we might still use the term "fairness" in the paper, when we refer to certain standard terminology, e.g., "fairness through unawareness" or "fairness metrics".</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_FrhxRvZ">Bias and Fairness in PI for Health and Well-being Literature</head><p xml:id="_RY5vrVJ"><s xml:id="_qxnBBaB">With the widespread adoption of intelligent systems and applications in our everyday lives, accounting for data and model biases has gained significant traction in designing and deploying systems.</s><s xml:id="_YZpNhRb">Specifically, these notions have been studied extensively in domains such as natural language processing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref>, recommender systems <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b104">105]</ref>, and computer vision <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b105">106]</ref>.</s><s xml:id="_DnTDQUw">Yet, evidence for biases in the PI setting is lacking.</s><s xml:id="_xmZ9BHX">Closer to PI, fairness research in healthcare is still in its infancy <ref type="bibr" target="#b35">[36]</ref>.</s><s xml:id="_mmQNPVR">The digitization of medical data has enabled the scientific community to collect large amounts of heterogeneous, multi-modal data and develop ML algorithms for various medical tasks.</s><s xml:id="_D36AwyP">During the process, various limitations have been uncovered based on the three most prominent data types: medical image data, structured electronic health record (EHR) data, and textual data.</s></p><p xml:id="_sgZaJWV"><s xml:id="_mzDecBZ">Medical imaging has been the most widely used data source for ML in healthcare, and biases in them have received attention <ref type="bibr" target="#b55">[56]</ref>.</s><s xml:id="_YA3zFWT">For example, Larrazabal et al. <ref type="bibr" target="#b63">[64]</ref> utilizes two commonly used X-ray image datasets to diagnose various chest diseases under different gender imbalance conditions and showcase that the minority gender group systematically performs worse than the majority gender group.</s><s xml:id="_Uqk6GPF">Similarly, according to Adamson and Smith <ref type="bibr" target="#b0">[1]</ref>, relying on ML for skin cancer screening may exacerbate potential racial disparities in dermatology.</s></p><p xml:id="_HNE3vgN"><s xml:id="_ZFDtmKD">On a different note, EHR systems store multi-modal, heterogeneous patient data, such as demographics, diagnoses, and clinical records, and have been used for various tasks, such as medical concept extraction, mortality prediction, and disease inference.</s><s xml:id="_23Qn8MJ">Regarding EHR data fairness, Meng et al. <ref type="bibr" target="#b70">[71]</ref> identify race-level differences in the predictions of neural network models on the MIMIC-IV dataset <ref type="bibr" target="#b56">[57]</ref>, with Black and Hispanic patients being less likely to receive interventions or receiving interventions of shorter average duration.</s><s xml:id="_k6asDRT">Similarly, R√∂√∂sli et al. <ref type="bibr" target="#b81">[82]</ref> reveals a strong class imbalance problem and significant fairness concerns for Black and publicly insured ICU patients in the same dataset.</s></p><p xml:id="_x66HQas"><s xml:id="_TmpppWB">Concerning textual EHR data, Chen et al. <ref type="bibr" target="#b18">[19]</ref> examines clinical and psychiatric notes to predict intensive care unit mortality and 30-day psychiatric readmission.</s><s xml:id="_qjrXKtb">Their analysis reveals differences in prediction accuracy, and biases are present in terms of gender and insurance type for mortality prediction and insurance policy for psychiatric 30-day readmission.</s><s xml:id="_hR5fz6v">Within the same scope, Zhang et al. <ref type="bibr" target="#b111">[112]</ref> train deep embedding models on medical notes from the MIMIC-III database <ref type="bibr" target="#b57">[58]</ref>, and find that classifiers trained from their embeddings exhibit statistically significant differences in performance, often favoring the majority group regarding gender, language, ethnicity, and insurance status.</s></p><p xml:id="_ZfW2sEB"><s xml:id="_eUksmpA">Yet, despite the emerging research on biases in healthcare, its proximity to PI, and the widespread adoption of PI technologies, biases in PI have been barely explored.</s><s xml:id="_g4aHP2J">Paviglianiti and Pasero <ref type="bibr" target="#b78">[79]</ref> reported gender biases in digital biomarkers using Vital-ECG, a wearable smart device that collects electrocardiogram and plethysmogram signals.</s><s xml:id="_HApdESn">Still, their study is limited to quantifying learning bias, far from a complete study of bias in the PI life cycle.</s><s xml:id="_5dzjAhc">On the contrary, inspired by relevant works in related domains <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55]</ref>, our work aims to raise awareness and set up a systematic approach to comprehensively analyze data and model biases in PI systems, highlighting the multiple facets of bias that may affect system fairness.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_Kad54ft">PERSONAL INFORMATICS BIASES: SETTING &amp; CONFIGURATION</head><p xml:id="_huFcwdB"><s xml:id="_PEqYk7Q">In this section, we discuss frameworks capturing biases (Section 3.1) and our use case configuration, acting as a starting point for our investigation (Section 3.2).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_XuFz4Eg">Sources of Bias in the Machine Learning Life Cycle Framework</head><p xml:id="_jYsp27j"><s xml:id="_VPStv98">There exist various frameworks for capturing bias in ML applications, e.g., in the context of the Web <ref type="bibr" target="#b9">[10]</ref>, autonomous systems <ref type="bibr" target="#b24">[25]</ref> or crowdsourced labeling <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_3FSCutA">However, most tend to be domain-specific.</s><s xml:id="_P49w77G">Yet, Suresh and Guttag <ref type="bibr" target="#b91">[92]</ref> has introduced a framework for understanding sources of harm throughout the ML life cycle, independent of the application domain.</s><s xml:id="_653Kyue">Given its generic and comprehensive nature and, thus, suitability for the PI use case, we consider this as a basis for our study.</s><s xml:id="_uFU8nNU">As per <ref type="bibr" target="#b91">[92]</ref>, the ML life cycle consists of two streams, the data generation stream and the model building and implementation stream, containing seven sources of bias-related harms, as shown in Figures <ref type="figure" target="#fig_1">1a</ref> and <ref type="figure" target="#fig_1">1b</ref>, respectively, and defined below:  ‚Ä¢ Historical biases can occur even if the data are flawlessly sampled by reflecting real-world biases against one or more groups of people.</s><s xml:id="_hjZwAWV">For example, gender gaps in certain fields can result in language models linking certain job-related terms, such as nurse or programmer, with female or male descriptors, respectively <ref type="bibr" target="#b14">[15]</ref>.</s><s xml:id="_EuE7RPv">‚Ä¢ Representation biases can occur when sampling methods lead to underrepresenting population segments.</s><s xml:id="_tsgdDs7">For example, in popular image datasets, skewed towards the US or Europe, result in performance degradation when categorizing images from underrepresented regions <ref type="bibr" target="#b27">[28]</ref>.</s><s xml:id="_HZdQvRp">‚Ä¢ Measurement biases can occur when choosing, collecting, and calculating features and labels for the prediction problem.</s><s xml:id="_mcTMEAA">For example, in medical contexts, diagnosis is frequently a stand-in for a health condition; yet, some gender and race groups face elevated risks of misdiagnosis, or underdiagnosis <ref type="bibr" target="#b52">[53]</ref>.</s><s xml:id="_FVaTcdE">‚Ä¢ Aggregation biases can occur when a universal model is applied to that should be differentiated based on underlying user groups.</s><s xml:id="_rv4nYsp">For example, when training natural language processing models on generic data, the nuances and contextual meanings of street slang can be lost <ref type="bibr" target="#b40">[41]</ref>.</s><s xml:id="_7dmkqEd">‚Ä¢ Learning biases can occur when modeling choices amplify performance gaps across user segments.</s><s xml:id="_Mru82BE">For example, prioritizing privacy in a model can diminish the impact of underrepresented groups data <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_QuqZGNG">‚Ä¢ Evaluation biases can occur when the benchmark population is not representative of the target population.</s><s xml:id="_MX6kXnW">For example, dark-skinned women comprise only a small percentage of popular facial image benchmarks, leading to worse intersectional performance of commercial facial analysis tools <ref type="bibr" target="#b15">[16]</ref>.</s><s xml:id="_2VUT85W">‚Ä¢ Deployment biases can occur when there exists a mismatch between the problem a model is designed to solve and how it is actually utilized.</s><s xml:id="_8ecerbZ">For example, risk assessment tools in criminal justice can be used in "off-label" ways, such as determining the length of a sentence <ref type="bibr" target="#b22">[23]</ref>.</s></p><p xml:id="_skP5eBz"><s xml:id="_x5mFKmD">In the following section, we introduce the use case through which we explore bias in PI for mHealth.</s><s xml:id="_fwZmp7J">We then show empirically and analytically how Suresh and Guttag's seven sources of bias translate in the PI domain.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_FfJGbjY">Exploring Bias through the Largest Digital Biomarkers mHealth Dataset</head><p xml:id="_fZfnUpT"><s xml:id="_Yh9Hx6a">To enable our analysis of bias in the PI life cycle, we require an indicative use case.</s><s xml:id="_GuxbvRh">For this purpose, we utilize the MyHeart Counts dataset <ref type="bibr" target="#b51">[52]</ref>, the largest collection of digital biomarkers in the mHealth domain to date, enabling us to perform the most comprehensive analysis of bias across diverse user demographics.</s><s xml:id="_wj6UN6t">Nevertheless, our methodology can be potentially generalized to other PI datasets ( ¬ß6).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1" xml:id="_r3rVarJ">Data Description.</head><p xml:id="_NR6WjuU"><s xml:id="_Ub77kqm">Up till recently, generic, population-scale PI datasets were uncommon due to cost, privacy concerns, and data protection regulations.</s><s xml:id="_akMpsvg">Existing open datasets were small-to medium-sized <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b103">104]</ref> or domain-constrained, e.g., to human-activity recognition (HAR) <ref type="bibr" target="#b5">[6]</ref>.</s><s xml:id="_AcSaEmd">The release of data from the MyHeart Counts Cardiovascular Health Study from 50K participants in the US, changed this situation.</s><s xml:id="_wD9pCsR">Participants completed surveys and a 6-minute walk test and contributed PI data via a mobile application.</s><s xml:id="_DuGZX6u">Approximately 1 out of 10 participants ( = 4920) shared their basic PI data, such as step count, distance covered, burned calories, and flights climbed.</s><s xml:id="_BVGMkRd">We combine these data with survey responses to attain the following user attributes: gender, ethnicity, age, BMI, and health conditions, such as heart condition, hypertension, joint problem, and diabetes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2" xml:id="_yY8TR2Y">Data Preprocessing.</head><p xml:id="_RJtyAMC"><s xml:id="_HJ3Vdue">To ensure sufficient sample size per user group and compatibility with bias metrics, we binarize non-binary user attributes, such as ethnicity, age, and BMI, as seen in Table <ref type="table" target="#tab_0">1</ref>.</s><s xml:id="_93jM8TR">This grouping creates two user groups per protected attribute, namely a majority group ("privileged") and a minority group ("unprivileged").</s><s xml:id="_bzBRBpY">Note that the usage of the term "privilege" in this work does not necessarily coincide with real-world "privilege".</s><s xml:id="_BKddjun">For example, users with unhealthy BMI are the majority, and hence "privileged" user segment in our dataset, whereas one could argue that the opposite applies in reality.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3" xml:id="_rZanh8c">Data</head><p xml:id="_nUJN8XC"><s xml:id="_bt5BT3K">Labeling.</s><s xml:id="_y5e4ZRx">The MyHeart Counts dataset does not introduce any prediction tasks.</s><s xml:id="_dT2z6Bf">To this end, we select the next-day physical activity prediction from historical data use case <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b99">100]</ref> for model training.</s><s xml:id="_mqweRqN">In other words, based on the user's past activity, we try to predict how many steps they will perform the next day (see Table <ref type="table" target="#tab_1">2</ref>), e.g., to enable personalized goal-setting.</s><s xml:id="_j5VJ94c">Basic digital behavioral biomarkers, such as steps, are easy to collect and commonplace in the literature, enabling the reproducibility of our findings.</s><s xml:id="_NuZTHUQ">Also, they are the largest available sensed modality in the My Heart Counts dataset, allowing us to exploit more data for our analysis.</s><s xml:id="_bNxVeSM">At the same time, according to the World Health Organization (WHO), physical activity has significant health benefits for hearts, bodies, and minds, contributing to preventing and managing noncommunicable diseases such as cardiovascular diseases, cancer, and diabetes <ref type="bibr" target="#b106">[107]</ref>.</s><s xml:id="_rQrqYXZ">Strikingly, physical inactivity has been identified by the WHO as the fourth leading risk factor for global mortality, accounting for 6% of deaths globally <ref type="bibr" target="#b76">[77]</ref>, highlighting the importance of the selected use case.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4" xml:id="_zfSzMXd">Bias Measures.</head><p xml:id="_x8Pesjb"><s xml:id="_45QJK8Y">To measure bias, ML researchers have quantified fairness metrics that operationalize fairness definitions (See Appendix B).</s><s xml:id="_2NSWzMt">For this work, we utilize the widely used Disparate Impact Ratio (DIR), which is the ratio of base or selection rates between unprivileged and privileged groups, assuming equal ability to perform physical activity across demographics:</s></p><formula xml:id="formula_0">Disparate Impact Ratio = Pr(ùë¶ + | ùê∫0) Pr(ùë¶ + | ùê∫1)</formula><p xml:id="_kQ2t57E"><s xml:id="_KzJrkn6">where  + is the actual or predicted positive outcome label (base or selection rate, respectively), 0 is the minority (protected) group, and 1 is the majority group.</s><s xml:id="_6c5zM3Y">Values lower than 1 mean the majority group has a higher proportion of positive outcomes; a value of 1 indicates demographic parity.</s><s xml:id="_rQprDxb">For example, a value of 0.8 for a dataset with gender as protected attribute means that for every male receiving a high activity goal, only 0.8 females do so.</s><s xml:id="_kFfKR85">According to the "4/5 rule" <ref type="bibr" target="#b17">[18]</ref>, accepted values lie within [0.8,1.25],</s><s xml:id="_vXrxWQM">but such ranges are not universally accepted and are context-dependent <ref type="bibr" target="#b23">[24]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_J9gcQKP">EXPLORING BIAS IN PERSONAL INFORMATICS DATA GENERATION</head><p xml:id="_jJ4ZmuN"><s xml:id="_WamJJTF">Bias in the data generation stream can take the form of historical, representation, and measurement biases, as seen in Figure <ref type="figure" target="#fig_1">1a</ref>.</s><s xml:id="_nVzrWPJ">In this section, we explore all three sources, answering to RQ1: Are PI data susceptible to biases?</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_ukmngnJ">Historical Bias</head><p xml:id="_y9SSWMF"><s xml:id="_xVHxeur">Historical biases are domain-rather dataset-dependent, and hence not necessarily quantifiable.</s><s xml:id="_NHqWXZS">Hence, for completeness, we state the main findings of the related literature on the PI domain.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1" xml:id="_tkgVu7C">Physical Activity</head><p xml:id="_s5zkQan"><s xml:id="_Kq2Ar8C">Inequalities.</s><s xml:id="_JBgc5kq">Physical activity data, such as step counts, are among the most common digital biomarkers, and constitute the majority of the extracted MyHeart Counts data, with 4920 users of step tracking compared to 626 users of sleep tracking.</s><s xml:id="_XdDFcsC">However, inequalities in physical activity are well-documented in literature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b75">76]</ref>.</s><s xml:id="_UcVfHHV">Althoff et al. <ref type="bibr" target="#b3">[4]</ref> reveal variability in physical activity worldwide, where reduced activity in females explains a large portion of the observed activity inequality.</s><s xml:id="_CVrTmV9">Overall, the World Health Organization reports that "girls, women, older adults, people of low socioeconomic position, people with disabilities and chronic diseases, marginalized populations, indigenous people and the inhabitants of rural communities often have less access to safe, accessible, affordable and appropriate spaces and places in which to be physically active" <ref type="bibr" target="#b75">[76]</ref>.</s><s xml:id="_pfpt8ss">Such real-world inequalities can manifest into the behavioral data we use to train our models.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KBP5W9A">4.1.2</head><p xml:id="_vnzpnU9"><s xml:id="_PJ5XDcs">The Digital Divide.</s><s xml:id="_sNWPkjV">Similarly, as the world rapidly digitalizes, it threatens to exclude those that remain offline.</s><s xml:id="_29jQrKe">Almost half the world's population, the majority of them women or citizens of developing countries, are still disconnected <ref type="bibr" target="#b71">[72]</ref>.</s><s xml:id="_fKY7CuE">Even in the connected world, male internet users outnumber their female counterparts.</s><s xml:id="_TH3K2fP">This "digital divide" encompasses even more discrepancies, such as the digital infrastructure quality and connectivity speed in rural or remote areas and the required skills to navigate technology <ref type="bibr" target="#b20">[21]</ref>.</s><s xml:id="_sktas5J">Therefore, it is clear that technological systems, including PI, are limited in their ability to capture the diversity of the world population, due to pre-existing inequalities in digital access and literacy.</s></p><p xml:id="_8kvD6wy"><s xml:id="_GnXqCD8">4.1.3</s><s xml:id="_qNkvJUA">BYOD Study Design Biases.</s><s xml:id="_m5AUVZz">PI technologies are used for data collection in clinical research, resulting in newfound demographic imbalances.</s><s xml:id="_3msZY7N">Studies adopting a bring-your-own-device (BYOD) design, such as MyHeart Counts, are more user-friendly, achieve better participant compliance, potentially reduce the bias of introducing new technologies, and accelerate data collection from larger cohorts <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>.</s><s xml:id="_DjUXdqw">However, Cho et al. <ref type="bibr" target="#b21">[22]</ref> identifies significant demographic disparities regarding race (50-85% white cohorts) in BYOD studies.</s><s xml:id="_kPKBW8y">Their findings align with the reported demographic divide existent in the composition of wearable users.</s><s xml:id="_pKYCjUy">Even though the gap is narrowing, a recent report <ref type="bibr" target="#b34">[35]</ref> documents that most existing wearable users are fit adults between 25-34 and that whilst females are more likely to own activity trackers, 63% of smartwatch owners are male.</s><s xml:id="_tbfBJfY">Hence, the technology and participant cohorts in PI in the context of BYOD studies subject datasets to the same biases exposed in the activity inequality and the digital divide literature.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_3Xug3H5">Representation Bias</head><p xml:id="_dqNh8P5"><s xml:id="_bCJNFtW">We discuss representation biases across three dimensions: misrepresented, underrepresented, and unevenly sampled populations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1" xml:id="_FAkr97j">Misrepresented Populations.</head><p xml:id="_6FR43VY"><s xml:id="_VfE7rUq">Representation bias can emerge when the sample population does not reflect the general population (bias in rows).</s><s xml:id="_GRDXret">In the MyHeart Counts dataset, we compare the ratios of majority and minority user segments as defined in Table <ref type="table" target="#tab_0">1</ref> with the real-world ratios extracted from US population censuses.</s><s xml:id="_cvzPwhQ">Specifically, we utilize the US Census Bureau (gender, race, and age <ref type="bibr" target="#b16">[17]</ref> distributions), the Centers for Disease Control and Prevention (BMI <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, joint issues <ref type="bibr" target="#b94">[95]</ref>, hypertension <ref type="bibr" target="#b39">[40]</ref>, and diabetes <ref type="bibr" target="#b74">[75]</ref>), and the American Heart Association (heart condition <ref type="bibr" target="#b95">[96]</ref>) data.</s><s xml:id="_vv6hqS9">Figure <ref type="figure">2</ref> showcases the results of this comparison in a radar plot.</s><s xml:id="_Hvdpxqr">For example, while in the general US population, we have approximately 1 female per 1 male (ratio of 1.0 in pink), in the MyHeart Counts data, we have 0.2 females per 1 male, highlighting the substantial underrepresentation of women.</s><s xml:id="_ERaEytZ">The same applies to race, age, and hypertension segments, where the minority classes in the dataset (non-white users, users less than 45, and users with hypertension, respectively) do not reflect real-world ratios.</s><s xml:id="_pHuRwmj">An interesting finding is that, while in the US, there exist approximately 0.3 underweight, overweight, or obese people for every person with normal weight, in the dataset, this ratio is doubled.</s><s xml:id="_hqA6874">Hence, possibly due to historical biases and design choices, our analysis of the MyHeart Counts data (Figure <ref type="figure">2</ref>) provides evidence that PI datasets might not be representative of target populations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2" xml:id="_ebAHuuV">Underrepresented</head><p xml:id="_mAnVnHx"><s xml:id="_cCvpN92">Populations.</s><s xml:id="_G7Ry4hB">PI datasets can still include underrepresented groups (bias in rows) even if sampled perfectly.</s><s xml:id="_FZgPqU3">Figure <ref type="figure" target="#fig_2">4</ref> shows significant imbalances in the number of samples between minority and majority user segments across almost all protected attributes.</s><s xml:id="_2pMfBZ9">We notice that even for representative sampling, e.g., users with joint or heart problems, the minority group is still significantly underrepresented.</s><s xml:id="_uauvMMj">Thus, the model might be less robust for those users because it has less data to learn from.</s><s xml:id="_237sxF2">Overall, we see that the MyHeart Counts data are skewed towards white, fit males, which needs to be considered in the model-building phases.</s><s xml:id="_k2f9Uth">An ideal PI dataset should be representative of the target population while having enough minority samples.</s><s xml:id="_JRChQW7">However, building large-scale PI datasets is challenging due to the required effort and cost.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3" xml:id="_ZzWeHVg">Unevenly Sampled Populations.</head><p xml:id="_SvHPGPp"><s xml:id="_GaqWscm">Even if sampling is representative and equal (e.g., 50% male and 50% female users), the dataset can still suffer from representation bias if the sampling method is uneven, e.g., active males but inactive females (bias in columns).</s><s xml:id="_5V7pXmG">This is also the case in MyHeart Counts.</s><s xml:id="_BaZCz4Y">Figure <ref type="figure">3</ref> shows the DIR value per protected attribute (i.e., the ratio of recorded high activity for unprivileged versus privileged groups).</s><s xml:id="_wC3MqYp">For our use case, a value of  &lt; 0.8 means that the minority sample is significantly less active than the majority</s></p><p xml:id="_Er8WNyg"><s xml:id="_dFQyymH">Gender Race BMI Heart Condition Joint Issue Hypertension Age 0 0.2 0.4 0.6 0.8 1 Real Ratios Dataset Ratios Real vs Dataset Population Ratio Comparison Fig. 2. Real (pink) versus dataset (green) ratios across population segments in the MyHeart Counts dataset.</s><s xml:id="_7WVHX3q">The ratio is calculated as the number of the minority class divided by the number of the majority class samples.</s><s xml:id="_QuxAdg3">Larger distances indicate larger deviations.</s><s xml:id="_JYGGG4q">Gender, age, race, and to a lesser extent hypertension and BMI show representation bias.</s><s xml:id="_PysCCfN">0.00 0.25 0.50 0.75 1.00 1.25 Disparate Impact Ratio Gender Race BMI Heart Condition Joint Issue Hypertension Diabetes Age Protected Attribute Optimal Value Disparate Impact Ratio for Protected Attributes Fig. 3.</s><s xml:id="_tnJ5Dhz">A bar plot showing the DIR (ratio of base rates) per protected attribute.</s><s xml:id="_7TvqY23">We notice that there exist biases in columns for diabetes patients, users with joint issues, and non-white minorities.</s><s xml:id="_2U2Hzcm">While the data is borderline biased against women and people with unhealthy weight (underweight, overweight, or obese).</s><s xml:id="_VnwFbMF">Male Non-male Gender 0 10000 20000 30000 40000 50000 Number of samples White Non-white Race Abnormal Normal BMI False True Heart Condition False True Joint Issue False True Hypertension False True Diabetes &lt;65 &gt;=65 Age Majority vs. Minority Group Representation based on Number of Samples sample.</s><s xml:id="_jcm4hxm">Specifically, in the MyHeart Counts data, diabetes patients, users with joint issues, racial minorities, and to a smaller extent, women, racial minorities, and overweight and obese users systematically perform lower step counts in the dataset compared to their majority counterparts.</s><s xml:id="_fdcyJEX">On the contrary, users of different age groups with or without hypertension or heart issues do not differ significantly regarding step counts in the data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_4yMCYjQ">Measurement Bias</head><p xml:id="_XAFVm33"><s xml:id="_SfkKyGd">In this section, we focus on the input modalities and their accuracy and discrepancies during data collection.</s></p><p xml:id="_egtAA9x"><s xml:id="_hUpZ2nB">4.3.1 Device Differences.</s><s xml:id="_HwYVVdA">Data in the MyHeart Counts HealthKit dataset originate from different sources (i.e., 33% iPhone, 11% Apple Watch, and 56% third parties.</s><s xml:id="_RvEdnwx">iPhones use integrated sensors, including accelerometer, gyroscope, GPS, and magnetometer to detect and calculate step counts.</s><s xml:id="_2WK3NNU">The motion coprocessor unit reads the sensor data and communicates with the CMMotionActivityManager to classify user activity.</s><s xml:id="_GMNSKNH">This process cannot be fully replicated in Apple watches due to differences in placement, fit, and usage habits.</s><s xml:id="_Kem7GjS">Phones may underestimate step counts due to non-carrying time, while watches have been found more accurate for measuring daily step counts for healthy adults [5, 29, 101].</s><s xml:id="_XpMhBEW">MyHeart Counts HealthKit data show statistically significant differences ( &lt; 0.05) in watch ownership across segments based on gender (46% of males have at least one watch entry vs. 28% females), heart condition (38% with vs. 26% without), and ethnicity (41% non-white vs. 36% white).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2" xml:id="_4ZQ5CRx">Model</head><p xml:id="_w38yPC9"><s xml:id="_EKXY4vd">Differences.</s><s xml:id="_PzG2M79">Accuracy differences have been reported across consecutive generations of phone devices <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_sjnZjzG">Incremental hardware changes may increase the quantity, modality, and quality of data available for the device to classify user activity.</s><s xml:id="_kRNcfJE">For instance, the existence of specialized coprocessors, "always-on" capabilities, and revised recognition algorithms in newer phones can improve classification accuracy.</s><s xml:id="_eUJM3tU">In the MyHeartCounts data, we encounter various phone models, spanning five generations.</s><s xml:id="_FgAcAzV">We identify statistically significant differences ( &lt; 0.05) in phone ownership based on gender and BMI.</s><s xml:id="_AACB6YC">Specifically, females and people with normal BMI tend to own older and cheaper phones with fewer capabilities (see Figure <ref type="figure" target="#fig_3">5</ref>).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3" xml:id="_Qu4Ja9A">General Input Modality Differences.</head><p xml:id="_tNm6q3Q"><s xml:id="_zDFd6wn">Finally, most MyHeart Counts data comes from third parties, such as alternative wearables or fitness and well-being apps.</s><s xml:id="_atUjkgZ">This is common in the PI domain, given the abundance and heterogeneity of available data sources.</s><s xml:id="_3BnMePs">In our use case, we identify statistically significant differences in third party usage based on gender (91% males have at least one third party entry compared to 85% females) and diabetes condition (97% with vs. 90% without).</s><s xml:id="_Gec8Sy9">However, different input devices or apps are proven to have different accuracies, likely to create measurement accuracy discrepancies between different users <ref type="bibr" target="#b31">[32]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_6Mxsk3e">EXPLORING BIAS IN PERSONAL INFORMATICS MODEL BUILDING AND IMPLEMENTATION</head><p xml:id="_S7guEmP"><s xml:id="_dDhBv3E">Bias in the model building and implementation stream can take the form of aggregation, learning, evaluation, and deployment biases, as seen in Figure <ref type="figure" target="#fig_1">1b</ref>.</s><s xml:id="_sdv3xAG">In this section, we discuss all four sources, providing an answer to RQ3, namely: Do ML models inherit PI data biases?</s><s xml:id="_9TKTAeZ">Do they mitigate, propagate, or maybe even amplify them?</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_VD8dNZz">Aggregation Bias</head><p xml:id="_F7MchFw"><s xml:id="_uT8m4bz">We evaluate aggregation bias by plotting the DIR (selection rate, i.e., rate of high activity goals predictions) for different user segments' predictions based on heart conditions, hypertension, joint issues, diabetes, race, BMI, gender, and age.</s><s xml:id="_ceH9q5A">We utilize two baseline models to capture the notions of "fairness through awareness" <ref type="bibr" target="#b29">[30]</ref> and "fairness through unawareness" <ref type="bibr" target="#b62">[63]</ref>.</s><s xml:id="_BFPzACR">In fairness through awareness, fairness is captured by the principle that similar individuals should have similar classification outcomes.</s><s xml:id="_8mKUCxb">In our use case, the similarity is defined based on user demographics in the absence of other features.</s><s xml:id="_MkPBGfx">In practical terms, the aware model is trained on a feature set that includes protected attributes per user.</s><s xml:id="_yJdAa8T">On the other hand, fairness through unawareness is satisfied if no protected attributes are explicitly used in the learning process <ref type="bibr" target="#b101">[102]</ref>.</s></p><p xml:id="_jgPyzw6"><s xml:id="_wEvjWch">Linear (48, 100) Dropout (0.2) Linear (100, 100) Dropout (0.2) Linear (100, 2) ReLU ReLU MLP LSTM (48, 200) Dropout (0.2) LSTM (200, 200) Dropout (0.2) LSTM (200, 200) ReLU LSTM Linear (200, 2) 1D Conv (1, 64, 24) Dropout (0.2) 1D Conv (64, 64, 24) Dropout (0.2) ReLU CNN Linear (384, 2) Max Pooling (2) ReLU Max Pooling (2)</s></p><p xml:id="_E9DAc3w"><s xml:id="_QEwvqvZ">Fig. <ref type="figure">6</ref>.</s><s xml:id="_TTHyWYw">Alternative model architectures evaluated: MLP, LSTM, and CNN.</s><s xml:id="_t8eJDkU">Fig. <ref type="figure">7</ref>.</s><s xml:id="_zMuaAuz">A comparison of DIR between data and models shows no significant differences in terms of fairness (DIR).</s></p><p xml:id="_PwY5XBy"><s xml:id="_vyrybGZ">5.1.1</s><s xml:id="_XT4aWdG">Models' Description.</s><s xml:id="_5h99ujU">Our baseline models and hyperparameters are sourced from prior work in the field of physical activity prediction that benchmarked six distinct learning paradigms from traditional ML models to advanced deep learning architectures on the MyHeart Counts dataset <ref type="bibr" target="#b12">[13]</ref>.</s><s xml:id="_bScAer2">For the scope of our work, we choose their best-performing model, a Long Short-Term Memory (LSTM) recurrent neural network, that achieves a Mean Absolute Error (MAE) of 1087 steps, beating previous state-of-the-art approaches by 67% on the task of physical activity prediction.</s><s xml:id="_rr4aUtp">For more details on the model architecture see Appendix A.</s></p><p xml:id="_EQzk865"><s xml:id="_sZRvNz5">To further validate our model choice (LSTM), we also conduct comparative fairness assessments between three deep learning models (Figure <ref type="figure">6</ref>):</s></p><p xml:id="_AcAUkNn"><s xml:id="_YKAXThV">‚Ä¢ 3-layered Multilayer perceptron (MLP) with dropout ‚Ä¢ 2-layered Convolutional Neural Network (CNN) with max-pooling and dropout ‚Ä¢ 3-layered Long Short-Term Memory (LSTM) with dropout As per Figure <ref type="figure">7</ref>, both alternatives perform similarly to the "unaware" LSTM model regarding fairness metrics (i.e., only 0-2% deviation in DIR), indicating the generalizability of our claims across learning paradigms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2" xml:id="_EkphDMp">Single Attribute</head><p xml:id="_bMn7qh3"><s xml:id="_W9xE5sU">Biases. Figure <ref type="figure">8</ref> presents experimentation results concerning ML model biases measured via DIR.</s><s xml:id="_buzxqhg">Specifically, aware learning models are not foolproof against data biases in most cases (joint issues, diabetes, gender), and even amplify them for specific protected attributes (hypertension); (2) Even excluding protected attributes from the training process of unaware models does not guarantee unbiased results in line with prior work <ref type="bibr" target="#b79">[80]</ref>.</s><s xml:id="_bqzgwr6">Fairness through unawareness is also ineffective due to the presence of proxy features, i.e., features that work like proxies for protected attributes.</s><s xml:id="_JECUa5T">Through them, bias propagates from data to models: for example, a person's walking behavior (measured in step counts) is a good predictor of gender, BMI, and age, which can thus be inferred, despite being hidden during training <ref type="bibr" target="#b60">[61]</ref>.</s><s xml:id="_Me7CfCG">Overall, diabetes patients have the largest bias gap compared to non-diabetic users, partially attributed to their highly biased training data.</s><s xml:id="_RxgGstH">Yet, users with hypertension have the largest difference between data and model biases since models trained on seemingly unbiased data introduce bias during the learning process.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3" xml:id="_gHE6vbx">Intersectional Biases.</head><p xml:id="_xrkWVdM"><s xml:id="_vSPaZh9">We also examine intersectional biases, as shown in Figure <ref type="figure" target="#fig_4">9</ref>; namely we quantify the biases of the unaware model conditioned on protected attribute combinations.</s><s xml:id="_hmUMRjB">Specifically, we consider two attributes at a time and two different combination strategies: minority-minority vs. rest (e.g., diabetic women) and majority-majority vs. rest (e.g., non-diabetic men).</s><s xml:id="_yCCEp5F">Keeping the diabetes attribute fixed, our results highlight</s></p><p xml:id="_Z2Nx4FU"><s xml:id="_awhV5gv">Heart Condition Hypertension Joint Issue Diabetes Race BMI Gender Age Protected Attribute 0.0 0.2 0.4 0.6 0.8 1.0 1.2 DIR Optimal Value Data and Baseline Models' DIR Modality Data Aware Model Unware Model Personalized Model</s></p><p xml:id="_MYpxdvn"><s xml:id="_UuSVYt7">Fig. <ref type="figure">8</ref>. DIR comparison between data, aware, unaware baseline, and personalized models.</s><s xml:id="_vqEbCSU">We see that the "one-sizefits-all" models propagate or, in some cases, amplify existing representation biases.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_gPVYj6q">Heart Condition Hypertension</head><note type="other" xml:id="_m6Q2uce">Joint</note><p xml:id="_vyZz2rT"><s xml:id="_aJraQn7">Issue Race BMI Gender Age Protected Attribute 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 DIR Optimal Value Intersectional Biases conditioned on Diabetes Condition Modality Individual Protected Attribute Intersectional (Minorities Only) Intersectional (Majorities Only)  the widening intersectional biases for individuals who belong to multiple minorities (in pink) across almost all attributes (with the exception of BMI, where individuals with unhealthy BMI are the majority group, despite usually being considered unprivileged in practice).</s><s xml:id="_ZE5J3Wh">The largest gap appears in individuals with more than one health condition, such as diabetic heart patients and diabetic patients aged 65+.</s><s xml:id="_rGqfgrd">At the same time, individuals who do not belong to any minority groups (in purple), benefit across all attributes.</s><s xml:id="_yBa69Qj">The trends in aggregation bias indicate that PI models do not tackle diverse user segments equally well and reflect or even amplify representation biases existing in the data, especially regarding intersectional biases.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_HvFQJPz">Learning Bias</head><p xml:id="_yZ4xst4"><s xml:id="_u7rH56z">Personalization in prevalent in the PI literature, straying from the "one-size-fits-all" mentality and its shortcomings, as discussed above.</s><s xml:id="_fXQR3VQ">Contrary to generic models, personalized models are fine-tuned given the data of a single user or user segment.</s><s xml:id="_MMxEkqK">Accounting for such interindividual variability has been proven to dramatically improve prediction performance in various tasks within the PI domain, such as pain detection, engagement estimation, and stress prediction from ubiquitous devices data <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b92">93]</ref>.</s><s xml:id="_FTD9yCS">Given the increasing popularity of the personalization paradigm, in this study, we investigate whether personalization as a modeling choice can amplify performance disparities across different user segments in the data, given the existence of representation bias.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_GEgGgMh">Heart Condition Hypertension</head><note type="other" xml:id="_Z37QkJc">Joint</note><p xml:id="_9ePWXxk"><s xml:id="_mQQn53v">Issue Race BMI Gender Protected Attribute 0.7 0.8 0.9 1.0 1.1 1.2 DIR Optimal Value Random versus Sampled Test Set Comparison Model Unaware Aware Personalized Data Bias 0 1</s></p><p xml:id="_kgmuUVb"><s xml:id="_4aT3jAc">Fig. <ref type="figure" target="#fig_1">11</ref>.</s><s xml:id="_BW86zmG">A comparison of DIR between different test sets across models.</s><s xml:id="_nkbbSFY">"Ideal" test sets in terms of data bias (dashed lines) tend to hide imperfections in the trained models compared to the original, "realistic" test sets (continuous lines).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1" xml:id="_v7fmF7f">Model Description.</head><p xml:id="_AsVts6N"><s xml:id="_ZZ69s8K">We base our approach on the CultureNet package <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b83">84]</ref> for building generalized and culturalized deep models to estimate engagement levels from face images of children with Autism Spectrum Condition.</s><s xml:id="_QABBP7X">Specifically, we utilize our deep LSTM model, which is trained on data from all users, we freeze the network parameters tuned to both minority and majority user groups, as described in Section 5.1, and then finetune the last layer, i.e., a linear fully-connected layer, to each user group separately based on the MyHeart Counts protected attributes (health condition, hypertension, joint issues, diabetes, race, BMI, gender, age).</s><s xml:id="_u9peHVC">Figure <ref type="figure" target="#fig_5">10</ref> delineates the personalization process.</s><s xml:id="_Vxutewz">Appendix A provides a formal definition of the learning process.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2" xml:id="_tKxjqUG">Single Attribute</head><p xml:id="_hbx6xJp"><s xml:id="_ZQZSruj">Biases.</s><s xml:id="_HGH8FQs">While we could not identify significant performance benefits either for the privileged or unprivileged groups by utilizing personalization in our use case, we encountered significant bias shortcomings (Figure <ref type="figure">8</ref>).</s><s xml:id="_Xzzb2PY">Specifically, across all protected attributes (with a borderline exception of race), personalized models are more biased than either aware or unaware models or both.</s><s xml:id="_NHDyUPE">Users with diabetes present an extreme case.</s><s xml:id="_ZM7859z">The personalized model "learns" that diabetics are less active than healthy users in the dataset and thus provides solely low activity goals, even to active diabetics.</s><s xml:id="_amBgmgT">The intuition behind this behavior lies in the training process; personalized models amplify data representation biases through fine-tuning.</s><s xml:id="_NnChcsD">Our findings highlight that a common modeling choice in PI, such as personalization, can negatively affect biases and asks for bias-aware personalization approaches to rip the benefits of user tailoring without leading to biased results.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_QhRN2tN">Evaluation Bias</head><p xml:id="_ZjnV3gm"><s xml:id="_JKnqk5J">5.3.1 Benchmark Selection.</s><s xml:id="_Bj2cBKE">ML models are optimized on training and validation data but evaluated on test benchmarks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b49">50]</ref>.</s><s xml:id="_CvSXnVF">However, the ubiquitous computing community still suffers from a lack of larger benchmarks beyond HAR <ref type="bibr" target="#b5">[6]</ref> and sleep classification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b110">111]</ref>.</s><s xml:id="_ybfxVCp">Also, benchmarks within the community often do not represent the target population.</s><s xml:id="_kQdJtw9">For example, within the fall detection domain, due to safety concerns, datasets usually comprise imitated falls performed by younger people while they are deployed on older people <ref type="bibr" target="#b90">[91]</ref>.</s><s xml:id="_zaCyxMR">Yet, a misrepresentative benchmark encourages deploying models that perform well only on the benchmark population.</s><s xml:id="_Z5tfaSw">To illustrate our point, given the lack of established benchmarks in PI, we devise two distinct test sets for comparison purposes: our original, "realistic" (random) test set, and a sampled subset of the latter ("ideal"), with demographic parity at base rate (DIR = 1.0).</s><s xml:id="_Fa4tTJX">We then evaluate our models on these two test sets.</s><s xml:id="_Aeb7xfP">Figure <ref type="figure" target="#fig_1">11</ref> presents the results of our experimentation, where it is clear that the ideal test set, imitating a "fair" world, consistently shows lower bias than the "realistic" test set.</s><s xml:id="_5dCeV22">Better performance is defined as smaller deviations from the optimal DIR value of 1.0.</s><s xml:id="_xeNwQvF">Essentially, an ideal-world benchmark is "hiding" the imperfections of our trained model, which has been proven to propagate or even amplify biases based on the original, random test set.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2" xml:id="_XaDg59Z">Evaluation Metric Selection.</head><p xml:id="_JkuPEv7"><s xml:id="_Jb2yVap">Evaluation bias can also emerge from the metric used to quantify the models' performance.</s><s xml:id="_yYmYhg9">For instance, group fairness hybrid metrics, such as error rates, are prone to imbalances and can hide disparities in other types of bias metrics, such as WAE metrics (see Appendix B).</s><s xml:id="_hNjqUcG">Similarly, aggregate measures, such as accuracy, can hide subgroup under-performance or conceal shortcomings in other metrics <ref type="bibr" target="#b91">[92]</ref>.</s><s xml:id="_hzcN5zb">From this lens, False Positives (FP) and False Negatives (FN), i.e., Type I and Type II errors, respectively, in these scenarios are not critical, and models have been developed to maximize True Positives (TPs).</s><s xml:id="_TN7cxyz">This dominant view promotes deployment bias in novel use cases with the emergence of health-related intelligence embedded into PI systems.</s><s xml:id="_Df8BKPR">For example, given ECG sensor data and AFib detection functionality, Type II errors should be minimized to avoid loss of life.</s><s xml:id="_jk3wdw6">It is thus critical to reassess the conceptualization of PI systems' evaluation practices and datasets and tailor them to their context.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" xml:id="_SvM4tcu">Deployment Bias</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2" xml:id="_tEpZqds">Development in Isolation.</head><p xml:id="_7zBEYqh"><s xml:id="_4yQqQS3">ML models for PI systems are built and evaluated as if they were fully autonomous.</s><s xml:id="_Pa2r32B">In reality, they operate in a complex socio-ethical system moderated by institutions and human decision-makers, also known as the "framing trap" <ref type="bibr" target="#b85">[86]</ref>.</s><s xml:id="_4Y7FTN3">Users may share their mHeatlh data with physicians for interpretation and disease management.</s><s xml:id="_fT9vcgt">Despite good performance in isolation, they may lead to harmful consequences due to human biases, e.g., confirmation bias.</s><s xml:id="_hnhFaMC">Specifically, physicians are more likely to believe AI that supports current practices and opinions <ref type="bibr" target="#b77">[78]</ref>.</s><s xml:id="_UrwNtm3">At the same time, research shows that physicians' perceptions about black male patients' physical activity behavior were significant predictors of their recommendations for surgery, independent of clinical factors, appropriateness, payer, and physician characteristics <ref type="bibr" target="#b98">[99]</ref>.</s><s xml:id="_YyUX7Tm">Such complicated interconnections highlight how evaluating a system in isolation creates unrealistic notions of its benefits and harms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3" xml:id="_xU8fNmK">Biased Interpretation.</head><p xml:id="_6MQUGxv"><s xml:id="_WB2sq2P">Interpreting biased data can result in self-trackers making incorrect inferences or inappropriate tracking decisions.</s><s xml:id="_4wKv4XT">Discomfort with the information revealed and concerns about data quality -which may not be consistent across demographics-can lead to PI abandonment <ref type="bibr" target="#b33">[34]</ref>.</s><s xml:id="_r3yPvPG">Additionally, discrepancies between users' expectations and biased data and subjectivity and uncertainty in data interpretation can fuel rumination (i.e., anxious self-attention and fear of failure), hindering self-improvement efforts and increasing the likelihood of abandonment.</s><s xml:id="_znMmJEJ">This is particularly relevant for health tracking and vulnerable populations, such as those with chronic illnesses, mental health conditions, and women facing fertility challenges, where the association of goals with identity and critical outcomes may increase the propensity for rumination <ref type="bibr" target="#b30">[31]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_3txcC9A">GENERALIZABILITY</head><p xml:id="_5RAJnzY"><s xml:id="_MrQtH64">This section aims to (i) demonstrate the straightforward applicability of our methodology to other datasets and (ii) reveal initial insights about the generality of our findings and future steps.</s><s xml:id="_MU4xqPF">While our analysis was conducted on the MyHeart Counts dataset, some of our findings can be potentially generalized to other scenarios in PI and mHealth.</s><s xml:id="_CxMJ8bU">To showcase this, we apply part of our experiments on two distinct datasets:</s></p><p xml:id="_3UwHBV5"><s xml:id="_xf2xqZc">‚Ä¢ LifeSnaps is a newly-released, medium-scale, multi-modal dataset containing 71M rows of anthropological data, collected unobtrusively for the total course of more than four months by 71 participants.</s><s xml:id="_gVS2uCC">Based on data availability, we consider three protected attributes in Lifesnaps, namely gender, age, and BMI.</s><s xml:id="_nueYaSN">Also, given the lack of official benchmark tasks, we consider the "next-day physical activity</s></p><p xml:id="_j6DX9P5"><s xml:id="_Gv3An4U">prediction" task for model training, same with the MyHeart Counts dataset.</s><s xml:id="_XUY2UTx">gender bmi age 0 0.2 0.4 0.6 0.8 1 1.2 1.4 Real Ratios Dataset Ratios Real vs Dataset Population Ratio Comparison (a) Misrepresented Populations Male Non-male gender 0 500 1000 1500 2000 2500 3000 Number of samples Normal Abnormal bmi &lt;30 &gt;=30 age Majority vs. Minority Group Representation (#Samples) (b) Underrepresented Populations 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Disparate Impact Ratio gender bmi age Protected Attribute Optimal Value Disparate Impact Ratio for Protected Attributes (c) Unevenly Sampled Populations ‚Ä¢ MIMIC-III is an established, large-scale clinical dataset consisting of information concerning more than 38K patients admitted to intensive care units (ICU) at a large tertiary care hospital in the US.</s><s xml:id="_hV72qE3">Based on data availability, in MIMIC-III, we consider six protected attributes, namely gender, ethnicity, language, insurance, religion, and age.</s><s xml:id="_QxENs7U">Contrary to LifeSnaps or MyHeart Counts, there exists a public benchmark suite that includes four different clinical prediction tasks for MIMIC-III <ref type="bibr" target="#b50">[51]</ref>.</s><s xml:id="_gjz5NZ6">For this analysis, we utilize the "in-hospital mortality" task as a binary classification equivalent to the "next-day physical activity prediction" task.</s></p><p xml:id="_MACDju7"><s xml:id="_zVBAaHH">In exploring biases, we identified both commonalities and differences across PI datasets.</s><s xml:id="_NZMyC8q">Regarding the data generation stream, representation biases seem to be the norm in PI datasets, naturally leading to learning and aggregation biases in the model building and implementation stream and highlighting the need for increased awareness among researchers and practitioners in the field.</s><s xml:id="_gruSXJu">Having said that, the identified biases are distinct in each dataset, emerging mostly from their recruitment methodology and the availability of protected attributes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1" xml:id="_CqjqUfk">Bias in Rows Commonalities</head><p xml:id="_zybkY6N"><s xml:id="_GH2RFPT">Both datasets, similarly to MyHeart Counts, suffer from some type of "bias in rows" as seen in Figures <ref type="figure" target="#fig_7">12a</ref> and <ref type="figure" target="#fig_1">13a</ref>.</s><s xml:id="_Bkd9B9F">Specifically, LifeSnaps and MIMIC-III suffer from misrepresented populations.</s><s xml:id="_sYKugbG">In LifeSnaps (Figures <ref type="figure" target="#fig_7">12a</ref> and <ref type="figure" target="#fig_7">12b</ref>), younger people are overrepresented due to university-based recruitment, while in MIMIC-III (Figures <ref type="figure" target="#fig_1">13a</ref> and <ref type="figure" target="#fig_1">13b</ref>) older people are overrepresented due to ICU-based recruitment.</s><s xml:id="_JKJbJdE">Additionally, while gender and ethnicity</s></p><p xml:id="_CjdQDKc"><s xml:id="_qUnemxA">GENDER ETHNICITY LANGUAGE INSURANCE RELIGION AGE 0 0.2 0.4 0.6 0.8 1 1.2 Real Ratios Dataset Ratios Real vs Dataset Population Ratio Comparison (a) Misrepresented Populations 65+ &lt;65 AGE 0 5000 10000 15000 20000 Number of samples White Non-white ETHNICITY Male Female GENDER Insured Uninsured INSURANCE English Non-English LANGUAGE ChristianNon-christian RELIGION Privileged vs. Unprivileged Group Representation (#Samples) Underrepresented Populations 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Disparate Impact Ratio GENDER ETHNICITY LANGUAGE INSURANCE RELIGION AGE Protected Attribute Optimal Value Disparate Impact Ratio for Protected Attributes (c) Unevenly Sampled Populations LANGUAGE INSURANCE RELIGION ETHNICITY GENDER AGE Protected Attribute 0.00 0.25 0.50 0.75 1.00 1.25 DIR Optimal Value Optimal Value Optimal Value Disparate Impact Ratio for Data and Models Modality Data Model Data Model Data Model (d) Data vs. Model Biases Fig. 13.</s><s xml:id="_cwp5aC4">MIMIC-III biases in data generation and model building streams.</s><s xml:id="_e5zkTGj">Data biases are amplified by learning models.</s></p><p xml:id="_jZBP6QB"><s xml:id="_M2UGMBt">representation is improved compared to MyHeart Counts, still white males are overrepresented in all three datasets.</s><s xml:id="_WrPwvek">MIMIC-III, similarly to MyHeartCounts, suffers from underrepresented populations, such as uninsured, non-white, non-English-speaking, or non-christian users (Figure <ref type="figure" target="#fig_1">13b</ref>).</s><s xml:id="_9EwCBj7">These biases are, in turn, propagated to the baseline learning models (Figure <ref type="figure" target="#fig_1">13d</ref>), in line with prior work <ref type="bibr" target="#b81">[82]</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_5nC7nMe">Bias in Columns Differences</head><p xml:id="_CBWQbSU"><s xml:id="_sfqfetK">When "bias in columns" is explored, contrary to the MyHeart Counts data, both datasets are evenly sampled in terms of outcome labels, namely physical activity in LifeSnaps and in-hospital mortality for MIMIC-III (Figures <ref type="figure" target="#fig_7">12c</ref> and <ref type="figure" target="#fig_1">13c</ref>, respectively).</s><s xml:id="_X6nDR2h">Hence, these findings may not directly generalize to other PI datasets but are still included in our methodology for completeness and visibility.</s><s xml:id="_UV3jGYV">Specifically, contrary to population demographics which can capture misrepresented and underrepresented groups, an analysis for unevenly sampled populations is not commonly performed during data exploration, whereas in certain cases, such as MyHeart Counts, it could reveal behavioral discrepancies across populations.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" xml:id="_NkpZ8cq">Model Bias</head><p xml:id="_zWKrGwm"><s xml:id="_rzYPdZy">In line with prior research <ref type="bibr" target="#b81">[82]</ref>, we encounter biases in MIMIC's deep learning benchmark models.</s><s xml:id="_cdjEseE">Specifically, as seen in Figure <ref type="figure" target="#fig_1">13d</ref>, despite even data sampling, models amplify biases for non-English-speaking, uninsured, female, and elderly users.</s><s xml:id="_PmwRsp5">Similar trends, independent of demographics, were noticed in the MyHeart Counts dataset.</s></p><p xml:id="_a59M4Dr"><s xml:id="_wbg5p67">On the contrary, we do not notice any statistically significant differences in the LifeSnaps dataset (Figure <ref type="figure" target="#fig_7">12d</ref>), possibly due to its limited sample size compared to the other two datasets.</s></p><p xml:id="_sBay4SQ"><s xml:id="_jUVHEUm">Overall, these findings concerning generalizability highlight the need for comprehensive data and model evaluation in PI and, by extension, mHealth.</s><s xml:id="_ZBvzAD8">It is high time PI researchers and practitioners looked beyond performance-only metrics to human-centric metrics capturing biases and demographic parity.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_JdBPD7y">DISCUSSION</head><p xml:id="_Z3H9ZkB"><s xml:id="_auw7pvm">Unacknowledged biases in PI applications for health and well-being can have critical personal and societal consequences; they can perpetuate health disparities <ref type="bibr" target="#b68">[69]</ref>, result in misdiagnoses or delayed diagnoses <ref type="bibr" target="#b73">[74]</ref>, affect resource allocation and access to healthcare services <ref type="bibr" target="#b87">[88]</ref>, or reinforce stereotypes and stigma <ref type="bibr" target="#b67">[68]</ref>.</s><s xml:id="_9x6Zu75">This section discusses our findings concerning biases in PI systems and provides guidelines on mitigating identified biases in the ML life cycle ( ¬ß7.1).</s><s xml:id="_Bg38gq8">It also delineates the limitations of our work and areas for future research ( ¬ß7.2).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1" xml:id="_a8FQXkb">Findings &amp; Implications</head><p xml:id="_sG3ARPJ"><s xml:id="_h7AFFEm">7.1.1</s><s xml:id="_suC3Pwr">Data Generation Stream.</s><s xml:id="_Z6p7r5M">As illustrated by our findings, pre-existing historical biases are present in digital biomarkers, due to well-documented phenomena, such as the global inequality in physical activity and the digital divide, leading to data generation that is not representative of the general population.</s><s xml:id="_pfzsbFb">This is indeed the case in the MyHeart Counts dataset, where female, non-white, underweight, overweight or obese, young, and hypertensive users, are undersampled.</s><s xml:id="_9u8NWdr">Unacknowledged historical biases, though, can creep into the ML pipeline perpetuating social injustices.</s><s xml:id="_5pmpKCK">Yet, even within well-sampled user groups, data imbalances, either in terms of user attributes or measured behaviors, are still prevalent due to realistic differences across user segments.</s><s xml:id="_X7XAFDs">Specifically, in PI, we see significant underrepresentation of minority groups across all protected attributes and measured behavioral differences -not necessarily realistic-for users with diabetes, joint issues, unhealthy BMI, non-white users, and females.</s><s xml:id="_cuUefgu">Unresolved representation biases can lead to performance discrepancies for minority groups, which in turn might lead to differences in treatment or care <ref type="bibr" target="#b45">[46]</ref>.</s><s xml:id="_TJgJNJZ">Finally, PI data is susceptible to measurement biases, due to the heterogeneity in input modalities, performance and hardware differences across generations of devices, and usage of third-multiple party apps of unknown accuracy.</s><s xml:id="_hq4kcxD">Females are especially affected by such biases in our dataset, as they tend to own older devices with fewer capabilities and use more fitness-related third-party apps.</s><s xml:id="_Nu6jSsM">Unknown measurement biases in seemingly "objective" sensor data can lead to errors in downstream tasks that disproportionally affect certain protected groups.</s></p><p xml:id="_d7XWXwJ"><s xml:id="_wC8BmHW">In an initial attempt to offer guidance to researchers in the field of ubiquitous computing, we present the following guidelines in the context of the data generation stream on how to mitigate the impact of historical, representation, and measurement biases, respectively:</s></p><p xml:id="_r8jh5EH"><s xml:id="_WkgpM56">Guideline #1: To identify historical biases relevant to the use case at hand, consult prior literature and domain experts (e.g., oximeters are proven susceptible to biases against darker skin tones <ref type="bibr" target="#b45">[46]</ref>) or conduct small-scale feasibility studies with relevant and diverse demographics.</s><s xml:id="_sF5kdHX">Guideline #2: If data are self-collected, aim for diverse user recruitment and collect and report relevant protected attributes (e.g., via datasheets for datasets and data statements).</s><s xml:id="_z7qXf24">Otherwise, evaluate algorithms in generalizable cross-dataset benchmarks [108] and inclusive synthetic data <ref type="bibr" target="#b97">[98]</ref>, whenever possible.</s><s xml:id="_3kTJaPa">In either case, consider appropriate data manipulation actions to alleviate biases, e.g., re-sampling/rebalancing populations conditioned on demographic attributes.</s></p><p xml:id="_6y8smKy"><s xml:id="_rUQUF6h">Guideline #3: When working with data originating from diverse devices, investigate device ownership differences conditioned on demographic attributes.</s><s xml:id="_hxWtBt2">Also, incorporate uncertainty estimation approaches and be transparent about possible measurement error effects in downstream tasks.</s></p><p xml:id="_nYehkp5"><s xml:id="_N22WGYp">Summing up, we believe that our findings shed light on the biases that can creep into the data generation and model building, and implementation streams of PI technologies.</s><s xml:id="_mxxQZ3R">While our mitigation guidelines are by no means exhaustive, they provide a starting point for researchers and practitioners to incorporate bias assessments "by design" in the life cycle of their works to alleviate the potential negative effects of such biases.</s></p><p xml:id="_6AqGVtB"><s xml:id="_URE89CW">7.1.2</s><s xml:id="_Wzj2XuT">Model Building and Implementation Stream.</s><s xml:id="_WHP7xFZ">Based on our results, digital biomarkers representation biases can be propagated or even amplified by learning models, regardless of the inclusion of protected attributes in the feature set.</s><s xml:id="_UjcjuxF">This is due to the existence of proxy variables in PI data that can be used by models to infer hidden protected attributes.</s><s xml:id="_qWkH4JX">Such aggregation biases are also prevalent in our use case for users with joint issues, diabetes, hypertension, and female users.</s><s xml:id="_nS9kQF6">These biases may (or may not) lead to discrimination depending on the context.</s><s xml:id="_guq7Mnr">Yet, mitigating them is the safest way to ensure fairness.</s><s xml:id="_Ag3zqcQ">Additionally, common learning choices in PI, such as personalization, can introduce learning biases, if trained on biased data.</s><s xml:id="_5FaRtNF">In our case, they perform worse -in terms of bias-across all attributes, while, in extreme cases (e.g., diabetic users), they can even introduce maximum bias.</s><s xml:id="_vWWhKFa">Accuracy gains emerging from alternative learning choices can be tempting, but their trade-offs should be thoroughly assessed.</s><s xml:id="_p6jPt6r">On a different note, our empirical results illustrate that model performance is highly susceptible to the representativeness of the PI benchmark used and highlight how evaluation biases can affect ubiquitous models in the evaluation phase.</s><s xml:id="_mWHDwkW">In such cases, performance and fairness drift can emerge if the evaluation data is not representative of the target population.</s><s xml:id="_vmwgQpZ">Finally, the application of ML in PI is not free of deployment biases, which can emerge from outdated evaluation practices emerging from the PI systems' early applications or the false assumption of autonomous PI systems' existence.</s><s xml:id="_dCfSnXU">Yet, mischoosing evaluation metrics or focusing solely on aggregate metrics can hide discrepancies in performance for minority groups.</s><s xml:id="_yMKz3FB">Developing high-stakes systems in isolation might also lead to (unintended) system misuse.</s></p><p xml:id="_uFyeSsh"><s xml:id="_U2ppGKN">To provide guidance in the context of model building and implementations, we offer guidelines to alleviate the potential negative effects of the aggregation, learning, evaluation, and deployment biases, respectively: Guideline #4: Utilize fairness toolkits, such as FairLearn, AIF360, and Aequitas for implementations of pre-, in-, and post-processing bias mitigation algorithms and fairness metrics.</s><s xml:id="_wpPB2Cq">Guideline #5: Move beyond accuracy in evaluating learning paradigms by incorporating fairness metrics in the evaluation pipeline of ML models, conditioning performance on intersections of protected attributes.</s><s xml:id="_bxCpqa7">Guideline #6: Aim for representative and realistic evaluation datasets -beyond carefully-curated benchmarks-, if available, or reassess your model after deployment.</s><s xml:id="_pBedpH6">Re-train with the target population's data if you encounter performance drifts conditioned on demographic attributes.</s><s xml:id="_28hHypb">Guideline #7: Choose multiple, appropriate fairness metrics based on "fairness trees" <ref type="bibr" target="#b84">[85]</ref> and domain expertise for the use case at hand.</s><s xml:id="_9XFAHtv">Consider a human-in-the-loop design approach for high-stakes applications to account for human biases that affect system design.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2" xml:id="_9aEZDcy">Limitations &amp; Future Work</head><p xml:id="_VvRhX3P"><s xml:id="_wQA6Wsh">While MyHeart Counts offers scale and access to protected attributes, as outlined in ¬ß3, and our generalizability study supports our findings as described in ¬ß6, additional analyses may be necessary to fully comprehend bias in PI.</s><s xml:id="_nw2jQT3">For instance, certain protected attributes in the dataset have incomplete categorization, e.g., gender is treated as a binary concept, while others might be fully absent regardless of relevance to the use case, e.g., physical or mental disability and pregnancy.</s><s xml:id="_kbubU9x">At the same time, the selected dataset is US-based, not capturing activity patterns across the global population.</s><s xml:id="_WsAShu9">Hence our findings might not be directly applicable across protected attributes and geographical contexts.</s><s xml:id="_9xMsXa5">Finally, while activity tracking is the most common functionality in PI systems and prior work has highlighted worldwide physical activity inequality <ref type="bibr" target="#b3">[4]</ref>, our dataset does not capture more advanced health features, such as heart monitoring and fertility tracking.</s><s xml:id="_4c2sDUD">Still, it is important to recognize that different use cases might incorporate different biases.</s><s xml:id="_hcSbWGx">Hence, while our findings shed light on the previously unexplored field of PI biases, they should be further corroborated across different contexts, such as demographics, geographical regions, and use cases.</s></p><p xml:id="_8aqmNpJ"><s xml:id="_n3QAfE2">Appropriate PI datasets for fueling future fairness research in the domain are still lacking.</s><s xml:id="_gSesB5V">Due to the sensitivity of the data at hand, many datasets are proprietary with restrictive Institutional Review Board (IRB) agreements, but inclusive, open datasets could significantly advance the domain.</s><s xml:id="_F4AzCJ3">Also, given the prevalence of small-scale datasets, future work should focus on quantifying biases in small digital biomarkers data, as realistically, most institutions will never acquire big data <ref type="bibr" target="#b10">[11]</ref>.</s><s xml:id="_NcsJNWH">Additionally, due to closed-sourced data and algorithms, there is a lack of established benchmarks, especially regarding emerging PI tasks, such as fertility prediction, or AFib detection.</s><s xml:id="_pTYDewF">To this end, similarly to the work of Harutyunyan et al. <ref type="bibr" target="#b50">[51]</ref>, future work should create inclusive and representative benchmarks for tasks within the PI domain.</s><s xml:id="_2JP2aCT">Beyond that, there is work to be done in quantifying and mitigating bias in sequential physiological and behavioral data.</s><s xml:id="_3d8WhHG">For instance, many PI tasks are formulated as regression problems, but regression-specific fairness metrics and mitigation approaches are limited in the literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref>.</s><s xml:id="_TnjHvEd">Finally, due to privacy considerations for sensitive digital biomarkers, many times PI data are not accompanied by protected attributes for the population they describe, making it cumbersome to perform a bias and fairness evaluation.</s><s xml:id="_7Dxg9cu">To this end, future work should investigate the space of "fairness in unawareness", or, in other words, how you can quantify and mitigate biases in the absence of protected attributes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" xml:id="_x9jXvUV">CONCLUSIONS</head><p xml:id="_7FAtjPb"><s xml:id="_6EBM7MG">This paper presents the first-of-its-kind, comprehensive study of bias in PI by analyzing the most extensive digital biomarkers data to date.</s><s xml:id="_yF8dPww">In response to our RQs, we show that bias exists across all stages of the life cycle, both in the data generation and model building and implementation streams.</s><s xml:id="_xKUEtCZ">Different user minorities are affected by diverse types of bias, but users with diabetes, joint issues, or hypertension and female users show higher degrees of impact adversity in our MyHeart Counts use case due to representation, aggregation, and learning biases.</s><s xml:id="_9PcYGjF">Our findings echo concerns similar to those raised in the evaluation for healthcare technologies <ref type="bibr" target="#b2">[3]</ref>.</s><s xml:id="_4HVv3kH">While some of our findings are specific to the investigated use case, they can mostly be extended to other PI tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hv2Kugy">A MODEL ARCHITECTURES A.1 Baseline Model</head><p xml:id="_ZKDH3dm"><s xml:id="_hh2UdCC">For our baseline model, we consider the following setting: we are given a time-series dataset  = { ,0 ,  ,1 } of users segmented into two groups, 0 and 1, based on protected attribute  (e.g., gender, age, etc.).</s><s xml:id="_ZtCBpK7">The user data within each group are denoted as  , = { , 1 , . . .</s><s xml:id="_Ak9bdRA">,  ,  }, where  = {0, 1} and K is the number of users per group, conditioned on protected attribute .</s><s xml:id="_pTeUS3f">Furthermore, the data of each user are stored as   = {  ,   }, where input time series (step count values) of users  = 1, . . .</s><s xml:id="_pUmaVnG">, , are stored in   ‚àà R   1 , where   = 48 (unaware model) is the length (in time steps) of a sample daily activity in the data, or   = 56 (aware model) is the length of a sample daily activity in the data plus the protected attribute features.</s><s xml:id="_WjGEYCM">Formally, our deep neural network architecture receives as input the users' daily activity samples ( ) and passes them through LSTM layers with parameters   = {  ,   }, weight matrix, and bias, respectively, for each layer , to produce the output ≈∑.</s><s xml:id="_x86hjRA">The optimization of Table <ref type="table">4</ref>. WAE Metrics' definitions, formulas, and task and bias interpretations specific to our use case.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_P74mXjc">Metric Definition</head><p xml:id="_K4NCtZH"><s xml:id="_sMh6zrJ">Formula Task Interpretation Bias Interpretation</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_BKccvjG">DIR</head><p xml:id="_GJfqVkp"><s xml:id="_rdnUqhw">The ratio of base or selection rates between unprivileged and privileged groups.</s><s xml:id="_YHfaDvw">A low  ( &lt; 1) indicates that the unprivileged user group systematically receives fewer high activity goals.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_rHZQX62">SPD</head><p xml:id="_ehxbcHS"><s xml:id="_ETTbVBN">The difference in selection rates between unprivileged and privileged groups.</s><s xml:id="_pjg5Xq6">Fairness Metrics' Value Ranges.</s><s xml:id="_5gJBCvw">Figure <ref type="figure" target="#fig_10">15</ref> gives a graphical overview of all fairness metrics discussed.</s><s xml:id="_EtRpqWs">We notice that difference-based metrics have a different range of values compared to ratio-based metrics.</s><s xml:id="_Uwg5xQV">Specifically, difference-based metrics are within the [-100%, +100%] range, while ratio-based metrics are within the [0, ‚àû] range.</s><s xml:id="_Bh5CPa2">For both categories, a value of 1.0 is optimal, indicating demographic parity.</s><s xml:id="_euHaJyK">Anything greater or less than the optimal value indicates some level of bias.</s><s xml:id="_HQRJyY4">According to AIF360, accepted difference-based metrics' values are within the range [-0.1, +0, 1], and accepted ratio-based metrics' values are within [0.8,1.25],</s><s xml:id="_FNDrukk">but such ranges are not universally accepted and might be adjusted on a task-by-task basis.</s><s xml:id="_bQ8UN8p">Table <ref type="table">7a</ref> indicates which user group, namely privileged or unprivileged, benefits based on a group fairness metric's value.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Y3KcesJ">FOR Ratio</head><p xml:id="_hc4R5AD"><s xml:id="_eJUZAka">The ratio between the outcomes wrongly categorized as negative, i.e., FN, and the total number of classified negative outcomes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ayeq4F4">ùêπùëÇùëÖ ùê∑=u ùêπùëÇùëÖ ùê∑=p</head><p xml:id="_RUeHNB5"><s xml:id="_Xa8AaQz">From all the users that were given low activity goals -rightfully so or not-, how many were actually highly active?</s></p><p xml:id="_T9vQNZU"><s xml:id="_62tdqFc">A high  Ratio (FOR Ratio &gt; 1) indicates that the unprivileged user group systematically receives more wrong low activity goals compared to the privileged user group.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_VkYKS5m">ERR</head><p xml:id="_6PUgMW6"><s xml:id="_5DBp6hM">The ratio between the erroneous outcomes, i.e., FN or FP, and the total number of outcomes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_f2hBeSz">ùê∏ùëÖ ùê∑=u ùê∏ùëÖ ùê∑=p</head><p xml:id="_G6xQM8x"><s xml:id="_CXxtcJJ">, where ER =   +    +  How many times was the activity level prediction wrong?</s></p><p xml:id="_kRybPqb"><s xml:id="_N9PdZrA">A high  (ERR &gt; 1) indicates that the unprivileged user group systematically receives more wrong goalslow or high-compared to the privileged user group.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><figDesc><div><p xml:id="_c9ceqqJ"><s xml:id="_qGwvGEE">(a) Sources of harm in the data generation stream.(b)</s><s xml:id="_UFDxzsP">Sources of harm in the model building and implementation stream.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc><div><p xml:id="_S7bfeRV"><s xml:id="_UECvVu4">Fig. 1.</s><s xml:id="_6tzyAUs">Sources of harm in the data (top) and model building and implementation (bottom) streams [92].</s><s xml:id="_2eH9FKh">The training, test, and benchmark sets are common across figures.</s></p></div></figDesc><graphic coords="5,103.23,237.57,364.98,115.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc><div><p xml:id="_pQCEWeU"><s xml:id="_Sap2mHd">Fig. 4. A bar plot showcasing the number of samples per user segment split based on various protected attributes.</s><s xml:id="_qWNJDAC">We see significant underrepresentation of minority user segments across almost all attributes.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc><div><p xml:id="_myVvAGz"><s xml:id="_rerrGGs">Fig. 5. Differences in the price of participants' phones as of September 2016 based on gender (left) and BMI (right).</s><s xml:id="_DpZAkHA">Females and people with BMI within the normal range tend to own older and cheaper phones with fewer capabilities.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc><div><p xml:id="_4Zqwaqv"><s xml:id="_U385y3A">Fig.9.</s><s xml:id="_pscRbRW">DIR unaware baseline model comparison between single-attribute and intersectional user groups.</s><s xml:id="_qUwFCpq">Intersectional groups are drawn from the minority and majority classes.</s><s xml:id="_qxUZyv4">The "one-size-fits-all" models' biases are more prevalent in intersectional models.</s></p></div></figDesc><graphic coords="12,148.29,336.19,315.44,78.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc><div><p xml:id="_ARAg37q"><s xml:id="_hVnzguN">Fig. 10.</s><s xml:id="_4rWKNmb">Our personalized deep learning architecture inspired by CultureNet [84].</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5. 4 . 1</head><label>41</label><figDesc><div><p xml:id="_vysK9J8"><s xml:id="_wZYthTx">Changing Deployment Scenarios.</s><s xml:id="_xcG5xAN">PI's most active research areas are Human-Activity Recognition and Sleep Classification.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 12 .</head><label>12</label><figDesc><div><p xml:id="_TdMpNaW"><s xml:id="_uhf2ph7">Fig. 12. LifeSnaps biases in the data generation and model building and implementation streams.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc><div><p xml:id="_wndBCRq"><s xml:id="_VFEUF6z">Pr( ≈∂ = pos_label | D = u) Pr( ≈∂ = pos_label | D = p)How many users receive high activity goals in the unprivileged group compared to the privileged group?</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc><div><p xml:id="_b4dEPne"><s xml:id="_Ascu4yz">Pr( ≈∂ = pos_label | D = u) -Pr( ≈∂ = pos_label | D = p) Same as above A low  ( &lt; 0)indicates that the unprivileged user group systematically receives fewer high activity goals.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 .</head><label>15</label><figDesc><div><p xml:id="_euZTnmn"><s xml:id="_mPn2s2S">Fig. 15.</s><s xml:id="_FRCQQDN">A graphical overview of the fairness metrics discussed.</s></p></div></figDesc><graphic coords="30,103.23,104.59,405.55,485.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="25,125.76,104.61,360.49,139.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc><div><p xml:id="_cm57859"><s xml:id="_dGXXhrp">The protected attributes in the MyHeart Counts data.</s><s xml:id="_wWwBnX9">For the purpose of the bias analysis, we binarize non-binary attributes to ensure a sufficient sample size per group and compatibility with popular bias metrics.</s></p></div></figDesc><table><row><cell></cell><cell>Original Protected Attribute Values</cell><cell cols="2">Binarized Protected Attribute Values</cell></row><row><cell>Attribute</cell><cell>Original Groups</cell><cell>Majority Group</cell><cell>Minority Group</cell></row><row><cell>Gender</cell><cell>Male, Female, N/A</cell><cell>Male</cell><cell>Female</cell></row><row><cell>Ethnicity</cell><cell cols="2">White, Asian, Black, Hispanic, American Indian, Pacific Islander, Other, N/A White</cell><cell>Non-white</cell></row><row><cell>Age</cell><cell>Integer Number, N/A</cell><cell cols="2">&lt;65 (lower risk of complications) &gt;=65 (higher risk of complications)</cell></row><row><cell>BMI</cell><cell>Real Number (height and weight), N/A</cell><cell>&lt;18.5 or =&gt;25 (unhealthy)</cell><cell>=&gt;18.5 and &lt;25 (healthy)</cell></row><row><cell cols="2">Heart Condition Yes, No, N/A</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Hypertension</cell><cell>Yes, No, N/A</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Joint Problem</cell><cell>Yes, No, N/A</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Diabetes</cell><cell>Yes, No, N/A</cell><cell>No</cell><cell>Yes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc><div><p xml:id="_SRPdxwe"><s xml:id="_dwxfrr8">Example input data for the physical activity prediction use case.</s><s xml:id="_BXkAeUv">The step counts per hour for the past 48 hours are the features, and the total number of the next day's steps is the label.</s><s xml:id="_fSycb9p">The user ID and timestamps are not used for learning.</s></p></div></figDesc><table><row><cell></cell><cell>Features</cell><cell></cell><cell>Label</cell></row><row><cell cols="4">user_id timestamp steps at t-48h ... steps at t-1h next day's steps</cell></row><row><cell>1</cell><cell>23-11-2022 1040</cell><cell>... 300</cell><cell>8500</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 .</head><label>5</label><figDesc><div><p xml:id="_B7yGsD9"><s xml:id="_DfMKh6C">Hybrid Metrics' definitions, formulas, and task and bias interpretations specific to our use case.</s></p></div></figDesc><table><row><cell>Metric</cell><cell>Definition</cell><cell>Formula</cell><cell cols="5">Task Interpretation Bias Interpretation</cell></row><row><cell cols="2">FPR Ratio The ratio between the num-</cell><cell>ùêπ ùëÉùëÖ ùê∑=u</cell><cell cols="2">From all the low active</cell><cell>A</cell><cell>low</cell><cell>ùêπ ùëÉùëÖ</cell><cell>Ratio</cell></row><row><cell></cell><cell>ber of negative outcomes</cell><cell>ùêπ ùëÉùëÖ ùê∑=p</cell><cell cols="2">users, how many</cell><cell cols="2">(FPR Ratio</cell><cell>&lt;</cell><cell>1) indi-</cell></row><row><cell></cell><cell>wrongly categorized as posi-</cell><cell></cell><cell cols="2">wrongfully received</cell><cell cols="3">cates that the privileged low</cell></row><row><cell></cell><cell>tive, i.e., false positives (FP),</cell><cell></cell><cell cols="2">high activity goals?</cell><cell cols="3">active user group system-</cell></row><row><cell></cell><cell>and the total number of ac-</cell><cell></cell><cell></cell><cell></cell><cell cols="3">atically receives more high</cell></row><row><cell></cell><cell>tual negative outcomes re-</cell><cell></cell><cell></cell><cell></cell><cell cols="3">activity goals compared to</cell></row><row><cell></cell><cell>gardless of classification.</cell><cell></cell><cell></cell><cell></cell><cell cols="3">the unprivileged low active</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">user group.</cell></row><row><cell cols="2">FNR Ratio The ratio between the num-</cell><cell>ùêπ ùëÅ ùëÖ ùê∑=u</cell><cell cols="2">From all the highly</cell><cell>A</cell><cell>high</cell><cell>ùêπ ùëÅ ùëÖ</cell><cell>Ratio</cell></row><row><cell></cell><cell>ber of positive outcomes</cell><cell>ùêπ ùëÅ ùëÖ ùê∑=p</cell><cell cols="2">active users, how</cell><cell cols="3">(FNR Ratio &gt; 1) indi-</cell></row><row><cell></cell><cell>wrongly categorized as neg-</cell><cell></cell><cell>many</cell><cell>wrongfully</cell><cell cols="3">cates that the unprivileged</cell></row><row><cell></cell><cell>ative, i.e., false negatives</cell><cell></cell><cell cols="2">received low activity</cell><cell cols="3">highly active user group</cell></row><row><cell></cell><cell>(FN), and the total number</cell><cell></cell><cell>goals?</cell><cell></cell><cell cols="3">systematically</cell><cell>receives</cell></row><row><cell></cell><cell>of actual positive outcomes</cell><cell></cell><cell></cell><cell></cell><cell cols="3">more low activity goals</cell></row><row><cell></cell><cell>regardless of classification.</cell><cell></cell><cell></cell><cell></cell><cell cols="3">compared to the privileged</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">highly active user group.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_Qz6PwBW"><s xml:id="_MQSEUzk">Proc.</s><s xml:id="_VHmQCFt">ACM Interact.</s><s xml:id="_zAV4QRu">Mob.</s><s xml:id="_n2tBkX9">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_Gb4eJ64">Publication date: September 2023.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_crC4mzY"><s xml:id="_QueuhBn">Proc.</s><s xml:id="_Mw2X3RE">ACM Interact.</s><s xml:id="_Nfxxrtg">Mob.</s><s xml:id="_fbGu93Q">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_VqdTD4D">Publication date: September 2023.</s><s xml:id="_MmeAxDq">Uncovering Bias in Personal Informatics ‚Ä¢ 139:5</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_t5tWBDq"><s xml:id="_z4SXUu3">Proc.</s><s xml:id="_KUK8mRx">ACM Interact.</s><s xml:id="_QPRaeAN">Mob.</s><s xml:id="_XJzfAa4">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_G54Wnth">Publication date: September 2023.</s><s xml:id="_5FEEu3v">Uncovering Bias in Personal Informatics ‚Ä¢ 139:7</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p xml:id="_KbZQH6D"><s xml:id="_3NH9Eqw">Proc.</s><s xml:id="_3TKJ6sj">ACM Interact.</s><s xml:id="_eypKr2m">Mob.</s><s xml:id="_T2HCm8Z">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_DaQrEB7">Publication date: September 2023.</s><s xml:id="_KvyWqjU">Uncovering Bias in Personal Informatics ‚Ä¢ 139:17</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p xml:id="_7EdETCZ"><s xml:id="_YCgJEax">Proc.</s><s xml:id="_S7bcFtR">ACM Interact.</s><s xml:id="_dCR3R8Y">Mob.</s><s xml:id="_z2xTc7u">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_Q8DBCKp">Publication date: September 2023.</s><s xml:id="_JWFRhQR">Uncovering Bias in Personal Informatics ‚Ä¢ 139:19</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p xml:id="_FCM7Rfz"><s xml:id="_GyEhmbE">Proc.</s><s xml:id="_gEvJNhH">ACM Interact.</s><s xml:id="_XT3Fsdk">Mob.</s><s xml:id="_Qeg7vJ9">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_DYwKZr7">Publication date: September 2023.</s><s xml:id="_pAXxnsa">Uncovering Bias in Personal Informatics ‚Ä¢ 139:27</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p xml:id="_YmYdqwK"><s xml:id="_48ZUBVt">Proc.</s><s xml:id="_vXNZPTN">ACM Interact.</s><s xml:id="_D5aZaEt">Mob.</s><s xml:id="_4q7gQGu">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_BXF6XYx">Publication date: September 2023.</s><s xml:id="_Rb8ehB5">139:28 ‚Ä¢ Yfantidou et al.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p xml:id="_aNFVuhd"><s xml:id="_xKbTGYb">Proc.</s><s xml:id="_DyxCZH6">ACM Interact.</s><s xml:id="_uybgAcJ">Mob.</s><s xml:id="_qHyqJJX">Wearable Ubiquitous Technol., Vol. 7, No. 3, Article 139.</s><s xml:id="_mvYhJX8">Publication date: September 2023.</s><s xml:id="_YmVWuTS">Uncovering Bias in Personal Informatics ‚Ä¢ 139:29</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_TCXbJnV">ACKNOWLEDGMENTS</head><p xml:id="_mmFVayg"><s xml:id="_JdE8m8Q">This project has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Sk≈Çodowska-Curie</rs> grant agreement No <rs type="grantNumber">813162</rs>.</s><s xml:id="_BSdWeaK">The content of this paper reflects only the authors' view and the Agency and the Commission are not responsible for any use that may be made of the information it contains.</s><s xml:id="_m69EBqe">Results presented in this work have been produced using the <rs type="institution">Aristotle University of Thessaloniki Compute Infrastructure</rs> and Resources.</s><s xml:id="_2gqxjKZ">The authors would like to acknowledge the support provided by the <rs type="funder">Scientific Computing Office</rs> throughout the progress of this research work.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nYVhhPr">
					<idno type="grant-number">813162</idno>
					<orgName type="grant-name">Marie Sk≈Çodowska-Curie</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_etFj4TE"><p xml:id="_XNFWNYc"><s xml:id="_FaV7thw">the network parameters for LSTM layers is obtained by minimizing the binary cross entropy loss   defined as: where  represents the number of training samples from both datasets { ,0 ,  ,1 }.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_b8wMjry">A.2 Personalized Model</head><p xml:id="_g9RpmAk"><s xml:id="_FeTFwcT">For our personalized model, formally, the learning during the fine-tuning process is attained through the last layer in the network, one for the minority and one for the majority user group.</s><s xml:id="_s2PWaW9">Before further optimization, the layers are initialized as  ,0</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CyY2PCW">4</head><p xml:id="_n2qx9n8"><s xml:id="_vzjPVME">‚Üê  4 and  ,1</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ugq9ABj">4</head><p xml:id="_CJQyhNX"><s xml:id="_HPcNgeD">‚Üê  4 , and then fine-tuned using the data from 0 ( ,0 ) and 1 ( ,1 ), respectively, for each protected attribute  as:</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_hjvgvtA">B FAIRNESS TAXONOMY IN MACHINE LEARNING</head><p xml:id="_a3D5Xb8"><s xml:id="_zCgwbUu">Viewed through the lens of quantitative science, ML research has broadly grouped fairness into two categories: individual fairness and group fairness.</s><s xml:id="_9HbJnnX">In the broad sense, group fairness partitions the general population into groups based on sensitive (a.k.a.</s><s xml:id="_xYhH4GN">protected) attributes and seeks statistical equality across groups.</s><s xml:id="_FyHp4TD">On the other hand, individual fairness seeks for similar individuals to be treated similarly <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b72">73]</ref>.</s></p><p xml:id="_a8KDaaq"><s xml:id="_TYFsd8p">In individual fairness, determining whether individuals are similar requires first defining what features are relevant to fairness <ref type="bibr" target="#b37">[38]</ref>.</s><s xml:id="_SDMEZTf">However, in PI, it would be incomplete to define such similarity solely based on digital behavioral biomarkers, such as steps, or heart rate, as hidden contextual information might be significantly more relevant.</s><s xml:id="_69emzt7">To this end, we proceed with group fairness from now on.</s><s xml:id="_hTJQYUH">This decision translates to our use case as exploring significant differences in allocation, representation, or error rates regarding future step goals across different population segments.</s><s xml:id="_6EsfZNd">For example, do females get systematically lower step goals than males?</s><s xml:id="_c7wX2T9"><ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b108">109]</ref>.</s><s xml:id="_kUqPmk5">The WAE viewpoint considers that all groups have similar abilities to perform the task, e.g., all groups of people are equally capable of walking more, while the WYSIWYG viewpoint holds that the data reflect each group's ability to perform the task, e.g., some groups of people might be less capable.</s></p><p xml:id="_DUhxXSE"><s xml:id="_s8Czj5m">Overall, every metric tries to quantify the difference in performance between privileged and unprivileged users.</s><s xml:id="_6xARK5f">Here, we present the most common fairness metrics across all viewpoints, namely WAE, WYSIWYG, and hybrid (Figure <ref type="figure">14</ref>).</s><s xml:id="_5jRFaqm">The confusion matrix (see Table <ref type="table">3</ref>) is the heart of ML performance measurement and is also used in the fairness metrics definitions below.</s><s xml:id="_NhYs2G5">All metrics are expressed as ratios or differences between unprivileged (u) and privileged (p) groups.</s><s xml:id="_RvuhZkp">Note that  is the user sample, ≈∂ is the predicted label, and "pos_label" is the favorable outcome scenario (e.g., high physical activity).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_W5r9vwa">WAE Metrics.</head><p xml:id="_sx9fjU3"><s xml:id="_hyjNnCS">The WAE viewpoint supports that data, e.g., step counts, may contain biases, so the distribution differences across groups should not be mistaken for a difference in ability.</s><s xml:id="_NXNacqr">The two most commonly used WAE metrics are the Disparate Impact Ratio (DIR) and the Statistical Parity Difference (SPD) (Table <ref type="table">4</ref>).</s></p><p xml:id="_ajGgpR8"><s xml:id="_yAq8SKh">Hybrid Metrics.</s><s xml:id="_fD7JYkM">Hybrid metrics lie in-between the two viewpoints.</s><s xml:id="_Q6dKcUS">The most commonly used hybrid metrics, depending on the problem's context, are the False Positive Rate (FPR) Ratio, the False Negative Rate (FNR) Ratio, the False Omission Rate (FOR) Ratio, and the Error Rate Ratio (ERR) (Table <ref type="table">5</ref>).</s></p><p xml:id="_ABvfCV3"><s xml:id="_YwJE8G3">WYSIWYG Metrics.</s><s xml:id="_eSqDrtb">The WYSIWYG viewpoint supports that data correlates well with future behavior and that they can be used to compare the users' abilities.</s><s xml:id="_aM6JMdV">The two most commonly used WAE metrics are the Average Odds Difference (AOD) and the Equal Opportunity Difference (EOD) (Table <ref type="table">6</ref>).</s><s xml:id="_tdZWKH5">Table <ref type="table">6</ref>.</s><s xml:id="_pFXBnuD">WYSIWYG Metrics' definitions, formulas, and task and bias interpretations specific to our use case.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_jFgBdaz">Metric Definition</head><p xml:id="_ZhN38QE"><s xml:id="_HRyrWwd">Formula Task Interpretation Bias Interpretation</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mVE9Sje">EOD</head><p xml:id="_Xcdy757"><s xml:id="_T7dr8kA">The difference of true positive rates between the unprivileged and the privileged groups.</s></p><p xml:id="_Ge9wRXk"><s xml:id="_Ut46smf">TPR =u -TPR =p From all the highly active users, how many were actually given high activity goals?</s></p><p xml:id="_x9nS4rH"><s xml:id="_BhcVRaW">A low  ( &lt; -0.1) indicates that the unprivileged highly active user group systematically receives fewer high activity goals compared to the privileged highly active user group.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zFhWKmm">AOD</head><p xml:id="_S3YpwXZ"><s xml:id="_jpavQPj">The average difference between the FPR and the TPR between unprivileged and privileged groups.</s></p><p xml:id="_sYwKH6T"><s xml:id="_Sas6qQJ">( =u - =p )+( =u - =p ) 2</s></p><p xml:id="_ymy5XWN"><s xml:id="_eDA78tr">TBD TBD</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_BQAdDjN">Machine learning and health care disparities in dermatology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Adewole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avery</forename><surname>Adamson</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yaRXfe4">JAMA dermatology</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="1247" to="1248" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Adewole S Adamson and Avery Smith. 2018. Machine learning and health care disparities in dermatology. JAMA dermatology 154, 11 (2018), 1247-1248.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_VnvBfmf">Fair regression: Quantitative definitions and reduction-based algorithms</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dud√≠k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_kCFVqqR">International Conference on Machine Learning. PMLR, JMLR</title>
		<meeting><address><addrLine>Campridge, MA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alekh Agarwal, Miroslav Dud√≠k, and Zhiwei Steven Wu. 2019. Fair regression: Quantitative definitions and reduction-based algorithms. In International Conference on Machine Learning. PMLR, JMLR, Campridge, MA, United States, 120-129.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_bKGuWmR">Fairness in machine learning for healthcare</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Aurangzeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carly</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Teredesai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mztCzEx">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3529" to="3530" />
		</imprint>
	</monogr>
	<note type="raw_reference">Muhammad Aurangzeb Ahmad, Arpit Patel, Carly Eckert, Vikas Kumar, and Ankur Teredesai. 2020. Fairness in machine learning for healthcare. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. Association for Computing Machinery, New York, NY, USA, 3529-3530.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_sp2pgdQ">Large-scale physical activity data reveal worldwide activity inequality</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Althoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rok</forename><surname>Sosiƒç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abby</forename><forename type="middle">C</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">L</forename><surname>Delp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature23018</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_enEGdcZ">Nature</title>
		<imprint>
			<biblScope unit="volume">547</biblScope>
			<biblScope unit="page" from="336" to="339" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tim Althoff, Rok Sosiƒç, Jennifer L Hicks, Abby C King, Scott L Delp, and Jure Leskovec. 2017. Large-scale physical activity data reveal worldwide activity inequality. Nature 547, 7663 (2017), 336-339.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_fRADrVn">How well iPhones measure steps in free-living conditions: cross-sectional validation study</title>
		<author>
			<persName><forename type="first">Shiho</forename><surname>Amagasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masamitsu</forename><surname>Kamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Sasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noritoshi</forename><surname>Fukushima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Min</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeru</forename><surname>Inoue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_7RRZuNK">JMIR mHealth and uHealth</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10418</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shiho Amagasa, Masamitsu Kamada, Hiroyuki Sasai, Noritoshi Fukushima, Hiroyuki Kikuchi, I-Min Lee, Shigeru Inoue, et al. 2019. How well iPhones measure steps in free-living conditions: cross-sectional validation study. JMIR mHealth and uHealth 7, 1 (2019), e10418.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_gmNvuWr">A public domain dataset for human activity recognition using smartphones</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><forename type="middle">Parra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reyes</forename><surname>Ortiz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-40728-4_54</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hwukaPH">Proceedings of the 21th International European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
		<meeting>the 21th International European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning<address><addrLine>Basel, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>MDPI</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
	<note type="raw_reference">Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra Perez, and Jorge Luis Reyes Ortiz. 2013. A public domain dataset for human activity recognition using smartphones. In Proceedings of the 21th International European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. MDPI, Basel, Switzerland, 437-442.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_8wFs5qP">Empowering people to live a healthier day</title>
		<author>
			<persName><surname>Apple</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5ZRgT6X">Health Report</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">60</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Apple. 2022. Empowering people to live a healthier day. Health Report 1 (2022), 60.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>Apple</surname></persName>
		</author>
		<ptr target="https://www.apple.com/gr/newsroom/2022/07/how-apple-is-empowering-people-with-their-health-information/" />
		<title level="m" xml:id="_VddWBCV">How Apple is empowering people with their health information -apple</title>
		<imprint>
			<date type="published" when="2022-11-14">2022. 14-Nov-2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Apple. 2022. How Apple is empowering people with their health information -apple.com. https://www.apple.com/gr/newsroom/ 2022/07/how-apple-is-empowering-people-with-their-health-information/. [Accessed 14-Nov-2022].</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main" xml:id="_Fcghgyg">Exploring fairness in machine learning for international development</title>
		<author>
			<persName><forename type="first">Yazeed</forename><surname>Awwad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Najafian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Teodorescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>CITE MIT D-Lab</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note type="raw_reference">Yazeed Awwad, Richard Fletcher, Daniel Frey, Amit Gandhi, Maryam Najafian, and Mike Teodorescu. 2020. Exploring fairness in machine learning for international development. Technical Report. CITE MIT D-Lab.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_vteP8x5">Bias on the Web</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209581</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6nDvPAg">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="54" to="61" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ricardo Baeza-Yates. 2018. Bias on the Web. Commun. ACM 61, 6 (2018), 54-61.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<ptr target="https://www.kdnuggets.com/2018/10/big-small-right-data.html" />
		<title level="m" xml:id="_SmdMjBd">BIG, small or Right Data: Which is the proper focus</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ricardo Baeza-Yates. 2018. BIG, small or Right Data: Which is the proper focus. https://www.kdnuggets.com/2018/10/big-small-right- data.html.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_nz48FDJ">Differential privacy has disparate impact on model accuracy</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_aPcXYC9">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15479" to="15488" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eugene Bagdasaryan, Omid Poursaeed, and Vitaly Shmatikov. 2019. Differential privacy has disparate impact on model accuracy. Advances in Neural Information Processing Systems 32 (2019), 15479--15488.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_c6Dv3nm">UBIWEAR: An end-to-end, data-driven framework for intelligent physical activity prediction to empower mHealth interventions</title>
		<author>
			<persName><forename type="first">Asterios</forename><surname>Bampakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Yfantidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athena</forename><surname>Vakali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mpqPaxp">2022 IEEE International Conference on E-health Networking, Application &amp; Services (HealthCom)</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
	<note type="raw_reference">Asterios Bampakis, Sofia Yfantidou, and Athena Vakali. 2022. UBIWEAR: An end-to-end, data-driven framework for intelligent physical activity prediction to empower mHealth interventions. In 2022 IEEE International Conference on E-health Networking, Application &amp; Services (HealthCom). IEEE, IEEE, New York, NY, United States, 56-62.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_g4UxScd">Rehumanized crowdsourcing: A labeling framework addressing bias and ethics in machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nat√£</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monchu</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZQtfTZW">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nat√£ M Barbosa and Monchu Chen. 2019. Rehumanized crowdsourcing: A labeling framework addressing bias and ethics in machine learning. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 1-12.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_xhYWeVw">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">T</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NeGnS9x">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4356" to="4364" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in Neural Information Processing Systems 29 (2016), 4356--4364.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_7CCHvsW">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_T7YwJDW">Conference on fairness, accountability and transparency. PMLR, JMLR</title>
		<meeting><address><addrLine>Campridge, MA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="77" to="91" />
		</imprint>
	</monogr>
	<note type="raw_reference">Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency. PMLR, JMLR, Campridge, MA, United States, 77-91.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main" xml:id="_HRSDbSk">National Population by Characteristics</title>
		<idno type="DOI">10.3886/icpsr06218.v1</idno>
		<ptr target="https://www.census.gov/data/tables/time-series/demo/popest/2020s-national-detail.html" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>United States Census Bureu</publisher>
			<biblScope unit="page" from="2020" to="2021" />
		</imprint>
	</monogr>
	<note type="raw_reference">United States Census Bureu. 2022. National Population by Characteristics: 2020-2021. https://www.census.gov/data/tables/time- series/demo/popest/2020s-national-detail.html</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_fWfmJWh">A clarification of the nuances in the fairness metrics landscape</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Castelnovo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Crupi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greta</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Regoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppina</forename><surname>Ilaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">Claudio</forename><surname>Penco</surname></persName>
		</author>
		<author>
			<persName><surname>Cosentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kbw2SyW">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4209</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alessandro Castelnovo, Riccardo Crupi, Greta Greco, Daniele Regoli, Ilaria Giuseppina Penco, and Andrea Claudio Cosentini. 2022. A clarification of the nuances in the fairness metrics landscape. Scientific Reports 12, 1 (2022), 4209.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_hsxzBp2">Can AI help reduce disparities in general medical and mental health care?</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Irene Y Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><surname>Ghassemi</surname></persName>
		</author>
		<idno type="DOI">10.1001/amajethics.2019.167</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_M9VZkPY">AMA journal of ethics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Irene Y Chen, Peter Szolovits, and Marzyeh Ghassemi. 2019. Can AI help reduce disparities in general medical and mental health care? AMA journal of ethics 21, 2 (2019), 167-179.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_3dG7uKH">Racial/ethnic differences in sleep disturbances: the Multi-Ethnic Study of Atherosclerosis (MESA)</title>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phyllis</forename><surname>Zee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><forename type="middle">L</forename><surname>Lutsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sogol</forename><surname>Javaheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmela</forename><surname>Alc√°ntara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">A</forename><surname>Chandra L Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><surname>Redline</surname></persName>
		</author>
		<idno type="DOI">10.5665/sleep.4732</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jgPNaQM">Sleep</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="877" to="888" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiaoli Chen, Rui Wang, Phyllis Zee, Pamela L Lutsey, Sogol Javaheri, Carmela Alc√°ntara, Chandra L Jackson, Michelle A Williams, and Susan Redline. 2015. Racial/ethnic differences in sleep disturbances: the Multi-Ethnic Study of Atherosclerosis (MESA). Sleep 38, 6 (2015), 877-888.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_pdpmU3J">Bridging the digital divide: measuring digital literacy</title>
		<author>
			<persName><forename type="first">Krish</forename><surname>Chetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Qigui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nozibele</forename><surname>Gcora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaya</forename><surname>Josie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wenwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Z8eqR68">Economics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Krish Chetty, Liu Qigui, Nozibele Gcora, Jaya Josie, Li Wenwei, and Chen Fang. 2018. Bridging the digital divide: measuring digital literacy. Economics 12, 1 (2018), 1-17.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_ewHRBkr">Demographic Imbalances Resulting From the Bring-Your-Own-Device Study Design</title>
		<author>
			<persName><forename type="first">Peter Jaeho</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Mobashir Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen</forename><surname>Shandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leatrice</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geetika</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brinnae</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Bent</surname></persName>
		</author>
		<author>
			<persName><surname>Ginsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MnvV7rZ">JMIR mHealth and uHealth</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">29510</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Peter Jaeho Cho, Jaehan Yi, Ethan Ho, Md Mobashir Hasan Shandhi, Yen Dinh, Aneesh Patil, Leatrice Martin, Geetika Singh, Brinnae Bent, Geoffrey Ginsburg, et al. 2022. Demographic Imbalances Resulting From the Bring-Your-Own-Device Study Design. JMIR mHealth and uHealth 10, 4 (2022), e29510.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_UZFAuJA">Punishing risk</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zu2dnEN">Geo. LJ</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Erin Collins. 2018. Punishing risk. Geo. LJ 107 (2018), 57.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_BQnS9gA">Uniform guidelines on employee selection procedures</title>
		<author>
			<orgName type="collaboration">Equal Employment Opportunity Commission</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6ZS229B">Fed Register</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="216" to="243" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Equal Employment Opportunity Commission et al. 1990. Uniform guidelines on employee selection procedures. Fed Register 1 (1990), 216-243.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_qs7T6pE">Algorithmic Bias in Autonomous Systems</title>
		<author>
			<persName><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">John</forename><surname>London</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5vbY54b">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia; Washington, DC, United States</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4691" to="4697" />
		</imprint>
	</monogr>
	<note>IJCAI&apos;17)</note>
	<note type="raw_reference">David Danks and Alex John London. 2017. Algorithmic Bias in Autonomous Systems. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (Melbourne, Australia) (IJCAI&apos;17). AAAI Press, Washington, DC, United States, 4691-4697.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_BVuuUPS">Considerations for Conducting Bring Your Own &quot;Device</title>
		<author>
			<persName><forename type="first">Charmaine</forename><surname>Demanuele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Lokker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Jhaveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pirinka</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sezgin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><surname>Geoghegan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Izmailova</surname></persName>
		</author>
		<author>
			<persName><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mf5QtwA">BYOD) Clinical Studies. Digital biomarkers</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Charmaine Demanuele, Cynthia Lokker, Krishna Jhaveri, Pirinka Georgiev, Emre Sezgin, Cindy Geoghegan, Kelly H Zou, Elena Izmailova, and Marie McCarthy. 2022. Considerations for Conducting Bring Your Own &quot;Device&quot;(BYOD) Clinical Studies. Digital biomarkers 6, 2 (2022), 47-60.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_bvy3QRC">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_6UxsQKB">2009 IEEE conference on computer vision and pattern recognition</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. IEEE, IEEE, New York, NY, United States, 248-255.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_kfDjUmU">Bringing the people back in: Contesting benchmark machine learning datasets</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Hanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Amironesei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hilary</forename><surname>Nicole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Klaus Scheuerman</surname></persName>
		</author>
		<idno>arXiv preprint 2007.07399. 1-6</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note type="raw_reference">Emily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, Hilary Nicole, and Morgan Klaus Scheuerman. 2020. Bringing the people back in: Contesting benchmark machine learning datasets. Technical Report. arXiv preprint 2007.07399. 1-6 pages.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main" xml:id="_E3X8SNM">Walk this way: validity evidence of iphone health application step count in laboratory and free-living conditions</title>
		<author>
			<persName><forename type="first">Markus J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="DOI">10.1080/02640414.2017.1409855</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_tv7zE8H">Journal of sports sciences</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1695" to="1704" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Markus J Duncan, Kelly Wunderlich, Yingying Zhao, and Guy Faulkner. 2018. Walk this way: validity evidence of iphone health application step count in laboratory and free-living conditions. Journal of sports sciences 36, 15 (2018), 1695-1704.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main" xml:id="_Xg4FurN">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="DOI">10.1145/2090236.2090255</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_jKng7BD">Proceedings of the 3rd innovations in theoretical computer science conference</title>
		<meeting>the 3rd innovations in theoretical computer science conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
	<note type="raw_reference">Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in theoretical computer science conference. Association for Computing Machinery, New York, NY, USA, 214-226.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_DJ3GCkm">Beyond self-reflection: introducing the concept of rumination in personal informatics</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Victoria Eikey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><forename type="middle">Marques</forename><surname>Caldeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayara</forename><surname>Costa Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">L</forename><surname>Borelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Mazmanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qm8s4eq">Personal and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="601" to="616" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Elizabeth Victoria Eikey, Clara Marques Caldeira, Mayara Costa Figueiredo, Yunan Chen, Jessica L Borelli, Melissa Mazmanian, and Kai Zheng. 2021. Beyond self-reflection: introducing the concept of rumination in personal informatics. Personal and Ubiquitous Computing 25, 3 (2021), 601-616.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_m6PJaXQ">Are currently available wearable devices for activity tracking and heart rate monitoring accurate, precise, and medically beneficial?</title>
		<author>
			<persName><forename type="first">Fatema</forename><surname>El-Amrawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nounou</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gFnWZKz">Healthcare informatics research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="315" to="320" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fatema El-Amrawy and Mohamed Ismail Nounou. 2015. Are currently available wearable devices for activity tracking and heart rate monitoring accurate, precise, and medically beneficial? Healthcare informatics research 21, 4 (2015), 315-320.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_33RNJTt">Mapping and taking stock of the personal informatics literature</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Daniel A Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayara</forename><surname>Caldeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Costa Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><forename type="middle">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucretia</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyang</forename><surname>Ho Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuer</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TRwJ6Nu">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel A Epstein, Clara Caldeira, Mayara Costa Figueiredo, Xi Lu, Lucas M Silva, Lucretia Williams, Jong Ho Lee, Qingyang Li, Simran Ahuja, Qiuer Chen, et al. 2020. Mapping and taking stock of the personal informatics literature. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 4 (2020), 1-38.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_8AAEavD">Beyond abandonment to next steps: understanding and designing for life after personal informatics tool use</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Daniel A Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Caraway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">A</forename><surname>Fogarty</surname></persName>
		</author>
		<author>
			<persName><surname>Munson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xpArhUd">Proceedings of the 2016 CHI conference on human factors in computing systems</title>
		<meeting>the 2016 CHI conference on human factors in computing systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1109" to="1113" />
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel A Epstein, Monica Caraway, Chuck Johnston, An Ping, James Fogarty, and Sean A Munson. 2016. Beyond abandonment to next steps: understanding and designing for life after personal informatics tool use. In Proceedings of the 2016 CHI conference on human factors in computing systems. 1109-1113.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main" xml:id="_HvuDeHF">Wearable technology and the IoT</title>
		<author>
			<persName><forename type="first">Ericsson</forename><surname>Consumerlab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ericsson ConsumerLab. 2016. Wearable technology and the IoT.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_M7pjtj9">Fair machine learning in healthcare: A review</title>
		<author>
			<persName><forename type="first">Qizhang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno>arXiv preprint 2206.14397. 1-21</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note type="raw_reference">Qizhang Feng, Mengnan Du, Na Zou, and Xia Hu. 2022. Fair machine learning in healthcare: A review. Technical Report. arXiv preprint 2206.14397. 1-21 pages.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="https://www.fitbit.com/global/us/technology/irregular-rhythm" />
		<title level="m" xml:id="_J4kjMM2">Irregular Rhythm -fitbit</title>
		<imprint>
			<publisher>Fitbit</publisher>
			<date type="published" when="2022-11-14">2022. 14-Nov-2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fitbit. 2022. Irregular Rhythm -fitbit.com. https://www.fitbit.com/global/us/technology/irregular-rhythm. [Accessed 14-Nov-2022].</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main" xml:id="_TN89Jrb">What&apos;s Fair about Individual Fairness?</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Fleisher</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461702.3462621</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uEz9p9Y">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society<address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="480" to="490" />
		</imprint>
	</monogr>
	<note type="raw_reference">Will Fleisher. 2021. What&apos;s Fair about Individual Fairness?. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, United States, 480-490.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_ftRWj3n">Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health</title>
		<author>
			<persName><forename type="first">Audace</forename><surname>Richard Rib√≥n Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olusubomi</forename><surname>Nakeshimana</surname></persName>
		</author>
		<author>
			<persName><surname>Olubeko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">561802</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Richard Rib√≥n Fletcher, Audace Nakeshimana, and Olusubomi Olubeko. 2021. Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health. , 561802 pages.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main" xml:id="_2GKEj4H">Hypertension cascade: hypertension prevalence, treatment and control estimates among US adults aged 18 years and older applying the criteria from the American College of Cardiology and American Heart Association&apos;s 2017 Hypertension Guideline-NHANES</title>
		<author>
			<persName><forename type="first">)</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2019. 2016</date>
			<biblScope unit="page">2013</biblScope>
		</imprint>
		<respStmt>
			<orgName>Centers for Disease Control, Prevention (CDC</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Centers for Disease Control, Prevention (CDC), et al. 2019. Hypertension cascade: hypertension prevalence, treatment and control estimates among US adults aged 18 years and older applying the criteria from the American College of Cardiology and American Heart Association&apos;s 2017 Hypertension Guideline-NHANES 2013-2016.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_X6XRchN">Artificial intelligence and inclusion: Formerly gang-involved youth as domain experts for analyzing unstructured twitter data</title>
		<author>
			<persName><forename type="first">Desmond</forename><forename type="middle">U</forename><surname>William R Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">A</forename><surname>Gaskell</surname></persName>
		</author>
		<author>
			<persName><surname>Mcgregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pXYaqB8">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="56" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">William R Frey, Desmond U Patton, Michael B Gaskell, and Kyle A McGregor. 2020. Artificial intelligence and inclusion: Formerly gang-involved youth as domain experts for analyzing unstructured twitter data. Social Science Computer Review 38, 1 (2020), 42-56.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_4CmUZQv">The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rcX8Hkw">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="136" to="143" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. 2021. The (im) possibility of fairness: Different value systems require different mechanisms for fair decision making. Commun. ACM 64, 4 (2021), 136-143.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Cheryl</forename><forename type="middle">D</forename><surname>Fryar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">D</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Afful</surname></persName>
		</author>
		<idno type="DOI">10.15620/cdc:134503</idno>
		<title level="m" xml:id="_x3vMZKM">Prevalence of underweight among adults aged 20 and over: United States, 1960-1962 through 2017-2018</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Cheryl D Fryar, Margaret D Carroll, and Joseph Afful. 2020. Prevalence of underweight among adults aged 20 and over: United States, 1960-1962 through 2017-2018.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Cheryl</forename><forename type="middle">D</forename><surname>Fryar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">D</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Afful</surname></persName>
		</author>
		<title level="m" xml:id="_dxUV9uK">Prevalence of overweight, obesity, and severe obesity among adults aged 20 and over: United States, 1960-1962 through 2017-2018</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>7 pages</note>
	<note type="raw_reference">Cheryl D Fryar, Margaret D Carroll, Joseph Afful, et al. 2020. Prevalence of overweight, obesity, and severe obesity among adults aged 20 and over: United States, 1960-1962 through 2017-2018. , 7 pages.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main" xml:id="_j9gKAY5">ThinkActive: designing for pseudonymous activity tracking in the classroom</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Garbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chatting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Kharrufa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZnSF6rE">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
	<note type="raw_reference">Andrew Garbett, David Chatting, Gerard Wilkinson, Clement Lee, and Ahmed Kharrufa. 2018. ThinkActive: designing for pseudonymous activity tracking in the classroom. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 1-13.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main" xml:id="_rTYna5V">Assessment of racial and ethnic differences in oxygen supplementation among patients in the intensive care unit</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Raphael Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharine</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CgtcUBf">JAMA internal medicine</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="849" to="858" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eric Raphael Gottlieb, Jennifer Ziegler, Katharine Morley, Barret Rush, and Leo Anthony Celi. 2022. Assessment of racial and ethnic differences in oxygen supplementation among patients in the intensive care unit. JAMA internal medicine 182, 8 (2022), 849-858.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<idno type="DOI">10.21786/bbrc/14.5/43</idno>
		<ptr target="https://www.radicati.com/wp/wp-content/uploads/2021/Mobile_Statistics_Report,_2021-2025_Executive_Summary.pdf" />
		<title level="m" xml:id="_tH8ZDHX">Mobile Statistics Report</title>
		<imprint>
			<publisher>The Radicati Group</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2021" to="2025" />
		</imprint>
	</monogr>
	<note type="raw_reference">The Radicati Group. 2021. Mobile Statistics Report, 2021-2025. https://www.radicati.com/wp/wp-content/uploads/2021/Mobile_ Statistics_Report,_2021-2025_Executive_Summary.pdf</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main" xml:id="_JwT4A6D">Error Parity Fairness: Testing for Group Fairness in Regression Tasks</title>
		<author>
			<persName><forename type="first">Furkan</forename><surname>Gursoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><surname>Kakadiaris</surname></persName>
		</author>
		<idno type="DOI">10.1109/icdmw58026.2022.00027</idno>
		<idno>arXiv preprint 2208.08279. 1-12</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note type="raw_reference">Furkan Gursoy and Ioannis A Kakadiaris. 2022. Error Parity Fairness: Testing for Group Fairness in Regression Tasks. Technical Report. arXiv preprint 2208.08279. 1-12 pages.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_PkBDASn">Worldwide trends in insufficient physical activity from 2001 to 2016: a pooled analysis of 358 population-based surveys with 1‚Ä¢ 9 million participants</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Guthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><forename type="middle">A</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leanne</forename><forename type="middle">M</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiona</forename><forename type="middle">C</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_52ZTcj5">The lancet global health</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1077" to="e1086" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Regina Guthold, Gretchen A Stevens, Leanne M Riley, and Fiona C Bull. 2018. Worldwide trends in insufficient physical activity from 2001 to 2016: a pooled analysis of 358 population-based surveys with 1‚Ä¢ 9 million participants. The lancet global health 6, 10 (2018), e1077-e1086.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_P9NcfAw">The movielens datasets: History and context</title>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_QqNVT5Q">ACM Transactions on Interactive Intelligent Systems (TIIS)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. ACM Transactions on Interactive Intelligent Systems (TIIS) 5, 4 (2015), 1-19.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_RqBzg45">Multitask learning and benchmarking with clinical time series data</title>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>David C Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_gaWtGaq">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan. 2019. Multitask learning and benchmarking with clinical time series data. Scientific data 6, 1 (2019), 1-18.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main" xml:id="_PzQkJtC">Physical activity, sleep and cardiovascular health data for 50,000 individuals from the MyHeart Counts Study</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Steven G Hershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megan</forename><surname>Shcherbina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasbanoo</forename><surname>Doerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Moayedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daryl</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mildred</forename><forename type="middle">K</forename><surname>Waggott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">E</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Rosenberger</surname></persName>
		</author>
		<author>
			<persName><surname>Haskell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CKgCP5c">Scientific data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Steven G Hershman, Brian M Bot, Anna Shcherbina, Megan Doerr, Yasbanoo Moayedi, Aleksandra Pavlovic, Daryl Waggott, Mildred K Cho, Mary E Rosenberger, William L Haskell, et al. 2019. Physical activity, sleep and cardiovascular health data for 50,000 individuals from the MyHeart Counts Study. Scientific data 6, 1 (2019), 1-10.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main" xml:id="_aWqYEdT">Racial bias in pain assessment and treatment recommendations, and false beliefs about biological differences between blacks and whites</title>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Kelly M Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Trawalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M Norman</forename><surname>Jordan R Axt</surname></persName>
		</author>
		<author>
			<persName><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bVAf8aS">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="4296" to="4301" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kelly M Hoffman, Sophie Trawalter, Jordan R Axt, and M Norman Oliver. 2016. Racial bias in pain assessment and treatment recommendations, and false beliefs about biological differences between blacks and whites. Proceedings of the National Academy of Sciences 113, 16 (2016), 4296-4301.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main" xml:id="_EXQuhEV">Bias in automated speaker recognition</title>
		<author>
			<persName><forename type="first">Wiebke</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hutiri</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pN22rKe">2022 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="230" to="247" />
		</imprint>
	</monogr>
	<note type="raw_reference">Wiebke Toussaint Hutiri and Aaron Yi Ding. 2022. Bias in automated speaker recognition. In 2022 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, USA, 230-247.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_8vmVArH">Tiny, Always-on and Fragile: Bias Propagation through Design Choices in On-Device Machine Learning Workflows</title>
		<author>
			<persName><forename type="first">(</forename><surname>Wiebke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">)</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Hutiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Yi Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhil</forename><surname>Kawsar</surname></persName>
		</author>
		<author>
			<persName><surname>Mathur</surname></persName>
		</author>
		<idno type="DOI">10.1145/3591867</idno>
		<ptr target="https://doi.org/10.1145/3591867JustAccepted" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_2DnCpYk">ACM Trans. Softw. Eng. Methodol</title>
		<imprint>
			<date type="published" when="2023-04">2023. apr 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wiebke (Toussaint) Hutiri, Aaron Yi Ding, Fahim Kawsar, and Akhil Mathur. 2023. Tiny, Always-on and Fragile: Bias Propagation through Design Choices in On-Device Machine Learning Workflows. ACM Trans. Softw. Eng. Methodol. (apr 2023). https://doi.org/10. 1145/3591867 Just Accepted.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_gYV47eK">Artificial intelligence in healthcare: past, present and future</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sufeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haipeng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongjun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1136/svn-2017-000101</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_m54HJux">Stroke and vascular neurology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="230" to="243" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fei Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang, Qiang Dong, Haipeng Shen, and Yongjun Wang. 2017. Artificial intelligence in healthcare: past, present and future. Stroke and vascular neurology 2, 4 (2017), 230-243.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Bulgarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41597-023-01945-2</idno>
		<title level="m" xml:id="_TGcNV2g">Mimic-iv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alistair Johnson, Lucas Bulgarelli, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark. 2020. Mimic-iv.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_YBCrJNk">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zuQX98M">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. 2016. MIMIC-III, a freely accessible critical care database. Scientific Data 3, 1 (2016), 1-9.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main" xml:id="_FMHyJY9">Using artificial intelligence on dermatology conditions in Uganda: A case for diversity in training data sets for machine learning</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Henry Kamulegeya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Okello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Mark</forename><surname>Bwanika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Musinguzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Lubega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Rusoke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faith</forename><surname>Nassiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>B√∂rve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_W7jxz6J">BioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Louis Henry Kamulegeya, Mark Okello, John Mark Bwanika, Davis Musinguzi, William Lubega, Davis Rusoke, Faith Nassiwa, and Alexander B√∂rve. 2019. Using artificial intelligence on dermatology conditions in Uganda: A case for diversity in training data sets for machine learning. BioRxiv (2019).</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main" xml:id="_hUqaVgj">Money talks: tracking personal finances</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jofish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaye</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Mccuistion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Gulotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Z22u4us">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
	<note type="raw_reference">Joseph Jofish Kaye, Mary McCuistion, Rebecca Gulotta, and David A Shamma. 2014. Money talks: tracking personal finances. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 521-530.</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main" xml:id="_tg4Tx3y">What your Fitbit Says about You: De-anonymizing Users in Lifelogging Datasets</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Kazlouski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Marchioro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Markatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_uqeXKPW">Proceedings of the 18th International Conference on Security and Cryptography -SECRYPT</title>
		<meeting>the 18th International Conference on Security and Cryptography -SECRYPT<address><addrLine>Setubal, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="806" to="811" />
		</imprint>
	</monogr>
	<note type="raw_reference">Andrei Kazlouski, Thomas Marchioro, and Evangelos Markatos. 2022. What your Fitbit Says about You: De-anonymizing Users in Lifelogging Datasets. In Proceedings of the 18th International Conference on Security and Cryptography -SECRYPT. SciTePress, Setubal, Portugal, 806-811.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main" xml:id="_fQPN8BW">TimeAware: Leveraging framing effects to enhance personal productivity</title>
		<author>
			<persName><forename type="first">Young-Ho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Ho Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eun</forename><forename type="middle">Kyoung</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bongshin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwonhyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwook</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.1145/2858036.2858428</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_w6P8QKg">Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2016 CHI Conference on Human Factors in Computing Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="272" to="283" />
		</imprint>
	</monogr>
	<note type="raw_reference">Young-Ho Kim, Jae Ho Jeon, Eun Kyoung Choe, Bongshin Lee, KwonHyun Kim, and Jinwook Seo. 2016. TimeAware: Leveraging framing effects to enhance personal productivity. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery, New York, NY, USA, 272-283.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main" xml:id="_eqFwz45">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_eT3dNGw">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual fairness. Advances in Neural Information Processing Systems 30 (2017).</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_PKUE8Tn">Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Agostina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicol√°s</forename><surname>Larrazabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><forename type="middle">H</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enzo</forename><surname>Milone</surname></persName>
		</author>
		<author>
			<persName><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bhEtYmD">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="12592" to="12594" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Agostina J Larrazabal, Nicol√°s Nieto, Victoria Peterson, Diego H Milone, and Enzo Ferrante. 2020. Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis. Proceedings of the National Academy of Sciences 117, 23 (2020), 12592-12594.</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_JhTCKYa">User fairness in recommender systems</title>
		<author>
			<persName><forename type="first">Jurek</forename><surname>Leonhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avishek</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Megha</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xwrYTVJ">Companion Proceedings of the The Web Conference 2018</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="101" to="102" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jurek Leonhardt, Avishek Anand, and Megha Khosla. 2018. User fairness in recommender systems. In Companion Proceedings of the The Web Conference 2018. Association for Computing Machinery, New York, NY, United States, 101-102.</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_JdpBPBm">A stage-based model of personal informatics systems</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anind</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jodi</forename><surname>Forlizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZuqzscW">Proceedings of the SIGCHI conference on human factors in computing systems</title>
		<meeting>the SIGCHI conference on human factors in computing systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="557" to="566" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ian Li, Anind Dey, and Jodi Forlizzi. 2010. A stage-based model of personal informatics systems. In Proceedings of the SIGCHI conference on human factors in computing systems. Association for Computing Machinery, New York, NY, USA, 557-566.</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main" xml:id="_MHN99MF">Multi-task neural networks for personalized pain recognition from physiological signals</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fb4z3jX">Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
	<note type="raw_reference">Daniel Lopez-Martinez and Rosalind Picard. 2017. Multi-task neural networks for personalized pain recognition from physiological signals. In 2017 Seventh International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW). IEEE, IEEE, New York, NY, United States, 181-184.</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_2jVj6P6">Self-tracking cultures: towards a sociology of personal informatics</title>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Lupton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PgS65kR">Proceedings of the 26th Australian computer-human interaction conference on designing futures: The future of design</title>
		<meeting>the 26th Australian computer-human interaction conference on designing futures: The future of design</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
	<note type="raw_reference">Deborah Lupton. 2014. Self-tracking cultures: towards a sociology of personal informatics. In Proceedings of the 26th Australian computer-human interaction conference on designing futures: The future of design. 77-86.</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main" xml:id="_yzbpGV6">Adopting the sensemaking perspective for chronic disease self-management</title>
		<author>
			<persName><forename type="first">Lena</forename><surname>Mamykina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arlene</forename><forename type="middle">M</forename><surname>Smaldone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzanne</forename><forename type="middle">R</forename><surname>Bakken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XU5rxFK">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="406" to="417" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lena Mamykina, Arlene M Smaldone, and Suzanne R Bakken. 2015. Adopting the sensemaking perspective for chronic disease self-management. Journal of biomedical informatics 56 (2015), 406-417.</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main" xml:id="_2zQYzxk">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ysGBvfQ">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM Computing Surveys (CSUR) 54, 6 (2021), 1-35.</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main" xml:id="_AKMMqw7">Mimic-if: Interpretability and fairness evaluation of deep learning models on mimic-iv dataset</title>
		<author>
			<persName><forename type="first">Chuizheng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loc</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv preprint 2102.06761</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note type="raw_reference">Chuizheng Meng, Loc Trinh, Nan Xu, and Yan Liu. 2021. Mimic-if: Interpretability and fairness evaluation of deep learning models on mimic-iv dataset. Technical Report. arXiv preprint 2102.06761.</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main" xml:id="_dHWXc8S">With Almost Half of World&apos;s Population Still Offline, Digital Divide Risks Becoming &apos;New Face of Inequality</title>
		<author>
			<persName><forename type="first">Amina</forename><surname>Mohammed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Deputy Secretary-General Warns General Assembly</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Amina Mohammed. 2021. With Almost Half of World&apos;s Population Still Offline, Digital Divide Risks Becoming &apos;New Face of Inequality, &apos;Deputy Secretary-General Warns General Assembly.</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main" xml:id="_9CjKc6Z">This thing called fairness: Disciplinary confusion realizing a value in technology</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">A</forename><surname>Deirdre K Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Kroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richmond</forename><forename type="middle">Y</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vxF8Rfe">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<publisher>CSCW</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="36" />
		</imprint>
	</monogr>
	<note type="raw_reference">Deirdre K Mulligan, Joshua A Kroll, Nitin Kohli, and Richmond Y Wong. 2019. This thing called fairness: Disciplinary confusion realizing a value in technology. Proceedings of the ACM on Human-Computer Interaction 3, CSCW (2019), 1-36.</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main" xml:id="_FCuNfff">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName><forename type="first">Ziad</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_P3ATAj2">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (2019), 447-453.</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" xml:id="_EpuPkJQ">National diabetes statistics report</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
		<respStmt>
			<orgName>US Department of Health, Human Services, et al</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">US Department of Health, Human Services, et al. 2020. National diabetes statistics report, 2020.</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" xml:id="_QchtZuC">Global action plan on physical activity 2018-2030: more active people for a healthier world</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>World Health Organization</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization. 2019. Global action plan on physical activity 2018-2030: more active people for a healthier world. World Health Organization, Geneva, Switzerland.</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" xml:id="_TGVSbtR">Global recommendations on physical activity for health</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>World Health Organization</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="raw_reference">World Health Organization et al. 2010. Global recommendations on physical activity for health. World Health Organization, Geneva, Switzerland.</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main" xml:id="_qQADCpg">Addressing bias in artificial intelligence in health care</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Ravi B Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><forename type="middle">S</forename><surname>Teeple</surname></persName>
		</author>
		<author>
			<persName><surname>Navathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mg9bCh8">Jama</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="page" from="2377" to="2378" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ravi B Parikh, Stephanie Teeple, and Amol S Navathe. 2019. Addressing bias in artificial intelligence in health care. Jama 322, 24 (2019), 2377-2378.</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main" xml:id="_JjFuBMD">VITAL-ECG: a de-bias algorithm embedded in a gender-immune device</title>
		<author>
			<persName><forename type="first">Annunziata</forename><surname>Paviglianiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eros</forename><surname>Pasero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qw2byHH">2020 IEEE International Workshop on Metrology for Industry 4.0 &amp; IoT. IEEE</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="314" to="318" />
		</imprint>
	</monogr>
	<note type="raw_reference">Annunziata Paviglianiti and Eros Pasero. 2020. VITAL-ECG: a de-bias algorithm embedded in a gender-immune device. In 2020 IEEE International Workshop on Metrology for Industry 4.0 &amp; IoT. IEEE, IEEE, New York, NY, United States, 314-318.</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main" xml:id="_EYuzuFA">Discrimination-aware data mining</title>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Wqf33nv">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="560" to="568" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. 2008. Discrimination-aware data mining. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. Association for Computing Machinery, New York, NY, United States, 560-568.</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main" xml:id="_p7KpPuU">A Review on Fairness in Machine Learning</title>
		<author>
			<persName><forename type="first">Dana</forename><surname>Pessach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Shmueli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mEJeuTB">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dana Pessach and Erez Shmueli. 2022. A Review on Fairness in Machine Learning. ACM Computing Surveys (CSUR) 55, 3 (2022), 1-44.</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main" xml:id="_WdB924b">Peeking into a black box, the fairness and generalizability of a MIMIC-III benchmarking model</title>
		<author>
			<persName><forename type="first">Eliane</forename><surname>R√∂√∂sli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Selen</forename><surname>Bozkurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Hernandez-Boussard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g9nAK6T">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Eliane R√∂√∂sli, Selen Bozkurt, and Tina Hernandez-Boussard. 2022. Peeking into a black box, the fairness and generalizability of a MIMIC-III benchmarking model. Scientific Data 9, 1 (2022), 1-13.</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main" xml:id="_uJ4ax3Y">Measuring engagement in robot-assisted autism therapy: a cross-cultural study</title>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeryoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lea</forename><surname>Mascarell-Maricic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_mSztV9u">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ognjen Rudovic, Jaeryoung Lee, Lea Mascarell-Maricic, Bj√∂rn W Schuller, and Rosalind W Picard. 2017. Measuring engagement in robot-assisted autism therapy: a cross-cultural study. Frontiers in Robotics and AI 4 (2017), 36.</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main" xml:id="_wNtRwQe">CultureNet: a deep learning approach for engagement intensity estimation from face images of children with autism</title>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuria</forename><surname>Utsumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeryoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">Castell√≥</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj√∂rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_hXgQZRa">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="339" to="346" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ognjen Rudovic, Yuria Utsumi, Jaeryoung Lee, Javier Hernandez, Eduardo Castell√≥ Ferrer, Bj√∂rn Schuller, and Rosalind W Picard. 2018. CultureNet: a deep learning approach for engagement intensity estimation from face images of children with autism. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, IEEE, New York, NY, United States, 339-346.</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main" xml:id="_Q6sB52s">Aequitas: A bias and fairness audit toolkit</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Saleiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedict</forename><surname>Kuester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Hinkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abby</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Anisfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><forename type="middle">T</forename><surname>Rodolfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rayid</forename><surname>Ghani</surname></persName>
		</author>
		<idno>arXiv preprint 1811.05577</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note type="raw_reference">Pedro Saleiro, Benedict Kuester, Loren Hinkson, Jesse London, Abby Stevens, Ari Anisfeld, Kit T Rodolfa, and Rayid Ghani. 2018. Aequitas: A bias and fairness audit toolkit. Technical Report. arXiv preprint 1811.05577.</note>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main" xml:id="_HNcf36m">Fairness and abstraction in sociotechnical systems</title>
		<author>
			<persName><forename type="first">Danah</forename><surname>Andrew D Selbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><surname>Vertesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Y58Yqk8">Proceedings of the conference on fairness, accountability, and transparency</title>
		<meeting>the conference on fairness, accountability, and transparency<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="59" to="68" />
		</imprint>
	</monogr>
	<note type="raw_reference">Andrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh Venkatasubramanian, and Janet Vertesi. 2019. Fairness and abstraction in sociotechnical systems. In Proceedings of the conference on fairness, accountability, and transparency. Association for Computing Machinery, New York, NY, USA, 59-68.</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main" xml:id="_8zsPaGU">Toward Personalized Affect-Aware Socially Assistive Robot Tutors for Long-Term Interventions with Children with Autism</title>
		<author>
			<persName><forename type="first">Zhonghao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Groechel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shomik</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kourtney</forename><surname>Chima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ognjen</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><forename type="middle">J</forename><surname>Matariƒá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_PCSwXtE">ACM Transactions on Human-Robot Interaction (THRI)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhonghao Shi, Thomas R Groechel, Shomik Jain, Kourtney Chima, Ognjen Rudovic, and Maja J Matariƒá. 2022. Toward Personalized Affect-Aware Socially Assistive Robot Tutors for Long-Term Interventions with Children with Autism. ACM Transactions on Human- Robot Interaction (THRI) 11, 4 (2022), 1-28.</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main" xml:id="_FPR3f8M">Racial bias in pulse oximetry measurement</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Michael W Sjoding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodore</forename><forename type="middle">J</forename><surname>Dickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">E</forename><surname>Iwashyna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Gay</surname></persName>
		</author>
		<author>
			<persName><surname>Valley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DG9qYDb">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">383</biblScope>
			<biblScope unit="page" from="2477" to="2478" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michael W Sjoding, Robert P Dickson, Theodore J Iwashyna, Steven E Gay, and Thomas S Valley. 2020. Racial bias in pulse oximetry measurement. New England Journal of Medicine 383, 25 (2020), 2477-2478.</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main" xml:id="_qrtSfJm">Self-Supervised Transfer Learning of Physiological Representations from Free-Living Wearable Data</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Spathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><surname>Perez-Pozuelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soren</forename><surname>Brage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">J</forename><surname>Wareham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Mascolo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3450439.3451863</idno>
		<ptr target="https://doi.org/10.1145/3450439.3451863" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZHJsjMx">Proceedings of the Conference on Health, Inference, and Learning (Virtual Event, USA) (CHIL &apos;21)</title>
		<meeting>the Conference on Health, Inference, and Learning (Virtual Event, USA) (CHIL &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dimitris Spathis, Ignacio Perez-Pozuelo, Soren Brage, Nicholas J. Wareham, and Cecilia Mascolo. 2021. Self-Supervised Transfer Learning of Physiological Representations from Free-Living Wearable Data. In Proceedings of the Conference on Health, Inference, and Learning (Virtual Event, USA) (CHIL &apos;21). Association for Computing Machinery, New York, NY, USA, 69-78. https://doi.org/10.1145/ 3450439.3451863</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<ptr target="https://www.statista.com/statistics/487291/global-connected-wearable-devices/" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_PahPbYQ">Global connected wearable devices 2016-2022</title>
		<title level="s" xml:id="_rZg9fWH">Statista -statista</title>
		<imprint>
			<publisher>Statista</publisher>
			<date type="published" when="2022-11-14">2022. 14-Nov-2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Statista. 2022. Global connected wearable devices 2016-2022 | Statista -statista.com. https://www.statista.com/statistics/487291/global- connected-wearable-devices/. [Accessed 14-Nov-2022].</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main" xml:id="_Jp9jQxe">SisFall: A fall and movement dataset</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Sucerquia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos√©</forename><surname>David L√≥pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jes√∫s</forename><forename type="middle">Francisco</forename><surname>Vargas-Bonilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YKBN6dR">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">198</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Angela Sucerquia, Jos√© David L√≥pez, and Jes√∫s Francisco Vargas-Bonilla. 2017. SisFall: A fall and movement dataset. Sensors 17, 1 (2017), 198.</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main" xml:id="_3yEh38j">A framework for understanding sources of harm throughout the machine learning life cycle</title>
		<author>
			<persName><forename type="first">Harini</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<idno type="DOI">10.1145/3465416.3483305</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_gug3gGk">Equity and access in algorithms, mechanisms, and optimization</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
	<note type="raw_reference">Harini Suresh and John Guttag. 2021. A framework for understanding sources of harm throughout the machine learning life cycle. In Equity and access in algorithms, mechanisms, and optimization. Association for Computing Machinery, New York, NY, United States, 1-9.</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main" xml:id="_KaJJyqA">Personalized multitask learning for predicting tomorrow&apos;s mood, stress, and health</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natasha</forename><surname>Jaques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehimwenma</forename><surname>Nosakhare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akane</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosalind</forename><surname>Picard</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2017.2784832</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ns9Y8Rk">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="213" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sara Taylor, Natasha Jaques, Ehimwenma Nosakhare, Akane Sano, and Rosalind Picard. 2017. Personalized multitask learning for predicting tomorrow&apos;s mood, stress, and health. IEEE Transactions on Affective Computing 11, 2 (2017), 200-213.</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Oura</forename><surname>Team</surname></persName>
		</author>
		<idno type="DOI">10.58489/2836-5836/009</idno>
		<ptr target="https://ouraring.com/blog/blood-oxygen-sensing-spo2/" />
		<title level="m" xml:id="_W4RNgj2">New to Oura: Blood Oxygen Sensing (SpO2) -ouraring</title>
		<imprint>
			<date type="published" when="2022-11-14">2022. 14-Nov-2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Oura Team. 2022. New to Oura: Blood Oxygen Sensing (SpO2) -ouraring.com. https://ouraring.com/blog/blood-oxygen-sensing-spo2/. [Accessed 14-Nov-2022].</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main" xml:id="_BqU3brm">Prevalence of Arthritis and Arthritis-Attributable Activity Limitation-United States, 2016-2018</title>
		<author>
			<persName><forename type="first">Kristina</forename><forename type="middle">A</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louise</forename><forename type="middle">B</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Guglielmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Boring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">A</forename><surname>Okoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lindsey</forename><forename type="middle">M</forename><surname>Duca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">G</forename><surname>Helmick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ntgGJej">Morbidity and Mortality Weekly Report</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page">1401</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kristina A Theis, Louise B Murphy, Dana Guglielmo, Michael A Boring, Catherine A Okoro, Lindsey M Duca, and Charles G Helmick. 2021. Prevalence of Arthritis and Arthritis-Attributable Activity Limitation-United States, 2016-2018. Morbidity and Mortality Weekly Report 70, 40 (2021), 1401.</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main" xml:id="_K2myKty">Heart disease and stroke statistics-2022 update: a report from the American Heart Association</title>
		<author>
			<persName><forename type="first">Connie</forename><forename type="middle">W</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">W</forename><surname>Aday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Zaid I Almarzooq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">Z</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcio</forename><forename type="middle">S</forename><surname>Beaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><forename type="middle">K</forename><surname>Bittencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfred</forename><forename type="middle">E</forename><surname>Boehme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">April</forename><forename type="middle">P</forename><surname>Buxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Carson</surname></persName>
		</author>
		<author>
			<persName><surname>Commodore-Mensah</surname></persName>
		</author>
		<idno type="DOI">10.1161/cir.0000000000001052</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_yhAABhY">Circulation</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="153" to="e639" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Connie W Tsao, Aaron W Aday, Zaid I Almarzooq, Alvaro Alonso, Andrea Z Beaton, Marcio S Bittencourt, Amelia K Boehme, Alfred E Buxton, April P Carson, Yvonne Commodore-Mensah, et al. 2022. Heart disease and stroke statistics-2022 update: a report from the American Heart Association. Circulation 145, 8 (2022), e153-e639.</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main" xml:id="_x3Y8a7S">Extrasensory app: Data collection in-the-wild with rich user interface to self-report behavior</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Vaizman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadir</forename><surname>Weibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_xtEgNXE">Proceedings of the 2018 CHI conference on human factors in computing systems</title>
		<meeting>the 2018 CHI conference on human factors in computing systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yonatan Vaizman, Katherine Ellis, Gert Lanckriet, and Nadir Weibel. 2018. Extrasensory app: Data collection in-the-wild with rich user interface to self-report behavior. In Proceedings of the 2018 CHI conference on human factors in computing systems. Association for Computing Machinery, New York, NY, USA, 1-12.</note>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main" xml:id="_Csagfbq">Decaf: Generating fair synthetic data using causally-aware generative networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Van Breugel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trent</forename><surname>Kyono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeroen</forename><surname>Berrevoets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g5KD4mC">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22221" to="22233" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Boris van Breugel, Trent Kyono, Jeroen Berrevoets, and Mihaela van der Schaar. 2021. Decaf: Generating fair synthetic data using causally-aware generative networks. Advances in Neural Information Processing Systems 34 (2021), 22221-22233.</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main" xml:id="_s2pRpVP">Physicians&apos; perceptions of patients&apos; social and behavioral characteristics and race disparities in treatment recommendations for men with coronary artery disease</title>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Van Ryn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diana</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Malat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="DOI">10.2105/ajph.2004.041806</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_f3uT9eb">American journal of public health</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="351" to="357" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Michelle Van Ryn, Diana Burgess, Jennifer Malat, and Joan Griffin. 2006. Physicians&apos; perceptions of patients&apos; social and behavioral characteristics and race disparities in treatment recommendations for men with coronary artery disease. American journal of public health 96, 2 (2006), 351-357.</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main" xml:id="_q7dVm8G">WeMoD: A Machine Learning Approach for Wearable and Mobile Physical Activity Prediction</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Vasdekis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Yfantidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Efstathiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athena</forename><surname>Vakali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CUDvPKh">2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="385" to="390" />
		</imprint>
	</monogr>
	<note type="raw_reference">Dimitrios Vasdekis, Sofia Yfantidou, Stefanos Efstathiou, and Athena Vakali. 2022. WeMoD: A Machine Learning Approach for Wearable and Mobile Physical Activity Prediction. In 2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops). IEEE, IEEE, New York, NY, United States, 385-390.</note>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main" xml:id="_YzRqX9U">Tracking steps on apple watch at different walking speeds</title>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Veerabhadrappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Duffy Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">D</forename><surname>Renninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Rhudy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">B</forename><surname>Dreisbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristin</forename><forename type="middle">M</forename><surname>Gift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_hG32ygs">Journal of general internal medicine</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="795" to="796" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Praveen Veerabhadrappa, Matthew Duffy Moran, Mitchell D Renninger, Matthew B Rhudy, Scott B Dreisbach, and Kristin M Gift. 2018. Tracking steps on apple watch at different walking speeds. Journal of general internal medicine 33, 6 (2018), 795-796.</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main" xml:id="_MP4Ad3p">Fairness definitions explained</title>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cq8xTkz">IEEE/ACM International Workshop on Software Fairness (FairWare)</title>
		<meeting><address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sahil Verma and Julia Rubin. 2018. Fairness definitions explained. In 2018 IEEE/ACM International Workshop on Software Fairness (FairWare). IEEE, IEEE, New York, NY, United States, 1-7.</note>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main" xml:id="_TDg9CSJ">FairCharge: A data-driven fairnessaware charging recommendation system for large-scale electric taxi fleets</title>
		<author>
			<persName><forename type="first">Guang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Desheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YMGrDBT">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="25" />
		</imprint>
	</monogr>
	<note type="raw_reference">Guang Wang, Yongfeng Zhang, Zhihan Fang, Shuai Wang, Fan Zhang, and Desheng Zhang. 2020. FairCharge: A data-driven fairness- aware charging recommendation system for large-scale electric taxi fleets. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, (2020), 1-25.</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main" xml:id="_AzsxnaT">StudentLife: assessing mental health, academic performance and behavioral trends of college students using smartphones</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanglin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Harari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Tignor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dror</forename><surname>Ben-Zeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">T</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_2nTy8bR">Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing</title>
		<meeting>the 2014 ACM international joint conference on pervasive and ubiquitous computing<address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rui Wang, Fanglin Chen, Zhenyu Chen, Tianxing Li, Gabriella Harari, Stefanie Tignor, Xia Zhou, Dror Ben-Zeev, and Andrew T Campbell. 2014. StudentLife: assessing mental health, academic performance and behavioral trends of college students using smartphones. In Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing. Association for Computing Machinery, New York, NY, United States, 3-14.</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main" xml:id="_YgsHZg6">A survey on the fairness of recommender systems</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5SM5tSw">ACM Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yifan Wang, Weizhi Ma, Min Zhang*, Yiqun Liu, and Shaoping Ma. 2022. A survey on the fairness of recommender systems. ACM Journal of the ACM (JACM) 111 (2022), 1-43.</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main" xml:id="_NnKKJHs">Towards fairness in visual recognition: Effective strategies for bias mitigation</title>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Karakozis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BJcdnZd">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition<address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8919" to="8928" />
		</imprint>
	</monogr>
	<note type="raw_reference">Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. 2020. Towards fairness in visual recognition: Effective strategies for bias mitigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. IEEE, New York, NY, United States, 8919-8928.</note>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><surname>Who</surname></persName>
		</author>
		<title level="m" xml:id="_nB7KgEm">Report of the Formal Meeting of Member States to Conclude the Work on the Comprehensive Global Monitoring Framework, Including Indicators, and a Set of Voluntary Global Targets for the Prevention and Control of Non-Communicable Diseases</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">WHO. 2012. Report of the Formal Meeting of Member States to Conclude the Work on the Comprehensive Global Monitoring Framework, Including Indicators, and a Set of Voluntary Global Targets for the Prevention and Control of Non-Communicable Diseases.</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main" xml:id="_MH624Vg">GLOBEM: Cross-Dataset Generalization of Longitudinal Human Behavior Modeling</title>
		<author>
			<persName><forename type="first">Xuhai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subigya</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Sefidgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woosuk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">S</forename><surname>Kuehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><forename type="middle">F</forename><surname>Huckins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_K4ydhYG">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xuhai Xu, Xin Liu, Han Zhang, Weichen Wang, Subigya Nepal, Yasaman Sefidgar, Woosuk Seo, Kevin S Kuehn, Jeremy F Huckins, Margaret E Morris, et al. 2023. GLOBEM: Cross-Dataset Generalization of Longitudinal Human Behavior Modeling. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 6, 4 (2023), 1-34.</note>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main" xml:id="_f2Sgchs">Avoiding disparity amplification under different worldviews</title>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Yeom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tschantz</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zDUTHSJ">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="273" to="283" />
		</imprint>
	</monogr>
	<note type="raw_reference">Samuel Yeom and Michael Carl Tschantz. 2021. Avoiding disparity amplification under different worldviews. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing Machinery, New York, NY, USA, 273-283.</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main" xml:id="_dvm6E6r">Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Yfantidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Constantinides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Spathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athena</forename><surname>Vakali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Quercia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahim</forename><surname>Kawsar</surname></persName>
		</author>
		<idno>arXiv preprint 2303.15585</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note type="raw_reference">Sofia Yfantidou, Marios Constantinides, Dimitris Spathis, Athena Vakali, Daniele Quercia, and Fahim Kawsar. 2023. Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. Technical Report. arXiv preprint 2303.15585.</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main" xml:id="_TQBDtNy">The National Sleep Research Resource: towards a sleep data commons</title>
		<author>
			<persName><forename type="first">Guo-Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remo</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rueschman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mobley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Redline</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocy064</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_j9ktfeD">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1351" to="1358" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Guo-Qiang Zhang, Licong Cui, Remo Mueller, Shiqiang Tao, Matthew Kim, Michael Rueschman, Sara Mariani, Daniel Mobley, and Susan Redline. 2018. The National Sleep Research Resource: towards a sleep data commons. Journal of the American Medical Informatics Association 25, 10 (2018), 1351-1358.</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main" xml:id="_qAyPrqV">Hurtful words: quantifying biases in clinical contextual word embeddings</title>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_j7aqjZH">proceedings of the ACM Conference on Health, Inference, and Learning</title>
		<meeting>the ACM Conference on Health, Inference, and Learning<address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="110" to="120" />
		</imprint>
	</monogr>
	<note type="raw_reference">Haoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew McDermott, and Marzyeh Ghassemi. 2020. Hurtful words: quantifying biases in clinical contextual word embeddings. In proceedings of the ACM Conference on Health, Inference, and Learning. Association for Computing Machinery, New York, NY, United States, 110-120.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
