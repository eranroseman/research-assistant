<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_TnxE3gS">Conversational Health Agents: A Personalized LLM-Powered Agent Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">⋄ https://github.com/Institute4FutureHealth/CHA ⋄ https://opencha.com</p>
				</availability>
				<date type="published" when="2024-09-26">September 26, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>M.Sc</roleName><forename type="first">Mahyar</forename><surname>Abbasian</surname></persName>
							<email>abbasiam@uci.edu</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of California , Irvine</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Ph.D</roleName><forename type="first">Iman</forename><surname>Azimi</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of California , Irvine</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Ph.D</roleName><forename type="first">Amir</forename><forename type="middle">M</forename><surname>Rahmani</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of California , Irvine</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Ph.D</roleName><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> University of California , Irvine</note>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_VQsE5tU">Conversational Health Agents: A Personalized LLM-Powered Agent Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-09-26">September 26, 2024</date>
						</imprint>
					</monogr>
					<idno type="MD5">8CF4445108D81892AA3A15B956E91851</idno>
					<idno type="arXiv">arXiv:2310.02374v5[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T10:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_8gz6mvm"><p xml:id="_H8ufN3a"><s xml:id="_MeyzHTu">Conversational Health Agents (CHAs) are interactive systems that provide healthcare services, such as assistance and diagnosis.</s><s xml:id="_2PXp7MX">Current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation aspects.</s><s xml:id="_xpyEEQa">However, they offer limited agent capabilities, specifically lacking multi-step problem-solving, personalized conversations, and multimodal data analysis.</s><s xml:id="_JPuhD7y">Our aim is to overcome these limitations.</s><s xml:id="_jrujcW4">We propose openCHA, an open-source LLM-powered framework, to empower conversational agents to generate a personalized response for users' healthcare queries.</s><s xml:id="_hUvEqQq">This framework enables developers to integrate external sources including data sources, knowledge bases, and analysis models, into their LLM-based solutions.</s><s xml:id="_7MaDVq8">openCHA includes an orchestrator to plan and execute actions for gathering information from external sources, essential for formulating responses to user inquiries.</s><s xml:id="_Aw4WTdj">It facilitates knowledge acquisition, problemsolving capabilities, multilingual and multimodal conversations, and fosters interaction with various AI platforms.</s><s xml:id="_RuBrany">We illustrate the framework's proficiency in handling complex healthcare tasks via two demonstrations and four use cases.</s><s xml:id="_sm2zNZN">Moreover, we release openCHA as open source available to</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="28" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="29" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="30" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="31" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="32" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="33" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="34" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="35" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="36" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="37" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="38" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="39" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_zY8pwPn"><p xml:id="_HJXf6m3"><s xml:id="_xm5w4Eb">the community via GitHub ⋄,⋄ .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Nf4cZ35">INTRODUCTION</head><p xml:id="_WcJvnWw"><s xml:id="_Uq7pEVr">Artificial intelligence (AI), particularly large language model (LLM)-based conversational systems, has attracted immense global attention in recent years.</s><s xml:id="_fMYrrDz">These systems have revolutionized the field by enabling unprecedented access to and interaction with vast amounts of textual information.</s><s xml:id="_mNWDxm4">LLMs can aggregate and process comprehensive or focused segments of textual knowledge existing online, delivering contextually relevant, goal-oriented, and interactive access to this knowledge for anyone who needs it.</s><s xml:id="_sERr4Sa">The advent of LLMs has transformed early, simple conversation systems like Alexa and Siri, demonstrating significant effectiveness across diverse domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</s><s xml:id="_GD9mTmQ">Conversational systems can now engage in open-ended conversations and provide relevant, contextual information in a more natural and engaging way.</s></p><p xml:id="_nd7X5t6"><s xml:id="_xBP6wAn">While the field of AI has long explored intelligent agents, their focus has primarily been on analyzing the environment and making decisions based on gathered information.</s><s xml:id="_J3GkX6u">Early AI research often concentrated on physical world problems, fueled by advancements in computer vision, audio processing, and other areas of multimodal perceptual understanding.</s><s xml:id="_htubKQs">However, in dynamic environments like health management, where personalized and constantly evolving human health states are crucial, intelligent agents need to accurately capture these states through various means, including conversational interactions and access to personal user data.</s><s xml:id="_Q7N7BrS">This information needs to be collected and analyzed, leveraging the vast knowledge gathered through research and practitioners' experience.</s></p><p xml:id="_pBhyxwS"><s xml:id="_YGURpE4">Conversational Health Agents (CHAs) hold significant potential to address the challenges of dynamic health management environments.</s><s xml:id="_86WqTME">Thanks to the emergence of LLMs, CHAs can now understand user interactions through multimodal conversations, encompassing text, speech, and potentially other modalities.</s><s xml:id="_7pJHGxS">By analyzing these interactions, CHAs need to identify the necessary data, information, computational processes, and knowledge sources required to comprehend the user's evolving health state.</s><s xml:id="_QZRVfUK">This information is then translated into actionable insights that effectively guide healthcare management.</s><s xml:id="_PQ2Fbvs">In essence, CHAs should combine the power of LLM-based conversions with agents' capabilities, leveraging external data and information sources to navigate the complexities of personalized health environments and provide customized support for users:</s></p><p xml:id="_bbHQ2zG"><s xml:id="_kFNKcAq">• Conversation is the fundamental mode of human interaction.</s><s xml:id="_wemxqmd">Throughout the ages, conversations have consistently served as the primary source of knowledge and the catalyst for societal actions.</s><s xml:id="_Er9NVpA">Recently, numerous studies have substantiated the efficacy, usability, and overall satisfaction associated with the conversational aspect of CHAs <ref type="bibr" target="#b3">[4]</ref>.</s><s xml:id="_mJXPfd8">In healthcare settings, empathy <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and companionship <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> necessitate personalized conversations.</s></p><p xml:id="_A2aNpuC"><s xml:id="_Msfyx3a">• Agents should be furnished with conversational tools, interfaces, computational capabilities, and access to external resources to enhance the quality of healthcare delivery <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_hj8bn3Z">Given the intricacies of the healthcare domain <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, agents should understand users' queries, decompose them into the components of knowledge possession, perform health data access and analysis, and apply reasoning to respond effectively to specific situations <ref type="bibr" target="#b10">[11]</ref>.</s><s xml:id="_jrA5wVN">Their adaptability is essential, aligning their evolution with advancements in healthcare technology and literature <ref type="bibr" target="#b9">[10]</ref>.</s><s xml:id="_P4eygEM">Therefore, agents empower the abilities for personalization, multimodality, and continuous up-to-dateness <ref type="bibr" target="#b5">[6]</ref>.</s></p><p xml:id="_YGPab5J"><s xml:id="_3br423g">Our current exploration centers on the development of CHAs using the latest technological developments in AI, LLMs, and mHealth, where it has shown efficacy in the continuous collection of lifestyle and physiological data from users. Figure <ref type="figure">1</ref> shows an overview of the CHA main components.</s><s xml:id="_dR2FHXD">These indispensable components stand poised to facilitate the creation of exceptionally efficient CHAs.</s></p><p xml:id="_uXTj4Ud"><s xml:id="_sKqApBe">Existing LLMs, such as ChatGPT <ref type="bibr" target="#b0">[1]</ref>, BioGPT <ref type="bibr" target="#b11">[12]</ref>, ChatDoctor <ref type="bibr" target="#b12">[13]</ref>, and Med-PaLM <ref type="bibr" target="#b2">[3]</ref>, are currently active in the medical knowledge domain.</s><s xml:id="_4YJHKyp">These LLMs can be served as CHAs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>.</s><s xml:id="_3Ra6mJZ">However, they merely focus on the conversational aspects, offering limited agent capabilities such as basic text-based chat interfaces and lacking multi-step problem-solving capabilities.</s><s xml:id="_STy8HcB">They lack access to users' personally collected longitudinal data and electronic health records (EHRs), which include crucial information like vital signs, biosignals (e.g., electrocardiogram), medical images, and demographic data.</s><s xml:id="_BcW7v5S">Consequently, their responses tend to be generic and may not address individual health circumstances adequately.</s><s xml:id="_WnNpXnx">Moreover, they struggle Figure <ref type="figure">1</ref>: A conversational health agent including 1) a conversation component to enable user interaction and 2) a health agent for problem-solving and determining the optimal sequences of actions, leveraging health information.</s></p><p xml:id="_7w26Mwn"><s xml:id="_NkNNvvp">to incorporate the latest health insights, leading to potentially outdated responses <ref type="bibr" target="#b17">[18]</ref>.</s><s xml:id="_FkAvavv">Furthermore, these chatbots do not seamlessly integrate with established and existing AI models and tools <ref type="bibr" target="#b18">[19]</ref> for multimodal predictive modeling, rendering previous healthcare efforts obsolete.</s></p><p xml:id="_8Xn6gSw"><s xml:id="_TCfmpbm">In light of the significant advancements in technology and its paramount importance for both humanity and the environment, it becomes imperative that we synergize all available tools and harness knowledge from diverse sources to craft CHAs that offer a trustworthy, understandable, and actionable environment for a global audience.</s><s xml:id="_5mAy3AW">Presently, we stand on the threshold of crafting frameworks capable of delivering information in the most userfriendly and culturally attuned manner possible.</s><s xml:id="_jHNsHVM">This paper aims to introduce an initial iteration of such agents and lay the foundation for developing more sophisticated tools as our journey unfolds.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_4xsQdkG">BACKGROUND AND SIGNIFICANCE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_JgTVwGH">Related Work</head><p xml:id="_Af5jaY7"><s xml:id="_QuWutuG">Efforts in developing LLM-based CHAs can be categorized into three main groups: LLM Chatbots, Specialized Health LLMs, and Multimodal Health LLMs.</s><s xml:id="_WRAnkWd">LLM Chatbots employ and evaluate current chatbots (e.g., Chat-GPT) in executing distinct healthcare functions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</s><s xml:id="_DncqHQu">For instance, Chen et al. <ref type="bibr" target="#b22">[23]</ref> examined ChatGPT's efficacy in furnishing dependable insights on cancer treatment-related inquiries.</s></p><p xml:id="_t7d3cg4"><s xml:id="_G5NGuhx">Specialized Health LLMs delved deeper into the fundamental aspects of LLMs, aiming to enhance conversational models' performance by creating entirely new LLMs pretrained specifically for healthcare or fine-tuning existing models.</s><s xml:id="_FzpmcRr">Notable examples include initiatives such as ChatDoctor <ref type="bibr" target="#b12">[13]</ref>, MedAlpaca <ref type="bibr" target="#b1">[2]</ref>, and BioGPT <ref type="bibr" target="#b11">[12]</ref>.</s><s xml:id="_hBWe8Jd">This category emerged in response to research indicating that general-domain LLMs often struggle with healthcarespecific tasks due to domain shift <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and relying solely on prompt engineering may not significantly improve their healthcare-specific performance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>.</s></p><p xml:id="_Tq7AnaQ"><s xml:id="_YAEAcRg">Multimodal Health LLMs involve a novel trajectory by integrating multimodality into LLMs for diagnostic functions.</s><s xml:id="_KQsDMfu">For instance, Tu et al. <ref type="bibr" target="#b2">[3]</ref> investigated the potential of foundational transformer concepts in LLMs to amalgamate diverse modalities-videos, images, signals, and text-culminating in a multimodal generative model.</s><s xml:id="_BkGXnY9">Xu et al. <ref type="bibr" target="#b28">[29]</ref> introduced an LLM-aligned multimodal model, coupling chest X-ray images with radiology reports for Xray-related tasks.</s><s xml:id="_bWjFxH8">Similarly, Belyaeva et al. <ref type="bibr" target="#b29">[30]</ref> incorporated tabular health data into LLMs, yielding multimodal healthcare capabilities.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_EKGuacT">Existing Research Gaps and Challenges</head><p xml:id="_RQdCqHU"><s xml:id="_wdsgytb">Knowledge-groundedness and personalization in CHAs require tailored interactions that transcend basic dialogues, ensuring inclusivity through versatile, multimodal multilingual interfaces.</s><s xml:id="_eBVgdTS">The goal is to create CHAs that not only excel in conversational skills but also exhibit agent capabilities, enabling them to engage in critical thinking and strategic planning as proficient problem solvers.</s><s xml:id="_2C7scMG">Despite the great efforts in developing CHAs, the existing services and models suffer from the following limitations: i) Insufficient support for comprehensive personalization, particularly in cases necessitating real-time access to individualized data.</s><s xml:id="_V26VBbU">A substantial portion of users' healthcare data, primarily images, time-series, tabular data, and all other users' measured personal data streams is housed within healthcare platforms.</s><s xml:id="_eZvXUjS">Currently, CHAs have limited access to this data, primarily during the training and fine-tuning phases of LLM development, or they are completely severed from user data thereafter.</s><s xml:id="_Zx4ptMj">The absence of accurate user healthcare information -including continuous data from wearable devices, mHealth applications, and similar sources -hampers the performance of these agents, confining their capabilities to furnish generic responses, offer general guidelines, or potentially provide inaccurate answers.</s></p><p xml:id="_47Ar6Cp"><s xml:id="_w5ZnXg3">ii) Limited capacity to access up-to-date knowledge and retrieve the most recent healthcare knowledge base.</s><s xml:id="_uvFwyfA">Conventional LLMs depend on limited data and Internet-derived knowledge during their training phase, leading to three primary challenges.</s><s xml:id="_RhNNjzc">They tend to exhibit biases favoring populations with the most abundant online content, underscoring the importance of accessing the latest, relevant data.</s><s xml:id="_QUCBHGn">Recently introduced LLM-based services (e.g., ChatGPT4 <ref type="bibr" target="#b0">[1]</ref>) offer Internet search, but this is still insufficient for healthcare applications due to the large number of websites propagating false information.</s><s xml:id="_zYZxAJT">They lack updates on newly reliable Internet resources, a critical shortcoming in healthcare where novel, reliable, and evaluated treatments and modifications to previous recommendations are frequently ignored.</s><s xml:id="_tMdwvK2">Lastly, their reliance on outdated or less pertinent data makes identifying instances of hallucination <ref type="bibr" target="#b30">[31]</ref> problematic.</s><s xml:id="_U2mx7yd">Lack of up-to-date information reduces the trustworthiness and credibility of generated responses <ref type="bibr" target="#b17">[18]</ref>.</s></p><p xml:id="_Q5HQzjk"><s xml:id="_pF22kMQ">iii) Lack of seamless integration with established, multimodal data analysis tools and predictive models that require external execution.</s><s xml:id="_T3M5Djk">Current agents often overestimate the computational capabilities of generative AI, leading to an under-utilization of well-established healthcare analysis tools, despite their proficiency in managing diverse data types <ref type="bibr" target="#b18">[19]</ref>.</s></p><p xml:id="_Fv9wFJM"><s xml:id="_DuQ7sSk">iv) Lack of multi-step problem-solving capabilities.</s><s xml:id="_zUnzyNN">Existing LLM-based CHAs are typically specialized for specific tasks or deficient in robust data analysis capabilities.</s><s xml:id="_4wa78bP">For example, Xu et al. <ref type="bibr" target="#b28">[29]</ref> model performs X-ray image reporting relying solely on X-ray images, ignoring other modalities, such as vital signs recorded in time-series format.</s><s xml:id="_3YYfneq">Additionally, the existing CHAs cannot address intricate sequential tasks (i.e., act as problem solvers).</s><s xml:id="_AdD6e6B">Incorporating LLMs into CHAs requires integrating sequential reasoning, personalized health history analysis, and data fusion.</s></p><p xml:id="_BUpAJvk"><s xml:id="_vjzFFxX">LLMs solely are insufficient to tackle the previously mentioned challenges.</s><s xml:id="_7YDEWXh">To make them practical for real-world applications, we need a comprehensive framework that harnesses LLMs while integrating various auxiliary components and external resources.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_xSNE6ct">Key Contributions</head><p xml:id="_n9zMY58"><s xml:id="_uaSkpXu">In this article, we present a holistic LLM-powered framework for the development of CHA, aiming to rectify the limitations mentioned above.</s><s xml:id="_zQSMHTU">We delineate the components of our framework and proceed with a case study demonstrating our agent's capacity.</s><s xml:id="_rnXMB4J">The framework is a problem solver that provides personalized responses by utilizing contemporary Internet resources and advanced multimodal healthcare analysis tools, including machine learning methods.</s><s xml:id="_pfumxYQ">Significant contributions are as follows.</s></p><p xml:id="_2m4HqJu"><s xml:id="_CKWEQZD">1. We propose an LLM-powered Orchestrator, acting as a problem solver, to address healthcare-related queries by analyzing input queries, gathering the required information, performing actions, and offering personalized responses.</s></p><p xml:id="_eBnwm2J"><s xml:id="_DNYWCwA">2. We introduce external healthcare data sources, knowledge bases, and AI and Analysis models critical for enabling CHAs to offer reliable, trustworthy, and up-to-date responses.</s></p><p xml:id="_RFjP5N6"><s xml:id="_gHXGavK">3. We incorporate multimodal and multilingual capabilities into the framework, increasing usability.</s></p><p xml:id="_HNHkaDP"><s xml:id="_SVD24eg">4. We show the framework's effectiveness using two demos and four use cases.</s></p><p xml:id="_ZYpkpTM"><s xml:id="_XV3zsTK">5. We release the CHA framework as open source on GitHub ⋄ along with detailed documentation ⋄ , inviting the community to leverage and integrate it into their solutions.</s></p><p xml:id="_dxPJJuK"><s xml:id="_jPaReEw">External Sources AI and Analysis Models Bio-signal Processing Image Processing Tabular Data Processing Privacy API Knowledge Base Knowledge Extractor Retrieval Models Search Engines Privacy API Healthcare Data Source Wearable Data Tabular Data (e.g., EHR) Image Data Privacy API Person Doctor User Query Answer Orchestrator Task Executor Available Tasks Data Pipe Promptist Task Planner LLM Metadata Interface Mobile Application Web Application Desktop Application Response Generator LLM API Privacy Translators Speech to Text Gesture to Text</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_5JMzRb4">MATERIAL AND METHODS</head><p xml:id="_S7J8yxh"><s xml:id="_QNrnmqD">We design an LLM-powered framework with a central agent that perceives and analyzes user queries, provides appropriate responses, and manages access to external resources through Application Program Interfaces (APIs) or function calls.</s><s xml:id="_E2uSUeh">The user-framework interaction is bidirectional, ensuring a conversational tone for ongoing and follow-up conversations.</s><s xml:id="_c5aPNWY">Figure <ref type="figure" target="#fig_0">2</ref> shows an overview of the framework, including three major components: Interface, Orchestrator, and External Sources.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_wpSc4q5">Interface</head><p xml:id="_BVMGysd"><s xml:id="_HmHu3pm">Interface acts as a bridge between the users and agents, including interactive tools accessible through mobile, desktop, or web applications.</s><s xml:id="_8vVnJxQ">It integrates multimodal communication channels, such as text and audio.</s><s xml:id="_ZsFndmV">The Interface receives users' queries and subsequently transmits them to the Orchestrator (see Figure <ref type="figure" target="#fig_0">2</ref>).</s><s xml:id="_GYR8HMu">Within this framework, users can provide metadata (alongside their queries), including images, audio, gestures, and more.</s><s xml:id="_56ZhgRB">For instance, a user could cap-ture an image of their meal and inquire about its nutritional values or calorie content, with the image serving as metadata.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TjFkPbb">Orchestrator</head><p xml:id="_xehMccY"><s xml:id="_nZ3xBbK">The Orchestrator is the openCHA agent core, which is responsible for problemsolving, planning, executing actions, and providing an appropriate response based on the user query.</s><s xml:id="_teeFcPC">It incorporates the concept of the Perceptual Cycle Model <ref type="bibr" target="#b31">[32]</ref> in openCHA, allowing it to perceive, transform, and analyze the world (i.e., input query and metadata) to generate appropriate responses.</s><s xml:id="_cGjQcQH">To this end, the input data are aggregated, transformed into structured data, and then analyzed to plan and execute actions.</s><s xml:id="_299WKCm">Through this process, the Orchestrator interacts with external sources to acquire the required information, perform data integration and analysis, and extract insights, among other functions.</s><s xml:id="_MEQ3PkA">In the following, we outline five major components of the Orchestrator.</s></p><p xml:id="_8zFAVxf"><s xml:id="_MRrTqhF">The Task Planner is the LLM-enabled decision-making, planning, and reasoning core of the Orchestrator.</s><s xml:id="_FYj8ca9">Its primary responsibility is gathering all necessary information to answer users' queries.</s><s xml:id="_rHPD5KJ">To achieve this, it interprets the user's query and metadata, identifying the necessary steps for task execution.</s></p><p xml:id="_8ZZKnVA"><s xml:id="_ByXrkAs">To transform a user query into a sequence of tasks, we incorporate the Tree of Thought <ref type="bibr" target="#b32">[33]</ref> prompting methods into the Task Planner.</s><s xml:id="_eaegZHt">Using this prompting method, the LLM is asked to 1) generate three unique strategies (i.e., sequences of tasks to be called with their inputs), 2) describe the pros and cons of each strategy, and 3) select one as the best strategy.</s><s xml:id="_g7jrMYQ">An alternative prompting technique incorporated in openCHA is ReAct <ref type="bibr" target="#b33">[34]</ref>, which employs reasoning and action techniques to ascertain the essential tasks to be executed.</s><s xml:id="_7MXMVj8">openCHA offers users the flexibility to choose the prompting method that best meets the needs of their application.</s><s xml:id="_pDqnmCb">Other prompting techniques, such as Plan-and-Solve Prompting <ref type="bibr" target="#b34">[35]</ref>, could also be implemented and integrated as a Task Planner.</s></p><p xml:id="_nqvEceW"><s xml:id="_nv22655">We outline the process of creating and integrating a task into openCHA in Appendix 1.</s><s xml:id="_G5EdFyC">We also indicate how the task is converted into an appropriate prompt, enabling the Task Planner to recognize the available tasks and how to invoke them.</s><s xml:id="_KQmkxWM">Appendix 2 provides a detailed examination of the Task Planner's implementation, utilizing the Tree of Thought prompting method.</s></p><p xml:id="_XHmGRZp"><s xml:id="_NxTsKzg">In the proposed Orchestrator, the planning part is performed in English, leveraging the superior capabilities of LLMs in this language.</s><s xml:id="_9kvQ6A3">The framework can employ one of two distinct approaches if the query is in a language other than English.</s><s xml:id="_yq55mFP">The first approach retrains the source language and utilizes the language model capabilities in that language to generate responses.</s><s xml:id="_VyYCHm6">The second approach involves translating the query into English (e.g., using Google Translate), planning and executing the process in English, and translating the final answer back into the source language.</s></p><p xml:id="_9aP58ny"><s xml:id="_cQFunHA">The Task Executor carries out actuation within the Orchestrator by following the planning and task execution steps determined by the Task Planner.</s><s xml:id="_4sQ6DgY">The Task Executor has two primary responsibilities.</s><s xml:id="_U6SyhZ6">First, it acts as a data converter, converting the input query and metadata and preparing it to be used by the Task Planner.</s><s xml:id="_AArWwMh">For instance, if the question is in a language other than English, it will be translated into English using the Google Translate service <ref type="bibr" target="#b35">[36]</ref>.</s><s xml:id="_x8V84PH">Furthermore, if the metadata contains files or images, Task Executor sends the metadata details to Task Planner for planning.</s><s xml:id="_yCjt8Jg">Second, the Task Executor executes tasks generated by the Task Planner through interactions with external sources.</s><s xml:id="_hTmuZcR">The results are then relayed to the Task Planner to continue planning if needed.</s><s xml:id="_STjSN7Q">In the end, the Task Planner signals the end of the planning.</s><s xml:id="_7M3Zvum">In Appendix 2, we detail how the Task Planner translates planned tasks into execution instructions, enabling the Task Executor to properly carry out the tasks.</s></p><p xml:id="_VJvF2f2"><s xml:id="_yFzU2ge">It is crucial to emphasize that communication between the task planner and task executor is bidirectional.</s><s xml:id="_Dsm5Gjp">An iterative process continues between the Task Executor and Task Planner until the Task Planner accumulates sufficient information to respond appropriately to the user's inquiry.</s><s xml:id="_HA3dRPZ">This two-way exchange proves indispensable because, in specific scenarios, the Task Planner may necessitate intermediate information to determine subsequent actions.</s></p><p xml:id="_g9nT33N"><s xml:id="_quawxya">The Data Pipe is a repository of metadata and data acquired from External Sources through the execution of conversational sessions.</s><s xml:id="_xNe5fUB">This component is essential because numerous multimodal analyses involve intermediate stages, and their associated data must be retained for future retrieval.</s><s xml:id="_PnUrAQc">The intermediate data might be large, surpassing token limits, or challenging to comprehend and utilize by the Task Planner's or Response Generator's LLM.</s><s xml:id="_zz6P6Fa">The Data Pipe is automatically managed by the Task Executor.</s><s xml:id="_cVbhCQC">It monitors the stored metadata and intermediate data.</s></p><p xml:id="_vSjCJVA"><s xml:id="_zsNMHhG">The Data Pipe in openCHA can range from a simple in-memory key/value storage for intermediate data to a more complex database system.</s><s xml:id="_Un2HYUQ">The pro-posed framework allows developers to determine whether their tasks' results are intermediate or should be directly returned to the LLM.</s><s xml:id="_PpXgMxh">Appendix 1 details how developers can configure this setting.</s></p><p xml:id="_qytCnGn"><s xml:id="_e9Z6QYK">Additionally, we have implemented a mechanism whereby an intermediate result stored in the Data Pipe generates a unique key as the task's outcome.</s><s xml:id="_DYM9F4A">This key is then provided in the Task Planner prompt, aiding the Task Planner in recognizing and utilizing this data as necessary.</s><s xml:id="_SHEMxwu">Appendix 4 illustrates sample prompts generated for tasks and demonstrates how the Task Planner employs the Data Pipe key.</s></p><p xml:id="_Zxx9SwX"><s xml:id="_AAnFp8G">The Promptist is responsible for transforming query text or outcomes from External Sources into suitable prompts that can be supplied to either the Task Planner or the Response Generator.</s><s xml:id="_hqMfFR6">The Promptist provides the flexibility to modify and adapt each technique, allowing for seamless integration and customization.</s><s xml:id="_KtHzQcr">It can be implemented using existing prompting techniques, some of which are listed as follows.</s></p><p xml:id="_AsZT8nJ"><s xml:id="_ngabp42">LLM-REC, proposed by Lyu et al. <ref type="bibr" target="#b36">[37]</ref>, employs four unique prompting strategies to enrich text descriptions, enhancing personalized text-based recommendations.</s><s xml:id="_R95GqvA">The approach leverages the LLM to understand item characteristics, significantly improving recommendation quality.</s><s xml:id="_MVquHyR">Additionally, the Hao et al. <ref type="bibr" target="#b37">[38]</ref> method can be leveraged, which optimizes text-to-image prompt generation through a framework called prompt adaptation.</s><s xml:id="_AFXENGt">It automatically refines user inputs into model-preferred prompts.</s><s xml:id="_EWn6sMj">This process starts with supervised fine-tuning of a pretrained language model using a curated set of prompts.</s><s xml:id="_UywZjGB">It then employs reinforcement learning, guided by a reward function, to identify more effective prompts that produce aesthetically pleasing images aligned with user intentions.</s><s xml:id="_CxwPwkX">Furthermore, the instructions provided by OpenAI on creating more effective prompts can be used <ref type="bibr" target="#b38">[39]</ref>.</s></p><p xml:id="_cWdevhm"><s xml:id="_mQxap96">The Response Generator is an LLM-based module responsible for preparing the response.</s><s xml:id="_TCctSRz">It refines the gathered information by the Task Planner, converting it into an understandable format and inferring the appropriate response.</s><s xml:id="_EJG42rj">We separate the Response Generator and Task Planner to allow flexibility in choosing diverse LLM models and prompting techniques for these components.</s><s xml:id="_4CzJwng">This division ensures that the Task Planner focuses solely on planning without responding to users, while the Response Generator utilizes gathered information to deliver conclusive responses.</s><s xml:id="_AX9fZWd">This segregation facilitates the Response Generator in addressing aspects of empathy and companionship in conversations.</s><s xml:id="_kXp7xM3">In contrast, the Task Planner primarily handles personalization and the up-to-dateness of conversations.</s><s xml:id="_AvmMeQC">Appendix 3 outlines the implementation of the Response Generator and how it utilizes results collected by the Task Planner to respond to the user effectively.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_MAKQeJb">External Sources</head><p xml:id="_dNXKqVJ"><s xml:id="_SBz6XA4">External Sources play a pivotal role in obtaining essential information from the broader world.</s><s xml:id="_xxZfass">Typically, these External Sources furnish application program interfaces (APIs) that the Orchestrator can use to retrieve required data, process them using AI or analysis tools, and extract meaningful health information.</s><s xml:id="_gWEU6xF">In openCHA, we integrate with four primary external sources, which we found critical for CHAs (see Figure <ref type="figure" target="#fig_0">2</ref>).</s></p><p xml:id="_6htsC3V"><s xml:id="_uB728N3">Healthcare Data Source enables the collection, ingestion, and integration of data captured from a variety of sources, such as Electronic Health Record (EHR), smartphones, and smartwatches, for healthcare purposes <ref type="bibr" target="#b39">[40]</ref>.</s><s xml:id="_kcYMgT9">Examples of data sources are mHealth platforms and healthcare databases.</s><s xml:id="_rFBgYGF">mHealth platforms have garnered significant attention in the recent wave of healthcare digitalization, enabling ubiquitous health monitoring <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>.</s><s xml:id="_QpvHd5d">The data encompass various modalities, including biosignals (e.g., PPG collected via a smartwatch), images (e.g., captured via user's smartphone), videos, tabular data (e.g., demographic data gathered from EHR), and more.</s><s xml:id="_vBEHzuZ">Notable examples of such healthcare platforms include ZotCare <ref type="bibr" target="#b42">[43]</ref> and ilumivu <ref type="bibr" target="#b43">[44]</ref>, offering APIs for third-party integration.</s><s xml:id="_sEPm9Fc">In our context, the Orchestrator functions as a third party, accessing user data with their consent.</s></p><p xml:id="_sqfwSGv"><s xml:id="_DkA8pHt">Knowledge Base fetches the most current and pertinent data from healthcare sources, such as healthcare literature, reputable websites, or knowledge graphs using search engines or retrieval models <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>.</s><s xml:id="_kfqNcCA">Accessing this retrieved information equips CHAs with up-to-date, personalized knowledge, enhancing its trustworthiness while reducing hallucination and bias.</s><s xml:id="_ucm7VjW">openCHA allows the integration of various knowledge bases to be defined and configured as tasks.</s></p><p xml:id="_2u9s2xJ"><s xml:id="_YJKJKqP">AI and Analysis Models provide data analytics tools to extract information, associations, and insights from data <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b18">19]</ref>, playing a crucial role in the evolving landscape of LLM-healthcare integration, enhancing trustworthiness and personalization.</s><s xml:id="_pkSJtmu">They can perform various tasks, including data denoising, abstraction, classification, and event detection, to mention a few <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b18">19]</ref>.</s><s xml:id="_qVsxrMk">As generative models, LLMs cannot effectively perform extensive computations or act as machine learning inferences on data.</s><s xml:id="_9TyqmHj">The AI platforms empower our framework to leverage existing health data analytic approaches.</s></p><p xml:id="_bXgzuEF"><s xml:id="_auCXxUY">Translators effectively convert various languages into widely spoken languages, such as English, thereby enhancing the accessibility and inclusivity of CHAs.</s><s xml:id="_UR2r6hK">Existing agents face limitations that hinder their usability for large communities globally.</s><s xml:id="_fuwJzjp">Universal text literacy for CHAs often narrows their reach and positions them as a privilege <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>.</s><s xml:id="_VCty5NW">Many underserved communities face obstacles while using CHAs due to their educational disparities, financial constraints, and biases that favor developed nations within existing technological paradigms.</s><s xml:id="_JWrQvEr">Our framework integrates with Translator platforms and is designed to accommodate and support communication with diverse communities.</s><s xml:id="_2MXnMbz">This integration enhances the overall usability of CHAs.</s></p><p xml:id="_mFCpTHT"><s xml:id="_aTszYdB">The selection of the external sources is based on the information and knowledge they provide and their interaction with the Orchestrator.</s><s xml:id="_pduAv82">The Orchestrator handles various data types in the proposed framework, including text, JSON formatted data, or unstructured data such as images and audio.</s><s xml:id="_S39AVfM">This design ensures that any external source capable of returning results in these formats is supported.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Bu7Ey5T">DEMONSTRATION</head><p xml:id="_FkW9hjK"><s xml:id="_5NXkrze">We demonstrate the capabilities of openCHA through three distinct demos.</s><s xml:id="_NNrGdMN">These demonstrations highlight how LLMs' planning and reasoning abilities can effectively comprehend user queries and translate them into appropriate task executions.</s><s xml:id="_CkUYCC6">Each demonstration involves linking a set of implemented tasks to openCHA.</s><s xml:id="_EtdBCt5">Subsequently, we highlight openCHA's planning proficiency in effectively sequencing tasks in the correct order with appropriate inputs and executing the tasks.</s><s xml:id="_UuuKyyF">After executing all necessary tasks, the results are conveyed to the Response Generator to provide the final response.</s><s xml:id="_2akqU8a">The overview of the implemented tasks and their utilization in each demo is depicted in Figure <ref type="figure">3</ref>. 5 REM sleep enough during August 2020?",</s><s xml:id="_k5Qa8rr">"How much is the total step count of Patient 5 during August 2020?" or "Provide an activity summary of Patient 5 during 2020".</s></p><p xml:id="_66ewF8r"><s xml:id="_HUmgvdb">For this demo, we implement two tasks for retrieving sleep and physical activity from a health monitoring dataset <ref type="bibr" target="#b52">[53]</ref>.</s><s xml:id="_KMYaaPw">The data utilized in this demo is a part of an extensive longitudinal study focusing on the mental health of college students, as documented in <ref type="bibr" target="#b52">[53]</ref>.</s><s xml:id="_v4mJxw5">Moreover, we develop analytical tasks capable of executing basic statistical analysis (e.g., computing trends and averaging).</s><s xml:id="_jPWPvPr">Finally, we also add Google Search and Extract Text tasks so that the current setup of openCHA can access internet information.</s><s xml:id="_pMZzT2d">The involved tasks in this demo are marked by red dots in Figure <ref type="figure">3</ref>. Figure <ref type="figure" target="#fig_2">4</ref> illustrates sample queries and the corresponding responses from openCHA.</s><s xml:id="_XT3Czuu">The Task Planner identifies the correct tasks, their execution order, and the appropriate inputs to be used.</s><s xml:id="_RKUN5tK">Table <ref type="table" target="#tab_1">1</ref> showcases the strategy chosen by the Task Planner using the Tree of Thought prompting technique.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mpDAeQm">Demo 2: Objective stress level estimation with multilanguage interaction</head><p xml:id="_6MPh7qn"><s xml:id="_JSgmtgf">Demo 2 indicates the openCHA's capability to conduct signal processing and objective stress level estimation.</s><s xml:id="_rZrqvbe">We also show that openCHA can answer the query in multiple languages.</s><s xml:id="_X6HfPXa">This is achieved by interacting with a  To fulfill our objective, we implemented three distinct tasks (yellow dots in Figure <ref type="figure">3</ref>).</s><s xml:id="_vK7uxSD">The first task involved acquiring Photoplethysmogram (PPG) data from the patients.</s><s xml:id="_yyJng8H">PPG data were gathered using Samsung Gear Sport smartwatches <ref type="bibr" target="#b53">[54]</ref>, with a sampling frequency of 20 Hz, while participants were in free-living conditions.</s><s xml:id="_v33NTM4">The data is part of the <ref type="bibr" target="#b52">[53]</ref>.</s><s xml:id="_ZjZssh5">The second task performs PPG signal processing to extract heart rate variability (HRV) metrics.</s><s xml:id="_hkX7SsW">For this purpose, we utilize the Neurokit <ref type="bibr" target="#b54">[55]</ref> Python library.</s><s xml:id="_BkZTcD8">In our case study, we extract a total of 32 HRV parameters, including metrics such as the root mean square of successive differences between normal heartbeats (RMSSD), low-frequency (LF), and high-frequency (HF) values <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>.</s><s xml:id="_anY44Ed">The third task estimates stress levels based on HRV using an AI model.</s><s xml:id="_GTQxcFp">Initially, we employed an autoencoder to reduce the dataset's 32 HRV features to 12. Subsequently, a four-layer neural network categorizes the 12 features into five stress levels.</s><s xml:id="_Ee5Qc35">The evaluation of the stress estimation model demonstrates an 86% accuracy rate on a test set.</s><s xml:id="_XcsTHGP">Figure <ref type="figure" target="#fig_3">5</ref> depicts example queries and the corresponding responses from openCHA.</s><s xml:id="_KD34pca">The Task Planner's approach involves initially retrieving the PPG data of Patient 5 on August 29th, 2020.</s><s xml:id="_8WCH4Vc">Subsequently, the obtained result is forwarded to the PPG analysis task to extract HRV metrics.</s><s xml:id="_q5Ku5KQ">Lastly, the planner initiates the execution of stress analysis tasks, providing the HRV metrics for this task.</s><s xml:id="_yGz8ePy">Table <ref type="table" target="#tab_2">2</ref> displays the chosen strategy by the Task Planner uti-lizing the Tree of Thought prompting technique.</s><s xml:id="_A9AkN65">The ultimate estimated stress, along with an explanation, is then returned to the user.</s><s xml:id="_P4MfRAn">openCHA Use cases To indicate the usability of openCHA across various applications, we outline several use cases that have utilized the framework in their research as follows.</s><s xml:id="_fkP8TuP">1. ChatDiet [58] introduced a personalized, nutrition-oriented food recommendation agent, utilizing openCHA as its core implementation.</s><s xml:id="_vCuRCKP">By integrating personal and population models as external sources, Chat-Diet offered tailored food suggestions.</s><s xml:id="_tPuXs4b">It enhanced traditional food recommendation services by delivering dynamic, personalized, and explainable recommendations.</s><s xml:id="_QxyvZUX">In a case study, ChatDiet achieved an effectiveness rate of 92%, outperforming solutions like ChatGPT. 2. Knowledge-infused LLM-powered CHA for diabetic patients <ref type="bibr" target="#b58">[59]</ref> is developed by integrating domain-specific knowledge and analytical tools as external sources using openCHA.</s><s xml:id="_TcNtjRJ">This integration included incorporating American Diabetes Association dietary guidelines and deploying analytical tools for nutritional intake calculation, resulting in superior performance compared to GPT4 in managing diabetes through tailored dietary recommendations, as demonstrated by an evaluation of 100 diabetes-related questions.</s></p><p xml:id="_CQBDhkX"><s xml:id="_QV8APr3">3. openCHA was employed to develop an agent for evaluating the safety and reliability of mental health chatbots <ref type="bibr" target="#b59">[60]</ref>.</s><s xml:id="_c5WF8xp">This agent's evaluation capabilities were compared with expert assessments and several existing LLMs, including GPT4, Claude, Gemini, and Mistral.</s><s xml:id="_gB4wfD8">Guidelines and benchmarks introduced by experts and Internet search served as external sources linked to openCHA.</s><s xml:id="_66qmQjE">The agent demonstrated superior accuracy, achieving the lowest mean absolute error (MAE) against experts' scores -a reduction by a factor of 1 compared to LLMs' scores, with the maximum MAE being 10 -and provided unbiased evaluation scores.</s></p><p xml:id="_sJREx5x"><s xml:id="_NVQ2HYt">4. The Empathy-enhanced CHA <ref type="bibr" target="#b60">[61]</ref> was developed to interpret and respond to users' emotional states through multimodal dialogue, representing a significant step forward in providing contextually aware and empathetically resonant support in the mental health field.</s><s xml:id="_bqTTHph">This paper utilized speech-to-text, text-to-speech, and speech emotion detection models as external sources connected to openCHA.</s></p><p xml:id="_d7BBjrz"><s xml:id="_N5CxJ9B">To see more demonstrations on how the openCHA works in real setup, we have uploaded multiple YouTube videos ⋄, ⋄, ⋄, ⋄</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WUjkUAr">DISCUSSION openCHA Potentials and Limitations</head><p xml:id="_2BpxENu"><s xml:id="_zRRFW7j">In this section, we briefly discuss our proposed framework's capabilities, potentials, and limitations.</s></p><p xml:id="_hxYbMVy"><s xml:id="_Us7NYzy">Flexibility: openCHA provides a high level of flexibility to integrate LLMs with external data sources, knowledge bases, and analytical tools.</s><s xml:id="_A9ct9qR">The proposed components can be developed and replaced according to the requirements of the healthcare application in question.</s><s xml:id="_K2sBmsQ">For instance, new external sources can be effortlessly integrated and introduced as new tasks into openCHA.</s><s xml:id="_dQPyTJV">The LLMs employed in openCHA can be readily swapped with fine-tuned or more healthcare-specific LLMs.</s><s xml:id="_hAhaUjt">Similarly, the Planner prompting technique and decision-making processes are modifiable.</s><s xml:id="_Q9Zr4Xy">This flexibility facilitates collaboration among diverse research communities, enabling them to contribute to various aspects of CHAs.</s><s xml:id="_VSc6Xe9">Appendix 1 shows how a new task can be defined and introduced into openCHA.</s><s xml:id="_pJUkNH7">Explainability: openCHA enhances explainability for CHAs, allowing users to inquire about the tools and actions used to generate a response.</s><s xml:id="_V5ene7V">As detailed in Appendices 2 and 3, openCHA maintains a "previous actions" section that records past conversations and tasks.</s><s xml:id="_k67AVtc">When queried about task usage, it lists the executed tasks and their applications, enhancing transparency and fostering trust between users and CHAs.</s><s xml:id="_vSe6RaE">For instance, in Demo 2, when a user asks, "Name the tasks used," openCHA responds by detailing that PPG and HRV data were utilized to determine stress levels.</s><s xml:id="_j2JUuUf">An example of this interaction is shown in Figure <ref type="figure" target="#fig_4">6</ref>.</s><s xml:id="_KECvUBE">Personalization: The openCHA framework enhances personalization by integrating individual information and analytics tools from healthcare systems or local databases as external sources.</s><s xml:id="_MXgHurt">The quality of these external ⋄ <ref type="url" target="https://www.youtube.com/watch?v=w48sPlF5zhs">https://www.youtube.com/watch?v=w48sPlF5zhs</ref> ⋄ <ref type="url" target="https://www.youtube.com/watch?v=PWxL_OgWGfE&amp;t=3s">https://www.youtube.com/watch?v=PWxL_OgWGfE&amp;t=3s</ref> ⋄ <ref type="url" target="https://www.youtube.com/watch?v=c-7IEBaRSyQ">https://www.youtube.com/watch?v=c-7IEBaRSyQ</ref> ⋄ <ref type="url" target="https://www.youtube.com/watch?v=rHXpk_P5n6Y">https://www.youtube.com/watch?v=rHXpk_P5n6Y</ref> sources greatly influences the effectiveness of the personalization.</s><s xml:id="_MEp9AuM">For example, ChatDiet <ref type="bibr" target="#b57">[58]</ref> utilizes personal dietary preferences and population data, along with an analysis of nutrients' effects on health outcomes like sleep quality, to enhance its food recommendations significantly.</s><s xml:id="_r3YSgFt">This strategy not only heightens the accuracy of the recommendations but also ensures they are precisely tailored to meet individual dietary needs.</s><s xml:id="_y5NWeC2">Reliability: openCHA boosts the reliability of answers by leveraging validated information and computations as external sources.</s><s xml:id="_4mSbbjm">Our framework is tailored to effectively utilize existing LLMs for complex healthcare tasks, strategically offloading computational and sensitive information tasks to external sources while reserving LLMs primarily for reasoning and generating responses.</s><s xml:id="_PmkRQCF">For instance, the paper "Knowledge-infused LLM-powered CHA for diabetic patients" <ref type="bibr" target="#b58">[59]</ref> demonstrates the benefits of integrating external knowledge to accurately determine nutritional values and align them with established guidelines, highlighting inaccuracies in nutritional estimations when solely relying on GPT4 LLM for data access and calculations.</s><s xml:id="_jnefemJ">Latency: Utilizing multiple external sources offers benefits, but it can also affect the model's response time, potentially leading to increased latency in the CHA.</s><s xml:id="_NC9esqz">As the number of tasks and steps within the framework expands, there could be a rise in response time, which might diminish usability.</s><s xml:id="_bmjjRAk">Recent research by Singh et al. <ref type="bibr" target="#b61">[62]</ref> explores new methods for executing tasks in parallel when they are not dependent on each other.</s><s xml:id="_vtkrUFU">Toekn Limit: Token limits in LLMs present a challenge for accommodating tasks within the Task Planner.</s><s xml:id="_axZ9MTf">However, recent advancements <ref type="bibr" target="#b62">[63]</ref> indicate progress in extending LLM token limits, which helps mitigate this issue.</s><s xml:id="_Q9RVJgG">Privacy and Security: Privacy and security are crucial in this framework, particularly for healthcare applications dealing with user privacy concerns.</s><s xml:id="_MD28rDv">Strong privacy measures are essential to prevent unauthorized access, data breaches, and identity theft, with potentially severe consequences <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b64">65]</ref>.</s><s xml:id="_kVfrKZS">To address privacy and security issues, external sources should provide data confidentiality.</s><s xml:id="_AZjf3bv">For example, this can be enabled by either granting limited access to CHAs as third parties with user permission or employing deidentification and anonymization techniques <ref type="bibr" target="#b65">[66]</ref>.</s><s xml:id="_Yj86tXr">Another strategy is to prevent LLMs from using users' provided data for training and fine-tuning, ensuring that such data is not stored.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_Uc5UmgH">Study Limitations and Future Work</head><p xml:id="_XeYmpXx"><s xml:id="_SGEsab9">In this section, we outline the study limitations and future research directions.</s><s xml:id="_TBXZFgU">Planning Robustness: Since we utilize LLMs for planning and response generation, there is still the inherent risk of biases or trustfulness issues.</s><s xml:id="_6xUDrAH">Our framework aims to enhance the robustness of planning by integrating external sources to reduce these problems, though it cannot ensure their complete elimination.</s><s xml:id="_n62H7P7">To enhance the planning robustness, we will explore using Agentic design patterns like the self-consistency <ref type="bibr" target="#b66">[67]</ref> method or new reasoning techniques.</s><s xml:id="_7QmQBhu">Accuracy and Evaluation: Accuracy and evaluation in our framework hinge on the configuration choices made by researchers, such as the selected external sources, LLM, and planning technique.</s><s xml:id="_43EGSbv">Since knowledge, data, and analytics are outsourced to external sources, the quality of these sources plays a crucial role in enhancing accuracy; better external sources increase the likelihood of achieving superior results.</s></p><p xml:id="_ErkTZRr"><s xml:id="_gkuT2r3">Two distinct assessments are necessary to evaluate such systems.</s><s xml:id="_gvxCEn4">The first evaluates the accuracy of external sources, whether they are AI models or knowledge bases.</s><s xml:id="_ZdTwQJh">The second assesses the overall configured and constructed CHA to determine if it behaves as expected.</s><s xml:id="_BQbFXMn">Several metrics and evaluation methods are recommended in <ref type="bibr" target="#b67">[68]</ref>, with additional evaluation techniques explored in <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>.</s><s xml:id="_y64vDeX">These evaluations are application-specific, and our framework provides extensive customization capabilities to suit different use cases and requirements.</s><s xml:id="_sukDGK5">In our future work, we will explore more evaluation techniques.</s><s xml:id="_MNtQ4Nx">User Query Ambiguity: Understanding user intentions presents a significant challenge due to query ambiguity, often caused by vague or incomplete information and a lack of necessary external sources connected to openCHA for the specific application.</s><s xml:id="_YZBT2nV">To enhance response accuracy, our future work involves refining openCHA's ability to clarify user intentions.</s><s xml:id="_UJSy7wJ">Key strategies include employing targeted follow-up questions, improving comprehension of the user's situation, and the precision of responses.</s><s xml:id="_9y9nvW6">Scalability: We plan to address scalability issues within our system to ensure it can effectively manage increasing loads and accommodate more complex scenarios.</s><s xml:id="_tPDvMYc">This involves enhancing system architecture and resource allocation to support larger data volumes and more simultaneous users without compromising performance.</s><s xml:id="_2xF3K7c">We plan to investigate further how good the openCHA core scalability is in real-world scenarios.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_sw4jh9z">CONCLUSION</head><p xml:id="_3ZTv6Sg"><s xml:id="_P7RygxF">This paper presented openCHA, an LLM-powered framework designed to empower CHAs in effectively addressing healthcare-related queries through the analysis of input questions, data collection, action execution, and the delivery of personalized responses.</s><s xml:id="_Zxj8eXB">We demonstrated the framework's effectiveness using two different demos on general patient health record reporting, objective stress level estimation, and further use cases.</s><s xml:id="_jXNQ4AU">Additionally, we discussed the capabilities, limitations, and challenges of openCHA.</s><s xml:id="_PyRhWYT">Our future efforts will focus on improving planning robustness, examining accuracy and evaluation aspects, addressing user query ambiguity, and enhancing the scalability of openCHA.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KedET8a">APPENDIX 1 -SAMPLE TASK IMPLEMENTATION</head><p xml:id="_HesXMaT"><s xml:id="_GYmeew4">We encourage readers and openCHA users to review our documentation page ⋄ for the latest updates on openCHA.</s></p><p xml:id="_mJZPnaT"><s xml:id="_z3P7VZe">In this appendix, we will detail the curation of tasks within the openCHA framework and their introduction as external sources to the Task Planner.</s><s xml:id="_abdQHsd">Initially, we will describe the types of tasks that can be added to the framework.</s><s xml:id="_VqNDgnQ">Subsequently, we will outline the implementation process for tasks within the framework.</s><s xml:id="_buGP7yz">Finally, we will present an example of a task that has already been implemented, along with sample prompts associated with it.</s></p><p xml:id="_K3EjCWb"><s xml:id="_7QpkPR3">Tasks may be implemented locally or as a service.</s><s xml:id="_TEWEPJK">Local implementation entails coding all necessary components on-site to ensure successful task execution, which may involve providing a Python library, a GitHub repository, or a complete task implementation.</s><s xml:id="_V8fcB2f">Service-based implementation involves hosting services on a server and offering APIs for utilization.</s><s xml:id="_69m76Uf">In this scenario, the task should invoke these APIs using Python libraries.</s><s xml:id="_Kaka8Zs">It is incumbent upon the service provider to maintain the task and supply comprehensive documentation for users, including registration procedures, API key acquisition, and privacy policies.</s><s xml:id="_c87TGHa">Figure <ref type="figure" target="#fig_5">7</ref> displays an example of a task implementation in openCHA designed to perform a Google search and return the first search result URL.</s><s xml:id="_jNFQrju">Each task includes configurations to aid the Task Planner LLM in understanding when and how to use this task.</s><s xml:id="_DsqgqKF">The parameters are as follows: name Purpose: Uniquely identifies the task within the system, and this name will appear in the interface for task selection.</s></p><p xml:id="_Dtcr9vE"><s xml:id="_NEMUxn2">Convention: Generally in lowercase underscore_case, often derived from the task's functionality for better readability and maintainability.</s></p><p xml:id="_Hdy5Nq7"><s xml:id="_c3ZaZZq">Example: name = "google_search" chat_name:</s></p><p xml:id="_WCb47Xc"><s xml:id="_Gv6rcJe">Purpose: Used for referencing the task in user interfaces or chats, particularly for explainability.</s><s xml:id="_AaKhPxs">If a user inquires about the tasks used, these names will be displayed.</s></p><p xml:id="_VXvWAww"><s xml:id="_bHY37cp">Convention: Typically in CamelCase, should be descriptive yet concise.</s></p><p xml:id="_Jn89tMC"><s xml:id="_b6msAm7">Example: chat_name = "GoogleSearch" description:</s></p><p xml:id="_k7XP7q6"><s xml:id="_HZfYuUd">Purpose: Provides an explanation of the task's function.</s><s xml:id="_ehZ53X9">This information helps the Task Planner LLM decide when to deploy the task.</s></p><p xml:id="_fN4puMj"><s xml:id="_ZR483zK">Convention: The description should be comprehensive, outlining task capabilities, prioritization, or specific conditions for its use.</s><s xml:id="_kzEjYxu">For instance, ensuring the Task Planner prioritizes patient data analysis over frequent internet searches can be mentioned here with a low priority setting.</s></p><p xml:id="_kuKXbQ7"><s xml:id="_yxVsf7Z">Example: description = "Uses Google to search the internet for the requested query and returns the URL of the top website."</s><s xml:id="_Dr2JnFd">dependencies:</s></p><p xml:id="_4K3uVZr"><s xml:id="_gQNzfeQ">Purpose: Lists other tasks or services this task depends on, informing the Task Planner LLM of these dependencies during planning.</s></p><p xml:id="_FqKf282"><s xml:id="_KTTXSdf">Convention: An array of task identifiers that match the name attribute of dependent tasks.</s></p><p xml:id="_Ap987RY"><s xml:id="_7JHdYCr">Example: dependencies = [TaskType.FETCH_WEATHER_DATA] inputs:</s></p><p xml:id="_MMXz46G"><s xml:id="_ejSycuV">Purpose: Specifies the inputs required by the task.</s><s xml:id="_gSWnADf">This parameter is crucial as it guides the Task Planner LLM to provide the appropriate inputs for this task's "_execute" function.</s></p><p xml:id="_d9NfDQt"><s xml:id="_JcuEVyw">Convention: An array of strings, each describing a specific input.</s></p><p xml:id="_QC5Ntvp"><s xml:id="_2pvsEBw">Example: inputs = ["It should be a search query."]</s><s xml:id="_tUTrHQ5">outputs:</s></p><p xml:id="_JDJtbDf"><s xml:id="_TE7FeRr">Purpose: Details what the task returns, assisting the Task Planner LLM in understanding the return structure for proper handling of the output.</s></p><p xml:id="_PUbNaRY"><s xml:id="_xgFyh35">Convention: An array of strings describing each output, with a full description of the return data format.</s></p><p xml:id="_YdDEb79"><s xml:id="_2UfxVQX">Example: outputs = ["It returns a JSON object containing key: url.</s><s xml:id="_9WMQUM3">For example: 'url': '<ref type="url" target="http://google.com">http://google.com</ref>'"]</s><s xml:id="_EWjHbZf">output_type Purpose: Instructs the Orchestrator on how to handle the output of this task.</s><s xml:id="_TtnCmxd">If output_type is True, the data is stored in the Datapipe; otherwise, it is directly returned to the Task Planner LLM.</s><s xml:id="_fyj669C">This flexibility allows task designers to determine whether their task involves intermediate information for use by other tasks or provides the final answer.</s></p><p xml:id="_C3Tyzjv"><s xml:id="_xHBPq8J">Convention: Boolean values (True or False).</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_2nzpU8M">_execute function</head><p xml:id="_VwvGSgv"><s xml:id="_zdFWvDE">Purpose: This function must be implemented correctly.</s><s xml:id="_Sj4R8DU">If all other parameters are set accurately, developers can trust that the Task Planner LLM will invoke this function with the appropriate inputs.</s><s xml:id="_cMyUUNP">For instance, as shown in Figure <ref type="figure" target="#fig_5">7</ref>, the sole expected input is a search query.</s><s xml:id="_Xr6FBDa">The Task Planner LLM will correctly supply the search query as an input to the "_execute" function, accessible as the first element in the inputs array (since the only entry in the "inputs" configuration pertains to the search query, this array will contain just one item).</s><s xml:id="_rez7GN9">Utilizing the google_engine Python library, this function searches for the query online and returns the URL.</s><s xml:id="_d9sTX3u">Note that the result we are returning for the Google search task aligns with what is specified in the "outputs" parameter, demonstrating the coherence between the task setup and its execution.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_B65265K">APPENDIX 2 -TASK PLANNER SAMPLE IMPLEMENTATION</head><p xml:id="_zbnJACR"><s xml:id="_qmsHgqq">We encourage readers and openCHA users to review our documentation page ⋄ for the latest updates on openCHA.</s></p><p xml:id="_X46AUWP"><s xml:id="_uFSWxa9">In this appendix, we elaborate on the implementation of the Tree of Thought prompting technique.</s><s xml:id="_N5pH3sF">We describe how available tasks are introduced to the Tree of Thought, the process of curating and generating the planning prompt, and ultimately, how the final planning results are transformed into executable tasks by the Task Executor.</s></p><p xml:id="_NEFP6qv"><s xml:id="_MX3rjxS">The core component of the Task Planner is the selection of an appropriate planning technique.</s><s xml:id="_BdM89Np">Among various techniques using LLMs that have proven practical, we opted for the Tree of Thought approach.</s></p><p xml:id="_cvPZzs5"><s xml:id="_6CeC3cP">To effectively implement the Tree of Thought, we structured the planning process into two main sections.</s><s xml:id="_zrp2EPt">Initially, we prepare a comprehensive prompt that includes a list of available tasks, relevant metadata, records of previously performed tasks, conversation history, and user input.</s><s xml:id="_cvjYs8d">This setup ensures the planner is thoroughly informed about the tasks it can call upon, the metadata that should be used or passed along, and the context of past interactions to prevent redundant planning and maintain the continuity of conversations.</s></p><p xml:id="_sgwAfk2"><s xml:id="_yECM5uQ">Figure <ref type="figure" target="#fig_6">8</ref> illustrates the Tree of Thought planning prompt.</s><s xml:id="_nKF5jYu">In this phase, the Tree of Thought uses all this gathered information and the user's query to devise three distinct task sequences or strategies.</s><s xml:id="_WbmbyDB">Each strategy is designed to collect the necessary information to address the query efficiently.</s><s xml:id="_zJBaSrV">Next, it is asked to provide the pros and cons of each strategy, ultimately selecting the most suitable one as the final decision.</s></p><p xml:id="_4Pu7GfD"><s xml:id="_wNgEWmP">In the second stage of the planning process, our objective is to translate the chosen decision into sequences of task functions that the Orchestrator can understand and execute.</s><s xml:id="_Y8tvnVs">Within the Orchestrator, we have implemented a function named execute_task, which serves as an interface to retrieve and execute tasks with the appropriate inputs accurately.</s></p><p xml:id="_SUywjkw"><s xml:id="_UtTrMrK">Our second prompt instructs the LLM to invoke this execute_task function, ensuring that the correct inputs are provided for each task.</s><s xml:id="_WfTj36Y">Figure <ref type="figure">9</ref> displays this second prompt, showcasing how we structure these commands within the LLM to streamline task execution.</s><s xml:id="_8BU3vsm">Figure 10: The Response Generator prompt.</s></p><p xml:id="_fbGzQQp"><s xml:id="_8aGUjUg">feature allows developers to add information to better align the response generator with their specific task requirements.</s></p><p xml:id="_S7DQ8kE"><s xml:id="_VnufNP4">Table <ref type="table">3</ref>: Tree of Thought Planning first planning stage.</s><s xml:id="_4HwnQHS">As a knowledgeable and empathetic health assistant, your primary objective is to provide the user with precise and valuable information regarding their health and well-being.</s><s xml:id="_ZMu3RCb">Utilize the available tools effectively to answer health-related queries.</s><s xml:id="_sSKrtvM">Here are the tools at your disposal: ------------**google_search**: Uses google to search the internet the requested query and returns the url of the top website.</s><s xml:id="_9jzCKzu">This tool have the following outputs: It returns a json object containing key: **url**.</s><s xml:id="_hSQZB9h">For example: 'url': '<ref type="url" target="http://google.com">http://google.com</ref>'</s><s xml:id="_5uPaGEd">------------**extract_text**: Extract all the text on the current webpage This tool have the following outputs: An string containing the text of the scraped webpage.</s></p><p xml:id="_vRbhNna"><s xml:id="_FqpCS9Q">------------The following is the format of the information provided: MetaData: This contains the names of data files of different types, such as images, audio, video, and text.</s><s xml:id="_QETsErK">You can pass these files to tools when needed.</s><s xml:id="_pP9n7X4">History: The history of previous chats happened.</s><s xml:id="_QNv6CkB">Review the history of any previous responses relevant to the current query.</s><s xml:id="_jyNbWrH">PreviousActions: the list of actions that have already been performed.</s><s xml:id="_RtHpSWy">You should start planning, knowing that these actions are performed.</s><s xml:id="_kdEHAKP">Question: The input question that you must answer.</s><s xml:id="_SQNkEAF">Considering previously actions and their results, use the tools and provided information, first suggest three creative strategies with detailed explanation consisting of sequences of tools to properly answer the user query.</s><s xml:id="_RXxX8XA">Make sure the strategies are comprehensive enough and use proper tools.</s><s xml:id="_rxe8Knu">The tools constraints should be always satisfied.</s><s xml:id="_hTAHgkF">After specifying the strategies, mention the pros and cons of each strategy.</s><s xml:id="_FD6tZDS">In the end, decide the best strategy and write the detailed tool executions step by step.</s><s xml:id="_J3rX5He">start your final decision with 'Decision:'.</s><s xml:id="_fR8XKzu">Begin! ========================= USER: How to improve my sleep?</s><s xml:id="_ggDqpPT">CHA: ------------**google_search**: Uses google to search the internet for the requested query and returns the url of the top website.</s></p><p xml:id="_PgmxXty"><s xml:id="_vZCbfUk">The input to this tool should be a list of data representing: 1-It should be a search query.</s><s xml:id="_8n4gFa7">This tool will return the following data:</s></p><p xml:id="_ubpGYu9"><s xml:id="_K8vzqmd">-It returns a json object containing key: **url**.</s><s xml:id="_yMRXQMj">For example: 'url': '<ref type="url" target="http://google.com">http://google.com</ref>'</s><s xml:id="_weYTkuN">------------**extract_text**: Extract all the text on the current webpage The input to this tool should be a list of data representing: 1-url to extract the text from.</s><s xml:id="_QwaV4sY">It requires links which is gathered from other tools.</s><s xml:id="_cvGUUva">Never provide urls on your own.</s><s xml:id="_JqNDshv">This tool will return the following data:</s></p><p xml:id="_jwecum7"><s xml:id="_SeBsPEX">-An string containing the text of the scraped webpage.</s></p><p xml:id="_WHgYUka"><s xml:id="_EbREErV">------------</s></p><p xml:id="_bEBUvDb"><s xml:id="_NzSvax5">Table 5: Tree of Thought Planning second planning stage: generating the task execution code part 2. ========================= You are a skilled Python programmer who can solve problems and convert them into Python codes.</s><s xml:id="_8ARZwQ9">Using the selected final strategy mentioned in the 'Decision: ', create a python code inside a " 'python " ' block that outlines a sequence of steps using the Tools.</s><s xml:id="_4V9gvt8">Assume that there is a **self.execute_task**</s><s xml:id="_fb3zt9F">function that can execute the tools in it.</s><s xml:id="_8Ppp4dn">The execute_task receives the task name and an array of the inputs and returns the result.</s><s xml:id="_z5wtBnQ">Make sure that you always pass an array as a second argument.</s><s xml:id="_HmDHvKH">You can call tools like this: **task_result = self.execute_task('tool_name',</s><s xml:id="_wx8Q2H5">['input1', 'input2', ...])**.</s><s xml:id="_YHnXAPs">The flow should utilize this style to represent the tools available.</s><s xml:id="_w3gUgmK">Make sure all the execute_task calls outputs are stored in a variable.</s><s xml:id="_AaSgWYN">If a step's output is required as input for a subsequent step, ensure the Python code captures this dependency clearly.</s><s xml:id="_Y667kV4">The output variables should be directly passed as inputs with no changes in the wording.</s><s xml:id="_zUJbKa2">If the tool input is a datapipe, only put the variable as the input.</s><s xml:id="_rtgpHD8">For each tool, include necessary parameters directly without any names and assume each will return an output.</s><s xml:id="_hnTXkfX">The outputs' description are provided for each tool individually.</s><s xml:id="_N6ydhJk">Make sure you use the directives when passing the outputs.</s><s xml:id="_uhaVkky">Question: How to improve my sleep?</s><s xml:id="_hQ34ctq">Table 6: Sample generated code for task execution.</s><s xml:id="_zQg7cJA"># Step 1: Use google_search to find the top websites with tips to improve sleep.</s><s xml:id="_bbGaKsV">------========== System: .</s><s xml:id="_2JbYwNy">You are a very helpful, empathetic health assistant, and your goal is to help the user get accurate information about his/her health and well-being; using the Thinker gathered information and the History, Provide an empathetic, proper answer to the user.</s><s xml:id="_KWBYaPA">Consider Thinker as your trusted source, and use whatever it provides.</s><s xml:id="_CtQBuk6">Make sure that the answer is explanatory enough.</s><s xml:id="_nTRMJWK">Don't change Thinker returned URLs or references.</s><s xml:id="_59WZanD">Also, add explanations based on instructions from the Thinker.</s><s xml:id="_aq76vcX">Don't directly put the instructions in the final answer to the user.</s><s xml:id="_vjQTQxW">Never answer outside of the Thinker's provided information.</s><s xml:id="_WSDaUCx">Additionally, refrain from including or using any keys, such as 'datapipe:6d808840-1fbe-45a5-859a-abfbfee93d0e,' in your final response.</s><s xml:id="_CKuTeFU">Return all 'address:[path]' exactly as they are.User: How to improve my sleep?</s></p><p xml:id="_CdxrSzv"><s xml:id="_zAR97zM">Table <ref type="table">8</ref>: Task prompt with Data Pipe.</s></p><p xml:id="_2fXMGRs"><s xml:id="_pETfwH9">------------**affect_ppg_get**: Returns the ppg data for a specific patient over a date or a period (if two dates are provided).</s><s xml:id="_ExxDeGX">This will return the detailed raw data and store it in the Data Pipe.</s><s xml:id="_shnVVHB">The input to this tool should be a list of data representing: 1-user ID in string.</s><s xml:id="_pvevmbc">It can be referred to as user, patient, individual, etc. Start with 'par_' followed by a number (e.g., 'par_1').</s><s xml:id="_bKdXBXY">2-start date of the sleep data in a string with the following format: '%Y-%m-%d.'</s><s xml:id="_uTpVEne">3-end date of the sleep data in a string with the following format: '%Y-%m-%d.'</s><s xml:id="_C7JrBtS">If there is no end date, the value should be an empty string (i.e., ") This tool will return the following data:</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc><div><p xml:id="_TSu6pCe"><s xml:id="_5eCkGfY">Figure 2: An overview of the proposed LLM-powered framework leveraging a service-based architecture</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Demo 1 :Figure 3 :</head><label>13</label><figDesc><div><p xml:id="_5bwFby5"><s xml:id="_Fee3vrm">Figure 3: Overview of the implemented tasks and components and how they are used in the two demos.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc><div><p xml:id="_dndwwZA"><s xml:id="_4w3Tns4">Figure 4: Demo 1. Patient health record reporting and follow up.</s></p></div></figDesc><graphic coords="15,77.78,-16.08,454.92,598.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc><div><p xml:id="_TNcraAv"><s xml:id="_Mg2UvW9">Figure 5: Demo 2. Objective stress level estimation.</s><s xml:id="_XaMdpbT">The question is asked in English and Spanish.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc><div><p xml:id="_tYXqWkZ"><s xml:id="_wbtreZp">Figure 6: A example screenshot indicating how openCHA improves explainability in conversations.</s></p></div></figDesc><graphic coords="21,110.85,125.80,388.54,333.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc><div><p xml:id="_PGqfFEN"><s xml:id="_DPeYBVT">Figure 7: A sample implemented task inside openCHA framework.</s><s xml:id="_6t2CTQc">Each task includes prompt parameters which helps our Task Planner LLM understand what types of data are needed and how to provide them to the current task.</s></p></div></figDesc><graphic coords="28,110.85,47.89,388.54,562.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc><div><p xml:id="_CNCPetH"><s xml:id="_Gkpy4SB">Figure 8: The first stage Tree of Thought planning prompt</s></p></div></figDesc><graphic coords="32,110.85,146.23,388.52,216.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc><div><p xml:id="_ARfJ3M6"><s xml:id="_AHkdtE9">Figure 9: The second stage Tree of Thought planning prompt</s></p></div></figDesc><graphic coords="32,110.85,439.97,388.52,185.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><figDesc><div><p xml:id="_PwWnnf2"><s xml:id="_rhuBCet">search_query = "tips to improve sleep" search_result = self.execute_task('google_search',</s><s xml:id="_6SyY5TE">[search_query]) # Step 2: Use extract_text to extract the relevant information about improving sleep from the webpage.</s><s xml:id="_t3Cq93A">url = search_result['url'] sleep_tips_text = self.execute_task('extract_text',</s><s xml:id="_2Qb4QeP">[url])Table 7: Response generator sample prompt.</s><s xml:id="_gGNppun">===========Thinker: MetaData: History: ------google_search: ['tips to improve sleep'] 'url': 'https://www.mayoclinic.org/healthy-lifestyle/adult-health/indepth/sleep/art-20048379'------------extract_text:</s><s xml:id="_WWzcwCP">['https://www.mayoclinic.org/healthy-lifestyle/adulthealth/in-depth/sleep/art-20048379']</s><s xml:id="_g3KaUYy">Sleep tips: 6 steps to better sleep -Mayo Clinic This content does not have an English version.</s><s xml:id="_qkGWQRz">This content does not have an Arabic version.</s><s xml:id="_9VXGn9F">Skip to content Care at Mayo Clinic Patient-Centered Care About Mayo... (we cut the text to shorten the table)</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><figDesc><div><p xml:id="_CWpYCsg"><s xml:id="_s6exCmN">-returns an array of JSON objects which contains the following keys: **date (in milliseconds)**: epoch format **ppg**: is the ppg value.</s><s xml:id="_a4K3xH8">**hr (in beats per minute)**: is the heart rate of the patient.</s><s xml:id="_ddy56vg">The result will be stored in the Data Pipe.</s><s xml:id="_AF9f7vr">------------Table 9: Sample code generation when a task's result is stored in the Data Pipe.</s><s xml:id="_jYzgPk2"># Step 1: Get PPG data for patient 5 for the entire month of August 2020 ppg_data_result = self.execute_task('affect_ppg_get',</s><s xml:id="_6ZBUtMb">['par_5', '2020-08-01', '2020-08-31']) # Step 2: Analyze the HRV parameters from the obtained PPG data hrv_analysis_result = self.execute_task('affect_ppg_analysis',</s><s xml:id="_QgmYUEA">[ppg_data_result]) # Step 3: Estimate the stress level for patient 5 during August 2020 using the HRV analysis results stress_level_result = self.execute_task('affect_stress_analysis',</s><s xml:id="_5SKtHPY">[hrv_analysis_result])</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc><div><p xml:id="_Ucp7CWG"><s xml:id="_3NR5Czz">The strategies suggested by Tree of Thought prompting technique for the first question.</s></p></div></figDesc><table><row><cell>I will proceed to directly analyze the REM sleep data of</cell></row><row><cell>Patient 5 for August 2020, providing a precise and specific</cell></row><row><cell>conclusion.</cell></row><row><cell>Decision Execution:</cell></row><row><cell>1. Use the sleep_get tool to obtain the REM sleep data for</cell></row><row><cell>Patient 5 in August 2020.</cell></row><row><cell>2. Utilize the sleep_analysis tool to analyze the REM sleep</cell></row><row><cell>duration and efficiency for August 2020.</cell></row></table><note xml:id="_9dMb87D"><p><s xml:id="_YSTHCjH">translator, health data sources, and AI models.</s><s xml:id="_QtpgMKE">Examples of interactions include inquiries such as "Retrieve the stress level of Patient 5 on August 29th, 2020" and "What is the average heart rate of Patient 5 during August 2020?"</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc><div><p xml:id="_CyH37ga"><s xml:id="_8VV4dCj">The strategies suggested by Tree of Thought prompting technique for the first question.</s></p></div></figDesc><table><row><cell>The best strategy provides both detailed PPG analysis and an</cell></row><row><cell>estimation of the stress level, which offers a comprehensive view</cell></row><row><cell>of the patient's health status.</cell></row><row><cell>Execution:</cell></row><row><cell>Decision 1. Use ppg_get tool to retrieve the PPG data for Patient 5</cell></row><row><cell>during August 29th, 2020.</cell></row><row><cell>2. Analyze the PPG data with ppg_analysis tool to obtain the</cell></row><row><cell>heart rate.</cell></row><row><cell>3. Use stress_analysis to estimate the stress level based on the</cell></row><row><cell>obtained PPG data.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc><div><p><s xml:id="_HsXS24s">Tree of Thought Planning second planning stage: generating the task execution code part 1.Decision: I will go with Strategy 1 as it provides the most recent and relevant information on the internet, which is crucial for improving sleep.Now, let's proceed with the detailed tool executions for Strategy 1:1.Use the "google_search" tool to find the top websites with tips to improve sleep.2.</s><s xml:id="_Wetmma2">Once we have the top website, we can use the "extract_text" tool to extract the relevant information about improving sleep from the webpage.</s></p></div></figDesc><table><row><cell>Let's start with step 1.</cell></row><row><cell>=========================</cell></row><row><cell>Tools:</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_hyRNzFx"><s xml:id="_qBTeMqU">⋄ https://github.com/Institute4FutureHealth/CHA</s><s xml:id="_uF9htna">⋄ https://docs.opencha.com/</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p xml:id="_8T6YQSp"><s xml:id="_TR5aN8q">⋄ https://docs.opencha.com/index.html</s></p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_b2nnacE">APPENDIX 3 -RESPONSE GENERATOR IMPLEMENTATION</head><p xml:id="_4QEfmvr"><s xml:id="_DJYX5jK">We encourage readers and openCHA users to review our documentation page ⋄ for the latest updates on openCHA.</s></p><p xml:id="_DEVDnyB"><s xml:id="_kAquSRh">In this appendix, we provide a detailed explanation of the Response Generator's implementation.</s><s xml:id="_szVGv5a">We begin by distinguishing between the Response Generator and the Task Planner.</s><s xml:id="_TcsBZz6">Following this, we demonstrate how the outcomes of planning and task execution are delivered to the Response Generator.</s><s xml:id="_n6gdpPA">Lastly, we present the prompts employed to ensure that the Response Generator depends on the tasks' responses rather than the internal knowledge of the LLM used for generating responses.</s></p><p xml:id="_zFuBVSE"><s xml:id="_WXhnCmW">One of the key objectives of the Agentic approach is to create a comprehensive mega prompt that allows an LLM to generate more precise answers by incorporating all necessary information.</s><s xml:id="_SWPxKnM">Our innovation centers on dynamically generating this mega prompt in response to user queries.</s><s xml:id="_WuVgxSJ">The Task Planner initiates this process by determining the sequence of tasks it deems necessary to accurately address the current question.</s></p><p xml:id="_K6CJ96W"><s xml:id="_GAm89vT">Once planning is complete, the Orchestrator compiles the results of these tasks to create the mega prompt.</s><s xml:id="_rxu66JF">The composition of this prompt is influenced by the nature of the tasks and their outputs.</s><s xml:id="_Uh7y9Qz">For instance, if a task's output_type is set to true, indicating that the result should be stored in the Datapipe for subsequent analysis task use, the Orchestrator will omit this result from the prompt.</s><s xml:id="_KnMHP37">The focus in this case is on the analysis's outcome rather than intermediate results.</s><s xml:id="_dZpvRPM">Similarly, for tasks that produce metadata, such as an image, the metadata itself isn't passed directly to the Response Generator.</s><s xml:id="_eaPd7NX">Instead, descriptive information about the metadata, like details of a plot image and relevant data insights, are provided to the Response Generator, indicating that this information accompanies the image.</s></p><p xml:id="_55e82xw"><s xml:id="_t6HMzbM">Figure <ref type="figure">10</ref> illustrates the prompt provided to the Response Generator LLM.</s><s xml:id="_XXdzwBa">Initially, the Orchestrator formats answers derived from the task results, which we refer to as the "thinker" part of the prompt.</s><s xml:id="_mEsnTaw">The LLM is then instructed to rely exclusively on this thinker prompt, disregarding its internal knowledge.</s><s xml:id="_aJGb9jY">Additionally, we include guidelines to help the LLM understand specific system implementations, such as Datapipe access keys or metadata details.</s></p><p xml:id="_QbHDPHe"><s xml:id="_xeXKqEW">In the openCHA framework, this prompt is customizable.</s><s xml:id="_VBnfZ2h">A "prefix"</s></p><p xml:id="_r49nwBn"><s xml:id="_8SUScMS">⋄ <ref type="url" target="https://docs.opencha.com/index.html">https://docs.opencha.com/index.html</ref></s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_QSm2Kdk">APPENDIX 4 -SAMPLE PROMPTS</head><p xml:id="_5MN8FnK"><s xml:id="_UG7NfW2">In this Appendix, we demonstrate the generated prompt for a simple example involving two tasks: Internet search and webpage information extraction.</s><s xml:id="_dPtzBHP">The user's query, "How to improve my sleep," is first directed to the Task Planner.</s><s xml:id="_s5GeMpv">Table <ref type="table">3</ref> displays the initial planning stage where the Tree of Thought Planner is requested to develop three strategies, evaluate their pros and cons, and choose one to execute.</s><s xml:id="_EgkKCU6">Table <ref type="table">4</ref> presents the prompt for the second planning stage, where the LLM is instructed to generate Python code that the Orchestrator can execute based on the chosen strategy, along with the necessary inputs.</s><s xml:id="_CerhMJD">Table <ref type="table">5</ref> and 6 provides an example of the generated code, illustrating how the LLM correctly inputs the query "tips to improve sleep" for the google_search task.</s><s xml:id="_WcGBsha">Table <ref type="table">7</ref> shows how openCHA creates a prompt to assist the Response Generator in utilizing the tasks' results to respond to the user's query effectively.</s><s xml:id="_8MRXJCF">Additionally, Table <ref type="table">8</ref> provides an example of a prompt for a task that has multiple inputs and outputs, with the result being stored in the Data Pipe.</s><s xml:id="_Rre9BRy">Table <ref type="table">9</ref> illustrates the process of passing the Data Pipe key between tasks.</s><s xml:id="_BgUnmXm">In this setup, each task returns a Data Pipe key, and within the generated code, the output from one task is appropriately passed to the next task as its input.</s><s xml:id="_kKtbMhM">The execute_task function manages access to the Data Pipe when a key is provided, ensuring that the results of the tasks are not directly transmitted to the LLM.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_2dPyEvd">ChatGPT: OpenAI&apos;s Conversational AI Model</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://openai.com/chatgpt" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">OpenAI. ChatGPT: OpenAI&apos;s Conversational AI Model; Accessed: January 2024. https://openai.com/chatgpt.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main" xml:id="_A3UTwrZ">MedAlpaca-An Open-Source Collection of Medical Conversational AI Models and Training Data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oberhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Löser</surname></persName>
		</author>
		<idno>arXiv:230408247. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Han T, Adams LC, Papaioannou JM, Grundmann P, Oberhauser T, Löser A, et al. MedAlpaca-An Open-Source Collection of Medical Conversational AI Models and Training Data. arXiv preprint arXiv:230408247. 2023.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main" xml:id="_Mu4DxWU">Towards Generalist Biomedical AI</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Driess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schaekermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1056/aioa2300138</idno>
		<idno>arXiv:230714334. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Tu T, Azizi S, Driess D, Schaekermann M, Amin M, Chang PC, et al. Towards Generalist Biomedical AI. arXiv preprint arXiv:230714334. 2023.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_bNmZTqz">The effectiveness of artificial intelligence conversational agents in health care: systematic review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Milne-Ives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Shehadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mole</surname></persName>
		</author>
		<idno type="DOI">10.2196/20346</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Pa4rt3p">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">20346</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Milne-Ives M, de Cock C, Lim E, Shehadeh MH, de Pennington N, Mole G, et al. The effectiveness of artificial intelligence conversational agents in health care: systematic review. Journal of medical Internet research. 2020;22(10):e20346.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_FsfvNDM">The role of empathy in health and social care professionals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moudatsou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stavropoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Philalithis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koukouli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kqWGHyJ">Healthcare</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2020">2020</date>
			<publisher>MDPI</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Moudatsou M, Stavropoulou A, Philalithis A, Koukouli S. The role of empathy in health and social care professionals. In: Healthcare. vol. 8. MDPI; 2020. p. 26.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main" xml:id="_D9eJKT9">Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khatibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oniani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsh</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thieme</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41746-024-01074-z</idno>
		<idno>arXiv:230912444. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Abbasian M, Khatibi E, Azimi I, Oniani D, Abad ZSH, Thieme A, et al. Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI. arXiv preprint arXiv:230912444. 2023.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_jPhjJmA">Multichannel Mobile Companions for Personalized Healthcare Opportunities and Challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guimarães</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2023.03.112</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_g8zzaxX">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="838" to="843" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Moreira A, Quintas C, Guimarães T, Santos MF. Multichannel Mobile Companions for Personalized Healthcare Opportunities and Challenges. Procedia Computer Science. 2023;220:838-43.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main" xml:id="_yH4T4R4">How Are Consumers Using Generative AI?</title>
		<author>
			<persName><forename type="first">O</forename><surname>Moore</surname></persName>
		</author>
		<ptr target="https://a16z.com/how-are-consumers-using-generative-ai/" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Moore O. How Are Consumers Using Generative AI?; Accessed: January 2024. https: //a16z.com/how-are-consumers-using-generative-ai/.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_FU4XJUb">Investigating conversational agents in healthcare: Application of a technical-oriented taxonomy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Denecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5TgaxqV">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="page" from="1289" to="1296" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Denecke K, May R. Investigating conversational agents in healthcare: Application of a technical-oriented taxonomy. Procedia Computer Science. 2023;219:1289-96.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_VEyfwP2">The challenge of complexity in health care</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Plsek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Greenhalgh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6C5kQBC">Bmj</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">7313</biblScope>
			<biblScope unit="page" from="625" to="628" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Plsek PE, Greenhalgh T. The challenge of complexity in health care. Bmj. 2001;323(7313):625-8.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_DZue7UC">Complexity and health-yesterday&apos;s traditions, tomorrow&apos;s future</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Sturmberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pNndqXf">J Eval Clin Pract</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="543" to="548" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sturmberg JP, Martin CM. Complexity and health-yesterday&apos;s traditions, tomorrow&apos;s future. J Eval Clin Pract. 2009;15(3):543-8.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_8enQmxW">BioGPT: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.1093/bib/bbac409</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6Q3y7GQ">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">409</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luo R, Sun L, Xia Y, Qin T, Zhang S, Poon H, et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics. 2022;23(6):bbac409.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_8H57KyJ">ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_dRNuF8s">Cureus</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Li Y, Li Z, Zhang K, Dan R, Jiang S, Zhang Y. ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. Cureus. 2023;15(6).</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main" xml:id="_ubuCvZ9">FeelFit-Design and Evaluation of a Conversational Agent to Enhance Health Awareness</title>
		<author>
			<persName><forename type="first">P</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Beinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fitte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Behne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teuteberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICIS</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Meier P, Beinke JH, Fitte C, Behne A, Teuteberg F. FeelFit-Design and Evaluation of a Conversational Agent to Enhance Health Awareness. In: ICIS; 2019. .</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_cz49sVG">Survey of conversational agents in health</title>
		<author>
			<persName><forename type="first">Jlz</forename><surname>Montenegro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Da Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Righi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kvXFcjM">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Montenegro JLZ, da Costa CA, da Rosa Righi R. Survey of conversational agents in health. Expert Systems with Applications. 2019;129:56-67.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_WnjFpbK">Conversational agents in health care: scoping review and conceptual analysis</title>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Car</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dhinagaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Kowatsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Theng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XZ7kAK3">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">17158</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tudor Car L, Dhinagaran DA, Kyaw BM, Kowatsch T, Joty S, Theng YL, et al. Conversational agents in health care: scoping review and conceptual analysis. Journal of medical Internet research. 2020;22(8):e17158.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_7Z2c635">Conversational agents in healthcare: a systematic review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Laranjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Kocaballi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bashir</surname></persName>
		</author>
		<idno type="DOI">10.1093/jamia/ocy072</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bdyqCmS">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1248" to="1258" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Laranjo L, Dunn AG, Tong HL, Kocaballi AB, Chen J, Bashir R, et al. Conversational agents in healthcare: a systematic review. Journal of the American Medical Informatics Association. 2018;25(9):1248-58.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main" xml:id="_DdMfAEn">A survey of large language models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<idno>arXiv:230318223. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, et al. A survey of large language models. arXiv preprint arXiv:230318223. 2023.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_ZZhr2UU">Randomized Controlled Trials Evaluating AI in Clinical Practice: A Scoping Evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Topol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="DOI">10.1101/2023.09.12.23295381</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bkbb7GE">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2029" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Han R, Acosta JN, Shakeri Z, Ioannidis J, Topol E, Rajpurkar P. Randomized Con- trolled Trials Evaluating AI in Clinical Practice: A Scoping Evaluation. medRxiv. 2023:2023-09.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main" xml:id="_ZDZHmxY">ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns</title>
		<author>
			<persName><forename type="first">M;</forename><surname>Sallam</surname></persName>
		</author>
		<author>
			<persName><surname>Mdpi</surname></persName>
		</author>
		<idno type="DOI">10.3390/healthcare11060887</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YhmwErK">Healthcare</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">887</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sallam M; MDPI. ChatGPT utility in healthcare education, research, and prac- tice: systematic review on the promising perspectives and valid concerns. Healthcare. 2023;11(6):887.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main" xml:id="_EKJ7B3K">ChatGPT and the future of medical writing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Radiological Society of North America</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Biswas S. ChatGPT and the future of medical writing. Radiological Society of North America; 2023.</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main" xml:id="_MdbpBvu">OpenAI ChatGPT generated literature review: Digital twin in healthcare</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Aydın</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Karaarslan</surname></persName>
		</author>
		<idno>SSRN 4308687</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Aydın Ö, Karaarslan E. OpenAI ChatGPT generated literature review: Digital twin in healthcare. Available at SSRN 4308687. 2022.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_86JpGQB">The utility of ChatGPT for cancer treatment information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Aerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Savova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Mak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wJJx3Ed">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2023" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chen S, Kann BH, Foote MB, Aerts HJ, Savova GK, Mak RH, et al. The utility of ChatGPT for cancer treatment information. medRxiv. 2023:2023-03.</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_By7Zdwr">BioSignal Copilot: Leveraging the power of LLMs in drafting reports for biomedical signals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kothur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nikpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kavehei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TU4AvXn">medRxiv</title>
		<imprint>
			<biblScope unit="page" from="2023" to="2026" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liu C, Ma Y, Kothur K, Nikpour A, Kavehei O. BioSignal Copilot: Leveraging the power of LLMs in drafting reports for biomedical signals. medRxiv. 2023:2023-06.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main" xml:id="_GfPwrfY">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CvWyJ7T">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics. 2020;36(4):1234-40.</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_dwUMKW8">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3458754</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZctjxxX">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH). 2021;3(1):1-23.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mcneal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Washington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv:220308410. 2022</idno>
		<title level="m" xml:id="_EwgnDz4">Thinking about gpt-3 in-context learning for biomedical ie? think again</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Gutierrez BJ, McNeal N, Washington C, Chen Y, Li L, Sun H, et al. Think- ing about gpt-3 in-context learning for biomedical ie? think again. arXiv preprint arXiv:220308410. 2022.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main" xml:id="_tmpYcbv">Gpt-3 models are poor few-shot learners in the biomedical domain</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Blagec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Haberl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samwald</surname></persName>
		</author>
		<idno>arXiv:210902555. 2021</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Moradi M, Blagec K, Haberl F, Samwald M. Gpt-3 models are poor few-shot learners in the biomedical domain. arXiv preprint arXiv:210902555. 2021.</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main" xml:id="_23k8nRC">ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sieniek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<idno>arXiv:230801317. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Xu S, Yang L, Kelly C, Sieniek M, Kohlberger T, Ma M, et al. ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders. arXiv preprint arXiv:230801317. 2023.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main" xml:id="_zb56K5k">Multimodal LLMs for health grounded in individual-specific data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Belyaeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hormozdiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Mclean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Furlotte</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-47679-2_7</idno>
		<idno>arXiv:230709018. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Belyaeva A, Cosentino J, Hormozdiari F, McLean CY, Furlotte NA. Multimodal LLMs for health grounded in individual-specific data. arXiv preprint arXiv:230709018. 2023.</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main" xml:id="_rdUBEkm">Hallucinations in large multilingual translation models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Guerreiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Waldendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Colombo</surname></persName>
		</author>
		<idno>arXiv:230316104. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Guerreiro NM, Alves D, Waldendorf J, Haddow B, Birch A, Colombo P, et al. Hal- lucinations in large multilingual translation models. arXiv preprint arXiv:230316104. 2023.</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_UeMtFET">anticipating, and imagining</title>
		<author>
			<persName><forename type="first">U</forename><surname>Neisser</surname></persName>
		</author>
		<author>
			<persName><surname>Perceiving</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_DTS4KV2">Perception and Cognition: Issues in the Foundations of Psychology</title>
		<meeting><address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>University of Minnesota Press</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Neisser U. Perceiving, anticipating, and imagining. In: Perception and Cognition: Issues in the Foundations of Psychology. University of Minnesota Press, Minneapolis; 1978. .</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main" xml:id="_Rynkwsu">Tree of thoughts: Deliberate problem solving with large language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno>arXiv:230510601. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yao S, Yu D, Zhao J, Shafran I, Griffiths TL, Cao Y, et al. Tree of thoughts: De- liberate problem solving with large language models. arXiv preprint arXiv:230510601. 2023.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main" xml:id="_sqQ7XGS">React: Synergizing reasoning and acting in language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<idno>arXiv:221003629. 2022</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, et al. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:221003629. 2022.</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main" xml:id="_XKDZNFC">Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rkw</forename><surname>Lee</surname></persName>
		</author>
		<idno>arXiv:230504091. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Wang L, Xu W, Lan Y, Hu Z, Lan Y, Lee RKW, et al. Plan-and-solve prompt- ing: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:230504091. 2023.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main" xml:id="_s7cHQps">Google Translate</title>
		<ptr target="https://py-googletrans.readthedocs.io/en/latest/" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
		<respStmt>
			<orgName>Google</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Google. Google Translate; Accessed: January 2024. https://py-googletrans. readthedocs.io/en/latest/.</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main" xml:id="_bPsqCrG">LLM-Rec: Personalized Recommendation via Prompting Large Language Models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2024.findings-naacl.39</idno>
		<idno>arXiv:230715780. 2023</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Lyu H, Jiang S, Zeng H, Xia Y, Luo J. LLM-Rec: Personalized Recommendation via Prompting Large Language Models. arXiv preprint arXiv:230715780. 2023.</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main" xml:id="_XpkqwSw">Optimizing prompts for text-to-image generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename></persName>
		</author>
		<idno>arXiv:221209611. 2022</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Hao Y, Chi Z, Dong L, Wei F. Optimizing prompts for text-to-image generation. arXiv preprint arXiv:221209611. 2022.</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main" xml:id="_MPHBwBJ">ChatGPT: Prompt engineering instrauctions</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<ptr target="https://platform.openai.com/docs/guides/prompt-engineering/strategy-write-clear-instructions" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">OpenAI. ChatGPT: Prompt engineering instrauctions.; Accessed: January 2024. https://platform.openai.com/docs/guides/prompt-engineering/ strategy-write-clear-instructions.</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main" xml:id="_RXYnyEs">ZotCare: a flexible, personalizable, and affordable mhealth service provider</title>
		<author>
			<persName><forename type="first">S</forename><surname>Labbaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rahmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Sm5kSCZ">Frontiers in Digital Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Labbaf S, Abbasian M, Azimi I, Dutt N, Rahmani AM. ZotCare: a flexible, person- alizable, and affordable mhealth service provider. Frontiers in Digital Health. 2023;5.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main" xml:id="_FBTNz79">A Survey on Trend, Opportunities and Challenges of mHealth Apps</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jusoh</surname></persName>
		</author>
		<idno type="DOI">10.3991/ijim.v11i6.7265</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rqNTaww">Int J Interact Mob Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="73" to="85" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jusoh S. A Survey on Trend, Opportunities and Challenges of mHealth Apps. Int J Interact Mob Technol. 2017;11(6):73-85.</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main" xml:id="_Yq7S4Nn">mHealth technologies for chronic diseases and elders: a systematic review</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chiarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Masella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_AC4t8Kz">IEEE Journal on Selected Areas in Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6" to="18" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Chiarini G, Ray P, Akter S, Masella C, Ganz A. mHealth technologies for chronic diseases and elders: a systematic review. IEEE Journal on Selected Areas in Commu- nications. 2013;31(9):6-18.</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main" xml:id="_XbcT8bN">ZotCare: A Flexible, Personalizable, and Affordable mHealth Service Provider</title>
		<author>
			<persName><forename type="first">S</forename><surname>Labbaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahmani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_DtzmFcq">Frontiers in Digital Health</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1253087</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Labbaf S, Abbasian M, Azimi I, Dutt N, Rahmani A. ZotCare: A Flexible, Personaliz- able, and Affordable mHealth Service Provider. Frontiers in Digital Health;5:1253087.</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ilumivu</surname></persName>
		</author>
		<author>
			<persName><surname>Ilumivu</surname></persName>
		</author>
		<ptr target="https://ilumivu.com/" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">ilumivu. ilumivu; Accessed: January 2024. https://ilumivu.com/.</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carignan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Edgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fusi</surname></persName>
		</author>
		<idno>arXiv:231116452. 2023</idno>
		<title level="m" xml:id="_sYvgq6c">Can generalist foundation models outcompete special-purpose tuning? case study in medicine</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Nori H, Lee YT, Zhang S, Carignan D, Edgar R, Fusi N, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:231116452. 2023.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main" xml:id="_ptkw6Eg">Information Retrieval: searching in the 21st Century</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470033647.ch1</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
	<note>Information retrieval models</note>
	<note type="raw_reference">Hiemstra D. Information retrieval models. Information Retrieval: searching in the 21st Century. 2009:1-19.</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main" xml:id="_sucfx4B">A comparison of text retrieval models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WXTwghG">The computer journal</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Turtle HR, Croft WB. A comparison of text retrieval models. The computer journal. 1992;35(3):279-90.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main" xml:id="_KK5vwY2">Can generalist foundation models outcompete specialpurpose tuning? Case study in medicine</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv. 2023</idno>
		<imprint/>
	</monogr>
	<note type="raw_reference">Nori H, Lee Y, Zhang S, et al.. Can generalist foundation models outcompete special- purpose tuning? Case study in medicine. arXiv. 2023;.</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main" xml:id="_yNMwFDV">As artificial intelligence goes multimodal, medical applications multiply</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.adk6139</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VdVApTA">Science</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<date type="published" when="2023">2023. 6663</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Topol EJ. As artificial intelligence goes multimodal, medical applications multiply. Science. 2023;381(6663).</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main" xml:id="_aAFrnUP">Health information needs, sources, and barriers of primary care patients to achieve patient-centered care: A literature review</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Steege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Koopman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Belden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Canfield</surname></persName>
		</author>
		<idno type="DOI">10.1177/1460458215602939</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_skxbwMN">Health informatics journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="992" to="1016" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Clarke MA, Moore JL, Steege LM, Koopman RJ, Belden JL, Canfield SM, et al. Health information needs, sources, and barriers of primary care patients to achieve patient-centered care: A literature review. Health informatics journal. 2016;22(4):992- 1016.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main" xml:id="_VwQFjKH">Folk computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1145/641205.641223</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YgDWvw6">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="27" to="29" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jain R. Folk computing. Communications of the ACM. 2003;46(4):27-9.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Borovoy</surname></persName>
		</author>
		<idno type="DOI">10.35483/acsa.am.110.71</idno>
		<title level="m" xml:id="_tJCDdMV">Folk computing: Designing technology to support face-to-face community building</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note type="raw_reference">Borovoy RD. Folk computing: Designing technology to support face-to-face commu- nity building [Ph.D. thesis]. Massachusetts Institute of Technology; 2002.</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main" xml:id="_Rq3XYbp">Physiological and Emotional Assessment of College Students Using Wearable and Mobile Devices During the 2020 Covid-19 Lockdown: An Intensive, Longitudinal Dataset</title>
		<author>
			<persName><forename type="first">S</forename><surname>Labbaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yunusova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<publisher>Longitudinal Dataset</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Labbaf S, Abbasian M, Nguyen B, Lucero M, Ahmed MS, Yunusova A, et al. Phys- iological and Emotional Assessment of College Students Using Wearable and Mobile Devices During the 2020 Covid-19 Lockdown: An Intensive, Longitudinal Dataset. Longitudinal Dataset. 2023.</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main" xml:id="_MxRdykg">Samsung Gear Watch</title>
		<author>
			<persName><surname>Samsung</surname></persName>
		</author>
		<ptr target="https://www.samsung.com/us/mobile/wearables/smartwatches/gear-sport-blue-sm-r600nzbaxar/" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Samsung. Samsung Gear Watch; Accessed: January 2024. https://www.samsung. com/us/mobile/wearables/smartwatches/gear-sport-blue-sm-r600nzbaxar/.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main" xml:id="_MNXUesF">Neu-roKit2: A Python toolbox for neurophysiological signal processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Brammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lespinasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-020-01516-y</idno>
		<ptr target="https://doi.org/10.3758%2Fs13428-020-01516-y" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_x7BqzH5">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1689" to="1696" />
			<date type="published" when="2021">2021 feb</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Makowski D, Pham T, Lau ZJ, Brammer JC, Lespinasse F, Pham H, et al. Neu- roKit2: A Python toolbox for neurophysiological signal processing. Behavior Re- search Methods. 2021 feb;53(4):1689-96. Available from: https://doi.org/10.3758% 2Fs13428-020-01516-y.</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main" xml:id="_92Q2yv8">An overview of heart rate variability metrics and norms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Ginsberg</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpubh.2017.00258</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5Vm795q">Frontiers in public health</title>
		<imprint>
			<biblScope unit="page">258</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shaffer F, Ginsberg JP. An overview of heart rate variability metrics and norms. Frontiers in public health. 2017:258.</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main" xml:id="_kmw7ZHy">Impact of COVID-19 Pandemic on Sleep Including HRV and Physical Activity as Mediators: A Causal ML Approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Khatibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Labbaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Borelli</surname></persName>
		</author>
		<idno type="DOI">10.1109/bsn58485.2023.10331423</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mzSVYEC">IEEE 19th International Conference on Body Sensor Networks (BSN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note type="raw_reference">Khatibi E, Abbasian M, Azimi I, Labbaf S, Feli M, Borelli J, et al. Impact of COVID- 19 Pandemic on Sleep Including HRV and Physical Activity as Mediators: A Causal ML Approach. In: 2023 IEEE 19th International Conference on Body Sensor Networks (BSN). IEEE; 2023. p. 1-4.</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main" xml:id="_TW79j7F">ChatDiet: Empowering personalized nutrition-oriented food recommender chatbots through an LLMaugmented framework</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khatibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_53abhgu">Smart Health</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">100465</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yang Z, Khatibi E, Nagesh N, Abbasian M, Azimi I, Jain R, et al. ChatDiet: Em- powering personalized nutrition-oriented food recommender chatbots through an LLM- augmented framework. Smart Health. 2024;32:100465.</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main" xml:id="_E4r2dzE">Knowledge-Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Patients</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khatibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<idno>arXiv:240210153. 2024</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Abbasian M, Yang Z, Khatibi E, Zhang P, Nagesh N, Azimi I, et al. Knowledge- Infused LLM-Powered Conversational Health Agent: A Case Study for Diabetes Pa- tients. arXiv preprint arXiv:240210153. 2024.</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main" xml:id="_ZfZs8xV">Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bounds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<idno>arXiv:240804650. 2024</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Park JI, Abbasian M, Azimi I, Bounds D, Jun A, Han J, et al. Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based Evaluation Tools. arXiv preprint arXiv:240804650. 2024.</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main" xml:id="_TFVPNfW">Empathy Through Multimodality in Conversational Interfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<idno>arXiv:240504777. 2024</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Abbasian M, Azimi I, Feli M, Rahmani AM, Jain R. Empathy Through Multimodality in Conversational Interfaces. arXiv preprint arXiv:240504777. 2024.</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main" xml:id="_aHAQuFM">An LLM-Tool Compiler for Fused Parallel Function Calling</title>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stamoulis</surname></persName>
		</author>
		<idno>arXiv:240517438. 2024</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Singh S, Karatzas A, Fore M, Anagnostopoulos I, Stamoulis D. An LLM-Tool Com- piler for Fused Parallel Function Calling. arXiv preprint arXiv:240517438. 2024.</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main" xml:id="_m3kSsMv">A next-generation AI assistant for your tasks</title>
		<author>
			<persName><forename type="first">Claude</forename><surname>Claude</surname></persName>
		</author>
		<ptr target="https://www.anthropic.com/product" />
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
	<note>no matter the scale</note>
	<note type="raw_reference">Claude. Claude: A next-generation AI assistant for your tasks, no matter the scale; Accessed: January 2024. https://www.anthropic.com/product.</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main" xml:id="_kf6AkVx">An exhaustive survey on security and privacy issues in Healthcare 4</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hathaliya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tanwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4UBhYta">Computer Communications</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="311" to="335" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hathaliya JJ, Tanwar S. An exhaustive survey on security and privacy issues in Healthcare 4.0. Computer Communications. 2020;153:311-35.</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main" xml:id="_Wcg6snR">Privacy in the age of medical big data</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KgZX94w">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="43" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Price WN, Cohen IG. Privacy in the age of medical big data. Nature medicine. 2019;25(1):37-43.</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main" xml:id="_c8GyZhj">A survey of privacy preserving data publishing using generalization and suppression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.12785/amis/080321</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_uF6RQmk">Applied Mathematics &amp; Information Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1103</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xu Y, Ma T, Tang M, Tian W. A survey of privacy preserving data publishing using generalization and suppression. Applied Mathematics &amp; Information Sciences. 2014;8(3):1103.</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main" xml:id="_gJSmW75">Self-consistency improves chain of thought reasoning in language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">E</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno>arXiv:220311171. 2022</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, et al. Self-consistency im- proves chain of thought reasoning in language models. arXiv preprint arXiv:220311171. 2022.</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main" xml:id="_XMqzJ3C">Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abbasian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khatibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oniani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakeri</forename><surname>Hossein Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Thieme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Sx6zfra">NPJ Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">82</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Abbasian M, Khatibi E, Azimi I, Oniani D, Shakeri Hossein Abad Z, Thieme A, et al. Foundation metrics for evaluating effectiveness of healthcare conversations powered by generative AI. NPJ Digital Medicine. 2024;7(1):82.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
