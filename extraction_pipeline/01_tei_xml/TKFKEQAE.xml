<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_p29tYTv">Incorporating causal factors into reinforcement learning for dynamic treatment regimes in HIV</title>
				<funder ref="#_Uejj4Vs #_mB3Wh2N #_qh9cRtR #_3KzQFa5">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">© The Author(s). 2019 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.</p>
				</availability>
				<date type="published" when="2018-12-02">2 December 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chao</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>1</label> School of Computer Science and Technology , Dalian University of Technology , No. 2 , Linggong Road , 116024 Dalian , China</note>
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>No. 2 Linggong Road</addrLine>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>2</label> Department of Computer Science , Hong Kong Baptist University , Kowloon Tong , Hong Kong , China</note>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<addrLine>Kowloon Tong</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinzhao</forename><surname>Dong</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>1</label> School of Computer Science and Technology , Dalian University of Technology , No. 2 , Linggong Road , 116024 Dalian , China</note>
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>No. 2 Linggong Road</addrLine>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiming</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>2</label> Department of Computer Science , Hong Kong Baptist University , Kowloon Tong , Hong Kong , China</note>
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<addrLine>Kowloon Tong</addrLine>
									<settlement>Hong Kong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoqi</forename><surname>Ren</surname></persName>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>1</label> School of Computer Science and Technology , Dalian University of Technology , No. 2 , Linggong Road , 116024 Dalian , China</note>
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<addrLine>No. 2 Linggong Road</addrLine>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">†</forename><surname>Chao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>4th</label> From China Health Information Processing Conference Shenzhen , China.</note>
								<orgName type="institution">Health Information Processing Conference</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country>China China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_qethxeu">Incorporating causal factors into reinforcement learning for dynamic treatment regimes in HIV</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-02">2 December 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">1F8D77FB04BA26C60E15156F647D56A0</idno>
					<idno type="DOI">10.1186/s12911-019-0755-6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T12:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_BpBu2VB">Reinforcement learning</term>
					<term xml:id="_WERFgcn">Dynamic treatment regime</term>
					<term xml:id="_ta2XxsM">HIV</term>
					<term xml:id="_xBG9uYB">Causal factors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_CaQbK78"><p xml:id="_RyhYDh7"><s xml:id="_g5Bu5RA">Background: Reinforcement learning (RL) provides a promising technique to solve complex sequential decision making problems in health care domains.</s><s xml:id="_6g37weM">However, existing studies simply apply naive RL algorithms in discovering optimal treatment strategies for a targeted problem.</s><s xml:id="_RfbtQuP">This kind of direct applications ignores the abundant causal relationships between treatment options and the associated outcomes that are inherent in medical domains.</s><s xml:id="_rNvgF9f">Methods: This paper investigates how to integrate causal factors into an RL process in order to facilitate the final learning performance and increase explanations of learned strategies.</s><s xml:id="_tEs7Ban">A causal policy gradient algorithm is proposed and evaluated in dynamic treatment regimes (DTRs) for HIV based on a simulated computational model.</s><s xml:id="_EQ7MKpp">Results: Simulations prove the effectiveness of the proposed algorithm for designing more efficient treatment protocols in HIV, and different definitions of the causal factors could have significant influence on the final learning performance, indicating the necessity of human prior knowledge on defining a suitable causal relationships for a given problem.</s><s xml:id="_FtrWC2v">Conclusions: More efficient and robust DTRs for HIV can be derived through incorporation of causal factors between options of anti-HIV drugs and the associated treatment outcomes.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="595.0" lry="791.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_en2Bxve">Background</head><p xml:id="_YcJFg7G"><s xml:id="_ESEg2xw">Reinforcement learning (RL) <ref type="bibr" target="#b0">[1]</ref> has achieved tremendous achievements in solving complex sequential decision making problems in various health care domains, such as treatment in HIV <ref type="bibr" target="#b1">[2]</ref>, cancer <ref type="bibr" target="#b2">[3]</ref>, diabetics <ref type="bibr" target="#b3">[4]</ref>, schizophrenia <ref type="bibr" target="#b4">[5]</ref>, and sepsis <ref type="bibr" target="#b5">[6]</ref>.</s><s xml:id="_523gPzJ">In such typical RL implementations, a model designer normally formulates the learning components (the objective, state, action and reward etc.), specifies the presentation and efficiency techniques, and then simply lets the RL algorithms run until a satisfactory solution is obtained.</s><s xml:id="_8wJVsPK">Such fully automated and blackbox learning processes ignore rich knowledge encoded in causal relationships between variables like duration, dose or type of treatments, and the corresponding therapeutic outcomes.</s><s xml:id="_KAAw9zu">Thus, the learned policies may not be interpretable enough to explain why some policies are helpful while others are not <ref type="bibr" target="#b6">[7]</ref>.</s></p><p xml:id="_vAddepN"><s xml:id="_3wypgu2">Discovering effective treatment strategies for HIVinfected individuals remains one of the most significant challenges in medical research.</s><s xml:id="_jNT88gj">To date, the effective way to treat HIV makes use of a combination of anti-HIV drugs (i.e., antiretrovirals) in the form of Highly Active Antiretroviral Therapy (HAART) to inhibit the development of drug-resistant HIV strains <ref type="bibr" target="#b7">[8]</ref>.</s><s xml:id="_kQW4FMs">Patients suffering from HIV are typically prescribed a series of treatments over time in order to maximize the long-term positive outcomes of reducing patients' treatment burden and improving adherence to medication.</s><s xml:id="_xFTwY2k">However, due to the differences between individuals in their immune responses to treatment at a particular time, discovering the optimal drug combinations and scheduling strategy is a difficult task in both medical research and clinical trials.</s><s xml:id="_JpseHZF">In this paper, we propose a causal policy gradient (CPG) algorithm that is able of incorporating causal factors into an RL process in order to facilitate the final learning performance and increase explanations of learned strategies.</s><s xml:id="_DbqJPhc">We illustrate how CPG can be applied to solve DTRs problems in HIV.</s><s xml:id="_ubQ8nw3">Experiments prove the effectiveness of CPG in designing more efficient and robust treatment protocols in HIV.</s><s xml:id="_69vpax8">The remaining paper is organized as follows.</s><s xml:id="_qR9VVjb">We first discuss some related work and introduce the main principle of CPG algorithm.</s><s xml:id="_8gmSKZJ">We then provide the details of implementation of CPG in HIV treatment.</s><s xml:id="_rbkkxTr">Finally, we conclude the paper by pointing out some directions for future work.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_xZHeg2M">Related work</head><p xml:id="_P9FZEEg"><s xml:id="_SWxY6hx">RL has been applied to DTRs in HIV by several studies.</s><s xml:id="_7nZZXhW">Ernst et al. <ref type="bibr" target="#b8">[9]</ref> first introduced RL techniques in computing Structured Treatment Interruption (STI) strategies for HIV infected patients.</s><s xml:id="_fnRf4gs">Using a mathematical model <ref type="bibr" target="#b7">[8]</ref> to artificially generate the clinical data, a batch RL method, i.e., fitted Q iteration (FQI) with extremely randomized trees, was applied to learn an optimal drug prescription strategy in an off-line manner.</s><s xml:id="_s8nuaJA">The derived STI strategy is featured with a cycling between the two main anti-HIV drugs: Reverse Transcriptase Inhibitors (RTI) and Protease Inhibitors (PI).</s><s xml:id="_Zhx7xgR">Using the same mathematical model, Parbhoo <ref type="bibr" target="#b9">[10]</ref> further implemented three kinds of batch RL methods, FQI with extremely randomized trees, neural FQI and least square policy iterations (LSPI), to the problem of drug scheduling and HIV treatment design.</s><s xml:id="_WDz7WqG">Results indicated that each learning technique had its own advantages and disadvantages.</s><s xml:id="_WqhWYD5">Moreover, a testing based on a ten-year period of real clinical data from 250 HIV-infected patients in Charlotte Maxeke Johannesburg Academic Hospital, South Africa, verified that the RL methods were capable of suggesting treatments that are reasonably compliant with those suggested by clinicians.</s></p><p xml:id="_82nmdXR"><s xml:id="_hk7XRuY">The authors in <ref type="bibr" target="#b10">[11]</ref> used the Q-learning algorithm in HIV treatment and obtained a good performance and high functionality in controlling the free virions for both certain and uncertain HIV models.</s><s xml:id="_nw5Gcah">A mixture-of-experts approach was proposed in <ref type="bibr" target="#b1">[2]</ref> to combine the strengths of both kernel-based regression methods (i.e., historyalignment model) and RL (i.e., model-based Bayesian POMDP model) for HIV therapy selection.</s><s xml:id="_w7rSPe3">Making use of a subset of the EuResist database consisting of HIV genotype and treatment response data for 32,960 patients, together with the 312 most common drug combinations in the cohort, the treatment therapy derived by the mixtureof-experts approach outperform those derived by using each method alone.</s><s xml:id="_wyr5Zw2">Marivate et al. <ref type="bibr" target="#b11">[12]</ref> formalized a routine to accommodate multiple sources of uncertainty in batch RL methods to better evaluate the effectiveness of treatments across subpopulations of HIV patients.</s><s xml:id="_N2qUEvC">Killian et al. <ref type="bibr" target="#b12">[13]</ref> similarly attempt to address and identify the variations across subpopulations in the development of HIV treatment policies by transferring knowledge between task instances.</s></p><p xml:id="_CTf6Yu7"><s xml:id="_fFaXbzX">Unlike the above studies that mainly focus on valuebased RL for developing treatment policies in HIV, we are the first to evaluate policy gradient RL methods in such problems.</s><s xml:id="_xN9XJqJ">Moreover, in this paper, we aim at modeling causal relationships between the options of anti-HIV drugs and the associated treatment effect, and introducing such causal factors into policy gradient learning process, in order to facilitate the final learning process and increase its interpretation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_W634GHN">Methods</head><p xml:id="_ZdwWBsw"><s xml:id="_dUun6h8">In this section, we first provide basic introduction to RL and particularly the policy gradient RL, and then present the main procedure of the proposed causal policy gradient algorithm.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_59nfDpf">Policy gradient RL</head><p xml:id="_g7sW54C"><s xml:id="_dVuKKkZ">RL enables an agent to learn effective strategies in sequential decision making problems by trial-and-error interactions with its environment <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_cEFmBeF">The Markov decision process (MDP) has been used to formalize an RL problem, which has a long history in the research of theoretic decision making in stochastic settings.</s><s xml:id="_ZeUkvMy">Formally, an MDP can be defined by a 5-tuple M = (S, A, P, R, γ ), where S is a finite state space, and s t ∈ S denotes the state of the agent at time t; A is a set of actions available to the agent, and a t ∈ A denotes the action that the agent performs at time t; P(s, a, s ) : S × A × S →[ 0, 1] is a Markovian transition function when the agent transits from state s to state s after taking action a; R : S × A → is a reward function that returns the immediate reward R(s, a) to the agent after taking action a in state s; and γ ∈ [0, 1] is a discount factor.</s><s xml:id="_cjxTJXy">An agent's policy π : S × A →[ 0, 1] is a probability distribution that maps an action a ∈ A to a state s ∈ S. When given an MDP and a policy π, the expected reward of following this policy when starting in state s, V π (s), can be defined as follows:</s></p><formula xml:id="formula_0">V π (s) = E π ∞ t=0 γ t R(s t , π(s t ))|s 0 = s (1)</formula><p xml:id="_QsvVUh6"><s xml:id="_fJnjSzV">and can also be defined recursively using the Bellman operator B π :</s></p><formula xml:id="formula_1">B π V π = R(s, π(s)) + γ s ∈S P(s, a, s )V π (s ). (<label>2</label></formula><formula xml:id="formula_2">)</formula><p xml:id="_k5v8ucn"><s xml:id="_Zhgv6dx">Since the Bellman operator B π is a contraction mapping of value function V, there exists a fixed point of value V π such that B π V π = V π in the limit.</s><s xml:id="_yPwfRbr">The goal of an MDP problem is to compute an optimal policy π * such that V π * (s) ≥ V π (s) for every policy π and every state s ∈ S.</s></p><p xml:id="_TUtcSqD"><s xml:id="_5Rbbq9R">Broadly, there are mainly two types of solutions to an RL problem: the value-function based solutions that maintain a value function whereby a policy can be derived, and the direct policy search solutions that try to estimate the policy directly without representing a value function explicitly <ref type="bibr" target="#b13">[14]</ref>.</s><s xml:id="_XtQ9Yna">The former include the modelbased dynamic programming methods such as value iterations (VI) and policy interactions (PI), or direct RL methods such as temporal difference (TD) methods (e.g., Q-learning <ref type="bibr" target="#b14">[15]</ref>).</s><s xml:id="_ecJ8e4P">Direct policy gradient (PG) is typical policy search method, which can parameterize the policy and estimate the gradient relative to policy parameters.</s><s xml:id="_d6nrMNG">Its update rule is given as follows:</s></p><formula xml:id="formula_3">θ U(θ) ← 1 m m i=1 θ logπ θ (τ , θ)R(τ ) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p xml:id="_UAzGArq"><s xml:id="_EBEGVYt">where τ is the trajectory, θ is the parameter, and m is the number of trajectories.</s><s xml:id="_PbdYKgh">In Eq. ( <ref type="formula" target="#formula_3">3</ref>), θ U(θ) is the gradient of the policy, π θ (τ , θ) is the probability of the occurrence of a trajectory(τ ), θ logπ θ (τ , θ) is the steepest direction when τ changes with θ, and R(τ ) is the reward of a trajectory to control the updating direction and step size of parameter.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_q4zU2s2">Incorporating causal factors into policy gradient RL</head><p xml:id="_6zdPQVx"><s xml:id="_QgMMtJy">The direct PG algorithm only considers each state and expected value of the actions, but does not relate any causal effect between the actions and final performance.</s><s xml:id="_Fjmyxpb">This is contradictory to the fact that, in many fields, there exhibit various kinds of correlations between actions and corresponding outcomes.</s><s xml:id="_NHC9gxT">This is more prominent in the medical area where different options such as medicine dosage, treatment type and duration would usually give rise to various treatment outcomes of patients.</s><s xml:id="_JjNeKFc">To model this kind of causal relationships, we introduce a causal factor C (B|A) of event B due to event A as follows:</s></p><formula xml:id="formula_5">C (B|A) = P(B/A) -P(B/ ¬ A) (<label>4</label></formula><formula xml:id="formula_6">)</formula><p xml:id="_zKUQJBJ"><s xml:id="_faf4guD">where P(B/A) is the probability of event B conditioned on event A, and P(B/ ¬ A) is the probability of event B given that event A did not happen.</s><s xml:id="_HZwu9bg">Expanding Eq. ( <ref type="formula" target="#formula_5">4</ref>) gets the following equation:</s></p><formula xml:id="formula_7">C (B|A) = P(A ∩ B)/P(A) -P( ¬ A ∩ B)/(1 -P(A)) (5)</formula><p xml:id="_mSFfGkd"><s xml:id="_2cM6YnN">where P(A) represents the probability of occurrence of event A, P(A∩B) represents the probability of event A and event B occurring at the same time, and P( ¬ A ∩ B) represents the probability that event A does not happen, but event B happens at the same time.</s><s xml:id="_SugSHPQ">The causal factor C (B|A) can be computed using a sampling method proposed in <ref type="bibr" target="#b15">[16]</ref>.</s></p><p xml:id="_ZSUrFtx"><s xml:id="_56UPyqq">If causal factor C is positive, there is a causal relationship between event A and event B, because event B occurred because of A (that is, event A is the cause of event B) and negative otherwise.</s><s xml:id="_9JR8R5Z">Causal factors C can be incorporated into the policy gradient learning process as follows:</s></p><formula xml:id="formula_8">θ U(θ) ← 1 m m i=1 θ logπ θ (τ , θ) * R(τ ) * C (6)</formula><p xml:id="_WV2HjB4"><s xml:id="_aem9RvX">where τ , θ, θ U(θ), π θ (τ , θ), and θ logπ θ (τ , θ) are the same as Eq. ( <ref type="formula" target="#formula_3">3</ref>).</s><s xml:id="_VfSjN2T">The product of C and R(τ ) controls the updating direction and step size of parameters, in order to indicate how causes (i.e., decisions along the trajectory) affect the final performance for each trajectory.</s><s xml:id="_39RvwRv">Table <ref type="table">1</ref> gives the full sketch of the proposed CPG algorithm based on the Monte Carlo policy gradient method ERINFORCE <ref type="bibr" target="#b16">[17]</ref> that has decomposed the trajectory into states and actions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cQf2sg8">Results</head><p xml:id="_DFeEDWW"><s xml:id="_R83Yjbq">In this section, we evaluate CPG in the treatment of HIV to verify its effectiveness.</s><s xml:id="_2q8sYWm">We first briefly introduce the DTR problem in HIV and its RL formulations.</s><s xml:id="_jyQ9dcc">We then use the direct PG algorithm to simulate HIV treatment, and investigate how the proposed CPG algorithm can be applied to solve this problem.</s><s xml:id="_nWXz6P6">Finally, we provide some Table <ref type="table">1</ref> The Causal Policy Gradient (CPG) Algorithm G← average future return from step t;</s></p><formula xml:id="formula_9">C = P(A ∩ B)/P(A) -P( ¬ A ∩ B)/P(1 -P(A)); θ ← θ + α θ logπ(a t |s t , θ) * G * C;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_m68J8tn">End for</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mTRRnTZ">Return θ;</head><p xml:id="_5m55vgY"><s xml:id="_jX67CPQ">End CPG discussions on the shortcomings of current research that need to be addressed in the future work.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_3t85VYN">MDP for DTRs in HIV</head><p xml:id="_gXjkhYz"><s xml:id="_yE45UVh">The simulated HIV treatment model <ref type="bibr" target="#b8">[9]</ref> consists of a six dimensional continuous state space, including the concentrations of healthy CD 4+ T-lymphocytes (T 1 ), healthy macrophages (T 2 ), healthy infected CD 4+ T-lymphocytes T * 1 , infected macrophages (T * 2 ), free virus particles (V ) and HIV-specific cytotoxic T-cells (E).</s><s xml:id="_mHAxfSM">The full drug interaction model is given by the Appendix.</s></p><p xml:id="_SRVURtJ"><s xml:id="_GAzEnE8">While anti-retroviral treatment regimens are sometimes augmented by other types of drugs that enhance the effect of anti-HIV treatment, bolster the immune system, or reduce side effects, our current effort focuses on representatives of two main classes of enzymes: reverse transcriptase inhibitor (RTI) and protease inhibitor (PI).</s><s xml:id="_xG4HaXD">RTI prevents HIV RNA from being converted into DNA, thus blocking integration of the viral code into the target cell.</s><s xml:id="_JQ72ztt">On the other hand, PI affects the viral assembly process in the final stage of the viral life cycle, preventing the proper cutting and structuring of the viral proteins before their release from the host cell.</s><s xml:id="_Jsx7yDj">PI therefore effectively reduces the number of infectious virus particles released by an infected cell.</s><s xml:id="_AdGheMd">In all, there are four treatment regimens: only RTI on, only PI on, RTI and PI on, RTI and PI off.</s><s xml:id="_cyWbsKK">The four medication regimens are treated as four discrete actions.</s></p><p xml:id="_VmCRdjY"><s xml:id="_NQqbQq8">The reward of the process at time t can be defined as 1000E t -0.1V t -20000ξ 2 1t -2000ξ 2 2t <ref type="bibr" target="#b8">[9]</ref>.</s><s xml:id="_zWEUgju">The ξ 1t is set to 0.7 when the RTI is cycled on, while ξ 2t is set to 0.3 when PI is cycled on; and the ξ 1t or ξ 2t is set to 0, when RTI or PI is off.</s><s xml:id="_ncKf6YY">The formula of the reward value indicates that the increase of E or the decrease of V is conducive to obtaining larger rewards (i.e., promoting the treatment of HIV), while excessive application of enzymes would damage the cells and thus decrease the reward.</s><s xml:id="_t8BTEp8">As shown in <ref type="bibr" target="#b8">[9]</ref>, in the absence of treatment, the model has three equilibrium points as given in Table <ref type="table" target="#tab_0">2</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_cdczd65">RL for HIV treatment</head><p xml:id="_HNAp38S"><s xml:id="_pfTDKVj">We first apply the direct PG to the treatment of HIV.</s><s xml:id="_ddtxerp">The non-healthy locally stable equilibrium point was taken as the initial state.</s><s xml:id="_NPdW4YN">During the experiment, the drug is taken every 5 days and a course of treatment is observed for 600 days.</s><s xml:id="_3XK5qSE">Supposing there are 300 patients, each patient is in the unhealthy state initially.</s><s xml:id="_hf89HcU">The decay factor γ is 0.85, and the learning rate λ is 0.02.</s><s xml:id="_875jpPU">For the first patient, we used the policy gradient algorithm to generate a random strategy.</s><s xml:id="_wfsh3Xh">Figure <ref type="figure" target="#fig_0">1</ref> shows the random DTR for the first patient and the continuously learned DTR for other patients during the Simulated HIV Treatment.</s><s xml:id="_bHxyX3N">For the first patient (Fig. <ref type="figure" target="#fig_1">1a</ref>), RTI and PI were randomly chosen, indicating an irregular therapeutic process.</s><s xml:id="_DSwKgxb">The RL algorithm gradually learned that continuous use of RTI would have a significant healing effect on HIV (Fig. <ref type="figure" target="#fig_1">1b-c</ref>).</s><s xml:id="_7AEt9jr">For the 300th patient (Fig. <ref type="figure" target="#fig_1">1d</ref>), the algorithm finally learned a strategy that the RTI and PI were continuously used throughout the treatment.</s></p><p xml:id="_mNVjkSZ"><s xml:id="_jnyrCTA">Figure <ref type="figure" target="#fig_2">2</ref> shows the continuous change in the number of six cells (T 1 , T 2 , T * 1 , T * 2 , V, E) over time for the first patient (i.e., before learning).</s><s xml:id="_Nrp7Geb">The number of each cell in Fig. <ref type="figure" target="#fig_2">2a</ref>-f fluctuate greatly, and the number of cells change irregularly.</s><s xml:id="_BGFQtys">The number of T 1 , T 2 and E do not increase significantly, and the number of T * 1 , T * 2 and V also did not decrease much.</s><s xml:id="_JAKvmWc">Therefore, this treatment effect is very poor because the patient's condition has not improved.</s><s xml:id="_wzrTMxy">The patient was still in a non-healthy state after one course of treatment.</s></p><p xml:id="_KSTaKwt"><s xml:id="_6qHAxGb">Figure <ref type="figure" target="#fig_4">3</ref> shows the continuous change in the number of six cells after learning over 300 patients.</s><s xml:id="_DYxCxfU">As shown in Fig. <ref type="figure" target="#fig_1">1d</ref>, the patient has been administered by two enzymes (RTI and PI) continuously after learning and the number of each cell in Fig. <ref type="figure" target="#fig_4">3</ref> has regular change.</s><s xml:id="_MTpG3gu">After 100 days of treatment, T 1 , T 2 and E increase significantly compared to the initial state, while the number of T * 1 , T * 2 and V cells reduce significantly.</s><s xml:id="_DdyT3YS">In the end, the number of the six cells reach to a dynamic equilibrium as follows: (T 1 , T 2 , T * 1 , T * 2 , V , E) ≈ (800000, 100, 3000, 50, 5000, 40).</s><s xml:id="_8sy8fwN">Therefore, this treatment has a better therapeutic effect because the patient's health condition has improved.</s><s xml:id="_jg65Neh">However, compared with the non-healthy locally stable equilibrium point, the number of T 1 , T 2 and E cells were still low, while the number of the other three kinds of cells were still highly harmful to humans.</s><s xml:id="_kpQpTXn">The patients were still in a transitional state of non-health to healthy state after the treatment.</s><s xml:id="_HEcXkgY">At this time, the patients need to keep on taking medication, because they relied on the drug to maintain the current healthy state.</s><s xml:id="_5QMNWex">Once the medication was stopped, the number of harmful V cells may rebound greatly.</s></p><p xml:id="_GxSnhjP"><s xml:id="_GxSdsuH">Figure <ref type="figure">4</ref> shows the patients' reward at each decision step before learning and after learning.</s><s xml:id="_xzfpMgT">The reward before  learning fluctuates greatly, even emerging negative value, which indicates the ineffectiveness of initial treatment strategy.</s><s xml:id="_mN7SBBT">The reward after learning converges to a higher value, and finally stabilized at around 3800.</s><s xml:id="_KCqR24a">Thus, the medication regimen has a better therapeutic effect after using the policy gradient RL method.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_nX4emGU">Applying CPG for HIV treatment</head><p xml:id="_KT8dnTb"><s xml:id="_xyH33ja">Let the initial state, parameter value, observation period, patient number, initial strategy and other relevant variables be the same as "RL for HIV treatment" section.</s><s xml:id="_5vRX78m">We also apply CPG to the HIV model.</s><s xml:id="_5dqbmrx">To define the causal factor, let event A represent the action taken each time (i.e., adding enzyme RTI or PI), event ¬ A mean no enzyme action, and event B mean the outcome of V &gt; 415 (i.e., the number of free virus particles is greater than 415).</s><s xml:id="_svYG4uB">Thus, P(A) indicates the probability of taking an enzymatic action at each time, P(A ∩ B) represents the probability of simultaneous occurrence of free virus particles being greater than 415 and adding enzyme at the same time, and P( ¬ A ∩ B) represents the probability of simultaneous occurrence of free virus particles being greater than 415 without adding enzyme.</s><s xml:id="_SR4gx7p">For each cause of treatment, we can count the frequencies of each event and then use these frequencies to indicate the corresponding probabilities.</s><s xml:id="_uCCPceP">Figure <ref type="figure" target="#fig_5">5a</ref> compares the performance of direct PG and CPG algorithm, where the blue and yellow line represent cumulative reward over 600 days using the direct PG algorithm and the CPG algorithm, respectively.</s><s xml:id="_GxraFty">After about 100 episodes, the final treatment strategy can be learned using the direct PG algorithm.</s><s xml:id="_vuuTqnJ">Since the CPG algorithm can employ the causal factors to reason about the outcome of patients (i.e., how they are affected by the V cells) and the treatment (i.e., administration of enzyme), the policy learning process can be greatly promoted.</s><s xml:id="_YPQ2bSQ">CPG can learn the same treatment strategy as PG in less 50 episodes, indicating that the two enzymes are used continuously and the patients need to take the drug to maintain healthy.</s><s xml:id="_B9EDMMA">Compared to the direct PG algorithm, CPG is more efficient and robust by improving the learning speed in terms of cumulative reward and convergence rate.</s><s xml:id="_yScXbue">the learning proceeds, the causal factors increase and reach a dynamic balance after around 50 episodes.</s><s xml:id="_x62CYr7">Due to constantly random exploration in the learning process, the causal factor is always changing and finally close to 1.</s><s xml:id="_JsGWmPQ">In the initial treatment phase, the patient's V = 63919 is much greater than 415.</s><s xml:id="_NY8MJFp">As learning proceeds, the medication policy became better and better and the regulative effect of causal factors also became Fig. <ref type="figure">4</ref> The evolution of reward of a the first patient; and b the 300th patient stronger.</s><s xml:id="_4DfZSFe">After 50 episodes, the CPG algorithm learned the strategy of continuous dosing of both enzymes, so causal factors also reached the state of dynamic equilibrium.</s></p><p xml:id="_Brynh3d"><s xml:id="_9ksWkg9">The CPG algorithm requires prior definition of the causal factors in terms of causal events and outcome events.</s><s xml:id="_JyTE7gT">Defining different causal factors has quite different effects on the final learning performance.</s><s xml:id="_H7wHJzp">To test this, we defined different causal factors (C1 and C2) in Table <ref type="table" target="#tab_1">3</ref>.</s></p><p xml:id="_V9fSMP5"><s xml:id="_YFsQEDt">Figure <ref type="figure">6a</ref> compares the learning performance of the direct PG with different defined causal factors using CPG, in which CPG-C1 and CPG-C2 represent the CPG with causal factor C1 and C2, respectively.</s><s xml:id="_q8yF994">The main difference between C1 and C2 lies in the definition of B event.</s><s xml:id="_m5JCRDQ">In CPG-C1, event B means that the number of free virus particles is greater than 415, while in CPG-C2, event B means that the number of healthy macrophages is less than 621.</s><s xml:id="_rbVSZPQ">As shown in Fig. <ref type="figure">6a</ref>, CPG-C1 has the best performance and the fastest convergence.</s><s xml:id="_2czEf7c">Compared with CPG-C2, the causal factor definition of C1 is more reasonable (the convergence effect and convergence speed are better).</s><s xml:id="_gTHGB5d">In the HIV model, V has a greater effect on human health than T 2 , because V cells are the direct factors that influence the development of disease.</s><s xml:id="_GGbswdU">Therefore, different definitions of causal factor can greatly affect the learning performance of CPG algorithm, and an appropriate causal</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SFH5fR2">Discussion</head><p xml:id="_TAy8ztK"><s xml:id="_rEVvGdA">Although we are able to derive an effective treatment strategy using the above direct PG and CPG algorithms, the solutions still only converged to a sub-health state that must be maintained by continuous dosing of both enzymes.</s><s xml:id="_MVWjUTK">In order to derive the optimal drug-free treatment strategies, we directly solve the HIV model using the Lagrangian function formula introduced in <ref type="bibr" target="#b7">[8]</ref> (which is given by the Appendix).</s><s xml:id="_RYpRnHq">By solving Lagrangian function given by Eq. ( <ref type="formula">17</ref>), the optimal control ξ * 1 and ξ * 2 are characterized by Eqs. ( <ref type="formula">7</ref>) and <ref type="bibr" target="#b7">(8)</ref>, respectively <ref type="bibr" target="#b7">[8]</ref>:</s></p><formula xml:id="formula_10">ξ * 1 = max(a 1 , min(b 1 , (η 1 -η 3 + ρ 1 * η 5 ) * k 1 * V * T 1 -(η 2 -η 4 + ρ 2 * η 5 ) * f * k 2 * V * T 2 /2 * R 1 )) (7) ξ * 2 = max a 2 , min b 2 , η 5 * NT * δ * T * 1 + T * 2 /2 * R 2<label>(8)</label></formula><p xml:id="_Vh6ev5q"><s xml:id="_54CJNVj">where ξ 1 ∈[0, 1) and ξ 2 ∈[0, 1) are the control variables representing RTIs and PIs, respectively.</s><s xml:id="_maNdGJV">In order to get a better strategy, we set a 1 =0.0, a 2 =0.0, b 1 =0.7 and b 2 =0.3.</s><s xml:id="_r5AcFPK">We used partial differential equations to solve the dynamics parameters, and applied Eqs. ( <ref type="formula">7</ref>) and ( <ref type="formula" target="#formula_10">8</ref>) to obtain the optimal strategy.</s><s xml:id="_BrD4ram">Figure <ref type="figure">6b</ref> plots the computed optimum strategy, in which the red line and the blue line represent the dosing of PI and RTI, respectively.</s><s xml:id="_XjT6gAh">It is clear to see that after 400 days of treatment, the two drugs are stopped, indicating a drug-free healthy stable state of patients.</s><s xml:id="_rBunmEE">The reasons why the general RL algorithms such as PG and CPG in this paper could not discover the optimal drug-free solutions lie in two main perspectives.</s><s xml:id="_FRWub7a">On one hand, the MDP model adopted in this paper only considers four discrete actions that involve two types of enzyme and assigns a predefined fixed value of 0.7 and 0.3 to the parameters of ξ 1 and ξ 2 .</s><s xml:id="_j5BckDP">This highly simplification makes it difficult or impossible to fully explore the whole state of the model in order to derive the optimal solution.</s><s xml:id="_62UxNdc">Moreover, the reward function used in the MDP model is too abstract to reflect the complex dynamics of the treatments.</s><s xml:id="_5RuK5ub">On the other hand, the policy gradient algorithms themselves did not incorporate any sophisticated exploration strategies during the learning process.</s><s xml:id="_GhdjaX9">This is a critical problem since HIV treatment has long been recognized as a well-known testbed for evaluating advanced exploration algorithms in RL research <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</s><s xml:id="_J5UPPte">Previous studies have shown that the basin of attraction of the healthy steady-state in HIV is relatively small compared to the one of the non-healthy steady state.</s><s xml:id="_3tAh9Kw">Thus, in the absence of drugs, perturbation of the uninfected steady state by adding as little as virus would lead to asymptotic convergence towards the non-healthy steady state.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_K4eJQZ5">Conclusions</head><p xml:id="_tSYWnZZ"><s xml:id="_7JD5tYM">Simulation-based DTR design has a series of advantages over cytopathological treatment in that it can avoid the harm to the patients during the exploration of drug, provide a large amount of treatment experience for the disease with insufficient case in reality, reduce the cost of actual treatment and shorten the duration of treatment.</s><s xml:id="_WDTUs8E">In this paper, we investigated the role of RL in DTRs for simulated patients with HIV.</s><s xml:id="_VsZmSkD">We showed that both the direct PG and its causal extension could obtain a better medication regimen after a period of learning, but the CPG algorithm was more efficient and robust due to incorporation of causal factors between options of anti-HIV drugs and the associated treatment outcomes.</s><s xml:id="_VjgvFjF">We also showed that different definitions of the causal factor could have significant influence on the final learning performance, indicating the necessity of human prior knowledge on defining a suitable causal relationships for a given problem.</s><s xml:id="_6N4TsJn">How to discover the most beneficial or optimal causal factors from historical interaction trajectories is thus important to automate the whole learning process.</s><s xml:id="_hMbNMWF">This will be left for our future work for further investigation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CaEk3XR">Appendix</head><p xml:id="_QcZvTfY"><s xml:id="_7HAdHeC">This mathematical model of the HIV is described by the following set of ordinary differential and Lagrangian equations: V = 70(1 -ξ 2 ) * (T * 1 + T * 2 ) -13V -8.0 * 10 -7 * (1 -ξ 1 ) * T 1 + 1.0 * 10 -4 * (1 -0.34ξ 1 ) * T 2 * V (13) E = 1+ 0.3 * T * 1 + T * 2 * E T * 1 + T * 2 + 100 -0.25 * T * 1 + T * 2 * E T * 1 + T * 2 + 500 -0.1E (14) R = 0.1 * V + 20000 * ξ 2 1 + 2000 * ξ 2 2 -1000E (15) W = W 11 * (ξ 1a 1 ) + W 12 (b 1 -ξ 1 ) + W 21 (ξ 2a 2 ) + W 22 (b 2 + ξ 2 ) (16)</s></p><formula xml:id="formula_11">T 1 = 10000 -0.</formula><formula xml:id="formula_12">L = R + η 1 * T 1 + η 2 * T 2 + η 3 * T * 1 + η 4 * T * 2 + η 5 * V + η * E -W (17)</formula><p xml:id="_95Dpf3S"><s xml:id="_uEX9jrM">where T 1 , T 2 , T * 1 , T * 2 , V, E are the number of six cells; ξ 1 ∈[0, 1) and ξ 2 ∈[0, 1) are the control variables representing RTIs and PIs, respectively; W ij &gt; 0 are the penalty multipliers; and η n are the adjoint variables.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc><div><p xml:id="_FApkeWT"><s xml:id="_hKtR3Nk">The CPG Algorithm Function CPG Input: a differentiable policy parameterizations π(a|s, θ), ∀a ∈A, s∈S, θ ∈ R d , C=0; Initialize policy parameter θ ; Repeat forever: Define event A and event B; Generate an episode s 0 , a 0 , r 1 , ..., s T-1 , a T-1 , r T , following π(a|s, θ ); For each step of the episode t=0,...,T-1:</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc><div><p xml:id="_rBJSX96"><s xml:id="_uY4V93F">Fig. 1 The medication regimen a before learning; b-c during learning; and d after learning</s></p></div></figDesc><graphic coords="5,64.96,94.17,467.08,352.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc><div><p xml:id="_RX8HKQf"><s xml:id="_DfZa7zM">Fig. 2 The evolution of the six types of cells for the first patient (i.e., before learning).</s><s xml:id="_NDw4SKB">a-f corresponds to the continuous change of T 1 , T 2 , T * 1 , T * 2 , E and V cells, respectively</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><figDesc><div><p xml:id="_kn8jySH"><s xml:id="_XwxmfCJ">Figure5bshows the dynamic changes of causal factor (C) from the first episode to the 300th episode.</s><s xml:id="_zHFNbY6">During the early training stage, the causal factor is quite low, indicating little effect on the learning process.</s><s xml:id="_uTFFXDb">As</s></p></div></figDesc><graphic coords="6,63.82,93.78,467.08,549.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3</head><label>3</label><figDesc><div><p xml:id="_TtW6ShN"><s xml:id="_Sm6wPzV">Fig. 3 The evolution of the six types of cells after learning over 300 patients.</s><s xml:id="_kT9wJbA">a-f corresponds to the continuous change of T 1 , T 2 , T * 1 , T * 2 , E and V cells, respectively</s></p></div></figDesc><graphic coords="7,64.96,93.84,467.08,550.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 a</head><label>5</label><figDesc><div><p xml:id="_BZCAZg5"><s xml:id="_yVT5ge7">Fig. 5 a Comparison of the performance of direct PG and CPG algorithm; b Dynamic evolution of causal factor C during learning</s></p></div></figDesc><graphic coords="8,63.82,529.62,467.08,183.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>01T 1 - 1 ( 11 ) T * 2 = 2 - 2 ( 12 )Fig. 6 aR</head><label>1111222126</label><figDesc><div><p xml:id="_dvJ2pjx"><s xml:id="_GKu2YHm">Fig. 6 a Comparison of reward values for causal algorithms with different causal factors; b The optimum strategy in HIV treatment</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="10,63.82,94.14,467.08,192.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc><div><p xml:id="_pKuNB5V"><s xml:id="_YycgKJJ">Different equilibrium points of the six cells</s></p></div></figDesc><table><row><cell>Equilibrium point</cell><cell>T 1</cell><cell>T 2</cell><cell>T  *  1</cell><cell>T  *  2</cell><cell>V</cell><cell>E</cell></row><row><cell>The healthy, unstable state</cell><cell>10 6</cell><cell>3198</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>10</cell></row><row><cell>The healthy, locally stable state</cell><cell>967839</cell><cell>621</cell><cell>76</cell><cell>6</cell><cell>415</cell><cell>353108</cell></row><row><cell>The non-healthy, locally stable state</cell><cell>163573</cell><cell>5</cell><cell>11945</cell><cell>46</cell><cell>63919</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc><div><p xml:id="_eQgNNQW"><s xml:id="_SE6EhhF">Definition of different causal factors</s></p></div></figDesc><table><row><cell>Event</cell><cell>C1</cell><cell>C2</cell></row><row><cell>A</cell><cell>adding enzyme RTI or PI</cell><cell>adding enzyme RTI or PI</cell></row><row><cell>¬ A</cell><cell>without adding enzyme</cell><cell>without adding enzyme</cell></row><row><cell>B</cell><cell>V &gt; 415</cell><cell>T 2 &lt; 621</cell></row><row><cell cols="3">definition can significantly speed up the performance of</cell></row><row><cell cols="2">the algorithm.</cell><cell></cell></row></table></figure>
		</body>
		<back>


			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CbdA6d8">Acknowledgements</head><p xml:id="_r67c4NT"><s xml:id="_5TUQb4k">Not applicable.</s></p></div>
			</div>


			<div type="funding">
<div><head xml:id="_SmM37VE">Funding</head><p xml:id="_Dk57ycu"><s xml:id="_p53v8V3">This work is supported by the <rs type="programName">Hongkong Scholar Program</rs> under Grant No. <rs type="grantNumber">XJ2017028</rs>, and <rs type="programName">Dalian High Level Talent Innovation Support Program</rs> under Grant <rs type="grantNumber">2017RQ008</rs>.</s><s xml:id="_EtyERSh">About this supplement This article has been published as part of <rs type="programName">BMC Medical Informatics and Decision Making Volume 19 Supplement</rs> 2, 2019: Proceedings from the 4th <rs type="programName">China Health Information Processing Conference</rs> (CHIP 2018).</s><s xml:id="_U8CNVRJ">The full contents of the supplement are available online at URL. <ref type="url" target="https://bmcmedinformdecismak.biomedcentral.com/articles/supplements/volume-19-supplement-2">https://bmcmedinformdecismak.  biomedcentral.com/articles/supplements/volume-19-supplement-2</ref>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Uejj4Vs">
					<idno type="grant-number">XJ2017028</idno>
					<orgName type="program" subtype="full">Hongkong Scholar Program</orgName>
				</org>
				<org type="funding" xml:id="_mB3Wh2N">
					<idno type="grant-number">2017RQ008</idno>
					<orgName type="program" subtype="full">Dalian High Level Talent Innovation Support Program</orgName>
				</org>
				<org type="funding" xml:id="_qh9cRtR">
					<orgName type="program" subtype="full">BMC Medical Informatics and Decision Making Volume 19 Supplement</orgName>
				</org>
				<org type="funding" xml:id="_3KzQFa5">
					<orgName type="program" subtype="full">China Health Information Processing Conference</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mHF3QRu">Availability of data and materials</head><p xml:id="_KQ5FtRh"><s xml:id="_JDtUD7j">The datasets used and/or analysed during the current study available from the first author on reasonable request.</s></p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ppHQhXV">Authors' contributions</head><p xml:id="_HjfAPjr"><s xml:id="_Ug8ng47">YC proposed the idea and drafted the manuscript.</s><s xml:id="_cHBYxbR">DY and RG contributed to the implementation, collection, analysis, and interpretation of experimental data.</s><s xml:id="_zK7Qe74">LJ supervised the research and proofread the manuscript.</s><s xml:id="_vrfV63F">All authors contributed to the preparation, review, and approval of the final manuscript and the decision to submit the manuscript for publication.</s><s xml:id="_DYbPEFF">All authors read and approved the final manuscript.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_prHr2HT">Ethics approval and consent to participate</head><p xml:id="_jn3XXda"><s xml:id="_tAGDBqY">Not applicable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mbCkVsw">Consent for publication</head><p xml:id="_RQD39Md"><s xml:id="_jdYzTza">Not applicable.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YZAdhp5">Competing interests</head><p xml:id="_YtxGwRF"><s xml:id="_YvcABv8">The authors declare that they have no competing interests.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_M5RFPxc">Publisher's Note</head><p xml:id="_veBpX3E"><s xml:id="_AR9afEJ">Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main" xml:id="_DVtW87q">Reinforcement Learning: An Introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>The MIT press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton RS, Barto AG. Reinforcement Learning: An Introduction. Cambridge: The MIT press; 1998.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_Y6HaMaT">Combining kernel and model based learning for hiv therapy selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parbhoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bogojeska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sDsjjnz">AMIA Summits Transl Sci Proc</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">239</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Parbhoo S, Bogojeska J, Zazzi M, Roth V, Doshi-Velez F. Combining kernel and model based learning for hiv therapy selection. AMIA Summits Transl Sci Proc. 2017;2017:239.</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_GnZkDNG">Deep reinforcement learning for automated radiation adaptation in lung cancer</title>
		<author>
			<persName><forename type="first">H-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ten</forename><surname>Haken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Naqa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">E</forename></persName>
		</author>
		<idno type="DOI">10.1002/mp.12625</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XW3RQgW">Med Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="6690" to="6705" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tseng H-H, Luo Y, Cui S, Chien J-T, Ten Haken RK, Naqa IE. Deep reinforcement learning for automated radiation adaptation in lung cancer. Med Phys. 2017;44(12):6690-705.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_f66khmd">Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Daskalaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Mougiakakou</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0158722</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Nh5zgWz">PloS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">158722</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Daskalaki E, Diem P, Mougiakakou SG. Model-free machine learning in biomedicine: Feasibility study in type 1 diabetes. PloS ONE. 2016;11(7): 0158722.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_ewEh5GU">Informing sequential clinical decision-making through reinforcement learning: an empirical study</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shortreed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Stroup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-010-5229-0</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ee78wjN">Machine learning</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="109" to="136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shortreed SM, Laber E, Lizotte DJ, Stroup TS, Pineau J, Murphy SA. Informing sequential clinical decision-making through reinforcement learning: an empirical study. Machine learning. 2011;84(1-2):109-36.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main" xml:id="_BbPHfQZ">Representation and reinforcement learning for personalized glycemic control in septic patients</title>
		<author>
			<persName><forename type="first">W-H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00654</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Weng W-H, Gao M, He Z, Yan S, Szolovits P. Representation and reinforcement learning for personalized glycemic control in septic patients. 2017. arXiv preprint arXiv:1712.00654.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_dwq5Z4j">Interpretable policies for reinforcement learning by genetic programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Udluft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Runkler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.engappai.2018.09.007</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_e3nfrrP">Eng Appl Artif Intell</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="158" to="169" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hein D, Udluft S, Runkler TA. Interpretable policies for reinforcement learning by genetic programming. Eng Appl Artif Intell. 2018;76:158-69.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_Y3tsUaQ">Dynamic multidrug therapies for hiv: Optimal and sti control approaches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-D</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TRe9Huk">Math Biosci Eng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="241" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Adams BM, Banks HT, Kwon H-D, Tran HT. Dynamic multidrug therapies for hiv: Optimal and sti control approaches. Math Biosci Eng. 2004;1(2): 223-41.</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_gWuCU96">Clinical data based optimal sti strategies for hiv: a reinforcement learning approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-B</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehenkel</surname></persName>
		</author>
		<idno type="DOI">10.1109/cdc.2006.377527</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CXeR7FH">45th IEEE Conference on Decision and Control</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="667" to="672" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ernst D, Stan G-B, Goncalves J, Wehenkel L. Clinical data based optimal sti strategies for hiv: a reinforcement learning approach. In: 45th IEEE Conference on Decision and Control. New York: IEEE; 2006. p. 667-72.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main" xml:id="_nDVyDrp">A reinforcement learning design for hiv clinical trials</title>
		<author>
			<persName><forename type="first">S</forename><surname>Parbhoo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note type="raw_reference">Parbhoo S. A reinforcement learning design for hiv clinical trials. 2014. PhD thesis.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_dayBt2X">Control the population of free viruses in nonlinear uncertain hiv system using q-learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gholizade-Narm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noori</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13042-017-0639-y</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6GGaUqE">Int J Mach Learn Cybern</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1169" to="1179" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gholizade-Narm H, Noori A. Control the population of free viruses in nonlinear uncertain hiv system using q-learning. Int J Mach Learn Cybern. 2018;9(7):1169-79.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_ARUpkqQ">Quantifying uncertainty in batch personalized sequential decision making</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Marivate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chemali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_4SJugQG">AAAI Workshop: Modern Artificial Intelligence for Health Analytics</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>The AAAI Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Marivate VN, Chemali J, Brunskill E, Littman ML. Quantifying uncertainty in batch personalized sequential decision making. In: AAAI Workshop: Modern Artificial Intelligence for Health Analytics. Cambridge: The AAAI Press; 2014.</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main" xml:id="_MMruxwj">Transfer learning across patient variations with hidden parameter markov decision processes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v31i1.11065</idno>
		<idno type="arXiv">arXiv:1612.00475</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Killian T, Konidaris G, Doshi-Velez F. Transfer learning across patient variations with hidden parameter markov decision processes. 2016. arXiv preprint arXiv:1612.00475.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_BY929G7">Reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wiering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Otterlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_beDBDsx">Adapt Learn Optim</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Wiering M, Van Otterlo M. Reinforcement learning. vol 12. Adapt Learn Optim. Berlin: Springer; 2012.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_RdyM3US">Q-learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00992698</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_xHxN6fB">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Watkins CJ, Dayan P. Q-learning. Mach Learn. 1992;8(3-4):279-92.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main" xml:id="_qe44n6m">Causal explanation under indeterminism: A sampling approach</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Merck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kleinberg</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v30i1.10088</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_BjJBcdk">AAAI</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>The AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1037" to="1043" />
		</imprint>
	</monogr>
	<note type="raw_reference">Merck CA, Kleinberg S. Causal explanation under indeterminism: A sampling approach. In: AAAI. Cambridge: The AAAI Press; 2016. p. 1037-43.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_bBhnudW">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno type="DOI">10.1007/bf00992696</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_qxr95jG">Mach Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Williams RJ. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach Learn. 1992;8(3-4):229-56.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_WbZuvNx">Bounded optimal exploration in mdp</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v30i1.10230</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Ybz29xm">AAAI</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>The AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1758" to="1764" />
		</imprint>
	</monogr>
	<note type="raw_reference">Kawaguchi K. Bounded optimal exploration in mdp. In: AAAI. Cambridge: The AAAI Press; 2016. p. 1758-64.</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_SaEr5HD">Pac optimal exploration in continuous space markov decision processes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pazis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Parr</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v27i1.8678</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qzsHKhU">AAAI</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>The AAAI Press</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pazis J, Parr R. Pac optimal exploration in continuous space markov decision processes. In: AAAI. Cambridge: The AAAI Press; 2013.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
