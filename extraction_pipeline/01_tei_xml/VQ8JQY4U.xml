<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0"
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_8w9CkhY">Optimizing Adaptive Notifications in Mobile Health Interventions Systems: Reinforcement Learning from a Data-driven Behavioral Simulator</title>
				<funder>
					<orgName type="full">Playful Data-driven Active Urban Living</orgName>
				</funder>
				<funder>
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_AARRKnd">
					<orgName type="full">SIA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
					<p type="raw">© The Author(s) 2021</p>
				</availability>
				<date type="published" when="2021-10-18">18 October 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shihan</forename><surname>Wang</surname></persName>
							<email>s.wang2@uu.nl</email>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Informatics Institute , University of Amsterdam , Amsterdam , Netherlands</note>
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<note type="raw_affiliation"><label>2</label> Information and Computing Sciences , Utrecht University , Utrecht , Netherlands</note>
								<orgName type="department">Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<settlement>Utrecht</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><surname>Chao Zhang</surname></persName>
							<affiliation key="aff2">
								<note type="raw_affiliation"><label>3</label> Department of Psychology , Utrecht University , Utrecht , Netherlands</note>
								<orgName type="department">Department of Psychology</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<settlement>Utrecht</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<note type="raw_affiliation"><label>5</label> Human-Technology Interaction , Eindhoven University of Technology , Eindhoven , Netherlands</note>
								<orgName type="department">Human-Technology Interaction</orgName>
								<orgName type="institution">Eindhoven University of Technology</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ben</forename><surname>Kröse</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Informatics Institute , University of Amsterdam , Amsterdam , Netherlands</note>
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<note type="raw_affiliation"><label>4</label> Digital Life , Amsterdam University of Applied Sciences , Amsterdam , Netherlands</note>
								<orgName type="department">Digital Life</orgName>
								<orgName type="institution">Amsterdam University of Applied Sciences</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
							<affiliation key="aff0">
								<note type="raw_affiliation"><label>1</label> Informatics Institute , University of Amsterdam , Amsterdam , Netherlands</note>
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
								<address>
									<settlement>Amsterdam</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_mhxDxEG">Optimizing Adaptive Notifications in Mobile Health Interventions Systems: Reinforcement Learning from a Data-driven Behavioral Simulator</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-18">18 October 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">328ECB4FA2EC5F1DA710E6A5F5DFA266</idno>
					<idno type="DOI">10.1007/s10916-021-01773-0</idno>
					<note type="submission">Received: 4 June 2021 / Accepted: 20 September 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.2" ident="GROBID" when="2025-08-31T13:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<label type="revision">a91ee48</label>
					<label type="parameters">startPage=-1, endPage=-1, consolidateCitations=2, consolidateHeader=2, consolidateFunders=1, includeRawAffiliations=true, includeRawCitations=true, includeRawCopyrights=true, generateTeiIds=true, generateTeiCoordinates=[all], sentenceSegmentation=true, flavor=null</label>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_X5qwvft">Mobile health intervention</term>
					<term xml:id="_Xn5jTTT">Adaptive agent</term>
					<term xml:id="_hTJ7sdp">Reinforcement learning</term>
					<term xml:id="_QSPh44Z">Human simulator</term>
					<term xml:id="_xka5zx6">Just-in-time adaptive intervention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_T9kECZG"><p xml:id="_YBvhebc"><s xml:id="_aB3hz2D">Mobile health (mHealth) intervention systems can employ adaptive strategies to interact with users.</s><s xml:id="_Z25wb7p">Instead of designing such complex strategies manually, reinforcement learning (RL) can be used to adaptively optimize intervention strategies concerning the user's context.</s><s xml:id="_3Ktwg6m">In this paper, we focus on the issue of overwhelming interactions when learning a good adaptive strategy for the user in RL-based mHealth intervention agents.</s><s xml:id="_9MAsCDe">We present a data-driven approach integrating psychological insights and knowledge of historical data.</s><s xml:id="_cXPEwUe">It allows RL agents to optimize the strategy of delivering context-aware notifications from empirical data when counterfactual information (user responses when receiving notifications) is missing.</s><s xml:id="_yQjxVmv">Our approach also considers a constraint on the frequency of notifications, which reduces the interaction burden for users.</s><s xml:id="_6ZaaRK5">We evaluated our approach in several simulation scenarios using real large-scale running data.</s><s xml:id="_cz34cxA">The results indicate that our RL agent can deliver notifications in a manner that realizes a higher behavioral impact than context-blind strategies.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="595.276" lry="790.866"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_FdCxmsu">Introduction</head><p xml:id="_5xTJJmJ"><s xml:id="_wdNGH78">Adaptive interventions have emerged as a new perspective of prevention and treatment in healthcare <ref type="bibr" target="#b0">[1]</ref>.</s><s xml:id="_7pfUZTz">The just-in-time adaptive intervention (JITAI) is an adaptive intervention design concept, aiming to provide the right type /amount of support at the right time based on an individual's changing internal and external states <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</s><s xml:id="_sCnUmR2">Though JITAIs can be administered through several means (e.g.</s><s xml:id="_FDZCdcc">in-person and computer), the ubiquity of mobile devices allows for continuous participant monitoring and delivery of personalized interventions.</s><s xml:id="_75n5pqw">Mobile health systems (agents) with JITAIs have proven effective in preventing certain health threats (e.g.</s><s xml:id="_ZjGvuAH">overeating <ref type="bibr" target="#b3">[4]</ref>, smoking <ref type="bibr" target="#b4">[5]</ref> and prolonged sedentary behaviors <ref type="bibr" target="#b5">[6]</ref>) and eliciting beneficial health outcomes (e.g.</s><s xml:id="_6YEvxrq">increased physical activity <ref type="bibr" target="#b6">[7]</ref> and self-management support related to chronic diseases <ref type="bibr" target="#b7">[8]</ref>).</s><s xml:id="_abSXqF6">However, the design of such interventions is demanding and the interaction with the user can be complex.</s><s xml:id="_GtuhmSP">Reinforcement learning (RL) based agents have been used to optimize mobile healthcare interventions adaptively <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>, which make use of historical data or data collected on the run.</s><s xml:id="_U2CHDmS">The problem of historical data is that it often misses counterfactual information (i.e.</s><s xml:id="_devzRbx">what would have been the outcome had interventions or circumstances been different).</s><s xml:id="_GDYNmmf">The problem of data collected during the intervention is that it requires many interactions in a short period, which add burden for the user and adversely impact engagement <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>.</s></p><p xml:id="_ZwVzArj"><s xml:id="_N5dpkSP">Throughout the paper, we focus on optimizing the delivery of context-aware notifications in mobile health systems.</s><s xml:id="_MnGvnhc">These notifications are sent in an adaptive manner dependent on the temporal and environmental context of users, motivating them to perform a target activity.</s><s xml:id="_tFsk2pR">To solve the two mentioned problems, based on a framework that combines historical data and psychological theories about human decision-making, we developed a simulation environment to optimize the timing of these notifications.</s><s xml:id="_NDsddH8">Moreover, to restrict interaction burden, we adapted an RL algorithm by incorporating a constraint on the number of notifications that can be sent within a period.</s><s xml:id="_ZzMRe5C">Finally, we conducted a case study on promoting running activity to demonstrate our approach.</s><s xml:id="_wPUpMDz">A dataset covering over 10K real users' running activity was used to build our simulator and evaluate our RL agent.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_evgqSQJ">Related work</head><p xml:id="_AEQJvGw"><s xml:id="_Xq8xQt9">For the optimization of JITAI intervention in mHealth systems, several different strategies were taken by researchers using RL <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref>.</s><s xml:id="_3Gd49yF">However, most of those RL approaches require the agent to interact many times with the user before performing well.</s><s xml:id="_rz3khtb">To shorten the online learning process, several researchers followed the concept of transfer learning to perform faster learning in mHealth settings.</s><s xml:id="_9HQ8xJF">Tabatabaei et al. <ref type="bibr" target="#b14">[15]</ref> and Tomkins et al. <ref type="bibr" target="#b15">[16]</ref> make RL algorithms quickly learn from the limited experience at the beginning stage by considering similar users.</s><s xml:id="_Nr9UYdg">Gonul et al. <ref type="bibr" target="#b16">[17]</ref> transfer the common knowledge acquired in other environments to get faster convergence.</s><s xml:id="_VPTyEp9">Without constraints on the intervention frequency, those RL approaches might still bother users by too many interventions during fast learning.</s><s xml:id="_qu8AGVw">While they concentrate on using data collected during the online interventions, we follow another direction to solve this challenge, i.e., incorporating prior knowledge from historical data to optimize the policy in advance.</s><s xml:id="_VaMe3XE">Similar to our approach, Liao et al. <ref type="bibr" target="#b17">[18]</ref> and Ameko et al. <ref type="bibr" target="#b18">[19]</ref> integrate prior distributions using collected data in an RL optimization process.</s><s xml:id="_DC57fnE">However, they apply relative small datasets in pre-learning because experimental data for specific intervention situations are often involving user interaction and therefore expensive to collect.</s><s xml:id="_22Wvsg6">Our framework allows learning prior knowledge from historical data collected without interacting with users, which makes the usage of largescale data possible.</s><s xml:id="_WXPq8N5">To avoid many interactions in a short period, our approach for the first time performs a structural study to incorporate a constraint on interaction frequency in RL-based mHealth systems.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_KsMxRmp">Methodology</head><p xml:id="_GeFpP8M"><s xml:id="_uRUwHyn">We model how users sequentially decide on whether to perform a target activity when receiving notifications (we use running as an example in this paper, in this case, the mobile agent sends notifications for promoting running activities).</s><s xml:id="_dTGpCzX">We formalize our problem (i.e.</s><s xml:id="_CFvA6nx">learning the optimal strategy for delivering notifications) as a finite horizon Markov Decision Process (MDP) <ref type="bibr" target="#b19">[20]</ref>.</s><s xml:id="_nErYaPP">Figure <ref type="figure">1</ref> presents an overview of our approach.</s><s xml:id="_Uc3ADHd">Here, the agent represents a mobile system that interacts with a target user (i.e. the environment) to optimize the strategy.</s><s xml:id="_JBauAfw">Our agent and environment interact in a sequence of discrete and finite time steps {1, 2, .., t} , which can be naturally broken into episodes.</s><s xml:id="_rfXHUAs">At each time step, the agent observes a representation of the environment and selects an action accordingly (two possible actions in our case: send a notification or not send).</s><s xml:id="_cB8mXFC">The environment then passes a numerical reward back to the agent.</s><s xml:id="_6eD9CAD">Based on this feedback mechanism, the agent adapts its policy to maximize an expected long-term reward.</s><s xml:id="_qF8KysD">Since too frequent interactions with the environment are not desirable in mHealth settings, we constrained the maximum number of notifications sent in each week (i.e.</s><s xml:id="_aDddgbh">episode).</s><s xml:id="_uwQCypq">In this paper, our optimization goal is to wisely deliver a restricted number of notifications to maximize the user's weekly running frequency.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_CccFUEZ">Insight from psychological theories</head><p xml:id="_vCtKNDb"><s xml:id="_3bMtXxh">Conceptually, it can be assumed that users' decisions to engage in certain activities (e.g., running) after notifications take two steps, option generation and option evaluation <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref>.</s><s xml:id="_95EHzW7">At any decision moment, behavioral options have Fig. <ref type="figure">1</ref> The overview of our methodology, including the agent-environment interaction in the MDP model and three key components developed in both environment and agent.</s><s xml:id="_ZheWP9G">The approach optimizes the delivery of context-aware notifications from empirical data to be generated in memory before they can be compared to inform a final choice.</s><s xml:id="_dw84B4E">Memory accessibility of different options during option generation is influenced by environmental cues, including system notifications.</s><s xml:id="_7z6XSvt">When a user receives a notification for running, the memory accessibility of running reaches its maximum.</s><s xml:id="_ZBcdJXP">This accessibility then gradually decreases in the form of a memory decay until the next notification is received.</s><s xml:id="_pQEUte4">The form of memory decay, or forgetting curve, is modeled as exponential functions in the psychology literature <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>.</s></p><p xml:id="_84gZADa"><s xml:id="_D795dA2">After being generated, a target option (running) has to compete with other generated behavioral options (e.g.</s><s xml:id="_ptmZK3Q">working on a paper) in terms of how much they satisfy a user's personal goals, such as being healthy and productive.</s><s xml:id="_VQr58S4">According to classic decision-making models <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, the goal-satisfying values of options, weighted by the importance of the goals, are transformed into subjective utilities, and the option with the highest subjective utility will be chosen.</s><s xml:id="_buyQHm9">Without enumerating all goal-related attributes, two types of attributes are important for running behavior.</s><s xml:id="_x3CcwCM">First, a user's momentary context (e.g.</s><s xml:id="_zWvbuKw">time and weather) can have great impacts on decisions because the options' goalsatisfying values depend on the contextual variables <ref type="bibr" target="#b26">[27]</ref>.</s><s xml:id="_g6f3XCm">For example, a Sunday morning with good weather makes running more enjoyable and also less interfering with one's work-related goals.</s><s xml:id="_bPHCtqy">Second, recently having a run ought to temporarily lower the utility of running.</s><s xml:id="_ng8Jmxw">After a run, one's body certainly needs time to recover to a level that is sufficient for running again.</s><s xml:id="_ZXVn6DR">Furthermore, having a run satisfies running-related goals and attenuates the importance of the goals.</s><s xml:id="_Cn23CRR">As people pursue multiple goals, this psychological mechanism allows people to switch to other goals and engage in behaviors that satisfy those goals (e.g.</s><s xml:id="_auvPAFe">finishing a manuscript to be productive) <ref type="bibr" target="#b27">[28]</ref>.</s><s xml:id="_DwSBVg3">In summary, three key determinants of running decisionmemory accessibility of running, urge of running, and personal context -were derived from the above theories and included in our computational model.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_xF57Y57">Computational model</head><p xml:id="_P8hKdzt"><s xml:id="_Vg7WukF">We formalized the above procedure as a dynamic Bayesian network (DBN).</s><s xml:id="_qVuYMMn">As a probabilistic graphical model, the DBN considers a set of variables and their conditional dependencies over adjacent time steps <ref type="bibr" target="#b28">[29]</ref>.</s><s xml:id="_rNKC3Yd">In this way, we generated a stochastic human simulator to make decisions based on both contextual and cognitive states sequentially.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YFHyqrB">Representation and topology of the DBN</head><p xml:id="_XtsHfAJ"><s xml:id="_nGsusPu">Following the psychological theories above, we defined five variables and their dependencies in our DBN as follows:</s></p><p xml:id="_gSctKyF"><s xml:id="_U687E3q">-A t represents whether an user decides to take a target activity (running) at time t.</s><s xml:id="_fDPVc7E">-M t is the user's memory accessibility of running at time t.</s><s xml:id="_v3tgD3R">-U t is the user's urge to run at time t.</s><s xml:id="_3QKUkrP">-C t is the personal context of the user at time t.</s></p><p xml:id="_QQmbRmY"><s xml:id="_d7xAcZg">-N t represents whether the user receives a notification at time t.</s></p><p xml:id="_7NU49K9"><s xml:id="_5CbYkA9">The variable M t and U t are real values in (0, 1).</s><s xml:id="_h4GB66F">The variable N t and A t are binary values ∈ {0, 1} , where '1' represents 'receive a notification' and 'decide to run' respectively.</s><s xml:id="_gvUbgZZ">The variable C t includes a set of contextual features, defined as a vector of values.</s><s xml:id="_GBKf29R">Under the first-order Markov assumption, we proposed a topological structure of the DBN, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_pRNuhtN">Definitions and inference of the DBN</head><p xml:id="_YS9H946"><s xml:id="_VRsvrUH">We specified transition probabilities in the DBN from either empirical data or psychological insights.</s><s xml:id="_uZtAGSj">Based on Insight from Psychological Theories, the state transitions of U t and M t were defined as Eqs. 1 and 2 in Fig. <ref type="figure" target="#fig_0">2</ref>, where the nota- tion represents the Kronecker delta function <ref type="bibr" target="#b29">[30]</ref>.</s><s xml:id="_HEh9KsN">Given a certain A t-1 and N t , we deterministically have U t and M t .</s><s xml:id="_ebkqQ4Z">The parameters and define the changing rate of urge and memory accessibility.</s><s xml:id="_bKQjwbJ">While memory accessibility decreases exponentially, the urge to run increases linearly over time.</s></p><p xml:id="_FjPYH6s"><s xml:id="_cpFu6b9">We also defined the transition from a joint observation of M t and U t to a target activity A t as Eq. 3 in Fig. <ref type="figure" target="#fig_0">2</ref>. In particular, we proposed to calculate two probabilities P(C t ) and P(C t |A t ) from empirical data (for details, see Data Description and Processing).</s><s xml:id="_4zvqWDT">Given these probabilities, we used the following equation to estimate how a user reacts to notifications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_W5HMpPy">Reinforcement learning algorithm</head><p xml:id="_NNHwtDX"><s xml:id="_Qpu6E2p">To learn the optimal policy (i.e. a stochastic mapping between a personal state of the user and an action to take) in our restricted setting, we adopted a policy gradient RL algorithm, REINFORCE <ref type="bibr" target="#b30">[31]</ref>.</s><s xml:id="_kPCYyxr">The REINFORCE algorithm with baseline and restriction is outlined in Algorithm 1.</s><s xml:id="_Dns9W2a">Our algorithm updates based on episodes.</s><s xml:id="_hr7K6Km">In each episode, it performs a gradient step on a neural network to optimize the policy parameter .</s><s xml:id="_UMSCPrx">We inserted a baseline function Ḡt inside the expectation to reduce the high variance, using the average of all returns G t in the past n episodes.</s><s xml:id="_7EazykT">Moreo- ver, to integrate with the restricted setting, we adjusted the procedures of action selection and policy adaptation in the REINFORCE algorithm.</s><s xml:id="_PuF8Gzd">Inspired by clipping the continuous action space in policy gradient <ref type="bibr" target="#b31">[32]</ref>, we constrained the ( <ref type="formula">1</ref>)</s></p><formula xml:id="formula_0">P(A t |M 0⋯t-1 , U 0⋯t-1 , C 0⋯t , N 0⋯t ) = P(A t |C t , N t , A t-1 , M t-1 , U t-1 ) = ∑ M t ∑ U t P(C t |A t ) P(C t ) ⋅ P(A t |U t , M t ) ⋅ P(M t |N t , M t-1 ) ⋅ P(U t |A t-1 , U t-1 ) .</formula><p xml:id="_Q6BC5Mu"><s xml:id="_s99vpgn">probability of certain discrete actions.</s><s xml:id="_eNQcWAA">After reaching the maximum number of notifications in each episode, the probability of sending a notification is always 0. In this way, we make sure our RL algorithm learns to deliver a restricted number of notifications according to the given momentary state.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_amdAmQR">Simulation experiments using real data</head><p xml:id="_srQx5Yt"><s xml:id="_KathM2U">We demonstrated the performance of our approach in a case study, aiming at promoting running activities by sending context-aware notifications.</s><s xml:id="_NrbkZDC">Our approach was evaluated in a simulation environment using real running data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ZfAsvCm">Experimental data and settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_fDumZkp">Data description and processing</head><p xml:id="_dhsXSHY"><s xml:id="_EN7SD9b">We used two datasets to derive the context related distributions in Eq. 1. First, a running dataset was used to derive the distribution P(C t |A t ) , measuring the relation between user context and running behavior.</s><s xml:id="_QweDQ8P">The data contains around 406K runs contributed by over 10K Dutch users while using a mobile fitness app from 2013-03 to 2017-03 <ref type="bibr" target="#b32">[33]</ref>.</s></p><p xml:id="_csjHAc7"><s xml:id="_jv7qngw">For each run, a set of metadata is collected and timestamp and weather information at the beginning are marked.</s><s xml:id="_qCGGVn7">We considered six variables in the data, namely 'hour of the day', 'weekday', 'temperature', 'weather type', 'wind type' and 'humidity type'.</s><s xml:id="_bWSX844">An example of context data is {8:00, Monday, -2, cloudy, moderate wind, moderate humidity}.</s></p><p xml:id="_62Q8xFY"><s xml:id="_QpvtHrE">Second, an open dataset provided by the Royal Netherlands Meteorological Institute (KNMI)<ref type="foot" target="#foot_1">foot_1</ref> was used to derive P(C t ) , the prior distribution of contextual information (general Dutch weather), which contains around 439K records of hourly weather.</s><s xml:id="_zc8tFb2">To make the two datasets comparable, we used the weather data over the same period of the running data.</s></p><p xml:id="_3FSrW9e"><s xml:id="_4ZMV3pt">We derived distribution P(C t |A t ) and P(C t ) from the run- ning and the weather dataset in a same manner.</s><s xml:id="_veRwerc">Thus, we only demonstrate how we derived the context distribution from the running data.</s><s xml:id="_3RYwgfG">Since data are only available when a running activity is performed, we concentrate on computing the distribution P(C t |A t = 1) , which is a joint distribution of all contextual variables.</s><s xml:id="_4GBjFgW">Since we noticed that the feature 'weekday' is conditionally independent with other features, we learned the distribution P(weekday t |A t = 1) by comput- ing probabilities of all seven values in the categorized feature 'weekday'.</s><s xml:id="_8CwjAKN">We also extracted the joint distribution of all the other features.</s><s xml:id="_rFRQNUR">For each combination of the discrete variables (weather, wind and humidity), we learned a separate multivariate Gaussian distribution for continuous variables (hour and temperature) using maximum likelihood estimation.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_u3Mqrhu">Setting of simulation with real contextual data</head><p xml:id="_9S76CZs"><s xml:id="_Cu4wgaW">We implemented our simulation experiments using python 2 .</s><s xml:id="_EPuAZC7">The RL algorithm was developed based on pytorch 3 , and our RL agent and simulation environment were built following the framework of OpenAI gym 4 .</s><s xml:id="_CpZmWda">In the simulation, the agent makes a decision on whether to send notification at every hour from 8:00 to 20:00.</s><s xml:id="_2F3CXGV">Only when the user performs a run before the next decision time step (within one hour), the agent gets a reward of 1.0 (otherwise zero reward).</s><s xml:id="_WPCBgPQ">In our environment, each episode is one week and maximum of 14 notifications are allowed in each week.</s><s xml:id="_NCxvKgt">We also provided realistic context information in the simulation environment by using empirical data in the used KNMI dataset.</s><s xml:id="_44QuSGZ">Based on the results of a simulator verification 5 , we set memory retention rate ( in Eq. 2) at 0.8 and urge recovery rate ( in Eq. 1) at 0.05.</s><s xml:id="_kJVArWN">The discount factor and learning rate are set to 1 and 0.001 respectively.</s><s xml:id="_GvU9bwg">We ran each simulation 20 times.</s><s xml:id="_22EusPk">In each run, the environment starts at 0:00 of a random date with its corresponding real weather data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_zRbrSxw">Experimental results</head><p xml:id="_8agMr8u"><s xml:id="_MhFGy6R">We evaluated our data-driven RL approach in two experiments.</s><s xml:id="_JDMJ7yb">To set a comparable environment, we randomly initialize a single simulation environment for all agents of each experiment at every simulation run.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_WWwTtAN">Evaluation of context-aware policy</head><p xml:id="_f5WPDRy"><s xml:id="_xSEAg6w">The first experiment aims to examine whether the policy learned by our data-driven approach outperforms general rule-based policies (not considering the contextual Fig. <ref type="figure">3</ref> The simulation results shown the average reward of agents in the sliding windows of 500 episodes information of users).</s><s xml:id="_bDFxjQ8">We compared our RL-based agent (R agent) with three baseline agents.</s><s xml:id="_MrdrcYX">All four agents send the same number of notifications per episode, but use different strategies.</s><s xml:id="_rhFEMbz">Three strategies of the baseline agents are (1) 'random week agent' sends 14 notifications randomly in each week; (2) 'random day agent' sends 2 notifications randomly in each day; (3) 'fixed agent' sends 2 notifications per day and they were evenly distributed (at 12:00 and 16:00).</s><s xml:id="_Nn2gUtS">The performance of agents is shown in Fig. <ref type="figure">3</ref>-left.</s><s xml:id="_8cjmeK4">We observed an obvious increase in the reward of R agent, while three others hold a relatively stable performance.</s><s xml:id="_KS3Tshk">It indicates our approach adaptively optimizes the policy to send a restricted amount of notifications with respect to user's momentary context, and afterward outperforms all context-blind agents.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_A5pUtPW">Evaluation of restricted policy</head><p xml:id="_gnTaskT"><s xml:id="_RjDTt2E">In the second experiment, we evaluated the efficiency of our restricted notification setting and how well the RL agents perform when incorporating this constraint during the learning in two different ways.</s><s xml:id="_DkUQAWb">One is applied and described in our RL algorithm of Reinforcement Learning Algorithm (R agent).</s><s xml:id="_5dt7tTR">Second is to integrate it into the simulation environment: after the maximum number of notifications is reached in an episode, a notification will not be sent even if the algorithm decides to send one (C agent).</s><s xml:id="_YqM5g5d">In Fig. <ref type="figure">3</ref>-right, we found that although the R agent learns faster than the C agent (consistent with results shown in <ref type="bibr" target="#b31">[32]</ref>), two agents show a similar performance after learning.</s><s xml:id="_9gJWsvy">In addition, we set up the B agent, which had no restriction on the number of notifications sent in each episode before 15,000 episodes.</s><s xml:id="_WFasdtw">Afterwards, we integrated the restriction in its environment, leading to a dramatic performance drop in Fig. <ref type="figure">3</ref>-right.</s><s xml:id="_x4ZdYsK">This phenomenon demonstrates the different performances from an agent without restriction during learning (agent B) and agents with restriction during learning (both the agent R and agent C).</s><s xml:id="_uFP2Q84">It indicates that the policy learned without considering the restriction hardly performs well in a restricted mHealth setting, suggesting the importance of modeling this practical restriction in training RL algorithms.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RP5nrDd">Interpretation of learned policy</head><p xml:id="_ZAeAP2f"><s xml:id="_wQP49vn">We further evaluated our approach by visualizing the detailed information of episodes in the learning process.</s><s xml:id="_CzBu3k7">Results of episode No. 100, 1500 and 16000 in a run of the R agent are presented in Fig. <ref type="figure" target="#fig_1">4</ref>, which correspond to a policy before learning, a policy at the end of the first rapid learning process and a policy at the stable stage of learning in Fig. <ref type="figure">3</ref>-left.</s><s xml:id="_5wj8acJ">We observed that at the beginning stage (episode 100), the R agent sends all notifications early in the episode.</s><s xml:id="_XVuaGab">Afterwards, the agent learns to spread the restricted number of notifications over the entire episode (see episode 1500).</s><s xml:id="_eD7DfDX">This is the first strategy our agent learns, which leads to the first increase of the reward in Fig. <ref type="figure">3</ref>.</s><s xml:id="_fbM6yEf">Moreover, the R agent learns to send notifications based on contextual situations.</s><s xml:id="_SUzreFQ">Notifications are sent in the decision points with very bad situations (dark blue ones) in the first two episodes, but almost all of them are sent under very good situations (dark red ones) in episode 16000.</s><s xml:id="_qmQnftf">Finally, as indicated in green color in Fig. <ref type="figure" target="#fig_1">4</ref>, the R agent realized that the simulated users are unlikely to run again in the hours following a recent run.</s><s xml:id="_GCDAq2q">Hence, the strategy of 'not sending notification after a run' seems to be learned.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_YxaRqdY">Conclusion and future work</head><p xml:id="_eYyH6uE"><s xml:id="_DzBEWgW">In this paper, we explored the practical usage of adaptive and intelligent agents in personal mobile health intervention and developed an RL-based agent to optimize the strategy of adaptively delivering context-aware notifications.</s><s xml:id="_QJZFKTa">The simulation results showed that the policy learned by our RL agent is more efficient than manually defined strategies without context awareness.</s><s xml:id="_bE8SUen">In particular, our work made two contributions to perform this practical learning task without bothering users too much.</s><s xml:id="_b3YCkmz">First, when incorporating prior knowledge from historical data and psychological theories for optimizing the policy, our proposed dynamic Bayes network can handle empirical data with various context space and flexible target activity.</s><s xml:id="_znrHNVW">Second, we constrained notification frequency in a period and adapted an RL algorithm for this constraint.</s><s xml:id="_Karg6PZ">As far as we know, such constraint was never structurally studied and evaluated in a mHealth setting, our results provide evidence that it is essential to take the frequency restriction of certain actions into account in the learning process of RL.</s><s xml:id="_GtBfkWb">For future work, it would be interesting to examine the efficiency of various state-of-art RL algorithms considering this constraint.</s><s xml:id="_dJuGK4w">Also, the practical usage of our approach should be further evaluated in trials with real users.</s><s xml:id="_jF6R4GJ">We have conducted a small-scale feasibility study <ref type="bibr" target="#b33">[34]</ref>.</s><s xml:id="_RWfBzBD">Based on the initial results and learned lessons, we plan a longer study to evaluate the effectiveness of our pre-learned delivery strategy for comparable user groups.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2</head><label>2</label><figDesc><div><p xml:id="_WG6Quf9"><s xml:id="_CDEjEbj">Fig. 2 Topological structure and transition probabilities of our dynamic Bayesian network</s></p></div></figDesc><graphic coords="3,56.68,57.76,481.92,142.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc><div><p xml:id="_3rdFaYU"><s xml:id="_hjtYw4E">Fig. 4 Information of three episodes in the R (R-REIN-FORCE) agent.</s><s xml:id="_4P6nKr5">Each circle represents one decision point, marked by hour and weekday.</s><s xml:id="_K3kGPzh">Black on the left side means 'a notification', and black on the right side means 'a run'.</s><s xml:id="_Vcraefe">The color of a circle represents the context desirability for running.</s><s xml:id="_V8Y8s2S">While red and blue color correspond to the high and low desirability respectively, darker is more extreme</s></p></div></figDesc><graphic coords="6,184.25,57.76,360.00,192.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="2,56.68,57.76,481.92,109.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="4,117.64,287.78,360.00,206.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><graphic coords="5,56.68,57.76,481.92,193.44" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p xml:id="_r92FW2Z"><s xml:id="_VJ3N7zP">Journal of Medical Systems (2021) 45: 102</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p xml:id="_SvAyaCn"><s xml:id="_dZeaAux">https:// knmi. nl/ neder land-nu/ klima tolog ie/ uurge gevens (last access on Oct 15th</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p xml:id="_BxUSj77"><s xml:id="_XkeAfGz">2021)2 https:// github. com/ sw1989/ RLfor PAUL 3 https:// pytor ch. org/ 4 https:// github. com/ openai/ gym<ref type="bibr" target="#b4">5</ref> For details about the verification, see the supplementary information.</s></p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head xml:id="_U3AuyTP">Acknowledgements</head><p xml:id="_vfkg6Ha"><s xml:id="_77YyC8c">The authors thank the cooperator <rs type="institution">MYLAPS</rs> for providing the mobile application dataset, as well as anonymous reviewers for providing precious comments.</s></p></div>
			</div>
			<div type="funding">
<div xml:id="_gt3PF4R"><p xml:id="_vWduRkz"><s xml:id="_kJESX79">Funding This work is funded by <rs type="funder">Playful Data-driven Active Urban Living</rs> project under <rs type="funder">NWO</rs> and <rs type="funder">SIA</rs> grant <rs type="grantNumber">629.004.013</rs>.</s></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_AARRKnd">
					<idno type="grant-number">629.004.013</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_qKnY8Ty">Supplementary Information</head><p xml:id="_QRxgCP7"><s xml:id="_AASGFjJ">The online version contains supplementary material available at <ref type="url" target="https://doi.org/10.1007/s10916-021-01773-0">https:// doi. org/ 10. 1007/ s10916-021-01773-0</ref>.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_6Z63S8e">Declarations</head><p xml:id="_3GEq28a"><s xml:id="_ugx5GdB">Research involving human and animal participants This article does not contain any studies with human participants or animals performed by any of the authors.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_z4ruBVH">Conflicts of interest</head><p xml:id="_mBn8Zra"><s xml:id="_GfyUdTC">The authors declare that they have no conflict of interest.</s></p><p xml:id="_DcKq9yh"><s xml:id="_9P6eNst">Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made.</s><s xml:id="_AG9nHXN">The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material.</s><s xml:id="_Q3E2hfv">If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</s><s xml:id="_cURtSVc">To view a copy of this licence, visit <ref type="url" target="http://creativecommons.org/licenses/by/4.0/">http:// creat iveco mmons. org/ licen ses/ by/4. 0/</ref>.</s><s xml:id="_Dwya8nc">Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main" xml:id="_3Jz4cKE">A conceptual framework for adaptive preventive interventions</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Bierman</surname></persName>
		</author>
		<idno type="DOI">10.1023/b:prev.0000037641.26017.00</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_k3Y59xY">Prevention science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="185" to="196" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Collins, L.M., Murphy, S.A., Bierman, K.L.: A conceptual frame- work for adaptive preventive interventions. Prevention science 5(3), 185-196 (2004)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main" xml:id="_bXt4sUX">A systematic review of just-in-time adaptive interventions (Jitais) to promote physical activity</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hardeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Houghton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naughton</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12966-019-0792-7</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sAQfpxZ">International Journal of Behavioral Nutrition and Physical Activity</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hardeman, W., Houghton, J., Lane, K., Jones, A., Naughton, F.: A systematic review of just-in-time adaptive interventions (Jitais) to promote physical activity. International Journal of Behavioral Nutrition and Physical Activity 16(1), 31 (2019)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main" xml:id="_yhyvEPp">Just-in-time adaptive interventions (Jitais) in mobile health: key components and design principles for ongoing health behavior support</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nahum-Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Spring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Witkiewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_VuPsKbP">Annals of Behavioral Medicine</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="446" to="462" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Nahum-Shani, I., Smith, S.N., Spring, B.J., Collins, L.M., Witkiewitz, K., Tewari, A., Murphy, S.A.: Just-in-time adaptive interventions (Jitais) in mobile health: key components and design principles for ongoing health behavior support. Annals of Behavioral Medicine 52(6), 446-462 (2017)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main" xml:id="_T5N3eVz">Return of the Jitai: applying a justin-time adaptive intervention framework to the development of m-health solutions for addictive behaviors</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Flack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juarascio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_bDp9yrr">International journal of behavioral medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="673" to="682" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Goldstein, S.P., Evans, B.C., Flack, D., Juarascio, A., Manasse, S., Zhang, F., Forman, E.M.: Return of the Jitai: applying a just- in-time adaptive intervention framework to the development of m-health solutions for addictive behaviors. International journal of behavioral medicine 24(5), 673-682 (2017)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main" xml:id="_WTDeQWn">Assessing the availability of users to engage in just-in-time intervention in the natural environment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sarker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.1145/2632048.2636082</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qd5x8je">Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="909" to="920" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sarker, H., Sharmin, M., Ali, A.A., Rahman, M.M., Bari, R., Hossain, S.M., Kumar, S.: Assessing the availability of users to engage in just-in-time intervention in the natural environment. In: Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pp. 909-920 (2014)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main" xml:id="_M7BeKt2">Behavioral response to a just-in-time adaptive intervention (Jitai) to reduce sedentary behavior in obese adults: Implications for Jitai optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bond</surname></persName>
		</author>
		<idno type="DOI">10.1037/hea0000304</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rajPpz9">Health Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">S</biblScope>
			<biblScope unit="page">1261</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thomas, J.G., Bond, D.S.: Behavioral response to a just-in-time adaptive intervention (Jitai) to reduce sedentary behavior in obese adults: Implications for Jitai optimization. Health Psychology 34(S), 1261 (2015)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main" xml:id="_4UYFYcK">Encouraging physical activity in patients with diabetes: intervention using a reinforcement learning system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Feraru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kozdoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tennenholtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hochberg</surname></persName>
		</author>
		<idno type="DOI">10.2196/jmir.7994</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_NzM5DE5">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">338</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yom-Tov, E., Feraru, G., Kozdoba, M., Mannor, S., Tennenholtz, M., Hochberg, I.: Encouraging physical activity in patients with diabetes: intervention using a reinforcement learning system. Journal of medical Internet research 19(10), e338 (2017)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main" xml:id="_MdW4axs">A reinforcement learning based algorithm for personalization of digital, just-intime, adaptive interventions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gönül</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Namlı</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coşar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">İ</forename><forename type="middle">H</forename><surname>Toroslu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artmed.2021.102062</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_zVVSmmd">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">102062</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Gönül, S., Namlı, T., Coşar, A., and Toroslu, İ.H.: A reinforcement learning based algorithm for personalization of digital, just-in- time, adaptive interventions. Artificial Intelligence in Medicine 115, 102062 (2021)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main" xml:id="_eQ8N5kG">mhealth app using machine learning to increase physical activity in diabetes and depression: clinical trial protocol for the diamante study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Figueroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hernandez-Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cemballi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gomez-Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miramontes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yom-Tov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_2KJAsKb">BMJ open</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">34723</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Aguilera, A., Figueroa, C.A., Hernandez-Ramos, R., Sarkar, U., Cemballi, A., Gomez-Pathak, L., Miramontes, J., Yom-Tov, E., Chakraborty, B., Yan, X., et al.: mhealth app using machine learn- ing to increase physical activity in diabetes and depression: clini- cal trial protocol for the diamante study. BMJ open 10(8), e034723 (2020)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main" xml:id="_JaekKqd">Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Forman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kerrigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Butryn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Juarascio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ontañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Dallal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Crochiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moskow</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10865-018-9964-1</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_z4EmtG5">Journal of behavioral medicine</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="276" to="290" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Forman, E.M., Kerrigan, S.G., Butryn, M.L., Juarascio, A.S., Manasse, S.M., Ontañón, S., Dallal, D.H., Crochiere, R.J., Moskow, D.: Can the artificial intelligence technique of reinforce- ment learning use continuously-monitored digital data to optimize treatment for weight loss? Journal of behavioral medicine 42(2), 276-290 (2019)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main" xml:id="_YeDvGt8">My behavior: automatic personalized health feedback from user behaviors and preferences using smartphones</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.1145/2750858.2805840</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_yYGcwJP">Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</title>
		<meeting>the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="707" to="718" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rabbi, M., Aung, M.H., Zhang, M., Choudhury, T.: My behav- ior: automatic personalized health feedback from user behaviors and preferences using smartphones. In: Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, pp. 707-718. ACM (2015)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main" xml:id="_h4wQEMs">Smartphone applications to support weight loss: current perspectives</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Pfammatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Spring</surname></persName>
		</author>
		<idno type="DOI">10.2147/ahct.s57844</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_YeptXwb">Advanced health care technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Pellegrini, C.A., Pfammatter, A.F., Conroy, D.E., Spring, B.: Smartphone applications to support weight loss: current perspec- tives. Advanced health care technologies 1, 13 (2015)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main" xml:id="_pZCgmD5">Trial without error: Towards safe reinforcement learning via human intervention</title>
		<author>
			<persName><forename type="first">W</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlmüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ZC3qr8Y">Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems</title>
		<meeting>the 17th International Conference on Autonomous Agents and MultiAgent Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2067" to="2069" />
		</imprint>
	</monogr>
	<note type="raw_reference">Saunders, W., Sastry, G., Stuhlmüller, A., Evans, O.: Trial with- out error: Towards safe reinforcement learning via human inter- vention. In: Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp. 2067-2069 (2018)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main" xml:id="_BGpaDPJ">Personalizing mobile fitness apps using reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fukuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Flowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Castillejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aswani</surname></persName>
		</author>
		<idno type="DOI">10.2196/mhealth.9117</idno>
	</analytic>
	<monogr>
		<title level="s" xml:id="_t2cytEn">CEUR workshop proceedings</title>
		<imprint>
			<biblScope unit="volume">2068</biblScope>
			<date type="published" when="2018">2018</date>
			<publisher>NIH Public Access</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhou, M., Mintz, Y., Fukuoka, Y., Goldberg, K., Flowers, E., Kaminsky, P., Castillejo, A., Aswani, A.: Personalizing mobile fitness apps using reinforcement learning. In: CEUR workshop proceedings, vol. 2068. NIH Public Access (2018)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main" xml:id="_SGb8deB">Narrowing reinforcement learning: Overcoming the cold start problem for personalized health interventions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tabatabaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Halteren</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-03098-8_19</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GCGxur5">International Conference on Principles and Practice of Multi-Agent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="312" to="327" />
		</imprint>
	</monogr>
	<note type="raw_reference">Tabatabaei, S.A., Hoogendoorn, M., van Halteren, A.: Narrowing reinforcement learning: Overcoming the cold start problem for personalized health interventions. In: International Conference on Principles and Practice of Multi-Agent Systems, pp. 312-327. Springer (2018)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-021-05995-8</idno>
		<title level="m" xml:id="_RDBz4nQ">Intelligent pooling in thompson sampling for rapid personalization in mobile health</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tomkins, S., Liao, P., Yeung, S., Klasnja, P., Murphy, S.: Intel- ligent pooling in thompson sampling for rapid personalization in mobile health (2019)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main" xml:id="_wc6vaQa">Optimization of just-in-time adaptive interventions using reinforcement learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gonul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Namli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Sinaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cosar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Toroslu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-92058-0_32</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qKkkRS8">International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="334" to="341" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gonul, S., Namli, T., Baskaya, M., Sinaci, A.A., Cosar, A., Toroslu, I.H.: Optimization of just-in-time adaptive interven- tions using reinforcement learning. In: International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, pp. 334-341. Springer (2018)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main" xml:id="_fH8xG53">Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greenewald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Klasnja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3381007</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_wt3xjXc">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Liao, P., Greenewald, K., Klasnja, P., Murphy, S.: Personalized heartsteps: A reinforcement learning algorithm for optimizing physical activity. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4(1), 1-22 (2020)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main" xml:id="_dk48UcB">Online contextual multi-armed bandits for mobile health interventions: A case study on emotion regulation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ameko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Beltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boukhechba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Teachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383313.3412244</idno>
	</analytic>
	<monogr>
		<title level="m" xml:id="_KpBf3eF">Fourteenth ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="249" to="258" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ameko, M.K., Beltzer, M.L., Cai, L., Boukhechba, M., Teachman, B.A., Barnes, L.E.: Online contextual multi-armed bandits for mobile health interventions: A case study on emotion regulation. In: Fourteenth ACM Conference on Recommender Systems, pp. 249-258 (2020)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main" xml:id="_6AAyaj4">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sutton, R.S., Barto, A.G.: Reinforcement learning: An introduc- tion. MIT press (2018)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main" xml:id="_xNc469H">Why option generation matters for the design of autonomous e-coaching systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kamphorst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalis</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00146-013-0532-5</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_XDpr2Qs">AI &amp; SOCIETY</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="88" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kamphorst, B., Kalis, A.: Why option generation matters for the design of autonomous e-coaching systems. AI &amp; SOCIETY 30(1), 77-88 (2015)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main" xml:id="_MmU5n84">Changing behavior by memory aids: A social psychological model of prospective memory and habit development tested with dynamic field data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tobias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_J4yjfjz">Psychological review</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="408" to="438" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tobias, R.: Changing behavior by memory aids: A social psy- chological model of prospective memory and habit development tested with dynamic field data. Psychological review 116(2), 408-438 (2009)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main" xml:id="_r9xRYxb">Theory integration for lifestyle behavior change in the digital age: An adaptive decisionmaking framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lakens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Ijsselsteijn</surname></persName>
		</author>
		<idno type="DOI">10.2196/17127</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_WzEnGdK">Journal of Medical Internet Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">17127</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhang, C., Lakens, D., IJsselsteijn, W.A.: Theory integration for lifestyle behavior change in the digital age: An adaptive decision- making framework. Journal of Medical Internet Research 23(4), e17127 (2021)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main" xml:id="_YtxaX7T">The precise time course of retention</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wenzel</surname></persName>
		</author>
		<idno type="DOI">10.1037//0278-7393.25.5.1161</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_X6wyAFE">Journal of Experimental Psychology: Learning, Memory, and Cognition</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1161" to="1176" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rubin, D.C., Hinton, S., Wenzel, A.: The precise time course of retention. Journal of Experimental Psychology: Learning, Mem- ory, and Cognition 25(5), 1161-1176 (1999)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main" xml:id="_eC9g3rZ">The foundations of statistics</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Savage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Courier Corporation</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Savage, L.J.: The foundations of statistics. Courier Corporation (1972)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main" xml:id="_KnsAHUP">Theory of games and economic behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_SVnj5YP">Bull. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="498" to="504" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Von Neumann, J., Morgenstern, O.: Theory of games and eco- nomic behavior. Bull. Amer. Math. Soc 51(7), 498{504 (1945)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main" xml:id="_J5HYwYj">Momentary assessment of contextual influences on affective response during physical activity</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Dunton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leventhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_MWKDHyX">Health Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1145</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dunton, G.F., Liao, Y., Intille, S., Huh, J., Leventhal, A.: Momen- tary assessment of contextual influences on affective response dur- ing physical activity. Health Psychology 34(12), 1145 (2015)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main" xml:id="_93cd6TK">Dynamics of multiplegoal pursuit</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Louro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pieters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zeelenberg</surname></persName>
		</author>
		<idno type="DOI">10.1037/0022-3514.93.2.174</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_rwGW768">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">174</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Louro, M.J., Pieters, R., Zeelenberg, M.: Dynamics of multiple- goal pursuit. Journal of personality and social psychology 93(2), 174 (2007)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main" xml:id="_94ed24y">Dynamic bayesian networks: A state of the art</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mihajlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petkovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente Document Repository</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">Mihajlovic, V., Petkovic, M.: Dynamic bayesian networks: A state of the art. University of Twente Document Repository (2001)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main" xml:id="_vpeke3N">Advanced calculus</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="DOI">10.1017/s0025557200027741</idno>
		<imprint>
			<date type="published" when="1952">1952</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Kaplan, W.: Advanced calculus. Pearson Education India (1952)</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main" xml:id="_bkGgHF9">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KQycaCP">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Williams, R.J.: Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8(3- 4), 229-256 (1992)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main" xml:id="_ETzpmTf">Clipped action policy gradient</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Maeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ANBWBqe">Proceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<meeting>the 35th International Conference on Machine Learning, Machine Learning Research<address><addrLine>Stockholm Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR, Stockholmsmässan</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1597" to="1606" />
		</imprint>
	</monogr>
	<note type="raw_reference">Fujita, Y., Maeda, S.i.: Clipped action policy gradient. In: Pro- ceedings of the 35th International Conference on Machine Learning, Proceedings of Machine Learning Research, vol. 80, pp. 1597-1606. PMLR, Stockholmsmässan, Stockholm Sweden (2018)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main" xml:id="_dwKfkk7">What are good situations for running? a machine learning study using mobile and geographical data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scheider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sporrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deutekom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Timmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kröse</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpubh.2020.536370</idno>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Vmf2HCM">Frontiers in Public Health</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">985</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S., Scheider, S., Sporrel, K., Deutekom, M., Timmer, J., Kröse, B.: What are good situations for running? a machine learn- ing study using mobile and geographical data. Frontiers in Public Health 8, 985 (2021)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main" xml:id="_qMGZjAQ">Reinforcement learning to send reminders at right moments in smartphone exercise application: A feasibility study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sporrel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>De Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ettema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nibbeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deutekom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kröse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HheYKRA">International Journal of Environmental Research and Public Health</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">6059</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Wang, S., Sporrel, K., van Hoof, H., Simons, M., de Boer, R.D., Ettema, D., Nibbeling, N., Deutekom, M. and Kröse, B.: Reinforce- ment learning to send reminders at right moments in smartphone exercise application: A feasibility study. International Journal of Environmental Research and Public Health, 18(11), 6059 (2021)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
