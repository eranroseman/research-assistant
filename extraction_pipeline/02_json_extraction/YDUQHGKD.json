{
  "paper_id": "YDUQHGKD",
  "title": "A Comprehensive Typology for Program Evaluation",
  "abstract": "Chen begins his discussion by challenging the basic formative/ summative dichotomy introduced by Scriven, arguing that it has been confusing. He analyzes Scriven's 1991 paper, \"Beyond ...\" which, he feels, clarifies some of the prior confusions, but creates others. Based on what he sees as limitations in Scriven's dichotomy, Chen offers his own conceptual framework that he argues allows more complete classification of evaluation types. He proposes a typology formed by crossing two evaluation functions (\"improvement\" and \"assessment\") with two program stages (\"process\" and \"outcome\"), resulting in four basic types of evaluation that he sees as comprehensive enough to encompass the breadth of evaluations that practitioners encounter, while providing a framework that can accommodate mixed-type evaluations.",
  "year": 1990,
  "date": "1990",
  "journal": "Evaluation Practice",
  "publication": "Evaluation Practice",
  "authors": [
    {
      "forename": "Huey-Tsyh",
      "surname": "Chen",
      "name": "Huey-Tsyh Chen",
      "affiliation": "Department of Sociology , University of Akron , Akron , OH 44325. \n\t\t\t\t\t\t\t\t Department of Sociology \n\t\t\t\t\t\t\t\t University of Akron \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 44325. \n\t\t\t\t\t\t\t\t\t Akron \n\t\t\t\t\t\t\t\t\t OH"
    }
  ],
  "doi": "10.1177/002194369002700112",
  "sections": [
    {
      "title": "INTRODUCTION",
      "text": "Taxonomies represent an important tool to any discipline for classifying phenomena and activities, guiding research, facilitating communication, and developing knowledge. The dis- tinction between formative and summative evaluation has been regarded as fundamental to program evaluation and has been widely used in the research and literature since Scriven introduced them in 1967  (Shadish, et al., 1991) . In a more recent article,  Scriven (1991)  defined formative evaluation as &dquo;... evaluation designed, done, and intended to support the process of improvement, and normally commissioned or done by, and delivered to someone who can make improvements  (p. 20) .&dquo; In the same article, Scriven defined summative evalu- ation as &dquo;the rest of evaluation: in terms of intentions, it is evaluation done for, or by, any observers or decision makers (by contrast with developers) who need valuative conclusions for any other reasons besides development  (ibid, p. 20) .&dquo; According to Scriven, the best way to illustrate the distinction between formative and summative evaluation is through the anal-. ogy given by Robert Stake: &dquo;When the cook tastes the soup, that's formative evaluation; when the guest tastes it, that's summative evaluation (ibid, p. 19).&dquo; Despite the popularity of this distinction, there has been some confusion in the litera- ture-and in practice-with applying these concepts. Although  Scriven (ibid)  attempts to clarify this confusion, his discussion tends to be highly provocative. His arguments create more controversy. The persistent problems attending the formative/summative distinction may be indicative of its limitations. This article attempts to examine the underlying sources of the difficulties in the formative/summative distinction and proposes a broader conceptual framework for dealing with these problems. SOME LIMITATIONS IN THE FORMATIVE/SUMMATIVE DISTINCTION One of the major difficulties with the formative/summative dichotomy is that the terms, as cur- rently defined, do not cover many relevant, important kinds of evaluations. As a result, there are discrepancies between the distinction as defined by Scriven and the actual usage of these concepts, even in Scriven's own work (e.g.,  Scriven, 1991) . Since Robert Stake's soup tasting analogy is regarded by Scriven as the best way of illustrating the formative/summative distinc- tion, I will use it here to illustrate the reasons why the scope of the distinction has limits.\n\nAccording to Stake's analogy, when &dquo;the cook tastes the soup,&dquo; that act represents for- mative evaluation because tasting occurs during the cooking process for the purpose of improving the soup. For example, if the cook tastes the soup and feels that it needs salt, s/he adds the salt in order to improve the flavor. Hence Scriven's concept of formative evaluation is essentially a kind of process evaluation with an emphasis on improvement.\n\nIn my view, this concept of formative evaluation has some shortcomings. The cook does not always taste the soup for the purpose of improvement. The cook may taste the soup to determine whether the soup is good enough to serve to the guests at all, especially if it is a new recipe. Upon testing the soup, s/he may feel it is good enough to serve to the guests. Alterna- tively, s/he may decide that the soup is awful and not worth improving, and eliminate the soup from the menu. In this case, the cook has not tasted the soup for the purpose of improvement as Scriven and Stake see it, but to reach a valuative conclusion.\n\nTo illustrate, a Chinese cook who is a friend of mine once tried to prepare a new and dif- ficult dish called Peking Duck for his restaurant. Tasting his product, he found that the skin of the duck was not as crispy as it was supposed to be, nor the meat as flavorful. Convinced that Peking Duck was beyond his capability as a chef, he decided not to prepare the dish again. Thus, in both examples, when the cook tastes the product for valuative assessment, it is not a formative evaluation, nor is it, exactly, a summative evaluation.\n\nAs these examples show, Scriven's formative/summative distinction is narrow.\n\nReturning to Stake's analogy, when &dquo;the guest tastes the soup,&dquo; this is regarded as a sum- mative evaluation since, according to Scriven's definition, the guest provides a conclusive opinion of the soup. Guests' opinions could be used by the restaurant owner or the cook as a means of determining whether to continue serving the soup to guests in the future. This con- cept of summative evaluation also has shortcomings. For example, the opinion of guests who taste the soup is not always limited to the purpose of determining the merit. It is highly likely that the restaurant owner or the cook would elicit opinions from the guests for the purpose of improving the soup in the future. In this case, if we go by Scriven's formulation, the guests who taste the soup are performing neither summative evaluation nor formative evaluation. This also demonstrates the limitations in Scriven's formative/summative distinction. That Stake's analogy, deemed by Scriven to be the best way of illustrating the formative/ summative distinction, still leads to confusion and problems in classifying relevant evaluation activities, indicates a need for a broader conceptual framework to provide a more complete classification of evaluation activities."
    },
    {
      "title": "A CONCEPTUAL FRAMEWORK FOR BASIC EVALUATION TYPES",
      "text": "Based on  Scriven's  distinction, a broader conceptual framework can be developed to classify basic evaluation types. The conceptual framework is a basic typology made of two domains: the program stage focused on by the evaluation, and the function of the evaluation. In terms of program stages, evaluation can focus on program process (such as studying program implementation) and/or on program outcome (such as examining the impact of the program on its goals). In terms of evaluation functions  (Chen, 1990) , evaluation can serve the function of assessment, as in judging the merit or worth of a program, and/or the function of improve- ment, including both instrumental and conceptual use. In Scriven's definition, the dichotomy consists of only process evaluation, serving a program improvement function (formative evaluation); and outcome evaluation, serving a merit assessment function (summative.evalu- ation). As will be explained later, however, it is important to note that process evaluation is not necessarily limited to program improvement purposes. Similarly, outcome evaluation is not necessarily limited to merit assessment purposes. Scriven's distinction excludes these important evaluation phenomena. A more comprehensive framework can be developed by taking the full range of program stages and evaluation functions into consideration as shown in Figure  1 . This typology consists of the following four basic types of evaluation: processimprovement evaluation, process-assessment evaluation, outcome-improvement evaluation, and outcome-assessment evaluation. 1. Process-Improvement Evaluation '\n\nProcess-improvement evaluation refers to providing information on relative strengths/ weaknesses in program structure or implementation processes for the purpose of improving the program (instrumental use), or for enlightening decision making in general (conceptual use). This type of evaluation is broader than Scriven's formative evaluation, since formative evaluation only focuses on instrumental use. Process-improvement evaluation usually does not provide a direct overall assessment regarding the success or failure of program implementation. For example, a process-improvement evaluation of a family planning program may indicate that more married couples can be persuaded to utilize birth control devices in an underdeveloped country if the service providers or counselors are local people, rather than outside health workers. This information does not provide a direct overall judgment of the merits of program implementation. Decision makers and program designers can use the infor- mation, however, to strengthen the program by training more local people to become service providers or counselors (instrumental use). Alternatively, decision makers may use the evalu- ation information in future program designs in other settings (conceptual use)."
    },
    {
      "title": "Process-Assessment Evaluation",
      "text": "This type of evaluation is conducted for the purpose of judging the merits of the implementation process. While not included in Scriven's distinction, process-assessment evaluation is frequently used. Unlike process-improvement evaluation, process-assessment evaluation attempts to judge whether the implementation of a program is a success or a failure, appropri- ate or inappropriate. A good example of process-assessment evaluation is quality control, as when a product is rejected in the production line when it fails to meet certain criteria.\n\nVivid examples of process-assessment evaluation are the investigative reports seen on popular TV programs, such as 60 Minutes and 20120. In these programs, reporters use hidden cameras to document whether services delivered by such places as psychiatric hospitals, nurs- ing homes, child care centers, restaurants, and auto repair shops are appropriate."
    },
    {
      "title": "Outcome-Improvement Evaluation",
      "text": "This type of evaluation assesses the relative strengths and/or weaknesses of program ele- ments or implementation processes as they affect program outcomes, in both the instrumental or conceptual sense. The evaluation information is useful to a program in achieving its goals, but it does not provide an overall judgment of program effectiveness or ineffectiveness, as in summative evaluation. For example, an HIV prevention program may consist of three treat- ment elements, such as a video session, a group session, and a lecture. An outcome-improvement evaluation may be conducted to determine which elements are crucial or more important than others in affecting the outcome. The information is useful for emphasizing successful elements and correcting the unsuccessful ones, but it stops short of judging the overall effec- tiveness of the program.\n\nIn another example, a service agency may have two types of social workers, high labor- intensive case managers and low labor-intensive care managers. An evaluator can apply an outcome-improvement evaluation to determine which kind of social worker is more cost effective for the agency. Similarly, a company surveying consumers for their opinions on the strengths and weaknesses of various elements or dimensions of a product is engaging in another example of outcome-improvement evaluation. This type of evaluation provides infor- mation that is useful to the company for improving the product in the market, but does not provide an overall valuative conclusion about the product."
    },
    {
      "title": "Outcome-Assessment Evaluation",
      "text": "The purpose of an outcome-assessment evaluation is to provide an overall judgment of a program in terms of its merit or worth. Scriven's definition of summative evaluation is synon- ymous with this category. A typical example of outcome-assessment evaluation is one that determines whether a program has achieved its goals. Another example frequently used by Scriven is the product evaluations published in Consumer Reports.\n\nThis new typology in Figure  1  expands the scope of program evaluation, compared with Scriven's distinction, and eliminates some of the difficulties found in the soup tasting analogy. Formerly, when the cook tasted the soup for judgment purposes, it did not fit into the original formative/summative distinction. But it could now be resolved and classified as a process- assessment evaluation. Similarly, when the guest tastes the soup for improvement purposes, the action could now be classified as an example of outcome-improvement evaluation.\n\nAdditionally, while Scriven's distinction mainly concerns instrumental use, the new typology covers both instrumental and conceptual usage. Furthermore, the typology also shat- ters the myth that process evaluation is always a kinder, gentler kind of evaluation in which evaluators are afraid to make tough judgments about the program. Process-improvement eval- uation may be kinder and gentler, but not necessarily process-assessment evaluation. For example, TV investigative reports that expose the wrong-doings in a psychiatric hospital, auto shop, restaurant, or day care center, have resulted in changes in service delivery, the firing of managers and employees, and even the closing of the agencies or businesses in question. In such cases, process evaluations were tougher than many summative evaluations in terms of critical assessment and impact.\n\nMoreover, the new typology also shatters the myth that outcome evaluation must always be carried out as a &dquo;macho, cowboy type&dquo; of evaluation that threatens program providers and fails to offer any information about the program. An outcome-assessment evaluation may be a macho, cowboy type of evaluation, providing only a &dquo;go/no-go&dquo; assessment, but the out- come-improvement evaluation can provide useful information for enhancing the effectiveness of a program without creating a great threat to its existence. For example, the survival of a program is not threatened by an outcome-improvement evaluation that indicates that program effectiveness can be improved by modifying some treatment elements or implementation procedures."
    },
    {
      "title": "WHICH TYPE OF EVALUATION IS PREFERABLE?",
      "text": "One of the most persistent debates in program evaluation is whether summative evaluation is preferred over formative evaluation, or vice versa (e.g.,  Cronbach, et al. 1980; Scriven, 1991) . On the one hand, Cronbach (1980) prefers formative evaluation, arguing that evaluation should contribute to program improvement by facilitating the discussions of alternative policy options and clarifying important policy problems and issues. Evaluations providing &dquo;go/no- go&dquo; information, it is argued, are rarely used in policy processes. On the other hand, Scriven  (1981)  prefers summative evaluation, basically arguing that society requires a discipline like evaluation to determine if a product or program is good or bad, so as to effectively allocate social resources; detect waste, fraud, or incompetence; and to generally meet social and con- sumer needs. Furthermore, because most product and personnel evaluations are summative, it follows (in Scriven's mind) that other types of evaluation should be summative in nature  (Scriven, 1991) . ' Both Cronbach and Scriven are persuasive when they articulate the importance of their favored evaluation type, but neither is very convincing when they devalue the opposite type of evaluation. For example, Cronbach effectively argues the importance of formative evalua- tion in enlightening the policy process  (Cronbach, et al. 1980) . His attack on summative eval- uation, which argues that &dquo;evaluations are used almost entirely in a formative manner when they are used,&dquo; is over-stated and problematic. Scriven (1991)  effectively defuses Cronbach's s argument by pointing out that there are many instances of summative evaluation being used, as can be seen in repercussions from numerous product evaluations, ETS evaluations, personnel evaluations, and reports from the Congressional Budget  Office. Similarly, Scriven is convincing when he argues the importance of summative evalua- tion by saying that valuing is essential in program evaluation  (1991) . By comparison, his crit- icism of formative evaluation is not convincing. Different stakeholder groups have different evaluation needs. Program evaluation would largely limit its usefulness if it were to ignore the needs of program managers and implementors for program improvement. In fact, it would be detrimental to the discipline of program evaluation if our attentions were overwhelmingly dominated by summative evaluation. Program evaluation would become a narrow, monoto- nous, and rigid discipline lacking in creativity and dynamics if all evaluation projects resem- bled those in Consumer Reports.\n\nThere is no need to advocate one type of evaluation by devaluing another. Each type is useful for different program stakeholders under different circumstances. Without knowing the context, it is pointless to argue which type of evaluation is more important. It would be more beneficial for us to examine the contextual factors that enable each type of evaluation to be most effective. The influence of context is my next point of contention."
    },
    {
      "title": "RELATIONSHIPS AMONG BASIC TYPES OF EVALUATION",
      "text": "In my reading of Scriven's 1991 article, he first attests that formative and summative evalua- tion are two different types of evaluation, and then claims that the first of the 10 great fallacies is to presume that formative and summative evaluations are intrinsically different. Scriven asserts that the line between formative and summative evaluations can only be drawn in refer- ence to the context. Scriven does not specify what this context is, but he does offer two inter- esting examples: a book review and an annual checkup. He argues that a book review is a summative evaluation, although if the same review is used to improve the next edition of the book, then it becomes a formative evaluation. Similarly, an annual health checkup could be formative or summative depending on the result of the checkup.\n\nI find these statements intriguing but confusing. If there is no intrinsic difference between formative and summative evaluation, then one formative evaluation could also be used to judge the merit of program accomplishments, as in summative evaluation. For exam- ple, a formative evaluation of an education program may find that students like the program, but that the program could be improved by reinforcing teachers' commitments to the newly proposed curriculum. Scriven's statements about the influence of context suggest that the stu- dents' and teachers' attitudes alone could lead directly to an overall judgment about the program's success. Few of us would regard this judgment as valid.\n\nIt is intriguing to note, however, how Scriven uses a book review and an annual health checkup to demonstrate how a summative evaluation could be used for formative purposes. My examination of the these two examples reveals, however, that they are nei- ther &dquo;purely&dquo; summative nor &dquo;purely&dquo; formative under Scriven's distinction. In reviewing a book, a reviewer offers an overall judgment on the merit of the book. In order to con- vince others that her/his judgment is sound, however, the reviewer also needs to explain how and why she/he has reached the overall conclusion. This generally entails comment- ing on which parts of the book are interesting or make important contributions, and which parts are problematic. Some reviewers even provide concrete suggestions for revising the book. In other words, a book review often contains information that serves both assess- ment and improvement functions at the program outcome stage. According to the typology in Figure  1 , such a review is no longer a basic type of evaluation, but a mixture of outcome-assessment evaluation (for readers) and an outcome-improvement evaluation (for the author and the book company).\n\nSimilarly, an annual health checkup appears at first glance to be a summative evaluation.\n\nIn order to conclude a client's health status, however, a number of tests and examinations had to have been performed. The tests and examinations provide useful information on which bodily parts or functions may need medical treatment for overall improvement of health in the future. Thus the annual checkup is also a mixture of outcome-assessment evaluation and out- come-improvement evaluation. Since a book review and an annual checkup are not purely formative or purely summa- tive, these two examples cannot be used as evidence to support Scriven's argument that there is no intrinsic difference between formative and summative evaluation. Scriven's arguments are not without merit, however; they illustrate the need for mixed types of evaluation, as explained in the next section.\n\nAlthough one basic type of evaluation should not be used as a substitute for another, it is possible to use one as a proxy or surrogate for another under some circumstances. This kind of application, however must be applied with great caution  (Chen, 1990) . For example, if out- come-assessment evaluation is not available, process-assessment evaluation could, with cau- tion, be used as a preview or early warning system of an outcome-assessment evaluation. More specifically, a process-assessment evaluation can serve as a necessary precursor to an outcome- assessment evaluation. In other words, if a process-assessment evaluation indicates that a pro- gram fails in implementation, it would imply that an outcome assessment evaluation would sim- ilarly find the program lacking. Implementation failure foreshadows program failure.\n\nOn the other hand, if a process-assessment evaluation finds that a program has been implemented successfully, this does not guarantee that the program would be deemed effec- tive by an outcome-assessment evaluation. The process-assessment evaluation only suggests that the program is working well in terms of implementation. An outcome-assessment evalu- ation of the program is still needed to assess whether the program has succeeded or failed."
    },
    {
      "title": "THE DEVELOPMENT OF MIXED TYPE EVALUATIONS",
      "text": "The typology in Figure  1  implies that the four basic types of evaluation each serves a single function (i.e., improvement or assessment), during discrete program stage (i.e., process or out- Figure  2 . Basic and Mixed Types of Evaluation come). In practice, however, program evaluation over-rides and sometimes mixes these basic types. A comprehensive typology showing the relationship of the mixed type evaluations with the basic types is seen in Figure  2 .\n\nIn Figure  2 , Type 1 to Type 4 are basic types of evaluation; Type 5 to Type 10 are mixed types of evaluation. The arrows in the figure identify the basic types that make up a particular mixed type of evaluation. For example, a comprehensive process evaluation (Type 5) is a combination of a process-improvement evaluation (Type 1) and a process-assessment evalua- tion (Type 2).\n\nRather than simply involving two independent basic evaluations, mixed type evaluations require the meaningful integration of two or more basic types. One general integration strat- egy is called structural integration, where the evaluation design contains the mechanisms for linking multiple program stages or evaluation functions. One such mechanism is dimension elaboration. In this mechanism, crucial dimensions related to a program are elaborated in such detail that the evaluation information can be used for both program assessment and improvement purposes. A good example to illustrate dimension elaboration is evaluation carried out by the Occupational Safety and Health Administration (OSHA). These evaluations are com- prehensive process evaluations (Type 5), in which dimension elaboration is used as a mecha- nism to link a process-assessment evaluation and a process-improvement evaluation. OSHA inspectors may evaluate a factory to determine whether or not the factory passes or fails in a checklist of safety and health rules and regulations. The checklist is so specific, however, that these inspections can also be used for improvement. For those who fail the inspection, the inspector provides information concerning areas that need correction to satisfy safety stan- dards. Other regulatory agencies, such as the Environmental Protection Agency, also perform a similar type of evaluation.\n\nIn addition, normative evaluations (Chen, 1990) in the theory-driven evaluation perspective are also a comprehensive process evaluation (Type 5), using the mechanisms of dimen- sion elaboration to link basic types of evaluation. This kind of evaluation provides a comprehensive assessment of major dimensions of program implementation. The results of this type of evaluation could be used to judge the merits of program implementation and to strengthen the implementation process  (Chen,  in press; a).\n\nAnother mechanism of structural integration is causal elaboration. Two basic types are linked into a mixed type through causal relationships. For example, the intervening mecha- nism evaluation  (Chen, 1990 ) is a comprehensive outcome evaluation (Type 6) that integrates both outcome-improvement evaluation (Type 3) and outcome-assessment evaluation (Type 4). This type of evaluation elaborates causal mechanisms underlying a program so that it examines not only whether the program has an impact, but why, and which mechanisms influence program success or failure (Chen, in press; b).\n\nAnother general integrating strategy is called sequential integration, because it links dif- ferent types of evaluations in sequential order. That is, one type of evaluation has to be fin- ished before another type of evaluation starts. Mixed types 7, 8, 9, and 10 in Figure  2  often use sequential integration. When using comprehensive assessment evaluation (Type 9), an evalu- ator first evaluates the merits of program implementation and then evaluates program effec- tiveness. Information from implementation and outcome assessment provides a comprehensive merit judgment of the overall program. In addition, by using the sequential and structural integration, the typology in Figure  2  can be extended to cover additional mixed types that are made up of three or four basic types.\n\nBecause evaluations often serve multiple stakeholders and diverse purposes over time, mixed type evaluations can be particularly helpful. The following circumstances provide examples of this usefulness:\n\n1. When evaluation is not a one-shot study, but used as a mechanism for continu- ous feedback to program stakeholders. As the program progresses and changes, the need for evaluation is different from time to time. Under such conditions, mixed types of evaluation allow the evaluator flexibility in meeting the changing needs.\n\n2. When evaluation has to respond to multiple stakeholder groups with different needs. Some stakeholders, such as clients and funding agencies, may require infor- mation on merit assessment; others, such as program implementors, may need information on program improvement. Under this condition, mixed-type evalua- tions provide information to meet the diversity of needs. 3. When stakeholders require comprehensive information about the program. Chen (1994)  argued that, due to the complexity of the program implementation pro- cesses, the effectiveness of the labor intensity programs cannot be appropriately assessed without answering both of the following two questions: &dquo;Does the pro- gram achieve its goals?&dquo; and &dquo;How does the program achieve its goals?&dquo; Mixed- type evaluations provide such comprehensive information."
    },
    {
      "title": "CONCLUSIONS AND DISCUSSIONS",
      "text": "In the last three decades, evaluation theorists have made invaluable contributions to program evaluation (e.g.,  Shadish, et al., 1991) . As pioneers attempting to get their messages across,' these theorists tend to make statements, or take positions that are bold, provocative, and even extreme. Such statements can generate confusion and controversy, but they can also offer rich theoretical implications. Accordingly, we need to think critically about theoretical contribu- tions so as to discern the strengths and limitations in their arguments. This enables us to expand our knowledge of program evaluation.\n\nIn proposing this perspective, this paper does not aim to devalue any important contribu- tions made by  Scriven. The formative-summative distinction is both inspiring and useful. In my view, however, program evaluation has outgrown Scriven's distinction. There is a need for a more comprehensive taxonomy to accommodate the range of evaluations being con- ducted. This paper proposes an alterative typology for meeting this need. The typology iden- tifies additional evaluation types, such as process-assessment evaluation, outcome- improvement evaluation, and incorporates mixed-type evaluations, which together may potentially enhance evaluators' options, flexibility, and dynamics in conducting program evaluation."
    },
    {
      "text": "Figure 1. Basic Types of Evaluation"
    }
  ],
  "references": [
    {
      "title": "Theory-driven evaluations",
      "authors": [
        "H Chen"
      ],
      "year": 1990,
      "doi": "10.1177/002194369002700112"
    },
    {
      "title": "Theory-driven evaluations: Need, difficulties, and options",
      "authors": [
        "H Chen"
      ],
      "year": 1994,
      "doi": "10.1177/109821409401500109"
    },
    {
      "title": "Normative evaluation of an anti-drug abuse program",
      "authors": [
        "H Chen"
      ],
      "doi": "10.1016/s0149-7189(96)00050-x"
    },
    {
      "title": "Evaluating the process and outcome of a garbage reduction program in Taiwan",
      "authors": [
        "H Chen"
      ],
      "doi": "10.1177/0193841x9702100102"
    },
    {
      "title": "Toward reform ofprogram evaluation",
      "authors": [
        "L Cronbach",
        "S Ambron",
        "S Dornbusch",
        "R Hess",
        "R Hornik",
        "D Phillips",
        "D Walker",
        "S Weiner"
      ],
      "year": 1980
    },
    {
      "title": "Beyond formative and summative evaluation",
      "authors": [
        "M Scriven"
      ],
      "year": 1991
    },
    {
      "title": "Product evaluation",
      "authors": [
        "M Scriven"
      ],
      "year": 1981
    },
    {
      "title": "Foundations of program evaluation",
      "authors": [
        "W Shadish",
        "T Cook",
        "Jr",
        "L Leviton"
      ],
      "year": 1991
    }
  ],
  "num_references": 8
}
