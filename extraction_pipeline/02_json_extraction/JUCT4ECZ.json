{
  "paper_id": "JUCT4ECZ",
  "title": "Multi task opinion enhanced hybrid BERT model for mental health analysis",
  "abstract": "Understanding the nuanced emotions and points of view included in user-generated content remains challenging, even though text data analysis for mental health is a crucial instrument for assessing emotional well-being. Most current models neglect the significance of integrating viewpoints in comprehending mental health in favor of single-task learning. To offer a more thorough knowledge of mental health, in this study, we present an Opinion-Enhanced Hybrid BERT Model (Opinion-BERT), built to handle multi-task learning for simultaneous sentiment and status categorization. With the help of TextBlob and SciPy, we extracted opinions and dynamically constructed new opinion embeddings to complement the pre-trained BERT model. Using a hybrid architecture, these embeddings are integrated with the contextual embeddings of BERT, whereby the CNN and BiGRU layers collected local and sequential characteristics. This combination helps our model to identify and categorize user status and attitudes from the text more accurately, which leads to more accurate mental health assessments. When we compared the performance of Opinion-BERT to some baseline models, including BERT, RoBERTa, and DistilBERT, we found that it performed much better. Opinion-enhanced embeddings are crucial for improving performance, as demonstrated by our multi-task learning framework's 96.77% sentiment classification accuracy of 94.22% status classification accuracy. This work provides a more nuanced understanding of emotions and psychological states by demonstrating the potential of combining opinion and sentiment data for mental health analysis in a multi-task learning environment.",
  "year": 2016,
  "date": "2016",
  "journal": "Lancet Psychiatr",
  "publication": "Lancet Psychiatr",
  "authors": [
    {
      "forename": "Md",
      "surname": "Hossain",
      "name": "Md Hossain",
      "affiliation": "1  Department of Computer Science and Engineering , Bangladesh University of Business and Technology , Dhaka 1216 , Bangladesh. \n\t\t\t\t\t\t\t\t Department of Computer Science and Engineering \n\t\t\t\t\t\t\t\t Bangladesh University of Business and Technology \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 1216 \n\t\t\t\t\t\t\t\t\t Dhaka \n\t\t\t\t\t\t\t\t\t Bangladesh"
    },
    {
      "forename": "Md",
      "surname": "Hossain",
      "name": "Md Hossain"
    },
    {
      "forename": "M",
      "surname": "Mridha",
      "name": "M Mridha",
      "affiliation": "1  Department of Computer Science and Engineering , Bangladesh University of Business and Technology , Dhaka 1216 , Bangladesh. \n\t\t\t\t\t\t\t\t Department of Computer Science and Engineering \n\t\t\t\t\t\t\t\t Bangladesh University of Business and Technology \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 1216 \n\t\t\t\t\t\t\t\t\t Dhaka \n\t\t\t\t\t\t\t\t\t Bangladesh"
    },
    {
      "forename": "Mejdl",
      "surname": "Safran",
      "name": "Mejdl Safran",
      "affiliation": "3  Research Chair of Online Dialogue and Cultural Communication , Department of Computer Science , College of Computer and Information Sciences , King Saud University , 11543 Riyadh , Saudi Arabia. \n\t\t\t\t\t\t\t\t Research Chair of Online Dialogue and Cultural Communication \n\t\t\t\t\t\t\t\t Department of Computer Science \n\t\t\t\t\t\t\t\t College of Computer and Information Sciences \n\t\t\t\t\t\t\t\t King Saud University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 11543 \n\t\t\t\t\t\t\t\t\t Riyadh \n\t\t\t\t\t\t\t\t\t Saudi Arabia"
    }
  ],
  "doi": "10.1038/s41598-025-86124-6",
  "arxiv": "arXiv:1810.04805",
  "keywords": [
    "Opinion-BERT",
    "Opinions embedding",
    "Hybrid BERT",
    "Mental health sentiment analysis",
    "Multi-task learning"
  ],
  "sections": [
    {
      "title": "Literature review",
      "text": "The increased availability of textual data from social media, online forums, and patient records has drawn considerable interest in the use of natural language processing (NLP) in mental health studies in recent years. Traditional machine learning techniques have been the mainstay of early approaches, but the development of deep learning models such as BERT, RoBERTa, and GPT, has made substantial progress in the field. However, these models frequently fail to consider the complex role that feelings and views play in texts on mental health. This overview of the literature looks at multi-task learning models, sentiment analysis, and mental health detection, emphasizing the significant developments and gaps that our Opinion-Enhanced Hybrid BERT model seeks to fill.\n\nMachine learning and NLP techniques are increasingly being applied to early mental health diagnosis through text analysis on various platforms. Singh et al.  12  explored SMS-based mental health assessments using classifiers such as decision trees, random forest, and logistic regression, with logistic regression achieving 93% accuracy. Inamdar et al.  13  focused on detecting mental stress in Reddit posts, using ELMo and BERT embeddings, achieving an 0.76 F1 score with Logistic Regression. Alanazi et al.  14  investigated the influence of financial news on public mental health and found that SLCNN was the most effective with 93.9% accuracy in sentiment classification. A systematic review conducted by Abd Rahman et al.  15  highlighted SVM as a commonly used model for mental health detection in Online Social Networks, emphasizing challenges such as data quality and ethical concerns. Abdulsalam et al.  16  introduced an Arabic suicidality detection dataset with 5719 tweets, demonstrating that the AraBert model outperforms traditional models such as SVM and Random Forest, achieving 91% accuracy and an F1 score of 88%. Almeqren et al.  17  applied Arabic Sentiment Analysis (ASA) to predict anxiety levels during the COVID-19 pandemic in Saudi Arabia, using a Bi-GRU model and a custom Arabic Psychological Lexicon (AraPh), reaching 88% accuracy in classifying anxiety from tweets. Ameer et al.  18  focused on detecting depression and anxiety from 16,930 Reddit posts, with the RoBERTa model achieving the highest accuracy of 83%, highlighting the role of automation in supporting mental health providers. Su et al.  19  conducted a scoping review on deep learning applications in mental health, categorizing studies across clinical, genetic, vocal, and social media data, underscoring DL's potential for early detection, while noting challenges in model interpretability and data diversity. Together, these studies demonstrate the transformative potential of AI in enhancing early mental health diagnosis across various platforms and languages.\n\nSeveral studies have highlighted the effectiveness of multi-task learning (MTL) in detecting mental health conditions and improving sentiment analysis across various domains. Liu and Su  20  explored a BERT-based MTL framework for mental health detection on social media, achieving superior performance on the Reddit SuicideWatch and Psychiatric-disorder Symptoms datasets compared to single-task models and generalpurpose Large Language Models (LLMs). Buddhitha and Inkpen  21  focused on suicide ideation detection using CNN-based MTL models applied to Reddit and Twitter, outperforming baselines and emphasizing the role of comorbid conditions such as PTSD in improving prediction accuracy. Sarkar et al.  22  introduced AMMNet, an MTL model that integrates word embeddings and topic modeling to predict depression and anxiety in Reddit posts, demonstrating strong performance through active learning despite data limitations. Li et al.  23  applied MTL to detect depression in dialogues, combining tasks such as emotion, dialogue act, and topic classification, and achieved a 70.6% F1 score on dialogue datasets. Similarly, Plaza-Del-Arco et al.  24  proposed an MTL model for hate speech detection in Spanish tweets, combining sentiment analysis and emotion classification, where their MTLsent+emo configuration outperformed the single-task learning models. Finally, Jin et al.  25  developed a sentiment classification MTL model (MTL-MSCNN-LSTM), that integrates multi-scale CNN and LSTM to capture global and local text features in commodity reviews, achieving higher accuracy and F1 scores across six datasets. These studies collectively demonstrate the potential of MTL in improving the prediction accuracy, handling multiple tasks, and addressing challenges such as data sparsity and model efficiency across different application areas.\n\nMultiple studies have focused on accelerating opinion-embedding approaches, primarily using attention mechanisms and state-of-the-art architectures in sentiment analysis. Malik et al.  26  introduced a hybrid model combining multi-head attention with Bi-LSTM and Bi-GRU classifiers, achieving a remarkable 95% accuracy, 97% recall, and 96% F1 score on student feedback sentiment analysis using advanced embeddings such as FastText and RoBERTa. In a similar manner, Chen et al.  27  proposed a BERT-based dual-channel hybrid neural network that integrates a CNN and BiLSTM with attention mechanisms, which significantly improves sentiment analysis on hotel review datasets, achieving 92.35% accuracy. On the other hand, Dimple Tiwari and Nagpal  28  introduced KEAHT, a knowledge-enriched hybrid transformer for analyzing sentiment related to COVID-19 and farmer protests, incorporating LDA topic modeling and BERT, demonstrating its ability to handle complex social media sentiment analysis tasks. The AEC-LSTM model introduced by Huang et al.  29  uses a novel approach that integrates emotional intelligence and attention mechanisms, resulting in superior performance on real-world datasets for sentiment classification. In contrast, Han and Kando 30 focused on improving opinion expression detection using a BiLSTM-CRF model with deep contextualized embeddings such as BERT, showing superior results for detecting subjective expressions. Jebbara and Cimiano 31 explored character-level embeddings to improve opinion target expression (OTE) extraction, addressing issues such as misspellings and domain-specific terminology, whereas Liu et al.  32  highlight the effectiveness of RNNs, particularly LSTMs, over traditional CRF models in fine-grained opinion mining, achieving enhanced performance by fine-tuning word embeddings. These studies highlight the evolution of sentiment analysis models, emphasizing the need for advanced embeddings, attention mechanisms, and hybrid architectures to improve sentiment classification and opinion mining across diverse applications.\n\nIn conclusion, the progress made in sentiment analysis, especially concerning mental health, highlights the revolutionary potential of deep learning and machine learning methods. Sophisticated models that use RNNs, CNNs, and transformer architectures such as BERT have replaced traditional lexicon-based methods, improving the capacity to extract subtle emotions from textual input. Multi-task learning (MTL) and attention processes have helped handle complicated tasks and improve prediction accuracy in recent experiments conducted on a variety of platforms. Sentiment analysis addresses the issues of data variety and requirement for interpretability in AI models. As it continues to be incorporated, it presents potential paths for early mental health diagnoses and interventions."
    },
    {
      "title": "Data description",
      "text": "The dataset employed in this study is an extensive and painstakingly assembled set of utterances from several platforms annotated with mental health statuses. It combines information from other Kaggle datasets, such as the 3k Conversations dataset for chatbots and depressions. The Reddit Cleaned, Human Stress Prediction, Bipolar Mental Health Dataset, Reddit Mental Health Data, Students Anxiety and Depression Dataset, Suicidal Mental Health Dataset, and Suicidal Tweet Detection Dataset are resources related to mental health and anxiety prediction. One of the following seven mental health statuses is associated with the statements in this dataset: Normal, Depression, Suicidal, Anxiety, Stress, Bipolar, or Personality Disorder. Every entry in the data was marked with a particular mental health condition, and it was gathered from a variety of sites, such as Reddit and social media. The dataset, obtained from Kaggle, is already labeled and does not include any user-level information, such as the number of posts per user.\n\nThe dataset, which is organized with variables such as unique_id, statements, and mental health status, is a priceless tool for researching mental health trends, creating sophisticated chatbots for mental health, and conducting in-depth sentiment analyses.\n\nImportant insights into the attitudes and mental health conditions of the sample are shown in Fig.  1 . The distribution of mental health statuses is shown in Fig.  1a , where \"Normal\" accounts for the biggest percentage (31%), followed by \"Depression\" (20.2%) and \"Suicidal\" instances (20.2%). Smaller percentages are explained by other categories including \"Anxiety\" (7.3%), \"Bipolar\" (5.3%), \"Stress\" (4.9%), and \"Personality Disorder\" (2.0%). The frequency of mental health conditions such as depression and suicidal thoughts in the data was demonstrated by this distribution.\n\nThe sentiment distribution is shown in Fig.  1b , where \"Positive\" sentiment is most prevalent at 40.3%, followed by \"Negative\" emotion at 37.6% and \"Neutral\" sentiment at 22.1%. This sentiment breakdown provides crucial context for sentiment analysis in the study of mental health trends by reflecting the emotional tones connected to mental health statuses.\n\nFigure  2  shows that the majority of posts in the dataset are relatively short, with most containing fewer than 500 words and a significant concentration below 100 words. The sharp decline in the number of posts as the word count increases highlights the dominance of shorter posts in user-generated content. This pattern suggests that users tend to communicate their thoughts in brief formats, which is typical for social media or digital platforms where brevity is often encouraged. Posts exceeding 1,000 words are rare, further reinforcing the preference for concise communication. Understanding this distribution is critical for natural language processing tasks, as it helps us set appropriate tokenization limits, ensuring that most posts are fully captured while minimizing the need for truncation or excessive padding. Additionally, this insight allows us to design models optimized for the typical length of user content, improving efficiency and accuracy in downstream tasks like classification, sentiment analysis, or topic modeling."
    },
    {
      "title": "Data pre-processing",
      "text": "Data cleaning Data cleaning requires several important procedures to ensure the quality and integrity  33  . Duplicate entries were identified and eliminated to avoid redundancy. Special characters, URLs, and other non-alphanumeric components were removed to streamline text data. Furthermore, the text was transformed to lowercase to preserve uniformity and eliminate problems due to case sensitivity. Missing values were carefully handled by imputing suitable values depending on the context or eliminating impacted rows. Finally, to improve the analysis's emphasis on the main ideas, stopwords-common words that had minimal bearing on the statements' meanings-were eliminated. All of these procedures worked together to provide a clean and trustworthy dataset for further processing and examination."
    },
    {
      "title": "Lemmatization",
      "text": "Lemmatization, which focuses on breaking words down to their most basic or root forms, is an essential stage in text preparation. In contrast to stemming, which frequently truncates words, lemmatization seeks to yield legitimate words by considering meaning and context. First, to maintain uniformity, all punctuation was removed and the text was converted to lowercase. Following the tokenization of the text into individual words, frequent stop words were eliminated. Next, each surviving word is lemmatized with the help of WordNetLemmatizer from NLTK  34, 35  , which guarantees that terms like \"running\" and \"runs\" are reduced to their root, \"run. \" This method aids in text normalization, which increases the accuracy of the ensuing analytical jobs. The lemmatized tokens were subsequently combined into a processed statement, prepared for the subsequent stages of feature extraction and modeling."
    },
    {
      "title": "Data augmentation with synonym replacement",
      "text": "As part of our data augmentation process, we used a synonym replacement technique to improve the variety and resilience of the dataset  36, 37  . This method finds words and replaces them with synonyms by using the WordNet lexical database  38  . Using WordNet synsets, the procedure begins by extracting synonyms for every word in a given text. A synonym is randomly selected from the list of potential synonyms for every word in the text. This replacement introduces modifications in wording while maintaining the textual meaning up to a predetermined number of times (n=3, in this example). Subsequently, the modified texts were combined and added to the dataset, increasing both the quantity and variety. To provide a more complete dataset, several copies of each original text item were created and appended using synonyms. Using the synonym_replacement function, this method significantly increases the diversity of the dataset and can help machine learning models that have been trained on it perform better and become more broadly applicable. For consistency in further analysis or training, the original labels were retained in the augmented dataset created using both original and freshly generated texts."
    },
    {
      "title": "Sentiment analysis with TextBlob",
      "text": "During the sentiment analysis stage, TextBlob is used to assess each statement in the supplemented dataset to ascertain its sentiment polarity  39  . Based on the polarity score, p, where p \u2208 [-1, 1], TextBlob divides attitudes into three categories: \"Positive, \" \"Neutral, \" or \"Negative. \" The following section describes how sentiment categorization was performed.\n\nFunction analysis_sentiment_by_status was used to perform this categorization. It processes each statement s and the status that accompanies it status, evaluates the sentiment, and yields a tuple (s, status, sentiment). After the sentiments are combined, we can determine the number of sentiment types for each status by examining a DataFrame called sentiment_df. In particular, the counts were acquired by employing\n\nwhere the number of statements with status i and sentiment j is represented by sentiment_countsij. This distribution sheds light on the distribution of feelings across various mental health states and is described in sentiment_counts. Analyzing the emotional tone of utterances in connection with reported mental health issues requires an understanding of this distribution.\n\nTo ensure the robustness of our findings, we also experimented with other sentiment analysis tools, including VADER and Afinn. These tools offer varying approaches to sentiment scoring and were used to reassess the dataset's sentiment labels. The comparative analysis revealed that while TextBlob provided a balanced distribution of sentiment categories, VADER detected more nuanced polarity shifts, especially in \"Neutral\" and \"Negative\" sentiments. A summary of these results and their impact on the final model is presented in the Results section."
    },
    {
      "title": "Opinions",
      "text": "We identified important opinion-related terms using spaCy's English NLP model to extract and sanitize subjective expressions from textual input  40  . We refined the extraction procedure by removing auxiliary verbs and concentrating on adjectives, adverbs, and verbs likely to communicate subjective feelings by running each text through the model. For convenience, the detected opinions were combined into a string format. In conclusion, we have used a cleaning mechanism after extraction. This function eliminated non-alphabetic letters and phrases that were too repetitive or insignificant, leaving only the significant and original expressions. The end product is a dataset enhanced with pertinent and clean opinion phrases that, offers a strong basis for additional research or model training.\n\nThe average number of opinions for each sentiment type is presented in Table  1 . With 36.31 opinions on average, the \"Positive\" category has the most opinions, closely followed by the \"Negative\" category with 33.80 opinions. The \"Neutral\" category, on the other hand, has a far lower average-just 3.64 opinions per feeling. This shows that users record fewer neutral comments and prefer to express stronger sentiments more frequently in terms of positivity or negativity. There might be a general trend toward polarized sentiment in the dataset, as seen by the imbalance in the number of neutral thoughts.\n\nThe opinions most frequently used in the dataset are listed in Table  2 . With 81,114 occurrences, the verb \"feel\" is the most common, suggesting that emotional expressiveness is important to the dataset. Verbs like \"know\" and \"want, \" which appear 52,940 and 50,827 times, respectively, are closely followed, indicating that knowledge and want are strongly associated with user attitudes. Additional frequently used words such as \"get, \" \"even, \" and \"really\" denote typical declarations of emphasis or intensity. These frequently expressed opinions draw attention to important phrases that users frequently use to communicate their feelings, ideas, and behaviors, thereby providing insights into the recurrent themes of the dataset."
    },
    {
      "title": "Data tokenization and label encoding",
      "text": "We used a two-step procedure of tokenization and label encoding to obtain textual input for model training. First, the raw text data were transformed into token IDs and attention masks appropriate for BERT-based models using BertTokenizer  9  . After processing the phrases in the DataFrame's statement column, the tokenize_data function generates input_ids and attention_masks using the given parameters of padding and truncating to a maximum length of 100 tokens, among other things. The text data were structured correctly for the model input owing to this tokenization.\n\nWe set the input sequence length to 100 tokens after conducting an exploratory analysis of our dataset. Initially, we examined the distribution of tokenized sentences and observed that the majority of instances contained fewer than 100 tokens. Specifically, our preliminary data analysis showed that approximately 90% of sentences in the dataset had a token count below this threshold, minimizing the amount of unnecessary padding for most samples. Selecting a slightly longer length would have increased computational overhead, while a shorter length risked truncating relevant context. Thus, a padding length of 100 tokens represented a balanced compromise between computational efficiency and preserving important textual information.\n\nThe next step involved label encoding, which converts category labels into numerical values required for model training  41  . For the status, emotion, and opinions_str columns, we initialized label encoders. These encoders enable the model to comprehend and handle categorical data efficiently by mapping distinct class labels to integers. The opinions labels were kept as integer encodings, whereas the status and emotion labels were transformed into one-hot encoded vectors by using the to_categorical function. Thorough preparation guarantees that both categorical and textual data are correctly prepared and incorporated into the training pipeline. These encoded labels are now included in the new DataFrame, which makes managing the data and training the models easier."
    },
    {
      "title": "Data splitting",
      "text": "The dataset was systematically divided into training, validation, and test sets to simplify the training and evaluation of the model. For compatibility with the scikit-learn's train_test_split function, the input tensors (input_ids and attention_masks) and encoded labels (status_labels, sentiment_labels, and opinions_labels) were first transformed from TensorFlow tensors to NumPy arrays.\n\nOpinion Frequency Opinion Frequency Feel 81,114 Go 32,254 Know 52,940 Think 31,005 Want 50,827 Make 30,437 Get 46,721 Even 46,529 Really 43,638 Table 1. Average number of opinions per sentiment category.\n\nInitially, the dataset was split into training and temporary sets, with 30% set aside for testing and validation and 70% going toward training. The temporary set was then divided evenly into the test and validation sets, each comprising 15% of the initial data. This method guarantees solid model training and objective assessment by evenly distributing data. The corresponding dataset sizes were 1,10,365 samples for the training, 23,650 samples for validation, and 23,650 samples for testing. These splits are essential for evaluating model performance, finetuning hyperparameters, and guaranteeing the applicability of the model to new data."
    },
    {
      "title": "Problem statement",
      "text": "Mental health analysis through text requires a deep understanding of both sentiments and subjective opinions expressed in user-generated content. This challenge can be effectively addressed using multi-task learning, in wich a model is trained to handle multiple related tasks simultanously. In this study, we aim to classify both sentiment and status using a BERT-based hybrid model that integrates external opinion embeddings to enhance the model's interpretative capabilities.\n\nInput: Let Xi represent the input sequence for the i-th instance, where\n\nrepresents a sequence of m tokens. In addition, let Oi denote the external opinion information associated with the input sequence, represented as an embedding.\n\n\u2022 Text input: Xi \u2208 R m\u00d7d (BERT embeddings of the input text sequence)\n\n\u2022 Opinion Input: Oi \u2208 R d (dense opinion embedding)\n\nOutput: The model is designed for multi-task learning, providing predictions for both sentiment and status classification:\n\n\u2022 Sentiment output:\n\nwhere S is the number of status classes.\n\n\u2022 Final output: The model predicts:\n\nwhere Yi is a pair consisting of the predicted sentiment and status labels for input sequence  Xi. This formulation encapsulates the multi-task nature of the problem, aiming to accurately classify both sentiment and status from text data enriched with opinion-based insights."
    },
    {
      "title": "Proposed methodology",
      "text": "Figure  3  illustrates the architecture of our Opinion-Enhanced Hybrid BERT Model, which is designed to handle two tasks simultaneously: sentiment classification and status classification in mental health-related text data. Our model enhances standard BERT embeddings with custom opinion embeddings, capturing subjective opinions expressed in the text. These opinions are critical for understanding mental health conditions. By combining contextual information from BERT with subjective insights through opinion embeddings, we provide a deeper understanding of emotional states.\n\nWe employed BERT as the foundation of our model to generate contextual embeddings. We extended this using a custom OpinionsEmbedding Layer that integrates opinion-based information. A hybrid feature extraction mechanism, utilizing both CNN and BiGRU layers, captures the local patterns and long-range dependencies in the text. Our model is structured within a multi-task learning framework, enabling us to perform sentiment and status classification in parallel, learning shared representations that improve performance on both tasks.\n\n1. BERT encoder: Our model is based on BERT and produces embeddings represented by HBERT. These BERT embeddings capture syntactic and semantic relationships between words, offering a comprehensive, context-aware representation of the input text. By leveraging BERT, we ensure the model identifies subtle nuances in meaning, which is essential for correctly understanding information linked to mental health. Each word is represented in relation to its surrounding context, which is particularly helpful when handling complex language patterns often found in mental health-related text, such as metaphorical language and emotive expressions.\n\nTo enhance sentiment and status classification accuracy, we complement the contextual understanding provided by HBERT with additional components, such as CNN, BiGRU, and attention-based opinion embeddings. This layered approach enables the model to capture nuanced changes in tone, inferred emotions, and subtle viewpoints."
    },
    {
      "title": "Opinion embeddings:",
      "text": "We incorporate a specific layer called the OpinionsEmbedding Layer, which extracts subjective information like opinions and emotions. This layer complements the contextual information captured by BERT embeddings. To extract sentiment-related subtleties, we employ an attention mechanism that highlights the most relevant textual elements. Specifically, we calculate queries Qi, keys Ki, and values Vi for each word in the input sequence:\n\nHere, Eo is the base opinion embedding derived from external sentiment annotations, while WQ i , WK i , and WV i are learned weight matrices that transform Eo into queries, keys, and values.\n\nNext, we compute the attention score Si, which determines the significance of each word in expressing opinions or emotions:\n\nThis score leverages learned weight matrices Wa, Ua, and a vector Va to highlight opinion-relevant words. Applying these scores to the values generates the final opinion embeddings Oopinions, which encapsulate the subjective dimensions of the text.\n\nWe then concatenate these opinion embeddings with the contextual embeddings from BERT to create a unified representation:\n\nThis combined embedding H combined enriches the model's understanding by integrating both factual and emotional insights, which is crucial for mental health studies."
    },
    {
      "title": "Hybrid architecture (CNN and BiGRU):",
      "text": "We process the combined embeddings H combined using a hybrid architecture that includes convolutional neural networks (CNNs) and bidirectional GRUs (BiGRUs). The CNN branch captures local patterns, such as emotionally charged phrases, using 1D convolution: HCNN = Conv1D(H combined , k)\n\nA max-pooling operation reduces the dimensionality while retaining significant features:\n\nSimultaneously, the BiGRU captures long-range dependencies and sequential relationships in the text. We generate the BiGRU output HBiGRU as:\n\nA global max-pooling operation further condenses the BiGRU output:\n\nBy combining the CNN and BiGRU outputs, our architecture captures both short-term and long-term dependencies, making it well-suited for analyzing nuanced emotional expressions."
    },
    {
      "title": "Multi-task learning:",
      "text": "We adopt a multi-task learning framework to perform sentiment classification and status classification simultaneously. This approach enables the model to generalize better by leveraging shared patterns between the two tasks. We apply dropout and layer normalization to the concatenated outputs from the CNN and BiGRU layers:\n\nFinally, two separate softmax layers generate predictions for sentiment and status classifications:\n\nThis framework effectively integrates data from both tasks, improving overall accuracy and efficiency.\n\nOur CNN-BiGRU hybrid architecture, combined with opinion embeddings and BERT, captures both subjective viewpoints and factual information in mental health-related texts. Consequently, we achieve a comprehensive understanding of emotional tone (sentiment) and broader emotional contexts (status). This approach makes our model a robust tool for analyzing text in the domain of mental health."
    },
    {
      "title": "Loss functions",
      "text": "For sentiment and status categorization, the described model uses categorical cross-entropy loss functions, that are subsequently applied to the respective outputs. Each task has an output layer, however, the two loss functions are calculated separately and combined to provide the overall loss.\n\nLstatus represents the categorical cross-entropy loss for the status classification problem, whereas Lsentiment represents the sentiment classification task. The overall loss function is the weighted sum of these individual losses and is represented by the model as L.\n\nwhere:\n\nHere:\n\n\u2022 ystatus,i is the true label for the status classification, where Cstatus is the number of status classes.\n\n\u2022 pstatus,i is the predicted probability for the status classification.\n\n\u2022 ysentiment,j is the true label for sentiment classification, where Csentiment denotes the number of sentiment classes. \u2022 psentiment,j is the predicted probability for the sentiment classification.\n\nWeights \u03b1 and \u03b2 balance the contributions of each loss term to the overall loss function, allowing for tailored optimization based on the importance of each task. These weights can be adjusted to consider task-specific needs or performance. For equal weighting, these weights were typically set to one.\n\nThe model minimizes the combined loss L, which guides the optimization process by balancing errors across both classification tasks to guarantee that it learns to perform well in both status and sentiment predictions."
    },
    {
      "title": "Experimental setup",
      "text": "The trials in this study were conducted using a machine with 16GB of RAM and an NVIDIA GeForce RTX 2060 GPU, which provided sufficient processing capability for deep learning model training. The implementation was completed using TensorFlow 2.10.1, a popular deep learning framework that facilitates GPU acceleration for effective training and inference. Python 3.9.19 was used to construct the codebase because it provides a stable environment integrating different libraries and performing machine learning operations. Fast training iterations and efficient use of computing resources were made possible by this configuration, which guaranteed a stable and consistent platform for testing the proposed Opinion-Enhanced Hybrid BERT Model."
    },
    {
      "title": "Baseline models",
      "text": "The baseline models used in this study were DistilBERT 11 , RoBERTa 10 , and BERT  9  . These sophisticated models are quite effective at comprehending text. Strong language comprehension was provided by the primary model, BERT. To outperform BERT, RoBERTa requires longer training times and more data. DistilBERT is a useful alternative to BERT, as it is a scaled-down version of the algorithm that operates more quickly without sacrificing much of its accuracy.\n\n\u2022 BERT-base-uncased: A transformer-based model that uses a masked language-modeling objective for pre-training, making it effective for various NLP tasks. \u2022 RoBERTa: An optimized version of BERT that modifies key hyperparameters, removes the next sentence prediction objective, and trains on larger mini-batches. \u2022 DistilBERT: A smaller, faster, and lighter version of BERT trained using knowledge distillation, providing competitive performance with reduced resource consumption.\n\nBecause these models have demonstrated efficacy in natural language processing tasks and consistently deliver high performance across a range of benchmarks, we chose them as baselines. We can evaluate efficiency and accuracy by comparing their different sizes and training approaches, which provide information about the tradeoffs associated with model selection."
    },
    {
      "title": "Ablation study",
      "text": "The goal of this ablation study was to evaluate the efficacy of several elements incorporated into the suggested model. We performed this by experimenting with different configurations and evaluating how well they performed in comparison to the entire BERT-CNN-BiGRU model with attention-based opinion embeddings.\n\nIn the original version, sentiment and status predictions were made using only the BERT encoder, thereby eliminating opinion embeddings. Subsequently, we simply used BERT and opinion embeddings to evaluate the model, not the hybrid CNN-BiGRU architecture, to see how opinion information contributed. We also experimented with the more straightforward hybrid models, BERT-BiGRU and BERT-CNN, to separate the effects of each element. To determine how adding or removing CNN, BiGRU, and opinion embeddings impacted the overall performance, each configuration was examined. The effects of adding or removing the CNN, BiGRU, and opinion embeddings on the overall performance were examined for each configuration. The full BERT-CNN-BiGRU model with attention-based opinion embeddings consistently outperformed all other configurations, according to the results, proving that attention-enhanced opinion integration and local and sequential feature extraction are essential for precise and nuanced mental health analysis."
    },
    {
      "title": "Hyperparameter settings",
      "text": "Certain hyperparameters were used to fine-tune the suggested Opinion-BERT model. The Adam optimizer was employed with a learning rate of 2e-5 and a batch size of 32. With an embedding dimension of 768, the length of the input sequence is restricted to 100 tokens. The CNN component employed 64 filters with a kernel size of 3, and MaxPooling1D came next. The attention mechanism features four heads. The 64 units of the BiGRU layer were used to record sequential data. The model was trained for 15 epochs with categorical cross-entropy as the loss function, and a dropout rate of 0.3 was used to avoid overfitting. Early halting with four-epoch patience was used to maximize training, and when the validation loss plateaued, the learning rate scheduler modified the learning rate. This setup allowed the model to accurately capture complex sentiments and status patterns in mental health texts."
    },
    {
      "title": "Evaluation metrics",
      "text": "Several assessment measures were used to evaluate how well the suggested Opinion-BERT model classified sentiment and status:\n\n\u2022 Accuracy: This metric measures the proportion of correct predictions to the total predictions. It is a straightforward indicator of how well the model performs in both sentiment and status classification tasks.\n\nAccuracy ="
    },
    {
      "title": "Number of Correct Predictions Total Number of Predictions",
      "text": "\u2022 Precision: Precision evaluates the accuracy of positive predictions. This is the ratio of true positive predictions to the total predicted positives, providing insight into the ability of the model to avoid false positives."
    },
    {
      "title": "Precision = True Positives True Positives + False Positives",
      "text": "\u2022 Recall (sensitivity): Recall assesses the model's capability to correctly identify positive instances. This is the ratio of true positive predictions to the total actual positives, indicating the effectiveness of the model in detecting relevant cases."
    },
    {
      "title": "Recall = True Positives True Positives + False Negatives",
      "text": "\u2022 F1-score: The F1-Score is the harmonic mean of the Precision and Recall, providing a balanced measure that accounts for both false positives and false negatives. This is particularly useful when dealing with unbalanced datasets.\n\n\u2022 AUC-ROC (area under the receiver operating characteristic curve): This metric evaluates the model's ability to distinguish between classes. A higher AUC indicates better performance, reflecting the trade-off between the true positive rate (recall) and the false positive rate.\n\nA thorough assessment of the model's efficacy in sentiment and status categorization is provided by these metrics combined, enabling a thorough examination of its prediction abilities and dependability while processing intricate text data on mental health."
    },
    {
      "title": "Results analysis"
    },
    {
      "title": "Baseline comparison",
      "text": "The suggested Opinion BERT model and the baseline models (BERT, RoBERTa, and DistilBERT) are thoroughly compared in Table  3  for the Status and Sentiment Classification tasks. As can be seen, the proposed model achieved the maximum accuracy, macro precision, recall, and F1-score metrics while continuously outperforming the baselines in both tasks. Opinion BERT outperformed the next best BERT, with an accuracy of 93.74% for Status Classification, compared to 90.98% for BERT. Opinion BERT outperformed BERT in Sentiment Classification, with 96.25% accuracy. Incorporation of attention-based opinion embeddings, which improve the model's comprehension of complex contextual information, is responsible for the model's higher performance. Furthermore, a higher F1-score in both tests indicates that Opinion BERT is competent at properly identifying both positive and negative examples, as seen by its more balanced accuracy and recall values. This demonstrates the effect of combining the BERT-CNN-GRU architecture with opinion-aware embeddings. The confusion matrices for status and sentiment classification tasks across different models are shown in Figs.  4  and  5 , respectively. The effectiveness of BERT, RoBERTa, DistilBERT, and the suggested model in categorizing mental health status into groups including stress, anxiety, depression, and personality disorder are contrasted in Fig.  4 . Improved prediction accuracy across several categories is demonstrated by the suggested model's decreased misclassification rates, particularly in more complicated categories like bipolar and suicidal. With a greater count in the diagonal element for the true class, for instance, it more accurately identifies bipolar instances. The confusion matrices for sentiment classification show how well various models distinguish between positive, neutral, and negative attitudes, as shown in Fig.  5 . Compared with the conventional BERT and RoBERTa models, the suggested model shows improved accuracy in predicting Positive and Neutral classes, lowering the number of cases that are incorrectly categorized. The proposed model exhibits fewer mistakes in differentiating between Positive and Neutral sentiments, which results in improved overall performance metrics. This is especially evident in the accurate classification of positive sentiment. The outcomes demonstrate the effectiveness of the proposed model design, which combines CNN, BiGRU, and sophisticated attention mechanisms to capture subtle linguistic signals essential for mood and status predictions."
    },
    {
      "title": "Model",
      "text": "The performance of several models across status and sentiment classification tasks are depicted by the ROC curves in Figs.  6  and  7 , respectively. These graphs, which plot the True Positive Rate (sensitivity) versus the False Positive Rate at different threshold values, demonstrate the capacity of the models to discriminate between the classes.\n\nA strong discriminative capacity is indicated by the proposed model's high AUC values across all mental health status categories, as shown in Fig.  6 . With an AUC of 1.00, the model attained perfect or almost perfect AUC values for categories, such as stress, anxiety, and suicidal thoughts. Interestingly, the suggested model achieves AUC values of 0.98, exceeding RoBERTa and DistilBERT, especially in more complicated categories such as depression and bipolar disorder, which sometimes offer difficulties because of overlapping symptoms. This demonstrates how well the proposed approach can identify minute patterns connected to various mental health issues.\n\nA strong performance in the emotion categorization challenge is also shown in Fig.  7 . The suggested model regularly outperformed the conventional BERT and RoBERTa models, producing AUC values of 0.99 or higher, across Positive, Neutral, and Negative attitudes. According to this, the architecture that combines CNN and BiGRU layers with sophisticated attention mechanisms is very good at capturing the subtleties of sentiments, which reduces misclassifications and improves overall precision, particularly when separating positive and neutral sentiments. These ROC curves highlight the accuracy and dependability of the proposed model in sentiment and status classification tasks."
    },
    {
      "title": "Comparison of sentiment lexicons",
      "text": "Table  4  highlights the performance comparison of sentiment lexicons-Afinn, VADER, and TextBlob-across status and sentiment classification tasks. TextBlob consistently outperforms the others, achieving the highest test accuracy of 93.74% for status classification and 96.25% for sentiment classification. It also demonstrates superior macro precision, recall, and F1-scores, reaching 94.17%, 94.48%, and 94.32% for status classification, and 96.50%, 96.68%, and 96.58% for sentiment classification. VADER shows strong performance, particularly in sentiment classification, with a test accuracy of 94.12% and an F1-score of 93.68%. Afinn performs reliably, achieving a sentiment classification F1-score of 91.97%. As shown in Table  4 , TextBlob proves to be the most effective lexicon for sentiment analysis, with VADER and Afinn providing solid alternatives for specific use cases."
    },
    {
      "title": "Ablation study",
      "text": "Table  5  displays the findings of the ablation research, which was conducted to assess how well different models performed on tasks involving the categorization of emotion and status. BERT with opinion embedding, BERT-CNN-BiGRU with attention-based opinion embedding, BERT-BiGRU with opinion embedding, BERT-CNN with opinion embedding, BERT-CNN-BiGRU without opinion embedding, and suggested Opinion BERT with attention-based opinion embedding are among the models that were evaluated. For both classification tasks, each model's test accuracy, F1-score, recall, and macro precision are displayed.\n\nNotably, the suggested Opinion BERT demonstrated its efficacy in capturing subtle semantic links through attention-based opinion embedding, with the highest test accuracy of 93.74% for status classification and 96.25% for sentiment classification. By concentrating on the important elements of the input text, this novel embedding technique improves the model's comprehension and contextualization of the attitudes conveyed. More accurate predictions are produced by the model's ability to evaluate the importance of various words or phrases according to their applicability to the attitude under study, owing to the inclusion of attention processes.\n\nAdditionally, BERT-CNN-BiGRU with conventional opinion embedding performed competitively, particularly in status categorization, where it achieved 91.94% accuracy. The CNN and BiGRU layers work together to effectively extract features and comprehend the context, both of which are essential for precise categorization.\n\nFurthermore, the findings show that opinion embedding is important for enhancing performance. Illustration of the significance of integrating domain-specific information, BERT-BiGRU with opinion embedding fared better than versions utilizing conventional embeddings alone, although it had lower accuracy than the suggested model. The BERT-CNN-BiGRU model without opinion embedding, on the other hand, performed the worst, highlighting the need for embedding techniques to increase model efficacy.\n\nOverall, the findings demonstrate how various model architectures and embedding tactics affect performance, highlighting the significance of customized methods in opinion analysis, especially with the inclusion of attention mechanisms in the suggested Opinion BERT. Lexicon Task Test accuracy (%) Macro precision (%) Recall (%) F1-score (%) Afinn Status classification 91.35 90.53 92.09 91.23 Sentiment classification 92.99 92.01 92.28 91.97 VADER Status classification 92.27 91.84 92.94 92.36 Sentiment classification 94.12 94.39 93.07 93.68 TextBlob Status classification 93.74 94.17 94.48 94.32 Sentiment classification 96.25 96.50 96.68 96.58 Table 4. Comparison of different sentiment lexicons. Table 5. Ablation study results for status and sentiment classification. OE opinion embedding, TOE traditional opinion embedding, w/ with, w/o without. a 92.35% accuracy rate for hotel reviews by combining BERT with CNN, BiLSTM, and Attention processes to overcome the conventional drawbacks of CNN and RNN. By using a Bi-LSTM architecture for social media data from sites such as Facebook, Instagram, and Twitter, Selva Mary et al.  44  were able diagnosed with depression, with an astounding 98.5% accuracy rate. Sowbarnigaa et al.  45  classified depression with a 93% accuracy rate using a CNN-LSTM combination. By using the EmoMent corpus and RoBERTa, Atapattu et al.  46  concentrated on South Asian settings and obtained F1 values of 0.76 and 0.77 for post-categorization and mental health prediction, respectively. The intricacy of the MAMS dataset was addressed by Wu et al.  47  work on multi-aspect sentiment analysis using a RoBERTa-TMM ensemble, which demonstrated good F1 scores in both ATSA (85.24%) and ACSA (79.41%). In contrast, our study uses a multi-input neural network called Opinion-BERT, which integrates CNN, BiGRU, Transformer blocks, token embeddings, and attention mechanisms. It achieved an accuracy of 93.74% for classifying mental health status and 96.25% for sentiment analysis. This demonstrates how well our method handles the complex and multidimensional characteristics of mental health literature."
    },
    {
      "title": "Discussion",
      "text": "The proposed Opinion-BERT model has shown great promise for improving mental health analysis through multi-task learning by integrating attention-based opinion embeddings. Understanding both explicit and subtle material in texts pertaining to mental health has been demonstrated to be greatly aided by the combination of opinion-specific data with contextual embeddings of BERT. This integrated method reflects the intricacies of human language, which frequently consists of a combination of ideas, feelings, and factual information that is difficult for standard models to comprehend. The effect of opinion embeddings on overall model performance is one noteworthy finding. Incorporating subjective signals is crucial because the ablation study makes it evident that adding opinion embeddings improves emotion and status classification tasks. According to these results, opinions are important in mental health analysis since they frequently give emotional states and behavioral inclinations context. To extract sentimentrelated information from the text, the Opinions Embedding Layer's attention mechanism effectively finds the most pertinent passages, which enhances the accuracy and dependability of the model.\n\nAdditionally, the hybrid design that included CNN and BiGRU layers worked well. Local patterns and relationships, including phrases and word n-grams, are well captured by the CNN component, but the BiGRU component represents the long-term context and sequential dependencies. In texts on mental health, which may contain subtle tone changes, sarcasm, or complicated emotional states that call for an awareness of both local and global text elements, this combination is particularly helpful.\n\nFurthermore, by utilizing shared information between various tasks, the multi-task learning architecture enabled the model to predict sentiment and status at the same time. This shared learning was beneficial because of its improved generalization and decreased the possibility of overfitting to a single task. The findings support the idea that multi-task learning works well in complicated fields like mental health, where overlapping variables affect different emotional states.\n\nAll things considered, the Opinion-BERT model shows promise for enhancing mental health analysis through the use of cutting-edge NLP approaches. A step toward developing more precise and thorough language models for this delicate and important topic has been made with the effective integration of context and subjective perspectives."
    },
    {
      "title": "Limitation and future work",
      "text": "Despite the encouraging outcomes of the Opinion-BERT model, it is important to recognize several limitations. First, both the quality and representativeness of the training data are critical to the success of the model. Real-\n\nCitation Dataset Problem statement Proposed methodology Accuracy Kokane et al. 42 Twitter dataset, Reddit dataset Detecting mental illness using NLP Transformers on social media. Analyzing mental health status through text analysis on Twitter and Reddit. DistilBERT 91% (Twitter), 84% (Reddit) Chen et al. 43 Hotel review datasets Traditional CNN ignores contextual semantic information. Traditional RNN has information memory loss and vanishing gradient. BERT + CNN + BiLSTM + Attention 92.35% Selva Mary et al. 44 User-generated content from Twitter, Facebook, and Instagram Detecting depression signs in social media content. Enhancing early intervention and support for mental health challenges. Bi-LSTM 98.5% Sowbarnigaa et al. 45 English language social media postings. Shared task introduced by ACL 2022. Detecting signs of depression from social media postings. Utilizing sentiment analysis to categorize depression indicators. CNN-LSTM Precision: 93% Atapattu et al. 46 EmoMent corpus (2802 Facebook posts from Sri Lanka and India) Detect mental health issues from text using NLP techniques. Develop emotionannotated mental health corpus from South Asian countries. RoBERTa F1: 0.76, Macro F1: 0.77 Wu et al. 47 NLPCC 2020 Shared Task 2 MAMS dataset Re-formalize ABSA as a multi-aspect sentiment analysis task. Address the complexity of the MAMS dataset with Transformer-based Multi-aspect Modeling. RoBERTa-TMM ensemble F1: 85.24% (ATSA), F1: 79.41% (ACSA) Our work Mental health The model aims to classify mental health-related text into status categories and sentiment labels using a multi-input neural network combining token embeddings, CNN, BiGRU, Transformer blocks, and attention mechanisms. Opinion-BERT Sentiment 96.25%, Status 93.74%\n\nTable 7. Comparison of various approaches for mental health detection and sentiment analysis.\n\nworld applications may perform less well if the dataset is undiversified or skewed toward particular emotions or mental health issues. While the model's dependence on attention-based embeddings is advantageous for capturing context, it can also present problems such as overfitting, especially in situations where there is a lack of data. Moreover, there is still uncertainty over the interpretability of the model's choices. Although the attention mechanism emphasizes significant terms, it might be challenging to comprehend the underlying logic of certain forecasts.\n\nTo strengthen the generalizability of the Opinion-BERT model across a range of demographics and circumstances, future research should concentrate on expanding and diversifying its datasets. Further improvements classification accuracy and resilience may include investigating different embedding structures and methodologies, such as adding knowledge graphs or strengthening multi-task learning techniques. Future research could also focus on including explainability tools to provide more precise information on how the model makes decisions. Finally, expanding the Opinion-BERT framework's scope and fostering a more thorough comprehension of mental health feelings across various settings may be achieved by applying it to other fields, including social media and healthcare."
    },
    {
      "title": "Conclusion",
      "text": "We introduced the Opinion-BERT model in this study, which effectively integrates attention-based opinion embeddings to classify emotional states and mental health conditions. Through extensive experimentation, including an ablation study, we demonstrated that incorporating opinion-related features significantly enhances the model's performance. Our results show that Opinion-BERT outperforms baseline models across key evaluation metrics, such as accuracy, precision, recall, and F1-score.\n\nOur findings emphasize the critical role of both emotional and contextual cues in improving the accuracy of machine learning models for mental health analysis. By combining sentiment-specific embeddings with advanced contextual representations from BERT, we lay a strong foundation for future advancements in sentiment analysis and multi-task learning frameworks.\n\nThis research opens new avenues for more effective and nuanced mental health evaluations, offering the potential for better understanding and intervention in this important field. Integrating sentiment analysis with mental health assessments not only improves classification accuracy but also contributes to more insightful and adaptable mental health monitoring tools.\n\nIn conclusion, the Opinion-BERT model takes a significant step forward in leveraging machine learning for mental health applications, offering a robust framework for future research and practical implementation in the field."
    },
    {
      "text": "Fig. 2. Distribution of post lengths in the dataset. The histogram displays the frequency of posts according to their word count, highlighting the most common post lengths and providing insight into the dataset's typical content size."
    },
    {
      "text": "Fig. 1. Mental health status and sentiment distributions in the dataset."
    },
    {
      "text": "Fig.3. Our proposed architecture combines dynamically produced opinion embeddings with BERT embeddings. We integrate these opinion embeddings with BERT's contextual representations, which are obtained via sentiment annotations. Our design uses a hybrid feature extraction technique to capture sequential relationships as well as local patterns. This mechanism consists of layers of CNN and BiGRU. We apply concatenation, dropout, and normalizing layers to the final representations, yielding outputs for status prediction and sentiment categorization."
    },
    {
      "text": "Fig. 4. Confusion matrices for status classification using different models."
    },
    {
      "text": "Fig. 5. Confusion matrices for sentiment classification using different models."
    },
    {
      "text": "Fig. 6. AUC-ROC (Area under the receiver operating characteristic curve) for status classification using different models."
    },
    {
      "text": "Fig. 7. AUC-ROC (Area under the receiver operating characteristic curve) for sentiment classification using different models."
    },
    {
      "text": "Most frequent opinions."
    },
    {
      "text": "Baseline comparison for status and sentiment classification."
    },
    {
      "text": "Comparison of predicted and actual mental health status and sentiment for various models."
    }
  ],
  "references": [
    {
      "title": "Estimating the true global burden of mental illness",
      "authors": [
        "D Vigo",
        "G Thornicroft",
        "R Atun"
      ],
      "year": 2016,
      "doi": "10.1016/s2215-0366(15)00505-2"
    },
    {
      "title": "Social media big data analytics: A survey",
      "authors": [
        "N Ghani",
        "S Hamid",
        "I Hashem",
        "E Ahmed"
      ],
      "year": 2019
    },
    {
      "title": "A systematic review of applications of natural language processing and future challenges with special emphasis in text-based emotion detection",
      "authors": [
        "S Kusal"
      ],
      "year": 2023
    },
    {
      "title": "Mental health prediction model on social media data using CNN-BILSTM",
      "authors": [
        "D Fudholi"
      ],
      "year": 2024
    },
    {
      "title": "A domain-independent framework for modeling emotion",
      "authors": [
        "J Gratch",
        "S Marsella"
      ],
      "year": 2004,
      "doi": "10.1016/j.cogsys.2004.02.002"
    },
    {
      "title": "The reader in the text: Essays on audience and interpretation",
      "year": 2014
    },
    {
      "title": "A complete process of text classification system using state-of-the-art NLP models",
      "authors": [
        "V Dogra"
      ],
      "year": 2022
    },
    {
      "title": "Transformer models for text-based emotion detection: A review of BERTbased approaches",
      "authors": [
        "F Acheampong",
        "H Nunoo-Mensah",
        "W Chen"
      ],
      "year": 2021
    },
    {
      "title": "pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": 2018
    },
    {
      "title": "A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu"
      ],
      "year": 2019
    },
    {
      "title": "Distilbert, a distilled version of BERT: Smaller, faster, cheaper and lighter",
      "authors": [
        "V Sanh",
        "L Debut",
        "J Chaumond",
        "T Wolf"
      ],
      "year": 2020,
      "doi": "10.5860/choice.169427"
    },
    {
      "title": "The early detection and diagnosis of mental health status employing NLP-based methods with ml classifiers",
      "authors": [
        "A Singh"
      ],
      "year": 2024,
      "doi": "10.37648/ijrmst.v17i01.009"
    },
    {
      "title": "Machine learning driven mental stress detection on reddit posts using natural language processing",
      "authors": [
        "S Inamdar",
        "R Chapekar",
        "S Gite",
        "B Pradhan"
      ],
      "year": 2023
    },
    {
      "title": "Public's mental health monitoring via sentimental analysis of financial text using machine learning techniques",
      "authors": [
        "S Alanazi"
      ],
      "year": 2022
    },
    {
      "title": "Application of machine learning methods in mental health detection: A systematic review",
      "authors": [
        "R Abd Rahman",
        "K Omar",
        "S Noah",
        "M Danuri",
        "M Al-Garadi"
      ],
      "year": 2020
    },
    {
      "title": "Detecting suicidality in Arabic tweets using machine learning and deep learning techniques",
      "authors": [
        "A Abdulsalam",
        "A Alhothali",
        "S Al-Ghamdi"
      ],
      "year": 2024,
      "doi": "10.1007/s13369-024-08767-3"
    },
    {
      "title": "Using deep learning to analyze the psychological effects of COVID-19",
      "authors": [
        "M Almeqren",
        "L Almuqren",
        "F Alhayan",
        "A Cristea",
        "D Pennington"
      ],
      "year": 2023,
      "doi": "10.3389/fpsyg.2023.962854"
    },
    {
      "title": "Mental illness classification on social media texts using deep learning and transfer learning",
      "authors": [
        "I Ameer",
        "M Arif",
        "G Sidorov",
        "H G\u00f3mez-Adorno",
        "A Gelbukh"
      ],
      "year": 2022
    },
    {
      "title": "Deep learning in mental health outcome research: A scoping review",
      "authors": [
        "C Su",
        "Z Xu",
        "J Pathak",
        "F Wang"
      ],
      "year": 2020,
      "doi": "10.1038/s41398-020-0780-3"
    },
    {
      "title": "Enhancing mental health condition detection on social media through multi-task learning",
      "authors": [
        "J Liu",
        "M Su"
      ],
      "year": 2024
    },
    {
      "title": "Multi-task learning to detect suicide ideation and mental disorders among social media users",
      "authors": [
        "P Buddhitha",
        "D Inkpen"
      ],
      "year": 2023,
      "doi": "10.3389/frma.2023.1152535"
    },
    {
      "title": "Predicting depression and anxiety on reddit: a multi-task learning approach",
      "authors": [
        "S Sarkar",
        "A Alhamadani",
        "L Alkulaib",
        "C Lu"
      ],
      "year": 2022,
      "doi": "10.1109/asonam55673.2022.10068655"
    },
    {
      "title": "Multi-task learning for depression detection in dialogs",
      "authors": [
        "C Li",
        "C Braud",
        "M Amblard"
      ],
      "year": 2022
    },
    {
      "title": "A multi-task learning approach to hate speech detection leveraging sentiment analysis",
      "authors": [
        "F Plaza-Del-Arco",
        "M Molina-Gonz\u00e1lez",
        "L Ure\u00f1a-L\u00f3pez",
        "M Mart\u00edn-Valdivia"
      ],
      "year": 2021
    },
    {
      "title": "Multi-task learning model based on multi-scale CNN and LSTM for sentiment classification",
      "authors": [
        "N Jin",
        "J Wu",
        "X Ma",
        "K Yan",
        "Y Mo"
      ],
      "year": 2020
    },
    {
      "title": "Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques",
      "authors": [
        "S Malik"
      ],
      "year": 2024,
      "doi": "10.7717/peerj-cs.2283"
    },
    {
      "title": "Sentiment analysis and research based on two-channel parallel hybrid neural network model with attention mechanism",
      "authors": [
        "N Chen",
        "Y Sun",
        "Y Yan"
      ],
      "year": 2023,
      "doi": "10.1049/cth2.12463"
    },
    {
      "title": "A knowledge-enriched attention-based hybrid transformer model for social sentiment analysis",
      "authors": [
        "D Tiwari",
        "B Nagpal",
        "Keaht"
      ],
      "year": 2022,
      "doi": "10.1007/s00354-022-00182-2"
    },
    {
      "title": "Attention-emotion-enhanced convolutional lstm for sentiment analysis",
      "authors": [
        "F Huang"
      ],
      "year": 2021,
      "doi": "10.1109/tnnls.2021.3056664"
    },
    {
      "title": "Opinion mining with deep contextualized embeddings",
      "authors": [
        "W Han",
        "N Kando"
      ],
      "year": 2019,
      "doi": "10.18653/v1/n19-3006"
    },
    {
      "title": "Improving opinion-target extraction with character-level word embeddings",
      "authors": [
        "S Jebbara",
        "P Cimiano"
      ],
      "year": 2017
    },
    {
      "title": "Fine-grained opinion mining with recurrent neural networks and word embeddings",
      "authors": [
        "P Liu",
        "S Joty",
        "H Meng"
      ],
      "year": 2015,
      "doi": "10.18653/v1/d15-1168"
    },
    {
      "title": "Data quality considerations for big data and machine learning: Going beyond data cleaning and transformations",
      "authors": [
        "V Gudivada",
        "A Apon",
        "J Ding"
      ],
      "year": 2017
    },
    {
      "title": "Lemmatization for ancient greek: An experimental assessment of the state of the art",
      "authors": [
        "A Vatri",
        "B Mcgillivray"
      ],
      "year": 2020
    },
    {
      "title": "Intelligent analysis of multimedia healthcare data using natural language processing and deep-learning techniques",
      "authors": [
        "R Bondugula",
        "S Udgata",
        "N Rahman",
        "K Sivangi"
      ],
      "year": 2022
    },
    {
      "title": "Reversible natural language watermarking using synonym substitution and arithmetic coding",
      "authors": [
        "L Xiang",
        "Y Li",
        "W Hao",
        "P Yang",
        "X Shen"
      ],
      "year": 2018,
      "doi": "10.3970/cmc.2018.03510"
    },
    {
      "title": "Lexsubcon: Integrating knowledge from lexical resources into contextual embeddings for lexical substitution",
      "authors": [
        "G Michalopoulos",
        "I Mckillop",
        "A Wong",
        "H Chen"
      ],
      "year": 2021,
      "doi": "10.18653/v1/2022.acl-long.87"
    },
    {
      "title": "Using wordnet synonym substitution to enhance UMLS source integration",
      "authors": [
        "K Huang",
        "J Geller",
        "M Halper",
        "Y Perl",
        "J Xu"
      ],
      "year": 2009,
      "doi": "10.1016/j.artmed.2008.11.008"
    },
    {
      "title": "Sentiment analysis about product and service evaluation of pt telekomunikasi indonesia tbk from tweets using textblob, naive bayes & k-nn method",
      "authors": [
        "R Hermansyah",
        "R Sarno"
      ],
      "year": 2020,
      "doi": "10.1109/isemantic50169.2020.9234238"
    },
    {
      "title": "Aspect based opinion mining of online reviews",
      "authors": [
        "A Hilal",
        "M Chachoo"
      ],
      "year": 2020,
      "doi": "10.37896/gor33.03/500"
    },
    {
      "title": "Text classification using label names only: A language model self-training approach",
      "authors": [
        "Y Meng"
      ],
      "year": 2020
    },
    {
      "title": "Predicting mental illness (depression) with the help of nlp transformers",
      "authors": [
        "V Kokane",
        "A Abhyankar",
        "N Shrirao",
        "P Khadkikar"
      ],
      "year": 2024,
      "doi": "10.1109/icdsis61070.2024.10594036"
    },
    {
      "title": "Sentiment analysis and research based on two-channel parallel hybrid neural network model with attention mechanism",
      "authors": [
        "N Chen",
        "Y Sun",
        "Y Yan"
      ],
      "year": 2023,
      "doi": "10.1049/cth2.12463"
    },
    {
      "title": "Enhancing conversational sentimental analysis for psychological depression prediction with BI-LSTM",
      "authors": [
        "G Selva Mary"
      ],
      "year": 2023
    },
    {
      "title": "Leveraging multi-class sentiment analysis on social media text for detecting signs of depression",
      "authors": [
        "K Sowbarnigaa"
      ],
      "year": 2023,
      "doi": "10.54254/2755-2721/2/20220660"
    },
    {
      "title": "Emoment: An emotion annotated mental health corpus from two south asian countries",
      "authors": [
        "T Atapattu"
      ],
      "year": 2022
    },
    {
      "title": "Transformer-based multi-aspect modeling for multi-aspect multi-sentiment analysis",
      "authors": [
        "Z Wu",
        "C Ying",
        "X Dai",
        "S Huang",
        "J Chen"
      ],
      "year": 2020,
      "doi": "10.1007/978-3-030-60457-8_45"
    }
  ],
  "num_references": 47
}
