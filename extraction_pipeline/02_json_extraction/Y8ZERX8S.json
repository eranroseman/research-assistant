{
  "paper_id": "Y8ZERX8S",
  "title": "MCMTC: A Pragmatic Framework for Selecting an Experimental Design to Inform the Development of Digital Interventions",
  "abstract": "Advances in digital technologies have created unprecedented opportunities to deliver effective and scalable behavior change interventions. Many digital interventions include multiple components, namely several aspects of the intervention that can be differentiated for systematic investigation. Various types of experimental approaches have been developed in recent years to enable researchers to obtain the empirical evidence necessary for the development of effective multiple-component interventions. These include factorial designs, Sequential Multiple Assignment Randomized Trials (SMARTs), and Micro-Randomized Trials (MRTs). An important challenge facing researchers concerns selecting the right type of design to match their scientific questions. Here, we propose MCMTC -a pragmatic framework that can be used to guide investigators interested in developing digital interventions in deciding which experimental approach to select. This framework includes five questions that investigators are encouraged to answer in the process of selecting the most suitable design: (1) Multiple-component intervention: Is the goal to develop an intervention that includes multiple components; (2) Component selection: Are there open scientific questions about the selection of specific components for inclusion in the intervention; (3) More than a single component: Are there open scientific questions about the inclusion of more than a single component in the intervention; (4) Timing: Are there open scientific questions about the timing of component delivery, that is when to deliver specific components; and (5) Change: Are the components in question designed to address conditions that change relatively slowly (e.g., over months or weeks) or rapidly (e.g., every day, hours, minutes). Throughout we use examples of tobacco cessation digital interventions to illustrate the process of selecting a design by answering these questions. For simplicity we focus exclusively on four experimental approaches-standard two-or multi-arm randomized trials, classic factorial designs, SMARTs, and MRTs-acknowledging that the array of possible experimental approaches for developing digital interventions is not limited to these designs.",
  "year": 2022,
  "date": "2022-03-09",
  "journal": "Cochrane Database Syst Rev",
  "publication": "Cochrane Database Syst Rev",
  "authors": [
    {
      "forename": "Alejandro",
      "surname": "Dom\u00ednguez-Rodr\u00edguez",
      "name": "Alejandro Dom\u00ednguez-Rodr\u00edguez"
    },
    {
      "forename": "Inbal",
      "surname": "Nahum-Shani",
      "name": "Inbal Nahum-Shani",
      "affiliation": "1  Insitute for Social Research , University of Michigan , Ann Arbor , MI , United States , \n\t\t\t\t\t\t\t\t Insitute for Social Research \n\t\t\t\t\t\t\t\t University of Michigan \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Ann Arbor \n\t\t\t\t\t\t\t\t\t MI \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "John",
      "surname": "Dziak",
      "name": "John Dziak",
      "affiliation": "2  Edna Bennett Pierce Prevention Research Center , The Pennsylvania State University , State College , PA , United States , \n\t\t\t\t\t\t\t\t Edna Bennett Pierce Prevention Research Center \n\t\t\t\t\t\t\t\t The Pennsylvania State University \n\t\t\t\t\t\t\t\t State College \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "forename": "David",
      "surname": "Wetter",
      "name": "David Wetter",
      "affiliation": "3  Huntsman Cancer Institute , University of Utah , Salt Lake City , UT , United States \n\t\t\t\t\t\t\t\t Huntsman Cancer Institute \n\t\t\t\t\t\t\t\t University of Utah \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Salt Lake City \n\t\t\t\t\t\t\t\t\t UT \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "affiliation": "University of Concepcion , Chile \n\t\t\t\t\t\t\t\t University of Concepcion \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Chile"
    },
    {
      "affiliation": "Johns Hopkins University , United States \n\t\t\t\t\t\t\t\t Johns Hopkins University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t United States"
    },
    {
      "affiliation": "Valencian International University , Spain \n\t\t\t\t\t\t\t\t Valencian International University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Spain"
    }
  ],
  "doi": "https://doi.org/10.13039/100000002",
  "keywords": [
    "Sequential Multiple Assignment Randomized Trial (SMART)",
    "Micro-Randomized Trial (MRT)",
    "factorial designs",
    "adaptive interventions",
    "just in time adaptive interventions",
    "digital interventions"
  ],
  "sections": [
    {
      "title": "INTRODUCTION",
      "text": "The widespread use, acceptability and convenience of digital technologies (e.g., mobile and wearable devices) have the potential to reduce structural barriers to treatment, making possible the delivery of behavioral interventions anytime and anywhere  (1) (2) (3) . Many digital interventions include multiple components. A component is any aspect of an intervention that can be separated out for systematic investigation  (4) . Examples of components in digital interventions include different content modules  (5) , levels of content tailoring [e.g., the degree to which information about the individual or context is used to produce the message  (6) ], intensity of human support  (7) , as well as other features designed to promote engagement such as gamification [i.e., the use of game-design elements in a non-game context  (8) ] and reminders  (9) . In their review of text messaging-based smoking cessation interventions, Kong et al.  (10)  conclude that to develop effective digital interventions to support tobacco cessation \"we need to broaden our understanding of the specific components of the interventions, for whom and how they can be used, and identify areas to improve already existing interventions\".\n\nVarious types of experimental approaches have been developed in recent years to enable researchers to obtain the empirical evidence necessary for the development and/or evaluation of effective multiple-component interventions. These include standard two-or multi-arm randomized trials [e.g., randomized controlled trials (RCTs)], factorial designs  (11, 12) , Sequential Multiple Assignment Randomized Trials [SMARTs  (13, 14) ], and Micro-Randomized Trials [MRTs  (15, 16) ]. An important challenge facing researchers interested in developing digital interventions concerns the selection of an appropriate experimental design to achieve their scientific goals. Here, we propose MCMTC, a pragmatic framework that can be used to guide investigators interested in developing digital interventions in deciding which experimental approach to select based on their scientific questions. MCMTC can be used not only as a guide for investigators considering potential studies, but also as a framework for teaching students the practical differences between various experimental design options."
    },
    {
      "title": "THE MCMTC PRAGMATIC FRAMEWORK",
      "text": "The MCMTC framework includes five questions that investigators can follow to guide their decision (Figure  1 ). In the next sections, we discuss each question using examples of tobacco cessation digital interventions to illustrate how answering these questions can guide the choice of a specific design. While a wide variety of study designs can inform the development of digital interventions, this manuscript focuses on four experimental approaches: standard two-or multi-arm randomized trials, classic factorial designs, SMARTs, and MRTs."
    },
    {
      "title": "Question 1: Multiple-Component Intervention?",
      "text": "The first question is whether investigators wish to develop an intervention that includes multiple components."
    },
    {
      "title": "No: The Goal Is Not to Develop a Multiple-Component Intervention",
      "text": "Examples where the answer to Question 1 is \"No\" include interventions either with a single component or else with multiple elements that should not be differentiated from each other for investigation. For example, suppose investigators develop a new type of nicotine replacement therapy (NRT) to support smoking cessation. A natural next step is to evaluate the effectiveness of this new NRT relative to a suitable alternative (e.g., the standard of care or another existing product). In this case, a randomized controlled trial (RCT) comparing the single component intervention to control would be a suitable experimental design. Next, suppose investigators develop a new digital intervention to support tobacco cessation which includes phone coaching combined with text messaging to help coordinate and schedule the calls with participants. This intervention includes two elements, phone coaching and text messaging, but these elements cannot be practically separated out for investigation because they are implemented in an interdependent way (i.e., text messaging to schedule calls could not be a standalone intervention given it supports phone coaching). Alternatively, suppose that the text messaging element focuses instead on encouraging participants to use brief self-regulatory strategies (e.g., deep breathing, calling a friend for support or taking a walk). In this case, text messaging and phone coaching can be separated out for investigation as they are not implemented in an interdependent way. However, suppose there is empirical evidence indicating that these two elements work together synergistically and hence should be combined, rather than implemented separately, to effectively promote tobacco cessation. In both cases, a natural next step investigators may take is to evaluate the combined effectiveness of the two elements, relative to a suitable alternative (e.g., standard of care). Hence, an RCT comparing the integration of coaching with text messaging to control may be a suitable experimental design.\n\nNotice that in the examples above the goal is not to empirically develop (i.e., systematically assemble the elements of) an intervention, but rather to evaluate the effectiveness of an intervention that either includes a single element or multiple elements. In these cases, investigators may consider a standard RCT comparing the effectiveness of the intervention as a whole to a suitable alternative. However, when the goal is to develop an intervention by investigating the effect of multiple components in order to find a favorable combination, a more complex design may be needed."
    },
    {
      "title": "Yes: The Goal Is to Develop a Multiple-Component Intervention",
      "text": "Examples where the answer to Question 1 is \"Yes\" include cases where specific aspects of the intervention can be separated out for investigation. For example (Example A), suppose investigators wish to develop a digital tobacco cessation intervention and they consider three elements: (1) a mobile app containing a collection of on-demand mindfulness-based meditation activities (Mobile); (2) daily prompts (via text messaging) encouraging participants to use brief self-regulatory strategies, such as deep breathing, calling a friend for support or taking a walk (Prompts); and (3) weekly online sessions containing brief quitting advice (Sessions). Suppose these elements can be feasibly separated out for investigation. In this case, the answer to Question 1 would be \"Yes\", leading investigators to Question 2."
    },
    {
      "title": "Question 2: Component Selection?",
      "text": "The second question is whether there are open scientific questions about the selection of specific components for inclusion in the intervention."
    },
    {
      "title": "No: There Are No Open Scientific Questions About the Selection of Components",
      "text": "Examples where the answer to Question 2 may be \"No\" include cases where investigators already have sufficient evidence (empirical or practical) to decide which components to include in the intervention. Suppose the investigators from the previous example have sufficient empirical evidence to conclude that all three components-Mobile, Prompts and Sessions-should be included in the intervention. If there are no questions about the efficacy of these components or their interactive effects, a natural next step would be to put together an intervention package that contains all three components and conduct an RCT to evaluate the effectiveness of the combined package, relative to a suitable control. In sum, MCMTC recommends that in the absence of scientific questions about the selection of intervention components, investigators may focus on scientific questions that concern the effectiveness of the multiple-component intervention package compared to a suitable alternative and hence consider a standard RCT.\n\nYes: There Are Open Scientific Questions About the"
    },
    {
      "title": "Selection of Components",
      "text": "Examples where the answer to Question 2 is \"Yes\" include cases where there is insufficient evidence to decide which component to include in the intervention package. For example (Example B), suppose investigators have sufficient evidence to conclude that the component Mobile should be included in the intervention package, but there is insufficient evidence to decide whether Prompts and Sessions should be included. Here, the investigators pose the following scientific questions (1) should a digital tobacco cessation intervention that includes Mobile also include Prompts? and (2) should a digital tobacco cessation intervention that includes Mobile also include Sessions? This means that there are open scientific questions about the selection of specific components for inclusion in the intervention. Hence, the answer to Question 2 would be \"Yes\", leading investigators to Question 3."
    },
    {
      "title": "Question 3: More Than One Component?",
      "text": "The third question is whether there are open scientific questions about the inclusion of more than a single component in the intervention."
    },
    {
      "title": "No: There Are No Open Scientific Questions About the Inclusion of More Than One Component",
      "text": "Examples where the answer to Question 3 may be \"No\" include cases where investigators consider multiple components for inclusion in the intervention package, but their scientific question is about only one of the components. For example, suppose investigators have sufficient evidence to conclude that the components Mobile and Prompts should be included in the intervention package, but there is insufficient evidence to decide whether Sessions should be included. Here, the investigators pose the following scientific question should a digital tobacco cessation intervention that includes Mobile and Prompts, also include Sessions? This question can be addressed via a two-arm randomized trial comparing the digital intervention with Sessions (i.e., an intervention package including all three components: Mobile, Prompts and Sessions) to a digital intervention without Sessions (i.e., an intervention package with only two components: Mobile and Prompts).\n\nExamples where the answer to Question 3 is \"No\" may also include cases where investigators consider multiple components, but their scientific question is about choosing a single one out of multiple components. For example, suppose investigators pose the following question is it most beneficial to offer Mobile, Prompt or Sessions? This question can be addressed by conducting a three-arm trial where participants are randomized to one of these three components. There might optionally also be a control arm receiving none of the components, so that the effectiveness (and not only the relative effectiveness) of each component can be investigated. However, in either case, combinations of components are not considered in this design.\n\nIn sum, MCMTC recommends that when there are open scientific questions only about the selection of a single component for inclusion in a multiple-component intervention, or the selection of a single component out of multiple components (but not about combinations of these components), then investigators may consider a standard two-or multi-arm randomized trial."
    },
    {
      "title": "Yes: There Are Open Scientific Questions About the Inclusion of More Than One Component",
      "text": "Examples where the answer to Question 3 is \"Yes\" include cases where there is insufficient evidence to make decisions about the inclusion of two or more intervention components. Consider Example B discussed above, where investigators pose scientific questions about the inclusion of two components, Prompts and Sessions. Here, there are open scientific questions about the inclusion of more than a single component. Hence, the answer to Question 3 would be \"Yes\", leading investigators to Question 4."
    },
    {
      "title": "Question 4: Timing of Intervention?",
      "text": "The fourth question is whether investigators wish to address scientific questions about the timing of delivering intervention components. These questions concern when it is best to deliver an intervention component or which component is most beneficial at different points in time."
    },
    {
      "title": "No: There Are No Open Scientific Questions About Intervention Timing",
      "text": "Examples where the answer to this question may be \"No\" include cases where investigators have scientific questions only about which intervention component to include throughout the study, or else to introduce at a single specific point in time. Consider Example B discussed above, where investigators pose two questions about the selection of two components at the beginning of the intervention. As an example, consider the first question: should a digital tobacco cessation intervention that includes Mobile also include Prompts? This question is about which component to introduce at a single time point-at study outset. This question concerns neither when to deliver these components, nor which component would be most beneficial at different time points. Since the questions posed in Example B do not concern intervention timing, the answer to Question\n\nTABLE 1 | 2 \u00d7 2 factorial N = 400."
    },
    {
      "title": "Experimental condition Factor",
      "text": "Prompts Sessions"
    },
    {
      "title": "Classic Factorial Designs",
      "text": "A factorial design is a randomized trial that includes two or more factors [i.e., independent variables manipulated in a systematic manner (  4 )]. For simplicity, suppose that each factor includes two levels: On (when the corresponding component is present) and Off (when the corresponding component is not present).\n\nIn a factorial design, the levels of each factor are crossed with the levels of the other factors to form a design with multiple experimental conditions. Consider Example B discussed above, where investigators have scientific questions about the inclusion of two components, Prompts and Sessions. To answer these questions, the investigators may consider a factorial experiment with two factors, one factor for each component. Using italicized abbreviations to represent experimental factors, Prompts refers to the factor corresponding to daily prompts via text messaging, and Sessions refers to the factor corresponding to weekly online sessions. Each factor will have two levels On and Off.\n\nIn the factorial design presented in Table  1 , the two levels of Prompts are crossed with the two levels of Sessions to form a design with 2 \u00d7 2 = 4 experimental conditions. Here, suppose that 400 individuals enter the study (throughout we assume no attrition for simplicity) and are randomized with equal probability (0.25) to each of the four experimental conditions. Suppose the primary outcome of interest was measured at the month 6 follow up. Data from this experimental design can be used to answer the two motivating questions about the inclusion of Prompts and Sessions by testing the main effect of each corresponding factor. When a factor has two levels, the main effect of this factor can be defined as the difference between the mean outcome at one level of this factor and the mean outcome at the other level of this factor, averaging over the levels of the other factors. Using data from the factorial experiment in Table  1 , the main effect of Prompts can be estimated by comparing the mean outcome across all the experimental conditions in which Prompts was set to On (conditions 1 and 2; n = 200; Table  1 ) to the mean outcome across all the conditions in which Prompts was set to Off (conditions 3 and 4; n = 200; Table  1 ). Similarly, the main effect of Sessions can be estimated by comparing the mean outcome across the experimental conditions in which Sessions was set to On (conditions 1 and 3; n = 200; Table  1 ) to the mean outcome across the conditions in which Sessions was set to Off (conditions 2 and 4; n = 200; Table  1 ).\n\nNotice that both main effects are estimated by using outcome information from the entire sample (N = 400). This is because factorial designs enable investigators to use outcome data from each study participant to test more than one main effect, thereby answering multiple scientific questions about the selection of intervention components. Collins et al.  (11)  described this property as the \"recycling\" of study participants and discussed the efficiency of this approach in estimating both main effects and interactions [also see  (4, 17) ].\n\nVarious types of factorial designs and analytic methods have been developed to accommodate scenarios where the implementation of a large number of experimental conditions is not practically feasible  (4, 18)  and where experimental subjects are clustered prior to the study [e.g., students in schools  (19) ], or become clustered during the study [e.g., study participants assigned to support groups;  (20, 21) ]. These methods enable investigators to leverage classic factorial designs to inform the development of a wide variety of behavioral interventions.\n\nYes: There Are Open Scientific Questions About Intervention Timing\n\nExamples where the answer to Question 4 may be \"Yes\" include cases where investigators have scientific questions about when it would be best to deliver a specific intervention component or which component to deliver at different points in time. As an example (Example C; see Figure  2 ), suppose an investigator wishes to develop a tobacco cessation intervention that begins with minimal technology-based support and then at week 2 provides more support to individuals who show early signs of non-response (i.e., those who self-report tobacco use within the past 7 days), whereas early responders (i.e., those who self-report no tobacco use within the past 7 days) continue with minimal technology-based support. Suppose that two scientific questions motivate the investigator: (1) should the initial intervention include a mobile app with a collection of on-demand mindfulness-based meditation activities (Mobile), or the mobile app combined with daily prompts recommending brief self-regulatory strategies (Mobile + Prompts)? (2) Should early non-responders be offered two online sessions containing brief quitting advice (Sessions) or more frequent weekly sessions (Sessions+)? These questions concern intervention timing as they focus on which component should be offered at different points in time-at study outset (week 0) and at week 2. Hence, the answer to Question 4 would be \"Yes\", leading investigators to Question 5."
    },
    {
      "title": "Question 5: Change Slowly or Rapidly?",
      "text": "The last question is whether the components in question are intended to address conditions that change relatively slowly (e.g., over months or weeks) or rapidly (e.g., every day, hours, minutes)?"
    },
    {
      "title": "Slowly: Components Address Conditions That Change Slowly",
      "text": "Examples where the answer to Question 5 may be \"slowly\" include cases where investigators are interested in developing an adaptive intervention (ADI) and they need to answer questions about the selection and adaptation of components in this intervention. ADIs are interventions that use dynamic (timevarying) information about the person's progress in the course of an intervention (e.g., early response status, adherence) to decide which intervention component to deliver at different decision points  (22, 23) . Each decision point represents a point in time in which a decision should be made in practice about whether and how to modify the intervention based on what is known about the individual's status and progress. Here, the term \"adaptation\" refers to the use of dynamic information about the person to decide whether and how to intervene. In the hypothetical example in Figure  2 , information about response status (operationalized in terms of whether or not the individual self-reported tobacco use within the past 7 days) is used to decide who should be offered more support and who should continue with the initial intervention. This adaptation process is initiated at week 2 because it is intended to address early signs of non-response, which are expected to unfold (in this example) over 2 weeks. The underlying assumption is that offering more support to those who show early signs of non-response at week 2 would prevent their likely failure to achieve long-term abstinence, whereas those who show early signs of response at week 2 are likely to succeed ultimately and hence would benefit from continuing with the same initial intervention.\n\nMore generally, the term ADI usually refers to interventions whose adaptation addresses conditions that change relatively slowly (e.g., over weeks or months). The adaptation is designed to increase the ultimate effectiveness of the intervention (by delivering the type/amount of intervention needed, when it is needed), while reducing cost and burden (by avoiding unnecessary treatment)  (23) (24) (25) . While technology offers tremendous potential for delivering ADIs, empirical data is often needed to inform the selection and adaptation of intervention components, and these knowledge gaps can motivate scientific questions for randomized studies [e.g.,  (26) (27) (28) (29) (30) ]. Consider Example C discussed above where there are two open scientific questions concerning the development of an ADI: (1) at week 0, should the intervention include only Mobile, or Mobile + Prompts? (2) at week 2, should early non-responders be offered Sessions, or Sessions+? These questions concern which component(s) to deliver at different decision points, in an intervention that intends to address conditions that change relatively slowly. Hence, as suggested in Figure  1 , investigators may consider a SMART design."
    },
    {
      "title": "Sequential Multiple Assignment Randomized Trial Designs",
      "text": "A SMART is a randomized trial that includes sequential randomizations  (13, 14) ; that is, some or all of the study participants are randomized more than once in the course of the study  (31) . Each randomization in a SMART is designed to answer scientific questions about the selection and adaptation of components at a specific decision point with the goal of empirically informing the development of an ADI. Consider again the two scientific questions outlined in Example C. The first question about which component to offer at week 0 can be answered by randomizing individuals at week 0 to either Mobile or Mobile+Prompt. The second question about which component to offer early non-responders at week 2 can be answered by re-randomizing early non-responders at week 2 to either Sessions or Sessions+. Responders do not get rerandomized and instead continue with the initial intervention. These sequential randomizations result in six experimental conditions, labeled A through F (Figure  3 ).\n\nSuppose that 400 individuals enter the study and that they are randomized with equal probability (0.5) at week 0. Also, for simplicity suppose that 50% of the participants become classified as non-responders. Suppose that non-responders are re-randomized at week 2 with equal probability (0.5) to the two subsequent components. As before, suppose that the primary outcome of interest is measured at the month 6 follow up. Figure  3  shows the number of participants in each experimental condition A-F based on these assumptions. Similar to the analysis described for Example B, the analyses for addressing the scientific questions in Example C leverage outcome information across multiple experimental conditions. Specifically, the first question can be answered by comparing the mean outcome across all the experimental conditions in which participants were offered Mobile initially (conditions A, B and C; n = 200; Figure  3 ) to the mean outcome across all the conditions in which participants were offered Mobile+Prompts initially (conditions D, E and F; n = 200; Figure  3 ). Notice that as before, this would involve using outcome data from the entire sample to estimate the effect, which can be viewed as the main effect of the initial component averaging over the subsequent components for responders and non-responders. The second scientific question can be answered by comparing the mean outcome across the two experimental conditions in which early non-responders were offered Sessions subsequently (conditions B and E; n = 100; Figure  3 ) to the mean outcome across the two conditions in which early nonresponders were offered Sessions+ subsequently (conditions C and F; n = 100; Figure  3 ). Notice that this comparison would involve using outcome data from the entire sample of early nonresponders to estimate the effect, so it can be viewed as the main effect of the subsequent components among early nonresponders, averaging over the initial components.\n\nNotice that similar to the classic factorial design discussed earlier, the SMART enables investigators to use outcome data from each study participant to test more than one main effect, thereby answering multiple scientific questions about the selection of components. Hence, SMART designs share a somewhat similar \"recycling\" property as classic factorial designs.\n\nIn recent years, various types of SMART designs and analytic methods have been developed to enable investigators to address specific scientific questions about the selection and adaptation of components. Examples include questions about the differences between ADIs that are embedded in a SMART, and the selection of the best embedded ADI [e.g.,  (22, 32, 33) ], how well components that are offered at different time points work together [e.g.,  (25, 34) ], and what type of information should be used to decide which component to offer [e.g.,  (35) (36) (37) (38) ]. These methods enable investigators to leverage the SMART to inform the development of a wide variety of ADIs."
    },
    {
      "title": "Rapidly: Components Address Conditions That Change Rapidly",
      "text": "Examples where the answer to Question 5 may be \"rapidly\" include cases where investigators are interested in developing a just in time adaptive intervention (JITAI) and they need to answer questions about the selection and adaptation of components in this intervention. A JITAI employs adaptation to address conditions that change relatively rapidly (e.g., every few days, hours, or minutes) and in the person's natural environment  (39, 40) . Similar to ADIs, the adaptation in JITAIs is designed to enhance the effectiveness of the intervention and to reduce burden by delivering the type/amount of intervention needed, only to those who need it, and only when they need it. However, because JITAIs are intended to address conditions that change rapidly and in daily life, where multiple demands compete for the person's attention and effort, the adaptation is also explicitly intended to minimize disruptions to the daily lives of individuals  (40, 41) .\n\nAs an example (Example D), consider a JITAI for preventing a smoking lapse by delivering a mobile-based prompt [e.g., via a push notification from a mobile app; see  (42) ] that recommends a self-regulatory strategy when individuals self-report an urge to smoke and are not driving a car. Participants' urge to smoke is monitored via Ecological Momentary Assessments (EMAs)  (43)  four times per day (triggered at randomly selected times and spread throughout the person's waking hours, with \u223c3 h between each), and assisted GPS technology is used to track and calculate their minute-by-minute mobility pattern  (44) (45) (46) . If the person self-reports high urge to smoke via the EMA and they are not driving a car, a prompt recommending a brief selfregulatory strategy is delivered on their mobile device. Otherwise, a prompt is not delivered (Figure  4 ). This intervention is adaptive because it uses dynamic information about the person's internal state (i.e., urge to smoke) and context (i.e., mobility pattern) to decide whether and how to intervene. This intervention is a JITAI because the adaptation is intended to address conditions that change rapidly (urge to smoke may emerge multiple times per day, representing risk for a smoking lapse) based on realworld context (urge to smoke may emerge in the person's natural environment, outside of standard treatment settings). The adaptation in this hypothetical JITAI is intended not only to prevent a smoking lapse (by addressing high urge to smoke) while avoiding unnecessary intervention (not delivering an intervention when urge to smoke is less than high), but also to minimize disruptions to the daily lives of individuals (by not delivering an intervention when the person is driving).\n\nAlthough advances in mobile and wireless technology offer tremendous potential for delivering JITAIs, researchers often need more empirical evidence to inform the selection and adaptation of components in a JITAI, and these knowledge gaps can motivate scientific questions for randomized studies [e.g.,  (47) (48) (49) ]. Consider Example D discussed above and suppose there is not yet sufficient evidence to determine (a) whether delivering a prompt that recommends a brief self-regulatory strategy is beneficial on average in preventing a lapse in the next 3 h, when individuals are not driving; and (b) what is the level of urge at which the prompt would be most beneficial. These questions concern the best component to deliver at different points in time in an intervention that intends to address conditions that change relatively rapidly. Hence, the answer to Question 5 would be \"rapidly\", leading investigators to consider an MRT."
    },
    {
      "title": "The Micro-Randomized Trial Design",
      "text": "Similar to the SMART, the MRT is a randomized trial that includes sequential randomizations  (15, 16) . However, MRTs include more frequent randomizations (relative to the SMART) because they are intended to provide data that investigators can use to answer questions about the selection and adaptation of components in a JITAI-an intervention that is motivated to address conditions that change rapidly. Accordingly, whereas the SMART is designed to answer questions about the longer-term effects of weeks or months of treatment on a distal outcome (e.g., an outcome measured at the month 6 follow up; Figure  3 ), the MRT is designed to answer questions about the short-term effects of relatively brief interventions on a proximal outcome (e.g., an outcome measured in the next 3 hours following a decision point).\n\nFor example, the MRT in Figure  5  is designed to answer the scientific questions outlined in Example D. Recall that these questions were: (1) whether delivering a prompt is beneficial on average in preventing a lapse in the next 3 hours when individuals are not driving; and (  2 ) what is the level of urge in which the prompt would be most beneficial. To answer these questions, following each EMA which involves assessment of smoking urge, the person is randomized (with 0.5 probability) to either deliver a prompt or no prompt, provided that the person is not driving. If the person is driving a car, no prompt is delivered.\n\nSimilar to classic factorial designs and SMART designs, MRTs make extremely efficient use of study participants to answer questions about the selection and adaptation of components in a JITAI. This efficiency is facilitated by capitalizing on both between-subject and within-subject contrasts in the primary outcome  (15, 50) . For example, consider the first scientific question. Here, the primary outcome is whether or not the person experiences a lapse in the next 3 h. This (binary) outcome is assessed following each randomization. Hence, the first question can be answered by comparing two probabilities:\n\n(1) the probability of experiencing a lapse in the next 3 h when the individual was not driving and a prompt was triggered following the EMA, and (2) the probability of experiencing a lapse in the next 3 h when the individual was not driving and a prompt was not triggered following the EMA. This difference can be estimated by pooling data across all study participants and also across all decision points in which the individual was not driving  (51) . This is an estimate of the (causal) main effect of delivering a prompt (vs. no prompt), at decision points in which the individual is not driving. The second scientific question can be answered by investigating whether the difference between the two probabilities described above varies depending on the levels of smoking urge (which was self-reported via the EMA immediately prior to randomization). That is, the data can be used to investigate whether the current level of urge moderates the causal effect of delivering (vs. not delivering) a prompt on the likelihood of lapse, provided that individuals are not driving. As before, this moderation analysis would use data across all study participants and across all decision points in which individuals were not driving  (51) . Estimates of the difference in lapse likelihood between delivering vs. not delivering a prompt at different levels of urge (e.g., low, moderate, and high) can be used to further identify the level(s) at which delivering (vs. not delivering) a prompt would be most beneficial.\n\nAlthough the MRT is a relatively new experimental design, various types of MRT designs and analytic methods have been developed, allowing investigators to address scientific questions about the development of JITAIs. Examples include studies in which the proximal outcome is continuous  (52)  and binary  (51) , and studies in which the randomization probabilities are stratified to provide sufficient data to detect an interaction between the intervention options and a time-varying covariate (e.g., urge to smoke)  (53) . These methods enable investigators to leverage the MRT to inform the development of a wide variety of JITAIs."
    },
    {
      "title": "DISCUSSION",
      "text": "This manuscript is intended to help investigators interested in developing a digital intervention select the most appropriate experimental design in light of their motivating scientific questions. Although existing tutorials can be used to guide the design and analysis of data from factorial designs [e.g.,  (11, 19, 20) ], SMARTs [e.g.,  (22, 25) ], and MRTs [e.g.,  (15, 16) ], the selection of an appropriate experimental design remains an important challenge. The current framework emphasizes that specifying the scientific questions that are to be addressed is a critical prerequisite for selecting an experimental design.\n\nAlthough the examples provided in this manuscript focus on the development of a tobacco cessation intervention, the current framework is behavior agnostic. Thus, it can be used to inform not only tobacco cessation interventions, but also a wide variety of interventions."
    },
    {
      "title": "Limitations",
      "text": "There are several limitations to the current framework. First, for simplicity we focus on four types of experimental approaches. However, other types of experimental approaches [e.g., singlecase designs  (54) , platform trials  (55) ] as well as non-randomized (observational) study designs  (56)  can be used to inform the development of digital interventions. Second, in much of this paper, we assumed that the scientific questions could be phrased in terms of a contrast between expected values of a single outcome between possible intervention options. However, a common challenge in intervention design is combining information from multiple goals and constraints, and often multiple stakeholders. A fruitful area for future methodological research would be analysis methods for more readily considering multiple outcomes or endpoints, and incorporating cost information (both monetary cost, or cost in time or effort)  (57) . Finally, the current framework does not provide guidance on how to address questions about the delivery of components at different timescales. Consider the following question: Under what conditions (e.g., level of urge, location) should a prompt that recommends a brief self-regulatory strategy be delivered to enhance therapeutic gains from weekly online quitting advice? This question concerns the synergy between components that are delivered at two timescales: (1) every few hours (prompts) and (  2 ) weekly (quitting advice). Future research is needed to develop new experimental approaches and analytic methods that will enable scientists to investigate these types of potential synergies."
    },
    {
      "title": "CONCLUSION",
      "text": "While the current framework is not all-inclusive, it represents an important step in the development of clear guidelines for selecting study designs to inform the development of effective and scalable digital interventions."
    },
    {
      "text": "FIGURE 1 | MCMTC: a pragmatic framework for selecting an experimental design to inform the development of digital interventions."
    },
    {
      "text": "FIGURE 2 | Hypothetical adaptive intervention (Example C)."
    },
    {
      "text": "FIGURE 3 | SMART study to answer example C scientific questions."
    },
    {
      "text": "FIGURE 4 | Hypothetical just in time adaptive intervention (Example D)."
    },
    {
      "text": "FIGURE 5 | A MRT to answer Example E scientific questions."
    }
  ],
  "references": [
    {
      "title": "Mobile phone-based interventions for smoking cessation",
      "authors": [
        "R Whittaker",
        "H Mcrobbie",
        "C Bullen",
        "A Rodgers",
        "Y Gu"
      ],
      "year": 2016,
      "doi": "10.1002/14651858.CD006611.pub4"
    },
    {
      "title": "Effectiveness of mobile health application use to improve health behavior changes: a systematic review of randomized controlled trials",
      "authors": [
        "M Han",
        "E Lee"
      ],
      "year": 2018,
      "doi": "10.4258/hir.2018.24.3.207"
    },
    {
      "title": "Mobile apps for health behavior change in physical activity, diet, drug and alcohol use, and mental health: systematic review",
      "authors": [
        "M Milne-Ives",
        "C Lam",
        "De Cock",
        "C Van Velthoven",
        "M Meinert"
      ],
      "year": 2020,
      "doi": "10.2196/17046"
    },
    {
      "title": "Optimization of Behavioral, Biobehavioral, and Biomedical Interventions: The Multiphase Optimization Strategy (MOST)",
      "authors": [
        "L Collins"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-319-72206-1_1"
    },
    {
      "title": "A smartphone app to reduce excessive alcohol consumption: identifying the effectiveness of intervention components in a factorial randomised control trial",
      "authors": [
        "D Crane",
        "C Garnett",
        "S Michie",
        "R West",
        "J Brown"
      ],
      "year": 2018,
      "doi": "10.1038/s41598-018-22420-8"
    },
    {
      "title": "Web-based smoking-cessation programs: results of a randomized trial",
      "authors": [
        "V Strecher",
        "J Mcclure",
        "G Alexander",
        "B Chakraborty",
        "V Nair",
        "J Konkel"
      ],
      "year": 2008,
      "doi": "10.1016/j.amepre.2007.12.024"
    },
    {
      "title": "Integrating human support into behavioral intervention technologies: the efficiency model of support",
      "authors": [
        "S Schueller",
        "K Tomasino",
        "D Mohr"
      ],
      "doi": "10.1037/h0101740"
    },
    {
      "title": "Use of gamification strategies and tactics in mobile applications for smoking cessation: a review of the UK mobile app market",
      "authors": [
        "N Rajani",
        "D Weth",
        "N Mastellos",
        "F Filippidis"
      ],
      "year": 2019,
      "doi": "10.1136/bmjopen-2018-027883"
    },
    {
      "title": "To prompt or not to prompt? A microrandomized trial of time-varying push notifications to increase proximal engagement with a mobile health app",
      "authors": [
        "N Bidargaddi",
        "D Almirall",
        "S Murphy",
        "Nahum Shani",
        "I Kovalcik",
        "M Pituch"
      ],
      "year": 2018,
      "doi": "10.2196/10123"
    },
    {
      "title": "Text messaging-based smoking cessation intervention: a narrative review",
      "authors": [
        "G Kong",
        "D Ells",
        "D Camenga",
        "S Krishnan-Sarin"
      ],
      "year": 2014,
      "doi": "10.1016/j.addbeh.2013.11.024"
    },
    {
      "title": "Design of experiments with multiple independent variables: a resource management perspective on complete and reduced factorial designs",
      "authors": [
        "L Collins",
        "J Dziak",
        "R Li"
      ],
      "year": 2009,
      "doi": "10.1037/a0015826"
    },
    {
      "title": "Experiments: Planning, Analysis, and Optimization",
      "authors": [
        "C Wu",
        "M Hamada"
      ],
      "year": 2011
    },
    {
      "title": "An experimental design for the development of adaptive treatment strategies",
      "authors": [
        "S Murphy"
      ],
      "year": 2005,
      "doi": "10.1002/sim.2022"
    },
    {
      "title": "A design for testing clinical strategies: biased adaptive within-subject randomization",
      "authors": [
        "P Lavori",
        "R Dawson"
      ],
      "year": 2000,
      "doi": "10.1111/1467-985x.00154"
    },
    {
      "title": "Sample size calculations for micro-randomized trials in mHealth",
      "authors": [
        "P Liao",
        "P Klasnja",
        "A Tewari",
        "S Murphy"
      ],
      "year": 2016,
      "doi": "10.1002/sim.6847"
    },
    {
      "title": "The micro-randomized trial for developing digital interventions: experimental design and data analysis considerations",
      "authors": [
        "T Qian",
        "A Walton",
        "L Collins",
        "P Klasnja",
        "S Lanza",
        "Nahum Shani"
      ],
      "year": 2022,
      "doi": "10.1037/met0000283"
    },
    {
      "title": "Sample size calculations for randomized controlled trials",
      "authors": [
        "J Wittes"
      ],
      "year": 2002,
      "doi": "10.1093/epirev/24.1.39"
    },
    {
      "title": "Developing multicomponent interventions using fractional factorial designs",
      "authors": [
        "B Chakraborty",
        "L Collins",
        "V Strecher",
        "S Murphy"
      ],
      "year": 2009,
      "doi": "10.1002/sim.3643"
    },
    {
      "title": "Multilevel factorial experiments for developing behavioral interventions: power, sample size, and resource considerations",
      "authors": [
        "J Dziak",
        "Nahum Shani",
        "I Collins"
      ],
      "year": 2012,
      "doi": "10.1037/a0026972"
    },
    {
      "title": "Multilevel factorial designs with experiment-induced clustering",
      "authors": [
        "Nahum-Shani I Dziak",
        "J Collins"
      ],
      "year": 2018,
      "doi": "10.1037/met0000128"
    },
    {
      "title": "Multilevel factorial designs in intervention development",
      "authors": [
        "Nahum-Shani I Dziak"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-319-91776-4_3"
    },
    {
      "title": "Experimental design and primary data analysis methods for comparing adaptive interventions",
      "authors": [
        "Nahum-Shani I Qian",
        "M Almirall",
        "D Pelham",
        "W Gnagy",
        "B Fabiano"
      ],
      "year": 2012,
      "doi": "10.1037/a0029372"
    },
    {
      "title": "conceptual framework for adaptive preventive interventions",
      "authors": [
        "L Collins",
        "S Murphy",
        "K Bierman"
      ],
      "year": 2004,
      "doi": "10.1023/b:prev.0000037641.26017.00"
    },
    {
      "title": "Introduction to adaptive interventions",
      "authors": [
        "L Collins"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-319-72206-1_1"
    },
    {
      "title": "SMART longitudinal analysis: a tutorial for using repeated outcome measures from SMART studies to compare adaptive interventions",
      "authors": [
        "Nahum-Shani I Almirall",
        "D Yap",
        "J Mckay",
        "J Lynch",
        "K Freiheit"
      ],
      "year": 2020,
      "doi": "10.1037/met0000219"
    },
    {
      "title": "Working memory training and high magnitude incentives for youth cannabis use: a SMART pilot trial",
      "authors": [
        "C Stanger",
        "E Scherer",
        "H Vo",
        "S Babbin",
        "A Knapp",
        "J Mckay"
      ],
      "year": 2020,
      "doi": "10.1037/adb0000480"
    },
    {
      "title": "A sequential multiple assignment randomized trial (SMART) protocol for empirically developing an adaptive preventive intervention for college student drinking reduction",
      "authors": [
        "M Patrick",
        "J Boatman",
        "N Morrell",
        "A Wagner",
        "G Lyden",
        "Nahum Shani"
      ],
      "year": 2020,
      "doi": "10.1016/j.cct.2020.106089"
    },
    {
      "title": "Main outcomes of M-bridge: a sequential multiple assignment randomized trial (SMART) for developing an adaptive preventive intervention for college drinking",
      "authors": [
        "M Patrick",
        "G Lyden",
        "N Morrell",
        "C Mehus",
        "M Gunlicks-Stoessel",
        "C Lee"
      ],
      "year": 2021,
      "doi": "10.1037/ccp0000663"
    },
    {
      "title": "Adaptive antiretroviral therapy adherence interventions for youth living with HIV through text message and cell phone support with and without incentives: protocol for a sequential multiple assignment randomized trial (SMART)",
      "authors": [
        "M Belzer",
        "K Macdonell",
        "S Ghosh",
        "S Naar",
        "J Mcavoy-Banerjea",
        "S Gurung"
      ],
      "year": 2018,
      "doi": "10.2196/11183"
    },
    {
      "title": "SMART: study protocol for a sequential multiple assignment randomized controlled trial to optimize weight loss management",
      "authors": [
        "A Pfammatter",
        "Nahum Shani",
        "I Dezelar",
        "M Scanlan",
        "L Mcfadden",
        "H Siddique"
      ],
      "year": 2019,
      "doi": "10.1016/j.cct.2019.05.007"
    },
    {
      "title": "An Introduction to Adaptive Interventions and SMART Designs in Education",
      "authors": [
        "Nahum-Shani I Almirall"
      ],
      "year": 2019,
      "doi": "10.1007/s13142-014-0265-0"
    },
    {
      "title": "Power analysis in a SMART design: sample size estimation for determining the best embedded dynamic treatment regime",
      "authors": [
        "W Artman",
        "Nahum Shani",
        "I Wu",
        "T Mckay",
        "J Ertefaie"
      ],
      "year": 2020,
      "doi": "10.1093/biostatistics/kxy064"
    },
    {
      "title": "Identifying a set that contains the best dynamic treatment regimes",
      "authors": [
        "A Ertefaie",
        "T Wu",
        "K Lynch",
        "Nahum Shani"
      ],
      "year": 2016,
      "doi": "10.1093/biostatistics/kxv025"
    },
    {
      "title": "data analysis method for using longitudinal binary outcome data from a SMART to compare adaptive interventions",
      "authors": [
        "J Dziak",
        "J Yap",
        "D Almirall",
        "J Mckay",
        "K Lynch",
        "Nahum Shani"
      ],
      "year": 2019,
      "doi": "10.1080/00273171.2018.1558042"
    },
    {
      "title": "Using intensive longitudinal data to identify early predictors of suicide-related outcomes in high-risk adolescents: practical and conceptual considerations",
      "authors": [
        "E Czyz",
        "J Yap",
        "C King",
        "Nahum Shani"
      ],
      "year": 2021,
      "doi": "10.1177/1073191120939168"
    },
    {
      "title": "Q-learning: a data analysis method for constructing adaptive interventions",
      "authors": [
        "Nahum-Shani I Qian",
        "M Almirall",
        "D Pelham",
        "W Gnagy",
        "B Fabiano"
      ],
      "year": 2012,
      "doi": "10.1037/a0029373"
    },
    {
      "title": "A SMART data analysis method for constructing adaptive treatment strategies for substance use disorders",
      "authors": [
        "Nahum-Shani I Ertefaie",
        "A Lu",
        "X Lynch",
        "K Mckay",
        "J Oslin"
      ],
      "year": 2017,
      "doi": "10.1111/add.13743"
    },
    {
      "title": "Q-learning: theory and applications",
      "authors": [
        "J Clifton",
        "E Laber"
      ],
      "year": 2020,
      "doi": "10.1146/annurev-statistics-031219-041220"
    },
    {
      "title": "Dynamic models of behavior for justin-time adaptive interventions",
      "authors": [
        "D Spruijt-Metz",
        "W Nilsen"
      ],
      "year": 2014,
      "doi": "10.1109/mprv.2014.46"
    },
    {
      "title": "Just-in-time adaptive interventions (JITAIs) in mobile health: key components and design principles for ongoing health behavior support",
      "authors": [
        "Nahum-Shani I Smith",
        "S Spring",
        "B Collins",
        "L Witkiewitz",
        "K Tewari"
      ],
      "year": 2018,
      "doi": "10.1007/s12160-016-9830-8"
    },
    {
      "title": "Building health behavior models to guide the development of just-in-time adaptive interventions: a pragmatic framework",
      "authors": [
        "Nahum-Shani I Hekler",
        "E Spruijt-Metz"
      ],
      "year": 2015,
      "doi": "10.1037/hea0000306"
    },
    {
      "title": "The mobile assistance for regulating smoking (MARS) micro-randomized trial design protocol",
      "authors": [
        "Nahum-Shani I Potter",
        "L Lam",
        "C Yap",
        "J Moreno",
        "A Stoffel"
      ],
      "year": 2021,
      "doi": "10.1016/j.cct.2021.106513"
    },
    {
      "title": "Ecological momentary assessment",
      "authors": [
        "S Shiffman",
        "A Stone",
        "M Hufford"
      ],
      "year": 2008,
      "doi": "10.1146/annurev.clinpsy.3.022806.091415"
    },
    {
      "title": "Addressing location uncertainties in GPSbased activity monitoring: a methodological framework",
      "authors": [
        "N Wan",
        "Lin Kan",
        "G Wilson"
      ],
      "year": 2017,
      "doi": "10.1111/tgis.12231"
    },
    {
      "title": "Classifying human activity patterns from smartphone collected GPS data: a fuzzy classification and aggregation approach",
      "authors": [
        "N Wan",
        "G Lin"
      ],
      "year": 2016,
      "doi": "10.1111/tgis.12181"
    },
    {
      "title": "Life-space characterization from cellular telephone collected GPS data",
      "authors": [
        "N Wan",
        "G Lin"
      ],
      "year": 2013,
      "doi": "10.1016/j.compenvurbsys.2013.01.003"
    },
    {
      "title": "Sense2Stop: a micro-randomized trial using wearable sensors to optimize a just-in-time-adaptive stress management intervention for smoking relapse prevention",
      "authors": [
        "S Battalio",
        "D Conroy",
        "W Dempsey",
        "P Liao",
        "M Menictas",
        "S Murphy"
      ],
      "year": 2021,
      "doi": "10.1016/j.cct.2021.106534"
    },
    {
      "title": "Notifications to improve engagement with an alcohol reduction app: protocol for a micro-randomized trial",
      "authors": [
        "L Bell",
        "C Garnett",
        "T Qian",
        "O Perski",
        "H Potts",
        "E Williamson"
      ],
      "year": 2020,
      "doi": "10.2196/18690"
    },
    {
      "title": "Translating strategies for promoting engagement in mobile health: a proof-of-concept microrandomized trial",
      "authors": [
        "Nahum-Shani I Rabbi",
        "M Yap",
        "J Philyaw-Kotov",
        "M Klasnja",
        "P Bonar"
      ],
      "year": 2021,
      "doi": "10.1037/hea0001101"
    },
    {
      "title": "Micro-randomized trial",
      "authors": [
        "G Xu",
        "Z Wu",
        "S Murphy"
      ],
      "year": 2014,
      "doi": "10.1002/9781118445112"
    },
    {
      "title": "Estimating time-varying causal excursion effects in mobile health with binary outcomes",
      "authors": [
        "T Qian",
        "H Yoo",
        "P Klasnja",
        "D Almirall",
        "S Murphy"
      ],
      "year": 2020,
      "doi": "10.1093/biomet/asaa070"
    },
    {
      "title": "Assessing time-varying causal effect moderation in mobile health",
      "authors": [
        "A Boruvka",
        "D Almirall",
        "K Witkiewitz",
        "S Murphy"
      ],
      "year": 2018,
      "doi": "10.1080/01621459.2017.1305274"
    },
    {
      "title": "The stratified micro-randomized trial design: sample size considerations for testing nested causal effects of time-varying treatments",
      "authors": [
        "W Dempsey",
        "P Liao",
        "S Kumar",
        "S Murphy"
      ],
      "year": 2020,
      "doi": "10.1214/19-aoas1293"
    },
    {
      "title": "Research designs to develop and evaluate technology-based health behavior interventions",
      "authors": [
        "J Dallery",
        "W Riley",
        "Nahum Shani"
      ],
      "year": 2014,
      "doi": "10.1093/med/9780199314027.003.0012"
    },
    {
      "title": "The platform trial: an efficient strategy for evaluating multiple treatments",
      "authors": [
        "S Berry",
        "J Connor",
        "R Lewis"
      ],
      "year": 2015,
      "doi": "10.1001/jama.2015.2316"
    },
    {
      "title": "Dose-response research in digital health interventions: concepts, considerations, and challenges",
      "authors": [
        "M Mcvay",
        "G Bennett",
        "D Steinberg",
        "C Voils"
      ],
      "year": 2019,
      "doi": "10.1037/hea0000805"
    },
    {
      "title": "Optimizing the cost-effectiveness of a multicomponent intervention using data from a factorial experiment: considerations, open questions, and tradeoffs among multiple outcomes",
      "authors": [
        "J Dziak"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-319-91776-4_7"
    }
  ],
  "num_references": 57
}
