{
  "paper_id": "6V7KP28U",
  "title": "Considerations for addressing bias in artificial intelligence for health equity",
  "abstract": "Health equity is a primary goal of healthcare stakeholders: patients and their advocacy groups, clinicians, other providers and their professional societies, bioethicists, payors and value based care organizations, regulatory agencies, legislators, and creators of artificial intelligence/machine learning (AI/ML)-enabled medical devices. Lack of equitable access to diagnosis and treatment may be improved through new digital health technologies, especially AI/ML, but these may also exacerbate disparities, depending on how bias is addressed. We propose an expanded Total Product Lifecycle (TPLC) framework for healthcare AI/ML, describing the sources and impacts of undesirable bias in AI/ML systems in each phase, how these can be analyzed using appropriate metrics, and how they can be potentially mitigated. The goal of these \"Considerations\" is to educate stakeholders on how potential AI/ML bias may impact healthcare outcomes and how to identify and mitigate inequities; to initiate a discussion between stakeholders on these issues, in order to ensure health equity along the expanded AI/ML TPLC framework, and ultimately, better health outcomes for all.",
  "year": 2015,
  "date": "2015",
  "journal": "Glob. Health Action",
  "publication": "Glob. Health Action",
  "authors": [
    {
      "forename": "Michael",
      "surname": "Abr\u00e0moff",
      "name": "Michael Abr\u00e0moff",
      "affiliation": "1  Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation , Washington , D.C. \n\t\t\t\t\t\t\t\t Collaborative Community for Ophthalmic Imaging Foundation \n\t\t\t\t\t\t\t\t Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Washington \n\t\t\t\t\t\t\t\t\t D.C",
      "email": "michael-abramoff@uiowa.edu",
      "orcid": "0000-0002-3490-0037"
    },
    {
      "forename": "Michelle",
      "surname": "Tarver",
      "name": "Michelle Tarver",
      "affiliation": "2  Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA. \n\t\t\t\t\t\t\t\t Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering \n\t\t\t\t\t\t\t\t University of Iowa \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Iowa City \n\t\t\t\t\t\t\t\t\t IA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "forename": "Nilsa",
      "surname": "Loyo-Berrios",
      "name": "Nilsa Loyo-Berrios",
      "affiliation": "2  Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA. \n\t\t\t\t\t\t\t\t Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering \n\t\t\t\t\t\t\t\t University of Iowa \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Iowa City \n\t\t\t\t\t\t\t\t\t IA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "forename": "Sylvia",
      "surname": "Trujillo",
      "name": "Sylvia Trujillo",
      "affiliation": "3  Center for Devices and Radiological Health , US Food and Drug Administration , Silver Spring , MD , USA. \n\t\t\t\t\t\t\t\t Center for Devices and Radiological Health \n\t\t\t\t\t\t\t\t US Food and Drug Administration \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Silver Spring \n\t\t\t\t\t\t\t\t\t MD \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "forename": "Danton",
      "surname": "Char",
      "name": "Danton Char",
      "affiliation": "4  OCHIN , Portland , OR , USA. Center for Biomedical Ethics , \n\t\t\t\t\t\t\t\t Center for Biomedical Ethics \n\t\t\t\t\t\t\t\t OCHIN \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Portland \n\t\t\t\t\t\t\t\t\t OR \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "forename": "Ziad",
      "surname": "Obermeyer",
      "name": "Ziad Obermeyer",
      "affiliation": "6  Stanford University School of Medicine , Division of Pediatric Cardiac Anesthesia , San Francisco , CA , USA. \n\t\t\t\t\t\t\t\t Division of Pediatric Cardiac Anesthesia \n\t\t\t\t\t\t\t\t Stanford University School of Medicine \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t San Francisco \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "forename": "Malvina",
      "surname": "Eydelman",
      "name": "Malvina Eydelman",
      "affiliation": "2  Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA. \n\t\t\t\t\t\t\t\t Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering \n\t\t\t\t\t\t\t\t University of Iowa \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Iowa City \n\t\t\t\t\t\t\t\t\t IA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "forename": "William",
      "surname": "Maisel",
      "name": "William Maisel",
      "affiliation": "2  Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering , University of Iowa , Iowa City , IA , USA. \n\t\t\t\t\t\t\t\t Departments of Ophthalmology and Visual Sciences, and Electrical and Computer Engineering \n\t\t\t\t\t\t\t\t University of Iowa \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Iowa City \n\t\t\t\t\t\t\t\t\t IA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "forename": "D",
      "surname": "Washington",
      "name": "D Washington"
    },
    {
      "forename": "D",
      "surname": "Michael",
      "name": "D Michael"
    },
    {
      "surname": "Abr\u00e0moff",
      "name": "Abr\u00e0moff",
      "affiliation": "1  Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group of the Collaborative Community for Ophthalmic Imaging Foundation , Washington , D.C. \n\t\t\t\t\t\t\t\t Collaborative Community for Ophthalmic Imaging Foundation \n\t\t\t\t\t\t\t\t Foundational Principles of Ophthalmic Imaging and Algorithmic Interpretation Working Group \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Washington \n\t\t\t\t\t\t\t\t\t D.C",
      "orcid": "0000-0002-3490-0037"
    },
    {
      "affiliation": "School of Public Health , University of California , Berkeley , CA , USA. \n\t\t\t\t\t\t\t\t School of Public Health \n\t\t\t\t\t\t\t\t University of California \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Berkeley \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA"
    }
  ],
  "doi": "10.1038/s41746-023-00913-9",
  "sections": [
    {
      "title": "INTRODUCTION",
      "text": "The US Department of Health and Human Services defines health equity 1 as the absence of avoidable disparities or differences among socioeconomic and demographic groups or geographic areas in health status and health outcomes such as disease, disability, or mortality  2  . While there are multiple reasons for avoidable health inequities  2  , lack of equitable access to diagnosis and treatment are prominent in diseases ranging from breast cancer, depression, to diabetic eye disease  [3] [4] [5] [6] [7] [8]  . Fostering health equity has been a goal of healthcare stakeholders: patients and their organizations, providers, ethicists, payors, regulators, legislators, and AI creators. With the exponential growth in new digital health technologies and the rise of artificial intelligence/machine learning (AI/ML)-enabled medical devices, innovators may potentiate existing disparities or instead, leverage opportunities to mitigate health inequities  9  .\n\nArtificial Intelligence (AI) systems can perform tasks that mimic human cognitive capabilities  10  , or may perform new functions that humans are unable to do  11  . Such AI systems are typically not explicitly programmed, the systems learn from data that reflect highly cognitive tasks that may otherwise be performed by trained healthcare professionals. In many cases, AI systems are intended to aid healthcare professionals (HCPs) in managing or treating patients; there are also AI systems intended to be used directly by patients to help manage a disease or condition  12  . Healthcare AI systems have the potential to foster access to healthcare for underserved populations, while improving care quality at both the level of the individual patient and the population, at reduced cost for patients, payors, and society  [2] [3] [4] [5] [6] [7] [8] 10, [12] [13] [14] [15] [16]  .\n\nSome healthcare AI-enabled devices have been authorized by FDA and have been in clinical use for over a decade, with more devices being currently developed. While the vast majority of AI systems intended to be used by HCPs serve to aid those HCPs, there are also AI/ML-enabled devices that make a clinical decision without human oversight, including the first point-of-care autonomous AI system on the US market  17  , which received national coverage and reimbursement thereby allowing widespread deployment  18, 19  . Thus, AI systems are increasingly in a position to help improve patient and population health outcomes and drive down cost, increase physician job satisfaction, and address health disparities  20, 21, 3, 22, 23  .\n\nHowever, adding AI to healthcare processes may unintentionally have undesired effects. Multiple studies have shown examples of the use of AI in healthcare (not evaluated by regulatory agencies) exacerbating, rather than mitigating, health disparities  24, 25  . This is especially the case where the systems that utilize AI do not adhere to rapidly emerging evidence-based standards  26  , or where these may be designed for non-marketed use but ultimately are used more broadly. One study of a widely-used AI system showed that while its stated goal was to identify patients who needed extra help with their complex health needs, its actual objective function (its \"achieved goal\") was to predict healthcare costs. This use of the AI system out of context resulted in sicker, Black patients receiving similar care to healthier, White patients despite needing higher acuity care. Thus, the inherent bias in the algorithm appeared to contribute to worse outcomes for Black patients by influencing the likelihood they would receive the appropriate level of care  25  .\n\nThese ethical and other concerns with AI in healthcare have been shown by a number of research studies  [27] [28] [29]  . Abramoff et al. proposed ethical frameworks for AI  [27] [28] [29] [30] [31]  to help proactively address the issue of undesirable algorithmic bias as well as other concerns with AI. More recently, the Foundational Considerations on Algorithmic Interpretation (FPOAI) workgroup of the Collaborative Community on Ophthalmic Imaging published their \"Foundational Considerations\"  32  on AI as a start to developing metrics for ethics, including metrics for \"Equity\"  33  , in order to be able to evaluate how specific AI systems adhere to various bioethical principles.\n\nBias in the healthcare process Undesirable bias (\"bias\" in short) in the conceptualization, development and application of AI-ML-enabled medical devices that is not acknowledged or addressed has the potential to exacerbate existing health inequities or create new disparities. In its recent Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan 10 , FDA articulated the importance of addressing bias in the development and use of AI/ML-enabled medical devices.\n\nHealthcare is the prevention, treatment, and management of medical conditions and the preservation of mental and physical well-being  34  . Through a series of processes and medical products implemented or delivered by healthcare providers, improved outcomes can be realized for patients and populations. Opportunities exist for AI alone or in combination with healthcare providers, to deliver healthcare solutions. The use of AI is rapidly expanding, and examples of AI that have been implemented include assistive AI for breast cancer screening, hypertension management, stroke management, and autonomous AI for diabetic eye exams. Ethical frameworks that consider the potential negative and positive implications of widespread collection, analysis and use of large datasets can be used to determine whether a given healthcare process meets the goal of achieving good health outcomes for all patients. Typically, a Pareto optimum is sought between multiple bioethical principles, such as beneficence, autonomy and equity (\"Justics\")  32  . How much a process meets a specific bioethical principles, can be quantified using 'metrics for ethics', and these may affect benefit-risk determinations. While it is beyond the scope of these \"Considerations\" to exhaustively list these metrics, we give some examples to illustrate the principle. For the principle of beneficence, it can be a common metric such as sensitivity, specificity, or clinical outcome, while for the principle of equity, it can be sensitivity disaggregated by demographic subgroup, differential clinical outcome across subgroups, or even population achieved sensitivity or specificity to measure the impact of access to the process, as we have defined previously  32, 35  .\n\nThe focus of these \"Considerations\" is to determine how much a given healthcare process (that may include AI) meets the equity principle. To illustrate, a given process may improve healthcare outcomes for a patient, or a population, on average. However, when we consider outcomes across the population in more detail, this assumed improvement through integration of the AI system may not be evenly distributed across the population, though disease characteristics including prevalence, severity and prognosis may be otherwise equally distributed. The AI may affect a large variance in outcome improvement for some groups compared to others, so that some groups may have substantially worse outcomes than others, such as in the example mentioned above  25  ."
    },
    {
      "title": "Measuring bias in AI systems",
      "text": "Bias in any part of the healthcare process can lead to differential impacts on different groups  36  , and historically has resulted in poorer health outcomes for underrepresented, underserved, and under-resourced groups 1 . Examples of such groups are groups that are defined by racial, ethnic, age, sex, gender, national origin, disability, religion, political, or genetic information characteristics 1 . Thus, such bias reduces the bioethical principle of \"Justice,\" as Char et al.  28  and Abramoff et al.  32  described. On the basis of such ethical frameworks, and the continuing development of \"metrics for ethics,\" bias can be quantified as differential impact of a healthcare process on a particular group. Humans delivering healthcare can also exhibit bias; for example, a recent study showed provider bias, where providers' charts documented Black patients' symptoms and signs in a more pejorative manner, with the potential to exacerbate health disparities  37  . Other studies suggest physician bias in caring for other populations as well  [38] [39] [40]  .\n\nHow much any process, whether partially delivered or aided by AI, or fully delivered by humans, meets the \"equity\" bioethical principle can be quantified in various ways. Such measurements are necessarily specific to the use case and patients' risk of harm being considered, but an emerging set of studies draws on new data to measure algorithmic performance. In fact, such measurements have shown that AI systems can counter bias by human decision makers  11  . Similarly, in a diagnostic process, there may be concern about equity in accuracy. Subgroup statistical testing for the presence or absence of an effect on diagnostic accuracy (\"accuracy disaggregation\") could be used to determine how well the equity principle is met  32, 41  . As an example, where there is concern about access to a diagnostic test, population-achieved sensitivity and specificity, which measures the impact of both access and sensitivity, has been proposed as a way to understand the impact of bias on population health, when including so-called invisible populations  32  . By allowing optimization of populationachieved sensitivity and specificity, this metric can aid in improving population outcomes through diagnostic assessment, including those performed by AI.\n\nUltimately, understanding and mitigation of AI bias starts with assessment and quantification of possible sources of bias along the entire lifecycle of an AI device. Identification of bias is only part of mitigation, and stakeholders will have to decide, based on the AI context and perceived benefit/burden ratios, the extent to which identified biases can and should be mitigated.\n\nThe AI total product lifecycle Bioethical analysis of the AI lifecycle by Char et al.  28  highlighted the pipeline, ranging from conception over development to deployment (\"access\") of AI systems, and the parallel pipeline of evaluation and oversight activities at each stage. On top of this model, we analyzed the key factors associated with ethical considerations, from the existing literature as well as newly identified. This pipeline model framework is useful for systematic ethical appraisals of AI systems from development through deployment, and for interdisciplinary collaboration of diverse stakeholders that will be required to understand and subsequently manage the ethical implications of AI in healthcare  28  . Abramoff et al. subsequently linked this model to specific metrics for the conception, design, development, training, validation and implementation phases of AI technologies in healthcare  32  .\n\nAnother approach to decompose AI bias into different components has been proposed  42  . The approach divides sources of algorithmic bias into three main components: (direct) model bias, training data variance and training data noise. However, this approach, which focuses on AI built exclusively from retrospectively collected data, incorrectly assumes that the reference standard (sometimes referred to as ground truth) to compare the AI system to is perfectly correct  32  , and focuses solely on the potential for bias in the AI/ML algorithm. It does not consider the other 'pipeline phases' as set forward by Char et al. nor the integration of the AI into the care process. In other words, it does not consider what matters most to patients and other stakeholders: whether or not the addition of an AI system to the care process results in a favorable change in carei.e., improved clinical outcome  43, 32  . Gianfrancesco et al. similarly limited their analysis to bias derived from characteristics of retrospective training sets derived from existing Electronic Health Record (EHR) data  44  .\n\nThe framework developed by Char et al., on the other hand, recognizes specific AI system pipeline phases: conception, development, calibration, implementation, evaluation, deployment, and oversight, and the ethical considerations, including \"equity\" along each of these phases.\n\nIn 2019, FDA illustrated how the Total Product Lifecycle (TPLC) approach to the regulation of medical devices similarly applied to AI systems that meet the definition of a medical device, in its Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning  45  . TPLC describes the different phases of a device, including software such as AI systems, from conceptualization to its impact once on the (US) market as the following:\n\n\u2022 Access and marketing, and\n\nThese TPLC phases map in a straightforward manner to the 'pipeline phases' as defined in Char et al.  28  and as operationalized in Abramoff et al.  32  , see also ref.  46  . Thus, our intent is to extend TPLC to ethical analysis, by considering the potential impact on equity at each of these phases, as well as the potential for mitigation of AI bias, as defined above. From an ethical perspective, the equity principle can be analyzed and optimized within each phase of the existing TPLC framework. Depending on which TPLC phase is considered, standard equity metrics can be added, such as for the development phase, key performance indicators in software development, quality systems and risk of population harm analysis, or, for the validation phase of diagnostic medical devices, and absence of racial or ethnic effects on sensitivity and efficacy  32  .\n\nThe proposed TPLC framework in Fig.  1  adds equity considerations for AI systems, including the wider context of where the AI system is used in healthcare, with the goal of net benefit for the entire target population  35  . This framework (Fig.  1 ) is not intended to be comprehensive for all bias risks and mitigations, however it does initiate a discussion on AI and healthcare equity along the TPLC. The present framework is intended to complement the principles outlined in the International Medical Device Regulators Forum's (IMDRF)  47  Software as a Medical Device Clinical Evaluation, FDA's Good Machine Learning Practice 48 the aforementioned documents, but specifically hone in on ways to identify and mitigate bias in the development and evaluation of AI-ML-enabled software. It helps illustrate the importance of proactively developing an analytical framework to aid in identifying sources of impactful bias along the TPLC before a proposed AI tool propagates health disparities.\n\nAs shown in Fig.  1 , there is the potential to favorably, or unfavorably, impact health equity at every phase of the TPLC. Phases differ in the nature of potential bias, as well as it and its effects on health equity can be quantified and mitigated. Importantly, the equity impact at each phase is independent of all other phases, in other words, even when all potential bias has been mitigated in earlier phases, the next phase can still introduce undesirable equity effects. While Fig.  1  is not exhaustive, it highlights the opportunities to consider equity and bias along the TPLC. Thus, equity, considered upstream in the development process, has potential ripple effects on the downstream health outcomes."
    },
    {
      "title": "Conception phase",
      "text": "At conceptualization of the AI/ML-enabled medical device, it is critical to think through the TPLC development paradigm and identify opportunities to address and mitigate bias. During the conception phase, consider the health conditions and the care process in which the AI system will be used. Determining which health condition(s) will be the focus of the AI/ML-enabled medical device, may at the outset be directed at fostering health equity. Technologies that target conditions where the burden of disease is shouldered by a specific segment of the population may lead to more opportunities for improving health outcomes in that population. Examples might include developing an AI/ML-enabled device that helps diagnose narrow angle glaucoma or open angle glaucoma, conditions with higher prevalence in Asian, or Black and Latino populations, respectively. In addition, it may be important to determine the setting in which the AI/ML-enabled device will be used at the outset to help identify and mitigate bias. This requires optimization between generalizability and scalability of an AI/ML algorithm on the one hand, and on the other hand, optimizing its development, training and validation for the populations in which it will be used. For example, if an AI system for a diagnostic process is developed, trained, and validated only on those with ready access to healthcare services, but intended and deployed as a screening tool for an entire community or population, some of who lack routine healthcare access, the differential healthcare access may be a major source of inequity and AI-induced bias. The impact of such differential access can be measured for example through population-achieved sensitivity 32 compared to overall sensitivity, i.e., the fraction of correctly identified disease cases in a sample, without regard to representativeness of that sample (or lack thereof).\n\nAdditionally, historical data used in the development of AI/MLenabled devices may be fraught with miscategorized, mislabeled or mis-tagged, and missingness that differentially impacts different segments of the population. For example, historically reported similarities in disease phenotype, prevalence, or severity across groups may not reflect the actual differences in disease phenotype, prevalence or disease severity across groups. This bias may result from historical differences in access to care, differential treatment and quality of care offered in the healthcare system  49  , as well as differing group concerns about data usage and ownership  50  . Such incorrect assumptions about the disease under study may lead to incorrect, biased AI systems from conceptualization  51  . Abramoff et al. asserts such bias may also be the logical result of 'vernacular medicine' which are regional biases in care that may not expand to broader communities  32  . The inclusion of various viewpoints, backgrounds, experience and expertise on the creator team (including engineers, data scientists, clinicians, and other AI creators) may be an additional opportunity to avoid or mitigate the continuation of such biases into \"vernacular AI\" during each phase of the TPLC."
    },
    {
      "title": "Design phase",
      "text": "During this phase, consider the equity implications related to intended use of the medical device. In addition to the health condition for which the device will be used, other aspects of intended use including the operators and needed skills to use the device (e.g., human factors/usability engineering); the ways in which the device will integrate into the clinical workflow, the length of time needed to effectively use the device and the associated burden on patients and providers; the target patient population; and disease spectrum can all impact utilization and broad access to the technology. Not addressing the ethical and clinical constraints that were described in the conception stage may result in solidifying bias in the AI design. AI validity, explainability and transparency all help assess the equity implications of the Software as a Medical Device's (SaMD)'s algorithm design  32  . The introduction of using racially invariant priors instead of fully the algorithm from training data may be one approach to prevent the introduction of AI bias  41  . The TPLC model is foundational to how FDA regulates medical devices, and Design and Development phases are typically rapidly iterated: we emphasize that biases introduced during each of these two phases will propagate to the other, if not mitigated before the next iteration."
    },
    {
      "title": "Development phase",
      "text": "In the development phase of the AI algorithm, training dataset selection is another opportunity to proactively include equity considerations. Initial considerations for the training sets used in AI/ML-enabled devices were published by FDA in 2021  10  , and further expanded in the Good Machine Learning Practice (GMLP) document  48  . It is important to consider whether the relevant characteristics of the intended patient population (such as age, gender, sex, race, and ethnicity), use, and measurement inputs are sufficiently represented in training and test datasets, to maximize generalization to the intended population in which the AI system will be used  48  . Bias issues may arise around a) retrospective use of historical datasets b) more or less inclusive contemporary or prospectively collected datasets, and c) clinical study verification (covered in Validation section). For example, use of historical datasets may reflect differential access to care and differential quality of care due to sociocultural forces may lead to skewed distribution in the training data  50  . Prospective collection of data for training datasets is not exempt from potential biases. The eligibility criteria or other aspects of the recruitment and enrollment process, such as the reward or time commitment for data collection (e.g., need to miss work) could potentially be a constraint for people with limited financial resourcesthe socalled \"invisible populations\". Finally, bias in the 'reference standard', for the training dataset, may be caused by using inadequate proxies for clinical outcomes as reference standard  32, 52  . For considerations around which reference standard to use, see Abramoff et al. 2021  32  As an example, if clinicians are used as the reference standard, their potential bias in their diagnosis may lead to bias in the training data, ultimately persisting as bias in the AI system  37  . Similarly, outcomes or proxies thereof used as reference standards may historical inequities for subgroups, so that access and bias in delivery of care for subgroups may result in differential outcomes for the same disease phenotype. Metrics for such training set bias may be assessed through subgroup analysis and stratification of characteristics."
    },
    {
      "title": "Validation phase",
      "text": "Important factors to consider in the validation phase for AI/MLenabled devices have been included in documents, such as the International Medical Device Regulators Forum (IMDRF's) Software as a Medical Device (SaMD): Clinical Evaluation  53  , and more recently in FDA's Guiding Principles for Good Machine Learning Practice (GMLP)  48  . We use the term validation consistent with how it is used within the context of medical device development, i.e., 'confirmation by examination and provision of objective evidence that the particular requirements for a specific intended use can be consistently fulfilled'  52, 54  . When considering bias in validation, it is critical to evaluate how well clinical study subjects are mirrored in the data sets on which the AI system was conceptualized, designed, and developed. Ensuring that the relevant characteristics of the intended patient population including age, gender, sex, race and ethnicity, are appropriately represented in a sample of adequate size in a clinical study, allows results to be reasonably generalized to the intended use population. Thoughtful evaluation will expose bias and enhance appropriate and generalizable performance across the intended patient population. In addition, diversity in clinical sites where the studies are conducted will be an important consideration to generate diverse validation studies. Historically disadvantaged groups may be more likely to receive care in clinics that may lack the resources for the trained operators necessary to be a study site  55  . By considering metrics for how similar to real-world use the trial is (e.g., metric for operator expertise and diagnosability), there may be an opportunity to expand inclusion of more diverse clinical sites in the trial. These approaches can be assessed for their impact on replicability of findings in other samples of patients such as whether preregistration and arm's length protocols are followed  32  ."
    },
    {
      "title": "Access and monitoring phases",
      "text": "The access and monitoring phase includes deployment, monitoring and surveillance of the AI/ML-enabled device's performance and may also be subject to bias in implementation. This phase is where we have an opportunity to more comprehensively consider and measure the cumulative effects of potential biases at all phases of the TPLC, with real world evidence. In other words, we can estimate whether the 'real-world realization' of the AI system as it was originally conceptualized, designed and developed, measurably impacts health equity. During this phase, creators can assess the a priori vision of how well the AI-enabled device fits into the clinical workflow, and is usable with the prespecified staff skills, usability, cost and other resource use  32  . For example, if monitoring shows that low resourced patients are unable to access the device because the clinics in those locations cannot afford the high cost of the device, such as in under-resourced, or rural clinics, then the goals of the AI-enabled device to impact health outcomes in this population may not be realized; this can be quantified by a metric like population achieved sensitivity  10  . This monitoring information may thus lead to re-conceptualization of the device, for example with lower cost hardware, and more sophisticated ML algorithms to increase accessibility of the device in these populations  56  . Receiving care with the AI system may also impart higher cost or higher copay for the patient which may impact patients' access differentially. AI-induced bias can be introduced here through mismatch or shifts in process completion. For example, a process that combines identifying and treating true cases of diabetic retinopathy in people with diabetes may be skewed towards negative outcomes if there is differential follow-through for treatment. This follow-through for treatment is also subject to the same social determinants of health that can lead to inequitable utilization of healthcare services, and thereby lead to biased assessments of the device's performance. While these factors are best considered in the concept phase, effects on equity can be quantified and mitigations implemented here. Metrics such as population achieved sensitivity and specificity, device underperformance in certain groups, and other metrics of equitable access and outcomes can be assessed across subgroups longitudinally, and may help determine at what stage of the TPLC there may be opportunities to mitigate inequities. The above shows the importance of monitoring AI's impact in the real world, and the limitations of current frameworks for how to think about monitoring and surveillance in such a real-world setting: discussion among all stakeholders is crucial."
    },
    {
      "title": "DISCUSSION",
      "text": "We describe the sources and impacts of bias in AI systems on health equity, and propose approaches for potential mitigation across the AI's Total Product Lifecycle (TPLC). These Considerations are the start of a discussion with all stakeholders, including bioethicists, AI creators, regulatory agencies, patient advocacy groups, clinicians and their professional societies, other provider groups, and payors and value-based care organizations. Equity analysis and bias mitigation consistent with the present, expanded TPLC, will allow AI creators, regulators, payors and healthcare practitioners to better understand how potential bias may impact healthcare decisions and outcomes. The many potential sources of bias that can be introduced or addressed along the different phases of the TPLC can be assessed using appropriate metrics and mitigated using tailored approaches. By focusing on the goal of ensuring health equity along the TPLC framework, stakeholders can collectively identify and mitigate inequities, leading to better health outcomes for all."
    },
    {
      "text": "Fig. 1 Total Product LifeCycle (TPLC) equity expanded framework with examples for each phase."
    }
  ],
  "references": [
    {
      "title": "Inequalities in health: definitions, concepts, and theories",
      "authors": [
        "M Arcaya",
        "A Arcaya",
        "S Subramanian"
      ],
      "year": 2015
    },
    {
      "title": "Racial/ethnic disparities and barriers to diabetic retinopathy screening in youths",
      "authors": [
        "C Thomas"
      ],
      "year": 2021,
      "doi": "10.1001/jamaophthalmol.2021.1551"
    },
    {
      "title": "Identifying key barriers to effective breast cancer control in rural settings",
      "authors": [
        "B Sprague"
      ],
      "year": 2021,
      "doi": "10.1016/j.ypmed.2021.106741"
    },
    {
      "title": "Health and racial disparity in breast cancer",
      "authors": [
        "C Yedjou"
      ],
      "year": 2019
    },
    {
      "title": "Disparities in diabetic retinopathy screening and disease for racial and ethnic minority populations-a literature review",
      "authors": [
        "P Nsiah-Kumi",
        "S Ortmeier",
        "A Brown"
      ],
      "year": 2009,
      "doi": "10.1016/s0027-9684(15)30929-9"
    },
    {
      "title": "Black-white differences in risk of developing retinopathy among individuals with type 2 diabetes",
      "authors": [
        "E Harris",
        "S Sherman",
        "A Georgopoulos"
      ],
      "year": 1999
    },
    {
      "title": "Diabetes and diabetic retinopathy in a Mexican-American population: proyecto VER",
      "authors": [
        "S West"
      ],
      "year": 2001,
      "doi": "10.2337/diacare.24.7.1204"
    },
    {
      "title": "Technologies and health inequities",
      "authors": [
        "S Timmermans",
        "R Kaufman"
      ],
      "year": 2020,
      "doi": "10.1146/annurev-soc-121919-054802"
    },
    {
      "title": "Digital Health Center of Excellence C. Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD) Action Plan",
      "year": 2021,
      "doi": "10.31032/ijbpas/2025/14.8.9309"
    },
    {
      "title": "An algorithmic approach to reducing unexplained pain disparities in underserved populations",
      "authors": [
        "E Pierson",
        "D Cutler",
        "J Leskovec",
        "S Mullainathan",
        "Z Obermeyer"
      ],
      "year": 2021,
      "doi": "10.1038/s41591-020-01192-7"
    },
    {
      "title": "Board of trustees policy summary",
      "year": 2019
    },
    {
      "title": "Artificial intelligence in health care: will the value match the hype",
      "authors": [
        "E Emanuel",
        "R Wachter"
      ],
      "year": 2019,
      "doi": "10.1001/jama.2019.4914"
    },
    {
      "authors": [
        "A Autonomous"
      ],
      "year": 2020
    },
    {
      "title": "What are health disparities and health equity? We need to be clear",
      "authors": [
        "P Braveman"
      ],
      "year": 2014,
      "doi": "10.1177/00333549141291s203"
    },
    {
      "title": "Healthy people: the role of law and policy in the Nation's Public Health Agenda",
      "authors": [
        "A Mcgowan",
        "K Kramer",
        "J Teitelbaum"
      ],
      "year": 2019,
      "doi": "10.1177/1073110519857320"
    },
    {
      "title": "FDA permits marketing of artificial intelligencebased device to detect certain diabetes-related eye problems",
      "year": 2018,
      "doi": "10.31525/fda2-ucm604357.htm"
    },
    {
      "title": "Proposal to Establish Values for Remote Retinal Imaging (CPT code 92229)",
      "authors": [
        "Medicaid Centers For Medicare",
        "Services"
      ],
      "year": 2021,
      "doi": "10.1377/forefront.20210129.937900"
    },
    {
      "title": "A reimbursement framework for artificial intelligence in healthcare",
      "authors": [
        "M Abramoff"
      ],
      "year": 2022,
      "doi": "10.1038/s41746-022-00621-w"
    },
    {
      "title": "Ensuring fairness in machine learning to advance health equity",
      "authors": [
        "A Rajkomar",
        "M Hardt",
        "M Howell",
        "G Corrado",
        "M Chin"
      ],
      "year": 2018,
      "doi": "10.7326/m18-1990"
    },
    {
      "title": "Machine learning, health disparities, and causal reasoning",
      "authors": [
        "S Goodman",
        "S Goel",
        "M Cullen"
      ],
      "year": 2018,
      "doi": "10.7326/m18-3297"
    },
    {
      "title": "The SEE study: safety, efficacy, and equity of implementing autonomous artificial intelligence for diagnosing diabetic retinopathy in youth",
      "authors": [
        "R Wolf"
      ],
      "year": 2021,
      "doi": "10.2337/dc20-1671"
    },
    {
      "title": "Cost-effectiveness of autonomous point-of-care diabetic retinopathy screening for pediatric patients with diabetes",
      "authors": [
        "R Wolf",
        "R Channa",
        "M Abramoff",
        "H Lehmann"
      ],
      "year": 2020,
      "doi": "10.1001/jamaophthalmol.2020.3190"
    },
    {
      "title": "stand for augmenting inequality in the era of covid-19 healthcare?",
      "authors": [
        "D Leslie"
      ],
      "year": 2021,
      "doi": "10.1136/bmj.n304"
    },
    {
      "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
      "authors": [
        "Z Obermeyer",
        "B Powers",
        "C Vogeli",
        "S Mullainathan"
      ],
      "year": 2019,
      "doi": "10.1126/science.aax2342"
    },
    {
      "title": "Algorithmic bias playbook",
      "authors": [
        "Z Obermeyer",
        "R Nissan",
        "M Stern"
      ],
      "year": 2021
    },
    {
      "title": "Lessons learned about autonomous AI: finding a safe, efficacious, and ethical path through the development process",
      "authors": [
        "M Abramoff",
        "D Tobey",
        "D Char"
      ],
      "year": 2020
    },
    {
      "title": "Identifying ethical considerations for machine learning healthcare applications",
      "authors": [
        "D Char",
        "M Abr\u00e0moff",
        "C Feudtner"
      ],
      "year": 2020
    },
    {
      "title": "Implementing machine learning in health care -addressing ethical challenges",
      "authors": [
        "D Char",
        "N Shah",
        "D Magnus"
      ],
      "year": 2018
    },
    {
      "title": "Diagnosing diabetic retinopathy with artificial intelligence: what information should be included to ensure ethical informed consent? Original research",
      "authors": [
        "F Ursin",
        "C Timmermann",
        "M Orzechowski",
        "F Steger"
      ],
      "year": 2021,
      "doi": "10.3389/fmed.2021.695217"
    },
    {
      "title": "Commentary: diagnosing diabetic retinopathy with artificial intelligence: what information should be included to ensure ethical informed consent",
      "authors": [
        "M Abramoff",
        "Z Mortensen",
        "C Tava"
      ],
      "year": 2021,
      "doi": "10.3389/fmed.2021.765936"
    },
    {
      "title": "Foundational considerations for artificial intelligence using ophthalmic images",
      "authors": [
        "M Abramoff"
      ],
      "year": 2022
    },
    {
      "title": "Fairness in machine learning: a survey",
      "authors": [
        "S Caton",
        "C Haas"
      ],
      "year": 2020,
      "doi": "10.48550/arXiv.2010.04053"
    },
    {
      "title": "Medicare: a strategy for quality assurance",
      "year": 1990
    },
    {
      "title": "Factors to Consider When Making Benefit-Risk Determinations in Medical Device Premarket Approval and De Novo Classifications",
      "year": 2019
    },
    {
      "title": "Addressing fairness, bias, and appropriate use of artificial intelligence and machine learning in global health",
      "authors": [
        "R Fletcher",
        "A Nakeshimana",
        "O Olubeko"
      ],
      "year": 2020,
      "doi": "10.3389/frai.2020.561802"
    },
    {
      "title": "Negative patient descriptors: documenting racial bias in the electronic health record",
      "authors": [
        "M Sun",
        "T Oliwa",
        "M Peek",
        "E Tung"
      ],
      "year": 2022,
      "doi": "10.1377/hlthaff.2021.01423"
    },
    {
      "title": "What factors lead to racial disparities in outcomes after total knee arthroplasty?",
      "authors": [
        "D Hu"
      ],
      "year": 2021,
      "doi": "10.1007/s40615-021-01168-4"
    },
    {
      "title": "Racial and socioeconomic differences in eye care utilization among Medicare beneficiaries with glaucoma",
      "authors": [
        "O Halawa"
      ],
      "year": 2021,
      "doi": "10.1016/j.ophtha.2021.09.022"
    },
    {
      "title": "Prisoner health care",
      "authors": [
        "D Gage",
        "L Goldfrank"
      ],
      "year": 1985
    },
    {
      "title": "Pivotal trial of an autonomous AI-based diagnostic system for detection of diabetic retinopathy in primary care offices",
      "authors": [
        "M Abr\u00e0moff",
        "P Lavin",
        "M Birch",
        "N Shah",
        "J Folk"
      ],
      "year": 2018
    },
    {
      "title": "Why is my classifier discriminatory?",
      "authors": [
        "I Chen",
        "F Johansson",
        "D Sontag"
      ],
      "year": 2018
    },
    {
      "title": "Making machine learning models clinically useful",
      "authors": [
        "N Shah",
        "A Milstein",
        "D Bagley Ph"
      ],
      "year": 2019,
      "doi": "10.1001/jama.2019.10306"
    },
    {
      "title": "Potential biases in machine learning algorithms using electronic health record data",
      "authors": [
        "M Gianfrancesco",
        "S Tamang",
        "J Yazdany",
        "G Schmajuk"
      ],
      "year": 2018,
      "doi": "10.1001/jamainternmed.2018.3763"
    },
    {
      "title": "Proposed Regulatory Framework for Modifications to Artificial Intelligence/Machine Learning (AI/ML)-Based Software as a Medical Device (SaMD)",
      "year": 2019
    },
    {
      "title": "Towards a Standard for Identifying and Managing Bias in Artificial Intelligence",
      "authors": [
        "R Schwartz"
      ],
      "doi": "10.6028/NIST.SP.1270"
    },
    {
      "title": "International Medical Device Regulators Forum -Software as a Medical Device",
      "year": 2014
    },
    {
      "title": "Good Machine Learning Practice for Medical Device Development: Guiding Principles (GMLP)",
      "authors": [
        "F Cdrh"
      ],
      "year": 2021
    },
    {
      "title": "Racial and ethnic disparities in the quality of health care",
      "authors": [
        "K Fiscella",
        "M Sanders"
      ],
      "year": 2016,
      "doi": "10.1146/annurev-publhealth-032315-021439"
    },
    {
      "title": "Patient apprehensions about the use of artificial intelligence in healthcare",
      "authors": [
        "J Richardson"
      ],
      "year": 2021,
      "doi": "10.1038/s41746-021-00509-1"
    },
    {
      "title": "Skin of color: biology, structure, function, and implications for dermatologic disease",
      "authors": [
        "S Taylor"
      ],
      "year": 2002
    },
    {
      "title": "International Medical Device Regulators Forum (IMDRF) Artificial Intelligence Medical Devices (AIMD) Working Group. Machine Learning-enabled Medical Devices: Key Terms and Definitions",
      "doi": "10.1109/etis64005.2025.10961721"
    },
    {
      "title": "International Medical Device Regulators Forum. Software as a Medical Device (SAMD): Clinical Evaluation",
      "year": 2016
    },
    {
      "title": "2018 Software Engineering -Guidelines for the Application of ISO 9001:2015 to Computer Software",
      "year": 2018
    },
    {
      "title": "Separate and unequal: clinics where minority and nonminority patients receive primary care",
      "authors": [
        "A Varkey"
      ],
      "year": 2009
    },
    {
      "title": "DATAcc Inclusivity Toolkit for Digital Health Measurement Product Development",
      "authors": [
        ") Datacc",
        "Dhmcc"
      ],
      "year": 2022
    }
  ],
  "num_references": 55
}
