{
  "paper_id": "SEK9E7NA",
  "title": "Designing Reinforcement Learning Algorithms for Digital Interventions: Pre-Implementation Guidelines",
  "abstract": "Online reinforcement learning (RL) algorithms are increasingly used to personalize digital interventions in the fields of mobile health and online education. Common challenges in designing and testing an RL algorithm in these settings include ensuring the RL algorithm can learn and run stably under real-time constraints, and accounting for the complexity of the environment, e.g., a lack of accurate mechanistic models for the user dynamics. To guide how one can tackle these challenges, we extend the PCS (predictability, computability, stability) framework, a data science framework that incorporates best practices from machine learning and statistics in supervised learning to the design of RL algorithms for the digital interventions setting. Furthermore, we provide guidelines on how to design simulation environments, a crucial tool for evaluating RL candidate algorithms using the PCS framework. We show how we used the PCS framework to design an RL algorithm for Oralytics, a mobile health study aiming to improve users' tooth-brushing behaviors through the personalized delivery of intervention messages. Oralytics will go into the field in late 2022.",
  "year": 2022,
  "date": "2022-07-22",
  "journal": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
  "publication": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol",
  "authors": [
    {
      "forename": "Anna",
      "surname": "Trella",
      "name": "Anna Trella",
      "affiliation": "1  School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA; \n\t\t\t\t\t\t\t\t School of Engineering and Applied Sciences \n\t\t\t\t\t\t\t\t Harvard University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 02420 \n\t\t\t\t\t\t\t\t\t Cambridge \n\t\t\t\t\t\t\t\t\t MA \n\t\t\t\t\t\t\t\t\t USA;",
      "email": "annatrella@g.harvard.edu",
      "orcid": "0000-0003-4779-9115"
    },
    {
      "forename": "Kel",
      "surname": "Zhang",
      "name": "Kel Zhang",
      "affiliation": "1  School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA; \n\t\t\t\t\t\t\t\t School of Engineering and Applied Sciences \n\t\t\t\t\t\t\t\t Harvard University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 02420 \n\t\t\t\t\t\t\t\t\t Cambridge \n\t\t\t\t\t\t\t\t\t MA \n\t\t\t\t\t\t\t\t\t USA;",
      "email": "kellywzhang@seas.harvard.edu",
      "orcid": "0000-0003-0850-4978"
    },
    {
      "forename": "Inbal",
      "surname": "Nahum-Shani",
      "name": "Inbal Nahum-Shani",
      "affiliation": "2  Institute for Social Research , University of Michigan , Ann Arbor , MI 48109 , USA; \n\t\t\t\t\t\t\t\t Institute for Social Research \n\t\t\t\t\t\t\t\t University of Michigan \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 48109 \n\t\t\t\t\t\t\t\t\t Ann Arbor \n\t\t\t\t\t\t\t\t\t MI \n\t\t\t\t\t\t\t\t\t USA;",
      "orcid": "0000-0001-6138-9089"
    },
    {
      "forename": "Vivek",
      "surname": "Shetty",
      "name": "Vivek Shetty",
      "affiliation": "3  Schools of Dentistry & Engineering , University of California , Los Angeles , CA 90095 , USA; \n\t\t\t\t\t\t\t\t Schools of Dentistry & Engineering \n\t\t\t\t\t\t\t\t University of California \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 90095 \n\t\t\t\t\t\t\t\t\t Los Angeles \n\t\t\t\t\t\t\t\t\t CA \n\t\t\t\t\t\t\t\t\t USA;",
      "email": "vshetty@ucla.edu",
      "orcid": "0000-0002-3167-3318"
    },
    {
      "forename": "Finale",
      "surname": "Doshi-Velez",
      "name": "Finale Doshi-Velez",
      "affiliation": "1  School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA; \n\t\t\t\t\t\t\t\t School of Engineering and Applied Sciences \n\t\t\t\t\t\t\t\t Harvard University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 02420 \n\t\t\t\t\t\t\t\t\t Cambridge \n\t\t\t\t\t\t\t\t\t MA \n\t\t\t\t\t\t\t\t\t USA;",
      "email": "finale@seas.harvard.edu",
      "orcid": "0000-0003-2886-3898"
    },
    {
      "forename": "Susan",
      "surname": "Murphy",
      "name": "Susan Murphy",
      "affiliation": "1  School of Engineering and Applied Sciences , Harvard University , Cambridge , MA 02420 , USA; \n\t\t\t\t\t\t\t\t School of Engineering and Applied Sciences \n\t\t\t\t\t\t\t\t Harvard University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 02420 \n\t\t\t\t\t\t\t\t\t Cambridge \n\t\t\t\t\t\t\t\t\t MA \n\t\t\t\t\t\t\t\t\t USA;",
      "email": "samurphy@fas.harvard.edu",
      "orcid": "0000-0002-2032-4286"
    }
  ],
  "doi": "https://doi.org/10.13039/100000002",
  "arxiv": "arXiv:2005.01643",
  "keywords": [
    "reinforcement learning (RL)",
    "online learning",
    "mobile health",
    "algorithm design",
    "algorithm evaluation"
  ],
  "sections": [
    {
      "title": "Introduction",
      "text": "There is growing interest in using online reinforcement learning (RL) to optimize the delivery of messages or other forms of prompts in digital interventions. In mobile health, RL algorithms have been used to increase the effectiveness of the content and timing of intervention messages designed to promote physical activity  [1, 2]  and to manage weight loss  [3] . In other areas, including the social sciences and education, RL algorithms are used to provide pretrial nudges to encourage court hearing attendance  [4] , to personalize math explanations  [5] , and to deliver quiz questions during lecture videos  [6] . Unlike games and work in some areas of robotics, digital intervention studies can be extremely costly to run. Furthermore, when the study is a preregistered clinical trial, once initiated, the trial protocol (including any online algorithms) cannot be altered without jeopardizing trial validity. Thus, design decisions are a \"one-way door\"  [7] ; once we commit to a set of design decisions, they are irreversible for the duration of the trial. To prevent poor design decisions that could be detrimental to the effectiveness and the validity of study results, RL algorithms must undergo a thorough design and testing process before deployment.\n\nThe development of an RL algorithm for digital interventions requires a multitude of design decisions. These decisions include how best to accommodate the lack of mechanistic models for dynamic human responses to digital interventions and how to ensure the robustness of the algorithm to potentially nonstationary/non-Markovian outcome distributions. Furthermore, one must ensure not only that the RL algorithm learns and quickly optimizes interventions but also that the algorithm runs stably and autonomously online within constrained amounts of time. One must also ensure that the RL algorithm can obtain data in a timely manner. Time and budgetary considerations may restrict the complexity of the RL algorithm that can be implemented. Furthermore, it is important to ensure the data collected by the RL algorithm can be used to inform future studies and address scientific, causal inference questions. Addressing these challenges in a reproducible, replicable manner is critical if RL algorithms are to play a role in optimizing digital interventions. Therefore, we need a framework for making design decisions for RL algorithms intended to optimize digital interventions.\n\nThe primary contributions of this work are twofold:"
    },
    {
      "title": "Decision Times",
      "text": "These are the times, indexed by t, at which the RL algorithm may deliver a treatment (via a smart device such as a desktop computer, smartwatch, smartphone, smart speaker, wearable, etc.). The cadence of the decision times (minute level, hourly, daily, etc.) depends on the type of digital intervention. For example, in Oralytics, we have two decision times per day, namely, one hour prior to the set morning and evening brushing windows specified by the user."
    },
    {
      "title": "State",
      "text": "S i,t \u2208 R d represents the ith user's state at decision time t. d is the number of features describing the user's state (e.g., current location, recent adherence to medication, current social setting, recent engagement with the intervention application, etc.). See Section 5.2 for the state definition for Oralytics."
    },
    {
      "title": "Action",
      "text": "A i,t \u2208 A represents the decision made by the RL algorithm for the ith user at decision time t. Treatment actions in digital interventions frequently include the action of not delivering any treatment at time t. For Oralytics, the action space is A := {0, 1}, where A i,t = 1 represents sending the user an engagement message and A i,t = 0 represents not sending an engagement message. See Section 5.1 for descriptions of the types of messages that can be sent in Oralytics."
    },
    {
      "title": "Reward",
      "text": "R i,t \u2208 R is the reward for the ith user at decision time t observed after taking action A i,t . The definition of the reward depends on the type of digital intervention. Examples include successfully completing a math problem, taking a medication, and increasing physical activity. In Oralytics, the reward is the subsequent brushing duration; see Section 5.2 for further discussion of the reward in Oralytics."
    },
    {
      "title": "Online RL Algorithms",
      "text": "Generally, online RL algorithms are composed of two parts: (a) fitting a model of the user and (b) an action selection strategy. The simplest type of user model is a model for the reward function, E[R i,t |S i,t , A i,t ]. In more general cases, a model for the sum of future rewards, conditional on the current state, S i,t , and action, A i,t , is also learned. The action selection strategy of the RL algorithm uses the user's current state S i,t , along with the learned user model and outputs the treatment action A i,t at each decision time t."
    },
    {
      "title": "Update Times",
      "text": "These are the times at which the RL algorithm is updated. Updating typically includes updating a model of the user (e.g., a model of the user's reward function). The RL algorithm updates using user i's current history of past states, actions, and rewards up to time t, denoted by H i,t-1 = {S i,s , A i,s , R i,s } t-1 s=1 . If the algorithm pools data across users, then the history of other users in the study, H j,t-1 for i = j, is used to update the model for user i. These updates can occur after each decision time or at longer time scales. For example, in  [1] , the decision times are 5 times per day, but the update times are only nightly. For Oralytics, the update cadence is once a week.\n\nAn online RL algorithm should quickly learn which action to deliver in which states for each user. One of the most widely used and simplest RL algorithms is a contextual bandit algorithm  [11] [12] [13] . As data accrues, a contextual bandit algorithm incrementally learns which action will lead to maximal reward in each state. The online algorithm sequentially updates its estimate of the reward function (the mean of the reward conditional on state and action) and selects actions. The performance of the algorithm is often measured by the sum of rewards-the faster the algorithm learns, the greater the sum."
    },
    {
      "title": "PCS Framework for Designing RL Algorithms for Digital Intervention Development",
      "text": "The PCS Framework  [8]  incorporates best practices from machine learning and applied statistics to provide a set of guidelines for assessing the quality of a prediction algorithm when used to address problems in real life. The goal is to enhance the scientific community's confidence in the prediction algorithm's performance in terms of predictability of results, computability in the implementation of the algorithm, and stability in the performance results of the learning algorithm across perturbations. PCS has been adopted and extended to other domains; see Section 4.3 for further discussion.\n\nAs in the prediction setting, there are a variety of design decisions one needs to make before deploying an RL algorithm, e.g., choosing the model class to use to approximate the reward function. While many of the original PCS principles can be used in the development and evaluation of RL algorithms, RL algorithm development also introduces new challenges for the PCS framework, particularly in the online digital intervention setting. First, the main task is not prediction, but rather in RL, the main goal is to select intervention actions so that average rewards across time are maximized for each user. We call this the goal of personalization  [14] . We generalize the PCS framework to include an evaluation of the ability of an online RL algorithm to personalize. Second, in digital intervention settings, it is important to evaluate the ability of the online RL algorithm to maximize rewards under realworld constraints. For example, there are often time constraints on computations, budgetary constraints on software engineering development, and constraints on the algorithm in terms of obtaining data in real time. Furthermore, the algorithm must run stably online without constant human monitoring and adjustment. The current PCS framework does not provide evaluation tools that deal with the above needs. We extend the PCS framework to the context of designing and evaluating online RL algorithms. Our extended framework focuses on providing confidence that the online RL algorithm will lead to greater effectiveness under real-world constraints and with stability."
    },
    {
      "title": "Personalization (P)",
      "text": "The original PCS Framework uses predictability (P) to ensure that models used in data science have good predictive accuracy on both seen and unseen data. Predictive accuracy is a simple, commonly used metric for evaluating such models, but in some cases, multiple evaluation metrics or domain-specific metrics are more appropriate. In our setting, the main task is personalization. Namely, the online RL algorithm should learn to select actions to maximize each user's average rewards. Instead of a predictive accuracy metric, we want a metric to validate the extent of personalization. For example, when choosing a metric to evaluate RL algorithms for multiple users, one may be interested not just in the average over the users' sums of rewards but in other metrics that capture the variation in the sum of rewards across users. Let N be the total number of users with T total decision times. We suggest the following metrics:"
    },
    {
      "title": "\u2022",
      "text": "Average of Users' Average (Across Time) Rewards: This metric is the average of all N users' rewards averaged across all T decision times, defined as 1\n\nThe metric serves as a global measure of the RL algorithm's performance."
    },
    {
      "title": "\u2022",
      "text": "The 25th Percentile of Users' Average (Across Time) Rewards: To compute this metric, first compute the average reward across time for each user, 1 T \u2211 T t=1 R i,t for each i = 1, 2, . . . , N; this metric is the lower 25th percentile of these average rewards across the N users. The metric shows how well an RL algorithm performs for the worst-off users, namely users in the lower quartile of average rewards across time."
    },
    {
      "title": "\u2022",
      "text": "Average Reward For Multiple Time Points: This metric is the average users' rewards across time for multiple time points t 0 = 1, 2, ..., T, defined as 1\n\nt=1 R i,t for each t 0 . These metrics can be used to assess the speed at which the RL algorithm learns across weeks in the trial."
    },
    {
      "title": "Computability (C)",
      "text": "Computability has to do with the efficiency and scalability of algorithms, decisions, and processes. While the original PCS framework focused on the computability of training and evaluating models, we also consider computability to include the ability to implement the algorithm within the constraints of the study. In the online RL setting, computability encompasses all issues related to ensuring that the RL algorithm can select actions and update in a timely manner while running online. The performance of the online RL algorithm must be evaluated under the constraints of the study; key RL algorithm design constraints that could arise include:\n\n\u2022 Timely Access to Reward and State Information: The investigators may have an ideal definition of the reward or state features for the algorithm; however, due to delays in communication between sensors, the digital application, and the cloud storage, the investigators' first choice may not be reliably available. Since RL algorithms for digital interventions must make decisions online, the development team must choose state features that will be reliably available to the algorithm at each decision time. Additionally, the team must also choose rewards that are reliably available to the algorithm at update times. \u2022 Engineering Budget: One should consider the engineering budget, supporting software needed, and time available to deliver a production-ready algorithm. If there are significant constraints, a simpler algorithm may be preferred over a sophisticated one because it is easier to implement, test, and set up monitoring systems for. \u2022 Off-Policy Evaluation and Causal Inference Considerations:\n\nThe investigative team often not only cares about the RL algorithm's ability to learn but also about being able to use data collected by the RL algorithm to answer scientific questions after the study is over. These scientific questions can include topics such as off-policy evaluation  [15, 16]  and causal inference  [17, 18] . Thus, the algorithm may be constrained to select actions probabilistically with probabilities that are bounded away from zero and one. This enhances the ability of investigators to use the resulting data to address scientific questions with sufficient power  [19] ."
    },
    {
      "title": "Stability (S)",
      "text": "Stability concerns how an RL algorithm's results change with minor perturbations and the documentation and reproducibility of results. In online RL, stability plays two roles. First, the RL algorithm must run stably and automatically without the need for constant human monitoring and adjustment. This is particularly critical as users abandon digital interventions that have inconsistent functionality (unstable RL algorithm)  [20, 21] . Second, the RL algorithm should perform well across a variety of potential real-world environments. A critical tool in assessing stability to perturbations of the environment is the use of simulation test beds. Test beds include a variety of plausible environmental variants, each of which encodes different concerns of the investigative team. The following are challenging attributes of probable environments in digital intervention problems that one could design test beds for:"
    },
    {
      "title": "\u2022",
      "text": "User Heterogeneity: There is likely some amount of user heterogeneity in response to actions, even when users are in the same context. User heterogeneity can be partially due to unobserved user traits (e.g., factors that are stable or change slowly over time, like family composition or personality type). The amount of between-user heterogeneity impacts whether an RL algorithm that pools data (partially or using clusters) across users to select actions will lead to improved rewards."
    },
    {
      "title": "\u2022",
      "text": "Non-Stationarity: Unobserved factors common to all users such as societal changes (e.g., a new wave of the pandemic), and time-varying unobserved treatment burden (e.g., a user's response to a digital intervention may depend on how many days the user has experienced the intervention) may make the distribution of the reward appear to vary with time, i.e., non-stationary.\n\n\u2022 High-Noise Environments: Digital interventions typically deliver treatments to users in highly noisy environments. This is in part because digital interventions deliver treatments to users in daily life, where many unobserved factors (e.g., social context, mood, or stress) can affect a user's responsiveness to an intervention. If unobserved, these factors produce noise. Moreover, the effect of digital prompts on a near-term reward tends to be small due to the nature of the intervention. Therefore, it is important to evaluate the algorithm's ability to personalize even in highly noisy, low signal-tonoise ratio environments."
    },
    {
      "title": "Simulation Environments for PCS Evaluation",
      "text": "To utilize the PCS framework, we advocate for using a simulation environment for designing and evaluating RL algorithms. We aim to compare RL algorithm candidates under real-world constraints (computability). Thus, we build multiple variants of the simulation environment, each reflecting plausible user dynamics (i.e., state transitions and reward distributions) (stability). We then simulate digital intervention studies for each simulation environment variant and RL algorithm candidate pairing. Finally, we use multiple metrics to evaluate the performance of the RL algorithm candidates (personalization).\n\nIn the case of digital interventions, there is often no mechanistic model or physical process for user behavioral dynamics, which makes it difficult to accurately model transitions (e.g., modeling a user's future level of physical activity as a function of their past physical activity, location, local weather). Note that the goal of developing the simulators is not to conduct model-based RL  [22] . Rather, here, the simulators represent a variety of plausible environments to facilitate the evaluation of the performance of potential RL algorithms in terms of personalization, computability, and stability across these environments. Existing data and domain expertise is most naturally used to construct the simulation environments. However, as is the case for Oralytics, the previously collected data may be scarce, i.e., we have few data points per user. Moreover, the data may only be partially informative, e.g., the data was collected under only a subset of the actions. Next, we provide guidelines for how to build an environment simulator in such challenging settings.\n\nBase Environment Simulator: To have the best chance possible of accurately evaluating how well different RL candidates will perform, we recommend first building a base environment simulator that mimics the existing data to the greatest extent possible. This involves carefully choosing a set of time-varying features and reward-generating model class that will be expressive enough to model the true reward distribution well. To check how well the simulated data generated by the model of the environment mimics the observed data, we recommend a variety of ways to compare distributions. This includes visual comparisons such as plotting histograms; comparing measures of the real data such as mean reward, between-user variance, and within-user variance to the same measures of the simulated data; and measuring how well the base model captured the variance in the data. Examples of these checks done for Oralytics are in Appendix A.  4 .\n\nVariant Environment Simulators: We recommend considering many variants or perturbed simulation environments to evaluate the stability of RL algorithms across multiple plausible environments. These variants can be used to address the concerns of the investigative team. For example, if the base simulator generates stationary rewards and the investigative team is concerned that the real reward distribution may not be stationary, a variant could incorporate nonstationarity into the environment dynamics.\n\nIf the previously collected data does not include particular actions, as was the case for Oralytics, we recommend consulting domain experts for a range of potential realistic effect sizes (differences in mean reward under the new action versus a baseline action). For example, in Oralytics, we only have data under no intervention and do not have data on rewards under the intervention. Thus, using the input of the domain experts on the team, we imputed several plausible treatment effects (varying by certain state features and the amount of heterogeneity in treatment effects across users)."
    },
    {
      "title": "Related Works 4.1. Digital Intervention Case Studies",
      "text": "Liao et al.  [1]  describes the development of an online RL algorithm for HeartSteps V2, a physical activity mobile health application. The authors highlight how their design decisions address specific challenges in designing RL algorithms by, for example, adjusting for longer-term effects of the current action and accommodating noisy data. However, they do not provide general guidelines for making design decisions for RL algorithms in digital intervention development.\n\nAnother related work is that of Figueroa et al.  [23] , which provides an in-depth case study of the design decisions, and the associated challenges and considerations, for an RL algorithm for text messaging in a physical activity mobile application serving patients with diabetes and depression. This case study provides guidelines to others developing RL algorithms for mobile health applications. Specifically, the authors first categorize the challenges they faced into 3 major themes: (1) choosing a model for decision making, (2) data handling, and (3) weighing algorithm performance versus effectiveness in the real world. They describe how they dealt with each challenge in the design process of their RL algorithm. In contrast, by expanding the PCS framework, this work introduces general guidelines for comprehensively evaluating RL algorithms. Moreover, we make recommendations for how to design a variety of simulation test beds even using only sparse and partially informative existing data, in service of PCS. The generality of the PCS framework makes it more widely applicable. For example, Figueroa et al.  [23]  has an existing dataset for all actions, which makes its recommendations less applicable to those designing algorithms with existing data only under a subset of actions. The PCS framework allows us to move beyond suggesting solutions to a specific set of challenges for a particular study by offering holistic guidelines for addressing challenges in developing simulation environments and evaluating algorithms."
    },
    {
      "title": "Simulation Environments in Reinforcement Learning",
      "text": "In RL, simulators (generative models) may be used to derive a policy from the generative model underlying the simulator (model-based learning). Agarwal et al.  [22]  uses simulation as an intermediate step to learn personalized policies in a data-sparse regime with heterogeneous users, where they only observe a single trajectory per user. Wei et al.  [24]  proposes a framework for simulating in a data-sparse setting by using imitation learning to better interpolate traffic trajectories in an autonomous driving setting. In contrast, in PCS, the simulator is used as a crucial tool for using the framework to design, compare, and evaluate RL algorithm candidates for use in a particular problem setting.\n\nThere exist many resources aiming to improve the design and evaluation of RL algorithms through simulation; however, in contrast to this work, they do not provide guidelines for designing plausible simulation environments using existing data. RecSim  [25]  gives a general framework but does not advise on the quality of the environment nor on how to make critical design decisions such as reward construction, defining the state space, simulating unobserved actions, etc. MARS-Gym  [26]  provides a full end-to-end pipeline process (data processing, model design, optimization, evaluation) and open-source code for a simulation environment for marketplace recommender systems. OpenAI Gym  [27]  is a collection of benchmark environments in classical control, games, and robotics where the public can run and compare the performance of RL algorithms.\n\nThere are also a handful of papers that build simulation environment test beds using real data. Wang et al.  [28]  evaluates their algorithm for promoting running activity with a simulation environment built using two datasets. Singh et al.  [29]  develops a simulation environment using movie recommendations to evaluate their safe RL approach. Korzepa et al.  [30]  uses a simulation environment to guide the design of personalized algorithms that optimize hearing aid settings. Hassouni et al.  [31, 32]  fits a realistic simulation environment using the U.S. timekeeping research project data. Their simulation environment creates daily schedules of activities for each user (i.e., sleep, work, workout, etc.)\n\nwhere each user is one of many different user profiles (i.e., workaholic, athlete, retiree) for the task of improving physical activity."
    },
    {
      "title": "PCS Framework Extensions",
      "text": "The PCS framework has been extended to other learning domains such as causal inference  [33] , network analysis  [34] , and algorithm interpretability  [35] . Despite the variety of these tasks, they can all be framed as supervised learning problems in batch data settings that can be evaluated in terms of prediction accuracy on a hold-out dataset. PCS has not been extended to provide guidelines for developing an online decision-making algorithm. This extension is needed because of the additional considerations, discussed above, present in a real-world RL setting. Additionally, while these papers focus on evaluating how well a model accurately predicts the outcome on training and hold-out datasets, we extend the framework to evaluate how well an algorithm personalizes to each user. Dwivedi et al. and Ward et al.  [33, 34]  implement the original computability principle by considering algorithm and process efficiency and scalability. Margot et al.  [35]  provides a new principle, simplicity, which is based on the sum of the lengths of generated rules. We extend computability to include the constraints of the study. Finally, these papers consider the stability of results across different changes to the data (e.g., bootstrapping or cross-validation) or design decisions (e.g., choice of representation space or the embedding dimension). Our framework focuses on how stable an algorithm is in plausible real-world environments that may be complex (e.g., due to user heterogeneity, nonstationary, high noise)."
    },
    {
      "title": "Case Study: Oral Health",
      "text": "In this case study, we demonstrate the use of PCS principles in designing an RL algorithm for Oralytics. Two main challenges are (1) we do not have timely access to many features and the reward is relatively noisy and (2) we have sparse, partially informative data to inform the construction of our simulation environment test bed. In addition, there are several study constraints."
    },
    {
      "title": "1.",
      "text": "Once the study is initiated, the trial protocol and algorithm cannot be altered without jeopardizing trial validity. 2.\n\nWe are using an online algorithm, so we may not have timely access to certain desirable state features or rewards."
    },
    {
      "title": "3.",
      "text": "We have a limited engineering budget. 4 .\n\nWe must answer post-study scientific questions that require causal inference or offpolicy evaluation.\n\nWe highlight how we handle these challenges by using the PCS framework, despite being in a highly constrained setting. The case study is organized as follows. In Section 5.1, we give background context and motivation for the Oralytics study. In Section 5.2, we explain the Oralytics sequential decision-making problem. In Section 5.3, we describe our process for designing RL algorithm candidates that can stably learn despite having a severely constrained features space and noisy rewards. Finally, in Section 5.4, we describe how we designed the simulation environment variants to evaluate the RL algorithm candidates; throughout, we offer recommendations for designing realistic environment variants and for constructing such environments using data for only a subset of actions."
    },
    {
      "title": "Oralytics",
      "text": "Oralytics is a digital intervention for improving oral health. Each user is provided a commercially available electric toothbrush with integrated sensors and Bluetooth connectivity as well as the Oralytics mobile application for their smartphone. There are two decision times per day (prior to the user's morning and evening brushing windows) when a message may or may not be delivered to the user via their smartphone. The types of messages focus on winning a gift for oneself, winning a gift for one's favorite charity, feedback on prior brushing, and educational information. Once a message is delivered to the user, the app records it so that a user is highly unlikely to receive the same message twice. Oralytics will be implemented with approximately 70 users in a clinical trial where the participant duration is 10 weeks; this means each user has T = 140 decision times. The study duration is 2 years and the expected weekly incremental recruitment rate is around 4 users. The Oralytics mobile app will use an online RL algorithm to optimize message delivery (i.e., treatment actions) to maximize an oral health-related reward (see below). To inform the RL algorithm design, we have access to data from a prior oral health study, ROBAS 2  [36] , and input from experts in oral and behavioral health. The ROBAS 2 study used earlier versions of both the electric toothbrush and the Oralytics application to track the brushing behaviors of 32 users over 28 days. Importantly, in ROBAS 2, no intervention messages were sent to the users."
    },
    {
      "title": "The Oralytics Sequential Decision-Making Problem",
      "text": "We now discuss how we designed the state space and rewards for our RL problem in collaboration with domain experts and the software team while considering various constraints. These decisions must be communicated and agreed upon with the software development team because they build the systems that provide the RL algorithm with the necessary data at decision and update times and execute actions selected by the RL algorithm.\n\n1. Choice of Decision Times: We chose the decision times to be prior to each user's specified morning and evening brushing windows, as the scientific team thought this would be the best time to influence users' brushing behavior.\n\n2. Choice of Reward: The research team's first choice of reward was a measure of brushing quality derived from the toothbrush sensor data from each brushing episode. However, the brushing quality outcome is often not reliably obtainable because it requires (1) that the toothbrush dock be plugged in and (  2 ) that the user be standing within a few feet of the toothbrush dock when brushing their teeth. Users may fail to meet these two requirements for a variety of reasons, e.g., the user brushes their teeth in a shared bathroom where they cannot conveniently leave the dock plugged in. Thus, we selected brushing duration in seconds as the reward (personalization) since 120 s is the dentist-recommended brushing duration and brushing duration is a necessary factor in calculating the brushing quality score. Additionally, brushing duration is expected to be reliably obtainable even when the user is far from the toothbrush dock when brushing (computability). Note that in Figure  2 , a small number of user-brushing episodes have durations over the recommended 120 s. Hence, we truncate the brushing time to avoid optimizing for overbrushing. Let D i,t denote the user's brushing duration. The reward is defined as R i,t := min(D i,t , 180)."
    },
    {
      "title": "Choice of State Features At Decision Time:",
      "text": "To provide the best personalization, an RL algorithm ideally has access to as many relevant state features as possible to inform a decision, e.g., recent brushing, location, user's schedule, etc. However, our choice of the state space is constrained by the need to get features reliably before decision and update times, as well as by our limited engineering budget. For example, we originally wanted a feature for the evening decision time to be the morning's brushing outcome; however, this feature may not be accessible in a timely manner. This is because in order for the algorithm to receive the morning brushing data, the Oralytics smartphone app requires the user to open the app and we do not expect most users to reliably open the app after every morning brush time before the evening brushing window. Further discussion of our choice of decision time state features can be found in Appendix B.1.\n\n4. Choice of Algorithm Update Times: In our simulations, we update the algorithm weekly. In terms of speed of learning (at least in idealized settings), it is best to update the algorithm after each decision time. However, due to computability considerations, we chose a slower update cadence. Specifically, for the Oralytics app, the consideration was that we can only update the policy used to select actions when the user opens the app. If the user did not open the app for many days, we would be unable to update the app after each decision time. Users may well fail to open the app for a few days at a time, so we chose weekly updates. In the future, we will explore other update cadences as well, e.g., once a day.\n\n0 100 200 300 Brushing Duration in Seconds 0% 5% 10% 15% 20% 25% 30% 35% 40% Percentage of Brushing Sessions The ROBAS 2 study had 32 users total and each user had 56 brushing windows (2 brushing windows per day for 28 days). If a user did not brush during a brushing window, their brushing duration is recorded as zero seconds. Note in the figure above that across all users and brushing windows, about 40% of brushing sessions had no brushing, that is, a brushing duration of zero seconds. The ROBAS 2 brushing durations are highly zero-inflated."
    },
    {
      "title": "Designing the RL Algorithm Candidates",
      "text": "Here, we discuss our use of the PCS framework to guide and evaluate the following design decisions for the RL algorithm candidates. There are some decisions that we have already made and other decisions that we encode as axes for our algorithm candidates to test in the simulation environment. See Appendix B for further details regarding the RL algorithm candidates.\n\n1. Choice of using a Contextual Bandit Algorithm Framework: We understand that actions will likely affect a user's future states and rewards, e.g., sending an intervention message the previous day may affect how receptive a user is to an intervention message today. This suggests that an RL algorithm that models a full Markov decision process (MDP) may be more suitable than a contextual bandit algorithm. However, the highly noisy environment and the limited data to learn from (140 decision times per user total) make it difficult for the RL algorithm to accurately model state transitions. Due to errors in the state transition model, the estimates of the delayed effects of actions used in MDP-based RL algorithms can often be highly noisy or inaccurate. This issue is exacerbated by our severely constrained state space (i.e., we have few features and the features we get are relatively noisy). As a result, an RL algorithm that fits a full MDP model may not learn much during the study, which could compromise personalization and offer a poor user experience. To mitigate these issues, we use contextual bandit algorithms, which fit a simpler model of the environment. Using a lower discount factor (a form of regularization) has been shown to lead to learning a better policy than using the true discount factor, especially in data-scarce settings  [37] . Thus, a contextual bandit algorithm can be interpreted as an extreme form of this regularization where the discount factor is zero. Finally, contextual bandits are the simplest algorithm for sequential decision making (computability) and have been used to personalize digital interventions in a variety of areas  [1, 2, 5, 23] .\n\n2. Choice of a Bayesian Framework: We consider contextual bandit algorithms that use a Bayesian framework, specifically posterior (Thompson) sampling algorithms  [38] . Posterior sampling involves placing a prior on the parameters of the reward approximating function and updating the posterior distribution of the reward function parameters at each algorithm update time. This allows us to incorporate prior data and domain expertise into the initialization of the algorithm parameters. In addition, Thompson sampling algorithms are stochastic (action selections are a not deterministic function of the data), which better facilitate causal inference analyses later on using the data collected in the study.\n\n3. Choice of Constrained Action Selection Probabilities: We constrain the action selection probabilities to be bounded away from zero and one in order to facilitate offpolicy and causal inference analyses once the study is over (computability). With help from the domain experts, we decided to constrain the action selection probabilities of the algorithm to be in the interval [0.35, 0.75].\n\nThe following are decisions we will test using the simulation environment. 4. Choice of the Reward Approximating Function: An important decision in designing the contextual bandit algorithm is how to approximate the reward function. We consider two types of approximations, a Bayesian linear regression model (BLR) and a Bayesian zero-inflated Poisson regression model (ZIP), which are both relatively simple, well studied, and well understood. For BLR, we implement action centering in the linear model  [1] . The linear model for the reward function is easily interpretable by domain experts and allows them to critique and inform the model. We consider the ZIP because of the zero-inflated nature of brushing durations in our existing dataset ROBAS 2; see Figure  2 . We expect the ZIP to provide a better fit to the reward function by the contextual bandit and thus lead to increased average rewards. Formal specifications for BLR and ZIP as reward functions can be found in Appendix B.2.1 and Appendix B.2.2, respectively.\n\nTo perform posterior sampling, both the BLR and ZIP models are Bayesian with uninformative priors. From the perspective of computability and stability, the posterior for the BLR has a closed form, which makes it easier to write software that performs efficient and stable updates. In contrast, for the ZIP, the posterior distribution must be approximated and the approach used to approximate the posterior is another aspect of the algorithm design that the scientific team needs to consider. See Appendix C for further discussion on how to update the RL algorithm candidates.\n\n5. Choice of Cluster Size: We consider clustering users with cluster sizes K = 1 (no pooling), K = 4 (partial pooling), and K = N = 72 (full pooling) to determine whether clustering in our setting will lead to higher sums of rewards (personalization). Note that 72 is the approximate expected sample size for the Oralytics study. Clustering-based algorithms pool data from multiple users to learn an algorithm per cluster (i.e., at update times, the algorithm uses H i,t-1 for all users i in the same cluster, and at decision times, the same algorithm is used to select actions for all users in the cluster). Clustering-based algorithms have been empirically shown to perform well when users within a cluster are similar  [39, 40] . In addition, we believe that clustering will facilitate learning within environments that have noisy within-user rewards  [41, 42] . There is a trade-off between no pooling and full pooling. No pooling may learn a policy more specific to the user later on in the study but may not learn as well earlier in the study when there is not a lot of data for that user. Full pooling may learn well earlier in the study because it can take advantage of all users' data but may not personalize as well as a no-pooling algorithm, especially if users are heterogeneous. We consider K = 4 for the balance partial pooling offers between the advantages and disadvantages between no pooling and full pooling. Moreover, four is the study's expected weekly recruitment rate and the update cadence is also weekly. We consider the two extremes and partial pooling as a way to explore this trade-off. A further discussion on choices of cluster size can be found in Appendix B.3."
    },
    {
      "title": "Designing the Simulation Environment",
      "text": "We build a simulator that considers multiple variants for the environment, each encoding a concern by the research team. The simulator allows us to evaluate the stability of results for each RL algorithm across the environmental variants (stability).\n\nFitting Base Models: Recall that the ROBAS 2 study did not involve intervention messages. However, we can still use the ROBAS 2 dataset to fit the base model for the simulation environment, i.e., a model for the reward (brushing duration in seconds) under no action. Two main approaches for fitting zero-inflated data are the zero-inflated model and the hurdle model  [43] . Both the zero-inflated model and the hurdle model have (i) a Bernoulli component and (ii) a nonzero component. The zero-inflated model's Bernoulli component is latent and represents the user's intention to brush, while the hurdle model's Bernoulli component is observed and represents whether the user brushed or not. Therefore, the zero-inflated model's nonzero component models the user's brushing duration when the user intends to brush, and the hurdle model's nonzero component models the user's brushing duration conditional on whether the user brushed or not. Throughout the model fitting process, we performed various checks on the quality of the model to determine whether the fitted model was sufficient (Appendix A.4). This included checking whether the percentage of zero brush times simulated by our model was comparable to that of the original ROBAS 2 dataset. Additionally, we checked whether the model accurately captured the mean and variance of the nonzero brushing durations across users.\n\nThe first approach we took was to choose one model class (zero-inflated Poisson) and fit a single population-level model for all users in the ROBAS 2 study. However, a single population-level model was insufficient for fitting all users due to the high level of user heterogeneity (i.e., the between-user and within-user variance of the simulated brushing durations from the fitted model was smaller than the between-user and withinuser variance of brushing durations in the ROBAS 2 data). Thus, next, we decided to maintain one model class, but fit one model per user for all users. However, when we fit a zero-inflated Poisson to each user, we found that the model provided an adequate fit for some users but not for users who showed more variability in their brushing durations. The within-user variance simulated rewards from the model fit on those users was still lower than the within-user variance of the ROBAS 2 user data used to fit the model. Therefore, we considered a hurdle model  [43]  because it is more flexible than the zero-inflated Poisson. For Poisson distributions, the mean and variance are equal, whereas the hurdle model does not conflate the mean and variance.\n\nUltimately, for each user, we considered three model classes: (1) a zero-inflated Poisson, (2) a hurdle model with a square root transform, and (3) a hurdle model with a log transform, and chose one of these model classes for each user (Appendix A.2). Specifically, to select the model class for user i, we fit all three model classes using each user's data from ROBAS 2. Then, we chose the model class that had the lowest root mean squared error (RMSE) (Appendix A.3). Additionally, along with the base model that generates stationary rewards, we include an environmental variant with a nonstationary reward function; here, \"day in study\" is used as a feature in the environment's reward generating model (Appendix A.1).\n\nImputing Treatment Effect Sizes: To construct a model of rewards for when an intervention message is sent (a case for which we have no data), we impute plausible treatment effects with the interdisciplinary team and modify the fitted base model with these effects. Specifically, we impute treatment effects on the Bernoulli component and the nonzero component. We impute both types of treatment effects because the investigative team's intervention messages were developed to encourage users to brush more frequently and to brush for the recommended duration. Furthermore, because the research team believes that the users may respond differently to the engagement messages depending on the context and depending on the user, we included context-aware, population-level, and user-heterogeneous effects of the engagement messages as environmental variants (Appendix A.5).\n\nWe use the following guidelines to guide the design of the effect sizes:\n\n1.\n\nIn general, for mobile health digital interventions, we expect the effect (magnitude of weight) of actions to be smaller than (or on the order of) the effect for baseline features, which include time of day and the user's previous day brushing duration (all features are specified in Appendix A.1)."
    },
    {
      "title": "2.",
      "text": "The variance in treatment effects (weights representing the effect of actions) across users should be on the order of the variance in the effect of features across users (i.e., variance in parameters of fitted user-specific models).\n\nFollowing guideline 1 above, to set the population level effect size, we take the absolute value of the weights (excluding that for the intercept term) of the base models fitted for each ROBAS 2 user and the average across users and features (e.g., the average absolute value of weight for time of day and previous day brushing duration). For the heterogeneous (user-specific) effect sizes, for each user, we draw a value from a normal centered at the population effect sizes. Following guideline 2, the variance of the normal distributions is found by again taking the absolute value of the weights of the base models fitted for each user, averaging the weights across features, and taking the empirical variance across users. In total, there are eight environment variants, which are summarized in Table  1 . See Appendix A for further details regarding the development of the simulation environments."
    },
    {
      "title": "Experiment and Results",
      "text": "We evaluate the RL algorithm candidates in each of the environment variants (stability). Specifically, the RL algorithm candidates will be comprised of a posterior sampling algorithm with two different reward models: (i) a Bayesian linear regression model (BLR) and (ii) a zero-inflated Poisson regression model (ZIP); see Appendices B and C for more discussion of these algorithms. Additionally, for each of these two reward approximating functions, we will consider different cluster sizes (with k = 1, 4, N users). Each cluster will have one RL algorithm instantiation per cluster (no data shared across clusters). We cluster users by their entry date into the study (e.g., the first k users are in the first cluster, the next k users are in the second cluster, and so on). Since for the real Oralytics study we will incrementally recruit users into the study at a rate of about four users per week, for our experiments, we also simulate four users entering the study every week. To simulate a study, we draw N = 72 users (approximately the expected sample size for the Oralytics study) with replacement and cluster them by their entry date into the simulated study. The algorithms for each cluster are updated weekly with the first update taking place after one week (at decision time t = 14 for each cluster).\n\nTo evaluate personalization, we use the following metrics to compare algorithms: average rewards (average across users and time) and the 25th percentile of average rewards (averaged over time) across users. The purpose of looking at the 25th percentile of average rewards across users is to evaluate how well the algorithms perform on the worst off users (i.e., users who have a lower average reward than the average user). We ran 100 Monte Carlo trials for each environmental variant and algorithm candidate pairing. Table  2  shows the average and the 25th percentile of users' average (across time) rewards. Figure  3  shows the average reward over time.\n\nWe highlight the following takeaways from our experiments:\n\n1."
    },
    {
      "title": "BLR vs. ZIP:",
      "text": "We prefer BLR to ZIP. BLR with cluster size k = N results in higher user rewards than all other RL algorithm candidates in all environments in terms of average reward and 25th percentile reward (Table  2 ) and for average reward across all user decision times (Figure  3 ). It is interesting to note that BLR with cluster size k = 4 performs comparably to ZIP for all cluster sizes k (Table  2 , Figure  3 ). Originally, we hypothesized that ZIP would perform better than BLR because the ZIP-based algorithms can better model the zero-inflated nature of the rewards. We believe that the ZIP-based algorithms suffered in performance because they require fitting more parameters and thus require more data to learn effectively. On the other hand, the BLR model trades off bias and variance more effectively in our data-sparse settings.\n\nBeyond considerations of their ability to personalize, we also prefer the BLR-based RL algorithms because they have an easy-to-compute closed-form posterior update (computability and stability). The ZIP-based algorithms involve using approximate posterior sampling, which is more computationally intensive and numerically unstable. In addition, BLR with action centering is robust, namely, it is guaranteed to be unbiased even when the baseline reward model is incorrect  [1] . BLR with action centering specifically does not require the knowledge of the baseline features at decision time (See Appendix C.1.1). This means that baseline features only need to be available at update time and we can incorporate more features that were not available in real time at the decision time.\n\nTable  2 . Average and 25th Percentile Rewards. Average and 25th percentile rewards are defined in Section 3.1. The naming convention for environment variants is found in Table  1 . \"k\" refers to the cluster size. Average rewards are averaged across time, users, and 100 trials. For the 25th percentile rewards, we average rewards for each user across time, find the lower 25th percentile across N = 72 users, and then average that across 100 trials. The value in the parenthesis is the standard error of the mean. The best performing algorithm candidate in each environment variant is bolded. BLR (k = N) performs better than other algorithm candidates across all simulated environments. Notice that the average rewards are lower than the 120-s dentist-recommended brushing duration. This is because of the zero-inflated nature of our setting (i.e., the user does not brush)."
    },
    {
      "title": "RL Algorithm Candidates",
      "text": "Rewards RL Algorithm S_Het NS_Het S_Pop NS_Pop ZIP k = 1 100.038 (0.597) 102.566 (0.526) 107.184 (0.626) 109.379 (0.552) ZIP k = 4 100.463 (0.586) 103.035 (0.539) 108.217 (0.609) 110.242 (0.562) ZIP k = N 100.791 (0.596) 103.391 (0.546) 108.410 (0.617) 110.542 (0.554) BLR k = 1 97.196 (0.585) 99.691 (0.527) 103.692 (0.615) 105.590 (0.546) BLR k = 4 99.772 (0.590) 102.310 (0.547) 107.568 (0.619) 109.454 (0.547) BLR k = N 101.267 (0.590) 104.024 (0.542) 108.974 (0.610) 111.201 (0.546) 25th Percentile Rewards RL Algorithm S_Het NS_Het S_Pop NS_Pop ZIP k = 1 67.907 (1.150) 73.830 (0.403) 74.898 (1.016) 78.651 (0.556) ZIP k = 4 68.865 (1.067) 73.836 (0.464) 75.933 (1.114) 80.413 (0.629) ZIP k = N 69.448 (1.201) 74.580 (0.475) 76.312 (1.122) 80.424 (0.648) BLR k = 1 65.600 (1.139) 70.703 (0.457) 70.915 (1.024) 74.782 (0.596) BLR k = 4 68.045 (1.122) 73.322 (0.505) 75.766 (1.097) 79.809 (0.622) BLR k = N 69.757 (1.171) 75.393 (0.427) 77.272 (1.096) 81.675 (0.583) ZIP (k = 1) ZIP (k = 4) ZIP (k = N) BLR (k = 1) BLR (k = 4) BLR (k = N) 20 40 60 80 100 120 140 User Decision Times 90 92 94 96 98 100 102 Average Reward"
    },
    {
      "title": "2.",
      "text": "Cluster Size: RL algorithms with larger cluster sizes k perform better overall, especially for simulation environments with population-level treatment effects (rather than heterogeneous treatment effects). At first glance, one might think that algorithms with smaller cluster sizes may perform better because they can learn more personalized policies for users (especially in environments with heterogeneous treatment effects). Interestingly, though, the algorithms with larger cluster sizes performed better across all environments in terms of average reward (average across users and time) and the 25th-percentile of the average reward (average over time) across users (Table  2 ); this means that the RL algorithm candidates with larger cluster sizes performed better for both the average user and for the worst off users. The better performance of algorithms with larger cluster sizes is likely due to their ability to reduce noise and learn faster by leveraging the data of multiple users to learn. Even though the algorithms with larger cluster sizes are less able to model and learn the heterogeneity across users, this is outweighed by the immense benefit of sharing data across users to learn faster and reduce noise.\n\nThere are some limitations to these experiments. Fixed Reward Noise Variance for BLR: The BLR algorithm includes a noise variance hyperparameter (\u03b7 2 in Equation (A3)). In our experiments, we set \u03b7 2 to the reward variance observed in the ROBAS 2 data set. Assuming that \u03b7 2 is known is unrealistic; in the future, we plan to learn \u03b7 2 along with other BLR algorithm parameters in the real study. The known value of \u03b7 2 could be a reason that BLR performed comparably to ZIP.\n\nMore Distinct and Complex Simulation Environments: We may not be looking widely enough across environment variants to find settings where these algorithms perform differently. With sufficient data per user in a highly heterogeneous user environment, we expect cluster size k = 1 to do the best. In future work, we aim to add simulation environments with greater heterogeneity and less noise to see if large cluster sizes still perform well, and we aim to create more complex simulation environment variants that are more distinct (e.g., environments where users may differ by heterogeneous demographic features like age and gender). Additionally, we want to impute state features of interest in the real study that were not present in the data set, such as phone engagement.\n\nAdditional RL Algorithm Candidate Considerations. We also aim to consider other axes for algorithm candidates such as algorithms with other update cadences (e.g., every night or biweekly) and algorithms with an informative prior. In initial simulations using algorithms with informative priors, we found that since the same (limited amount) of ROBAS 2 data was used to build both the simulation environment and the prior, the algorithms did not need to learn much to perform well. An open question is how to develop both simulation environments and informative priors in a realistic way using a limited amount of data. Finally, we will also explore additional design decisions such as how to carefully design the feature space for the RL algorithm.\n\nThese investigations will determine the final algorithm that goes into the actual study."
    },
    {
      "title": "Discussion and Future Work",
      "text": "In this paper, we present the first extension of the PCS framework for designing RL algorithms in digital intervention settings. The case study demonstrates how to use the PCS framework to make design decisions and highlights our ongoing work in designing the Oralytics RL algorithm. This work helps fellow researchers understand and balance the benefits and drawbacks of certain aspects of the RL algorithm they use for their digital intervention studies. We consider three model classes: (1) a zero-inflated Poisson, (2) a hurdle model with a square root transform, and (3) a hurdle model with a log transform, and choose one out of these three model classes for each user. We define these three model classes below. Additionally, below g(S) is the baseline feature vector of the current state defined in Appendix A.1, w i,b , w i,p , w i,\u00b5 are user-specific weight vectors, \u03c3 2 i,u is the user-specific variance for the normal component, and sigmoid(x) = 1 1+e -x is the sigmoid function.\n\n( Note that the nonzero component of this model, Y 2 , can also be represented as a constant times a noncentral chi-squared, where the noncentrality parameter is the square of the mean of the normal distribution.\n\n(3) Hurdle Model with Log Transform for Brushing Duration\n\nBrushing Duration in Seconds : D = ZY Since we want to simulate brushing duration in seconds, we also round outputs of the hurdle models to the nearest whole integer. Notice that the zero-inflated Poisson model is a mixture model with a latent state. The Bernoulli draw Z is latent and represents the user's intention to brush, and the Poisson models the user's brushing duration when they intend to brush (this is because the brush time can still be zero when the user intends to brush). On the other hand, the hurdle model provides a model for brushing duration conditional on whether the user brushed or not. The Bernoulli draw Z in the hurdle model is observed.\n\nNote that the hurdle model is used for the simulation environment only and not the RL algorithm. The hurdle model conditions on a collider (e.g., whether the person brushes their teeth), thus potentially leading to causal bias  [44, 45] . For example, consider an unobserved cause U, intervention A, whether the user brushed or not Z, brushing duration D, and a directed acyclic graph with A \u2192 Z, U \u2192 Z, U \u2192 D, and Z \u2192 D. Then conditioning on collider Z of treatment opens a pathway from A to D through U  [46] . Suppose in reality A only impacts whether the user brushes their teeth but not the duration. Then, if we condition on Z to evaluate the impact of A on D, we may erroneously learn that A impacts the duration of brushing. This makes the hurdle unsuitable as a model for an RL algorithm that aims to learn causal effects."
    },
    {
      "title": "Appendix A.3. Fitting the Environment Base Models",
      "text": "We use ROBAS 2 data to fit the brushing duration model under action 0 (no message). For all model classes, we fit one model per user. All models were fit using MAP with a prior w i,b , w i,p , w i,\u00b5 \u223c N (0, I) as a form of regularization because we have sparse data for each user. Weights were chosen by running random restarts and selecting the weights with the highest log posterior density.\n\nFitting Hurdle Models: For fitting hurdle models for user i, we fit the Bernoulli component and the nonzero brushing duration component separately. We use D i,t to denote the ith ROBAS 2 user's brushing duration in seconds at the t time point. Set Z i,t = 1 if the original observation D i,t > 0 and 0 otherwise. We fit a model for this Bernoulli component. We then fit a model for the normal component to either the square root transform Y i,t = D i,t or to inverse-log transform Y i,t = exp(D i,t ) of the ith user's nonzero brushing duration.\n\nFitting Zero-Inflated Poisson Models: For the zero-inflated Poisson model, we jointly fit parameters for both the Bernoulli and the Poisson components. Since the brushing durations in the ROBAS 2 data were integer values, we did not have to transform the observation to fit the zero-inflated Poisson model.\n\nThe fitted parameters for the environment base models can be accessed at:  https://  github.com/StatisticalReinforcementLearningLab/pcs-for-rl/tree/main/sim_env_data (accessed on 1 June 2022)."
    },
    {
      "title": "Selecting the Model Class for Each User",
      "text": "To select the model class for user i, we fit all three model classes using user i's data from ROBAS 2. We then chose the model class that had the RMSE. Namely, we choose the model class with the lowest L i , where:\n\nRecall that D i,t is the brush time in seconds for user i at decision time t. Definitions of \u00ca[D i,t |S i,t ] for each model class are specified below in Table  A1 ."
    },
    {
      "title": "Average of Variances Within-User Brushing Durations:",
      "text": "This metric measures the average amount of within-user variance.\n\n1\n\nThe base models slightly overestimate the proportion of missed brushing windows in the ROBAS 2 data set. Our base models also slightly underestimate the average brushing duration. Our base models also for the most part slightly overestimate the between-user and within-user variance of rewards.\n\nTable A4. Comparing Moments Between Base Models and ROBAS 2 Data Set. Above, we use BDs to abbreviate Brushing Durations. Values for the Stationary and Nonstationary base models are averaged across 100 trials. Metrics ROBAS 2 Stationary Non-Stationary Proportion of Missed Brushing Windows 0.376674 0.403114 0.397812 Average Nonzero BDs 137.768129 131.308445 134.676955 Variance of Nonzero BDs 2326.518304 2392.955018 2253.177853 Variance of Average User BDs 1415.920148 1699.126897 1399.615330 Average of Variances of Within-User BDs 1160.723506 1405.944459 1473.239769 Appendix A.4.2. Measuring If a Base Model Captures the Variance in the Data\n\nWe measure how well the fitted base models captured (1) whether or not the user brushed and (2) the variance of the brush time when the users did brush. To measure point  (1)  for each user model i, we calculate the statistic:\n\nt > 0] -E[I[D i,t > 0]|S i,t ] Var[I[D i,t > 0]|S i,t ] 2 (A1) where E[I[D i,t > 0]|S i,t ] = 1sigmoid(S T i,t w i,b ) and Var[I[D i,t > 0]|S i,t ] = E[I[D i,t > 0]|S i,t ] \u2022 sigmoid(S T i,t w i,b ). To measure point (2) for each user model i, we calculate the statistic: U i := 1 \u2211 T t=1 I[D i,t > 0] T \u2211 t=1 I[D i,t > 0] D i,t -E[D i,t |S i,t , D i,t > 0] Var[D i,t |S i,t , D i,t > 0] 2 (A2) Definitions of E[D i,t |S i,t , D i,t > 0] and Var[D i,t |S i,t , D i,t > 0] for the nonzero component of each model class are specified in Table  A2 . For a user model to capture the variance in the data, U i should be close to 1. We calculate the empirical mean U = 1 N \u2211 N i=1 U i and standard deviation \u03c3 U = std(U i ), and the approximate 95% confidence interval is\n\n. Results are in Table  A5 . We can see that after computing the statistic for each user, the confidence interval is close to 1. We understand that the confidence intervals do not contain 1, which implies that we are overestimating the amount of variance in I[D > 0] and underestimating the amount of variance in D|D > 0. In the future, we hope to improve upon this statistic by considering nonlinear components in our base models.\n\nTable A5. Statistic i for Capturing Variance in the Data. Values are rounded to the nearest 3 decimal places. Metric Stationary Non-Stationary Equation (A1) U 0.811 0.792 Equation (A1) \u03c3 U 0.146 0.150 Equation (A1) Confidence Interval (0.760, 0.861) (0.739, 0.844) Equation (A2) U 3.579 3.493 Equation (A2) \u03c3 U 4.861 4.876 Equation (A2) Confidence Interval (1.895, 5.263) (1.803, 5.182) 0.8 0.6 0.4 0.2 0.0 Sizes 0% 5% 10% 15% 20% 25% Percentage B of Hurdle effect sizes B = -0.473 (a) 0.0 0.5 1.0 1.5 2.0 Effect Sizes 0% 2% 5% 8% 10% 12% 15% 18% Percentage N of Hurdle (Square Root) effect sizes N = 1.020 (b) 0.2 0.0 0.2 0.4 0.6 Effect Sizes 0% 5% 10% 15% 20% 25% Percentage N of Hurdle (Log) effect sizes N = 0.193 (c) 0.8 0.6 0.4 0.2 Effect Sizes 0% 5% 10% 15% 20% 25% Percentage B of ZIP effect sizes B = -0.473 (d) 0.0 0.2 0.4 0.6 Effect Sizes 0% 5% 10% 15% 20% 25% Percentage N of ZIP effect sizes N = 0.166 Recall that we have normal priors on \u03b8 i where \u03b8 i \u223c N (\u00b5 prior , \u03a3 prior ), where \u00b5 prior = 0 \u2208 R 3+3+4 and \u03a3 prior = diag(\u03c3 2 prior I 3 , \u03c3 2 prior I 3 , \u03c3 2 prior I 4 ). The posterior distribution of the weights given current history H i,t-1 , p(\u03b8 i |H i,t-1 ) is conjugate and is also normal.\n\nt-1 \u223c N (\u00b5 posterior i,t-1 , \u03a3 posterior i,t-1 ) \u03a3 posterior i,t-1 = 1 \u03b7 2 \u03a6 T i,1:t-1 \u03a6 i,1:t-1 + \u03a3 -1 prior -1 \u00b5 posterior i,t-1 = \u03a3 posterior i,t-1 1 \u03b7 2 \u03a6 T i,1:t-1 R i,1:t-1 + \u03a3 -1 prior \u00b5 prior Note that we fit \u03b7 2 to the ROBAS 2 dataset and fixed it for all of our experiments. For the real study, we are considering assigning a conjugate prior on \u03b7 2 and updating it at update times. Appendix C.1.2. Zero-Inflated Poisson Regression Model\n\nFor the zero-inflated Poisson regression model, the posterior distribution of the weights \u03b8 i = {\u03b1 i,b , \u03b2 i,b , \u03b1 i,p , \u03b2 i,p } given data H i,t-1 , p(\u03b8 i |H i,t-1 ) does not have a closed form. Therefore, we use Metropolis-Hastings (MH) with a normal proposal distribution as an approximate posterior sampling method.\n\nPosterior Density:\n\nThe log-likelihood of the zero-inflated Poisson regression model is: log f (R i,t |S i,t , A i,t ; \u03b8 i ) = log((1p) + p exp(-\u03bb)) R = 0 log p\u03bb + R log \u03bblog R! R = 1, 2, 3, ... where p = 1sigmoid m(S i,t ) T \u03b1 i,b + A i,t \u2022 f (S i,t ) T \u03b2 i,b is the probability of the user intending to brush, and \u03bb = exp m(S i,t ) T \u03b1 i,p + A i,t \u2022 f (S i,t ) T \u03b2 i,p is the expected Poisson count.\n\nTherefore, the log posterior density is: log p(\u03b8 i |H i,t-1 ) \u221d N \u2211 n=1 log f (R i,t |S i,t , A i,t ; \u03b8 i ) + log p(\u03b8 i )\n\nProposal Distribution:\n\nWe choose a normal distribution for our proposal distribution. At each step of MH, we propose a new sample given the old sample, \u03b8 k prop \u223c N (\u03b8 k old , \u03b3 2 I), where \u03b8 k denotes the kth value of \u03b8."
    },
    {
      "title": "Metropolis-Hastings Acceptance Ratio:",
      "text": "The Metropolis-Hastings acceptance ratio given a proposed sample \u03b8 prop and an old sample \u03b8 old is defined as: \u03b1(\u03b8 prop , \u03b8 old ) := min 1, p(\u03b8 prop )/q(\u03b8 prop |\u03b8 old ) p(\u03b8 old )/q(\u03b8 old |\u03b8 prop )\n\nSince our proposal distribution is symmetric, the log acceptance ratio becomes: log \u03b1(\u03b8 prop , \u03b8 old ) := min(0, log p(\u03b8 prop )log p(\u03b8 old ))"
    },
    {
      "text": "Figure 2.Histogram of brushing durations in seconds for all user brushing sessions in ROBAS 2. The ROBAS 2 study had 32 users total and each user had 56 brushing windows (2 brushing windows per day for 28 days). If a user did not brush during a brushing window, their brushing duration is recorded as zero seconds. Note in the figure above that across all users and brushing windows, about 40% of brushing sessions had no brushing, that is, a brushing duration of zero seconds. The ROBAS 2 brushing durations are highly zero-inflated."
    },
    {
      "text": "Figure 3. Average User Rewards Over Time. Above, we show simulation results of the six candidate algorithms (BLR and ZIP respectively for different cluster sizes k) across the four simulation environments. The y-axis is the mean and \u00b11.96 \u2022 standard error of the average user rewards ( R = 1 72 \u2211 72 i=1 1 t 0 \u2211 t 0 s=1 R i,s ) for decision times t 0 \u2208 [20, 40, 60, 80, 100, 120, 140] across 100 Monte Carlo simulated trials. Standard error is \u03c3 \u221a 100 where \u03c3 is the sample variance of the 100 Rs. (a) Stationary Base Model and Heterogeneous Effect Size; (b) Nonstationary Base Model and Heterogeneous Effect Size; (c) Stationary Base Model and Population Effect Size; (d) Nonstationary Base Model and Population Effect Size."
    },
    {
      "text": "Total Brushing Duration in Seconds = (Brushing Duration -172)/118 Normalized Day in Study When Fitting Model = (Day -14.5)/13.5 Normalized Day in Study When Generating Rewards = (Day -35.5)/34.5 Appendix A.2. Environment Base Model"
    },
    {
      "text": ") Zero-Inflated Poisson Model for Brushing Duration Z \u223c Bernoulli 1sigmoid(g(S) T w i,b ) Y \u223c Poisson exp g(S) T w i,p Brushing Duration in Seconds : D = ZY (2) Hurdle Model with Square Root Transform for Brushing Duration Z \u223c Bernoulli 1sigmoid g(S) T w i,b Y \u223c N g(S) T w i,\u00b5 , \u03c3 2 i,u Brushing Duration in Seconds : D = ZY 2"
    },
    {
      "text": "Figure A1. Effect sizes \u2206 i,B 's, \u2206 i,N 's, \u00b5 B , \u00b5 N for each base model class. The effect sizes are used to generate rewards under action A = 1 for the simulation environment. (a) Bernoulli component (hurdle); (b) Nonzero component (square root); (c) Nonzero component (log); (d) Bernoulli component (ZIP); (e) Poisson component (ZIP)."
    },
    {
      "text": "Four Environment Variants. We consider two environment base models (stationary and nonstationary) and two effect sizes (population effect size, heterogeneous effect size)."
    }
  ],
  "references": [
    {
      "title": "Bias/Intercept Term \u2208 R 2. Time of Day",
      "doi": "10.22501/rc.1166516"
    },
    {
      "title": "Prior Day Total Brushing Duration (Normalized) \u2208 R 4. Weekend Indicator (Weekday/Weekend) \u2208 {0",
      "doi": "10.7717/peerj.4248/fig-2"
    },
    {
      "title": "Proportion of Nonzero Brushing Sessions Over Past 7 Days \u2208",
      "doi": "10.1787/888933835022"
    },
    {
      "title": "Normalization of State Features We normalize features to ensure that state features are all in a similar range. The Prior Day Total Brushing Duration feature is normalized using z-score normalization (subtract mean and divide by standard deviation) and the Day in Study feature"
    },
    {
      "title": "Personalized HeartSteps: A Reinforcement Learning Algorithm for Optimizing Physical Activity",
      "authors": [
        "P Liao",
        "K Greenewald",
        "P Klasnja",
        "S Murphy"
      ],
      "year": 2020,
      "doi": "10.1145/3381007"
    },
    {
      "title": "Encouraging physical activity in patients with diabetes: Intervention using a reinforcement learning system",
      "authors": [
        "E Yom-Tov",
        "G Feraru",
        "M Kozdoba",
        "S Mannor",
        "M Tennenholtz",
        "I Hochberg"
      ],
      "year": 2017,
      "doi": "10.2196/jmir.7994"
    },
    {
      "title": "Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss?",
      "authors": [
        "E Forman",
        "S Kerrigan",
        "M Butryn",
        "A Juarascio",
        "S Manasse",
        "S Onta\u00f1\u00f3n",
        "D Dallal",
        "R Crochiere",
        "D Moskow"
      ],
      "year": 2019,
      "doi": "10.1007/s10865-018-9964-1"
    },
    {
      "title": "Stanford Computational Policy Lab Pretrial Nudges",
      "authors": [
        "S Allen"
      ],
      "year": 2022,
      "doi": "10.24148/cdrb2022-4"
    },
    {
      "title": "Bandit algorithms to personalize educational chatbots",
      "authors": [
        "W Cai",
        "J Grossman",
        "Z Lin",
        "H Sheng",
        "J Wei",
        "J Williams",
        "S Goel"
      ],
      "year": 2021,
      "doi": "10.1007/s10994-021-05983-y"
    },
    {
      "title": "Bandit Learning with Implicit Feedback",
      "authors": [
        "Y Qi",
        "Q Wu",
        "H Wang",
        "J Tang",
        "M Sun"
      ]
    },
    {
      "authors": [
        "S Bengio",
        "H Wallach",
        "H Larochelle",
        "K Grauman",
        "N Cesa-Bianchi",
        "R Garnett",
        "Eds",
        "Inc Curran Associates"
      ],
      "year": 2018,
      "doi": "10.2172/7218131"
    },
    {
      "title": "Letter to Amazon Shareholders",
      "authors": [
        "J Bezos"
      ],
      "year": 1997
    },
    {
      "title": "Veridical data science",
      "authors": [
        "B Yu",
        "K Kumbier"
      ],
      "year": 2020,
      "doi": "10.1073/pnas.1901326117"
    },
    {
      "title": "Reinforcement Learning: An Introduction",
      "authors": [
        "R Sutton",
        "A Barto"
      ],
      "year": 2018
    },
    {
      "title": "Reinforcement learning for personalization: A systematic literature review",
      "authors": [
        "F Hengst",
        "E Grua",
        "A El Hassouni",
        "M Hoogendoorn"
      ],
      "year": 2020,
      "doi": "10.3233/DS-200028"
    },
    {
      "title": "Bandit problems with side observations",
      "authors": [
        "C Wang",
        "S Kulkarni",
        "H Poor"
      ],
      "year": 2005,
      "doi": "10.1109/TAC.2005.844079"
    },
    {
      "title": "The epoch-greedy algorithm for contextual multi-armed bandits",
      "authors": [
        "J Langford",
        "T Zhang"
      ],
      "year": 2007
    },
    {
      "title": "From ads to interventions: Contextual bandits in mobile health",
      "authors": [
        "A Tewari",
        "S Murphy"
      ],
      "year": 2017,
      "doi": "10.1007/978-3-319-51394-2_25"
    },
    {
      "title": "What is personalization? Perspectives on the design and implementation of personalization in information systems",
      "authors": [
        "H Fan",
        "M Poole"
      ],
      "year": 2006,
      "doi": "10.1207/s15327744joce1603&4_2"
    },
    {
      "title": "Data-efficient off-policy policy evaluation for reinforcement learning",
      "authors": [
        "P Thomas",
        "E Brunskill"
      ],
      "year": 2016,
      "doi": "10.1007/978-3-319-96136-1_26"
    },
    {
      "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
      "authors": [
        "S Levine",
        "A Kumar",
        "G Tucker",
        "J Fu"
      ]
    },
    {
      "title": "Assessing time-varying causal effect moderation in mobile health",
      "authors": [
        "A Boruvka",
        "D Almirall",
        "K Witkiewitz",
        "S Murphy"
      ],
      "year": 2018,
      "doi": "10.1080/01621459.2017.1305274"
    },
    {
      "title": "Confidence intervals for policy evaluation in adaptive experiments",
      "authors": [
        "V Hadad",
        "D Hirshberg",
        "R Zhan",
        "S Wager",
        "S Athey"
      ],
      "year": 2021,
      "doi": "10.1073/pnas.2014602118"
    },
    {
      "title": "Power Constrained Bandits",
      "authors": [
        "J Yao",
        "E Brunskill",
        "W Pan",
        "S Murphy",
        "F Doshi-Velez"
      ],
      "year": 2021
    },
    {
      "authors": [
        "K Jung",
        "S Yeung",
        "M Sendak",
        "M Sjoding",
        "R Ranganath"
      ],
      "year": 2021
    },
    {
      "title": "Mobile Health Apps: Adoption, Adherence, and Abandonment",
      "authors": [
        "E Murnane",
        "D Huffaker",
        "G Kossinets"
      ],
      "year": 2015,
      "doi": "10.1145/2800835.2800943"
    },
    {
      "title": "Opportunities and Challenges for Smartphone Applications in Supporting Health Behavior Change: Qualitative Study",
      "authors": [
        "L Dennison",
        "L Morrison",
        "G Conway",
        "L Yardley"
      ],
      "year": 2013,
      "doi": "10.2196/jmir.2583"
    },
    {
      "title": "PerSim: Data-Efficient Offline Reinforcement Learning with Heterogeneous Agents via Personalized Simulators",
      "authors": [
        "A Agarwal",
        "A Alomar",
        "V Alumootil",
        "D Shah",
        "D Shen",
        "Z Xu",
        "C Yang"
      ]
    },
    {
      "title": "Adaptive learning algorithms to optimize mobile applications for behavioral health: Guidelines for design decisions",
      "authors": [
        "C Figueroa",
        "A Aguilera",
        "B Chakraborty",
        "A Modiri",
        "J Aggarwal",
        "N Deliu",
        "U Sarkar",
        "J Jay Williams",
        "C Lyles"
      ],
      "year": 2021,
      "doi": "10.1093/jamia/ocab001"
    },
    {
      "title": "Learning to simulate on sparse trajectory data",
      "authors": [
        "H Wei",
        "C Chen",
        "C Liu",
        "G Zheng",
        "Z Li"
      ],
      "year": 2020
    },
    {
      "title": "RecSim: A Configurable Simulation Platform for Recommender Systems",
      "authors": [
        "E Ie",
        "C Hsu",
        "M Mladenov",
        "V Jain",
        "S Narvekar",
        "J Wang",
        "R Wu",
        "C Boutilier"
      ]
    },
    {
      "title": "MARS-Gym: A Gym framework to model, train, and evaluate Recommender Systems for Marketplaces",
      "authors": [
        "M Santana",
        "L Melo",
        "F Camargo",
        "B Brand\u00e3o",
        "A Soares",
        "R Oliveira",
        "S Caetano"
      ],
      "doi": "10.1109/icdmw51313.2020.00035"
    },
    {
      "authors": [
        "G Brockman",
        "V Cheung",
        "L Pettersson",
        "J Schneider",
        "J Schulman",
        "J Tang",
        "W Zaremba",
        "Gym"
      ],
      "doi": "10.1063/pt.5.029931"
    },
    {
      "title": "Optimizing Adaptive Notifications in Mobile Health Interventions Systems: Reinforcement Learning from a Data-driven Behavioral Simulator",
      "authors": [
        "S Wang",
        "C Zhang",
        "B Kr\u00f6se",
        "H Van Hoof"
      ],
      "year": 2021,
      "doi": "10.1007/s10916-021-01773-0"
    },
    {
      "title": "Building healthy recommendation sequences for everyone: A safe reinforcement learning approach",
      "authors": [
        "A Singh",
        "Y Halpern",
        "N Thain",
        "K Christakopoulou",
        "E Chi",
        "J Chen",
        "A Beutel"
      ],
      "year": 2020
    },
    {
      "title": "Simulation Environment for Guiding the Design of Contextual Personalization Systems in the Context of Hearing Aids",
      "authors": [
        "M Korzepa",
        "M Petersen",
        "J Larsen",
        "M M\u00f8rup"
      ],
      "year": 2020,
      "doi": "10.1145/3386392.3399291"
    },
    {
      "title": "Personalization of Health Interventions Using Cluster-Based Reinforcement Learning",
      "authors": [
        "A Hassouni",
        "M Hoogendoorn",
        "M Van Otterlo",
        "E Barbaro"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-030-03098-8_31"
    },
    {
      "title": "A clustering-based reinforcement learning approach for tailored personalization of e-Health interventions",
      "authors": [
        "A Hassouni",
        "M Hoogendoorn",
        "M Van Otterlo",
        "A Eiben",
        "V Muhonen",
        "E Barbaro"
      ],
      "doi": "10.1145/3350546.3352527"
    },
    {
      "title": "Stable Discovery of Interpretable Subgroups via Calibration in Causal Studies",
      "authors": [
        "R Dwivedi",
        "Y Tan",
        "B Park",
        "M Wei",
        "K Horgan",
        "D Madigan",
        "B Yu"
      ],
      "year": 2020,
      "doi": "10.1111/insr.12427"
    },
    {
      "title": "Next waves in veridical network embedding",
      "authors": [
        "O Ward",
        "Z Huang",
        "A Davison",
        "T Zheng"
      ],
      "year": 2021,
      "doi": "10.1002/sam.11486"
    },
    {
      "title": "A new method to compare the interpretability of rule-based algorithms",
      "authors": [
        "V Margot",
        "G Luta"
      ],
      "year": 2021,
      "doi": "10.3390/ai2040037"
    },
    {
      "title": "A Scalable System for Passively Monitoring Oral Health Behaviors Using Electronic Toothbrushes in the Home Setting: Development and Feasibility Study",
      "authors": [
        "V Shetty",
        "D Morrison",
        "T Belin",
        "T Hnat",
        "S Kumar"
      ],
      "year": 2020,
      "doi": "10.2196/17347"
    },
    {
      "title": "The dependence of effective planning horizon on model accuracy",
      "authors": [
        "N Jiang",
        "A Kulesza",
        "S Singh",
        "R Lewis"
      ],
      "year": 2015,
      "doi": "10.1007/978-3-319-24804-2"
    },
    {
      "title": "A Tutorial on Thompson Sampling",
      "authors": [
        "D Russo",
        "B Roy",
        "A Kazerouni",
        "I Osband"
      ],
      "year": 2022,
      "doi": "10.1561/9781680834710"
    },
    {
      "title": "Group-driven reinforcement learning for personalized mhealth intervention",
      "authors": [
        "F Zhu",
        "J Guo",
        "Z Xu",
        "P Liao",
        "L Yang",
        "J Huang"
      ],
      "year": 2018,
      "doi": "10.1007/978-3-030-00928-1_67"
    },
    {
      "title": "IntelligentPooling: Practical Thompson sampling for mHealth",
      "authors": [
        "S Tomkins",
        "P Liao",
        "P Klasnja",
        "S Murphy"
      ],
      "year": 2021,
      "doi": "10.1007/s10994-021-05995-8"
    },
    {
      "title": "Multi-task learning for contextual bandits",
      "authors": [
        "A Deshmukh",
        "U Dogan",
        "C Scott"
      ],
      "year": 2017
    },
    {
      "title": "Horde of bandits using gaussian markov random fields",
      "authors": [
        "S Vaswani",
        "M Schmidt",
        "L Lakshmanan"
      ],
      "year": 2017,
      "doi": "10.19070/2377-8075-1700086"
    },
    {
      "title": "A comparison of zero-inflated and hurdle models for modeling zero-inflated count data",
      "authors": [
        "C Feng"
      ],
      "year": 2021,
      "doi": "10.1186/s40488-021-00121-4"
    },
    {
      "title": "Illustrating bias due to conditioning on a collider",
      "authors": [
        "S Cole",
        "R Platt",
        "E Schisterman",
        "H Chu",
        "D Westreich",
        "D Richardson",
        "C Poole"
      ],
      "year": 2010,
      "doi": "10.1093/ije/dyp334"
    },
    {
      "title": "Educational Note: Paradoxical collider effect in the analysis of non-communicable disease epidemiological data: A reproducible illustration and web application",
      "authors": [
        "M Luque-Fernandez",
        "M Schomaker",
        "D Redondo-Sanchez",
        "M Jose Sanchez Perez",
        "A Vaidya",
        "M Schnitzer"
      ],
      "year": 2019,
      "doi": "10.1093/ije/dyy275"
    },
    {
      "title": "Endogenous selection bias: The problem of conditioning on a collider variable",
      "authors": [
        "F Elwert",
        "C Winship"
      ],
      "year": 2014,
      "doi": "10.1146/annurev-soc-071913-043455"
    }
  ],
  "num_references": 52
}
