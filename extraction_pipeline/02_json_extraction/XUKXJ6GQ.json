{
  "paper_id": "XUKXJ6GQ",
  "title": "Federated Learning with Pareto Optimality for Resource Efficiency and Fast Model Convergence in Mobile Environments \u2020",
  "abstract": "Federated learning (FL) is an emerging distributed learning technique through which models can be trained using the data collected by user devices in resource-constrained situations while protecting user privacy. However, FL has three main limitations: First, the parameter server (PS), which aggregates the local models that are trained using local user data, is typically far from users. The large distance may burden the path links between the PS and local nodes, thereby increasing the consumption of the network and computing resources. Second, user device resources are limited, but this aspect is not considered in the training of the local model and transmission of the model parameters. Third, the PS-side links tend to become highly loaded as the number of participating clients increases. The links become congested owing to the large size of model parameters. In this study, we propose a resource-efficient FL scheme. We follow the Pareto optimality concept with the biased client selection to limit client participation, thereby ensuring efficient resource consumption and rapid model convergence. In addition, we propose a hierarchical structure with location-based clustering for device-to-device communication using k-means clustering. Simulation results show that with prate at 0.75, the proposed scheme effectively reduced transmitted and received network traffic by 75.89% and 78.77%, respectively, compared to the FedAvg method. It also achieves faster model convergence compared to other FL mechanisms, such as FedAvg and D2D-FedAvg.",
  "year": 2024,
  "date": "2024-04-12",
  "journal": "IEEE Internet Things J",
  "publication": "IEEE Internet Things J",
  "authors": [
    {
      "forename": "Garzia",
      "surname": "Fabio",
      "name": "Garzia Fabio"
    },
    {
      "forename": "Soodamani",
      "surname": "Ramalingam",
      "name": "Soodamani Ramalingam"
    },
    {
      "forename": "Gianluigi",
      "surname": "Ferrari",
      "name": "Gianluigi Ferrari"
    },
    {
      "forename": "June-Pyo",
      "surname": "Jung",
      "name": "June-Pyo Jung",
      "orcid": "0000-0003-2113-4676"
    },
    {
      "forename": "Young-Bae",
      "surname": "Ko",
      "name": "Young-Bae Ko",
      "orcid": "0000-0002-8799-1761"
    },
    {
      "forename": "Sung-Hwa",
      "surname": "Lim",
      "name": "Sung-Hwa Lim",
      "affiliation": "2  Department of AI Convergence Network , Ajou Univeristy , 206 , World Cup-ro , Suwon-si 16499 , Republic of Korea; \n\t\t\t\t\t\t\t\t Department of AI Convergence Network \n\t\t\t\t\t\t\t\t Ajou Univeristy \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 206 World Cup-ro \n\t\t\t\t\t\t\t\t\t 16499 \n\t\t\t\t\t\t\t\t\t Suwon-si \n\t\t\t\t\t\t\t\t\t Republic of Korea;",
      "orcid": "0000-0002-9783-9050"
    },
    {
      "affiliation": "Department of Multimedia , Namseoul University , 91 , Daehak-ro , Cheonan -si 31020 , Republic of Korea; \n\t\t\t\t\t\t\t\t Department of Multimedia \n\t\t\t\t\t\t\t\t Namseoul University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 91 Daehak-ro -si \n\t\t\t\t\t\t\t\t\t 31020 \n\t\t\t\t\t\t\t\t\t Cheonan \n\t\t\t\t\t\t\t\t\t Republic of Korea;"
    }
  ],
  "doi": "https://doi.org/10.13039/501100003725",
  "arxiv": "arXiv:1610.05492",
  "keywords": [
    "federated learning",
    "Pareto optimality",
    "mobile communication"
  ],
  "sections": [
    {
      "title": "Introduction",
      "text": "The accelerated development of big data has resulted in the increasing application of artificial intelligence (AI) technologies. The International Data Corporation has predicted that the amount of data generated through Internet of things (IoT) devices will reach 79.4 ZB in 2025  [1] , exceeding the capacities of IoT and mobile devices worldwide  [2] . Most of the data generated by a device are processed locally or in a remote cloud server. However, this process involves three main problems pertaining to the constraints associated with real environments and remote cloud servers  [3] :\n\n1.\n\nNetwork Congestion: network congestion on a remote cloud server occurs when numerous user devices simultaneously send data to the server. 2.\n\nPrivacy Leak: private user experience data may be leaked due to malicious network attacks during transmission."
    },
    {
      "title": "3.",
      "text": "Resource Constraints: the capacity of network resources (e.g., wireless channel subcarriers and bandwidth) and user devices (e.g., computing performances and battery life) is limited.\n\ncan effectively reduce the wireless communication traffic generated when the FL model is updated for each client."
    },
    {
      "title": "\u2022",
      "text": "We propose a biased client-selection method for a clustered structure by using Pareto optimality. This client-selection method employs high training loss values to accelerate model convergence and reduce resource consumption."
    },
    {
      "title": "Related Work",
      "text": "FL is an alternative distributed ML method. FL differs from conventional distributed ML in that it involves an extremely large number of clients with heterogeneous and unbalanced local data distributions. A key task of FL is the generation of learning models using the data collected from clients. These data are stored in local devices, which can help prevent privacy leaks and avoid model divergence due to insufficient data and failure to participate owing to a lack of resources (e.g., wireless channel subcarriers, bandwidth, computing performance, and battery life). The convergence of a learning model and resource consumption in FL exhibit a trade-off relationship. Thus, many researchers have attempted to improve the efficiency of FL by simultaneously optimizing its performance aspects.\n\nFederated averaging (FedAvg)  [11] , which is the most conventional FL mechanism, adjusts the batch size and epochs of federated stochastic gradient descent to average the gradient descent generated from a learning process, thereby significantly reducing the overall number of communication rounds by iterating more local updates on a client device. FedAsync  [12]  is an asynchronous FL mechanism for updating global models, in which the mixing weight is adaptively set as a function of staleness. Notably, in  [11, 12] , experiments were conducted on non-iid data, i.e., data that are not independently and identically distributed. However, a theoretical guarantee could not be realized in a convex optimization setting. In  [13] , convergence guarantee for FedAvg was ensured without the impractical assumptions that the data are iid and all clients are available.\n\nA large number of participating clients in FL may lead to server-side congestion and bottlenecks in aggregating client model parameters. Additionally, a large number of participating clients can affect the model convergence for non-iid data  [13] . By appropriately selecting the participating clients, the above-mentioned problems can be solved, and model convergence can be improved. Unlike FedAvg, in which clients are randomly selected, certain researchers  [14]  considered clients with high loss values and proved that biased client selection is directly related to model convergence. The FedCS FL protocol was developed  [15]  for selecting clients within a deadline to manage the resources of heterogeneous clients. This method employed biased client selection; however, it did not ensure the convergence of models for non-iid and heterogeneous data. Furthermore, stragglers may be present in a mobile communication or IoT environment, which cannot participate in FL because the network connection is not persistent or a client device has shut down. The presence of stragglers may hinder model convergence. Therefore, the FLANP FL framework was proposed  [16]  to alleviate the effect of stragglers by adaptively selecting clients in different communication rounds according to their computation speeds.\n\nFL mechanisms with various structures have been proposed. In the hierarchical FL (HFL) mechanism proposed in  [17] [18] [19] , a client and server communicated through an intermediate medium rather than a direct communication structure. In  [19] , the hierarchical edge federated learning (HED-FL) model enhances traditional FL with a multi-layered edge node architecture for energy-efficient learning. Two heuristic methods were also introduced to assess the effects of static and dynamic round execution across these layers. Moreover, a hierarchical cluster-based structure was developed  [17] , which divided clients into several clusters based on resource constraints. A leader node (LN) was elected, which was similar to an intermediate server. Only the LN directly communicated with a PS. Thus, the bottleneck that may have occurred in the PS was eliminated, and the consumption of communication resources was reduced. Similarly, an edge server was deployed between a PS and the clients  [18] . The edge association problem was solved using an evolutionary game between the clients and the edge server. The communication resource allocation problem between the edge server and PS was solved using a Stackelberg differential game.\n\nD2D and peer-to-peer (P2P) communication has been introduced to reduce the communication overhead for the efficient transmission of model parameters. Certain researchers  [20]  examined a social attribute that was used for k-means clustering in D2D communication. A software-defined networking controller was used for clustering by calculating the social attributes between devices, rather than clustering with an unspecified majority. Other researchers proposed algorithms for D2D communication with resource allocation  [21] , which could efficiently manage resources and interference. Zhang et al.  [22]  proposed a D2D-assisted hierarchical FL scheme to reduce the communication overhead in D2D environments. Semi-decentralized federated edge learning (SD-FEEL)  [23]  proposes a structure that aggregates clients' model parameters and exchanges model parameters with neighboring edge servers, followed by broadcasting the updated models. Two timescale hybrid FL (TT-HF)  [24]  extends the FL architecture through aperiodic local and global model consensus procedures based on D2D communications, proposing a new model of gradient diversity and an adaptive control algorithm. In another framework  [25] , clients communicated with one another without a server for aggregating model parameters in FL. Moreover, topology construction was conducted through deep reinforcement learning for P2P FL  [26] .\n\nThe novelty of our framework lies in the following aspects: In  [22, 24, 25] , they propose FL utilizing D2D communication, and  [23]  forms clusters for aggregating clients' model parameters, similar to our work. However, we have introduced the k-means clustering technique for the formation of D2D communication networks. This approach enables the selection of leader clients located in optimal positions without exceeding the communication distance threshold. By collecting and transmitting model parameters within clusters, it offers an ideal solution to alleviate server-side bottleneck issues. In  [27] , Min-Max Pareto optimization was used to manage the trade-off relationship between the algorithmic fairness and performance inconsistency for each client. FedMGDA+  [28] , which is similar to the framework proposed in  [27] , realizes the multi-objective optimization of robustness, fairness, and accuracy through the Pareto stationary solution. In contrast, we consider that the model performance is proportional to the client's resource consumption. Therefore, we solve the target problem by using the Pareto optimality and considering the trade-off relationship between the model convergence and resource consumption. In this manner, the proposed method is different from those described in  [27, 28] : A comparative analysis of FL methods, including FedPO, is encapsulated in Table  1 , which delineates the distinct communication method, hierarchical architecture, and client selection strategies employed by each technique."
    },
    {
      "title": "Table 1. Summary of FL technique characteristics."
    },
    {
      "title": "Method Communication Method Hierarchical Architecture Client Selection",
      "text": "FedAvg  [11]  central FedAsync  [12]  central POWER-OF-CHOICE Strategy  [14]  central \u2713 FedCS  [15]  central \u2713 HFL  [17] [18] [19]  \u2713 D2D-assisted hierarchical FL  [22]  D2D \u2713 SD-FEEL  [23]  Edge Server TT-HF  [24]  D2D P2P FL  [26]  P2P FedPO D2D \u2713 \u2713"
    },
    {
      "title": "Preliminaries"
    },
    {
      "title": "A Brief Overview on Federated Learning",
      "text": "FL performs ML from a federation of clients. The clients train a model using their data and update the training model using gradient descent. After training, the updated model parameters are sent to a PS. The PS updates a global model by aggregating the model parameters trained by each client and calculating the weighted averages according to the number of data samples of each client. The model is defined as the loss function generated by model parameter vector w as f i (x i , y i , w), where i denotes the input data, x i is a feature, and y i is a label. Considering the FL framework with K clients, we define D k as the local data sample for client k. Then, the loss function for each client on the local dataset can be expressed as in Equation (  1 ), where\n\nClient k updates its gradient descent for each local learning during round T, where the learning rate is \u03b7 > 0. The local model parameter in round t is w t k , defined in Equation  (2) .\n\nPS aggregates w t k for certain clients, and the weighted averages used to update the global model are\n\nThe goal is to find a model parameter that can converge for all clients and minimize the loss function:"
    },
    {
      "title": "Pareto Principle and Pareto Optimality",
      "text": "The Pareto principle, named after Italian economist Vilfredo Pareto, posits that 80% of all outcomes result from 20% of the causes. Originally observed in the early 20th century to describe the unequal distribution of wealth in Italy-where 20% of the population owned 80% of the land. In business, it is often used to focus on the most profitable products or the most engaged customers;\n\nIn the context of FL, we leverage this principle to allow a smaller but more crucial subset of clients to make significant contributions to the model's convergence while also alleviating some communication issues. This principle has shown us that not all clients contribute equally, but it does not offer guidance on how to strike a balance between various factors such as computational power, data quality, and contribution to model accuracy.\n\nThat is where the concept of Pareto optimality, which asserts that a society achieves maximum satisfaction when no individual can be made better off without making another individual worse off  [29] , comes into play. In contrast to the Pareto principle, which highlights inequality in contributions, Pareto optimality provides a framework for making trade-offs among competing objectives.\n\nFor instance, focusing solely on clients with high computational power could expedite the model's convergence but at the cost of underrepresenting clients with lower resources, thereby creating a biased model. Pareto optimality seeks to mitigate this by identifying a set of clients that provides the most balanced trade-off between model accuracy and representational fairness."
    },
    {
      "title": "Basic Definition",
      "text": "The concept of Pareto optimality in a multi-objective optimization context refers to a state being considered Pareto optimal if no other feasible states exist that improve at least one objective without worsening any of the other objectives. Mathematically, let A be a set of n-dimensional vectors representing possible states. A vector a \u2208 A is considered Pareto optimal if there does not exist any vector b \u2208 A that dominates a. Formally, the definition is as follows: \u2204b \u2208 A s.t. b i \u2265 a i \u2200i, and b i > a i for at least one i"
    },
    {
      "title": "Pareto Front",
      "text": "The Pareto front comprises all Pareto optimal points in the decision space, serving as an essential reference for decision-makers in multi-objective optimization scenarios. Mathematically, for a set A and objective functions f 1 , f 2 , . . . , f k , a point a belongs to the Pareto front if:\n\nHere, f i and f j are specific objectives among the k different objectives under consideration. This ensures that each point on the Pareto front is not dominated by any other point across all objectives  [30] ."
    },
    {
      "title": "FedPO: Federated Learning with Pareto Optimality 4.1. Problem Formulation",
      "text": "FedPO is an HFL method that uses biased client selection to identify the participating clients by solving for the Pareto optimality using the training loss and resource state of clients.\n\nIn FL, a trade-off relationship exists between the model convergence and resource consumption. More local training or communication rounds for aggregation are required to increase the convergence speed of the model, resulting in the consumption of large amounts of network and computational resources. We use a method  [14]  that selects clients with a high loss value to accelerate model convergence. Specifically, we select the client with the optimal state of model convergence (high loss value) and resources, following Pareto optimality. We assume a two-dimensional Euclidean space R 2 , ordered by a Pareto cone R 2 + , to solve a Pareto optimality point for resources and loss. In addition, we assume that E is a locally convex space, and C E is a convex pointed cone that defines a partial order (\u2265C E ) in E  [29] ."
    },
    {
      "title": "Definition 1.",
      "text": "We define A = {(\u03b1, \u03b2) \u2286 R 2 : \u03b1 \u2265 0, \u03b2 \u2265 0 , where \u03b1 and \u03b2 are the resource state and loss value for a client, respectively}."
    },
    {
      "title": "1.",
      "text": "We assume an elliptic function x 2 a 2 + y 2 b 2 = 1 to illustrate Pareto optimality."
    },
    {
      "title": "2.",
      "text": "Point c \u2208 A is an ideal maximum point of A if (\u03b1, \u03b2) \u2265 C E for every (\u03b1, \u03b2) \u2208 A and closest to the elliptic function.\n\nIn Definition 1, a is the highest resource state of the client in set R = {r 1 , ..., r k }, and the resource states are modeled to have a Gaussian distribution according to  [31] . b is the server's training loss value for the model parameters w t ps , which are aggregated at time t. a and b can be defined as follows:\n\nWe consider the client at point c that satisfies Definition 1 as the optimal client for participating in FL. generated clusters and"
    },
    {
      "title": "FedPO Framework",
      "text": "A centroid is a point located at the center of a cluster when j clusters are generated, and each centroid in M can be represented by C = {c 1 , . . . , c j }. The groups of LCs are expressed as LeaderClients = {lc 1 , . . . , lc j }. In a cluster, the client whose location is the closest to the centroid is selected as an LC, which acts as an intermediate server for HFL as certain clients are selected. Thus, LCs aggregate the updated model parameters of clients and transmit them to the server. The PS updates the global model using the weighted average of the aggregated model parameters, w LC , received from all LCs. The PS calculates Pareto optimality to select the clients that will participate in the next round of training for the LCs. In addition, in accordance with the Pareto principle, the proposed method involves fewer clients compared with those in conventional FL, thereby ensuring model convergence for heterogeneous data and the efficient transmission of model parameters. The details of the proposed method are presented in the following text. . . , distortion j }, where j does not exceed K/2 as the pairing for D2D communication. Additionally, we consider that at least two clients exist in each cluster. Therefore, when the number of clusters is j, the average Euclidean communication distance from lc j to the location of each client k belonging to cluster M j is expressed as follows:\n\nA threshold value is set for the communication distance. The average communication distance from each client in the clusters does not exceed the threshold, and the optimal j is the maximum value.\n\nHFL: Similar to  [17, 22] , we regard LCs as intermediate servers. In a cluster, the client located closest to the centroid is selected as the LC to minimize the distance between the LC and other clients in the cluster. The model parameters of the clients belonging to each cluster are transmitted to the LCs. The LCs aggregate the model parameters in round t and perform weighted averaging, as follows:\n\nThereafter, each LC sends the averaged model parameters to the PS, which aggregates these model parameters. At time t, the global model parameters in the PS are\n\nBiased client selection for Pareto principle and optimality: We use the Pareto principle and optimality to ensure model convergence and to optimize the resource consumption. Figure  2  shows the accuracy of biased and unbiased client-selection methods in FL. On the MNIST and FashionMNIST datasets, biased client selection leads to faster model convergence in the initial stage, and its accuracy is higher than that of unbiased client selection. This result can be interpreted considering the Pareto principle: a small number of clients selected through biased client selection can produce sufficient outcomes. Furthermore, we select clients in accordance with the Pareto optimality function based on two criteria: loss value and resource state. Therefore, according to the convergence analysis in  [14] , the loss value is adopted as the criterion for using Pareto optimality for client selection. The other criterion is the state of client resources because all clients have finite network and computational resources in actual environments."
    },
    {
      "title": "Algorithm",
      "text": "The process flow of FedPO is presented as Algorithm 1. In the case of FedAvg, the model parameters must be directly sent to all clients. In contrast, in the proposed method, the model parameters are sent only to the LCs by adopting HFL. In Algorithm 1, j, M, and w t M j are the number of clusters, set of clusters, and group model parameters of M j in the t-th round, respectively. The LocationBasedClusterting() procedure in line 3 of Algorithm 1 is presented as Algorithm 2. We build clusters for D2D communication according to client locations using k-means clustering. Algorithm 2 is used to identify the optimal number of clusters and adjust the limit on the communication distance to prevent resource wastage in D2D communication. As the proposed structure is HFL (lines 4 and 5 in Algorithm 1), the model parameters of the PS are transmitted to clients through lc j .\n\nIn addition, lc j transmits the model parameters received by the PS to clients at a low cost using D2D communication  [9] . The SelectClient() procedure in line 8 in Algorithm 1 selects participating clients (i.e., SelectedClients) for FL, which consist of subset K presented in Algorithm 3. The PS selects the clients that are closest to the elliptic function through Pareto optimality or whose loss value is larger than that of the PS. The number of participating clients is adjusted by applying prate to each cluster. However, in Algorithm 3, client k must be selected if its loss is larger than that of the global model. At this point, w t ps = w t M 7:\n\nEach client belonging to cluster M j trains w t M j and updates it using its local data 8:\n\nPS performs SelectClient(r t k , f (x i , y i , w t k ), prate)\n\nSelectedClients send w t k to lc j 10:\n\nPS aggregates and updates model\n\nClients are clustered based on their location using k-means clustering 2: Set of cluster M is generated 3:\n\nClient closest to the centroid of cluster M j is selected as lc j Algorithm 3: SelectClient Input: Resource state r t k , f (x i , y i , w t k ), participation fraction prate 1: Clients belonging to M j send loss value f (x i , y i , w t k ) and resource state r t k to lc j 2: PS selects clients following Pareto optimality with Definition 1\n\nlc j receives model parameters from a fraction of clients within |K| * prate based on the high loss value 5: end if Output: SelectedClients"
    },
    {
      "title": "Performance Evaluation"
    },
    {
      "title": "Simulation Settings",
      "text": "Environment. To test the model convergence, we simulate FL using PyTorch and measure the networking overhead using an off-the-shelf network simulation tool named OPNET. We assume a D2D communication network in an area of 2000 \u00d7 2000 m. Additionally, we assume that the base station (i.e., PS) is located at the centroid of the network. According to Algorithm 2, all clients in the cluster are located within a one-hop communication distance that does not exceed the threshold. The feasible communication distance (i.e., threshold) between clients is referred to from  [22, 32] . We select FedAvg  [11] , the most conventional method in FL, which involves aggregating the average of model parameters after multiple local trainings, and D2D-FedAvg  [22] , the system most similar to ours with features including D2D communication and a hierarchical structure, as our comparison targets. Moreover, we use the simulation parameters specified in  [22]  to compare the network overhead with those of FedAvg, D2D-FedAvg, and FedPO as detailed in Table  2 .\n\nFor the experiments, resources are defined to consist of the computational and communication resources of a client. The distribution of client resources is modeled as a Gaussian distribution, as mentioned previously. We assume that one resource unit is consumed considering the distance over which a client communicates with the PS and the basic resources consumed during the communication and computational processes  [11, 12] ."
    },
    {
      "title": "Simulation Results",
      "text": "Resource Efficiency. The proportion of traffic transmitted and received by the serverside of the trained model is shown in Figure  4a ,b, respectively. The number of participating clients is |K| = 100, iterations are performed for T = 1000 rounds, and the traffic values of FedAvg, D2D-FedAvg, and FedPO are compared. Figure  4a  shows that for FedAvg, the transmission traffic is similar for all prate values, because the model parameters are transmitted to all clients in every round. Furthermore, in the case of D2D-FedAvg, as the number of participating clients in FL increases according to prate, residual clients that cannot participate in the grouping process for D2D communication and thus communicate directly with the PS emerge, leading to slightly higher amounts of transmission traffic compared with those of FedPO. This phenomenon occurs because FedPO has a fixed amount of server-side transmission due to the use of location-based k-means clustering for the clients. The same result can be observed in Figure  4b , which represents the amount of traffic received. In the case of FedAvg, the number of clients sending data to the PS increases as prate increases. In contrast, both D2D-FedAvg and FedPO have an intermediate transmitter (e.g., MUE and LC) in the communication process, resulting in less traffic received by the server. This configuration can partially alleviate the potential bottleneck and delay problems, depending on the amount of data received by the PS. The exact ratios corresponding to Figure  4  are listed in Table  3 . In terms of the transmitted traffic from the server, the results in Table  3  show that D2D-FedAvg and FedPO have lower overhead than FedAvg, which transmits the parameters to all clients. D2D-FedAvg shows a slight increase in overhead with increasing participation rate, with values of 23.57%, 28.05%, and 37.99%. In contrast, FedPO maintains similar levels of overhead for all participation rates, with values of 23.73%, 23.67%, and 24.11%. In terms of the received traffic from the server, both D2D-FedAvg and FedPO exhibit similar overhead as that of FedAvg with a participation rate of 0.75. These results indicate the superiority of D2D-FedAvg and FedPO.\n\n(a) (b)  In FL, the clients play a crucial role in the learning process as they consume both computational and communication resources. Model Performance. Figures  6  and  7  show the model accuracies of FedAvg, D2D-FedAvg, and FedPO, where the LR and LSTM learning models are trained using the MNIST dataset with prate = 0.25, 0.50, and 0.75. We set a small number of clients (|K| = 20) to clearly demonstrate that biased client selection is superior to unbiased client selection. Figure  6  shows the results of the first 100 rounds out of 1000 rounds to demonstrate the initial convergence with the LR training model. When prate = 0.25, the accuracy of the initial training differs by that T = 100 by 14%. For the remaining values of prate, similar model accuracy is obtained as the number of participating clients increases. Figure  7  shows the accuracy of the LSTM training model for T = 1000 rounds on the MNIST dataset. Unlike that for the LR, the effectiveness of FedPO is considerably higher than those of FedAvg and D2D-FedAvg when prate = 0.25 and 0.50. The results for FashionMNIST are shown in Figure  8 . Additionally, the results show that FedPO achieves higher accuracy with fewer participating clients, particularly excelling in the case of LSTM models, similar to the results with MNIST. (a) (b) (c) Figure 7. Model accuracy on MNIST for different values of prate with LSTM training model. (a) prate = 0.25. (b) prate = 0.50. (c) prate = 0.75. (a) prate = 0.25 (b) prate = 0.50"
    },
    {
      "title": "Conclusions and Future Work",
      "text": "This paper proposes a new FL scheme named FedPO that uses k-means clustering for D2D communication and utilizes Pareto optimality to select participating clients based on their resource state and loss. The effectiveness of the proposed scheme is experimentally evaluated through experiments in comparison with two methods: FedAvg, the conventional centralized method, and D2D-FedAvg, a modified version for D2D communications.\n\nThus, FedPO is a promising approach for addressing bottlenecks, reducing serverside traffic, and saving client resources. Additionally, this method achieves faster model convergence in the initial rounds compared with the other methods.\n\nIn future work, additional experiments should be performed to evaluate the effect of environmental factors, such as communication instability and disconnection, on the FL performance. Furthermore, although we use Pareto optimality to select clients based on their loss and resource state, a wider range of considerations, such as battery life, connectivity, and computational capabilities of devices in real-world settings, may be considered for client selection. In addition, when selecting the threshold in k-means clustering, factors that may affect model convergence may be considered in addition to communication aspects.\n\nFuture research directions for FedPO implementation can be summarized as follows:\n\n\u2022 Considering the effect of environmental factors on the FL performance: future work can be aimed at examining the effects of factors such as communication instability, network disconnection, and device heterogeneity on the FL performance."
    },
    {
      "title": "\u2022",
      "text": "Optimizing the clustering approach: when selecting the threshold for k-means clustering, other factors affecting the model convergence, such as the data distribution and number of clusters, can be considered.\n\n\u2022 Evaluating the performance of the proposed approach in real-world scenarios: The experiments in this study are conducted in simulated environments. In future work, the performance of the proposed approach can be evaluated in real-world settings to assess its practicality and effectiveness."
    },
    {
      "text": "Figure 1 shows the proposed system model, which consists of clients, LCs, and the PS. The clients are represented by set K = {1, . . . , k}, where k is the number of clients. A client updates model parameter w k using the local data generated through user experience. The set of locations for client K is represented by Location = {l 1 , . . . , l k }. The clusters generated via k-means clustering are expressed as M = {M 1 , . . . , M j }, where j is the number of"
    },
    {
      "text": "Figure 1. System model. 1. K-means clustering for D2D communication: Compared with short-distance wireless communication [9], D2D communication effectively reduces resource consumption and network delay. We use D2D communication for transmitting the model parameters, training loss, and resource state of clients to the LCs for HFL. The PS builds an intranetwork for D2D communication using k-means clustering, based on the locations of clients. distortion j is the average communication distance between intraclients according to the number of clusters j: Distortion = {distortion 1 , .. . , distortion j }, where j does not exceed K/2 as the pairing for D2D communication. Additionally, we consider that at least two clients exist in each cluster. Therefore, when the number of clusters is j, the average Euclidean communication distance from lc j to the location of each client k belonging to cluster M j is expressed as follows:"
    },
    {
      "text": "Figure 2. Example of client selection with model accuracy when different methods are applied in LR learning on (a) MNIST and (b) FashionMNIST in a distributed setting with 20 clients and a participation rate (prate) of 0.25."
    },
    {
      "text": "Federated Learning with Pareto Optimality Input: participation rate prate, random location set of clients location, threshold for the distance between nodes threshold, client set K 1: Initialize model w 0 ps 2: for Communication round t = 1, 2, . . . , T do 3: PS performs LocationBasedClustering(K) 4: PS sends global model w t ps to LC lc j sends w t ps to each client belonging to cluster M j via D2D communication 6:"
    },
    {
      "text": "Parameters for D2D network simulation. Parameter Value Number of clients 100 Max. transmit power of the client, P max |dB 23 dBm Noise power level -174 dBm/Hz Transmit power of the parameter server 43 dBm Maximum distance between LC and clients 200 m threshold Model and Datasets. The learning model is tested using logistic regression (LR) and long short-term memory (LSTM), with the training data derived from MNIST and FashionMNIST datasets."
    },
    {
      "text": "Figure 3 shows the configuration using MNIST data, with different classes (typically, four classes) of clients with local data introduced to reflect the non-iid situation in the experiments. Disproportionate amounts of data are assumed to be held by clients, characteristic of non-iid situations."
    },
    {
      "text": "Figure 3. Class per 20 clients and unbalances in MNIST."
    },
    {
      "text": "Figure 4. Server-side traffic for different values of prates. (a) Server-side transmitted traffic for different values of prates. (b) Server-side received traffic for different values of prates."
    },
    {
      "text": "Computational resources are consumed during model training using local data, whereas communication resources are used for model parameter communication. The amounts of resources consumed and remaining for each client after each round represent important criteria for client participation and selection in the next round. To analyze the effect of different FL methods on the resource states of the clients, Figure 5 shows the sum of the remaining resources of clients in each round. We compare FedPO with FedAvg as both D2D-FedAvg and FedAvg have similar algorithms and resource consumption levels. FedPO outperforms FedAvg in terms of the resource conservation of clients. This phenomenon occurs owing to the Pareto optimality-based client selection method used in FedPO, which takes into account the client's available resources and loss during training. Consequently, FedPO selects those clients who will participate in FL without compromising their resources and model convergence and increases the remaining resources of the clients."
    },
    {
      "text": "Figure 5. Sum of remaining resources of clients at each round."
    },
    {
      "text": "Figure 6. Model accuracy on MNIST for different values of prate with LR training model. (a) prate = 0.25. (b) prate = 0.50. (c) prate = 0.75."
    },
    {
      "text": "Server-side traffic ratios of D2D-FedAvg, FedPO, and FedAvg."
    }
  ],
  "references": [
    {
      "title": "The world in 2025-predictions for the next ten years",
      "authors": [
        "R Taylor",
        "D Baron",
        "D Schmidt"
      ],
      "year": 2015,
      "doi": "10.1109/impact.2015.7365193"
    },
    {
      "title": "Fog and IoT: An overview of research opportunities",
      "authors": [
        "M Chiang",
        "T Zhang"
      ],
      "year": 2016,
      "doi": "10.1109/jiot.2016.2584538"
    },
    {
      "title": "Privacy and big data",
      "authors": [
        "B Gaff",
        "H Sussman",
        "J Geetter"
      ],
      "year": 2014,
      "doi": "10.1109/mc.2014.161"
    },
    {
      "title": "When edge meets learning: Adaptive control for resource-constrained distributed machine learning",
      "authors": [
        "S Wang",
        "T Tuor",
        "T Salonidis",
        "K Leung",
        "C Makaya",
        "T He",
        "K Chan"
      ],
      "year": 2018,
      "doi": "10.1109/infocom.2018.8486403"
    },
    {
      "title": "Federated learning: Strategies for improving communication efficiency",
      "authors": [
        "J Kone\u010dn \u1ef2",
        "H Mcmahan",
        "F Yu",
        "P Richt\u00e1rik",
        "A Suresh",
        "D Bacon"
      ],
      "doi": "10.1063/pt.5.029931"
    },
    {
      "title": "Combining Federated Learning and Edge Computing Toward Ubiquitous Intelligence in 6G Network: Challenges, Recent Advances, and Future Directions",
      "authors": [
        "Q Duan",
        "J Huang",
        "S Hu",
        "R Deng",
        "Z Lu",
        "S Yu"
      ],
      "year": 2023,
      "doi": "10.1109/comst.2023.3316615"
    },
    {
      "title": "Towards federated learning at scale: System design",
      "authors": [
        "K Bonawitz",
        "H Eichner",
        "W Grieskamp",
        "D Huba",
        "A Ingerman",
        "V Ivanov",
        "C Kiddon",
        "J Kone\u010dn \u1ef2",
        "S Mazzocchi",
        "B Mcmahan"
      ],
      "year": 2019,
      "doi": "10.1109/ieeeconf44664.2019.9049066"
    },
    {
      "title": "Project adam: Building an efficient and scalable deep learning training system",
      "authors": [
        "T Chilimbi",
        "Y Suzue",
        "J Apacible",
        "K Kalyanaraman"
      ],
      "year": 2014,
      "doi": "10.1109/msp.2010.134"
    },
    {
      "title": "A survey of device-to-device communications: Research issues and challenges",
      "authors": [
        "F Jameel",
        "Z Hamid",
        "F Jabeen",
        "S Zeadally",
        "M Javed"
      ],
      "year": 2018,
      "doi": "10.1109/comst.2018.2828120"
    },
    {
      "title": "Resource Efficient Cluster-Based Federated Learning for D2D Communications",
      "authors": [
        "J Jung",
        "Y Ko",
        "S Lim"
      ],
      "year": 2022,
      "doi": "10.1109/vtc2022-spring54318.2022.9860657"
    },
    {
      "title": "Communication-efficient learning of deep networks from decentralized data",
      "authors": [
        "B Mcmahan",
        "E Moore",
        "D Ramage",
        "S Hampson",
        "B Arcas"
      ],
      "year": 2017
    },
    {
      "title": "Asynchronous Federated Optimization",
      "authors": [
        "C Xie",
        "S Koyejo",
        "I Gupta"
      ]
    },
    {
      "title": "On the convergence of fedavg on non-iid data",
      "authors": [
        "X Li",
        "K Huang",
        "W Yang",
        "S Wang",
        "Z Zhang"
      ],
      "doi": "10.1090/mbk/121/79"
    },
    {
      "title": "Client selection in federated learning: Convergence analysis and power-of-choice selection strategies",
      "authors": [
        "Y Cho",
        "J Wang",
        "G Joshi"
      ]
    },
    {
      "title": "Client selection for federated learning with heterogeneous resources in mobile edge",
      "authors": [
        "T Nishio",
        "R Yonetani"
      ],
      "year": 2019,
      "doi": "10.1109/icc.2019.8761315"
    },
    {
      "title": "Straggler-resilient federated learning: Leveraging the interplay between statistical accuracy and system heterogeneity",
      "authors": [
        "A Reisizadeh",
        "I Tziotis",
        "H Hassani",
        "A Mokhtari",
        "R Pedarsani"
      ],
      "year": 2022,
      "doi": "10.1109/jsait.2022.3205475"
    },
    {
      "title": "Resource-efficient federated learning with hierarchical aggregation in edge computing",
      "authors": [
        "Z Wang",
        "H Xu",
        "J Liu",
        "H Huang",
        "C Qiao",
        "Y Zhao"
      ],
      "year": 2021,
      "doi": "10.1109/infocom42981.2021.9488756"
    },
    {
      "title": "Dynamic edge association and resource allocation in self-organizing hierarchical federated learning networks",
      "authors": [
        "W Lim",
        "J Ng",
        "Z Xiong",
        "D Niyato",
        "C Miao",
        "D Kim"
      ],
      "year": 2021,
      "doi": "10.1109/jsac.2021.3118401"
    },
    {
      "title": "A hierarchical, energy efficient, and dynamic approach for edge Federated Learning",
      "authors": [
        "F De Rango",
        "A Guerrieri",
        "P Raimondo",
        "G Spezzano",
        "Hed-Fl"
      ],
      "year": 2023,
      "doi": "10.1016/j.pmcj.2023.101804"
    },
    {
      "title": "A social-aware K means clustering algorithm for D2D multicast communication under SDN architecture",
      "authors": [
        "W Gong",
        "L Pang",
        "J Wang",
        "M Xia",
        "Y Zhang"
      ],
      "year": 2021,
      "doi": "10.1016/j.aeue.2021.153610"
    },
    {
      "title": "Resource allocation based on clustering for D2D communications in underlaying cellular networks",
      "authors": [
        "D Son",
        "C Yu",
        "D Kim"
      ],
      "year": 2014
    },
    {
      "title": "D2D-assisted federated learning in mobile edge computing networks",
      "authors": [
        "X Zhang",
        "Y Liu",
        "J Liu",
        "A Argyriou",
        "Y Han"
      ],
      "year": 2021,
      "doi": "10.1109/wcnc49053.2021.9417459"
    },
    {
      "title": "Semi-Decentralized Federated Edge Learning for Fast Convergence on Non-IID Data",
      "authors": [
        "Y Sun",
        "J Shao",
        "Y Mao",
        "J Wang",
        "J Zhang"
      ],
      "year": 2022,
      "doi": "10.1109/wcnc51071.2022.9771904"
    },
    {
      "title": "Semi-Decentralized Federated Learning With Cooperative D2D Local Model Aggregations",
      "authors": [
        "F Lin",
        "S Hosseinalipour",
        "S Azam",
        "C Brinton",
        "N Michelusi"
      ],
      "year": 2021,
      "doi": "10.1109/jsac.2021.3118344"
    },
    {
      "title": "Decentralized federated learning via SGD over wireless D2D networks",
      "authors": [
        "H Xing",
        "O Simeone",
        "S Bi"
      ],
      "year": 2020,
      "doi": "10.1109/spawc48557.2020.9154332"
    },
    {
      "title": "Learning-driven decentralized machine learning in resource-constrained wireless edge computing",
      "authors": [
        "Z Meng",
        "H Xu",
        "M Chen",
        "Y Xu",
        "Y Zhao",
        "C Qiao"
      ],
      "year": 2021,
      "doi": "10.1109/infocom42981.2021.9488817"
    },
    {
      "title": "Addressing algorithmic disparity and performance inconsistency in federated learning",
      "authors": [
        "S Cui",
        "W Pan",
        "J Liang",
        "C Zhang",
        "F Wang"
      ],
      "year": 2021
    },
    {
      "title": "Federated learning meets multi-objective optimization",
      "authors": [
        "Z Hu",
        "K Shaloudegi",
        "G Zhang",
        "Y Yu"
      ],
      "year": 2022,
      "doi": "10.1109/tnse.2022.3169117"
    },
    {
      "title": "Pareto optimality",
      "authors": [
        "A Chinchuluun",
        "A Karakitsiou",
        "A Mavrommati"
      ],
      "year": 2008
    },
    {
      "title": "Evolutionary computation and convergence to a pareto front",
      "authors": [
        "D Van Veldhuizen",
        "G Lamont"
      ],
      "year": 1998,
      "doi": "10.1007/bfb0027303"
    },
    {
      "title": "Adaptive federated learning in resource constrained edge computing systems",
      "authors": [
        "S Wang",
        "T Tuor",
        "T Salonidis",
        "K Leung",
        "C Makaya",
        "T He",
        "K Chan"
      ],
      "year": 2019,
      "doi": "10.1109/jsac.2019.2904348"
    },
    {
      "title": "Feasible D2D communication distance in D2D-enabled cellular networks",
      "authors": [
        "H Ding",
        "S Ma",
        "C Xing"
      ],
      "year": 2014,
      "doi": "10.1109/iccs.2014.7024754"
    },
    {
      "title": "The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of",
      "doi": "10.1016/j.mehy.2008.11.003"
    }
  ],
  "num_references": 33
}
