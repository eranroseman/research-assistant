{
  "paper_id": "EIN6VP94",
  "title": "The FAIIR conversational AI agent assistant for youth mental health service provision Check for updates",
  "year": 2021,
  "date": "2021",
  "journal": "Epidemiol. Psychiatr. Sci",
  "publication": "Epidemiol. Psychiatr. Sci",
  "authors": [
    {
      "forename": "Stephen",
      "surname": "Obadinma",
      "name": "Stephen Obadinma",
      "affiliation": "1  Electrical and Computer Engineering , Queen's University , 99 University Ave , Kingston , ON , Canada. \n\t\t\t\t\t\t\t\t Electrical and Computer Engineering \n\t\t\t\t\t\t\t\t Queen's University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 99 University Ave \n\t\t\t\t\t\t\t\t\t Kingston \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Alia",
      "surname": "Lachana",
      "name": "Alia Lachana",
      "affiliation": "3  Humber College , 205 Humber College Boulevard , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Humber College \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 205 Humber College Boulevard \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Maia",
      "surname": "Leigh",
      "name": "Maia Leigh",
      "affiliation": "2  Vector Institute , W1140-108 College Street , Schwartz Reisman Innovation Campus , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Vector Institute \n\t\t\t\t\t\t\t\t Schwartz Reisman Innovation Campus \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t College Street \n\t\t\t\t\t\t\t\t\t W1140-108 \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada",
      "orcid": "0000-0002-7108-7233"
    },
    {
      "forename": "Jocelyn",
      "surname": "Rankin",
      "name": "Jocelyn Rankin",
      "affiliation": "5  Kids Help Phone , 439 University Avenue , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Kids Help Phone \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 439 University Avenue \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Joanna",
      "surname": "Yu",
      "name": "Joanna Yu",
      "affiliation": "2  Vector Institute , W1140-108 College Street , Schwartz Reisman Innovation Campus , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Vector Institute \n\t\t\t\t\t\t\t\t Schwartz Reisman Innovation Campus \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t College Street \n\t\t\t\t\t\t\t\t\t W1140-108 \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Xiaodan",
      "surname": "Zhu",
      "name": "Xiaodan Zhu",
      "affiliation": "1  Electrical and Computer Engineering , Queen's University , 99 University Ave , Kingston , ON , Canada. \n\t\t\t\t\t\t\t\t Electrical and Computer Engineering \n\t\t\t\t\t\t\t\t Queen's University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 99 University Ave \n\t\t\t\t\t\t\t\t\t Kingston \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Darren",
      "surname": "Mastropaolo",
      "name": "Darren Mastropaolo",
      "affiliation": "5  Kids Help Phone , 439 University Avenue , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Kids Help Phone \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 439 University Avenue \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Deval",
      "surname": "Pandya",
      "name": "Deval Pandya",
      "affiliation": "2  Vector Institute , W1140-108 College Street , Schwartz Reisman Innovation Campus , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Vector Institute \n\t\t\t\t\t\t\t\t Schwartz Reisman Innovation Campus \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t College Street \n\t\t\t\t\t\t\t\t\t W1140-108 \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Roxana",
      "surname": "Sultan",
      "name": "Roxana Sultan",
      "affiliation": "2  Vector Institute , W1140-108 College Street , Schwartz Reisman Innovation Campus , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Vector Institute \n\t\t\t\t\t\t\t\t Schwartz Reisman Innovation Campus \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t College Street \n\t\t\t\t\t\t\t\t\t W1140-108 \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Elham",
      "surname": "Dolatabadi",
      "name": "Elham Dolatabadi",
      "affiliation": "2  Vector Institute , W1140-108 College Street , Schwartz Reisman Innovation Campus , Toronto , ON , Canada. \n\t\t\t\t\t\t\t\t Vector Institute \n\t\t\t\t\t\t\t\t Schwartz Reisman Innovation Campus \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t College Street \n\t\t\t\t\t\t\t\t\t W1140-108 \n\t\t\t\t\t\t\t\t\t Toronto \n\t\t\t\t\t\t\t\t\t ON \n\t\t\t\t\t\t\t\t\t Canada"
    },
    {
      "forename": "Maia",
      "surname": "Norman",
      "name": "Maia Norman"
    }
  ],
  "doi": "https://doi.org/10.13039/501100000024",
  "sections": [
    {
      "text": "Frontline crisis support plays a critical role in youth mental health services, where Crisis Responders (CRs) engage in conversations and assign issue tags to guide interventions. To enhance this process, we introduce FAIIR (Frontline Assistant: Issue Identification and Recommendation), an ensemble of domain-adapted transformer models trained on 780,000 conversations. FAIIR aims to reduce CR's cognitive burden, enhance issue identification accuracy, and streamline post-conversation administrative tasks. Evaluated on retrospective data, FAIIR achieves an average AUC ROC of 94%, an average F1-score of 64%, and an average recall score of 81%. During the silent testing phase, its performance remained robust, with less than a 2% drop in all metrics. CRs exhibited 90.9% agreement with its predictions, and expert agreement with FAIIR exceeded their agreement with original labels. These findings highlight FAIIR's potential to assist CRs in prioritizing urgent cases and ensuring appropriate resource allocation in crisis interventions.\n\nGlobally, one in seven young individuals aged 10-19 years old experience a mental health condition, making a significant contribution to the global burden of disability and disease  1  . Suicide ranks as the fourth leading cause of death among 15-29 year olds, and in Canada, one in five individuals will experience a mental illness by age 25  1, 2  . Despite 70% of mental illness starting during childhood or adolescence, only a fraction of young individuals are able to access appropriate care, leading to increasing rates of youth hospitalizations for mental health disorders  [2] [3] [4]  .\n\nCrisis support conversations are a critical component of mental health services, providing immediate, accessible, and empathetic support to youth in distress  5, 6  . This approach offers significant benefits, including early intervention, reduced healthcare burdens, and the potential to de-escalate life-threatening situations. However, several challenges persist, including the high demand for these services which is particularly evident in the volume of text conversations received by Kids Help Phone (KHP), a Canadian not-for-profit e-mental health organization. Since the launch of its text service in 2018, KHP has facilitated over 1 million Short Message Service (SMS) interactions, with a significant 51% increase observed during the COVID-19 pandemic in 2020. This heightened demand underscores the necessity of expanding the team of Crisis Responders (CRs), composed of both professionals and trained volunteers. The complexity of the issues discussed by youth during these conversations and the cognitive burden on CRs managing emotionally stressed individuals in potentially life-critical situations  7  add further challenges to crisis support services. Processing nuances in youth natural language from diverse populations and intersectionalities is not straightforward. Moreover, CRs must complete postconversation surveys to identify key issues such as suicide and abuse, further adding to their workload and time commitment.\n\nIn this study, we focus on building an efficient and scalable support mechanism leveraging Natural Language Processing (NLP) approaches to assist CRs. We utilize one of the largest crisis support conversational datasets, comprising 780,000 interactions by KHP. Our aim is to reduce CR's cognitive burden, improve the accuracy of issue identification, and streamline post-conversation administrative tasks. Dramatic recent improvements in the performance and availability of state-of-the-art large language models (LLMs)  [8] [9] [10]  have accelerated development and application of NLP tools to augment health services, alongside human experts  [11] [12] [13] [14]  . In the mental health context, NLP tools have been developed to identify signs of depression  [15] [16] [17]  and suicidal intentions  18  within social media posts. In addition, they can identify themes related to suicide  19  and mental health status  20  from clinical notes. These tools are also utilized to aid in triage and reduce wait times on message-based suicide support platforms  21  . These realworld applications highlight NLP's potential in augmenting mental health support across both clinical and non-clinical communication channels.\n\nTo this end, we developed a human-centric classification and information retrieval model called \"Frontline Assistant: Issue Identification and Recommendation\" (FAIIR). The FAIIR tool inherits intent identification outlined in the Neural Agent Assistant framework, designed to improve AIenabled conversational tasks  22  , as well as state-of-the-art transformer-based models built for long conversations  23  . The FAIIR development process first included optimizing and fine-tuning a suite of transformer models that classify conversations into a predefined set of 19 clinically-orientated care paradigms, referred to as issue tags, such as suicidality and abuse (refer to tags defined in Supplementary Table  1 ). Following this, the process encompassed ensemble techniques and human-in-the-loop validation with active engagement of CRs. CRs' involvement was crucial for validation, consensus-building, refining performance, and ensuring the tool's utility. To uphold responsible AI development standards, we conducted thorough evaluations for bias and fairness. Finally, we presented the outcomes of our initial silent trial, detailed our implementation plans, and explored additional potential use cases for the FAIIR tool."
    },
    {
      "title": "Results"
    },
    {
      "title": "Crisis support conversational dataset",
      "text": "To develop FAIIR, we leveraged 703,975 anonymized text-based crisis support conversations exchanged between service users and CRs from January 2018 to February 2023. Each conversation consists of multiple message exchanges (multi-turn dialogs), making it essential for the model to process long-form text effectively. Identifiable information was scrubbed to ensure privacy compliance. Conversations vary significantly in length in terms of tokens. Tokens in this context refer to discrete units of text that typically represent words, sub-words, characters, or punctuation obtained after breaking a sequence of text down so that it can be processed by a model. The average and median number of tokens per conversation were 913 and 850 respectively (Fig.  1f ) with the majority of conversations (53%) between 500 and 1500 tokens, and only a small number extending above 3000 (0.7%). Given this distribution, we set a 2000-token maximum input length, covering 94.4% of all conversations (Fig.  1f ). An optional demographic survey was completed by 17% of service users (59,603 survey respondents of a total of n = 340,512 overall service users), with responses originating mostly from individuals identifying as female (75%), heterosexual (55%), and of European ancestry (77.5%) as shown in Fig.  1b-e . It is essential to note that only a small number of overall service users completed the survey, and thus the results do not fully represent the distribution or demographic features of service users overall."
    },
    {
      "title": "FAIIR issue tag prediction",
      "text": "FAIIR is a multi-label classification model that classifies conversations into 19 predefined issue categories, with it being possible for multiple issue categories applying for a given conversation. However, the distribution of issue tags in the data is highly imbalanced (Fig.  1a ), with the most frequent tag, Anxiety/Stress, appearing in over 244,000 conversations, and the least frequent, Prank, appearing in only 2800 conversations. Additionally, issue tagging varies across conversations: 53.73% of conversations are assigned a single issue tag, while 46% contain between 2 and 9 issue tags, reflecting the complexity of the challenges faced by service users (Fig.  1g ).\n\nRegarding risk levels, the majority of conversations were classified as high-risk (13%) or medium-risk (87%) (Fig.  1h ), based on the methodology detailed in the \"Methods\" section: \"Priority flag pre-processing\"."
    },
    {
      "title": "Ensemble of Longformer models excelled in FAIIR issue tag predictions",
      "text": "We first compared four transformer-based LLMs, each fine-tuned on a dataset containing 50,000 randomly sampled conversations (see \"Methods\" section: \"Process of selecting model architecture\"). Transformer-based LLMs  24  are advanced deep learning models designed to analyze large volumes of text, such as patient-provider conversations, and identify patterns or extract insights. Among the models tested, Longformer demonstrated the best performance, excelling accuracy and ability to effectively handle long conversations without losing contextual meaning. Given Longformer's superior performance, the backend of the FAIIR tool was built using an ensemble of three Longformer 9 models which involves combining the predictions of separate models with slightly varied initialization and fine-tuning processes, aiming to enhance overall accuracy and reliability. These models were adapted to the mental health support domain through the technique of masked language modeling, a training process which aids in model understanding of specific language, terminology, and the context of words by leveraging the entire dataset to get the model to learn to predict masked tokens.\n\nAfter domain adaptation, the models were further fine-tuned for the multi-label classification task. This task involved training the models to identify multiple relevant issues at hand (see more details about predefined issue tags in Supplementary Table  1 ). The fine-tuning was conducted using a larger dataset of 563,180 conversations-422,385 used for training, and 140,795 for validation. The models were then evaluated retrospectively on an independent test set of 140,795 conversations to confirm their performance in a real-world context.\n\nThe performance of the ensemble model on the test set, broken down by individual issue tags, is detailed in Table  1 , while the average performance across all issue tags is shown in Fig.  2a . The majority of the area under the receiver operating characteristic curve (AUC ROC) scores exceeded 0.9, indicating strong overall model performance, with the lowest score being 0.74.\n\nDue to the imbalanced nature of the data, whereby certain issue tags are under-or over-represented, precision (proportion of correct positive predictions) is relatively low (<0.65). However, recall (proportion of true positives identified correctly) remains high (<0.9), alongside overall accuracy (0.94). This means that the model is highly effective at identifying relevant tags, but may occasionally also identify irrelevant issues in conversation, especially for less common topics.\n\nClassification thresholds define the level of confidence a model must have before assigning an issue tag. The model generates 19 confidence scores-one for each issue label-based on the probability that a label applies to a given conversation. A threshold is then used to set the minimum confidence score required to definitively assign a label. When selecting the optimal threshold for our context, a threshold of 0.25 was found to strike the best balance between precision and recall, thereby allowing the model to capture critical issues, even if irrelevant tags are sometimes included. At this threshold, the sample average F1-score (a combined measure of precision and recall) was 0.64, with a sample average recall of 0.81 and a sample average precision of 0.58. The trade-off between recall and precision is acceptable within this context, as it prioritizes capturing critical issues, even if some irrelevant issue tags are also included.\n\nThe model performed exceptionally well in identifying high-priority issues such as Suicide (F1-score = 0.73), Depression (F1-score = 0.75), Relationship Problems (F1-score = 0.73), Self Harm (F1-score = 0.69), Anxiety/Stress (F1-score = 0.69), and Third-Party conversations (F1score = 0.76). However, model performance was relatively poorer for rarer issues, including Other (F1-score = 0.35), Abuse, Emotional (F1-score = 0.46), Abuse, Physical (F1-score = 0.47), Isolated (F1-score = 0.56), Prank (F1-score = 0.45), and Testing (F1-score = 0.53). These results suggest that while the model is effective at identifying common or critical issues, additional refinement may be required to improve performance across less frequent categories. FAIIR tool predictions observed to be unbiased across demographic subgroups Table  2  demonstrates the performance (precision, recall, F1-score, and accuracy) of the FAIIR tool in predicting issue tags within 27 distinct subgroups across four demographic categories, representing 17% of overall service users (see distribution in Fig.  1b-e ). The standard deviation of F1scores within each demographic category is less than 0.025 (for Gender: \u00b10.023, Orientation: \u00b10.010, Identity: \u00b10.018, and Ethnicity: \u00b10.024), indicating a consistent model performance with a narrow gap across varying demographic subgroups. The results of a one-sample t-test (p value <0.001) show that there is no significant difference between the F1-scores of individual demographic subgroups and the overall performance of the model. This indicates that the model performs consistently across different demographic groups, and any differences in scores are unlikely due to sampling bias."
    },
    {
      "title": "Expert assessment and evaluation of FAIIR predictions",
      "text": "In total, we solicited 240 annotations from 12 experts, with each conversation undergoing independent assessment by three distinct assessors for open review and another three for blind review. The two types of review differed in the presentation of the FAIIR predictions: in the open review (Supplementary Table  3 ), assessors were provided with FAIIR's predicted issue tags as a reference, whereas in the blind review, assessors identified issue tags independently without any exposure to FAIIR's predictions."
    },
    {
      "title": "FAIIR's predictions validated by human experts",
      "text": "For results of the blind assessment demonstrated in Fig.  3 , on average across 40 conversations, 90.9% agreement (lowest: 33% and highest 100%) was achieved overall between CRs and the FAIIR tool, where FAIIR was able to predict 165 issue tags and by majority agreement it only missed 13 tags. Among all issue tags, agreement was reached in the majority of instances of Anxiety/Stress, Bully, Relationship, 3rd Party, Suicide, and Abuse, Emotional; with more frequent discordance for the issue tags of Grief, Self Harm, Abuse, Physical; Other, and Eating Body Image. https://doi.org/10.1038/s41746-025-01647-6 FAIIR understands the conversational context Figure  4  presents a comparison of consensus among the experts' blind responses, FAIIR tool prediction(s), and original issue tag(s) (referring to the issue tag(s) recorded in the training dataset, assigned following the original conversation). For all consensus comparison settings (Fig.  4 ), the level of agreement between the FAIIR tool and expert responses (average precision 0.62 \u00b1 0.22, average recall 0.82 \u00b1 0.13, and average F1-score 0.64 \u00b1 0.11; respectively) are higher than original annotations and expert responses (average precision 0.52 \u00b1 0.18, average recall 0.56 \u00b1 0.08, and average F1score 0.47 \u00b1 0.07; respectively). Thus, when using expert responses as a reference for comparison, FAIIR predictions align more closely with the experts' annotations than the original issue tags. The results of the unpaired t-test on the averaged measures reveal that the concordance between FAIIR and expert responses is significantly higher (p value <0.001) compared to the concordance between the original annotations and expert responses. Figure  4  also demonstrates FAIIR's final performance after refinement based on incorporating the experts' blind responses and adjusting the classification decision boundary to be less biased towards predicting the most common tags. Comparing the original threshold \"FAIIR vs. Experts\" and the updated threshold (UT) \"FAIIR (UT) vs. Experts\" in Fig.  4 , average precision (0.66 \u00b1 0.2) is improved while average recall (0.76 \u00b1 0.14) is decreased. F1score also improves for the full agreement on primary issue tags (0.53) and partial agreement via majority vote (0.6) settings, demonstrating benefits in the strictest consensus measures."
    },
    {
      "title": "FAIIR performance in silent trial consistent with development phase",
      "text": "Conversational data for the silent testing phase comprised 84,832 conversations collected between February and September 2023. The distribution of issue tags can be seen in Supplementary Fig.  2 , demonstrating that the silent testing dataset remains largely consistent with the data used for developing the FAIIR tool. However, certain tags like DNE are more common in the former dataset. In addition, this data naturally includes more up-to-date topics and events of 2023 such as natural disasters and political crises. Table  1  presents the evaluation performance of FAIIR predictions for each issue tag and Fig.  2b  indicates averaged metrics across all issue tags for three thresholds: 0.25, 0.5, and the adjusted threshold based on expert evaluation and assessment. An expected slight decrease in performance is noted and is attributable to the differences in distributions between the development and silent testing datasets. Similar to the retrospective testing setting, the majority of the AUC ROC scores are above 0.9, with the lowest score being 0.73. The sample averaged precision, recall, and F1 scores are 0.57, 0.79, and 0.62, respectively, for a threshold of 0.25, compared to 0.58, 0.81, and 0.64 for the retrospective values, indicating a drop of less than 2%."
    },
    {
      "title": "Discussion",
      "text": "We have successfully demonstrated the viability of employing an NLPbased frontline assistant tool to augment CRs by identifying the issues that service users may be experiencing in text-based support conversations. After analyzing a textual conversation, the FAIIR tool is able to recommend potential issues from a list of 19 predefined tags. FAIIR achieved an accuracy of 94%, a sample average F1-score of 64%, and a sample average recall score of 81% on the development set. This is a strong performance given the inherent subjectivity and noisiness of the data. Importantly, in the context of our fairness analyses, the FAIIR tool demonstrated equitable performance across all demographic groups of service users.\n\nOur study demonstrates the robustness and generalizability of the FAIIR tool, which is built upon recent advances in LLMs. In our silent testing phase, we observed less than a 2% drop in sample average precision, recall and F1-score, demonstrating the tool's strong potential for real-time deployment.\n\nOur investigation revealed that domain adaptation through selfsupervised learning significantly enhances tool performance, especially in supervised tasks and when addressing label biases. This finding is relevant to our study, where we noted potential biases in the original labeling process, as each conversation was labeled by a single annotator without subsequent review. Consequently, a large annotated dataset for a supervised task with multiple annotators per conversation would be optimal. However, given the scale of our study with over 780,000 individual conversations, extensive manual annotation was not feasible. We sought expert assessment and evaluation for edge-case conversations to enrich our ground truth annotations and explore the benefits of contextual learning. The experts' responses showed an overall agreement of 90.9% with the FAIIR tool's predictions. Notably, expert agreement with FAIIR exceeded their agreement with the original labels. This observation can be attributed to the extensive self-supervised training of FAIIR, which equipped the model with a strong contextual understanding. This training allowed the model to grasp comparing thresholds of 0.5 (orange) and 0.25 (blue) as well as overall AUC ROC model performance (gray). b Averaged performance of the FAIIR tool across all issue tags in the silent testing prospective test sets (n = 84,932), evaluated using previous two classification thresholds with the addition of an updated threshold (green). For silent testing, results for the silent testing overlay retrospective results, with decreases in performance highlighted in red and gains shown in a lighter shade. The AUC ROC bar represents the average ability of the tool to distinguish between issue tags across all categories. The tool's best overall performance is an F1-score of 0.64 on the retrospective test set and 0.62 on the silent testing prospective test set.\n\nhttps://doi.org/10.1038/s41746-025-01647-6  the nuances and relationships between words and phrases within the context. The expert evaluation not only demonstrated the strong performance of the model in context but also provided a valuable source of ground-truth issue tags for further model refinement. After refining the FAIIR tool by leveraging the blind survey outcomes, the precision metric exhibited a 4% increase.\n\nThe distribution of issue tags in our dataset was highly imbalanced, with the most frequent tag, Anxiety/Stress, appearing in more than 244,000 conversations, while the least frequent tag, Prank, appears in only 2800 conversations. This imbalance posed significant challenges during our model's training, which explains the relatively poorer performance of FAIIR across less-represented issue tags. For instance, the model encountered difficulties with applying tags such as Other, Abuse, Emotional; Abuse, Physical; Isolated, Prank, and Testing. Some of these challenges also stem from the inherent vagueness of certain tags. For example, Isolated can apply to a broad spectrum of conversations, but its relevance may be selectively applied. The Other tag presents similar difficulties, encompassing anything outside the scope of the set 19 issue tag categories. Tags like Prank and Abuse, Physical also suffered in performance due to their rarity in the dataset, making it challenging for the model to adequately learn to recognize these instances. While we implemented imbalanced learning techniques such as re-weighting and balanced samples, further techniques may be necessary to enhance performance. However, despite these challenges, the tool demonstrated exceptional performance, above that of random guessing.\n\nAnother challenge we encountered was the diversity and extensive length across conversations in our dataset. This was largely due to the therapeutic nature of the discussions, which primarily focused on mental health support and recovery. To manage this diversity, we established a maximum token length of 2000, covering approximately 95% of all conversations. This threshold not only accommodated the majority of our conversations, but also optimized the batch size for more efficient training, thereby accelerating the training process. The decision to use a Longformer model was primarily influenced by its well-suitedness for our context: its design for longer sequences and our need to handle lengthy sequences effectively made this model the correct choice.\n\nAn additional obstacle we faced was the varying quality of conversations, including differences in language use, grammar, and the presence of noise such as typos or slang. Addressing these issues required extensive pre- processing efforts, which introduced subjectivity and potential bias into the data.\n\nIn conjunction with the identification of primary issue tags, we established an explainability pipeline to facilitate the extraction of contextual keywords, referred to as \"natural keywords\" from each conversation. These keywords are dynamic tokens associated with the specific main issue tags in a conversation. To streamline the processing of natural keywords and facilitate communication with subject matter experts, we conducted word embedding visualization and bi-gram analysis at the aggregated level to demonstrate semantic relationships and word proximity in a specific context (see \"Methods\" section: \"Outcome interpretation and visualization\"). However, it is crucial to note that the reliability and meaningfulness of these natural keywords require further rigorous assessment, of which our future work will be comprised.\n\nOur study's primary limitation is its reliance on a predefined set of 19 issue tags for the identification of topics within a conversation. This limitation restricts the model's ability to extract information beyond the predefined list. In conversations of this nature, CRs have identified their eagerness to delve into dynamic and natural issue tags that are more youthcentered, as opposed to being limited by this predefined list. Such a dynamic model could permit for change over time or based on the context of the conversation, allowing for the tool to adapt to new issues it may not have encountered during training.\n\nOur immediate next step involves deploying the FAIIR tool for realtime issue tag identification. Simultaneously, we aim to enhance the pipeline by incorporating generative language models and decoder-based models to further improve dynamic issue tag predictions. Additionally, we hope to evaluate the usability and validity of the identified natural keywords via a panel of subject matter experts.\n\nGiven its positive performance and adaptiveness, the FAIIR tool demonstrates promise in application to not only the use case of issue classification described at length in this paper, but also more broadly within the mental health support context. Directly relevant applications of the FAIIR tool may include streamlining triage processes for crisis lines by identifying topics at hand, and increased robustness in flagging service users at risk. Via future works, we aim to explore the utility of the FAIIR tool in varied support contexts, building towards our goal of supporting both users and providers in leveraging NLP tools to their benefit.\n\nIn conclusion, the rising demand for youth mental healthcare and crisis support has become a pressing concern for both healthcare providers and users. This growing need has prompted active efforts to develop and deploy safe, trustworthy, and transparent conversational AI solutions that support Fig.  3  | Results of expert blind review. Experts' blind review results presented in a matrix format, whereby each row represents an issue tag and each column a conversation. Three reviewers assess each conversation, providing feedback on the issue tags predicted by the FAIIR tool: indicating their agreement or disagreement, and identifying missing tags, where applicable. Cells shaded in green indicate agreement between reviewer and model, while cells shaded in red represent missing tags. The letter \"A\" in the cell followed by a number indicates the total number of reviewers (of three total) in agreement with model predictions. The letter \"M\" in the cell followed by a number indicates the total number of reviewers who believe this issue tag was missed by the FAIIR tool.\n\nproviders by reducing the administrative burden inherent to providing support and guidance to young individuals facing mental health challenges. Our study contributes to the ongoing exploration of solutions by showcasing the development and evaluation of a front-line conversational agent assistant tool, while sharing lessons learned with the broader community. By demonstrating the effectiveness and feasibility of such solutions, our study paves the way for broader adoption and implementation of conversational AI models in mental health and crisis support services."
    },
    {
      "title": "Methods"
    },
    {
      "title": "Study design and setting",
      "text": "The study comprised two phases. The first phase involved building FAIIR for the identification of issue(s) that a young person might be experiencing from their textual conversations with trained CRs. Identification was performed using a list of 19 predefined issue tags. This phase laid the foundation for our work, where we diligently developed, fine-tuned, and evaluated FAIIR's capacity as an NLP tool to understand and predict issues. In the second phase, we validated the model's efficacy and accuracy through testing with domain experts and silent testing. This phase confirmed the practical applicability and real-world utility of FAIIR for CRs. Both phases utilized conversations related to crisis support services at KHP."
    },
    {
      "title": "Curation of study dataset",
      "text": "The primary conversational dataset used for building and evaluating the NLP models of FAIIR was comprised of 703,975 unique, scrubbed, multiturn dialog instances between service users and CRs via SMS from January 2018 until February 2023. An additional batch of 84,832 conversations from February to September 2023 was used for silent testing. It is important to note that some of these dialogs may originate from service users who engage in multiple instances of interaction with CRs; however, encounters from the same individual are not linked. In total, the training data represented conversations with 340,512 individual service users and 7937 CRs. The silent testing data represented 57,031 unique service users and 2038 CRs, with expected overlap between the individuals represented in both datasets.\n\nAt the end of each conversation, service users are asked to fill out an optional demographic survey. The demographic survey captures information including the helpfulness of the conversation to the user and demographics including their age range, ethnicity or cultural group, identification with any of ten identity groups (e.g., newcomer, refugee, deaf, blind, people with disabilities), and setting of current living (i.e., city, rural area, First Nations Reserve). Approximately 17% of service users typically complete this survey, most of whom identify as female, heterosexual and of European ancestry. Most conversations are flagged as medium-risk as shown in Fig.  1h  with the distribution of priority labels across the main conversational dataset, according to the priority flagging methods further described below.\n\nA total of 19 pre-defined issue tags currently serve to describe the range of topics raised by a user during a conversation, including topics such as Depressed, Anxiety/Stress, and Gender/Sexual Identity. Upon conversation conclusion, a CR manually assigns at least one available issue tag(s) to the conversation. Metrics related to tags are used in aggregate for insight generation, to best follow trends in youth issues, support CRs, as well as reporting to funders and other agencies. It is important to note that this labeling process is carried out by CRs at their own discretion, and according to their training. Due to limited resources and large volumes of service user inquiries, issue tags typically do not undergo additional review.\n\nData was anonymized, undergoing a process of scrubbing identifying information such as names and locations, which were automatically replaced with the placeholder [scrubbed]. In many instances, complete phrases and sentences were scrubbed due to the anonymization process. This process therefore introduced some noise due to the unintentional removal of harmless words, like turkey.\n\nPriority flag pre-processing At the start of each conversation, the system generates a priority flag based on the service user's first few words. Service users are then triaged into categories of either high, medium, low-risk, or \"no ground truth\" via an algorithm owned by Crisis Text Line. Medium risk is assigned when a user expresses suicidal thoughts or self harm, and high risk is assigned when an individual is deemed to be an \"imminent risk\", defined as having a combination of suicidal thoughts, a plan, access to means, and a 0-48 h timeline to end their life. The presence of any 56 English or 73 French words in an initial message from a user leads to their automatic triage to a higher priority level. According to the distribution in Fig.  1h  (main text), the vast majority of conversations (87%) were flagged as medium-risk, with about 13% being flagged as high risk. Almost no conversations (0.0001%) were flagged as low-risk. a Precision, b recall, and c F1-score measures were averaged across all issue tags and conversations. FA: Primary denotes full agreement on primary issue tags, PA: Primary Maj. denotes partial agreement on primary issue tags via majority vote, PA: Primary and Secondary Maj. denotes partial agreement on primary and secondary issue tags via majority vote; FA: Primary (greater than or equal to) 1 denotes full agreement on primary issue tags via at least one vote; and FA: Primary and Secondary (greater than or equal to) 1 denotes full agreement on primary and secondary issue tags via at least one vote. Average denotes the average performance across all five consensus criteria. One sample t-test was conducted to assess the statistical significance between average and FAIIR tool vs. original annotations (identified by two asterisks). The consensus among expert responses and FAIIR predictions after updating the threshold in accordance with expert assessment can be seen in the FAIIR (UT) vs. Experts bars.\n\nhttps://doi.org/10.1038/s41746-025-01647-6\n\nTo assess how the FAIIR model performs across the different assigned priority levels, similar to Table  1 , we collected fine-grained performance of the main FAIIR tool across our main metrics on the retrospective test set (n = 140,795). We divided the conversations in the test set into two main priority levels (\"Medium\" and \"High\") and report the results across the two thresholds (0.25 and 0.5) in Supplementary Table  4 . We observe that performance across the two risk categories has little variation, meaning there is little bias towards conversations according to priority and that the model is able to handle these different levels accordingly."
    },
    {
      "title": "Development of the FAIIR tool",
      "text": "We framed the issue tag identification task as a multi-label classification problem, where multiple labels can be assigned to a single instance. The development of the classifier followed two distinct stages. In the first stage, we compared and evaluated various pre-trained transformerbased language models, fine-tuning them on a randomly selected subset of the data for classification. Pre-training involves training models on large-scale text corpora to learn general language patterns, while finetuning adapts these models to a specific task using a smaller, taskspecific dataset. The second step involved domain adaptation, refining the model to better capture the nuances of youth mental health conversations. This was achieved through additional pre-training and finetuning on the full baseline training dataset, ensuring the classifier effectively recognized context-specific language patterns and issues relevant to the domain."
    },
    {
      "title": "Development step 1: model comparisons",
      "text": "We explored four primary variants of transformer models for processing lengthy documents and task-oriented conversational data. These models fall into two categories of \"encoder-only\" models-designed primarily for classification tasks and \"encoder-decoder\" models-which process input text using an encoder and generate output using a decoder  24  . The models evaluated included Longformer  9  , an encoder-only model with 149M parameters, Conversational BERT, an encoder-only model with 110M parameters  8, 25  , DialogLED, an encoder-decoder model with 139M parameters  26  , and MVP (Multi-task superVised Pre-training), an encoderdecoder model with 406M parameters  27  . During fine-tuning, these models, we incorporated a classification head, a single-layer neural network that converts the model's output into probability scores for each class label. In encoder-only models, this layer was applied to the [CLS] token, which represents the entire input. For encoder-decoder models, classification was based on the [EOS] token (DialogLED) or the first token in the sequence (MVP), following established conventions  28  .\n\nAll four models were fine-tuned on 50,000 conversations randomly sampled from the dataset. We built a 60/20/20 stratified training/validation/ test. The fine tuning on the full dataset took approximately 12 h per epoch on four A10 NVIDIA GPUs (24GB VRAM) with 16 CPU cores, each with an effective batch size of 16. Learning rates were tuned within the range of 1e -5 to 3e -5 . Max token lengths were applied for BERT, DialogLED, and MVP, while Longformer was capped at 2048 tokens for efficiency. This limits the length of input text that can be provided but provides faster processing. The optimal training durations for each model were determined through basic hyperparameter tuning (used to find optimal parameters such as learning rate and batch size), resulting in two epochs (training cycles) for BERT, three epochs for DialogLED, five epochs for Longformer, and two epochs for MVP. Threshold selection was a key consideration in determining how labels were assigned. Since this is a multi-label classification task, where each conversation can have multiple assigned tags, we experimented with different threshold values to optimize the balance between precision and recall. We systematically evaluated thresholds ranging from 0.25 to 0.5 on a validation set, measuring their impact on classification performance. Our analysis indicated that a threshold of 0.25 yielded the best trade-off between precision and recall, particularly for underrepresented labels."
    },
    {
      "title": "Process of selecting model architecture",
      "text": "The utilization of transformer-based models, such as Longformer, for classifying clinical or conversational data has been extensively explored in the literature. Studies, such as those of Li et al.  23  , have consistently demonstrated that Longformer models outperform shorter-sequence transformers like ClinicalBERT 29 in various downstream tasks, including clinical document classification. Similarly, in another study by Dai et al.  30  , which evaluated different approaches for classifying long documents using transformer architectures such as Longformer, it was concluded that employing transformer-based models designed for longer sequences is more effective and efficient than using shorter-sequence models like BERT. This finding is particularly relevant as BERT is constrained by a 512-token limit which prevents the processing of any text that has a token length greater than the limit, while Longformer's capability to handle longer sequences (up to eight times longer) proves advantageous for tasks requiring a broader context, such as analyzing lengthy conversational data. Other work such as those of Wang et al.  31  , Zhong et al.  26  and Ji et al.  32  highlight the significance of additional pre-training methods, such as masked language modeling and next-turn prediction, especially in the context of dialog data. Both studies emphasize the differences between general domain language and dialog, indicating that pre-training on a large corpus of domain-specific dialogs can significantly improve performance on downstream dialog tasks. Particularly Zhong et al.  26  demonstrate the benefits of pre-training Longformer using dialog-specific window-based denoising on lengthy dialogs, resulting in a substantial improvement in state-of-the-art tasks such as long dialog understanding. Lastly, Ji et al.  32  pre-train RoBERTa  33  , Longformer and XLNet  34  on mental healthcare domain data for the task of mental health classification, achieving superior results in most cases to the base models, demonstrating the effectiveness of significant pre-training on mental healthcare domain data in related downstream tasks.\n\nSupplementary Table  2  compares the performance of the four preliminary models, including Longformer, Conversational BERT, DialogLED, and MVP, all chosen for their suitability for handling either conversational data or long-documents. We used five metrics to evaluate model performance on the test data. The first metric is the standard classification \"accuracy\" which considers the total percentage of all of the 19 tags predicted by the model correctly for each instance across the full dataset. In this way, to attain full accuracy for a given conversation, the model must predict all of the correct set of tags assigned to the conversation, and not mistakenly predict any tags that are not in the correct set of tags.\n\nDue to the sparsity of assigned tags, with conversations tending to be tagged with only a few tags out of the 19 total, a classifier can attain a high accuracy by not predicting any tags, hence we use a second metric which is referred to as \"exact accuracy\", and assesses correctness based on the percentage of conversations where all the predicted issue tags are correct. As such, a single misidentified tag for a conversations means it is classified as an incorrect prediction. In addition to accuracy, we use three other metrics which we call \"sample average precision\", \"sample average recall\", and \"sample average F1-score\".\n\nIn the context of multi-label classification, the sample/example-based average calculates the three scores for each sample then averages the scores across all samples. For each sample, the entire set of predicted tags is considered in the calculation of the three scores without isolating each individual tag type, and are compared with the full set of true labels. This is unlike micro-averaging where the scores are calculated globally across all of the total true positives, false negatives and false positives, or macro-averaging where the scores are calculated across all of the true positive, false positive, and false negative counts for a specific tag i before taking the unweighted mean across the scores for all tags. This method of averaging provides a representative result for the entire distribution after assessing scores for each sample individually. The metrics displayed in Supplementary Table  2  are averages across all issue samples.\n\nBoth Longformer and Conversational BERT exhibit comparable high performance (Accuracy: 0.94 and sample average F1-score: 0.56). Conversational BERT offers the advantage of being pre-trained on an extensive corpus of conversation data, while Longformer excels in capturing longer sequences. Therefore, we leveraged Longformer due to the nature of our conversations (long sequences), with the intention of performing domain adaptation akin to Conversational BERT to improve its performance. The remaining two models, based on encoder-decoder architectures, underperformed (sample average F1-score <0.35) primarily because they were not originally designed for this particular multi-label classification task. Furthermore, both encoder-decoder models encountered significant practicality issues related to exceptionally lengthy training times and resource limitations, necessitating use of small batch sizes and long inference times. As a result, they were deemed sub-optimal choices for this specific task."
    },
    {
      "title": "Development step 2: final model development and optimization",
      "text": "For our final model, we employed an ensemble approach combining three Longformer models, each with slightly different initialization and finetuning settings. The choice of Longformer as our primary model was based on its superior performance and its capacity to effectively capture long conversations. Each Longformer underwent initial pre-training using the same approach, which included masked language modeling, where a portion of words in each conversation was masked, and the model learned to predict them, on the full baseline training dataset. We applied masking to 15% of tokens per conversation and pre-trained the models for one epoch with a maximum sequence length of 1500 tokens. AdamW  35  was used as the optimizer, and a linear scheduler with 500 warm-up steps was applied to improve training stability. Gradient accumulation was used to maintain an effective batch size of 64, ensuring efficient use of GPU resources. This pretraining step required approximately 24 h.\n\nFollowing the pre-training task, the Longformer models were finetuned on a label-balanced training/validation/test data split (60/20/20). Per recommendation of our domain experts, we incorporated additional context information related to the conversation's priority. Therefore, the beginning sentence included the statement: \"This conversation is of < < X > > priority\" with X representing one of the three priority levels assigned to each conversation. More on this process of generating these levels is discussed in the \"Methods\" section: \"Priority flag pre-processing\". Each Longformer model was fine-tuned for a maximum of three epochs using a batch size of 16, managed through gradient accumulation, with a learning rate set to 2e -5 . Standard Binary Cross Entropy loss was applied during the fine-tuning, with oversampling of conversations with less common issue tags specifically implemented on two of the ensemble models to address the class imbalance. We used AdamW as the weight optimizer and implemented a linear scheduler with the initial 20% of training steps."
    },
    {
      "title": "Evaluation of FAIIR predictions",
      "text": "Upon completion of the development of the FAIIR tool, we conducted two independent experiments to evaluate its efficacy and performance in generalization. The experiments included both expert assessment and silent testing of the tool and its predictions."
    },
    {
      "title": "Expert assessment and evaluation",
      "text": "Methods for expert assessment for FAIIR included conducting an evaluation survey completed by CRs. We invited 12 trained CRs to review 40 challenging conversations. The conversation selection criteria were diverse, focusing on those with more than four issue tags and including ambiguous cases where FAIIR's predictions were confident but incorrect based on our ground-truth labels. Our hypothesis was that for these edge cases, FAIIR requires a deep and nuanced understanding to perform well. Thus, our goal was to assess the model's ability to identify all relevant issue tags and navigate language nuances. Twenty of the 40 total conversations annotated were picked at random from the test set to get a representative sample of the data. All 20 conversations were originally labeled with four or more different issue tags. The reason for selecting these conversations is their coverage of a wide range of issues: the potential for annotators picking a different set of issue tags from each other is high, and perspectives to determine which issue tags truly apply to the conversation were of upmost importance. This was also important in evaluating whether the model was able to grasp all nuanced issue tags that may apply less directly to a given conversation.\n\nThe remaining 20 of the 40 conversations were mostly originally tagged with three or less issue tags, in an effort to promote a balance between conversations with many tags and those where only a small number may apply. Of these 20, handfuls of conversations were selected according to several differing criteria. A number were selected manually to cover all of the 19 different issue tags, in an effort to build consensus in the identification of all tags for the model to reference. A small sample of conversations were also selected as purposefully ambiguous cases: mainly long conversations which were only annotated with one or two issue tag(s). Although the issue tag(s) assigned at baseline were typically correct, these conversations were an opportunity to gain consensus on a spectrum of more nuanced tags for the purposes of model fine-tuning. The last few conversations were handpicked because they were perceived to be mislabelled in some way. For these conversations, the original issue tag(s) assigned appeared incorrect or incomplete, in that there was another key issue tag missing. Consensus building is important for these examples, in order to improve the original tag(s) assigned, where incorrect. These can also be complex cases for the model to navigate, and thus served as a helpful way to evaluate the tool's performance.\n\nEach conversation was independently reviewed by six CRs, divided into two groups. In the \"open review\", three CRs reviewed conversations with FAIIR's predicted issue tags explicitly provided. This approach aimed to evaluate whether the model's predictions were helpful, misleading, or partially correct in identifying the core issues within each conversation. CRs could either agree or disagree with the predicted tags and suggest corrections or refinements where necessary.\n\nIn the \"blind review\", the remaining three CRs reviewed the same conversations without any prior exposure to FAIIR's predicted tags. Instead, they independently identified issue tags based solely on the conversation content. Furthermore, they categorized the identified tags into primary issue tags (representing the most pressing concerns) and secondary issue tags (minor but relevant concerns). This approach established a baseline for comparison against FAIIR's predictions, ensuring that human assessments were made without any influence from the model.\n\nThe following five criteria were established to develop a consensus measure for comparison in the blind review setting, which is more challenging than the open review setting. Since human annotations categorize issue tags as primary (most pressing concerns) and secondary (minor but relevant concerns), we evaluated agreement with FAIIR's outputs based on these distinctions. Notably, FAIIR does not explicitly differentiate between primary and secondary issue tags-all predicted tags are treated equally. Therefore, for the purpose of comparison, we assessed agreement by mapping FAIIR's predicted tags to human annotations and measuring alignment using the following criteria:\n\n\u2022 Full agreement on primary issue tags (FA: 1\u00b0)-all primary issue tags identified by human annotators are also predicted by FAIIR. \u2022 Partial agreement on primary issue tags via majority vote (PA: 1\u00b0Maj.)\n\n-the majority of human annotators agree on a set of primary issue tags, and these tags overlap with FAIIR's predictions. \u2022 Partial agreement on primary and secondary issue tags via majority vote (PA: 1\u00b0+ 2\u00b0Maj.)-the majority of human annotators agree on a set of both primary and secondary issue tags, and these overlap with FAIIR's predictions. \u2022 Full agreement on primary issue tags via at least one vote (FA: 1\u00b0\u2265 1)at least one human annotator identified a primary issue tag that is also predicted by FAIIR. \u2022 Full agreement on primary and secondary issue tags via at least one vote (FA: 1\u00b0+ 2\u00b0\u2265 1)-at least one human annotator identified a primary or secondary issue tag that is also predicted by FAIIR."
    },
    {
      "title": "Model refinement-modifying the decision boundary",
      "text": "In our evaluation experiments, model refinement involved adjusting the decision boundary (threshold cutoff) to strike a balance between recall and precision. In most experiments, the FAIIR tool's predictions showed lower precision compared to recall, so we adjusted the threshold to reduce its frequency of outputting the most common tags while lowering the threshold for rare tags. For example, we set the threshold to 0.4 for the three most frequent classes: Anxiety/Stress, Depressed, and Relationship. For the next two most frequent classes, Suicide and Isolated, we adjusted the threshold to 0.3. These five classes encompass the majority of predicted issue tags from the model, hence we targeted them for increased thresholds. The remaining tags were set at a lower threshold of 0.2 to enhance the model's ability to capture them effectively.\n\nespecially the most vulnerable. This paper is aimed exclusively at applied research to improve service delivery and accessibility, with a special focus on the ethical application of Artificial Intelligence (AI) to benefit our service network and frontline staff. Through this collaboration, we are dedicated to developing technological tools that provide a personalized and user-friendly experience for those seeking help. Upholding the privacy and confidentiality of our service users is paramount; we adhere to an ethical statement aligned with KHP's privacy policy ( https://kidshelpphone.ca/privacy-policy/ ), including consent notice for research and rigorous data minimization. Our processes are transparent and accountable, compliant with Canadian privacy regulations. We meticulously remove all direct identifiers from research data, adhering to industry standards for data anonymization, and securely store all research data within KHP's infrastructure. This reflects our commitment to the highest standards of data security, confidentiality, and ethical practice. By prioritizing ethical data use, KHP can leverage research to improve our services and deliver the best possible support for youth across Canada, embodying our commitment to integrity, respect, and responsibility in every action we take."
    },
    {
      "text": "Fig. 1 | Dataset statistics. a 703,975 youth conversations with frontline crisis responders are classified into 19 pre-defined issue tags. Multiple tags may be assigned per conversation, as relevant. After each interaction, service users are invited to complete a demographic survey, gauging the conversation's helpfulness and the individual's demographics (age range, ethnicity, and identification with specific identity groups). b The distribution of gender identity in the aggregated surveys. c The distribution of sexual orientation of the aggregated surveys. d The distribution of identified groups in the aggregated surveys. e The distribution of ethnicity in the aggregated surveys. f Distribution of conversation lengths (# tokens). g Distribution of the number of issue tags assigned to conversations. h Distribution of priority labels assigned to conversations."
    },
    {
      "text": "Fig.2| FAIIR model performance. a Averaged performance of the FAIIR tool in predicting all 19 issue tags is shown for the retrospective test set (n = 140,795) comparing thresholds of 0.5 (orange) and 0.25 (blue) as well as overall AUC ROC model performance (gray). b Averaged performance of the FAIIR tool across all issue tags in the silent testing prospective test sets (n = 84,932), evaluated using previous two classification thresholds with the addition of an updated threshold (green). For"
    },
    {
      "text": "Fig. 4 | Issue consensus overview. Comparison of consensus among expert responses, FAIIR tool predictions, and original annotations from open review.a Precision, b recall, and c F1-score measures were averaged across all issue tags and conversations. FA: Primary denotes full agreement on primary issue tags, PA: Primary Maj. denotes partial agreement on primary issue tags via majority vote, PA: Primary and Secondary Maj. denotes partial agreement on primary and secondary issue tags via majority vote; FA: Primary (greater than or equal to) 1 denotes full agreement on primary issue tags via at least one vote; and FA: Primary and Secondary"
    },
    {
      "text": "The fine-grained performance of the FAIIR tool, an ensemble of three Longformer models, is reported for two datasets: a retrospective test set (n"
    },
    {
      "text": "The performance of FAIIR (with threshold 0.25) within subgroups of service users across four distinct demographic categories"
    }
  ],
  "references": [
    {
      "title": "Mental health of adolescents",
      "year": 2021,
      "doi": "10.2471/b09131"
    },
    {
      "title": "Making the Case for Investing in Mental Health in Canada",
      "year": 2016
    },
    {
      "title": "A growing need for youth mental health services in Canada: examining trends in youth mental health from 2011 to 2018",
      "authors": [
        "K Wiens"
      ],
      "year": 2020,
      "doi": "10.1017/s2045796020000281"
    },
    {
      "title": "Vision impairment and blindness",
      "year": 2023,
      "doi": "10.22215/etd/2012-06764"
    },
    {
      "title": "An evaluation of crisis hotline outcomes. Part 2: suicidal callers",
      "authors": [
        "M Gould",
        "J Kalafat",
        "J Harris Munfakh",
        "M Kleinman"
      ],
      "year": 2007,
      "doi": "10.1521/suli.2007.37.3.338"
    },
    {
      "title": "The effectiveness of crisis line services: a systematic review",
      "authors": [
        "A Hoffberg",
        "K Stearns-Yoder",
        "L Brenner"
      ],
      "year": 2020,
      "doi": "10.3389/fpubh.2019.00399"
    },
    {
      "title": "Mixedinitiative real-time topic modeling & visualization for crisis counseling",
      "authors": [
        "K Dinakar",
        "J Chen",
        "H Lieberman",
        "R Picard",
        "R Filbin"
      ],
      "year": 2015,
      "doi": "10.1145/2678025.2701395"
    },
    {
      "title": "pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": 2018,
      "doi": "10.48550/arXiv.1810.04805"
    },
    {
      "title": "Longformer: the long-document transformer",
      "authors": [
        "I Beltagy",
        "M Peters",
        "A Cohan"
      ],
      "year": 2020
    },
    {
      "title": "Llama 2: open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron"
      ],
      "year": 2023
    },
    {
      "title": "Machine learning and natural language processing in mental health: systematic review",
      "authors": [
        "Le Glaz"
      ],
      "year": 2021
    },
    {
      "title": "Using social media to help understand patientreported health outcomes of post-COVID-19 condition: natural language processing approach",
      "authors": [
        "E Dolatabadi"
      ],
      "year": 2023,
      "doi": "10.2196/45767"
    },
    {
      "title": "Discovering social determinants of health from case reports using natural language processing: algorithmic development and validation",
      "authors": [
        "S Raza",
        "E Dolatabadi",
        "N Ondrusek",
        "L Rosella",
        "B Schwartz"
      ],
      "year": 2023,
      "doi": "10.1186/s44247-023-00035-y"
    },
    {
      "title": "Natural language processing for clinical laboratory data repository systems: implementation and evaluation for respiratory viruses",
      "authors": [
        "E Dolatabadi"
      ],
      "year": 2023
    },
    {
      "title": "Detecting depression signs on social media: a systematic literature review",
      "authors": [
        "R Salas-Z\u00e1rate"
      ],
      "year": 2022,
      "doi": "10.3390/healthcare10020291"
    },
    {
      "title": "BlueMemo: depression analysis through Twitter posts",
      "authors": [
        "P Hu"
      ],
      "year": 2021,
      "doi": "10.24963/ijcai.2020/760"
    },
    {
      "title": "Depression and self-harm risk assessment in online forums",
      "authors": [
        "A Yates",
        "A Cohan",
        "N Goharian"
      ],
      "year": 2017,
      "doi": "10.48550/arXiv.1709.01848"
    },
    {
      "title": "# suicidal-a multipronged approach to identify and explore suicidal ideation in twitter",
      "authors": [
        "P Sinha"
      ],
      "year": 2019,
      "doi": "10.1145/3357384.3358060"
    },
    {
      "title": "Detection of suicidality in adolescents with autism spectrum disorders: developing a natural language processing approach for use in electronic health records",
      "authors": [
        "J Downs"
      ],
      "year": 2018
    },
    {
      "title": "Predicting mental conditions based on \"history of present illness\" in psychiatric notes with deep neural networks",
      "authors": [
        "T Tran",
        "R Kavuluru"
      ],
      "year": 2017,
      "doi": "10.1016/j.jbi.2017.06.010"
    },
    {
      "title": "Natural language processing system for rapid detection and intervention of mental health crisis chat messages",
      "authors": [
        "A Swaminathan"
      ],
      "year": 2023,
      "doi": "10.1038/s41746-023-00951-3"
    },
    {
      "title": "Bringing the state-of-the-art to customers: a neural agent assistant framework for customer service support",
      "authors": [
        "S Obadinma"
      ],
      "year": 2022,
      "doi": "10.18653/v1/2022.emnlp-industry.44"
    },
    {
      "title": "Clinicallongformer and clinical-bigbird: transformers for long clinical sequences",
      "authors": [
        "Y Li",
        "R Wehbe",
        "F Ahmad",
        "H Wang",
        "Y Luo"
      ],
      "year": 2022,
      "doi": "10.1093/jamia/ocac225"
    },
    {
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": 2017,
      "doi": "10.7717/peerjcs.1946/fig-10"
    },
    {
      "title": "bert-base-cased-conversational",
      "authors": [
        "Deeppavlov"
      ],
      "year": 2023
    },
    {
      "title": "DialogLM: pre-trained model for long dialogue understanding and summarization",
      "authors": [
        "M Zhong",
        "Y Liu",
        "Y Xu",
        "C Zhu",
        "M Zeng"
      ],
      "year": 2022,
      "doi": "10.1609/aaai.v36i10.21432"
    },
    {
      "title": "multi-task supervised pre-training for natural language generation",
      "authors": [
        "T Tang",
        "J Li",
        "W Zhao",
        "J.-R Wen",
        "Mvp"
      ],
      "year": 2023,
      "doi": "10.1038/s41746-025-01647-6"
    },
    {
      "title": "An exploration of encoderdecoder approaches to multi-label classification for legal and biomedical text",
      "authors": [
        "Y Kementchedjhieva",
        "I Chalkidis"
      ],
      "year": 2023,
      "doi": "10.18653/v1/2023.findings-acl.360"
    },
    {
      "title": "ClinicalBERT: modeling clinical notes and predicting hospital readmission",
      "authors": [
        "K Huang",
        "J Altosaar",
        "R Ranganath"
      ],
      "year": 2020
    },
    {
      "title": "Revisiting transformerbased models for long document classification",
      "authors": [
        "X Dai",
        "I Chalkidis",
        "S Darkner",
        "D Elliott"
      ],
      "doi": "10.18653/v1/2022.findings-emnlp.534"
    },
    {
      "authors": [
        "Y Goldberg",
        "Z Kozareva",
        "Y Zhang"
      ],
      "year": 2022
    },
    {
      "title": "CS-BERT: a pretrained model for customer service dialogues",
      "authors": [
        "P Wang",
        "J Fang",
        "J Reinspach"
      ],
      "year": 2021,
      "doi": "10.18653/v1/2021.nlp4convai-1.13"
    },
    {
      "title": "Domain-specific continued pretraining of language models for capturing long context in mental health",
      "authors": [
        "S Ji"
      ],
      "year": 2023,
      "doi": "10.48550/arXiv.2304.10447"
    },
    {
      "title": "RoBERTa: a robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu"
      ],
      "year": 2019,
      "doi": "10.48550/arXiv.1907.11692"
    },
    {
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "authors": [
        "Z Yang"
      ],
      "year": 2019
    },
    {
      "title": "Decoupled weight decay regularization",
      "authors": [
        "I Loshchilov",
        "F Hutter"
      ],
      "year": 2019
    },
    {
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "M Sundararajan",
        "A Taly",
        "Q Yan"
      ],
      "year": 2017,
      "doi": "10.48550/arXiv.1703.01365"
    }
  ],
  "num_references": 37
}
