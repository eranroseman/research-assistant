{
  "paper_id": "Z7ABMJ7W",
  "title": "A Survey on Differential Privacy for Medical Data Analysis",
  "abstract": "Machine learning methods promote the sustainable development of wise information technology of medicine (WITMED), and a variety of medical data brings high value and convenience to medical analysis. However, the applications of medical data have also been confronted with the risk of privacy leakage that is hard to avoid, especially when conducting correlation analysis or data sharing among multiple institutions. Data security and privacy preservation have recently played an essential role in the field of secure and private medical data analysis, where many differential privacy strategies are applied to medical data publishing and mining. In this paper, we survey research work on the applications of differential privacy for medical data analysis, discussing the necessity of medical privacy-preserving, the advantages of differential privacy, and their applications to typical medical data, such as genomic data and wearable device data. Furthermore, we discuss the challenges and potential future research directions for differential privacy in medical applications.",
  "year": 2023,
  "date": "2023-06-10",
  "journal": "BioMed Res Int",
  "publication": "BioMed Res Int",
  "authors": [
    {
      "forename": "Weikang",
      "surname": "Liu",
      "name": "Weikang Liu",
      "affiliation": "1  Cyberspace Institute of Advanced Technology , Guangzhou University , Guangzhou , China \n\t\t\t\t\t\t\t\t Cyberspace Institute of Advanced Technology \n\t\t\t\t\t\t\t\t Guangzhou University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Guangzhou \n\t\t\t\t\t\t\t\t\t China"
    },
    {
      "forename": "Yanchun",
      "surname": "Zhang",
      "name": "Yanchun Zhang",
      "affiliation": "1  Cyberspace Institute of Advanced Technology , Guangzhou University , Guangzhou , China \n\t\t\t\t\t\t\t\t Cyberspace Institute of Advanced Technology \n\t\t\t\t\t\t\t\t Guangzhou University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Guangzhou \n\t\t\t\t\t\t\t\t\t China",
      "email": "yanchun.zhang@vu.edu.auhong"
    },
    {
      "forename": "Hong",
      "surname": "Yang",
      "name": "Hong Yang",
      "affiliation": "1  Cyberspace Institute of Advanced Technology , Guangzhou University , Guangzhou , China \n\t\t\t\t\t\t\t\t Cyberspace Institute of Advanced Technology \n\t\t\t\t\t\t\t\t Guangzhou University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Guangzhou \n\t\t\t\t\t\t\t\t\t China",
      "email": "hyang@gzhu.edu.cn"
    },
    {
      "forename": "Qinxue",
      "surname": "Meng",
      "name": "Qinxue Meng",
      "affiliation": "3  College of Information Engineering , Suzhou University , Suzhou , China \n\t\t\t\t\t\t\t\t College of Information Engineering \n\t\t\t\t\t\t\t\t Suzhou University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Suzhou \n\t\t\t\t\t\t\t\t\t China",
      "email": "qinxue.meng@gmail.com"
    }
  ],
  "doi": "10.1007/s40745-023-00475-3",
  "arxiv": "arXiv:1710.01727",
  "keywords": [
    "Privacy computing",
    "Differential privacy",
    "Medical data",
    "Data publishing"
  ],
  "sections": [
    {
      "title": "Introduction",
      "text": "Recently, we have witnessed an increasing number of data science applications in sustainable development field of wise information technology of medicine (WITMED). More applications include drug discovery and disease surveillance, where personal information such as name, age, gender, postal code, profession, disease, and medical history can be collected, published and used by third-party terminal devices or authorities. The analysis and applications of medical data have become a hot topic in recent years  [1, 2] .\n\nCombining data science and modern medicine, the benefits of analyzing medical data span disease prediction, new drug research and development, auxiliary diagnosis and treatment, and health management. However, as more data is collected and processed through interconnected devices  [3] , privacy becomes a significant concern due to private sensitive information that may be contained within the data.\n\nIn data science research, data privacy-preserving has become increasingly significant in addressing security and privacy challenges. The development of privacyenhancing techniques, including differential privacy, secure multi-party computation and homomorphic encryption, is imperative for enabling privacy protection while collecting and analyzing data collaboratively. Additionally, transparent and accountable data governance frameworks that protect privacy and facilitate informed consent should be developed to ensure the responsible utilization of data. Therefore, adopting a comprehensive approach that encompasses both technical and ethical considerations is necessary to effectively address the privacy challenges that arise at the intersection of artificial intelligence and data science.\n\nAs for medical data analysis, we have observed that the attacks on medical datasets and models have increased rapidly in recent years. Therefore, the research on privacypreserving methods has become a crucial area of study in medical informatics field. Privacy computing can realize medical simulation, prediction and security statistical analysis of medical data with specific privacy-preserving levels. For publishing medical data, anonymous methods are capable of defending against linking attacks, skewness attacks and similarity attacks, to name a few. However, they do not have enough resistance to background knowledge  [4] . Differential privacy is not only robust to differential attacks, but also defending against all of the above attacks on medical sensitive data. Moreover, for publicly published models, differential privacy algorithms also prevent adversarial recovery of private information from the original medical data.\n\nIn recent years, there has been a surge in development of novel algorithms for differential privacy medical analysis, which this paper aims to conduct a survey on. And the efforts of this paper can be summarized as follows. First, we discuss why differential privacy is considerable in medical data publishing data and data mining. Second, we discuss typical differential privacy methods based on noises, which can help better understand existing work. Third, we analyze the limitations of differential privacy strategy and summarize possible future challenges, highlighting future research directions of medical applications of differential privacy.\n\nThe rest of this paper is structured as follows. Section 2 introduces privacy computing technology of medical data and the characteristics of anonymous methods. The fundamental theories of differential privacy and its noise mechanisms are obtained in Sect. 3. Section 4 illustrates applications of differential privacy to medical data. Subsequently, we analyze and discuss partial possible future challenges of differential privacy in Sect. 5. Section 6 concludes the paper."
    },
    {
      "title": "Medical Data Privacy Computing",
      "text": "The development of privacy connotation is dynamic, continuing to enrich its meaning with the progress of social politics, economic culture and the improvement of human consciousness. The so-called privacy computing is a series of privacy-preserving methods that protect sensitive data from being visible but available when using conjoint analysis and computing collaboratively on model data.\n\nUnlike secure blockchain framework  [5]  or some web attack detection techniques in cloud-IoT system  [6] , privacy computing has mainly integrated cryptography, artificial intelligence and computer hardware technologies into a relatively mature technical system represented by multi-party security computation, trusted execution environment and federated learning. Meanwhile, it also regards differential privacy, homomorphic encryption, zero-knowledge proof and others as auxiliary technology, providing a technical guarantee for data security and circulation.\n\nThe research on privacy problems can be divided into five categories: financial privacy, Internet privacy, medical privacy, political privacy and information privacy  [7] . Among them, medical privacy comes from a wide range of sources and complex types of medical data, mainly including information that patients do not want to be known to the outside world, such as genomic information, past medical history, medical records, etc. They are commonly stored in the form of electronic medical record (EMR), electronic health records (EHR) and personal health records (PHR).\n\nMedical data scattered in different institutions is difficult to interconnect each part, which may seriously restrict the output of clinical scientific research results. For this problem, privacy computing technology has the ability to provide a series of practical solutions to achieve data circulation and take full advantage of medical data. What's more, it can also solve the problem of insufficient samples from a single institution that leads to credibility loss of research results.\n\nDuring the COVID-19 epidemic prevention and control period  [8, 9] , analyses on medical services and tests, pulse count, body temperature and the overall effect of age and gender was done  [10, 11] . Furthermore, the use of privacy computing technology such as multi-party security computing enables researchers from all over the world to jointly conduct genome analysis of case samples and share sequencing results without disclosing detailed personal information, so as to implement real-time tracking of the current virus situation and prediction of future strain evolution  [1, 12] . This will help more countries diagnose COVID-19 patients efficiently and take effective measures in time.\n\nGenerally, genome analysis relies on a large number of personally private data. Using privacy computing will have the original genetic data sealing in local database and realize safe sharing of sensitive genomic data. Then the joint calculation and Fig.  1  Privacy-preserving life cycle of medical data association analysis will be carried out. In this way, various genome resources can be mined by different medical institutions under the premise of privacy-preserving.\n\nFor clinical medical research, utilizing local data protected by privacy computing technology can implement distributed statistical analysis algorithms to joint modeling and obtain related results, such as feasibility analysis of clinical research, cohort study with large samples, disease prediction and drug insight, etc. Therefore, the application of privacy computing will greatly improve medical research efficiency and accelerate the transformation of scientific research achievements.\n\nAs shown in Fig.  1 , the complete medical data life cycle incorporates data publishing, storing, mining and utilizing  [13, 14] . Data publishers, storage parties, miners and users are involved in this process. Both the private data threats and the corresponding privacy-preserving techniques are different at each phase.\n\nIn practical medical scenarios, the data publishing phase usually involves continuous release of medical data, and attracts the attention of adversaries, who are able to combine specific background knowledge to carry out a series of analyses and attacks on sensitive medical data. Thus, in data publishing phase, while ensuring efficient transmission and strong usability of data, considering how to safely and reliably deal with sensitive information which may be leaked is also a crucial issue for medical researchers and clinicians.\n\nTraditional anonymous publishing methods are usually adopted in the process of medical data release, including k-anonymity [15], l-diversity  [16]  and t-closeness  [17] . Through generalization, suppression and substitution of dataset tuples, they align identifier classification based on specific rules so as to meet the need for medical data desensitization release. Although anonymous approaches are capable of protecting sensitive plaintext information  [18, 19] , they cannot effectively prevent the attackers from using background knowledge depending on external databases to link attacks, and their privacy protection effect lacks strict theoretical proof. Exactly, the differential privacy computing technology mainly introduced in the following can make up for the disadvantages of anonymization methods to solve corresponding problem."
    },
    {
      "title": "Differential Privacy",
      "text": "In a hypothetical scenario, if data collectors have to collect the published patient diagnosis and treatment records from a hospital, differential privacy can protect sensitive information by adding random noise or disturbance to the original records, which not only cannot reveal certain personal data of a certain user in the datasets, but also ensures the overall statistical characteristics within specified bounds, thus maintaining data utility to a certain extent. That strategy greatly ensures the privacy and security of medical data.\n\nProposed by Dwork et al.  [20] , the concept of differential privacy comes from semantic security in cryptography. On the one hand, differential privacy makes it impossible for adversaries to distinguish the encryption results of different plaintexts. On the other hand, it provides a strict upper limit of privacy protection in mathematics, that is, privacy budget. To prevent differential attacks by adding random noise is the direct purpose of differential privacy, so that the adversary cannot effectively infer personal privacy while maximizing the availability of query results in neighboring datasets. The differential attack is that the adversary makes use of subtraction thinking in neighboring datasets to infer sensitive data of a certain person by comparing statistical results of queries.\n\nIn the data publishing phase, using differential privacy can ensure that one same data is queried in two neighboring datasets and the results are basically the same, so as to confuse the judgment of the adversary. In addition to guarding against differential attacks, differential privacy can also prevent link attacks based on background knowledge to a large extent."
    },
    {
      "title": "Definition",
      "text": "Generally speaking, differential privacy is defined as follows: Given a randomized algorithm (query function) M, P m is the set of all range values that M outputs, and S m \u2286 P m . For any two neighboring datasets D and D (at most differing on one-row data), if the algorithm M satisfies:\n\nThen it is said that algorithm M satisfies \u03b5-differential privacy, where the parameter \u03b5 is the privacy budget. As can be seen from Eq. (3.1) (or put e \u03b5 on the right side alone), the smaller the privacy budget is, the probability distribution of query results returned by M on neighboring datasets is more similar, accompanied by the harder it is for the adversary to distinguish the pair of neighboring datasets. It provides higher protection degree of sensitive data, but correspondingly, data utility will get worse gradually. On the contrary, a larger privacy budget will lower the degree of privacy protection and improve data utility.\n\nNotably, the probabilities of the third party querying neighboring datasets to get the same statistic value are only very close, not exactly equal. While protecting specific data from leakage, it is also essential to prevent the data from being completely randomized, leading to the loss of usability."
    },
    {
      "title": "Noise-Based Mechanisms",
      "text": "In this part, we discuss three noise mechanisms commonly used in differential privacy."
    },
    {
      "title": "Laplace Mechanism",
      "text": "The query request of the original dataset D is regarded as the value of a function f on D. Laplace mechanism is achieved by adding noise \u03b7 to f (D) and the result is f (D) + \u03b7. \u03b7 is a continuous random variable satisfying Lap 0, ( f ) \u03b5 distribution and its probability density function is:\n\nIn Eq. (3.2), the expected value of the Laplace distribution is 0, the variance is 2\u03bb 2 , and the parameter \u03bb reflects the amplitude of noise and the intensity of privacy protection. Larger \u03bb means the greater range of noise added and the higher degree of privacy protection. In addition, the sensitivity is also an important factor affecting the strength of privacy protection.\n\nGiven a query function f , if f : D \u2192 R (query result), the global sensitivity of f is:\n\nfor all neighboring datasets D and D'.\n\nThe global sensitivity reflects the maximum range of variation of a query function over neighboring datasets, in conjunction with privacy budgets to control the amount of generated noise."
    },
    {
      "title": "Gaussian Mechanism",
      "text": "The Gaussian noise is a mechanism to achieve (\u03b5, \u03b4)-differential privacy, which is defined as follows:\n\nHere in (3.4), the additive term \u03b4 denotes the probability of violating plain \u03b5differential privacy is allowed. Given a function f over dataset D, if \u03b5 < 1, \u03b4 \u2208 (0, 1) and \u03b4 \u2265 4  5 e -(\u03c3 \u03b5) 2 2  [21] , \u03b4 > 2ln 1.25 \u03b4 f /\u03b5, Gaussian noise mechanism can be expressed as:  [22, 23] , N is the standard Gaussian distribution with zero-mean Gaussian noise parameter \u03c3 and a standard deviation of f \u2022 \u03c3 . Compared with L 1 -sensitivity norm used by Laplace mechanism, Gaussian mechanism follows the same privacy composition, but uses the L 2 -sensitivity norm."
    },
    {
      "title": "Exponential Mechanism",
      "text": "The above two noise mechanisms are mainly used to protect numerical data, while the exponential mechanism is suitable for non-numerical data. It defines a practical evaluation function q, in charge of calculating a satisfaction score \u03c9 for each output scheme. The scheme with high score will have a higher probability to be published, the exponential mechanism satisfies:\n\nIn Formula (3.5), (q) is the global sensitivity of the evaluation function."
    },
    {
      "title": "Classification of Differential Privacy",
      "text": "Traditional differential privacy will gather the original datasets to a data center and then release relevant statistical information satisfying differential privacy, which is called centralized differential privacy (CDP). In other words, CDP's protection of sensitive information has always been based on the assumption that the third-party data collectors are trusted, that is, they will not steal or disclose sensitive information from users. However, in practical applications, users' privacy is still not guaranteed  [24] . An investigation in 2018 showed that most mobile health apps jeopardized users' privacy by violating data protection regulations and revealing sensitive information  [25] .\n\nIn view of this, local differential privacy (LDP)  [26]  emerges in the scenario of untrusted third-party data collectors. When suffering the same quantified privacy attacks of CDP, LDP will subdivide the protection of sensitive personal information. Specifically, LDP delivers data protection authority to each user, enabling users to protect sensitive personal information independently, thus achieving more thorough privacy preservation locally. At present, LDP has been mainly used in frequency estimation, mean estimation  [27]  and gradually been put into industrial applications. For example, Apple  [28]  applied it in iOS 10 operating system to protect user device data, and Google  [29]  used it to collect users' behavior statistics from the Chrome browser."
    },
    {
      "title": "Differential Privacy in Machine Learning",
      "text": "Recently, differential privacy has also been gradually applied in data mining field and combined with increasing machine learning algorithms.\n\nDifferential privacy depends on noise or disturbance, so compared with other privacy computing methods, it has low computational complexity, improving its application efficiency in the field of machine learning while providing more explicit privacy guarantees. Noises can not only be added to original data, objective function, output model parameters or features extracted by neural network  [30] , but also be disturbed or screened for sensitive features specified by users or automatically detected by the recognition network  [31, 32] . Shokri et al.  [33]  used differential privacy mechanisms to design a distributed learning method for privacy protection early on. In their method, privacy loss can be calculated according to the parameters of the model, but too many model parameters may lead to huge privacy loss. On this basis, Abadi et al.  [22]  improved it and introduced a more efficient gradient descent algorithm based on differential privacy, which has a smaller privacy budget and better performance. More importantly, Abadi et al.  [22]  also introduced a measuring method of privacy loss, Moment Accountant, to automate the calculation of privacy loss. The differential privacy stochastic gradient descent (DP-SGD) algorithm mentioned in the paper also laid the foundation for more scholars researching on machine learning of privacy protection in the future.\n\nApplying differential privacy to machine learning will reduce the probability that the adversary can reversely deduce sensitive personal information from the model in the original training datasets. Data utility and model security are both crucial in this process. On the one hand, it is necessary to reasonably select and control the privacy budget in the training process according to the privacy loss. Methods such as dynamic allocation of privacy budget  [34] , utilizing differential privacy post-processing property for noise reduction  [35] , or reducing privacy budget that may be caused by combination characteristics  [36]  can be considered. On the other hand, some model architectures that are more conducive to protecting user privacy can also be selected  [37, 38] ."
    },
    {
      "title": "Differential Privacy for Medical Data",
      "text": "In medical data, differential privacy is mainly applied to data publishing and data mining. In the data publishing phase, it can greatly prevent the privacy leakage caused by the data query based on background knowledge. In the data mining phase, it can resist the privacy leakage caused by the membership inference attack (MIA) of the adversary on the model.\n\nAs Fig.  2  shows, current applications research focuses on genomic data, medical wearable devices, electronic medical records and medical images, etc."
    },
    {
      "title": "Genomic Data",
      "text": "Genomic data in medicine is DNA sequence with genetic benefits of individuals, such particular data is difficult to change over the life cycle and of long-lived value  [39] [40] [41] . Given this, some enterprises may be tempted by commercial interests to violate the genetic privacy of others.\n\nGenome-wide association study (GWAS) is conducive to learning genomephenome associations by analyzing the statistical correlation between the variants of a case group (phenotype positive) and a control group (phenotype negative)  [4] .\n\nThe adversary may infer the potential traits and genotypes of victims depending on trait associations available from GWAS catalogue  [42] . In order to reduce the possibility Fig.  2  Differential privacy application to medical data of leaking genome privacy from published aggregate statistics of GWAS, differential privacy strategies can be widely introduced in it. For example, to a certain extent, differential privacy can prevent attackers from inferring the number and location of single nucleotide polymorphisms (SNPs) that might be significantly linked with certain diseases in the original genetic datasets, so as to protect the gene privacy  [43, 44] . For another example, the controlled noise in differential privacy can be added to query results from genomic database, which promotes genome openness while preserving privacy  [45, 46] . However, large scale of added noise to high-dimensional genomic data will inevitably degrade data utility. To address this problem, He et al.  [47]  proposed an effective method to factorize a huge-dimensional distribution into a set of local distributions, reducing the scale of added noise.\n\nMoreover, Almadhoun et al.  [48]  showed that the adversary could infer genome privacy from query results added noise by exploiting the correlations between the genomes of family members with dependency, then Almadhoun et al.  [49]  formalized the differential privacy notion to avoid sensitive information inference by adversary relying on tuples prior knowledge. Similar to this work, in order to strengthen the effect of differential privacy against correlation attacks, Yilmaz et al.  [50]  proposed a scheme which eliminates certain states of a SNP loosely correlated with previously shared SNPs. Chen et al.  [51]  researched on machine learning model's ability to defend against MIA on genomic data and evaluated the effect of model sparsity on privacy vulnerability with different differential privacy settings."
    },
    {
      "title": "Wearable Device Data",
      "text": "Medical wearable devices storing personal health data such as heart rate and blood sugar play an important role in disease diagnosis and treatment, and they made it possible to collect real-time medical health data continuously  [52] . Personal sensitive data stored in medical wearable devices need to be collected in real time, they also have a demand for privacy-preserving in data publishing.\n\nTu et al.  [53]  applied differential privacy to numerical mean stream data publishing of medical wearable devices, and adopted an adaptive sampling algorithm based on Kalman filter adjustment error to allocate privacy budgets, which improves the usability of published stream data. Kim et al.  [54]  added Laplace noise to salient points for collecting one-dimensional heart rate data, but existing large data error.\n\nRevolving around Laplace mechanism, researchers have extended a series of works to provide better data utility and privacy guarantee. Li et al.  [55]  proposed an improved randomized method to tackle stream medical data collection with a single attribute. That method incorporates random response and Laplace mechanism, further improving the availability of mean value estimation with stream data in medical wearable devices. Moreover, for partitioning or temporal medical datasets, the geometric technique  [56] , Haar Wavelet technique  [57] , bucket partition algorithm  [58]  and Fourier perturbation algorithm  [59]  have also been adopted to combine with Laplace distribution of differential privacy."
    },
    {
      "title": "Other Medical Data",
      "text": "As an inevitable product of modern information technology in the medical field, the electronic medical record is the carrier of various medical information in diagnosis and treatment process, greatly benefiting modern management of hospital medical records. Combining with LDP strategy, Wu et al.  [60]  designed a blockchain-enabled framework to provide attribute-based privacy protection for transactions. Medical diagnosis results also belong to a part of electronic medical records, Chen et al.  [61]  presented a differential privacy quasi-identifier classification scheme to tackle original disease dataset and defined privacy ratio for evaluating dataset vulnerability. Zhang et al.  [62]  designed an attribute association-based differential privacy classification tree method of data publishing, conducting experiments on real medical record datasets.\n\nIn addition, Ziller et al.  [63]  proposed an open-source software framework based on DP-SGD algorithm application to deal with medical imaging classification and semantic segmentation deep learning tasks. Yuan et al.  [64]  exploited collaborative deep learning with Gaussian noise mechanism to experiment on X-ray Images (Pneumonia) dataset and found the accuracy loss was small, affecting little to the results. Adnan et al.  [65]  indicated that federated learning with differential privacy has been the viable and reliable collaborative machine learning framework for medical image analysis."
    },
    {
      "title": "Discussions",
      "text": "Although differential privacy to medical data has made some achievements at present, it still faces difficulties and challenges in terms of practical application.\n\nFirstly, we still need to explore how to constantly improve data utility when medical data is shared and circulated across institutions, and to select suitable algorithm strategies to reduce global sensitivity and control privacy budget.\n\nSecond, due to the complexity of the scale and structure to medical data, rapidly increasing medical data has begun to be expressed in an unstructured form. As a popular method to describe networked data  [66, 67] , graph neural network (GNN) has also been successively applied to kinds of medical tasks by plenty of researchers, such as predicting chemical properties of molecules, biological interaction properties of proteins, drug recommendation, etc.  [68] [69] [70] . However, when the GNN models are uploaded to the server and the graph nodes or labels involve personal sensitive information, the process of learning graph data still has the possibility of privacy leakage. For this scenario, differential privacy strategy can also be used to add noise locally  [71, 72] . Combined with differential privacy, graph data has a more complex structure than general medical data types. On the one hand, the structural characteristics of the graph may extremely increase the global sensitivity of queries, resulting in excessive noises. On the other hand, since each user locally perturbs the data independently, how to ensure the relevance between original data and then build a graph structure with high availability based on disturbed data also become the main challenges in current practical applications.\n\nThird, existing privacy-preserving computation methods have their own limitations. Finding a reasonable trade-off between privacy-preserving intensity, data utility and algorithm execution efficiency has always been the common goal of these methods  [73] [74] [75] . Regarding differential privacy as a privacy-enhancing technique to combine with mainstream privacy computing methods like federated learning can be considered and widely applied to distributed training of decentralized medical data in the future."
    },
    {
      "title": "Conclusion",
      "text": "Due to increasingly large scale and complex structure, medical data contains sensitive personal information inevitably, and the demand for privacy-preserving is particularly prominent. In this survey, we discussed the development of differential privacy and its applications to medical data. As a privacy computing method with strict mathematical limitations and various implementations, differential privacy is capable of solving the security and efficiency challenges during medical data publishing and mining. Moreover, this work provided a reliable environment and solution for medical data analysis. Finally, we discussed major challenges and future research directions about the medical data applications of differential privacy."
    }
  ],
  "references": [
    {
      "title": "Big data analytics in healthcare",
      "authors": [
        "A Belle",
        "R Thiagarajan",
        "S Soroushmehr"
      ],
      "year": 2015,
      "doi": "10.1155/2015/370194"
    },
    {
      "title": "Advances in big data analytics: theory, algorithms and practices",
      "authors": [
        "Y Shi"
      ],
      "year": 2022,
      "doi": "10.1007/978-981-16-3607-3_1"
    },
    {
      "title": "Internet of things, real-time decision making, and artificial intelligence",
      "authors": [
        "J Tien"
      ],
      "year": 2017,
      "doi": "10.1007/s40745-017-0112-5"
    },
    {
      "title": "Differential privacy for data and model publishing of medical data",
      "authors": [
        "Z Sun",
        "Y Wang",
        "M Shu"
      ],
      "year": 2019,
      "doi": "10.1109/access.2019.2947295"
    },
    {
      "title": "Block-DEF: a secure digital evidence framework using blockchain",
      "authors": [
        "Z Tian",
        "M Li",
        "M Qiu"
      ],
      "year": 2019,
      "doi": "10.1016/j.ins.2019.04.011"
    },
    {
      "title": "A distributed deep learning system for web attack detection on edge devices",
      "authors": [
        "Z Tian",
        "C Luo",
        "J Qiu"
      ],
      "year": 2019,
      "doi": "10.1109/tii.2019.2938778"
    },
    {
      "title": "Privacy preservation in big data: a survey",
      "authors": [
        "B Fang",
        "Y Jia",
        "A Li"
      ],
      "year": 2016,
      "doi": "10.11959/j.issn.2096-0271.2016001"
    },
    {
      "title": "Culture vs policy: more global collaboration to effectively combat COVID-19",
      "authors": [
        "J Li",
        "K Guo",
        "Herrera Viedma",
        "E Lee",
        "H Liu",
        "J Zhong",
        "Z Gomes",
        "L Filip",
        "F Fang",
        "S \u00d6zdemir",
        "M Liu",
        "X Lu",
        "G Shi"
      ],
      "year": 2020,
      "doi": "10.1016/j.xinn.2020.100023"
    },
    {
      "title": "What are the underlying transmission patterns of COVID-19 outbreak? An age-specific social contact characterization",
      "authors": [
        "Y Liu",
        "Z Gu",
        "S Xia",
        "B Shi",
        "X Zhou",
        "Y Shi",
        "J Liu"
      ],
      "year": 2020,
      "doi": "10.1016/j.eclinm.2020.100354"
    },
    {
      "title": "What country, university, or research institute, performed the best on Covid-19 during the first wave of the pandemic?",
      "authors": [
        "P Radanliev",
        "D De Roure"
      ],
      "year": 2022,
      "doi": "10.1007/s40745-022-00406-8"
    },
    {
      "title": "Data analysis of COVID-19 hospital records using contextual patient classification system",
      "authors": [
        "V Gada",
        "M Shegaonkar",
        "M Inamdar"
      ],
      "year": 2022,
      "doi": "10.1007/s40745-022-00378-9"
    },
    {
      "title": "Overview of the development of privacy preserving computing",
      "authors": [
        "S Yan",
        "A Lv"
      ],
      "year": 2021,
      "doi": "10.12267/j.issn.2096-5931.2021.06.001"
    },
    {
      "title": "Introduction to business data mining",
      "authors": [
        "D Olson",
        "Y Shi"
      ],
      "year": 2007,
      "doi": "10.1093/ww/9780199540884.013.u41645"
    },
    {
      "title": "Optimization based data mining: theory and applications",
      "authors": [
        "Y Shi",
        "Y Tian",
        "G Kou",
        "Y Peng",
        "J Li"
      ],
      "year": 2011,
      "doi": "10.1007/978-0-85729-504-0"
    },
    {
      "title": "k-anonymity: a model for protecting privacy",
      "authors": [
        "L Sweeney"
      ],
      "year": 2002,
      "doi": "10.1142/s0218488502001648"
    },
    {
      "title": "l-diversity: privacy beyond k-anonymity",
      "authors": [
        "A Machanavajjhala",
        "D Kifer",
        "J Gehrke",
        "M Venkitasubramaniam"
      ],
      "year": 2007,
      "doi": "10.1145/1217299.1217302"
    },
    {
      "title": "t-closeness: privacy beyond k-anonymity and l-diversity",
      "authors": [
        "N Li",
        "T Li",
        "S Venkatasubramanian"
      ],
      "year": 2007,
      "doi": "10.1109/icde.2007.367856"
    },
    {
      "title": "An information-driven genetic algorithm for privacy-preserving data publishing",
      "authors": [
        "Y Ge",
        "H Wang",
        "J Cao"
      ],
      "year": 2022,
      "doi": "10.1007/978-3-031-20891-1_24"
    },
    {
      "title": "DSGA: a distributed segment-based genetic algorithm for multiobjective outsourced database partitioning",
      "authors": [
        "Y Ge",
        "Z Zhan",
        "J Cao"
      ],
      "year": 2022,
      "doi": "10.1016/j.ins.2022.09.003"
    },
    {
      "title": "Differential privacy",
      "authors": [
        "C Dwork"
      ],
      "year": 2006,
      "doi": "10.1007/11787006_1"
    },
    {
      "title": "The algorithmic foundations of differential privacy",
      "authors": [
        "C Dwork",
        "A Roth"
      ],
      "year": 2014,
      "doi": "10.1561/0400000042"
    },
    {
      "title": "Deep learning with differential privacy",
      "authors": [
        "M Abadi",
        "A Chu"
      ],
      "year": 2016,
      "doi": "10.1145/2976749.2978318"
    },
    {
      "title": "Federated learning with gaussian differential privacy",
      "authors": [
        "Z Chuanxin",
        "S Yi",
        "W Degang"
      ],
      "year": 2020,
      "doi": "10.1145/3438872.3439097"
    },
    {
      "title": "Survey on local differential privacy",
      "authors": [
        "Q Ye",
        "X Meng",
        "M Zhu",
        "Z Huo"
      ],
      "year": 2018,
      "doi": "10.13328/j.cnki.jos.005364"
    },
    {
      "title": "Security and privacy analysis of mobile health applications: the alarming state of practice",
      "authors": [
        "A Papageorgiou",
        "M Strigkos",
        "E Politou"
      ],
      "year": 2018,
      "doi": "10.1109/access.2018.2799522"
    },
    {
      "title": "Local privacy and statistical minimax rates",
      "authors": [
        "J Duchi",
        "M Jordan",
        "M Wainwright"
      ],
      "year": 2013,
      "doi": "10.1109/focs.2013.53"
    },
    {
      "title": "A comprehensive survey on local differential privacy toward data statistics and analysis",
      "authors": [
        "T Wang",
        "X Zhang",
        "J Feng"
      ],
      "year": 2020,
      "doi": "10.3390/s20247030"
    },
    {
      "title": "Apple's 'differential privacy' is about collecting your data---but not your data",
      "authors": [
        "A Greenberg"
      ],
      "year": 2016,
      "doi": "10.4324/9781315693620-20"
    },
    {
      "title": "Rappor: randomized aggregatable privacy-preserving ordinal response",
      "authors": [
        "\u00da Erlingsson",
        "V Pihur",
        "A Korolova"
      ],
      "year": 2014,
      "doi": "10.1145/2660267.2660348"
    },
    {
      "title": "Privacy-preserving deep inference for rich user data on the cloud",
      "authors": [
        "S Osia",
        "A Shamsabadi",
        "A Taheri"
      ],
      "year": 2017,
      "doi": "10.48550/arXiv.1710.01727"
    },
    {
      "title": "Privacy-cnh: A framework to detect photo privacy with convolutional neural network using hierarchical features",
      "authors": [
        "L Tran",
        "D Kong",
        "H Jin",
        "J Liu"
      ],
      "year": 2016,
      "doi": "10.1609/aaai.v30i1.10169"
    },
    {
      "title": "iPrivacy: image privacy protection by identifying sensitive objects via deep multi-task learning",
      "authors": [
        "J Yu",
        "B Zhang",
        "Z Kuang",
        "D Lin"
      ],
      "year": 2016,
      "doi": "10.1109/tifs.2016.2636090"
    },
    {
      "title": "Privacy-preserving deep learning",
      "authors": [
        "R Shokri",
        "V Shmatikov"
      ],
      "year": 2015,
      "doi": "10.1145/2810103.2813687"
    },
    {
      "title": "Differentially private model publishing for deep learning",
      "authors": [
        "L Yu",
        "L Liu",
        "C Pu",
        "M Gursoy",
        "S Truex"
      ],
      "year": 2019,
      "doi": "10.1109/sp.2019.00019"
    },
    {
      "title": "Improving deep learning with differential privacy using gradient encoding and denoising",
      "authors": [
        "M Nasr",
        "R Shokri"
      ],
      "year": 2020,
      "doi": "10.48550/arXiv.2007.11524"
    },
    {
      "title": "Evaluating differentially private machine learning in practice",
      "authors": [
        "B Jayaraman",
        "D Evans"
      ],
      "year": 2019,
      "doi": "10.48550/arXiv.1902.08874"
    },
    {
      "title": "A critical review on the use (and misuse) of differential privacy in machine learning",
      "authors": [
        "A Blanco-Justicia",
        "D S\u00e1nchez",
        "J Domingo-Ferrer"
      ],
      "year": 2022,
      "doi": "10.1145/3547139"
    },
    {
      "title": "Tempered sigmoid activations for deep learning with differential privacy",
      "authors": [
        "N Papernot",
        "A Thakurta",
        "S Song",
        "S Chien",
        "\u00da Erlingsson"
      ],
      "year": 2020,
      "doi": "10.1609/aaai.v35i10.17123"
    },
    {
      "title": "Privacy and security in the genomic era",
      "authors": [
        "E Ayday",
        "J Hubaux"
      ],
      "year": 2016,
      "doi": "10.1145/2976749.2976751"
    },
    {
      "title": "Patient privacy in the genomic era",
      "authors": [
        "J Raisaro",
        "E Ayday",
        "J Hubaux"
      ],
      "year": 2014,
      "doi": "10.1024/1661-8157/a001657"
    },
    {
      "title": "Privacy in the genomic era",
      "authors": [
        "M Naveed",
        "E Ayday",
        "E Clayton"
      ],
      "year": 2015,
      "doi": "10.1145/2767007"
    },
    {
      "title": "Addressing the threats of inference attacks on traits and genotypes from individual genomic data",
      "authors": [
        "Z He",
        "Y Li",
        "J Li"
      ],
      "year": 2017,
      "doi": "10.1007/978-3-319-59575-7_20"
    },
    {
      "title": "Privacy-preserving data exploration in genome-wide association studies",
      "authors": [
        "A Johnson",
        "V Shmatikov"
      ],
      "year": 2013,
      "doi": "10.1145/2487575.2487687"
    },
    {
      "title": "Scalable privacy-preserving data sharing methodology for genome-wide association studies",
      "authors": [
        "F Yu",
        "S Fienberg",
        "A Slavkovi\u0107"
      ],
      "year": 2014,
      "doi": "10.1016/j.jbi.2014.01.008"
    },
    {
      "title": "Reconciling utility with privacy in genomics",
      "authors": [
        "M Humbert",
        "E Ayday",
        "J Hubaux"
      ],
      "year": 2014,
      "doi": "10.1145/2665943.2665945"
    },
    {
      "title": "Differential privacy with bounded priors: reconciling utility and privacy in genome-wide association studies",
      "authors": [
        "F Tram\u00e8r",
        "Z Huang",
        "J Hubaux"
      ],
      "year": 2015,
      "doi": "10.1145/2810103.2813610"
    },
    {
      "title": "Achieving differential privacy of genomic data releasing via belief propagation",
      "authors": [
        "Z He",
        "Y Li",
        "J Li"
      ],
      "year": 2018,
      "doi": "10.26599/TST.2018.9010037"
    },
    {
      "title": "Inference attacks against differentially private query results from genomic datasets including dependent tuples",
      "authors": [
        "N Almadhoun",
        "E Ayday",
        "\u00d6 Ulusoy"
      ],
      "year": 2020,
      "doi": "10.1093/bioinformatics/btaa475"
    },
    {
      "title": "Differential privacy under dependent tuples-the case of genomic privacy",
      "authors": [
        "N Almadhoun",
        "E Ayday",
        "\u00d6 Ulusoy"
      ],
      "year": 2020,
      "doi": "10.1093/bioinformatics/btz837"
    },
    {
      "title": "Genomic data sharing under dependent local differential privacy",
      "authors": [
        "E Yilmaz",
        "T Ji",
        "E Ayday"
      ],
      "year": 2022,
      "doi": "10.1145/3508398.3511519"
    },
    {
      "title": "Differential privacy protection against membership inference attack on machine learning for genomic data",
      "authors": [
        "J Chen",
        "W Wang",
        "X Shi"
      ],
      "year": 2020,
      "doi": "10.1142/9789811232701_0003"
    },
    {
      "title": "Research on differential privacy for medical health big data processing",
      "authors": [
        "Y Hu",
        "L Ge",
        "G Zhang",
        "D Qin"
      ],
      "year": 2019,
      "doi": "10.1109/pdcat46702.2019.00036"
    },
    {
      "title": "Differential private average publishing of numerical stream data for wearable devices",
      "authors": [
        "Z Tu",
        "S Liu",
        "X Xiong",
        "J Zhao",
        "Z Cai"
      ],
      "year": 2020,
      "doi": "10.11772/j.issn.1001-9081.2019111929"
    },
    {
      "title": "Privacy-preserving aggregation of personal health data streams",
      "authors": [
        "J Kim",
        "B Jang",
        "H Yoo"
      ],
      "year": 2018,
      "doi": "10.1371/journal.pone.0207639"
    },
    {
      "title": "Local differential privacy protection for wearable device data",
      "authors": [
        "Z Li",
        "B Wang",
        "J Li",
        "Y Hua",
        "S Zhang"
      ],
      "year": 2022,
      "doi": "10.1371/journal.pone.0272766"
    },
    {
      "title": "PPM-HDA: privacy-preserving and multifunctional health data aggregation with fault tolerance",
      "authors": [
        "S Han",
        "S Zhao",
        "Q Li"
      ],
      "year": 2015,
      "doi": "10.1109/tifs.2015.2472369"
    },
    {
      "title": "A differential privacy protection scheme for sensitive big data in body sensor networks",
      "authors": [
        "C Lin",
        "P Wang",
        "H Song"
      ],
      "year": 2016,
      "doi": "10.1007/s12243-016-0498-7"
    },
    {
      "title": "Privacy-preserving mhealth data release with pattern consistency",
      "authors": [
        "M Hadian",
        "X Liang",
        "T Altuwaiyan"
      ],
      "year": 2016,
      "doi": "10.1109/GLOCOM.2016.7842173"
    },
    {
      "title": "Differential privacy for eye tracking with temporal correlations",
      "authors": [
        "E Bozkir",
        "O G\u00fcnl\u00fc",
        "W Fuhl"
      ],
      "year": 2021,
      "doi": "10.1371/journal.pone.0255979"
    },
    {
      "title": "Privacy-preserved electronic medical record exchanging and sharing: a blockchain-based smart healthcare system",
      "authors": [
        "G Wu",
        "S Wang",
        "Z Ning"
      ],
      "year": 2021,
      "doi": "10.1109/jbhi.2021.3123643"
    },
    {
      "title": "DP-QIC: a differential privacy scheme based on quasi-identifier classification for big data publication",
      "authors": [
        "S Chen",
        "A Fu",
        "S Yu"
      ],
      "year": 2021,
      "doi": "10.1007/s00500-021-05692-7"
    },
    {
      "title": "Differential privacy medical data publishing method based on attribute correlation",
      "authors": [
        "S Zhang",
        "X Li"
      ],
      "year": 2022,
      "doi": "10.1038/s41598-022-19544-3"
    },
    {
      "title": "Medical imaging deep learning with differential privacy",
      "authors": [
        "A Ziller",
        "D Usynin",
        "R Braren"
      ],
      "year": 2021,
      "doi": "10.1038/s41598-021-93030-0"
    },
    {
      "title": "Collaborative deep learning for medical image analysis with differential privacy",
      "authors": [
        "D Yuan",
        "X Zhu",
        "M Wei"
      ],
      "year": 2019,
      "doi": "10.1109/globecom38437.2019.9014259"
    },
    {
      "title": "Federated learning and differential privacy for medical image analysis",
      "authors": [
        "M Adnan",
        "S Kalra",
        "J Cresswell"
      ],
      "year": 2022,
      "doi": "10.1038/s41598-022-05539-7"
    },
    {
      "title": "HGNAS++: efficient architecture search for heterogeneous graph neural networks",
      "authors": [
        "Y Gao",
        "P Zhang",
        "C Zhou"
      ],
      "year": 2023,
      "doi": "10.1109/tkde.2023.3239842"
    },
    {
      "title": "GraphNAS++: distributed architecture search for graph neural networks",
      "authors": [
        "Y Gao",
        "P Zhang",
        "H Yang"
      ],
      "year": 2022,
      "doi": "10.1109/tkde.2022.3178153"
    },
    {
      "title": "Drug package recommendation via interaction-aware graph induction",
      "authors": [
        "Z Zheng",
        "C Wang",
        "T Xu"
      ],
      "year": 2021,
      "doi": "10.1145/3442381.3449962"
    },
    {
      "title": "NPI-GNN: predicting ncRNA-protein interactions with deep graph neural networks",
      "authors": [
        "Z Shen",
        "T Luo",
        "Y Zhou"
      ],
      "year": 2021,
      "doi": "10.1093/bib/bbab051"
    },
    {
      "title": "DeepRank-GNN: a graph neural network framework to learn patterns in protein-protein interfaces",
      "authors": [
        "M R\u00e9au",
        "N Renaud",
        "L Xue"
      ],
      "year": 2023,
      "doi": "10.1093/bioinformatics/btac759"
    },
    {
      "title": "Heterogeneous graph neural network for privacy-preserving recommendation",
      "authors": [
        "Y Wei",
        "X Fu",
        "Q Sun"
      ],
      "year": 2022,
      "doi": "10.48550/arXiv.2210.00538"
    },
    {
      "title": "Locally private graph neural networks",
      "authors": [
        "S Sajadmanesh",
        "D Gatic-Perez"
      ],
      "year": 2021,
      "doi": "10.1145/3460120.3484565"
    },
    {
      "title": "MDDE: multitasking distributed differential evolution for privacy-preserving database fragmentation",
      "authors": [
        "Y Ge",
        "M Orlowska",
        "J Cao"
      ],
      "year": 2022,
      "doi": "10.1007/s00778-021-00718-w"
    },
    {
      "title": "Knowledge transfer-based distributed differential evolution for dynamic database fragmentation",
      "authors": [
        "Y Ge",
        "M Orlowska",
        "J Cao"
      ],
      "year": 2021,
      "doi": "10.1016/j.knosys.2021.107325"
    },
    {
      "title": "Distributed memetic algorithm for outsourced database fragmentation",
      "authors": [
        "Y Ge",
        "W Yu",
        "J Cao"
      ],
      "year": 2020,
      "doi": "10.1109/tcyb.2020.3027962"
    },
    {
      "title": "Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations",
      "doi": "10.1163/0000-0000_wtco_com_1006"
    }
  ],
  "num_references": 76
}
