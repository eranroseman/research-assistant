{
  "paper_id": "UNSKLN4R",
  "title": "Large Language Models for Time Series: A Survey",
  "abstract": "Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets in diverse domains, and discusses the challenges and future opportunities of this emerging field.",
  "year": 2023,
  "date": "2023",
  "journal": "Scientific data",
  "publication": "Scientific data",
  "authors": [
    {
      "forename": "Xiyuan",
      "surname": "Zhang",
      "name": "Xiyuan Zhang",
      "affiliation": "University of California , San Diego \n\t\t\t\t\t\t\t\t University of California \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t San Diego",
      "email": "xiyuanzh@ucsd.edu"
    },
    {
      "forename": "Roy",
      "surname": "Chowdhury",
      "name": "Roy Chowdhury",
      "affiliation": "University of California , San Diego \n\t\t\t\t\t\t\t\t University of California \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t San Diego"
    },
    {
      "forename": "Rajesh",
      "surname": "Gupta",
      "name": "Rajesh Gupta",
      "affiliation": "University of California , San Diego \n\t\t\t\t\t\t\t\t University of California \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t San Diego",
      "email": "rgupta@ucsd.edu"
    },
    {
      "forename": "Jingbo",
      "surname": "Shang",
      "name": "Jingbo Shang",
      "affiliation": "University of California , San Diego \n\t\t\t\t\t\t\t\t University of California \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t San Diego",
      "email": "jshang@ucsd.edu"
    }
  ],
  "doi": "https://doi.org/10.13039/100006785",
  "arxiv": "arXiv:2301.11325",
  "sections": [
    {
      "title": "Introduction",
      "text": "Time series analysis plays a critical role in a variety of fields, including climate modeling, traffic management, healthcare monitoring and finance analytics. Time series analysis comprises a wide range of tasks such as classification  [Liu et al., 2023b] , forecasting  [Gruver et al., 2023] , anomaly detection, and imputation. Traditionally, these tasks have been tackled using classical signal processing techniques such as time-frequency analysis and decomposition-based approaches. More recently, deep learning approaches like Convolutional Neural Networks (CNNs), Long Short-Term Memory networks (LSTMs)  [Zhang et al., 2023a] , and Transformers  [Jin et al., 2023a]  have revolutionized this field and proved effective in extracting meaningful patterns from time series data, making them the primary approaches of time series analysis in various application domains.\n\nLLM Forecasting Classification Text Generation Anomaly Detection EEG Finance Traffic Audio Multiple Tasks Diverse Domains\n\nTable IoT Robotics ECG Interpolation Time Series Generation In recent years, Large Language Models (LLMs) have gained substantial attention particularly in the fields of Natural Language Processing (NLP) and Computer Vision (CV). Prominent models such as GPT-4 have transformed the landscape of text processing by offering unprecedented accuracy in tasks such as text generation, translation, sentiment analysis, question answering and summarization. In the CV domain, Large Multimodal Models (LMMs) have also facilitated advancements in image recognition, object detection, and generative tasks, leading to more intelligent and capable visual systems  [Girdhar et al., 2023] . Inspired by these successes, researchers are now exploring the potential of LLMs in the realm of time series analysis, expecting further breakthroughs, as shown in Figure  1 . While several surveys offer a broad perspective on large models for time series in general  [Jin et al., 2023b; Ma et al., 2023] , these do not specifically focus on LLMs or the key challenge of bridging modality gap, which stems from LLMs being originally trained on discrete textual data, in contrast to the continuous numerical nature of time series.\n\nOur survey uniquely contributes to the existing literature by emphasizing how to bridge such modality gap and transfer knowledge from LLMs for time series analysis. Our survey also covers more diverse application domains, ranging from climate, Internet of Things (IoT), to healthcare, traffic management, and finance. Moreover, certain intrinsic properties of time series, like continuity, auto-regressiveness, and dependency on the sampling rate, are also shared by audio, speech, and music data. Therefore, we also present representative LLM-based works from these domains to explore how we can use LLMs for other types of time series. We present a comprehensive taxonomy by categorizing these methodologies into five distinct groups, as shown in Figure 2. If we outline typical LLM-driven NLP pipelines in five stages -input text, tokenization, embedding, LLM, output -then each category of our taxonomy targets one specific stage in this pipeline. Specifically, (i) Prompting (input stage) treats time series data as raw text and directly prompts LLMs with time series; (ii) Time Series Quantization (tokenization stage) discretizes time series as special tokens for LLMs to process; (iii) Aligning (embedding stage) designs time series encoder to align time series embeddings with language space; (iv) Vision as Bridge (LLM stage) connects time series with Vision-Language Models (VLM) by employing visual representations as a bridge; (v) Tool Integration (output stage) adopts LLMs to output tools to benefit time series analysis. Beyond this taxonomy, our survey also compiles an extensive list of existing multimodal datasets that incorporate both time series and text. We conclude our paper by discussing future research directions in this emerging and promising field.\n\nWe maintain an up-to-date Github repository 1 which includes all the papers and datasets discussed in the survey."
    },
    {
      "title": "Background and Problem Formulation",
      "text": "Large language models are characterized by their vast number of parameters and extensive training data. They excel in understanding, generating, and interpreting human language, and recently represent a significant advancement in artificial intelligence. The inception of LLMs can be traced back to models like GPT-2, BERT, BART, and T5, which laid the foundational architecture. Over time, the evolution of these models has been marked by increasing complexity and capabilities, such as LLAMA-2, PaLM, and GPT-4. More recently, researchers have developed multimodal large language models to integrate and interpret multiple forms of data, such as text, images, and time series, to achieve a more comprehensive understanding of information.\n\n1  https://github.com/xiyuanzh/awesome-llm-time-series This survey focuses on how LLMs could benefit time series analysis. We first define the mathematical formulation for the input and output, which may contain time series or (and) text depending on the downstream tasks, as well as the models. Input. Denoted as x, composed of time series x s \u2208 R T \u00d7c and optional text data x t represented as strings, where T, c represent the sequence length and the number of features. Output. Denoted as y and may represent time series, text or numbers depending on the specific downstream task. For time series generation or forecasting task, y represents generated time series y s or predicted k-step future time series y T +1:T +k s . For text generation task, such as report generation, y represents text data y t . For time series classification or regression task, y represents numbers indicating the predicted classes or numerical values. Model. We use f \u03b8 parameterized by \u03b8, g \u03d5 parameterized by \u03d5, and h \u03c8 parameterized by \u03c8 to represent language, time series and vision models, where f \u03b8 is typically initialized from pre-trained large language models. We optimize parameters \u03b8, \u03d5 and \u03c8 through loss function L."
    },
    {
      "title": "Taxonomy",
      "text": "In this section, we detail our taxonomy of applying LLMs for time series analysis, categorized by five groups. We summarize the representative works, mathematical formulation, advantages and limitations of each category in Table  1 ."
    },
    {
      "title": "Prompting",
      "text": "Number-Agnostic Tokenization. The method treats numerical time series as raw textual data and directly prompts existing LLMs. For example, PromptCast  [Xue and Salim, 2022]  proposes prompt-based time series forecasting by converting numerical time series into text prompts and forecasting time series in a sentence-to-sentence manner. The input prompts are composed of context and questions following pre-defined templates, e.g., \"From {t 1 } to {t obs }, the average temperature of region {U m } was {x m t } degree on each\n\nTime Series Encoder 1 2 3 \u2026 K 2 i 3 K Time Series Decoder Codebook D D D Embedding (a) VQ-VAE based quantization method. Feature Extraction D Embedding 2 1 3 1 K-Means Masking Encoding (b) K-Means based quantization method. Figure 3: Two types of index-based quantization methods. day. What is the temperature going to be on {t obs }?\" Similar prompting methods have been applied to forecast Placeof-Interest (POI) customer flows (AuxMobLCast) and user's next location (LLM-Mob). Recent works also prompt PaLM-24B for health-related tasks such as activity recognition and daily stress estimate [Liu et al., 2023b]. For example, they prompt the model to \"classify the following accelerometer data in meters per second squared as either walking or running: 0.052, 0.052, 0.052, 0.051, 0.052, 0.055, 0.051, 0.056, 0.06, 0.064\". Other examples include extracting historical price features such as open, close, high, and low prices to prompt ChatGPT in a zero-shot fashion [Xie et al., 2023a].\n\nNumber-Specific Tokenization. More recently, LLM-Time  [Gruver et al., 2023]  pointed out that Byte Pair Encoding (BPE) tokenization has the limitation of breaking a single number into tokens that don't align with the digits, leading to inconsistent tokenization across different floating point numbers and complicating arithmetic operations. Therefore, following LLMs such as LLaMA and PaLM, they propose to insert spaces between digits to ensure distinct tokenization of each digit and use a comma (\",\") to separate each time step in a time series. They also scale time series to optimize token usage and keep fixed precision (e.g., two digits of precision) to efficiently manage context length. For example, they convert \"0.123, 1.23, 12.3, 123.0\" to \"1 2 , 1 2 3 , 1 2 3 0 , 1 2 3 0 0\". Meanwhile, BloomberGPT  [Wu et al., 2023]  trains on financial data with text and numerical data and places each digit in its own chunk to better handle numbers. Using similar space-prefixed tokenization, recent works also show that large language models are general pattern machines capable of sequence transformation, completion and improvement."
    },
    {
      "title": "Quantization",
      "text": "Quantization based method converts numerical data into discrete representations as input to LLMs. This approach can be further divided into two main categories based on the discretization technique employed.\n\nDiscrete Indices from VQ-VAE. The first type of quantization method transforms continuous time series into discrete indices as tokens. Among them one of the most popular methods is training a Vector Quantized-Variational AutoEncoder (VQ-VAE), which learns a codebook\n\nto capture the latent representations, as illustrated in Figure  3a . The method identifies the nearest neighbor k i of each step i of the encoded time series representation g \u03d5 (x s ) \u2208 R T S \u00d7D in the codebook (S denotes the cumulative stride of VQ-VAE encoder), and uses the corresponding indices k as the quantized input to language models:\n\nBased on VQ-VAE, Auto-TTE  [Chung et al., 2023]  quantizes ECGs into discrete formats and generates 12-lead ECG signals conditioned on text reports. DeWave  [Duan et al., 2023]  adapts VQ-VAE to derive discrete codex encoding and aligns it with pre-trained BART for open-vocabulary EEGto-text translation tasks. TOTEM  [Talukder and Gkioxari, 2023 ] also quantizes time series through VQ-VAE as input to Transformers for multiple downstream applications such as forecasting, classification, and translation. In the audio domain, UniAudio  [Yang et al., 2023]  tokenizes different types of target audio using Residual Vector Quantization (RVQ) (a hierarchy of multiple vector quantizers) and supports 11 audio generation tasks. VioLA unifies various crossmodal tasks involving speech and text by converting speech utterances to discrete tokens through RVQ. AudioGen learns discrete audio representations using vector quantization layers and generates audio samples conditioned on text inputs. Discrete Indices from K-Means. Apart from employing VQ-VAE, researchers have also explored K-Means clustering for index-based tokenization, which uses the centroid indices as discretized tokens, as shown in Figure  3b . Such methods are mostly applied in the audio domain. For example, SpeechGPT shows capability to perceive and generate multimodal contents using K-Means based discrete unit extractor. AudioLM discretizes codes produced by a neural audio codec using K-means clustering to achieve high-quality synthesis. It also combines discretized activations of language models pre-trained on audio using RVQ to capture long-term structure. Following the same quantization procedure, Au-dioPaLM  [Rubenstein et al., 2023]  aligns PaLM-2 and Au-dioLM with a joint vocabulary that can represent speech and text with discrete tokens. Discrete Indices from Other Techniques. Apart from the VQ-VAE and K-Means based time-domain quantization, Fre-qTST  [Li et al., 2023]  utilizes frequency spectrum as a common dictionary to discretize time series into frequency units with weights for downstream forecasting task. Text Categories. The second type of quantization converts numerical data into pre-defined text categories, which is primarily adopted in financial domain. For example, TDML  [Yu et al., 2023]  categorizes the weekly price fluctuations into 12 bins represented as \"Di\" or \"Ui\", where \"D\" indicates a decrease in price and \"U\" means an increase, and i = 1, 2, 3, 4, 5, 5+ represents the level of price change."
    },
    {
      "title": "Aligning",
      "text": "The third type of works trains a separate encoder for time series, and aligns the encoded time series to the semantic space\n\nTime Series Encoder Text Encoder Similarity LLM Matching (a) Aligning by similarity matching (Type one). Time Series Embed Text Embed Encoder Decoder Time Series Output Text Output LLM Prompts\n\n(b) Aligning with large language models as backbones (Type two), where the output could be time series (e.g., forecasting) or text (e.g., EEG-to-text) depending on the downstream tasks. of language models. These works can be further categorized into two groups based on their specific aligning strategies, as illustrated in Figure  4 .\n\nSimilarity Matching through Contrastive Loss. The first type of method aligns the time series embeddings with text embeddings through similarity matching, such as minimizing the contrastive loss:\n\n(2) where B, \u03b3 represent batch size and temperature parameter that controls distribution concentrations, and sim represents similarity score, typically computed as inner product:\n\n(3)\n\nFor instance, ETP  [Liu et al., 2023a]  integrates contrastive learning based pre-training to align electrocardiography (ECG) signals with textual reports. Contrastive framework is also used to align 17 clinical measurements collected in Intensive Care Unit (ICU) to their corresponding clinical notes  [King et al., 2023] . TEST  [Sun et al., 2023]  uses contrastive learning to generate instance-wise, feature-wise, and text-prototype-aligned time series embeddings to align with text embeddings. TENT  [Zhou et al., 2023b ] aligns text embeddings with IoT sensor signals through a unified semantic space using contrastive learning. JoLT  [Cai et al., 2023]  utilizes Querying Transformer (Q-Former) optimized with contrastive loss to align time series and text representations.\n\nSimilarity Matching through Other Losses. Apart from contrastive loss, other loss functions are also employed to optimize similarity matching between time series embeddings and text embeddings. ECG-LLM  [Qiu et al., 2023]  aligns the distribution between ECG and language embedding from ECG statements with an Optimal Transport based loss function to train an ECG report generation model. MTAM  [Han et al., 2022]  uses various aligning techniques, such as Canonical Correlation Analysis and Wasserstein Distance, as loss functions to align electroencephalography (EEG) features with their corresponding language descriptions. LLMs as Backbones. The second type of aligning method directly uses large language models as backbones following time series embedding layers. EEG-to-Text [Wang and  Ji, 2022]  feeds EEG embeddings to pre-trained BART for open vocabulary EEG-To-Text decoding and EEG-based sentiment classification. GPT4TS  [Zhou et al., 2023a]  uses patching embeddings as input to frozen pre-trained GPT-2 where the positional embedding layers and self-attention blocks are retained during time series fine-tuning. The method provides a unified framework for seven time series tasks, including few-shot or zero-shot learning. Following GPT4TS, researchers further incorporated seasonal-trend decomposition (TEMPO  [Cao et al., 2023] ), two-stage finetuning (LLM4TS  [Chang et al., 2023] ), domain descriptions (UniTime), graph attention mechanism (GATGPT), and spatial-temporal embedding module (ST-LLM). Time-LLM  [Jin et al., 2023a]  reprograms time series data into text prototypes as input to LLaMA-7B. It also provides natural language prompts such as domain expert knowledge and task instructions to augment input context. Lag-Llama builds univariate probabilistic time series forecasting model based on LLaMA architecture. In the audio, speech and music domains, researchers have also designed dedicated encoders to embed speech (WavPrompt, Speech LLaMA), music (MU-LLaMA), and general audio inputs (LTU  [Gong et al., 2023] , SALMONN  [Tang et al., 2023] ), and feed the embeddings to large language models."
    },
    {
      "title": "Vision as Bridge",
      "text": "Time series data can be effectively interpreted or associated with visual representations, which align closer with textual data and have demonstrated successful integrations with large language models. Therefore, researchers have also leveraged vision modality as a bridge to connect time series with LLMs. Paired Data. ImageBind  [Girdhar et al., 2023]  uses imagepaired data to bind six modalities (images, text, audio, depth, thermal, and Inertial Measurement Unit (IMU) time series) and learn a joint embedding space, enabling new emergent alignments and capabilities. PandaGPT  [Su et al., 2023]  further combines the multimodal encoders from ImageBind and large language models to enable visual and auditory instruction-following capabilities. IMU2CLIP  [Moon et al., 2022]  aligns IMU time series with video and text, by projecting them into the joint representation space of Contrastive Language-Image Pre-training (CLIP). AnyMAL  [Moon et al., 2023]  builds upon IMU2CLIP by training a lightweight adapter to project the IMU embeddings into the text token embedding space of LLaMA-2-70B. It is also capable of transforming data from other modalities, such as images, videos, audio, into the same text embedding space. Physics Relationships. IMUGPT  [Leng et al., 2023]   Time Series Plots as Images. CLIP-LSTM  [Wimmer and Rekabsaz, 2023]  transforms stock market data into sequences of texts and images of price charts, and leverages pre-trained CLIP vision-language model to generate features for downstream forecasting. Insight Miner  [Zhang et al., 2023b]  converts time series windows into images using lineplot, and feeds images into vision language model LLaVA to generate time series trend descriptions."
    },
    {
      "title": "Tool",
      "text": "This type of method does not directly use large language models to process time series. Instead, it applies large language models to generate indirect tools z(\u2022), such as code and API calls, to benefit time series related tasks.\n\nCode. CTG++  [Zhong et al., 2023]  applies GPT-4 to generate differentiable loss functions in a code format from text descriptions to guide the diffusion model to generate traffic trajectories. With this two-step translation, the large language model and diffusion model efficiently bridge the gap between user intent and traffic simulation. API Call. ToolLLM  [Qin et al., 2023]  introduces a general tool-use framework composed of data construction, model training, and evaluation. This framework includes API calls for time series tasks such as weather and stock forecasting. Text Domain Knowledge. SHARE [Zhang et al., 2023a] exploits the shared structures in human activity label names and proposes a sequence-to-sequence structure to generate label names as token sequences to preserve the shared label structures. It applies GPT-4 to augment semantics of label names. GG-LLM [Graule and Isler, 2023] leverages LLaMA-2 to encode world knowledge of common human behavioral patterns to predict human actions without further training. SCRL-LG [Ding et al., 2023] leverages LLaMA-7B as stock feature selectors to extract meaningful representations from news headlines, which are subsequently employed in reinforcement learning for precise feature alignments."
    },
    {
      "title": "Comparison within the Taxonomy",
      "text": "We compare the five categories of our taxonomy and provide general guidelines for which category to choose based on considerations of data, model, efficiency and optimization.\n\nData. When no training data is available and the objective is to apply LLM for time series in an zero-shot fashion, it is preferable to use prompting-based methods. This is because direct prompting enables the utilization of pre-trained language models' inherent capabilities without fine-tuning. However, representing numbers as strings can diminish the semantic value intrinsically tied to numerical data. Therefore, with adequate training data, quantization or aligningbased methods become more advantageous. As shown in Figure  2 , these two categories are the most extensively studied ones in existing literature. Furthermore, if time series data can be interpreted or associated with visual representations, these representations can be incorporated to utilize the intrinsic knowledge embedded in the vision modality or pre-trained vision-language models.\n\nModel. Prompting and tool integration methods tend to apply billion-parameter models as they often apply off-theself LLMs without architectural modifications. By contrast, aligning and quantization methods vary from million to billion-parameter models, depending on the specific application requirements and available computational resources.\n\nEfficiency. Prompting-based methods are not efficient for numerical data with high precision, as well as multivariate time series as it requires transforming each dimension into separate univariate time series, resulting in extremely long input. They are also less efficient for long-term predictions due to the computational demands of generating long sequences. These methods are more effective when dealing with simple numerical data that is richly interwoven with textual information, such as opening and closing stock prices in financial news articles. By contrast, quantization and aligning meth- ods are more efficient to handle long sequences, as time series are typically down-sampled or segmented into patches before feeding into large language models.\n\nOptimization. Depending on the specific discretization technique, quantization based method may require a twostage training process (such as first training the VQ-VAE model), which may result in sub-optimal performance compared with that achieved through end-to-end training in aligning methods. Using large language models as indirect tools empowers LLMs with more capabilities to manage numerical data, but also raises the level of complexity to optimize both LLMs and other components in an end-to-end fashion. Therefore, existing works of tool integration typically employ off-the-shelf LLMs without further fine-tuning."
    },
    {
      "title": "Multimodal Datasets",
      "text": "Applying LLMs for time series benefits from the availability of multimodal time series and text data. In this section, we introduce representative multimodal datasets organized by their respective domains (Table  2 ). Due to space limit, additional datasets are listed in our Github repository foot_11  .\n\nInternet of Things (IoT). Human activity recognition is an important task in IoT domain, which identifies human activities given time series collected with IoT devices (such as IMU sensors). The corresponding text data are the labels or text descriptions of these activities. Ego4D  [Grauman et al., 2022]  presents 3,670 hours of daily-life activity data with multiple modalities, including IMU time series, and dense temporallyaligned textual descriptions of the activities. Ego-Exo4D further offers three kinds of paired natural language datasets including expert commentary, narrate-and-act descriptions provided by the participants themselves, and atomic action descriptions similar as Ego4D. DeepSQA  [Xing et al., 2021]  presents a generalized Sensory Question Answering (SQA) framework to facilitate querying raw sensory data related to human activities using natural language.\n\nFinance. PIXIU  [Xie et al., 2023b]  presents multi-task and multi-modal instruction tuning data in the financial domain with 136K data samples. It contains both financial natural language understanding and prediction tasks, and covers 9 datasets of multiple modalities such as text and time series. MoAT  [Lee et al., 2023]  constructs multimodal datasets with textual information paired with time series for each timestep, such as news articles extracted with relevant keywords, mostly covering finance related domains such as fuel, metal, stock and bitcoin.\n\nHealthcare. Zuco datasets  [Hollenstein et al., 2019]  contain simultaneous eye-tracking and EEG during natural reading and during annotation. PTB-XL  [Wagner et al., 2020]  offers comprehensive metadata regarding ECG annotated by expert cardiologists, covering information such as ECG reports, diagnostic statements, diagnosis likelihoods, and signal-specific properties. Based on PTB-XL, ECG-QA  [Oh et al., 2023]  introduces the first Question Answering dataset for ECG analysis, containing 70 question templates that cover a wide range of clinically relevant ECG topics.\n\nAudio/Music/Speech. AudioSet is a collection of 2 million 10-second audio clips excised from YouTube videos and labeled with the sounds that the clip contains from a set of 527 labels. OpenAQA-5M  [Gong et al., 2023]  dataset consists of 1.9 million closed-ended and 3.7 million open-ended, diverse (audio, question, answer) tuples. MusicCaps  [Agostinelli et al., 2023]  is a high-quality music caption dataset, including 5.5K music clips. MTG-Jamendo is a dataset with 55,000 audio songs in various languages. Libri-Light is an English dataset encompassing 60,000 hours of speech data. Common-Voice  [Ardila et al., 2019]  is a multilingual speech dataset consisting of 7,335 validated hours in 60 languages. These datasets offer valuable benchmarks for multimodal time series and text analysis. These contain both time series focused tasks, including classification, which is evaluated using accuracy and macro-F1 scores, and forecasting, which utilizes metrics such as MSE, MAE, RMSE, and MAPE, as well as NLP focused tasks such as captioning, question answering, and translation, assessed through BLEU, ROUGE, METEOR, and EM scores, among others.\n\n6 Challenges and Future Directions"
    },
    {
      "title": "Theoretical Understanding",
      "text": "Existing works empirically show the benefits of applying LLMs for time series analysis. For example, recent works have empirically shown that large language models learn linear representations of space and time across multiple scales that are robust to prompting variations. Despite these empirical findings, there remains a gap in theoretical understanding of how models, primarily trained on textual data, can effectively interpret numerical time series. As a preliminary theoretical analysis, it is proved that Transformer models can universally approximate arbitrary continuous sequenceto-sequence functions on a compact domain  [Yun et al., 2019] . Additionally, GPT4TS  [Zhou et al., 2023a]  theoretically shows that such generic capability of large language models can be related to Principal Component Analysis (PCA), as minimizing the gradient with respect to the self-attention layer shares similarities with PCA. Further investigations on the generalizability of large language models on numerical data is essential to establish solid understanding of the synergy between LLMs and time series analysis."
    },
    {
      "title": "Multimodal and Multitask Analysis",
      "text": "Existing papers that apply LLMs for time series analysis mostly focus on single modality and single task at a time, such as forecasting, classification, text generation, and do not support simultaneous multimodal and multitask analysis. In computer vision and audio domains, models such as Unified-IO and UniAudio  [Yang et al., 2023]  have unified multiple input modalities into a sequence of discrete vocabulary tokens to support multiple tasks within a single transformer-based architecture. More research into leveraging LLMs for multimodal and multitask analysis would lead to more powerful time series foundation models."
    },
    {
      "title": "Efficient Algorithms",
      "text": "Time series, especially those that are multivariate or possess long history information may increase the computational complexity for existing large language models. Patching (treating each segmented patch as a token) has been a widely adopted strategy to improve performance as well as reduce complexity, but large patches may obscure the semantic information of time series and negatively impact the performance. Therefore, developing more efficient algorithms is especially crucial for facilitating large-scale time series analysis with LLMs and enhancing interactions with end users."
    },
    {
      "title": "Combining Domain Knowledge",
      "text": "Combining existing statistical domain knowledge with LLMs may further boost the model's capability for time series analysis. For example, TEMPO  [Cao et al., 2023]  applies time series seasonal-trend decomposition and treats decomposed components as different semantic inductive biases as input to the pre-trained transformer. FreqTST  [Li et al., 2023]  leverages insights from the frequency domain by tokenizing single time series into frequency units with weights for downstream forecasting. Further incorporating domain knowledge, such as wavelet decomposition, auto-correlation analysis, and empirical mode decomposition may augment LLMs' capabilities in analyzing time series data."
    },
    {
      "title": "Customization and Privacy",
      "text": "Existing works on large language models and time series analysis typically train a global model for all end users. Training customized models for different users based on the global model may bring further benefits and flexibility. Another important consideration is privacy, especially as many time series data are collected in private settings for clinical purposes or smart home applications. Federated learning offers a solution by enabling the training of machine learning models across multiple decentralized devices holding local data samples. Advancing research into model customization and user privacy preservation like federated learning would broaden the utility of LLM-empowered time series analysis."
    },
    {
      "title": "Conclusion",
      "text": "We present the first survey that systematically analyzes the categorization of transferring knowledge from large language models for numerical time series analysis: direct prompting, time series quantization, aligning, the use of the vision modality to connect text and time series, and the integration of large language models with other analytical tools. For each category, we introduce their mathematical formulation, representative works, and compare their advantages and limitations. We also introduce representative multimodal text and time series datasets in various domains such as healthcare, IoT, finance, and audio. Concluding the paper, we outline the challenges and emerging directions for potential future research of LLM-empowered time series analysis."
    },
    {
      "text": "Figure1: Large language models have recently been applied for various time series tasks in diverse application domains."
    },
    {
      "text": "Figure 2: Left: Taxonomy of LLMs for time series analysis (prompting, quantization, aligning which is further categorized into two groups as detailed in Figure 4, vision as bridge, tool integration). For each category, key distinctions are drawn in comparison to the standard LLM pipeline shown at the top of the figure. Right: We present representative works for each category, sorted by their publication dates. The use of arrows indicates that later works build upon earlier studies. Dark(light)-colored boxes represent billion(million)-parameter models. Icons to the left of the text boxes represent the application domains of domain-specific models, with icons' meanings illustrated in Figure 1."
    },
    {
      "text": "Figure 4: Two types of aligning based methods."
    },
    {
      "text": "generates IMU data from ChatGPT-augmented text descriptions.It first generates 3D human motion from text using pretrained motion synthesis model, and derives IMU data from 3D motion based on physics relationships of motion kinetics.Summary of five major categories of applying LLMs for time series analysis, including their respective subcategories, representative works, mathematical formulations, advantages and limitations. q and xv represent text-based quantization process and image data."
    },
    {
      "text": "Summary of representative time series and text multimodal datasets."
    }
  ],
  "references": [
    {
      "authors": [
        "Agostinelli"
      ],
      "year": 2023
    },
    {
      "title": "Musiclm: Generating music from text",
      "authors": [
        "Andrea Agostinelli",
        "I Timo",
        "Zal\u00e1n Denk",
        "Jesse Borsos",
        "Mauro Engel",
        "Antoine Verzetti",
        "Qingqing Caillon",
        "Aren Huang",
        "Adam Jansen",
        "Marco Roberts",
        "Tagliasacchi"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Ardila"
      ],
      "year": 2019
    },
    {
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": 2019
    },
    {
      "authors": [
        "Cai"
      ],
      "year": 2023,
      "doi": "10.5194/tc-2020-91-rc1"
    },
    {
      "title": "Jolt: Jointly learned representations of language and timeseries",
      "authors": [
        "Yifu Cai",
        "Mononito Goswami",
        "Arjun Choudhry",
        "Arvind Srinivasan",
        "Artur Dubrawski"
      ],
      "year": 2023,
      "doi": "10.1609/aaai.v38i21.30423"
    },
    {
      "authors": [
        "Cao"
      ],
      "year": 2023,
      "doi": "10.5194/acp-2016-553-rc2"
    },
    {
      "title": "Tempo: Prompt-based generative pre-trained transformer for time series forecasting",
      "authors": [
        "Defu Cao",
        "Furong Jia",
        "O Sercan",
        "Tomas Arik",
        "Yixiang Pfister",
        "Wen Zheng",
        "Yan Ye",
        "Liu"
      ],
      "year": 2023
    },
    {
      "year": 2023
    },
    {
      "title": "Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms",
      "authors": [
        "Ching Chang",
        "Wen-Chih Peng",
        "Tien-Fu Chen"
      ],
      "year": 2023,
      "doi": "10.1145/3719207"
    },
    {
      "year": 2023
    },
    {
      "title": "Text-to-ecg: 12-lead electrocardiogram synthesis conditioned on clinical text reports",
      "authors": [
        "Hyunseung Chung",
        "Jiho Kim",
        "Joonmyoung Kwon",
        "Ki-Hyun Jeon",
        "Min Lee",
        "Edward Choi"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Ding"
      ],
      "year": 2023
    },
    {
      "title": "Integrating stock features and global information via large language models for enhanced stock return prediction",
      "authors": [
        "Yujie Ding",
        "Shuai Jia",
        "Tianyi Ma",
        "Bingcheng Mao",
        "Xiuze Zhou",
        "Liuliu Li",
        "Dongming Han"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Duan"
      ],
      "year": 2023
    },
    {
      "title": "Dewave: Discrete encoding of eeg waves for eeg to text translation",
      "authors": [
        "Yiqun Duan",
        "Charles Zhou",
        "Zhen Wang",
        "Yu-Kai Wang",
        "Chin-Teng Lin"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Girdhar"
      ],
      "year": 2023
    },
    {
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyan Vasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": 2023,
      "doi": "10.1109/cvpr52729.2023.01457"
    },
    {
      "authors": [
        "Gong"
      ],
      "year": 2023,
      "doi": "10.5194/acp-2018-376-rc1"
    },
    {
      "title": "Listen, think, and understand",
      "authors": [
        "Yuan Gong",
        "Hongyin Luo",
        "Alexander Liu",
        "Leonid Karlinsky",
        "James Glass"
      ],
      "year": 2023,
      "doi": "10.1109/asru57964.2023.10389742"
    },
    {
      "authors": [
        "Isler Graule"
      ],
      "year": 2023
    },
    {
      "title": "Gg-llm: Geometrically grounding large language models for zero-shot human activity forecasting in human-aware task planning",
      "authors": [
        "A Moritz",
        "Volkan Graule",
        "Isler"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Grauman"
      ],
      "year": 2022,
      "doi": "10.7717/peerj.19588/fig-2"
    },
    {
      "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
      "authors": [
        "Kristen Grauman",
        "Andrew Westbury",
        "Eugene Byrne",
        "Zachary Chavis",
        "Antonino Furnari",
        "Rohit Girdhar",
        "Jackson Hamburger",
        "Hao Jiang",
        "Miao Liu",
        "Xingyu Liu"
      ],
      "year": 2022
    },
    {
      "authors": [
        "Gruver"
      ],
      "year": 2023
    },
    {
      "title": "Large language models are zero-shot time series forecasters",
      "authors": [
        "Nate Gruver",
        "Marc Finzi",
        "Shikai Qiu",
        "Andrew Gordon"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Han"
      ],
      "year": 2022,
      "doi": "10.1016/s1369-7021(22)00193-6"
    },
    {
      "title": "An empirical exploration of cross-domain alignment between language and electroencephalogram",
      "authors": [
        "William Han",
        "Jielin Qiu",
        "Jiacheng Zhu",
        "Mengdi Xu",
        "Douglas Weber",
        "Bo Li",
        "Ding Zhao"
      ],
      "year": 2022
    },
    {
      "authors": [
        "Hollenstein"
      ],
      "year": 2019,
      "doi": "10.7717/peerj.19053/fig-3"
    },
    {
      "title": "Zuco 2.0: A dataset of physiological recordings during natural reading and annotation",
      "authors": [
        "Nora Hollenstein",
        "Marius Troendle",
        "Ce Zhang",
        "Nicolas Langer"
      ],
      "year": 2019
    },
    {
      "title": "Time-llm: Time series forecasting by reprogramming large language models",
      "year": 2023
    },
    {
      "title": "Large models for time series and spatio-temporal data: A survey and outlook",
      "year": 2023
    },
    {
      "authors": [
        "King"
      ],
      "year": 2023
    },
    {
      "title": "Multimodal pretraining of medical time series and notes",
      "authors": [
        "Ryan King",
        "Tianbao Yang",
        "Bobak Mortazavi"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Lee"
      ],
      "year": 2023
    },
    {
      "title": "Moat: Multi-modal augmented time series forecasting",
      "authors": [
        "Geon Lee",
        "Wenchao Yu",
        "Wei Cheng",
        "Haifeng Chen"
      ],
      "year": 2023
    },
    {
      "title": "Generating virtual on-body accelerometer data from virtual textual descriptions for human activity recognition",
      "authors": [
        "Leng"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Li"
      ],
      "year": 2023
    },
    {
      "title": "Modeling time series as text sequence a frequencyvectorization transformer for time series forecasting",
      "authors": [
        "Junkai Li",
        "Weizhi Ma",
        "Yang Liu"
      ],
      "year": 2023
    },
    {
      "title": "Etp: Learning transferable ecg representations via ecg-text pre-training",
      "authors": [
        "Liu"
      ],
      "year": 2023
    },
    {
      "title": "Large language models are few-shot health learners",
      "authors": [
        "Liu"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Ma"
      ],
      "year": 2023
    },
    {
      "title": "A survey on time-series pre-trained models",
      "authors": [
        "Qianli Ma",
        "Zhen Liu",
        "Zhenjing Zheng",
        "Ziyang Huang",
        "Siying Zhu",
        "Zhongzhong Yu",
        "James Kwok"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Moon"
      ],
      "year": 2022
    },
    {
      "title": "Imu2clip: Multimodal contrastive learning for imu motion sensors from egocentric videos and text",
      "authors": [
        "Seungwhan Moon",
        "Andrea Madotto",
        "Zhaojiang Lin",
        "Alireza Dirafzoon",
        "Aparajita Saraf",
        "Amy Bearman",
        "Babak Damavandi"
      ],
      "year": 2022
    },
    {
      "authors": [
        "Moon"
      ],
      "year": 2023
    },
    {
      "title": "Anymal: An efficient and scalable anymodality augmented language model",
      "authors": [
        "Seungwhan Moon",
        "Andrea Madotto",
        "Zhaojiang Lin",
        "Tushar Nagarajan",
        "Matt Smith",
        "Shashank Jain",
        "Chun-Fu Yeh",
        "Prakash Murugesan",
        "Peyman Heidari",
        "Yue Liu"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Oh"
      ],
      "year": 2023,
      "doi": "10.7717/peerj.18888/fig-2"
    },
    {
      "title": "Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram",
      "authors": [
        "Jungwoo Oh",
        "Seongsu Bae",
        "Gyubok Lee",
        "Joon-Myoung Kwon",
        "Edward Choi"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Qin"
      ],
      "year": 2023
    },
    {
      "title": "Toolllm: Facilitating large language models to master 16000+ real-world apis",
      "authors": [
        "Yujia Qin",
        "Shihao Liang",
        "Yining Ye",
        "Kunlun Zhu",
        "Lan Yan",
        "Yaxi Lu",
        "Yankai Lin",
        "Xin Cong",
        "Xiangru Tang",
        "Bill Qian"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Qiu"
      ],
      "year": 2023,
      "doi": "10.5194/gmd-2018-256-rc2"
    },
    {
      "title": "Transfer knowledge from natural language to electrocardiography: Can we detect cardiovascular disease through language models?",
      "authors": [
        "Jielin Qiu",
        "William Han",
        "Jiacheng Zhu",
        "Mengdi Xu",
        "Michael Rosenberg",
        "Emerson Liu",
        "Douglas Weber",
        "Ding Zhao"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Rubenstein"
      ],
      "year": 2023,
      "doi": "10.7717/peerj-cs.2869/table-1"
    },
    {
      "title": "Audiopalm: A large language model that can speak and listen",
      "authors": [
        "Chulayuth Paul K Rubenstein",
        "Asawaroengchai",
        "Dung Duc",
        "Ankur Nguyen",
        "Zal\u00e1n Bapna",
        "F\u00e9lix Borsos",
        "De Chaumont",
        "Peter Quitry",
        "Dalia Chen",
        "Wei Badawy",
        "Eugene Han",
        "Kharitonov"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Su"
      ],
      "year": 2023
    },
    {
      "title": "Pandagpt: One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Sun"
      ],
      "year": 2023,
      "doi": "10.7717/peerj.19824/fig-8"
    },
    {
      "title": "Test: Text prototype aligned embedding to activate llm's ability for time series",
      "authors": [
        "Chenxi Sun",
        "Yaliang Li",
        "Hongyan Li",
        "Shenda Hong"
      ],
      "year": 2023,
      "doi": "10.2139/ssrn.4460608"
    },
    {
      "authors": [
        "Gkioxari Talukder"
      ],
      "year": 2023
    },
    {
      "title": "Time series modeling at scale: A universal representation across tasks and domains",
      "authors": [
        "J Sabera",
        "Georgia Talukder",
        "Gkioxari"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Tang"
      ],
      "year": 2023,
      "doi": "10.5194/bg-2016-113-ac3"
    },
    {
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Wagner"
      ],
      "year": 2020
    },
    {
      "title": "Ptb-xl, a large publicly available electrocardiography dataset",
      "authors": [
        "Patrick Wagner",
        "Nils Strodthoff",
        "Ralf-Dieter Bousseljot",
        "Dieter Kreiseler",
        "Fatima Lunze",
        "Wojciech Samek",
        "Tobias Schaeffter"
      ],
      "year": 2020
    },
    {
      "title": "Open vocabulary electroencephalography-to-text decoding and zero-shot sentiment classification",
      "authors": [
        "Zhenhailong Wang",
        "Heng Ji"
      ],
      "year": 2022
    },
    {
      "authors": [
        "Rekabsaz Wimmer"
      ],
      "year": 2023
    },
    {
      "title": "Leveraging vision-language models for granular market change prediction",
      "authors": [
        "Christopher Wimmer",
        "Navid Rekabsaz"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Wu"
      ],
      "year": 2023,
      "doi": "10.5194/gmd-2017-220-rc2"
    },
    {
      "title": "Bloomberggpt: A large language model for finance",
      "authors": [
        "Shijie Wu",
        "Ozan Irsoy",
        "Steven Lu",
        "Vadim Dabravolski",
        "Mark Dredze",
        "Sebastian Gehrmann",
        "Prabhanjan Kambadur",
        "David Rosenberg",
        "Gideon Mann"
      ],
      "year": 2023
    },
    {
      "title": "The wall street neophyte: A zero-shot analysis of chatgpt over multimodal stock movement prediction challenges",
      "authors": [
        "Xie"
      ],
      "year": 2023
    },
    {
      "title": "Pixiu: A large language model, instruction data and evaluation benchmark for finance",
      "authors": [
        "Xie"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Xing"
      ],
      "year": 2021
    },
    {
      "title": "Deepsqa: Understanding sensor data via question answering",
      "authors": [
        "Tianwei Xing",
        "Luis Garcia",
        "Federico Cerutti",
        "Lance Kaplan",
        "Alun Preece",
        "Mani Srivastava"
      ],
      "year": 2021
    },
    {
      "authors": [
        "Salim Xue"
      ],
      "year": 2022
    },
    {
      "title": "Promptcast: A new prompt-based learning paradigm for time series forecasting",
      "authors": [
        "Hao Xue",
        "Flora Salim"
      ],
      "year": 2022
    },
    {
      "year": 2023
    },
    {
      "title": "Uniaudio: An audio foundation model toward universal audio generation",
      "authors": [
        "Dongchao Yang",
        "Jinchuan Tian",
        "Xu Tan",
        "Rongjie Huang",
        "Songxiang Liu",
        "Xuankai Chang",
        "Jiatong Shi",
        "Sheng Zhao",
        "Jiang Bian",
        "Xixin Wu"
      ],
      "year": 2023
    },
    {
      "authors": [
        "Yu"
      ],
      "year": 2023
    },
    {
      "title": "Temporal data meets llm-explainable financial time series forecasting",
      "authors": [
        "Xinli Yu",
        "Zheng Chen",
        "Yuan Ling",
        "Shujing Dong",
        "Zongyi Liu",
        "Yanbin Lu"
      ],
      "year": 2023
    },
    {
      "year": 2019
    },
    {
      "title": "Are transformers universal approximators of sequence-to-sequence functions?",
      "authors": [
        "Chulhee Yun",
        "Srinadh Bhojanapalli",
        "Ankit Singh Rawat",
        "Sashank Reddi",
        "Sanjiv Kumar"
      ],
      "year": 2019
    },
    {
      "title": "Unleashing the power of shared label structures for human activity recognition",
      "authors": [
        "Zhang"
      ],
      "year": 2023,
      "doi": "10.1145/3583780.3615101"
    },
    {
      "title": "Insight miner: A time series analysis dataset for crossdomain alignment with natural language",
      "authors": [
        "Zhang"
      ],
      "year": 2023,
      "doi": "10.7717/peerj.19814/fig-2"
    },
    {
      "authors": [
        "Zhong"
      ],
      "year": 2023
    },
    {
      "title": "Language-guided traffic simulation via scene-level diffusion",
      "authors": [
        "Ziyuan Zhong",
        "Davis Rempe",
        "Yuxiao Chen",
        "Boris Ivanovic",
        "Yulong Cao",
        "Danfei Xu",
        "Marco Pavone",
        "Baishakhi Ray"
      ],
      "year": 2023,
      "doi": "10.1109/icra48891.2023.10161463"
    },
    {
      "title": "One fits all: Power general time series analysis by pretrained lm",
      "authors": [
        "Zhou"
      ],
      "year": 2023
    },
    {
      "title": "Tent: Connect language models with iot sensors for zero-shot activity recognition",
      "authors": [
        "Zhou"
      ],
      "year": 2023
    }
  ],
  "num_references": 88
}
