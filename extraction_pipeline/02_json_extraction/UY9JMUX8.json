{
  "paper_id": "UY9JMUX8",
  "title": "Investigating primary school children's embodied expression of programming Investigating primary school children's embodied expression of programming concepts concepts",
  "abstract": "The study of children's gestures has proved useful in understanding learning and conceptual development in subjects such as mathematics but has not yet been carried out in computing education. This paper presents an analysis of the way in which children describe programming concepts and their use of spontaneous co-speech gestures. We conducted two interviews at two different times (one directly after a programming activity, and one approximately two weeks later) with 45 primary school students in Saudi Arabia (aged 6 to 7). We analysed their responses when asked to explain two programming concepts: program and iteration. Participants using metaphorical gestures drew upon two overarching embodied metaphors in their explanations, namely (i) computing constructs as physical objects (ii) computing processes as a motion along the path. Participants moved their hands along one of three bodybased axes (longitudinal, transverse, and frontal) when referring to chronological sequences. These findings were broadly in keeping with those found in previous work on University computing students' gestures. However, our study also showed that gestures used by child learners whose first language is a right-to-left language (i.e., Arabic) had directional differences compared to the gestures used by adult learners whose first language is a left-to-right language (e.g., English). This work is the first step toward understanding young children's embodied descriptions of programming concepts following introductory programming activities, and the potential role of gestures in supporting their learning.",
  "year": 2023,
  "date": "2023-06-01",
  "journal": "ACM Transactions on Computing Education",
  "publication": "ACM Transactions on Computing Education",
  "authors": [
    {
      "forename": "Kate",
      "surname": "Howland",
      "name": "Kate Howland"
    },
    {
      "forename": "Abrar",
      "surname": "Almjally",
      "name": "Abrar Almjally"
    },
    {
      "forename": "Benedict",
      "surname": "Du Boulay",
      "name": "Benedict Du Boulay"
    }
  ],
  "doi": "10.1016/j.ijcci.2023.100574",
  "keywords": [
    "tangible user interface (TUI), graphical user interface (GUI), representational gesture (RG) Computing education",
    "Embodied cognition",
    "Gesture",
    "Metaphor",
    "Abstract, User interfaces"
  ],
  "sections": [
    {
      "title": "Introduction",
      "text": "There are ongoing calls to enhance computer education in schools  [1, 2]  and many countries have modified and extended their curricula. A key challenge is the relative lack of theoretical and empirical evidence about how children learn in computing domains. One theory of particular relevance is embodied cognition, which has been applied in designing and implementing learning activities and materials for other STEM domains, such as mathematics  [3] .\n\nIn particular, metaphor plays a significant role in the embodied cognition perspective  [4] ,and it has substantial implications for computing, especially in teaching  [5] . Metaphors can be expressed using different language or gestures, including hand movements produced when talking  [6] . Metaphors are important in human communication and reflect human cognition, often being used by people to visualise what they are thinking  [7] . Students and teachers routinely produce gestures to communicate information alongside speech and to enhance their listeners' comprehension  [8] . Furthermore, there is increasing evidence suggesting that gesture plays a vital, potentially fundamental role, in knowledge development and change. Thus, understanding the use of gestures in classrooms is important in developing a deeper understanding of instructional communication and knowledge change  [8] .\n\nTo date, there are no published empirical studies with children that focus on gesture and adopt an embodied cognition lens to explore the conceptual development of programming concepts and their retainment of those concepts. In this work, we investigate the following research questions:\n\n\u2022 What types of gesture do children use to describe computing concepts following an introductory programming activity?"
    },
    {
      "title": "Theoretical background",
      "text": "Theories of embodied cognition highlight the extent to which the body and the physical world influence cognition, pointing to the coevolution of body and behaviour, emotional states, and culture  [13] . Most embodied cognition theories imply the mind is not the only source of knowledge: instead, people make meaning about their surroundings from their body-based, lived experiences  [14]  and image schemas in the body's interaction with the world  [15, 16] .\n\nThere are many different viewpoints on embodied cognition. Wilson discusses six distinct claims often argued for in theories of embodied cognition, of which the first five are: (1) cognition is situated; (2) cognition is time-pressured;\n\n(3) we offload cognitive work onto the environment; (4) the environment is part of the cognitive system;  (5)  cognition is for action  [17]  . These five claims are related to the way humans use the environment as a dynamic resource to reduce 'online' cognitive demands. The sixth claim states that even our 'offline thinking' (thinking that is decoupled from the external environment, such as planning or remembering) is grounded in mental structures that originally evolved for interaction with the environment. Furthermore, we depend upon these embodied resources to think, reflect, reason, and communicate diverse ideas using conceptual mapping. For example, there is evidence that we use conceptual metaphors to comprehend abstract concepts in term of concrete entities  [18] . Abstract concepts refers to concepts that do not have physical constraints as they have no concrete representation in the physical world (e.g., emotions, metaphors, and thinking as an abstract action). [19] . In contrast, concrete concepts refer to entities with a presentation in the physical world and special constraints, such as the Sun in the sky or walking as a concrete action.\n\nThere is ongoing research on using embodied cognition to understand the way students learn and how they progress from concrete to abstract. Learners often express their ideas spontaneously through gesture  [20] . Many researchers study motion and gestures as a way to understand how people learn such as  [20]  and  [12]  and there is emerging research that addresses how young learners progress from concrete to abstract representations of computing concepts  [21] ."
    },
    {
      "title": "Embodiment and Learning Programming",
      "text": "Programming is a foundational concept in CS but learning to program is not easy  [22] . Even the most basic programming concepts, which are elementary to specialists, are commonly studied among novices and found difficult to understand  [22] . Programming concepts are precisely defined and implemented, and students are required to understand what certain constructs and concepts do, such as variables, variable assignments, and flow of execution  [23, 24] .\n\nProgramming languages, on the other hand, are cultural instruments with complex syntaxes that were often conceived and constructed for professional usage, rather than to facilitate learning  [24] . Learning programming is not easy because students find it conceptually difficult to understand; they do not grasp the program's critical features and do not know how to handle them by writing the program. [23] . Students find it hard to create a mental model of how computing concepts work, a prominent example of which is the concept of pointers  [25] . Responses such as this make iteration difficult to learn for students at the primary level. Abstraction in computing is ultimately about hiding information. Programmers write lines of code to instruct the compiler, and these lines of code do not demonstrate the complex computation and operations that are happening behind the scenes  [26] ."
    },
    {
      "title": "Conceptual metaphors",
      "text": "Conceptual metaphors represent embodied experiences as they involve the generation of image schemas that map metaphors to abstract concepts. For example, the concept of time is grounded in the experience of linear motion, and for many cultures, the past is behind while the future is in front  [27] . A conceptual metaphor involves the mapping between a source domain such as movement and a target domain such as time. The target domain is understood unconsciously in terms of the relations that are rooted in the source domain, which is entrenched in everyday sensorimotor experience. Thus, conceptual metaphors demonstrate the human ability to think and reason about abstract concepts  [28]  . In 2021, Bettin and Ott explored the perception of metaphors in computing education. They discussed the possible potential of using metaphors in computing education which can lead to greater development in learning and enrich the field of computing education  [5]  .\n\nMost empirical work in this area investigates conceptual metaphors that are expressed through language. For example, in mathematics, words such as \"count out\" and \"next\" might indicate a conceptualisation of numbers as a collection of objects or as points along a path  [18] . Language is not the only way people use conceptual metaphors: they can also be expressed using gestures, which provide another way to express our understanding of abstract concepts.\n\nIn 2009, Bakker et al. argued that abstract concepts are initially understood experientially before children can articulate them in speech, and that the limited number of metaphors that children use may be connected to conceptual difficulties for the age group  [29]  . This suggests that the use of conceptual metaphors reflects understanding and is powerful in communication and learning."
    },
    {
      "title": "Gestures",
      "text": "According to McNeill in 1992, gestures are spontaneous hand movements that accompany speech  [30] . Speech is considered analytic, while a gesture is more global and image-based, providing a unique lens to examine meaning  [31] . Gesture, unlike speech, involves iconic mapping as a vital mechanism of meaning creation. Mapping is a cognitive process of perceiving similarity and linking it between one thing and another. For example, the receiver is engaging in a mapping process when they see a hand in a certain configuration and conclude that the hand represents a physical entity being talked about in the accompanying discourse  [32] .\n\nApproximately 20 years of research literature has documented the potential benefits and roles of gestures to support learning, whether observed or produced  [33] . Gestures may simply be indicative of understanding  [29]  or deliberately used to convey understanding  [34] , but there is also evidence that they may actively help in processing and learning, including problem-solving  [35]  and/or generating new ideas  [20] . However, in some contexts, it is challenging to distinguish between the different roles of gestures to support learning.\n\nOne way in which gestures may support learning directly is by allowing cognitive offloading to the environment. Ping in 2008 found that children taught using gestures outperformed their counterparts on solving Piagetian conservation problems, regardless of the physical presence of objects during instruction  [34] . Cook et al. in 2012  found that performing gestures that coordinated with the speech while solving math problems enabled the participants to recall more information than when they did not gesture  [35] . Previous research has suggested that gestures enhance learning and problem solving by encouraging learners to link abstract concepts or to act out the abstract concept to give learners a concrete representation with which they may engage. In 1996, Schwartz et al. suggested that spontaneous gestures are physical illustrations of mental models. The authors found that their participants used hand gestures to represent the movement of gears when solving interlocking gear problems, which supported them in imagining the correct directions of movement of the gears. In turn, they progressively learned to abstract rules over mental depictions to solve gear problems  [36] .\n\nGestures can also convey knowledge that is not expressed in speech, either incidentally or deliberately, to assist the observer's understanding  [34] . Therefore, understanding gestures can provide teachers with insights into learners' thinking. For example, Novak and Goldin-Meadow described how the child stated that a given volume of water changed when it was poured from a tall, thin container into a short, fat container  [20] . Additionally, Teachers can assess learning using gestures  [37]  as they provide additional information about the speaker's conceptualisations by externalising dynamic visual imagery to support cognitive activity. Students' gestures and teachers' gestures can support communication and improve learning  [38]  and there are benefits identified in training teachers to use gestures  [39] ."
    },
    {
      "title": "Related work",
      "text": "Gestures have been studied in different learning fields, such as mathematics and reading comprehension  [40, 41] . However, there is little research describing the role of gestures in learning in computing education. In this section, we give an overview of the few studies that have explored the role of gestures in computing education.\n\nSolomon et al. conducted an exploratory study observing high school students with a total of eight participants and their gestures as part of an introductory computing course using Scratch  [42] . Students learned about variables, conditional statements, and loops for 12 weeks. At the end of the course, 15-minute interviews were conducted with the participants in which they explained the topics they had covered in the course. The researchers aimed to understand the types of gesture that students and teachers produced in the course. They used McNeil's textonym, which is a gesture taxonomy for mathematics  [31] . They reported some of the challenges of fitting McNeill's taxonomy to the data, emphasising the need for a particular conceptual framework to support gesture analysis in computing education. They stated that gestures in computing courses are potentially used as problem-solving strategies and to communicate students' understanding of abstract concepts.\n\nA more recent study by Manches et al. investigated the conceptual metaphors generated by university-level computing students  [12] . They examined gestures by asking 16 participants to explain three computing concepts (algorithm, loop and conditional). Although they were not asked to use gesture, the participants produced 368 representational and metaphorical gestures, suggesting that CS concepts are often embodied through gestures. Manches and colleagues found that participants mapped their gesturing onto two embodied metaphors. The first metaphor was computing constructs as a physical objects. In this case, participants simulated manipulating physical objects when referring to a range of computing constructs. The second metaphor represented computing processes as motion along a path, in which participants moved their hands along one of three body-based axes when referencing temporal sequences.\n\nSanford, et al., interviewed 10 CS instructors regarding their use of metaphor in the classroom  [43] . The purpose of this study was to understand the role that metaphors have for CS instructors, to identify the many types of metaphors that are employed, and to evaluate how well they facilitate learning. Their findings showed that instructors used metaphors widely. However, the instructors could not always explain the common questions and misunderstandings that students had when using those metaphors. Therefore, additional research is needed into student thinking about computing concepts when scaffolded by metaphors.\n\nAs a first study to explore teachers' uses of gesture in computer classrooms, Solomon et al., described teachers' use of embodied representations in the form of gestures, embodied language, and tools used in two case studies on teaching recursion  [44] . Using a grounded theory approach, they analysed video recordings of undergraduate computing instructors teaching recursion. They produced a conceptual framework of the gestures that teachers used in computer classrooms, which was the first step to understanding how embodied representation supported learners. However, the study could not distinguish between the intentional and non-intentional (spontaneous) use of embodied representations. Investigating whether the use of embodied representation is intentional might help in critical studies and reflect an understanding of what instructors are communicating. In mathematics, the use of scripted gestures has helped children to retain the knowledge they learned  [45] . This might apply in computing education, as well. However, computing education researchers first need to understand and explore what types of gesture teachers and students produce spontaneously in the classroom and what effects these gestures have on learning. This is what the next study addressed.\n\nAlmjally et al. investigated children's spontaneous gestures when learning fundamental programming concepts through either a tangible user interface (TUI) or a graphical user interface (GUI)  [46] . They explored the relationship between spontaneous gestures, interface type and learning outcomes in a programming lesson for primary school students aged 6-7. In their study, 34 participants engaged in a learning activity lasting approximately 37 minutes. The sessions were video recorded and subsequently coded and analysed using the Alibali gesture analysis scheme  [8] , adapted from research in mathematics education. They found a statistically significant difference between the mean learning gains of high-frequency gesturers and low-frequency gesturers, with the top quartile showing significant learning gains. Overall, 1020 gestures were generated, including 702 pointing, 306 literal representations and 12 metaphorical representations.\n\nSome additional studies have examined young children's verbal explanations of computing concepts such as  [47, 48] . However, these studies did not investigate gesture use, and were not conducted with children who had previously taken part in programming activities, so the concepts described were very general. This work provides a broader view of children's expression of computing concepts, with a focus on embodied expression. It explores the types of gesture that young children use when explaining programming concepts on two separate occasions (directly after a programming activity and approximately two weeks later). Additionally, this work explores how young children map abstract concepts to concrete concepts. Through this work, we aim to encourage other computing education researchers to investigate the roles of gesture further in computing education and other STEM subjects."
    },
    {
      "title": "Method",
      "text": "This study draws upon the learning sciences theoretical framework of embodied cognition and the methodological tools of cognitive linguistics and gesture research to investigate participants' use of gesture when responding to interview questions. The meaning of their gestures was interpreted from context, which was the verbal response to the structured interview  [32] . Interviews are often used with children to prompt explanations of their understanding of computing  [47] [48] [49] ."
    },
    {
      "title": "Participants",
      "text": "The study was conducted at an international school in Saudi Arabia. Participants were from 5 single-gender classes (two girls' classes, three boys' classes), and all had participated in a previous study with the researcher. The children learned fundamental computer concepts using a block-based programming language. Thus, participants were familiar with the basic programming concepts asked about in the interview. Fifty students in total participated (25 girls, 25 boys, all aged between 6 and 7). The participants were interviewed at two different times after the previous study, with two weeks between the interviews. This was to check in changes in understanding might occurred of delay in two weeks. Parents gave informed consent for their children to participate in the study. The focus on embodied responses in the study was not explicitly mentioned to participants, however, the parental consent form stated that this was the main focus of this study. Children gave verbal assent before each interview."
    },
    {
      "title": "Data collection",
      "text": "This study was conducted via interviews, in which participants were asked to explain their understanding of computing concepts. The approach of using interviews to prompt gestures is in line with the work of Solomon et al. and Manches et al.  [12, 42] , who investigated the use of gesture in computing education with older learners.\n\nAlthough observing gestures produced spontaneously in the classroom as Almjally et al.  [46]  did gives valuable naturalistic evidence about the use of gestures in learning, our goal here was to collect evidence on the types of gestures and conceptual metaphors children use when describing specific computing concepts, which meant that interviews were the most appropriate method. The interview provides a context for the generation of rich data over a short period and consistently among participants. This metho was therefore a good match for our aim of investigating the embodied representations (gestures) which children spontaneously produce while explaining computing concepts.\n\nThe way questions (see Table  1 ) were asked influenced children's explanations, and thus the questions were intended to be as open and non-prescriptive as possible  [49] . The questions were asked first in English and then translated immediately to Arabic, using the two languages alternately just as the children's teachers do in the classroom. Interviews were video recorded using a GoPro video camera positioned in such a way as to observe children's hand movements.\n\nAll participants were asked to explain the same concepts in the same order. For some questions, participants were given a code snippet on a sheet of paper that they were then asked to explain; this provision may have influenced gestures, such as deictic gestures used directly toward the resources  [17] . Therefore, we recorded all the types of gestures, including the deictic gesture.\n\nThis study adhered to university ethical guidelines and the ministry of education rules in Saudi Arabia. It was conducted at an international school in Riyadh, Saudi Arabia. Ethical approval was granted by both the universities and the ministry's ethics committee."
    },
    {
      "title": "Immediate and Delayed Interviews",
      "text": "This study was conducted via two interviews, the first interview aimed to capture students' gestures immediately after the learning session. The second interview was two weeks after the introductory programming activity and first interview. The role of the second, delayed, interview was to see how well they remembered the computing concepts and whether there were any changes in their gesturing as they recounted their understanding.\n\nGestures can support learning over time, such as in mathematics  [41, 45, 50]  and in learning a second language  [51]  and potentially be related to the role of gestures in encoding and retrieving information to and from memory  [52] .\n\nMany studies have investigated the use of gestures in terms of their immediate impact, but few studies have looked at their longer-term impact. Previous research on gestures has shown that the effect of engaging in gesturing was greater on the long-term assessment (over three weeks) compared to the short term (Cook, Mitchell, and Goldin-Meadow 2008a; Cook, Yip, and Goldin-Meadow 2010a) or even two days after learning  [53] . This might be related to the learning effects on memory after sleep. During sleep, learning-related neural networks are reactivated  [54] .\n\nThe periods between the first and second assessment in the literature varied from two days to three weeks. Therefore, in the experiment described in this paper, the second interview was conducted two weeks later due to the school schedule. Also, two weeks would have given children time to think further about the topic and have reduced (if any) the influence of researcher's gestures during the learning session."
    },
    {
      "title": "Interview Questions",
      "text": "The first question (Q1) was an ice breaker for the interview session to help make participants more comfortable. Subsequent questions prompted children to explain the meaning of \"program\" (Q2), the code of a simple program (Q3), the meaning of \"iteration\" (Q4) and the code of a complex program including iteration (Q5). We selected the concepts program and iteration because they are age-appropriate foundational concepts that primary students need to understand and use  [12, 55, 56] . Iteration  [57]  is considered a complex concept for young children that may require them to offload their cognitive work and use an embodied representation."
    },
    {
      "title": "Procedure and materials",
      "text": "Before the study, participants engaged in a single learning session lasting approximately 25 minutes. They learned fundamental programming concepts, including program and repeat. The researcher conducting the learning session took care not to use any metaphorical gestures whilst explaining concepts or elsewhere in the activity. The children participated in single-gender pairs and used a block-based programming environment to move a robot around a map to solve four programming tasks. Interview 1 aimed to capture the immediate representation of the programming concepts immediately after the programming session. The author interviewed the participants individually in a quiet room. In turn, the second interview was held two weeks later, the setting for which was a quiet room. This approach sought to give children time to grasp the knowledge they had learned in the learning session; it also enabled investigation into whether the participants' understanding and use of conceptual metaphors changed over time; and it assessed the effect of time on the use of gestures when explaining programming concepts. There was no formal computing classroom during the two weeks. The same questions were asked in both interviews, the only change being to Q1, which changed from \"What did you learn today in our session with the robot?\" to \"What did you learn last time in our session with the robot?\"\n\nParticipants were asked to stand when they were interviewed as Manches et al.  [12]  reported that sitting may obscure gesturing. The interviewer then asked the questions in Table  1 . The 'relevant stimuli' e.g., code snippets were shown when the questions Q3 or Q5 were asked or the child did not answer Q4 and needed a reminder of the Repeat block. Then the researcher took codes snippets away.\n\nEach interview took around 2 to 3 minutes, and the participants were all given the questions in the same order. During the interviews, prompts were given to get the participants to clarify what they meant or to correct their code reading (see Table  2 ).\n\nThe interviewer thanked the participants every time they answered a question. The interviewer tried to avoid making representational gestures because this may have led to mirroring. After the participants had finished, the interviewer thanked them again for their participation. Figure 2 Repeat block Figure 3 (Q5) complex code aimed to describing how a complex program works"
    },
    {
      "title": "Measures and Data analysis",
      "text": "The first author recorded and processed the videos of participants answering the interview questions. She then prepared them for analysis, which included reducing the data into themes through coding, data display and conclusion  [56] . Each recording was first reviewed, then translated from Arabic to English if needed, and transcribed. A structured Excel spreadsheet was created with the following categories for each question: researcher talk, gestures, participant's answer, participant's gesture type and gesture description. We filled in the categories in which we believed the researcher or participant had responded verbally or used gestures while asking and answering the questions. Additionally, we removed irrelevant parts of the interaction that were not related to the questions. The first author coded all the video data. A random sample of 20% of the data was generated for second coding by the second author. Inter-rater reliability was determined using Cohen's Kappa scores, with substantial agreement based on Fleiss  [58] . Results were as follows: Kappa score for the gesture type is 0.681."
    },
    {
      "title": "Verbal responses",
      "text": "'Verbal responses were not within the scope of this study; instead, they were transcribed and used to interpret the meaning of the children's use of gestures. The participants answered the questions using Arabic and English interchangeably, often changing between the languages in a single sentence. Therefore, the language that each participant used was hard to catch and code."
    },
    {
      "title": "Gesture responses",
      "text": "The first author coded all gestures, drawing upon the children's speech to guide their interpretation and recorded them in the transcript and the Excel spreadsheet. The gesture responses were then categorised based on Alibali and Nathan's scheme  [8]  and divided into two types of gesture, as follows:\n\n\u2022 Deictic gesturing (e.g., pointing) reflected the grounding of cognitive gestures in the physical environment (e.g., pointing to the function block). \u2022 Representational gesturing (RG): including gesturing that represented concrete or abstract concepts by hand (hand shape, or motion trajectory) or body (e.g., rotating the whole body to indicate a turn). It could be either a: o Literal gesture (i.e., iconic gesture): depicted aspects of meaning. It is important to note that in our analysis when a participant pointed their index finger to the right while explaining the Right block concept, we considered this to be an iconic gesture not referring to an object in the environment.\n\no Metaphorical gesture: e.g., identifying how children represented the abstract notion of iteration.\n\nFor the Metaphorical gestures identified, the authors worked together to identify conceptual mapping patterns across participants. Conceptual mapping is the process of mapping between the gesture and conceptual entities in children's speech."
    },
    {
      "title": "Results",
      "text": "The number of questions asked, the responses and prompts for each question as well as the prompt types are shown in Table  2  (e.g., in Interview 1, Question 1 was asked 45 times to the 45 participants, received 38 responses without prompt and 10 gestures without prompt, no prompts were given, 0 the non-answer,38 total number of answer after promote, 10 is the total number of gestures after prompt).\n\nQ1, an icebreaker, was added to the script after the first group was interviewed because the researcher noticed that participants seemed shy about the interview setting. During the data collection and due to technical issues with the camera, we lost 9 recordings (three in Interview 1 and five in Interview 2) and one participant withdrew from Interview 2. Additionally, due to interviewer error, not all participants were asked Q2.\n\nIdentifying the prompt type was vital to make sure that participants' gestures were not influenced by any researcher prompts. More prompts were given in the first interview than the second because participants were not familiar with the interview setting and the questions. The number of prompts was higher in questions in which participants were asked to explain written code (Q3, Q5). Participants sometimes just read the code, so the researcher prompted them to explain the code instead of just reading it. What kinds of gesture were produced?\n\nIn total, we coded 1066 gestures into three types: pointing, representation gesture (RG) either as a literal or as a metaphor: see Table  3  below for examples for each type. Table 3 examples of gesture type\n\nThe 1068 gestures were coded across 566 explanations (see Table  2 ). Three participants generated no gestures. Figure  7  illustrate the numbers and types of gestures recorded for each question. The total number of participants' gestures in Interview 1 ranged from 0 to 26, with a median of 11, while that for Interview 2 ranged from 0 to 21 (median 13). Additionally, an independent-samples t-test was run to determine differences in the number of overall gestures between the two interviews. Participants in Interview 2 made more gestures (N=45, M= 12.98, SD = 5.4) than in Interview 1 (N=47, M=10.30, SD=5.7), the different was a statistically significant difference (95% CI, -5.007 to -.353), t (90) =. -2.288, p =.024).\n\nFigure  7  Interviews, number and type of gestures generated for each question"
    },
    {
      "title": "Deictic gestures",
      "text": "A deictic gesture (e.g., pointing) is used to indicate an object's location  [30] . Participants pointed frequently when physical material was provided, such as in Q3 and Q5, by indicating a code block while explaining its functionality. These gestures guided both the participants and researcher to the related block. Tracing the code might have helped students while they tried to understand the control flow of the code. Deictic gestures might have reflected cognition and revealed students' understanding of the code execution, similar to Solomon et al. 2018 findings  [42]  . For example, when participants pointed to the left block in a simple program and said \"right\" whilst gesturing to the left, this let the researcher know that the participant had issues reading and or understanding the code or just muddled right and left.\n\n0 50 100 150 200 250 Number of gesture for each question Gesture types Interview 1 Q1 Q2 Q3 Q4 Q5 0 50 100 150 200 250 Number of gesture for each question Gesture types Interview 2 Q1 Q2 Q3 Q4 Q5"
    },
    {
      "title": "Literal gestures",
      "text": "A literal representational gesture (RG-literal) represents a concrete idea and conveys information about the size, shape or orientation of a discourse object  [59] . Such gestures are used to facilitate communication and simulate an action. Participants used literal gestures mostly to simulate the robot's action -forward, right and left. Others used literal gestures to represent a number that was important in the code. For example, one participant said \"the robot would repeat the code twice\" with a hand gesture for the number 2."
    },
    {
      "title": "Metaphorical gestures",
      "text": "Metaphorical representational gestures (RG-metaphorical) are used to help communicate abstract concepts  [60] . An abstract noun, as defined by Lexico, quoting from the Oxford Dictionary (2020), is \"a noun which refers to ideas, qualities, and conditions -things that cannot be seen or touched and things which have no physical reality\"  [61] . We considered the concepts of program and iteration as abstract concepts and therefore gestures used to communicate them as RG-metaphorical.\n\nThe noise block was a unique case as it had two aspects that might be communicated: the lack of robot movement compared to other blocks and the generation of a noise, see Section 5.2.1. The lack of movement aspect of a noise block was readily communicated via range of literal gestures, in that gesturing lack of movement is as straightforward as gesturing movement. Communicating the noise of the noise block through making a sound was not regarded as a gesture at all. However, it was not obvious how best to categorise the gestures that were used to communicate the sound aspect of the noise block and we chose to treat these gestures as RG-metaphorical in the sense that such gestures were in a different modality from sound and in that sense were metaphorical.\n\nParticipants produced a total of 53 RG-metaphorical gestures (Interview 1 = 24; Interview 2 = 29).\n\nHow are metaphorical gestures used to represent computing concepts?\n\nThis section explores the metaphorical gestures and what they reveal about how children conceptualise computing concepts."
    },
    {
      "title": "Noise block",
      "text": "The noise block, which makes a buzzing sound and does not involve any movement of the robot, was part of the simple program that participants explained. A noise block is a commonly occurring programming element in robot programming block environments, in addition to movement-related blocks. The noise block is one of the few blocks that are not tied to the spatial elements of the programming task. When this block is reached during program execution the robot emits a buzz without moving from where it is at that moment. Therefore, the noise block cannot so easily be represented via movement gestures; so it is also potentially more likely to be communicated via metaphorical representations.\n\nIn describing the effect of the noise block some participants focused on the lack of movement of the robot, others on the noise the robot made and others again on both aspects. For example, some participants demonstrated the noise with a gesture, and a few said \"pop\". Over both interviews 15 RG-metaphorical gestures and 30 pointing gestures were generated for the noise block, and there were 45 responses where no gesture was produced when the noise block was reached in Question 3.\n\nParticipants represented the noise metaphorically in many different ways, either by keeping the same position, representing the freezing by an action, or by demonstrating a movement. First, a few participants represented the noise as a moment of freezing, where the robot stopped moving and kept its previous position, which is exactly what the robot did, e.g., P40. Others represented the freezing by an action. For example, P26 and P27 clenched their hands, whereas P50 clasped her hands. P28 posed his hands as if they were holding something, whereas P18 held a handout flat and said \"pop,\" and P10 hit the table with her fist as a gesture to represent the noise. Other participants represented the noise block with different movements, even though the robot in the learning session did not move when it produced the noise. For example, P23, P40, and P42 used their whole body to dance when they reached the noise block, whereas others just moved their hands. P31 moved his hand left and right, whereas P35 moved his hands toward the front. P09 moved both of her hands in a circular motion as if they were a hurricane. P45 moved his hands in an arc motion."
    },
    {
      "title": "Program",
      "text": "Only two participants used gestures when asked about the meaning of a program, and they both answered that the program means repeat. For example, participant (P37) said that the program was a \"repeat\" while gesturing the form of a container. Therefore, their gestures were coded as a metaphorical representation of the repeat."
    },
    {
      "title": "Repeat",
      "text": "In Interview 1, 18 gestures represented the repeat either as a circle (8), an arc (6), a forward line (2), or a flowing arrow mark on the repeat block (2). In Interview 2, 20 gestures represented the repeat either as a circle (8), a container (4), an arc (4), through repetitive movements such as a wave (1), or by drawing the arrow found on the repeat block (1) and performing the action (2)."
    },
    {
      "title": "Computing constructs as physical objects",
      "text": "When explaining computing concepts, two participants simulated a pinching action described by Edwards in a 2009 study focusing on mathematical concepts  [62] . Similar to numbers that are represented physically, participants' experience with programming blocks might have influenced them to draw upon experiences of manipulating tangible representations of computing concepts. Pinch was used when describing programming blocks inside the repeat, e.g., Figure  8 . Additionally, participants represented repeat as a container, see Figure  9 , by creating the boundary of an object and using another hand to gesture into the hand. This was often associated with indicating that the code inside the repeat will be executed."
    },
    {
      "title": "Computing processes as a motion along a path",
      "text": "There are three main axes related to the body: longitudinal, frontal, and transverse:\n\nLongitudinal axis: the path up to down in front of the body, paralleling how the participants wrote the code from top to bottom. For example, a gesture referring to time-based computing processes might go downward in a longitudinal direction. One participant (P41) produced this gesture and said \"Repeat\" while his arm moved downward. He discussed algorithmic steps/instructions, while making a step going downward with his hand see Figure  10 .\n\nFrontal axis: this is the pathway moving away from the body where a circular gesture was projected outward from the body. Two participants (Interview 1: P44, Interview 2, P05) represented the repetition with two straight lines going forth and back (such as P44, see Figure  10 ). This supports the idea that participants were drawing a general body-based spatial metaphor of time, that the future was in front of them, conceptualising the robot movement.\n\nTransversal axis: this is the pathway moving from left to right across the body, or vice versa. English is written from left to right, whereas Arabic, the participants' native language, is written from right to left. This suggests that gestures communicating computational processes might trace a similar transversal axis. Twelve participants drew the arc/circular shape from right to left, anticlockwise (6 participants in Interview 1, and 6 participants in Interview 2) see Table  5 .\n\nGesture movement pathways were noticed when participants talked about repeating a program or what they had just said. For example, P20 read the first line of the code for Q3, \"left\", then said, \"It will do it again\", with her right hand (RH) moving in an arching gesture. Another gesture represented the repetition as a wave or climbing a hill from left to right (Figure  18 ). Moreover, some participants represented one circle of the repeat clockwise and the second circler anticlockwise, see Figure  20 ."
    },
    {
      "title": "Gesture size and frequency",
      "text": "The shape of the hand and whether one or two hands were used suggested differences in the perspective of the size of an imaginary object. Participants had a different perspective of the \"size\" because, for computing constructs, physical size has no meaning. For example, P09 represented the repeat using one big circle in both interviews, whereas P19 represented the repeat with a small circle. P36 represented the repeat with two circles, where the first was bigger than the second, see Figure  20 . Additionally, the circular movement frequency was not consistent across participants, ranging from one to three cycles. This is interesting because the repeat executes the command and then repeats it once. Therefore, the frequency of participants' movements might indicate their understanding of the repeat block. Table  5  shows the different representations and frequency of the circular motion. Figure 9, P17: \"Put repeat [hand gesture as container], then the left, forward, right [2 RG-literal], and the sound\"."
    },
    {
      "title": "Pinch Container"
    },
    {
      "title": "Longitudinal movement Frontal",
      "text": "Table 4 Pinch, Container, Longitudinal, frontal and other representations Figure 12, P09 gesture: \"It repeats. If you did it wrong, it would repeat it again\", with her palm open. Figure 13 P37: \"Do it again\". He drew two circles on the table with his hand closed."
    },
    {
      "title": "One big circle, Clockwise Drawing two circles Clockwise on the table",
      "text": "Figure  14 , P19: \"If we do a thing, we will do it again\", making a small circular motion with her hands clasped. X2 2X Figure 10 P41 moves his hand downward and saying \"Repeat\"\n\nGrasping with a small circle Clockwise Arc -Clockwise  Moving both hands at the same time in an anticlockwise motion, three times Two big circles, anticlockwise Waves, anticlockwise Right-to-left movement, anticlockwise Figure  20 , P36 \"Do it again\". He drew a big circle on the table with his hands gripped clockwise, followed by a smaller circle and hand-moving when talking about repeat anticlockwise. Tow circles, Clockwise and anticlockwise\n\nTable 5 Transversal axis examples\n\nIn two instances, when participants answered the question, \"What is a program?\", their use of a gesture revealed implicit knowledge which they did not express in words. For example, P45 said: \"yes program the stuff, like the robot\". It was not clear what participants meant when they said \"stuff\"; it was ambiguous. However, their gesture, hands moved in top of each other, represented the order of the programming blocks used in the learning activity (see Figure  21 ). In another example, P07 answered the question, \"What is a program?\" by saying \"do something now\" while holding both hands together. The participant then continued, \"Then you do the same thing there\" while turning A B their body to the side and turning their hands. The researcher did not know what the student meant by \"something\" and \"the same thing\". from the participant's gestures the researcher understood \"something\" as meaning the robot's movements. The participant was simulating its movement by moving their body."
    },
    {
      "title": "Representational Gesture Changes Over Time",
      "text": "This section describes metaphorical gesture changes across the two interviews to answer RQ3.3: 'Are there any differences evident in the use of gestures and conceptual metaphors when children are interviewed again, two weeks after the introductory programming activity, compared with that in a first interview immediately after the activity?'.\n\nThe process compared the use of conceptual metaphors in each interview.\n\n\u2022 Agreement-level 1: The concept was represented twice in one interview and followed the same category (e.g., P10).\n\n\u2022 Agreement-level 2: The concept was represented in the same category in Interviews 1 and 2 (e.g., P09).\n\n\u2022 Disagreement-level 1: The concept was represented twice in one interview and each time was in different category than the other (e.g., P19).\n\n\u2022 Disagreement-level 2 The concept was represented differently in each interview (e.g., P18).\n\n\u2022 Solo: The concept was represented once in Interview 1 or 2 (e.g., P01).\n\nFor the noise block, one participant generated an RG-metaphorical gesture in both interviews with consistent representations. Interview 1 featured five representations while Interview 2 featured eight . For the repeat concept, three participants (P09, P10, P20) maintained Agreement-level 2 for the repeat representation, while 10 participants represented the repeat differently in each interview (Disagreement-Level 2). The other six represented the repeat solo in Interview 1 or Interview 2. Additionally, a more detailed breakdown is shown for repeating concepts twice in the same interview (five disagreements and one agreement). An independent-samples t-test was run to determine differences in the number of overall gestures between the two interviews. Participants in Interview 2 made more gestures (N=45, M= 12.98, SD = 5.4) than in Interview 1 (N=47, M=10.30, SD=5.7) and the difference was statistically significant (95% CI, -5.007 to -.353), t (90) = -2.288, p =.024)."
    },
    {
      "title": "Discussion",
      "text": "Our overall goal in this study was to collect and analyse gesture responses that children used to describe computing topics after experiencing a programming activity, including the concepts of a program and of iteration. We explored how children used spontaneous gestures to convey computing concepts and how they conceptualised these concepts. Additionally, we developed an analytic framework suitable for understanding gestures generated within this domain. This study's findings indicate even for abstract concepts in computing education, the embodied nature of participants' understanding of computing concepts was made evident through their gestures.. The participants also produced a high number of literally representational gestures that mostly referred to concrete objects or processes. These often related to tangible materials utilised in the programming blocks or the robot's movement."
    },
    {
      "title": "What kinds of gesture did children produce?",
      "text": "The study identified 281 pointing, 734 RG-literal, and 53 RG-metaphorical gestures. Our findings support one of the central claims of embodied cognition, namely, that offline thinking involves mental simulations of perception and action. The representational gestures were produced in the absence of 'relevant stimuli', e.g., a physical object such as a screen or board to point to.\n\nImportantly, the representational nature of these gestures provided additional information to their speech to help us to analyse their thinking in this domain. Our findings suggest that embodied cognition has many insights to offer to computing education in which they would benefit from a greater understanding of conceptual development  [63] .\n\nThis study shows that the children aged 6-7 years old produced less gestural evidence of metaphorical understanding compared to that of conceptual understanding. Only 5% of the generated gestures were metaphorical, suggesting that spontaneous metaphorical thinking did not play a strong role in these children's reasoning about CS concepts. This is not surprising as the use of such metaphors is sophisticated and shows up more frequently with older children.\n\nAnother possible explanation might be that the participants who represented the repeat concept metaphorically understood it, as the use of a conceptual metaphor is the first sign of the understanding of the concept. In 2009, Bakker et al. showed that abstract concepts, after first being understood, can be represented in body movements before they can be explained in words  [29]  . Therefore, the limited use of metaphorical gestures may reflect a limited understanding of programming concepts (e.g., iteration and program). If children experience difficulties in explaining the concept in words and gestures, it is an indication that the concept is difficult for the age group. Further work should look at the use of metaphors in CS with older children.\n\nHow were metaphorical gestures used to represent computing concepts?\n\nDespite the participants' young age and their having no previous experience with a programming language (except for the 25 -minute learning session before the study), they generated 52 RG-metaphorical gestures. Several patterns of gesture were identified. Patterns included using a circular motion to communicate the repeat block with different frequencies and sizes, and different hand grasps to represent the robot's non-movement when making a noise.\n\nMost of the RG-metaphorical gestures that were identified described either a repeat or a noise. The way participants gestured suggested their degree of understanding. It was not possible to test the accuracy of these postulated degrees of understanding independently, but with this caveat, we offer some plausible interpretations. For example, participants representing the noise block with a movement might have indicated a low understanding of the noise block's functionality because the robot did not move when making a noise in the learning session. Conversely, the no-movement representation was either a literal representation of the freeze or metaphorical; either way, any representation might indicate that participants understood that the noise block meant that the robot would be in the same position. A literal representation of the freeze means simulating the robot's movement when it runs the Noise block by keeping the previous gestures that participants made for the previous Action block and saying \"noise\" or \"sound\".\n\nIn contrast, a metaphorical representation of the noise block as freeze involved the child using a gesture such as clenching, clasping or hitting the table to indicate his or her understanding that the noise block caused the robot to remain stationery. This could indicate how students had understood the idea and could be used by teachers to glean information about their students' learning stage.\n\nAlso, the frequency of circular gestures -ranging from 1 to 3 may indicate the repeated execution of a program (once, twice, or thrice) based on the number of times the child performed the circular motion. This could be reflective of the children's understanding of the repeat concept. Noteworthily, teachers can use the information that students convey with their hand movements to guide and assist their learning. For example, Kelly et al. in 2002 found that teachers can assess understanding by considering students' gestures  [37] . This was attributed to the fact that the information students convey using gestures is at the cutting edge of their knowledge (e.g., the noise block was not a moving action block, and they were aware of its functionality).\n\nThe approach used to categorise RG-metaphorical in this paper was similar to that of Manches et al.  [12] . In their approach, the authors categorised the gestures of undergraduate computer science students into the following image schemes: the computer as an object or container and the path-course goal schemes that serve as the foundation for mathematical concepts  [18] . It is worth noting that the metaphor outlined here does not seem to differ greatly between adults and children, but varying forms of metaphors arise. As a case in point, in contrast to the non-Arabic speakers in Manches et al.'s study  [12] , who all represented iteration using a clockwise gesture, half of the participants in this study represented the repeat in an anticlockwise circle. This may be due to the difference in meaning implied by the left-right/right-left text direction in English/Arabic. It is notable that half of our participants used a clockwise gesture despite the blocks-based language used in the learning activity representing the control flow of the repeat block in a clockwise direction. Insofar as the features of languages impact human conceptualisations of their surroundings, language may influence thought in a clear semantic way  [64] . Our findings indicate that when explaining programming concepts, the gestures used by these learners are notably different in terms of the aspect of their direction compared to the gestures used by learners whose first language is a left-to-right language (e.g., English).\n\nThe features of languages impact human conceptualisations of their surroundings and language may influence thought in a clear semantic way. Therefore, to increase the accessibility of CS education for a non-left-to-right language speaker, new or adapted versions of programming environments might potentially be needed to support learners' pre-existing culturally based understanding.\n\nTo increase the accessibility of programming and computing education among learners who do not speak English  [12, 64, 65]  , it might be necessary for new or adapted versions of programming environments to be designed to support learners' existing culturally based understandings.\n\nThe programming blocks used in the pre-programming session were placed from top to bottom, but there were some elements of the design that implied a left-to-right reading order. The blocks had labels written in English, were slightly aligned with the code's left to right orientation, and the arrow on the repeat blocks moved in a clockwise direction. This might have created a potential conceptual challenge for children if right-to-left reading was their main model. Although the children who participated in this research were studying at an international school that taught English as the main language, and the programming language implied a left-to-right direction, the overall cultural context in which the ideas were presented appeared to be strongly present.\n\nAdditionally, we noticed that several of the repeat gestures related to a special mapping of the code. A significant number of the participants explained the repeat by giving an example similar to that used in the learning session; some of the participants drew the arrow shown on the repeat block. Others represented it as a container similar to the repeat programming block with the other action blocks inside it. This was not surprising, as block-based programming language involves blocks with a visual representation written on them, affecting how the children simulated the repeat block.\n\nThere is likely to be an interaction between computing environments and computing metaphors, in which metaphors are used to design environments and then shape the metaphorical thinking of the users  [66] . Our findings suggest that the design of educational materials in computing education could help learners by exploring conceptual metaphors and could be applied to inform further design guidelines, for example, the representation of iteration. This lens might help identify possible conceptual challenges, such as visually representing iteration and noise dynamics. These challenges might impact the user's use of gestures when designing learning interfaces that involve gesture use to support learning. It is important to also note the culturally defined nature of some metaphors and consider the implications of children being asked to use computing environments whose metaphors clash with their existing conceptual metaphors.\n\nGestures in which the hand moves along a linear axis typically refer to computing processes rather than direct constructs. This reveals that gestures mark both the start and endpoints of a process and a sequence of steps along a path. The delineated path axis is also interesting. The longitudinal axis corresponds with vertical lines of code. The traversal axis corresponds with the cultural left/right direction of time, and the frontal axis appears to correspond with a culture of time in relation to the body. Although gestures along these axes seemed to simulate processes, it is interesting to note the points along the trajectory because they often corresponded with algorithmic steps. More work is needed to investigate the special metaphors that represent computing concepts, and more investigation is needed to understand the potential benefits of explicitly encouraging gesturing, such as the gestures in this paper, for making implicit knowledge explicit. In some countries, such as the UK, young children are expected to learn computing concepts from the age of 5  [65] , so it would be worth investigating whether encouraging children to use particular gestures might support their understanding of computing. Additionally, in other domains, there is evidence showing the benefits of teachers using specific gestures in the classroom  [38]  to improve students' learning  [44] .There is emerging work in computing education, such as Solomon et al. in 2020  [44]  who investigated the use of gestures by computer instructors. Further investigation is needed to evaluate the anecdotal evidence.\n\nAlthough this study was not designed to investigate the potential benefits of gestures, we noticed that gestures revealed information that students did not express in words. Gesture use was beneficial for our understanding of the participants' implicit knowledge. It can be used as a tool to facilitate understanding that is not expressed by words. For example, in our study, we spotted two instances where gestures revealed student knowledge and changed our classification of their understanding. This in line with Novack and Goldin-Meadow, who discovered that gesture reveals what learners know  [20] .\n\nAre there any differences in the use of gestures and conceptual metaphors when children are interviewed again, two weeks after the introductory programming activity and first interview?'\n\nThe study identified one level 2 agreement with the metaphorical representation of noise and 13 solo representations (five from Interview 1 and eight from Interview 2). There were three agreements, eight disagreements and six solo representations of the repeat concept across the pairs of interviews. Additionally, a single interview revealed five disagreements and one agreement with the repeat representation. Drawing a conclusion is tricky because of the small number of representation gestures that participants generated across the interviews. For instance, the participants might not have developed a full conceptual representation for the concept of repetition. Thus, further work with more participants and thus a greater number of RG-metaphorical gestures could help us better assess any changes in the conceptual metaphors over time.\n\nThe results showed that participants generated more gestures in Interview 2. This may suggest that the participants needed time to more fully comprehend the concepts that they learned in Study 2. This finding could also support the notion of the use of gestures as a tool to offload some cognitive load, as  Wilson (2002)  described in the use of the embodied cognition theory  [67] . Participants in Interview 1 were asked to explain the concepts immediately after they learned them: this process did not require much cognitive work as the concepts were new and, in the participants', short-term memory. In Interview 2, the students needed to retain and recall the concepts from their long-term memories, which may have required more cognitive work. Another explanation might be the consolidation of the meaning of embodied representations in Interview 2. These gestural representations may have been easier to recall compared to symbolic linguistic representations after two weeks. Further work is needed to investigate possible explanation."
    },
    {
      "title": "Implications",
      "text": "The findings of this study have a number of implications, these are discussed below, along with suggested directions for future research:\n\nThe Role of Gestures in learning -Encourage actions (gesturing and body): Using metaphorical gestures in computing classes by encouraging learners to use gestures might help reveal misconceptions in learners' knowledge that otherwise might not be picked up on. One of the most significant challenges CS teachers face with novices is helping them to build strategies and mental models to understand abstract concepts  [68] , of which CS is full. Although this research was limited to investigating the role of gestures, whole-body movement can be used to represent ideas in a similar way as gestures. This research presented several examples of children using their whole body to represent iteration or to represent noise through dancing. Gestures can be linked to the work related to bodybased action and embodied learning, such as CS Unplugged  [69] , based on the congruence between these embodied activities and computing concepts  [70] . This research is an encouraging sign for future researchers, showing the value of reflecting on the mapping between specific body actions and computing concepts, as well as the potential to adopt these actions through gestures, especially given the way that gestures can connect action, experience, and scientific language  [60] ."
    },
    {
      "title": "The Role of Metaphors in Communication, Learning and Explaining"
    },
    {
      "title": "-",
      "text": "The role of metaphoric gestures as a tool to facilitate communication for both the gesture producer and the gesture receiver: In this study, we observed that students used gestures. However, this research did not focus on the issue of exactly when children used metaphorical gestures or their reasons for using them.\n\nA similar set of questions arise as in the previous subsection. For example, do children feel that they spontaneously need to use metaphors in the form of gestures rather than using just speech to represent their understanding of a complex idea such as iteration? Or do children decide to use metaphorical gestures because they are talking to an adult whom these gestures may assist? Future research should explore the intention associated with using metaphorical gestures, enabling educators to use these gestures to identify any misconceptions that students might have and correct their understanding.\n\n-Depth and transfer of learning: research in mathematics education  [71]  indicates that the use of representational gestures can lead to differences in the depth and transfer of learning. Although this study examined gestures through two different interviews, more work is needed to examine how gesturing evolves with learners' ability over longer time. For example, does gesturing decrease over time as concepts get more established or does it increase over time as it seems to aid understanding? Future work is needed to examine how certain gestures are related to students' learning and how gestures can support different learning needs. This could be important in evaluating the potential of teaching gestures to assess and support learning  [12] .\n\n-Tool to convey knowledge: Scopelitis argues that gesture can be employed as a tool to build representations that the speaker and hearer can use to achieve a common understanding  [68] . One participant (P37) said that the program was a \"repeat\" while gesturing the form of a container, he described the meaning of a program with the help of a metaphorical gesture. Additionally, the participant expressed the concept of \"put stuff inside\" by simulating putting one programming block inside another programming block; he then expressed \"then it will repeat\" by gesturing a circular motion. The participant was likely trying to embody and share their mental representation of a program, which gave the hearer a concrete representation. This was very similar to Solomon's finding in the case study of a CS instructor who used the gesture as a tool to convey the concept of a list  [44] .\n\n-\n\nThe relationship between the development of metaphor and understanding: Even though we observed the gestures before and after a two-week gap, we did not trace a developmental path in the children's gestures and their relationship with understanding. Future work should explore how gestural metaphors change as children develop their computing knowledge."
    },
    {
      "title": "Teachers and Gestures",
      "text": "Teachers' observations of gestures: The research highlights that many children use hand gestures to explain and communicate programming concepts. This finding underlines the relevance of gestures in communicating an understanding of computing topics to an adult (in this case the researcher). The children used hand gestures to explain and communicate programming concepts. When teachers observe students' gestures in the classroom, this can enable them to assess the understanding of learners  [37] . This work documents and describes students' use of gestures while explaining computing concepts. Future research might seek to evaluate how teachers can increase the attention they pay towards children's use of gestures in the classroom, and how to assess the use of gestures. Since gestures can serve as a way to visualize a learner's understanding, teachers may be able to identify and assess potential strengths and weaknesses by considering them. In general, an educator's awareness of their students' thinking can be improved by paying attention to the gestures and by cautiously considering the reasons for these gestures and the conceptual mappings that they produce  [72] . Virtually all of the participants in the current study produced gestures that suggested hands-on or bodily simulation of the robot's movement. Many of these gestures were not very precise about how the robot moved (for example, if it turned, it moved its head only, or turned and moved one step simultaneously). Such details of the robot's movement being inaccurate might mean that students had limited understanding of how the code functioned; this could be addressed further."
    },
    {
      "title": "Limitations",
      "text": "Although we were limited to simple computing concepts because of the age group, we found promising results. Research with a wider population with more diverse age groups, cultures and first languages would be required to examine how understandings of the importance and use of gesture might be generalised. Different metaphors may serve to support understanding at different levels. We found a pattern that might reflect similarities across cultures or external representations that are possibly similar across contexts. A more in-depth view is needed to generalise the findings using the methodology used in this paper, as the meaning of a gesture can differ between occurrences  [12, 63] . Our study's findings demonstrate the value of this investigation in future work, as we discovered patterns across participants at a very young age, suggesting the potential to investigate overlap and different meanings and concepts.\n\nAdditionally, due to the participants' young age, this work only focused on two computing concepts-program, and iteration. Further research should examine more computing concepts and identify the factors influencing the embodied nature of different concepts. Such an approach might reveal more variations in metaphorical representation than those we found in this study.\n\nMoreover, although this study examined gestures via two interviews, relatively few participants generated RGmetaphorical gestures in both interviews. For this reason, we were not able to present an analysis of qualitative differences in gestures between the two interviews nor infer from gestures a change in their understanding of the concepts over time. Further work is needed to examine how gesture evolves with learners' abilities over time, how certain gestures are related to students' learning and how gestures can support different learning needs. This could be important in evaluating teaching gestures' potential to assess and support learning  [12] ."
    },
    {
      "title": "Conclusion",
      "text": "This study explored the embodied expression of two programming concepts, program and iteration. The role and implication of embodied cognition have been studied extensively in STEM subjects, especially mathematics and science education. This study contributes to children's computing education research by providing empirical evidence for the embodied nature of their understanding of computing concepts and draws attention to the potential benefits of exploring the role of embodied representations such as gestures in this domain. The findings show that the children aged 6-7 years old produced less gestural evidence of metaphorical understanding compared to that of conceptual understanding. Additionally, this study provides supporting evidence for the ideas that gestures might have been used as a tool to offload cognitive load and as a secondary channel of communication instead of just using speech. This research also suggests how gestures might have been an indication of the embodiment of the children's computing notions."
    },
    {
      "text": "Figure 1 (Q3) Simple code aimed to describing how a simple program works"
    },
    {
      "text": "Figure 4 participant pointing to the code Figure 5 participant using RG-literally to descript the left turn Figure 6 participant moving their hand in a circular motion to represent repeat Pointing RG-literal RG-metaphorical"
    },
    {
      "text": "Figure8P10\"we used that [referring to the repeat] that if we put two things will go and will return again.\""
    },
    {
      "text": "Figure 11, P44: Hand moves two times forward."
    },
    {
      "text": "Figure 15, P32: \"You can put things like left, then it will move left left, then it will go to the flag. The repeat will make it go again\" [hand gesture in an arc]."
    },
    {
      "text": "Figure 16, P39 pointed to the code from the previous question and said \"It does it double times\", gesturing with both hands in a circular motion."
    },
    {
      "text": "Figure 17, P18 \"here is Repeat\" pointing to the repeat.\" Inside there is right right\" [ pointing inside the repeat] \"it will repeat again and again\" [big circular motion with her hand open].\""
    },
    {
      "text": "Figure 18, P19 \"If there is something here [A], it will move twice [B]\"Figure19, P01 \"Doing it again and again [hand moving from right to left] and again\"."
    },
    {
      "text": "Figure 21 P45 used of gesture to reveal implicit knowledge"
    },
    {
      "text": "Interview questions"
    },
    {
      "text": "Prompts and responses for each question"
    }
  ],
  "references": [
    {
      "title": "Restart: The resurgence of computer science in UK schools",
      "authors": [
        "N Brown",
        "S Sentance",
        "T Crick",
        "S Humphreys"
      ],
      "year": 2014,
      "doi": "10.1145/2602484"
    },
    {
      "title": "Computational thinking",
      "authors": [
        "J Wing"
      ],
      "year": 2006,
      "doi": "10.1145/1118178.1118215"
    },
    {
      "title": "Math on a sphere: Using public displays to support children's creativity and computational thinking on 3D surfaces",
      "authors": [
        "S Hsi",
        "M Eisenberg"
      ],
      "year": 2012,
      "doi": "10.1145/2307096.2307137"
    },
    {
      "title": "Metaphors We Live by",
      "authors": [
        "George",
        "M Lakoff",
        "Johnson"
      ],
      "year": 1980
    },
    {
      "title": "Frozen in the Past: When it Comes to Analogy Fears, It's Time for Us to \"let it Go",
      "authors": [
        "B Bettin",
        "L Ott"
      ],
      "year": 2021,
      "doi": "10.1145/3430665.3456381"
    },
    {
      "title": "Gesture and Speech",
      "authors": [
        "A Leroi-Gourhan"
      ],
      "year": 1993,
      "doi": "10.7551/mitpress/14647.003.0028"
    },
    {
      "title": "Gesture and the process of speech production: We think, therefore we gesture",
      "authors": [
        "M Alibali",
        "S Kita",
        "A Young"
      ],
      "year": 2000,
      "doi": "10.1080/016909600750040571"
    },
    {
      "title": "Embodiment in Mathematics Teaching and Learning: Evidence From Learners' and Teachers' Gestures",
      "authors": [
        "M Alibali",
        "M Nathan"
      ],
      "year": 2012,
      "doi": "10.1080/10508406.2011.611446"
    },
    {
      "title": "Perspective-Taking and Object Construction: Two Keys to Learning, Constuctionism in Practice: Designing, Thinking, and Learning in a Digital World",
      "authors": [
        "E Ackermann"
      ],
      "year": 1996
    },
    {
      "title": "Distributed Cognition: Toward a New Foundation for Human-Computer Interaction Research",
      "authors": [
        "J Hollan",
        "E Hutchins",
        "D Kirsh"
      ],
      "year": 2000,
      "doi": "10.1145/353485.353487"
    },
    {
      "title": "Which cognitive abilities underlie computational thinking? Criterion validity of the Computational Thinking Test",
      "authors": [
        "M Rom\u00e1n-Gonz\u00e1lez",
        "J.-C P\u00e9rez-Gonz\u00e1lez",
        "C Jim\u00e9nez-Fern\u00e1ndez"
      ],
      "year": 2017,
      "doi": "10.1016/j.chb.2016.08.047"
    },
    {
      "title": "Identifying embodied metaphors for computing education",
      "authors": [
        "A Manches",
        "P Mckenna",
        "G Rajendran",
        "J Robertson"
      ],
      "year": 2020,
      "doi": "10.1016/j.chb.2018.12.037"
    },
    {
      "title": "Improving early reading comprehension using embodied CAI",
      "authors": [
        "A Glenberg",
        "A Goldberg",
        "X Zhu"
      ],
      "year": 2010,
      "doi": "10.1007/s11251-009-9096-7"
    },
    {
      "title": "The Missing Bodies of Mathematical Thinking and Learning Have Been Found",
      "authors": [
        "R Stevens"
      ],
      "year": 2011,
      "doi": "10.1080/10508406.2011.614326"
    },
    {
      "title": "Philosophy In The Flesh: The Embodied Mind And Its Challenge To Western Thought",
      "authors": [
        "George",
        "M Lakoff",
        "Johnson"
      ],
      "year": 1999
    },
    {
      "title": "The embodied mind: Cognitive science and human experience",
      "authors": [
        "F Varela",
        "Evan Thompson",
        "Eleanor Rosch",
        "J Kabat-Zinn"
      ],
      "year": 2016,
      "doi": "10.29173/cmplct8718"
    },
    {
      "title": "Six views of embodied cognition",
      "authors": [
        "M Wilson"
      ],
      "year": 2002,
      "doi": "10.3758/bf03196322"
    },
    {
      "title": "Where mathematics comes from : how the embodied mind brings mathematics into being",
      "authors": [
        "George",
        "R Lakoff",
        "N\u00fa\u00f1ez"
      ],
      "year": 2000
    },
    {
      "title": "Embodied cognition, abstract concepts, and the benefits of new technology for implicit body manipulation",
      "authors": [
        "K Dijkstra",
        "A Eerland",
        "J Zijlmans",
        "L Post"
      ],
      "year": 2014,
      "doi": "10.3389/fpsyg.2014.00757"
    },
    {
      "title": "Learning from Gesture: How Our Hands Change Our Minds",
      "authors": [
        "M Novack",
        "S Goldin-Meadow"
      ],
      "year": 2015,
      "doi": "10.1007/s10648-015-9325-3"
    },
    {
      "title": "Designing for concreteness fading in primary computing",
      "authors": [
        "A Trory",
        "K Howland",
        "J Good"
      ],
      "year": 2018,
      "doi": "10.1145/3202185.3202748"
    },
    {
      "title": "Notional machines and introductory programming education",
      "authors": [
        "J Sorva"
      ],
      "year": 2013,
      "doi": "10.1145/2483710.2483713"
    },
    {
      "title": "Some Difficulties of Learning to Program",
      "authors": [
        "B Boulay"
      ],
      "year": 1995,
      "doi": "10.2190/3lfx-9rrf-67t8-uvk9"
    },
    {
      "title": "To Write Code: The Cultural Fabrication of Programming Notation and Practice",
      "authors": [
        "I Arawjo"
      ],
      "year": 2020,
      "doi": "10.1145/3313831.3376731"
    },
    {
      "title": "Thinking with the Body: Conceptual Integration Through Gesture in Multiviewpoint Model Construction, Language and the Creative Mind",
      "authors": [
        "D Deliema",
        "S Francis"
      ],
      "year": 2013,
      "doi": "10.1017/9781316569856.010"
    },
    {
      "title": "Metaphor in computer science",
      "authors": [
        "T Colburn",
        "G Shute"
      ],
      "year": 2008,
      "doi": "10.1016/j.jal.2008.09.005"
    },
    {
      "title": "With the future behind them: Convergent evidence from Aymara language and gesture in the crosslinguistic comparison of spatial construals of time",
      "authors": [
        "R N\u00fa\u00f1ez",
        "E Sweetser"
      ],
      "year": 2006,
      "doi": "10.1207/s15516709cog0000_62"
    },
    {
      "title": "Conceptual metaphor theory: Some criticisms and alternative proposals",
      "authors": [
        "Z K\u00f6vecses"
      ],
      "year": 2008,
      "doi": "10.1075/arcl.6.08kov"
    },
    {
      "title": "Identifying embodied metaphors in children's soundaction mappings",
      "authors": [
        "S Bakker",
        "A Antle",
        "E Van Den Hoven"
      ],
      "year": 2009,
      "doi": "10.1145/1551788.1551812"
    },
    {
      "title": "Hand and Mind: What Gestures Reveal about Thought",
      "authors": [
        "D Mcneill"
      ],
      "year": 1994,
      "doi": "10.2307/1576015"
    },
    {
      "title": "Gestures and growth points in language disorders",
      "authors": [
        "D Mcneill",
        "S Duncan"
      ],
      "year": 2011,
      "doi": "10.4324/9780203848005.ch32"
    },
    {
      "title": "What we mean by meaning",
      "authors": [
        "F Parrill",
        "E Sweetser"
      ],
      "year": 2005,
      "doi": "10.1075/gest.4.2.05par"
    },
    {
      "title": "When our hands help us understand: A meta-analysis into the effects of gesture on comprehension",
      "authors": [
        "N Dargue",
        "N Sweller",
        "M Jones"
      ],
      "year": 2019,
      "doi": "10.1037/bul0000202"
    },
    {
      "title": "Hands in the Air: Using Ungrounded Iconic Gestures to Teach Children Conservation of Quantity",
      "authors": [
        "R Ping",
        "S Goldin-Meadow"
      ],
      "year": 2008,
      "doi": "10.1037/0012-1649.44.5.1277"
    },
    {
      "title": "Gestures, but not meaningless movements, lighten working memory load when explaining math",
      "authors": [
        "S Cook",
        "T Yip",
        "S Goldin-Meadow"
      ],
      "year": 2012,
      "doi": "10.1080/01690965.2011.567074"
    },
    {
      "title": "Shuttling between depictive models and abstract rules: Induction and fallback",
      "authors": [
        "D Schwartz",
        "J Black"
      ],
      "year": 1996,
      "doi": "10.1016/s0364-0213(99)80012-3"
    },
    {
      "title": "A helping hand in assessing children's knowledge: Instructing adults to attend to gesture",
      "authors": [
        "S Kelly",
        "M Singer",
        "J Hicks",
        "S Goldin-Meadow"
      ],
      "year": 2002,
      "doi": "10.1207/S1532690XCI2001_1"
    },
    {
      "title": "Teachers' gestures facilitate students' learning: A lesson in symmetry",
      "authors": [
        "L Valenzeno",
        "M Alibali",
        "R Klatzky"
      ],
      "year": 2003,
      "doi": "10.1016/s0361-476x(02)00007-3"
    },
    {
      "title": "Don ' t Just Tell Them , Show Them ! Teachers Can Intentionally Alter their Instructional Gestures",
      "authors": [
        "A Hostetter",
        "W Street",
        "M Alibali"
      ],
      "year": 2006
    },
    {
      "title": "Embodiment in Mathematics Teaching and Learning: Evidence From Learners' and Teachers' Gestures",
      "authors": [
        "M Alibali",
        "M Nathan"
      ],
      "year": 2012,
      "doi": "10.1080/10508406.2011.611446"
    },
    {
      "title": "The role of gesture in learning: Do children use their hands to change their minds?",
      "authors": [
        "S Cook",
        "S Goldin-Meadow"
      ],
      "year": 2006,
      "doi": "10.1207/s15327647jcd0702_4"
    },
    {
      "title": "Applying a Gesture Taxonomy to Introductory Computing Concepts",
      "authors": [
        "A Solomon",
        "M Guzdial",
        "B Disalvo",
        "B Shapiro"
      ],
      "year": 2018,
      "doi": "10.1145/3230977.3231001"
    },
    {
      "title": "Embodied Representations in Computing Education: How Gesture, Embodied Language, and Tool Use Support Teaching Recursion",
      "authors": [
        "A Solomon",
        "M Bae",
        "B Disalvo"
      ],
      "year": 2020,
      "doi": "10.22318/icls2020.2133"
    },
    {
      "title": "Gesturing makes learning last",
      "authors": [
        "S Cook",
        "Z Mitchell",
        "S Goldin-Meadow"
      ],
      "year": 2008,
      "doi": "10.1016/J.COGNITION.2007.04.010"
    },
    {
      "title": "Investigating children's spontaneous gestures when programming using TUIs and GUIs",
      "authors": [
        "A Almjally",
        "K Howland",
        "J Good"
      ],
      "year": 2020,
      "doi": "10.1145/3392063.3394408"
    },
    {
      "title": "Children's perception of computer programming as an aid to designing programming environments",
      "authors": [
        "R Sheehan"
      ],
      "year": 2003,
      "doi": "10.1145/953536.953548"
    },
    {
      "title": "It's Like a Giant Brain With a Keyboard\": Children's Understandings About How Computers Work",
      "authors": [
        "J Robertson",
        "A Manches",
        "H Pain"
      ],
      "year": 2017,
      "doi": "10.1080/00094056.2017.1343589"
    },
    {
      "title": "Young children's conceptions of computers, code, and the Internet",
      "authors": [
        "P Mertala"
      ],
      "year": 2019,
      "doi": "10.1016/j.ijcci.2018.11.003"
    },
    {
      "title": "Gesture-speech mismatch and mechanisms of learning: what the hands reveal about a child's state of mind",
      "authors": [
        "M Alibali",
        "S Goldinmeadow"
      ],
      "year": 1993,
      "doi": "10.1006/cogp.1993.1012"
    },
    {
      "title": "The Effects of Emblematic Gestures on the Development and Access of Mental Representations of French Expressions",
      "authors": [
        "L Allen"
      ],
      "year": 1995,
      "doi": "10.2307/330004"
    },
    {
      "title": "Gesturing makes memories that last",
      "authors": [
        "S Cook",
        "T Yip",
        "S Goldin-Meadow"
      ],
      "year": 2010,
      "doi": "10.1016/j.jml.2010.07.002"
    },
    {
      "title": "Memory for actions: Enactment and source memory",
      "authors": [
        "S Hornstein",
        "N Mulligan"
      ],
      "year": 2004,
      "doi": "10.3758/bf03196584"
    },
    {
      "title": "Reactivation of hippocampal ensemble memories during sleep",
      "authors": [
        "M Wilson",
        "B Mcnaughton"
      ],
      "year": 1979,
      "doi": "10.1126/science.8036517"
    },
    {
      "title": "QuickStart Primary Handbook",
      "authors": [
        "M Berry"
      ],
      "year": 2015
    },
    {
      "title": "Drawing Valid Meaning from Qualitative Data: Toward a Shared Craft",
      "authors": [
        "M Miles",
        "A Huberman"
      ],
      "year": 1984,
      "doi": "10.3102/0013189X013005020"
    },
    {
      "title": "Learning Algorithmic Thinking with Tangible Objects Eases Transition to Computer Programming",
      "authors": [
        "G Futschek",
        "J Moschitz"
      ],
      "year": 2011,
      "doi": "10.1007/978-3-642-24722-4_14"
    },
    {
      "title": "The Measurement of Interrater Agreement",
      "authors": [
        "J Fleiss",
        "B Levin",
        "M Paik"
      ],
      "year": 2004,
      "doi": "10.1002/0471445428.ch18"
    },
    {
      "title": "Visible embodiment: Gestures as simulated action",
      "authors": [
        "A Hostetter",
        "M Alibali"
      ],
      "year": 2008,
      "doi": "10.3758/pbr.15.3.495"
    },
    {
      "title": "Gestures: Their Role in Teaching and Learning",
      "authors": [
        "W.-M Roth"
      ],
      "year": 2001,
      "doi": "10.3102/00346543071003365"
    },
    {
      "title": "Types of nouns",
      "authors": [
        "Lexicon Lexicon"
      ],
      "year": 2018
    },
    {
      "title": "Gestures and conceptual integration in mathematical talk",
      "authors": [
        "L Edwards"
      ],
      "year": 2009,
      "doi": "10.1007/s10649-008-9124-6"
    },
    {
      "title": "Computational Thinking in K-12: A Review of the State of the Field",
      "authors": [
        "S Grover",
        "R Pea"
      ],
      "year": 2013,
      "doi": "10.3102/0013189x12463051"
    },
    {
      "title": "The word order of languages predicts native speakers' working memory",
      "authors": [
        "F Amici",
        "A S\u00e1nchez-Amaro",
        "C Sebasti\u00e1n-Enesco",
        "T Cacchione",
        "M Allritz",
        "J Salazar-Bonet",
        "F Rossano"
      ],
      "year": 2019,
      "doi": "10.1038/s41598-018-37654-9"
    },
    {
      "title": "National curriculum in England: computing programmes of study",
      "authors": [
        "Gov",
        "Uk"
      ],
      "year": 2013,
      "doi": "10.4324/9780203769263-6"
    },
    {
      "title": "Making Children Gesture Brings Out Implicit Knowledge and Leads to Learning",
      "authors": [
        "S Broaders",
        "S Cook",
        "Z Mitchell",
        "S Goldin-Meadow"
      ],
      "year": 2007,
      "doi": "10.1037/0096-3445.136.4.539"
    },
    {
      "title": "Six views of embodied cognition",
      "authors": [
        "M Wilson"
      ],
      "year": 2002,
      "doi": "10.3758/BF03196322"
    },
    {
      "title": "Made by Hand: Gestural Practices for the Building of Complex Concepts in Face-to-Face, One-on-One Learning Arrangements",
      "authors": [
        "S Scopelitis",
        "S Mehus",
        "R Stevens"
      ],
      "year": 2010,
      "doi": "10.22318/ICLS2010.1.1127"
    },
    {
      "title": "CS unplugged, outreach and CS kinesthetic activities",
      "authors": [
        "T Bell",
        "L Lambert",
        "D Marghitu"
      ],
      "year": 2012,
      "doi": "10.1145/2157136.2157410"
    },
    {
      "title": "Embodied learning: introducing a taxonomy based on bodily engagement and task integration",
      "authors": [
        "A Skulmowski",
        "G Rey"
      ],
      "year": 2018,
      "doi": "10.1186/S41235-018-0092-9/FIGURES/1"
    },
    {
      "title": "From Action to Abstraction",
      "authors": [
        "M Novack",
        "E Congdon",
        "N Hemani-Lopez",
        "S Goldin-Meadow"
      ],
      "year": 2014,
      "doi": "10.1177/0956797613518351"
    },
    {
      "authors": [
        "A Kendon",
        "Gesture"
      ],
      "year": 1997,
      "doi": "10.1146/annurev.anthro.26.1.109"
    }
  ],
  "num_references": 71
}
