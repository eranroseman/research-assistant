{
  "paper_id": "7ZTB6CVA",
  "title": "Instruments measuring evidence-based practice behavior, attitudes, and self-efficacy among healthcare professionals: a systematic review of measurement properties",
  "abstract": "Background Evidence-based practice (EBP) is well known to most healthcare professionals. Implementing EBP in clinical practice is a complex process that can be challenging and slow. Lack of EBP knowledge, skills, attitudes, self-efficacy, and behavior can be essential barriers that should be measured using valid and reliable instruments for the population in question. Results from previous systematic reviews show that information regarding highquality instruments that measure EBP attitudes, behavior, and self-efficacy in various healthcare disciplines need to be improved. This systematic review aimed to summarize the measurement properties of existing instruments that measure healthcare professionals' EBP attitudes, behaviors, and self-efficacy. \n Methods We included studies that reported measurement properties of instruments that measure healthcare professionals' EBP attitudes, behaviors, and self-efficacy. Medline, Embase, PsycINFO, HaPI, AMED via Ovid, and Cinahl via Ebscohost were searched in October 2020. The search was updated in December 2022. The measurement properties extracted included data on the item development process, content validity, structural validity, internal consistency, reliability, and measurement error. The quality assessment, rating of measurement properties, synthesis, and modified grading of the evidence were conducted in accordance with the COSMIN methodology for systematic reviews. Results Thirty-four instruments that measure healthcare professionals' EBP attitudes, behaviors or self-efficacy were identified. Seventeen of the 34 were validated in two or more healthcare disciplines. Nurses were most frequently represented (n = 53). Despite the varying quality of instrument development and content validity studies, most instruments received sufficient ( +) ratings on content validity, with the quality of evidence graded as \"very low\" in most cases. Structural validity and internal consistency were the measurement properties most often assessed, and reliability and measurement error were most rarely assessed. The quality assessment results and overall rating of these measurement properties varied, but the quality of evidence was generally graded higher for these properties than for content validity. Conclusions Based on the summarized results, the constructs, and the population of interest, several instruments can be recommended for use in various healthcare disciplines. However, future studies should strive to use qualitative methods to further develop existing EBP instruments and involve the target population.",
  "year": 2005,
  "date": "2005",
  "journal": "BMC Med Educ",
  "publication": "BMC Med Educ",
  "authors": [
    {
      "forename": "Nils",
      "surname": "Landsverk",
      "name": "Nils Landsverk",
      "affiliation": "1  Department of Rehabilitation Science and Health Technology , Faculty of Health Science , Oslo Metropolitan University , Oslo , Norway. \n\t\t\t\t\t\t\t\t Department of Rehabilitation Science and Health Technology \n\t\t\t\t\t\t\t\t Faculty of Health Science \n\t\t\t\t\t\t\t\t Oslo Metropolitan University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Oslo \n\t\t\t\t\t\t\t\t\t Norway",
      "orcid": "0000-0001-8902-4057"
    },
    {
      "forename": "Nina",
      "surname": "Olsen",
      "name": "Nina Olsen",
      "affiliation": "2  Department of Health and Functioning , Faculty of Health and Social Sciences , Western Norway University of Applied Sciences , Bergen , Norway. \n\t\t\t\t\t\t\t\t Department of Health and Functioning \n\t\t\t\t\t\t\t\t Faculty of Health and Social Sciences \n\t\t\t\t\t\t\t\t Western Norway University of Applied Sciences \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Bergen \n\t\t\t\t\t\t\t\t\t Norway",
      "orcid": "0000-0001-8902-4057"
    },
    {
      "forename": "Therese",
      "surname": "Brovold",
      "name": "Therese Brovold",
      "affiliation": "1  Department of Rehabilitation Science and Health Technology , Faculty of Health Science , Oslo Metropolitan University , Oslo , Norway. \n\t\t\t\t\t\t\t\t Department of Rehabilitation Science and Health Technology \n\t\t\t\t\t\t\t\t Faculty of Health Science \n\t\t\t\t\t\t\t\t Oslo Metropolitan University \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Oslo \n\t\t\t\t\t\t\t\t\t Norway"
    }
  ],
  "doi": "10.1186/s13012-023-01301-3",
  "keywords": [
    "Evidence-based practice",
    "Healthcare professional",
    "Attitudes",
    "Self-efficacy",
    "Behavior",
    "Development",
    "Instrument",
    "Validity",
    "Reliability",
    "Measurement error"
  ],
  "sections": [
    {
      "title": "Background",
      "text": "Evidence-based practice (EBP) is well known to most healthcare professionals. EBP refers to the integration of the best available research evidence with clinical expertise and patient characteristics and preferences  [1] . EBP has become the gold standard in healthcare. Implementing EBP in clinical practice is associated with high-quality care, such as improved patient outcomes, reduced costs, and increased job satisfaction  [2] [3] [4] [5] [6] .\n\nImplementing EBP in clinical practice is a complex process that is challenging and slow  [3, 7] . The implementation of EBP can be hindered by barriers, including organizational, cultural, or clinician-related factors. At clinician-related level, research shows that a lack of EBP knowledge, insufficient skills, negative attitudes, low selfefficacy, and lack of EBP behaviors can be essential barriers  [8, 9] . The different steps of the EBP process require that healthcare professionals understand the concepts of EBP (knowledge) and have the practical skills to do EBP activities, such as searching electronic databases or using critical appraisal tools (skills)  [1, 10] . Further, the healthcare professionals' confidence in their ability to perform EBP activities (self-efficacy), and their beliefs in the positive benefits of EBP (attitudes), are known to be associated with the likelihood of EBP being successfully implemented in clinical practice (behavior)  [10] [11] [12] .\n\nStrategies to improve EBP implementation should be tailored based on the healthcare professionals' perceived barriers  [13] [14] [15] . However, many healthcare institutions are unaware of potential barriers that could be related to EBP knowledge, skills, attitudes, self-efficacy, and behavior among their workers  [7] . These EBP constructs should be measured using valid and reliable instruments for the population in question  [10] . Former systematic reviews have recommended using and further developing instruments such as the Fresno test as a measure of EBP knowledge and skills across healthcare disciplines based on existing documentation of validity and reliability on this instrument  [7, 10, [16] [17] [18] [19] . However, such clear recommendations do not exist for instruments that measure EBP attitudes, self-efficacy, and behavior.\n\nAlthough several reviews have assessed instruments that measure EBP attitudes, behavior or self-efficacy  [20] [21] [22] [23] [24] [25] , none focused on all three constructs, nor did they include studies across different healthcare disciplines. For instance, Hoegen et al.  [20]  included only self-efficacy instruments, and Oude Rengerink et al.  [21]  included only instruments measuring EBP behavior. The reviews from Belita et al.  [25] , Hoegen et al.  [20] , Leung et al.  [22] , Fern\u00e1ndez-Dom\u00ednguez et al.  [24] , and Buchanan et al.  [23]  included studies from one specific healthcare discipline only. A review focusing on all three constructs are needed, given the known associations between these constructs  [10] [11] [12] . In addition, including studies across different healthcare disciplines could make the review more relevant for researchers targeting an interdisciplinary population.\n\nMethodological limitations across several previous reviews may influence whether one can trust existing recommendations. Although most of the reviews evaluated the included instruments' measurement properties  [20, [22] [23] [24] [25] , only Hoegen et al.  [20]  and Buchanan et al.  [23]  assessed the risk of bias in the studies included. In addition, none of the reviews rated the quality of the instruments' development processes in detail  [26] , and only Hoegen et al.  [20]  graded the quality of the total body of evidence per instrument using a modified GRADE (Grading of Recommendations Assessment, Development, and Evaluation) approach.\n\nIn short, the results from previous systematic reviews show that information regarding high-quality instruments that measure EBP attitudes, behavior, and self-efficacy among various healthcare disciplines is still lacking. A methodologically sound review is needed to evaluate whether instruments that measure EBP attitudes, behavior, and self-efficacy can be recommended across different healthcare disciplines."
    },
    {
      "title": "Objectives",
      "text": "This systematic review aimed to summarize the measurement properties of existing instruments that measure healthcare professionals' EBP attitudes, behaviors, and self-efficacy. We aimed to review the included studies' methodological quality systematically and to evaluate the instruments' development process, content validity, structural validity, internal consistency, reliability, and measurement error in accordance with the Consensusbased standards for the selection of health measurement instruments (COSMIN) methodology for systematic reviews  [26] [27] [28] ."
    },
    {
      "title": "Methods",
      "text": "This systematic review was conducted and reported following the PRISMA 2020 checklist (Preferred Reporting Items for Systematic Reviews and Meta-Analyses)  [29] . The checklist is presented in Additional file 5."
    },
    {
      "title": "Eligibility criteria",
      "text": "Studies were included if they met the following criteria: included healthcare professionals (e.g., nurses, physiotherapists, occupational therapists, medical doctors, psychologists, dentists, pharmacists, social workers) from primary or specialized healthcare; reported findings from the development of or the validation process of self-reported EBP instruments; described instruments measuring EBP attitudes, behavior or self-efficacy, or a combination of these EBP constructs; used a quantitative or qualitative design; and published in English or a Scandinavian language.\n\nStudies were excluded based on the following criteria: included undergraduate students or samples from school setting; did not present any psychometric properties; focused on evidence-based diagnosis or management rather than on EBP in general; focused on the effect of implementation strategies rather than on the development or validation of an instrument; and described instruments measuring only EBP knowledge or skills."
    },
    {
      "title": "Information sources",
      "text": "The following databases were included in two searches conducted in October 2020 and December 2022: MED-LINE, Embase, PsycINFO, HaPI, and AMED via Ovid, Cinahl via Ebscohost, Web of Science, and Google Scholar. In addition, we used other sources to supplement the search in the electronic databases, including searches in the reference lists of included studies and searches for gray literature. The gray literature search included targeted website searches, advanced Google searches, gray literature databases and catalogs of gray literature, and searches for theses, dissertations, and conference proceedings. The search strategy is described in Additional file 1."
    },
    {
      "title": "Search strategy",
      "text": "The search strategy was developed in consultation with and conducted by two academic librarians from OsloMet University Library. The search included terms that were related to or described the nature of the objectives and the inclusion criteria and were built around the following five elements: (1) evidence-based practice, (2) health personnel, (3) measurement and instruments, (4) psychometrics, and (5) behavior, attitude, self-efficacy."
    },
    {
      "title": "Selection process",
      "text": "Titles and abstracts of studies retrieved in the search were screened independently by two review team members (NGL and TB). The studies that potentially met the inclusion criteria were identified, and the full texts of these studies were assessed for eligibility by two review members (NGL and TB). In cases of uncertainty regarding inclusion of studies, a third review member was consulted to reach a consensus (NRO). The screening and full-text assessment were conducted using Covidence systematic review software  [30] ."
    },
    {
      "title": "Data extraction",
      "text": "Data extraction was piloted on four references using a standard form completed by the first author and checked by two other review members (NRO and TB). The following data on study characteristics were extracted: author(s), publication year, title, aim, study country, study design, sample size, response rate, population/healthcare discipline description, and study setting. Data on the instruments were also extracted, including instrument name, EBP constructs measured (EBP attitudes, behaviors, and self-efficacy), theoretical framework used, EBP steps covered (ask, search, appraise, integrate, evaluate), number of items, number of subscales, scale type, instrument language, availability of questions, and translation procedure. Data on the EBP constructs measured were based on definitions from the CREATE framework (Classification Rubric for Evidence-Based Practice Assessment Tools in Education)  [10] . In line with the CREATE framework, we defined the EBP constructs as follows: (1) EBP attitudes: the values ascribed to the importance and usefulness of EBP in clinical decision-making, (2) EBP selfefficacy: the judgment regarding one's ability to perform a specific EBP activity, and (3) EBP behavior: what is being done in practice. Finally, data on the instrument's measurement properties were extracted, including data on the item development process, content validity, structural validity, internal consistency, reliability, and measurement error. Data extraction on all items was performed by the first author."
    },
    {
      "title": "Study quality assessment",
      "text": "The review members (NGL, TB, and NRO) independently assessed the methodological quality of each study, using the COSMIN risk of bias checklist for systematic reviews of self-reported outcome measures  [27] . Two members reviewed each study. The COSMIN checklist contains standards referring to the quality of each measurement property of interest in this review  [27, 31] . The review members followed COSMIN's four-point rating system, rating the standard of each property as \"very good, \" \"adequate, \" \"doubtful, \" or \"inadequate\"  [27] . The lowest rating per measurement property was used to determine the risk of bias on that particular property, following the \"worst score counts\" principle  [32] . After all the studies were assessed separately by the review members, a consensus on the risk of bias ratings was reached in face-to-face meetings."
    },
    {
      "title": "Synthesis methods",
      "text": "The evidence synthesis process was conducted using the COSMIN methodology  [26, 31] . The review members rated all the results separately, and a consensus was reached in face-to-face meetings. Instrument development and content validity studies were rated independently by the review authors according to criteria determining whether the instrument's items adequately reflected the construct to be measured  [26] . These included five criteria on relevance, one criterion on comprehensiveness, and four criteria on comprehensibility  [26] . The relevance, comprehensiveness, and comprehensibility per study were rated as sufficient (+), insufficient (-), inconsistent (+ / -) or indeterminate (?). The reviewers also rated the instruments themselves. An overall rating was given for the relevance, comprehensibility, and comprehensiveness of each instrument, combining the results from the ratings of each study with the reviewers' ratings on the same instrument. The overall rating could not be indeterminate (?) because the reviewers' ratings were always available  [26] . The assessment of instrument development studies included evaluating the methods used to generate items (concept elicitation) and the methods used to test the new instrument  [26] . COSMIN recommends using qualitative methods, involving the target population, when developing instrument items  [26] .\n\nResults for structural validity, internal consistency, reliability, and measurement error were rated independently against the COSMIN criteria for good measurement properties  [28, 33, 34] . Each measurement property was rated as sufficient ( +), insufficient ( -) or indeterminate (?). To conclude each instrument, an overall rating was given for each instrument per property by jointly assessing the results from all the available studies. If the results per property per instrument were consistent, the results could be qualitatively summarized and rated overall as sufficient ( +), insufficient ( -), inconsistent (+ / -) or indeterminate (?). More information on the COSMIN criteria for good measurement properties is provided in Additional file 2. Details on the COSMIN guideline for assessing and calculating structural validity, internal consistency, reliability, and measurement error can be found elsewhere  (28, 31) ."
    },
    {
      "title": "Certainty assessment",
      "text": "After rating the summarized results per instrument per property against the criteria for good measurement properties, we graded the quality of this evidence to indicate whether or not the overall ratings were trustworthy. The GRADE approach is used to grade the quality of evidence on four levels: high, moderate, low, and very low  [35] . We used the COSMIN's modified GRADE approach, where four of the five original GRADE factors are adopted for grading the quality of evidence in systematic reviews of patient-reported outcome measures  [28] . We downgraded the quality of evidence when there was concern about the results related to any of these four factors: risk of bias, inconsistency, imprecision or indirectness. Further details on the modified GRADE approach are provided in \"COS-MIN methodology for systematic reviews of Patient-Reported Outcome Measures (PROMs)-user manual\"  [28] . The quality of evidence was not graded in cases where the overall rating for a measurement property was indeterminate (?)  [28] . Nor was evidence graded in cases where the overall ratings were inconsistent and impossible to summarize  [31] ."
    },
    {
      "title": "Results"
    },
    {
      "title": "Study selection",
      "text": "The search strategy identified 9405 studies. Five thousand five hundred and forty-two studies were screened for eligibility, and 156 were assessed in full text. Seventy-five studies were selected for inclusion. In addition, two studies were included via a search in gray literature. A total of 77 studies were included in the review. The PRISMA flow diagram is presented in Fig.  1 ."
    },
    {
      "title": "Study characteristics",
      "text": "The 77 included studies  comprised 34 instruments measuring EBP attitudes, behavior or self-efficacy, alone or combined. Twenty-four instruments measured EBP attitudes, 21 measured behavior, and 16 measured EBP self-efficacy. Most instruments were multidimensional and included different subscales (n = 25). Eight instruments were unidimensional, and two had indeterminate dimensionality. Nurses were most frequently represented in the included studies (n = 53), followed by physiotherapists (n = 19), occupational therapists (n = 10), medical doctors (n = 14), mental health workers (n = 16), and social workers (n = 7). Ten of the included instruments had been validated in three or more healthcare disciplines  [36, 45, 56, 66, 68, 81, 85, 89, 111] . Seven instruments had been validated in two healthcare disciplines  [47, 62, 63, 73, 75, 76, 82]  and 17 had been validated in only one discipline  [48, 64, 65, 71, 78-80, 87, 93, 95, 96, 102, 105, 107, 109, 110] . Details of the included studies and participants are presented in Additional file 3."
    },
    {
      "title": "Quality assessment and results of development and content validity studies",
      "text": "Of the 77 studies included, 33 focused on instrument development and 18 focused on content validity on already developed instruments. Table  1  summarizes the quality assessment, rating, and quality of evidence on the development and content validity per instrument. The quality of concept elicitation (development of items) was rated as \"adequate\" in three studies  [85, 93, 107] , where a clearly reported and appropriate method was used and a sample representing the target population was involved. A further 19 studies received a \"doubtful\" quality rating  [36, 45, 47, 48, 62, 66, 68, 76, 78, [80] [81] [82] 89 ,"
    },
    {
      "title": "Table 1 Summarized results on quality assessment, rating, and quality of evidence on the development and content validity per instrument",
      "text": "Overall rating of results: (\n\nQuality assessment: VG = very good; A = adequate; D = doubtful; I = inadequate Quality of evidence: Modified GRADE approach  [28, 31] . Quality levels: high, moderate, low, and very low Reasons for downgrade: risk of bias = \"RoB\", Inconsistency = \"Incon\", Imprecision = \"Impre\", Indirectness = \"Indir\" \"--\": No grade due to lack of questionnaire access * When based only on reviewer's rating, i.e., not enough evidence from or inadequate quality of development study and not enough evidence from or inadequate quality of content validity study\n\nDevelopment Content validity (COSMIN box 1 and 2) Concept elicitation (COSMIN box 1a) Relevance Comprehensiveness Comprehensibility Instrument (ref) COSMIN Quality rating Sample involved?"
    },
    {
      "title": "Rating of results"
    },
    {
      "title": "Quality of evidence Rating of results"
    },
    {
      "title": "Quality of evidence Rating of results"
    },
    {
      "title": "Quality of evidence",
      "text": "EBPAS  [36, 41, 97 -99] Doubtful Yes ( +) Very low* (-) Very Low* ( +) Moderate (rob) EBPAS-50 [45] Doubtful Yes ( +) Very low * ( +) Very low * ( +) Very low * EBPAS-36 [47, 100] Doubtful Yes ( +) Low (rob) (-) Very low * ( +) Moderate (rob) EBPQ [48, 49, 51, 52, 54, 101] Doubtful Yes ( +) Very low * ( +) Very low * ( +) Low (incon + rob) EBP Beliefs [56, 58, 59, 99] Inadequate No ( +) Very low * ( +) Very low * ( +) Moderate (rob) EBP Beliefs-Short [102] Inadequate No ( +) Very low * ( +) Very low * ( +) Very low * EBP Implement [56, 58] Inadequate No ( +) Very low * ( +) Very low * ( +) Moderate (rob) EBP Implement-Short [102] Inadequate No ( +) Very low * ( +) Very low * ( +) Very low * Ethiopian EBP Implement [111] Inadequate No ( +) Very low * ( +) Very low * ( +) Low (rob \u00d7 2) Al Zoubi Questionnaire [62] Doubtful Yes ( +) Very low * ( +) Very low * ( +) Very low * EBPP-S [63] Inadequate No ( \u00b1) Very low * (-) Very low * ----Jette [64, 103] Inadequate No ( +) Very low * ( +) Moderate (rob) ( +) Moderate (rob) Bernhardsson [65] Inadequate No ( +) Very low * ( +) Low (rob \u00d7 2) ( +) Low (rob \u00d7 2) EBP inventory [66, 67] Doubtful Yes ( +) Low (rob \u00d7 2) ( +) Very low * ( +) Moderate (rob) EPIC [68] Doubtful No ( +) Low (rob \u00d7 2) ( +) Low (rob \u00d7 2) ( +) Low (rob \u00d7 2) MPAS [71] Inadequate No ( +) Very low * (-) Very low * ( +) Very low * EBPPAS [73] Inadequate No ( +) Very low * ( +) Very low * ( +) Very low * SE-EBP [76] Doubtful Yes ( +) Very low * ( +) Very low * ( +) Very low * EBPSE [78] Doubtful Yes ( +) Very low * ( +) Very low * ( +) Very low * EBP Capability Beliefs [79] Inadequate No ( +) Very low * ( +) Very low * ( +) Very low * HEAT [80] Doubtful Yes ( +) Very low * ( +) Very low * ( +) Very low * EBP-KABQ [81] Doubtful No ( +) Very low * ( +) Very low * ( +) Very low * Quick EBP-VIK [82, 84] Doubtful No ( +) Very low * ( +) Very low * ( +) Moderate (rob) HS-EBP [85] Adequate Yes ( +) Very low * ( +) Very low * ( +) Very low * EBPRS [87, 88] Inadequate No ( \u00b1) Very low * (-) Very low * ( +) Very low * EBP2 [89, 90, 92] Doubtful No ( +) Very low * ( +) Very low * ( +) Moderate (rob) ISP-D [93] Adequate Yes ( +) Low (rob \u00d7 2) ( +) Very low * ( +) Low (rob \u00d7 2) EBNAQ [95] Doubtful Yes ( +) Low (rob \u00d7 2) ( +) Very low * ( +) Low (rob \u00d7 2) Diermayr [96] Doubtful Yes ( +) Very low * (-) Very low * ( +) Very low * EBP-COQ Prof [105, 106] Doubtful Yes ( +) Low (rob \u00d7 2) ( +) Very low * ( +) Moderate (rob) EIDM competence measure [107] Adequate Yes ( +) Moderate (rob) ( +) Very low * ( +) Moderate (rob) I-SABE [108] Doubtful Yes ( +) Low (rob \u00d7 2) ( +) Very low * ( +) Low (rob \u00d7 2) Noor EBM [109] Doubtful Yes ( +) Very low * ( +) Very low * ( +) Low (rob \u00d7 2) EBP-CBFRI [110] Doubtful No ( +) Very low * ( +) Very low * ( +) Very low * 95, 96, 105, 108-110]\n\n. Some of these studies used qualitative methods to generate items, but the method, or parts of it, was not clearly described. In other studies, it was doubtful whether the included sample was representative of the target population, and some used quantitative methods. Some studies were rated as \"doubtful\" if it was stated that authors of these studies had talked or discussed the items with relevant healthcare professionals as a part of concept elicitation, but it was doubtful whether this method was suitable. Finally, 12 studies received an \"inadequate\" quality rating for concept elicitation  [56, 63-65, 71, 73, 79, 87, 102, 111] . In these cases, it was clear that no qualitative methods that involved members of the target population were used when generating items. The item generation was usually based on theory, research, or existing instruments. Content validity was also assessed as part of the development studies with cognitive interviews or pilot tests or in separate content validity studies performed after the instrument was developed, primarily studies translating an instrument. Some development studies assessed comprehensibility  [47, 56, 65, 68, 73, 76, 78, 82, 87, 89, 93, 95, 105, [107] [108] [109] [110] [111]  or comprehensiveness  [65, 68]  with interviews or pilot tests on samples representing the target population. These were rated as either \"adequate\"  [93, 107]  or \"doubtful\" quality [47, 56, 65, 68, 73, 76, 78, 82, 87, 89, 95, 105, 108-111]. The rest of the development studies could not be rated, either because it was unclear whether a pilot test or interview was performed, or which aspect of content validity was assessed. Most of the content validity studies assessed comprehensibility [41, 49, 51, 52, 54, 58, 59, 84, 88, 90, 92, 97-101, 103, 106] and only a few assessed relevance or comprehensiveness [59, 84, 88, 99, 103]. All content validity studies were rated as doubtful quality [41, 49, 51, 52, 54, 58, 59, 84, 88, 90, 92, 97-101, 103, 106]."
    },
    {
      "title": "Results of synthesis and certainty of evidence on content validity",
      "text": "With the combined results from each study's ratings of relevance, comprehensiveness, and comprehensibility and the reviewers' ratings, each instrument was given an overall rating (Table  1 ). Most instruments were rated as sufficient ( +) on relevance and comprehensibility, and only 6 out of 34 instruments were rated as insufficient ( -) on comprehensiveness. The quality of evidence was graded as \"very low\" in most cases, primarily due to no content validity studies (or inadequate quality) and not enough evidence from (or inadequate quality of ) the development studies. The overall grade was, in these cases, based solely on the reviewers' ratings and was therefore downgraded to \"very low\"  [26] .\n\nSeven instruments (EBPAS-36, EBP Inventory, EPIC, ISP-D, EBNAQ, EBP-COQ Prof, and I-SABE) had \"low\" quality evidence of sufficient \"relevance\" from concept elicitation studies of doubtful quality  [26] . One instrument (EIDM competence measure) had \"moderate\" quality evidence of sufficient \"relevance\" from a development study of adequate quality. Two instruments (EPIC and Bernhardsson) had \"low\", and another (Jette) had \"moderate\" quality evidence of sufficient \"comprehensiveness\" from a development study of doubtful quality and a content validity study of doubtful quality  [26] .\n\nTen instruments (EBPAS, EBPAS 36, EBP inventory, EBP Beliefs, EBP Implement, Jette, Quick EBP VIK, EBP2, EBP-COQ Prof, and EIDM competence measure) had \"moderate\" quality evidence of sufficient \"comprehensibility\" from content validity studies of doubtful quality or development studies of adequate quality  [26] . In addition, eight instruments (EBPQ, EPIC, Bernhardsson, ISP-D, EBNAQ, I-SABE, Noor EBM, and Ethiopian EBP Implement) had \"low\" quality evidence of sufficient \"comprehensibility\" from development studies of doubtful quality or content validity studies of doubtful quality but with inconsistent results  [26] ."
    },
    {
      "title": "Quality assessment and results of structural validity and internal consistency studies",
      "text": "Structural validity was assessed in 63 studies and internal consistency in 69 studies. The quality assessment and results of rating of structural validity and internal consistency per study are presented in detail in Additional file 4.\n\nTo test structural validity, most studies used exploratory factor analyses (EFA) (n = 26) or confirmatory factor analyses (CFA) (n = 34), and two studies used IRT/ Rasch analyses. Since CFA is preferred over EFA in the COSMIN methodology  [31] , only the results of CFA were rated in studies where both EFA and CFA were conducted. The quality of structural validity testing was rated as \"very good\" in 33 studies  [36-38, 40, 42-44 , 47, 49, 50, 53, 55, 72, 74, 75, 77, 79-81, 84, 86, 88, 90, 92, 94, 97-100, 105, 106, 110], \"adequate\" in 19 studies [39, 45, 48, 51, 52, 57, 58, 60, 62, 69, 76, 89, 91, 95, 108, 109, 111], \"doubtful\" in 9 studies [46, 56, 59, 61, 63, 83, 102], and as \"inadequate\" in two studies  [66, 73] . In both cases inadequate ratings were given due to low sample sizes  [31] .\n\nTo test internal consistency of the items, most studies calculated and reported a Cronbach's alpha (n = 67), and two studies calculated and reported a person separation index. The quality of internal consistency calculations was rated as \"very good\" in 64 studies  [36-39, 41-45, 47-63, 66, 67, 69, 71-81, 83, 84, 86, 88-92, 94 , 95, 97, 99-102, 104-106, 108, 110] and as \"inadequate\" in five studies  [46, 60, 98, 109, 111] . Inadequate ratings were given when a Cronbach's alpha was not reported for each unidimensional subscale in a multidimensional instrument  [31] ."
    },
    {
      "title": "Results of synthesis and certainty of evidence of structural validity and internal consistency",
      "text": "Qualitatively summarized results, overall rating, and quality of evidence (COSMIN GRADE) on structural validity and internal consistency per instrument are presented in detail in Tables  2  and  3 .\n\nEighteen instruments were rated overall as sufficient ( +) structural validity (EBPAS, EBPAS-50, EBPQ, EBP Belief-single factor, EBP Implement-single factor, EBPP-S, EPIC, MPAS, HEAT, Quick EBP-VIK, HS-EBP, EBPRS, ISP-D, EBNAQ, EBP Beliefs short, EBP Implement Short, EBP-CBFRI, and Ethiopian EBP Implement), with the quality of evidence ranging from \"high\" to \"low. \" Reasons for downgrading the quality of evidence were either \"risk of bias\" or \"inconsistency\". Six instruments were rated overall as insufficient ( -) structural validity (EBP belief-multifactorial, EBP implement-multifactorial, EBPPAS-s, EBP-KABQ, EBP-COQ Prof, and I-SABE), with the quality of evidence ranging from \"high\" to \"moderate. \" The reasons for downgrading were \"inconsistency\" and \"risk of bias. \" Four instruments were rated overall as inconsistent (+ / -) structural validity (EBPPAS, SE-EBP, EBP2, and EBPAS-36). In these three cases, results were inconsistent and it was not possible to give an overall rating as sufficient or insufficient (e.g., an overall rating based on the majority of studies)  [31] . Finally, four instruments were rated overall as indeterminate (?) structural validity (Al Zoubi Q, EBP Inventory, EBP capability beliefs, and Noor EBM) because not all the information needed for a sufficient rating was reported  [31] .\n\nRegarding internal consistency, 16 instruments were rated overall as indeterminate (?) (EBP belief-multifactorial, EBP implement-multifactorial, Al Zoubi Q, EBP Inventory, EBPPAS, EBPPAS-s, SE-EBP, EBPSE, EBP capability beliefs, EBP-KABQ, EBP2, EBP-COQ Prof, I-SABE, Noor EBM, Ethiopian EBP Implement, and EBPAS-36). Most of these instruments had Cronbach's alpha values that met the criteria for sufficient internal consistency (\u03b1 > 0.70). However, since evidence of structural validity is a prerequisite of internal consistency, they were rated as indeterminate (?) according to the COSMIN methodology  [28] . Furthermore, the summarized result of internal consistency was rated and graded per subscale in cases of multifactorial instruments. This led to several instruments receiving different ratings on different subscales, such as sufficient ( +), insufficient ( -) or inconsistent (+ / -) (EBPAS, MPAS, Quick EBP VIK, ISP-D, and EBNAQ). Seven multifactorial and five unidimensional instruments were rated as sufficient ( +) on all subscales or full scales (EBPAS-50, EBPQ, EBP Beliefs-single factor, EBP Implement-single factor, EBPP-S, EPIC, HEAT, HS-EBP, EBPRS, EBP Beliefs-Short, EBP Implement-Short, and EBP-CBFRI). The quality of evidence ranged from \"high\" to \"low, \" and the most common reason for downgrading was that the quality of evidence of structural validity on the same instrument set the starting point for the grading of internal consistency  [31] ."
    },
    {
      "title": "Quality assessment and results of reliability and measurement error studies",
      "text": "Reliability was assessed in 22 studies, and measurement error in five studies. The quality assessment and results of the rating of reliability and measurement error per study are presented in detail in Additional file 4.\n\nTo test reliability, 18 studies calculated and reported an intraclass correlation coefficient (ICC), two used Pearson's correlation, and two used the percentage of agreement. The quality of reliability testing was rated as \"very good\" in two studies  [41, 67] , \"adequate\" in 12 studies  [39, 64, 66, 69, 83, 84, [89] [90] [91] [92] 105 , 106], \"doubtful\" in six studies  [46, 50, 52, 54, 70, 96] , and as \"inadequate\" in two studies  [65, 103] . Reasons for a \"doubtful\" rating were that time intervals between measurements were longer than recommended or it was unclear whether respondents were stable between measurements or whether only Pearson's or Spearman's correlation coefficients were calculated  [31] . The reason for the \"inadequate\" rating was that no ICC, Pearson's or Spearman's correlation coefficients were calculated  [31] .\n\nTo test measurement error, all studies calculated standard error of measurement (SEM), smallest (minimal) detectable change (SDC) or limits of agreement (LoA). Only one study reported information on minimal important change (MIC). The quality of measurement error testing was rated as \"very good\" in two studies  [41, 67] , \"adequate\" in two studies  [69, 92] , and as \"doubtful\" in one study  [70] . The reason for the \"doubtful\" rating was that a time interval between measurements was longer than recommended."
    },
    {
      "title": "Results of synthesis and certainty of evidence of reliability and measurement error",
      "text": "Qualitatively summarized results, overall rating, and quality of evidence (COSMIN GRADE) on reliability and measurement error are presented in detail in Tables  4  and  5 .\n\nThe summarized result of reliability was rated and graded per subscale in cases of multifactorial instruments. This led to four instruments receiving different overall ratings on different subscales, such as sufficient ( +), insufficient (-) or inconsistent (+ / -) reliability (EBPAS, EBPQ, Quick EBP-VIK, and EBP2). Three instruments were rated overall as sufficient ( +) The quality of evidence ranged from \"high\" to \"low. \" Reasons for downgrading the quality of evidence were either \"inconsistency, \" \"risk of bias\" or \"imprecision. \" Four instruments were rated overall as indeterminate (?) reliability (EBPAS-50, EBP (Jette), EBP (Bernhardsson), and EBP (Diermayr)). The reasons for indeterminate ratings were that ICC was not calculated, not reported or not reported in sufficient detail to allow rating and grading  [31] .\n\nRegarding measurement error, one instrument was rated overall as sufficient ( +), with the quality of evidence graded as \"moderate. \" It was downgraded for imprecision due to the small sample size. Since MIC was not defined, three other instruments were rated overall as indeterminate (?) measurement error  [31] ."
    },
    {
      "title": "Discussion",
      "text": "This review sought to summarize measurement properties of existing instruments that measure healthcare professionals' EBP attitudes, behaviors, and self-efficacy. We evaluated the instruments' development process, content validity, structural validity, internal consistency, reliability, and measurement error. Thirty-four instruments measuring EBP attitudes, behavior or self-efficacy, alone or combined, were identified.\n\nThe assessment of instrument development studies revealed that only three instruments received an \"adequate\" quality rating on concept elicitation (HS-EBP, ISP-D, and EIDM competence measure)  [85, 93, 107 ]. The rest were rated \"doubtful\" or \"inadequate. \" Reasons for \"doubtful\" ratings were mainly related to the quality of the qualitative methods used to generate items and \"inadequate\" ratings were given when no qualitative methods seemed to have been used. The use of well-designed qualitative methods when constructing the items is emphasized in the updated COS-MIN methodology (2018) that was used in this review  [26] . However, over two-thirds of the development studies included in this review were published before the updated COSMIN methodology was published in 2018  [26] . Thus, assessing instrument development studies based on a detailed and standardized methodology to which the developers did not have access when developing instruments can be somewhat strict. At the same time, the quality of the development process (concept elicitation) has not, to our knowledge, been rated in detail in previous reviews of EBP instruments  [20] [21] [22] [23] [24] [25] . Thus, our findings underpin the importance that future instrument development studies should involve the target population using qualitative methods to generate items for an EBP instrument.\n\nOverall rating of results: ( +) = sufficient result; ( -) = insufficient result; (?) = indeterminate result; ( \u00b1) = inconsistent results Quality of evidence: Modified GRADE-approach  [28, 31] . Quality levels: high, moderate, low, and very low Reasons for downgrade: Risk of bias = \"RoB\", Inconsistency = \"Incon\", Imprecision = \"Impre\", Indirectness = \"Indir\" CFI Comparative fit index, RMSEA Root mean square error of approximation, SRMR Standardized square residual, EFA Exploratory factor analysis, CFA Confirmatory factor analysis  The summarized results on internal consistency showed that several instruments were rated overall as indeterminate (?) despite meeting the criteria for a sufficient ( +) rating (Cronbach's alpha > 0.70). Although measuring \"how well items correlate, \" Cronbach's alpha is often misinterpreted as a measure of the dimensionality of a scale. Whether the scores on a scale reflect the dimensionality of the construct measured is defined as structural validity and is most often assessed by factor analysis ([112], p. 169-170,  [113] ). Evidence of unidimensionality of a scale or subscale is an assumption that needs to be verified before calculating Cronbach's alpha to assess the interrelatedness of the items  [113] . Though internal consistency helps assess whether items on a scale or subscale are related, evidence of structural validity must come first to ensure that the interrelated items are on a scale or subscale that also reflects the construct's dimensionality. The rating of internal Overall rating of results: (\n\nQuality of evidence: modified GRADE approach  [28, 31] . Quality levels: high, moderate, low, and very low Reasons for downgrade: risk of bias = \"RoB\", Inconsistency = \"Incon\", imprecision = \"Impre\", indirectness = \"Indir\" \u03b1 = Cronbach's alpha consistency in this review is based on the COSMIN criteria for whether or not evidence of unidimensionality on the scale exists  [31] . Indeterminate (?) ratings on internal consistency alone will not lead to an instrument not being recommended in this review, since this requires high-quality evidence of insufficient (-) measurement properties. This review's target population was healthcare professionals, and the number of healthcare disciplines on which an instrument was validated was one of the factors\n\nTable 4 Qualitatively summarized results, overall rating, and quality of evidence (GRADE) on reliability per instrument Quality of evidence: modified GRADE approach  [28, 31] . Quality levels: high, moderate, low, and very low Reasons for downgrade: risk of bias = \"RoB\", inconsistency = \"Incon\", imprecision = \"Impre\", indirectness = \"Indir\""
    },
    {
      "title": "Instrument (reference(s)) Summarized result on reliability",
      "text": "Overall rating Quality of evidence"
    },
    {
      "title": "Quality level Reason",
      "text": "EBPAS  [39, 41]  ICC range per subscale: 1. Requirements 0.55-0.\n\n80, 2. Appeal 0.40-0.56, 3. Openness 0.64-0.71, 4. Divergence 0.48-0.74 1, 3, 4: ( \u00b1) 2: ( -) No grade High EBPAS-50 [46] Pearson's correlation total scale = 0.344. Indeterminate rating (?) since ICC not calculated (?) No grade EBPQ [50, 52, 54] ICC range per subscale: 1. Attitudes: 0.44-0.86, 2. Practice: 0.78-0.84, 3. Knowledge/skills: 0.70-0.86 1: ( \u00b1) 2, 3: ( +) No grade Moderate -1 RoB Jette [64, 103] ICC ranged from 0.37 to 0.90 (50% > 0.70). Indeterminate rating (?) since ICC per subscale not reported (?) No grade Bernhardsson [65] Indeterminate rating (?) since no ICC or Pearson's or Spearman's correlations reported (?) No grade EBP inventory [66, 67] ICC range per subscale: 1. Decision making = 0.71-0.78, 2. Subjective norm = 0.63-0.86, 3. Attitude = 0.52-0.82, 4. Perceived behavioral control = 0.80-0.83, 5. Intention and behavior = 0.76-0.86 1, 4, 5: ( +) 2, 3: ( +) High Moderate -1 Incon EPIC [69, 70] ICC range = 0.89-0.92 ( +) Moderate -1 RoB Quick EBP-VIK [83, 84] ICC range per subscale: 1. Value = 0.51-0.57, 2. Knowledge = 0.70-0.88, 3. Implement = 0.63-0.84 1: ( -) 2: ( +) 3: ( \u00b1) High High No grade EBP2 [89-92] ICC range per subscale (five-factor model). 1. Relevance = 0.69-0.94, 2. Terminology = 0.79-0.94, 3. Confidence = 0.76-0.95, 4. Practice = 0.45-0.92, 5. Sympathy = 0.47-0.77 1, 4: ( +) 2, 3: ( +) 5: ( \u00b1) Moderate High No grade Incon EBP Diermayr [96] Mean ICC 0.67 (range 0.40-0.89). Indeterminate rating (?) since ICC per subscale not reported (?) No grade EBP-COQ Prof [105, 106] ICC per subscale (four-factor model): 1. Attitude toward EBP = 0.840, 2. EBP knowledge = 0.966, 3. EBP skills = 0.815, 4. EBP utilization = 0.876 ( +) Moderate -1 impre Instrument (reference) Summarized result on measurement error Overall rating Quality of evidence Quality level Reason EBPAS [41] LoA < SRD on all subscales ( +) Moderate -1 impre EBP inventory [67] MDC95 (absolute value) range from 3.5 to 7.2. Indeterminate rating (?) since MIC not defined (?) No grade EPIC [69, 70] MDC95 (absolute value) range from 4.1 to 4.6. Indeterminate rating (?) since MIC not defined (?) No grade EBP2 [92] SEM: ranging from 0.29 to 0.44. Indeterminate rating (?) since MIC not defined (?) No grade considered when making categories of recommendations. While 17 out of the 34 included instruments were validated on two or more healthcare disciplines, 17 were validated on only one [48, 64, 65, 71, 78-80, 87, 93, 95, 96, 102, 105, 107, 109, 110]. When an instrument is validated in only one healthcare discipline, the results from a validation study may not apply if an instrument is used on a population that differs from the one on which the instrument was validated ([114], p. 230-231). Studies have shown that there may be differences between healthcare disciplines in terms of self-reported levels of EBP knowledge, attitudes, and behavior [115, 116]. It is unknown whether interdisciplinary differences in EBP knowledge, attitudes or behavior would directly affect how the items in a questionnaire are understood or to what degree they are perceived as relevant. However, knowing that a questionnaire only can be considered valid for the population on which it has been validated ([112], p.58-59), readers of this review should bear in mind that the results may not be generalizable to other populations. Readers should have a clear conception of the population on which the instrument is tested and of the population intended to target when choosing an instrument for use in future studies or clinical practice. This review's inclusion of studies from various healthcare disciplines may have contributed new knowledge to the current evidence base, identifying several valid instruments over at least two disciplines.\n\nMost of the instruments included in this review were initially developed in English and in different Englishspeaking countries. Several of these instruments have been translated into other languages and used in various countries. Ideally, an instrument translation process should be conducted according to well-known guidelines to ensure that a translated instrument is valid in another language  [112, 117, 118] . In this review, we did not assess the quality of the translation process, as this was not part of the COSMIN methodology recommendations used to conduct this review  [26, 31] . As such, readers are advised to consider the quality of the translation process if they consider using results from studies included in this review that involved translations of instruments."
    },
    {
      "title": "Limitations",
      "text": "Variations in definitions of EBP constructs between the included studies presented a challenge in the review process. Clearly defined constructs are essential to instrument development and are a prerequisite for using quantitative questionnaires to measure nonobservable constructs like EBP attitudes, self-efficacy, and behavior ([112], p. 151-152). In some cases, the differences in definitions of constructs and use of terminology made it challenging to classify the included instruments in terms of the EBP constructs measured. To meet this challenge, we classified the instruments using the CREATE framework's definitions of EBP attitudes, self-efficacy, and behavior mentioned earlier in this review  [10] . For some instruments, the constructs were defined with names and terminology other than those used in the CREATE framework. The differences in definitions of constructs and use of terminology may also have affected the study selection of this review, with potentially relevant studies being overlooked and not being included. To meet this challenge, all titles and abstracts were screened by two independent review members, and a third reviewer was consulted in cases of uncertainty. Still, relevant studies and instruments may have been missed. Even though EBP theory, models, and frameworks exist, there is still a need to develop a more cohesive and clear theoretical articulation regarding EBP and the measurement of it  [10, 119] .\n\nFurthermore, all the included instruments are selfreported, the most common method to measure EBP constructs. Some consider only objectively measured EBP outcomes as high-quality instruments due to the potential of recall and social desirability biases in selfreported instruments  [16, 17, 22, 23] . Despite the risk of bias, others recommend using self-reported instruments as a practical option when time is an issue and an extensive, objective measurement is practically impossible  [119] . In addition, it has been questioned whether the extensive focus on objectivity in EBP instruments is the only right way forward, and qualitative and mixed methods have been suggested for a richer understanding of EBP  [119] . The use of a standardized and rigorous methodology (COSMIN) throughout this review may have reduced possible methodological limitations and increased the likelihood that the results and recommendations could be trusted, despite the potential risk of bias connected to self-reported instruments."
    },
    {
      "title": "Rationale for recommendations and implications of future research",
      "text": "Recommendations of instruments in this review are based on the summarized results and grading of the evidence concerning the construct and population of interest. The recommendations are guided by the COSMIN methodology but are not categorized similarly  [31] . The three categories are categorized based on the number of healthcare disciplines on which the instrument is validated and on the number of EBP constructs the instrument measures. Common for all three categories is that, for an instrument to be recommended, there must be evidence of sufficient ( +) content validity (any level) and no high-quality evidence of any insufficient ( -) measurement properties  [31] . Being recommended means that an instrument has the potential to be recommended, even though it does not have exclusively high-quality evidence of sufficient measurement properties. This aligns with research that suggests building upon existing instruments when measuring EBP attitudes, self-efficacy, and behavior  [10] . Using and adapting existing instruments could also help to avoid the so-called \"one-time use phenomenon, \" where an instrument is developed for a specific situation and not further tested and validated in other studies ([120], p.238)."
    },
    {
      "title": "Recommendations",
      "text": "Instruments validated in at least two healthcare disciplines that measure two or more of the constructs in question (attitudes, behavior, self-efficacy) include the following: EBP Inventory  [66] , Al Zoubi questionnaire  [62] , EBPPAS  [73] , HS-EBP  [85] , EBP2  [89] , and I-SABE  [108] . Furthermore, instruments validated in at least two healthcare disciplines but that measure only one of the constructs in question include the following: EBPAS-50  [45] , EBP Beliefs (single factor)  [56] , EBP implement (single factor)  [56] , EPIC  [68] , SE-EBP  [76] , and Ethiopian EBP Implement  [111] . Finally, instruments validated in only one discipline that measures one or more of the constructs in question include the following: EBPQ  [48] , EBP (Jette)  [64] , EBP  (Bernhardsson)    [65] , EBPSE  [78] , EBP Capability beliefs  [79] , HEAT  [80] , Quick EBP-VIK  [82] , ISP-D  [93] , EBNAQ  [95] , EBP Implement short [102], EIDM competence measure [107], Noor EBM [109], and EBP-CBFRI [110]."
    },
    {
      "title": "Conclusions",
      "text": "This review identified 34 instruments that measure healthcare professionals' EBP attitudes, behaviors, or self-efficacy. Seventeen instruments were validated in two or more healthcare disciplines. Despite the varying quality of instrument development and content validity studies, most instruments received sufficient ( +) ratings on content validity, though with a \"very low\" quality of evidence. The overall rating of structural validity, internal consistency, reliability, and measurement error varied, as did the quality of evidence.\n\nBased on the summarized results, the constructs, and the population of interest, we identified several instruments that have the potential to be recommended for use in different healthcare disciplines. Future research measuring EBP attitudes, behavior, and self-efficacy should strive to build upon and further develop existing EBP instruments. In cases where new EBP instruments are being developed, the generation of questionnaire items should include qualitative methods involving members of the target population. In addition, future research should focus on reaching a clear articulation of and a shared conception of EBP constructs."
    },
    {
      "text": "Fig. 1 PRISMA flow diagram of the selection process"
    },
    {
      "text": "Intraclass correlation coefficientRating of results: ( +) = sufficient result; ( -) = insufficient result; (?) = indeterminate result; ( \u00b1) = inconsistent results"
    },
    {
      "text": "Qualitatively summarized results, overall rating, and quality of evidence (GRADE) on structural validity per instrument"
    },
    {
      "text": "(continued)"
    },
    {
      "text": "Qualitatively summarized results, overall rating, and quality of evidence (GRADE) on internal consistency per instrument"
    },
    {
      "text": "(continued)"
    },
    {
      "text": "Qualitatively summarized results, overall rating, and quality of evidence (GRADE) on measurement error per instrument SEM Standard error of measurement, LoA Limits of agreement, SDC Smallest detectable change, MIC Minimal important change"
    }
  ],
  "references": [
    {
      "title": "Sicily statement on evidence-based practice",
      "authors": [
        "M Dawes",
        "W Summerskill",
        "P Glasziou",
        "A Cartabellotta",
        "J Martin",
        "K Hopayian"
      ],
      "year": 2005,
      "doi": "10.1186/1472-6920-5-1"
    },
    {
      "title": "Benefits of a regional evidence-based practice fellowship program: a test of the ARCC model",
      "authors": [
        "S Kim",
        "L Ecoff",
        "C Brown",
        "A Gallo",
        "J Stichler",
        "J Davidson"
      ],
      "year": 2017,
      "doi": "10.1111/wvn.12199"
    },
    {
      "title": "The First U S study on nurses' evidence-based practice competencies indicates major deficits that threaten healthcare quality, safety, and patient outcomes",
      "authors": [
        "B Melnyk",
        "L Gallagher-Ford",
        "C Zellefrow",
        "S Tucker",
        "B Thomas",
        "L Sinnott"
      ],
      "year": 2018,
      "doi": "10.1111/wvn.12269"
    },
    {
      "title": "The establishment of evidence-based practice competencies for practicing registered nurses and advanced practice nurses in real-world clinical settings: proficiencies to improve healthcare quality, reliability, patient outcomes, and costs",
      "authors": [
        "B Melnyk",
        "L Gallagher-Ford",
        "L Long",
        "E Fineout-Overholt"
      ],
      "year": 2014
    },
    {
      "title": "Does evidence-based practice improve patient outcomes? An analysis of a natural experiment in a Spanish hospital",
      "authors": [
        "J Emparanza",
        "J Cabello",
        "A Burls"
      ],
      "year": 2015
    },
    {
      "title": "Clinical guidelines and patient related outcomes: summary of evidence and recommendations",
      "authors": [
        "L De Vasconcelos",
        "L De Oliveira Rodrigues",
        "Mrc Nobre"
      ],
      "year": 2019
    },
    {
      "title": "Key considerations for selecting instruments when evaluating healthcare professionals' evidencebased practice competencies: A discussion paper",
      "authors": [
        "H Saunders",
        "K Vehvilainen-Julkunen"
      ],
      "year": 2018
    },
    {
      "title": "What do physical therapists think about evidence-based practice? A systematic review",
      "authors": [
        "T Da Silva",
        "Costa Lda",
        "C Garcia",
        "A Costa"
      ],
      "year": 2015
    },
    {
      "title": "Barriers to evidence-based practice implementation in physiotherapy: a systematic review and meta-analysis",
      "authors": [
        "M Paci",
        "G Faedda",
        "A Ugolini",
        "L Pellicciari"
      ],
      "year": 2021
    },
    {
      "title": "Sicily statement on classification and development of evidence-based practice learning assessment tools",
      "authors": [
        "J Tilson",
        "S Kaplan",
        "J Harris",
        "A Hutchinson",
        "D Ilic",
        "R Niederman"
      ],
      "year": 2011,
      "doi": "10.1186/1472-6920-11-78"
    },
    {
      "title": "Practitioner and organizational barriers to evidence-based practice of physical therapists for people with stroke",
      "authors": [
        "N Salbach",
        "S Jaglal",
        "N Korner-Bitensky",
        "S Rappolt",
        "D Davis"
      ],
      "year": 2007
    },
    {
      "title": "Practicing healthcare professionals' evidence-based practice competencies: an overview of systematic reviews",
      "authors": [
        "H Saunders",
        "L Gallagher-Ford",
        "T Kvist",
        "K Vehvil\u00e4inen-Julkunen"
      ],
      "year": 2019,
      "doi": "10.1111/wvn.12363"
    },
    {
      "title": "Framework of policy recommendations for implementation of evidence-based practice: a systematic scoping review",
      "authors": [
        "D Ubbink",
        "G Guyatt",
        "H Vermeulen"
      ],
      "year": 2013
    },
    {
      "title": "Tailored interventions to address determinants of practice",
      "authors": [
        "R Baker",
        "J Camosso-Stefinovic",
        "C Gillies",
        "E Shaw",
        "F Cheater",
        "S Flottorp"
      ],
      "year": 2015,
      "doi": "10.1002/14651858.CD005470.pub3"
    },
    {
      "title": "What drives change? Barriers to and incentives for achieving evidence-based practice",
      "authors": [
        "R Grol",
        "M Wensing"
      ],
      "year": 2004
    },
    {
      "title": "Instruments for evaluating education in evidence-based practice: a systematic review",
      "authors": [
        "T Shaneyfelt",
        "K Baum",
        "D Bell",
        "D Feldstein",
        "T Houston",
        "S Kaatz"
      ],
      "year": 2006
    },
    {
      "title": "A systematic review and taxonomy of tools for evaluating evidence-based medicine teaching in medical education",
      "authors": [
        "B Kumaravel",
        "J Hearn",
        "L Jahangiri",
        "R Pollard",
        "C Stocker",
        "D Nunan"
      ],
      "year": 2020
    },
    {
      "title": "Evidence-based practice educational intervention studies: a systematic review of what is taught and how it is measured",
      "authors": [
        "L Albarqouni",
        "T Hoffmann",
        "P Glasziou"
      ],
      "year": 2018,
      "doi": "10.1186/s12909-018-1284-1"
    },
    {
      "title": "Validation of the Fresno test of competence in evidence based medicine",
      "authors": [
        "K Ramos",
        "S Schafer",
        "S Tracz"
      ],
      "year": 2003
    },
    {
      "title": "Measuring self-efficacy and outcome expectancy in evidence-based practice: A systematic review on psychometric properties",
      "authors": [
        "P Hoegen",
        "Cma De Bot",
        "M Echteld",
        "H Vermeulen"
      ],
      "year": 2021
    },
    {
      "title": "Tools to assess evidence-based practice behaviour among healthcare professionals",
      "authors": [
        "Oude Rengerink",
        "K Zwolsman",
        "S Ubbink",
        "D Mol",
        "B Van Dijk",
        "N Vermeulen"
      ],
      "year": 2013
    },
    {
      "title": "Systematic review of instruments for measuring nurses' knowledge, skills and attitudes for evidence-based practice",
      "authors": [
        "K Leung",
        "L Trevena",
        "D Waters"
      ],
      "year": 2014
    },
    {
      "title": "Survey instruments for knowledge, skills, attitudes and behaviour related to evidence-based practice in occupational therapy: a systematic review",
      "authors": [
        "H Buchanan",
        "N Siegfried",
        "J Jelsma"
      ],
      "year": 2016,
      "doi": "10.1002/oti.1398"
    },
    {
      "title": "Validity and reliability of instruments aimed at measuring Evidence-Based practice in physical therapy: a systematic review of the literature",
      "authors": [
        "J Fern\u00e1ndez-Dom\u00ednguez",
        "A Ses\u00e9-Abad",
        "J Morales-Asencio",
        "Oliva-Pascual- Vaca",
        "A Salinas-Bueno",
        "I De",
        "Pedro G\u00f3mez"
      ],
      "year": 2014
    },
    {
      "title": "Measures of evidence-informed decision-making competence attributes: a psychometric systematic review",
      "authors": [
        "E Belita",
        "J Squires",
        "J Yost",
        "R Ganann",
        "T Burnett",
        "M Dobbins"
      ],
      "year": 2020,
      "doi": "10.1186/s12912-020-00436-8"
    },
    {
      "title": "COSMIN methodology for evaluating the content validity of patient-reported outcome measures: a Delphi study",
      "authors": [
        "C Terwee",
        "Cac Prinsen",
        "A Chiarotto",
        "M Westerman",
        "D Patrick",
        "J Alonso"
      ],
      "year": 2018
    },
    {
      "title": "COSMIN risk of bias checklist for systematic reviews of patientreported outcome measures",
      "authors": [
        "L Mokkink",
        "Hcw Vet",
        "Cac Prinsen",
        "D Patrick",
        "J Alonso",
        "L Bouter"
      ],
      "year": 2018,
      "doi": "10.1007/s11136-017-1765-4"
    },
    {
      "title": "COSMIN guideline for systematic reviews of patient-reported outcome measures",
      "authors": [
        "Cac Prinsen",
        "L Mokkink",
        "L Bouter",
        "J Alonso",
        "D Patrick",
        "Hcw Vet"
      ],
      "year": 2018
    },
    {
      "title": "The PRISMA 2020 statement: an updated guideline for reporting systematic reviews",
      "authors": [
        "M Page",
        "J Mckenzie",
        "P Bossuyt",
        "I Boutron",
        "T Hoffmann",
        "C Mulrow"
      ],
      "year": 2021
    },
    {
      "title": "Covidence systematic review software",
      "doi": "10.28920/dhm54.1.47-56"
    },
    {
      "title": "COSMIN methodology for systematic reviews of Patient-Reported Outcome Measures (PROMs) -user manual",
      "authors": [
        "L Mokkink",
        "C Prinsen",
        "D Patrick",
        "J Alonso",
        "L Bouter",
        "H Vet"
      ],
      "year": 2018
    },
    {
      "title": "Rating the methodological quality in systematic reviews of studies on measurement properties: a scoring system for the COSMIN checklist",
      "authors": [
        "C Terwee",
        "L Mokkink",
        "D Knol",
        "R Ostelo",
        "L Bouter",
        "H Vet"
      ],
      "year": 2012
    },
    {
      "title": "Validity and measurement precision of the PROMIS physical function item bank and a content validity-driven 20-item short form in rheumatoid arthritis compared with traditional measures",
      "authors": [
        "Oude Voshaar",
        "Ten Klooster",
        "P Glas",
        "C Vonkeman",
        "H Taal",
        "E Krishnan"
      ],
      "year": 2015
    },
    {
      "title": "Systematic review on the measurement properties of diabetes-specific patient-reported outcome measures (PROMs) for measuring physical functioning in people with type 2 diabetes",
      "authors": [
        "Ebm Elsman",
        "L Mokkink",
        "M Langendoen-Gort",
        "F Rutters",
        "J Beulens",
        "Pjm Elders"
      ],
      "year": 2022,
      "doi": "10.1136/bmjdrc-2021-002729"
    },
    {
      "title": "GRADE handbook for grading quality of evidence and strength of recommendations 2013",
      "authors": [
        "H Sch\u00fcnemann",
        "Bro\u017cek",
        "Oxman Guyatt"
      ],
      "year": 2013
    },
    {
      "title": "Mental health provider attitudes toward adoption of evidence-based practice: the Evidence-Based Practice Attitude Scale (EBPAS)",
      "authors": [
        "G Aarons"
      ],
      "year": 2004,
      "doi": "10.1023/b:mhsr.0000024351.12294.65"
    },
    {
      "title": "Confirmatory factor analysis of the Evidence-Based Practice Attitude Scale in a geographically diverse sample of community mental health providers",
      "authors": [
        "G Aarons",
        "E Mcdonald",
        "A Sheehan",
        "C Walrath-Greene"
      ],
      "year": 2007
    },
    {
      "title": "Psychometric properties and U S National norms of the Evidence-Based Practice Attitude Scale (EBPAS)",
      "authors": [
        "G Aarons",
        "C Glisson",
        "K Hoagwood",
        "K Kelleher",
        "J Landsverk",
        "G Cafri"
      ],
      "year": 2010
    },
    {
      "title": "Development and validation of the Dutch EBPAS-ve and EBPQ-ve for nursing assistants and nurses with a vocational education",
      "authors": [
        "K Maessen",
        "A Van Vught",
        "D Gerritsen",
        "M Lovink",
        "H Vermeulen",
        "A Persoon"
      ],
      "year": 2019
    },
    {
      "title": "Evaluating the properties of the Evidence-Based Practice Attitude Scale (EBPAS) in health care",
      "authors": [
        "C Melas",
        "L Zampetakis",
        "A Dimopoulou",
        "V Moustakis"
      ],
      "year": 2012
    },
    {
      "title": "Reliability of the Swedish version of the Evidence-Based Practice Attitude Scale assessing physiotherapist's attitudes to implementation of evidence-based practice",
      "authors": [
        "K Skavberg Roaldsen",
        "A Halvarsson"
      ],
      "year": 2019,
      "doi": "10.1371/journal.pone.0225467"
    },
    {
      "title": "Psychometric properties of the Norwegian version of the Evidence-Based Practice Attitude Scale (EBPAS): to measure implementation readiness",
      "authors": [
        "K Egeland",
        "T Ruud",
        "T Ogden",
        "J Lindstrom",
        "K Heiervang"
      ],
      "year": 2016,
      "doi": "10.1186/s12961-016-0114-3"
    },
    {
      "title": "Confirmatory factor analysis of the Evidence-Based Practice Attitude Scale (EBPAS) in a large and representative Swedish sample: is the use of the total scale and subscale scores justified?",
      "authors": [
        "Ahe Santesson",
        "M B\u00e4ckstr\u00f6m",
        "R Holmberg",
        "S Perrin",
        "H Jarbin"
      ],
      "year": 2020,
      "doi": "10.1186/s12874-020-01126-4"
    },
    {
      "title": "Measuring practitioner attitudes toward evidence-based treatments: a validation study",
      "authors": [
        "R Ashcraft",
        "S Foster",
        "A Lowery",
        "S Henggeler",
        "J Chapman",
        "M Rowland"
      ],
      "year": 2011,
      "doi": "10.1080/1067828x.2011.555276"
    },
    {
      "title": "Expanding the domains of attitudes towards evidence-based practice: the evidence based practice attitude scale-50",
      "authors": [
        "G Aarons",
        "G Cafri",
        "L Lugo",
        "A Sawitzky"
      ],
      "year": 2012
    },
    {
      "title": "Evaluating the Properties of the Evidence-Based Practice Attitude Scale (EBPAS-50) in Nurses in Turkey",
      "authors": [
        "D Yildiz",
        "B Fidanci",
        "C Acikel",
        "N Kaygusuz",
        "C Yildirim"
      ],
      "year": 2018
    },
    {
      "title": "The Evidence-based Practice Attitude Scale-36 (EBPAS-36): a brief and pragmatic measure of attitudes to evidence-based practice validated in US and Norwegian samples",
      "authors": [
        "M Rye",
        "E Torres",
        "O Friborg",
        "I Skre",
        "G Aarons"
      ],
      "year": 2017,
      "doi": "10.1186/s13012-017-0573-0"
    },
    {
      "title": "Development of an evidence-based practice questionnaire for nurses",
      "authors": [
        "D Upton",
        "P Upton"
      ],
      "year": 2006,
      "doi": "10.1111/j.1365-2648.2006.03739.x"
    },
    {
      "title": "A psychometric evaluation of the Korean version of the evidence-based practice questionnaire for nurses",
      "authors": [
        "Y-J Son",
        "Y Song",
        "S-Y Park",
        "J-I Kim"
      ],
      "year": 2014,
      "doi": "10.1080/10376178.2014.11081948"
    },
    {
      "title": "The development and validation of the Evidence-Based Practice Questionnaire: Japanese version",
      "authors": [
        "A Tomotaki",
        "H Fukahori",
        "I Sakai",
        "K Kurokohchi"
      ],
      "year": 2018,
      "doi": "10.1037/t72462-000"
    },
    {
      "title": "Psychometric Properties of the Chinese Version of the Evidence-Based Practice Questionnaire for Nurses",
      "authors": [
        "R Yang",
        "J Guo",
        "S Beck",
        "F Jiang",
        "S Tang"
      ],
      "year": 2019
    },
    {
      "title": "Cultural validation of the Turkish version of evidence-based practice questionnaire",
      "authors": [
        "A Zaybak",
        "U Gunes",
        "Y Dikmen",
        "G Arslan"
      ],
      "year": 2017
    },
    {
      "title": "A multisample model validation of the evidence-based practice questionnaire",
      "authors": [
        "A Sese-Abad",
        "De Pedro-Gomez",
        "J Bennasar-Veny",
        "M Sastre",
        "P Fernandez-Dominguez",
        "J Morales-Asencio"
      ],
      "year": 2014
    },
    {
      "title": "Cultural adaptation to Brazil and psychometric performance of the \"Evidence-Based Practice Questionnaire",
      "authors": [
        "K Rospendowiski",
        "Nmc Alexandre",
        "M Cornello"
      ],
      "year": 2014
    },
    {
      "title": "Validation of the Portuguese version of the Evidence-Based Practice Questionnaire",
      "authors": [
        "R Pereira",
        "A Guerra",
        "M Cardoso",
        "A Santos",
        "M De Figueiredo",
        "A Carneiro"
      ],
      "year": 2015
    },
    {
      "title": "The evidence-based practice beliefs and implementation scales: psychometric properties of two new instruments",
      "authors": [
        "B Melnyk",
        "E Fineout-Overholt",
        "M Mays"
      ],
      "year": 2008
    },
    {
      "title": "Factor analytical examination of the Evidence-Based Practice Beliefs Scale: indications of a two-factor structure",
      "authors": [
        "Cku Gr\u00f8nvik",
        "A \u00d8deg\u00e5rd",
        "S Bj\u00f8rkly"
      ],
      "year": 2016
    },
    {
      "title": "Adaptation and validation of the evidence-based practice beliefs and implementation scales into German",
      "authors": [
        "E Kerwien-Jacquier",
        "H Verloo",
        "F Pereira",
        "K Peter"
      ],
      "year": 2020
    },
    {
      "title": "Translation and validation of two evidence-based nursing practice instruments",
      "authors": [
        "H Thorsteinsson"
      ],
      "year": 2012
    },
    {
      "title": "Adaptation and validation of the Evidence-Based Practice Belief and Implementation scales for French-speaking Swiss nurses and allied healthcare providers",
      "authors": [
        "H Verloo",
        "M Desmedt",
        "D Morin"
      ],
      "year": 2017,
      "doi": "10.1111/jocn.13786"
    },
    {
      "title": "Reported use of evidence in clinical practice: a survey of rehabilitation practices in Norway",
      "authors": [
        "J Moore",
        "S Friis",
        "I Graham",
        "E Gundersen",
        "J Nordvik"
      ],
      "year": 2018
    },
    {
      "title": "Applying modern measurement approaches to constructs relevant to evidence-based practice among Canadian physical and occupational therapists",
      "authors": [
        "Al Zoubi",
        "F Mayo",
        "Rochette Thomas"
      ],
      "year": 2018
    },
    {
      "title": "Development and psychometric properties of the evidence-based professional practice scale (EBPP-S)",
      "authors": [
        "G Bernal",
        "N Rodriguez-Soto"
      ],
      "year": 2010
    },
    {
      "title": "Evidence-based practice: beliefs, attitudes, knowledge, and behaviors of physical therapists",
      "authors": [
        "D Jette",
        "K Bacon",
        "C Batty",
        "M Carlson",
        "A Ferland",
        "R Hemingway"
      ],
      "year": 2003
    },
    {
      "title": "Measuring evidence-based practice in physical therapy: translation, adaptation, further development, validation, and reliability test of a questionnaire",
      "authors": [
        "S Bernhardsson",
        "M Larsson"
      ],
      "year": 2013,
      "doi": "10.2522/ptj.20120270"
    },
    {
      "title": "The \"evidence-based practice inventory\": reliability and validity was demonstrated for a novel instrument to identify barriers and facilitators for Evidence Based Practice in health care",
      "authors": [
        "N Kaper",
        "M Swennen",
        "A Van Wijk",
        "C Kalkman",
        "N Van Rheenen",
        "Y Van Der Graaf"
      ],
      "year": 2015
    },
    {
      "title": "Crosscultural adaptation, internal consistency, test-retest reliability and feasibility of the German version of the evidence-based practice inventory",
      "authors": [
        "T Braun",
        "K Ehrenbrusthoff",
        "C Bahns",
        "L Happe",
        "C Kopkow"
      ],
      "year": 2019
    },
    {
      "title": "Creation and validation of the evidence-based practice confidence scale for health care professionals",
      "authors": [
        "N Salbach",
        "S Jaglal"
      ],
      "year": 2011
    },
    {
      "title": "Reliability and validity of the evidence-based practice confidence (EPIC) scale",
      "authors": [
        "N Salbach",
        "S Jaglal",
        "J Williams"
      ],
      "year": 2013
    },
    {
      "title": "Validation of the Evidence-Based Practice Confidence (EPIC) Scale With Occupational Therapists",
      "authors": [
        "J Clyde",
        "D Brooks",
        "J Cameron",
        "N Salbach"
      ],
      "year": 2016
    },
    {
      "title": "Provider attitudes toward evidence-based practices: are the concerns with the evidence or with the manuals?",
      "authors": [
        "C Borntrager",
        "B Chorpita",
        "C Higa-Mcmillan",
        "J Weisz"
      ],
      "year": 2009
    },
    {
      "title": "Cross-cultural validation of the modified practice attitudes scale: initial factor analysis and a new factor model",
      "authors": [
        "H Park",
        "C Ebesutani",
        "K Chung",
        "C Stanick"
      ],
      "year": 2018
    },
    {
      "title": "Development and validation of the Evidence-based Practice Process Assessment Scale: Preliminary findings",
      "authors": [
        "A Rubin",
        "D Parrish"
      ],
      "year": 2010
    },
    {
      "title": "Validation of the evidence-based practice Process Assessment Scale",
      "authors": [
        "A Rubin",
        "D Parrish"
      ],
      "year": 2011
    },
    {
      "title": "Validation of the Evidence-Based Practice Process Assessment Scale-Short Version",
      "authors": [
        "D Parrish"
      ],
      "year": 2011,
      "doi": "10.1177/1049731510389193"
    },
    {
      "title": "Validation of scales measuring self-efficacy and outcome expectancy in evidence-based practice",
      "authors": [
        "A Chang",
        "L Crowe"
      ],
      "year": 2011
    },
    {
      "title": "Psychometric properties of Korean Version of self-efficacy of Evidence-Based practice scale",
      "authors": [
        "E Oh",
        "Y Yang",
        "J Sung",
        "C Park",
        "A Chang"
      ],
      "year": 2016
    },
    {
      "title": "Evidence-Based practice selfefficacy scale preliminary reliability and validity",
      "authors": [
        "S Tucker",
        "M Olson",
        "D Frusti"
      ],
      "year": 2009
    },
    {
      "title": "Capability beliefs regarding evidence-based practice are associated with application of EBP and research use: validation of a new measure",
      "authors": [
        "L Wallin",
        "A Bostrom",
        "J Gustavsson"
      ],
      "year": 2012
    },
    {
      "title": "Psychometric testing of the health care evidence-based practice assessment tool",
      "authors": [
        "M Sleutel",
        "C Barbosa-Leiker",
        "M Wilson"
      ],
      "year": 2015
    },
    {
      "title": "A modified evidence-based practice-knowledge, attitudes, behaviour and decisions/outcomes questionnaire is valid across multiple professions involved in pain management",
      "authors": [
        "Q Shi",
        "B Chesworth",
        "M Law",
        "R Haynes",
        "J Macdermid"
      ],
      "year": 2014
    },
    {
      "title": "The development and content validity testing of the Quick-EBP-VIK: A survey instrument measuring nurses' values, knowledge and implementation of evidence-based practice",
      "authors": [
        "F Paul",
        "L Connor",
        "M Mccabe",
        "S Ziniel"
      ],
      "year": 2016
    },
    {
      "title": "Measuring nurses' value, implementation, and knowledge of evidence-based practice: further psychometric testing of the Quick-EBP-VIK Survey",
      "authors": [
        "L Connor",
        "F Paul",
        "M Mccabe",
        "S Ziniel"
      ],
      "year": 2017
    },
    {
      "title": "Translation, cultural adaptation, validation, and reliability study of the Quick-EBP-VIK instrument: Chinese version",
      "authors": [
        "C Zhou",
        "Y Wang",
        "S Wang",
        "J Ou",
        "Y Wu"
      ],
      "year": 2019
    },
    {
      "title": "Content validity of a health science evidence-based practice questionnaire (HS-EBP) with a web-based modified Delphi approach",
      "authors": [
        "J Fernandez-Dominguez",
        "A Sese-Abad",
        "J Morales-Asencio",
        "P Sastre-Fullana",
        "S Pol-Castaneda",
        "Pedro De",
        "J Gomez"
      ],
      "year": 2016
    },
    {
      "title": "Health Sciences-Evidence Based Practice questionnaire (HS-EBP) for measuring transprofessional evidence-based practice: Creation, development and psychometric validation",
      "authors": [
        "J Fernandez-Dominguez",
        "Pedro De",
        "J Gomez",
        "J Morales-Asencio",
        "M Bennasar-Veny",
        "P Sastre-Fullana",
        "A Sese-Abad"
      ],
      "year": 2017
    },
    {
      "title": "Determining registered nurses' readiness for evidencebased practice",
      "authors": [
        "L Thiel",
        "Y Ghosh"
      ],
      "year": 2008,
      "doi": "10.1111/j.1741-6787.2008.00137.x"
    },
    {
      "title": "The evidence-based practice readiness survey: a structural equation modeling approach for a Greek sample",
      "authors": [
        "A Patelarou",
        "V Dafermos",
        "H Brokalaki",
        "C Melas",
        "E Koukia"
      ],
      "year": 2015,
      "doi": "10.1097/xeb.0000000000000043"
    },
    {
      "title": "Development and psychometric testing of a trans-professional evidence-based practice profile questionnaire",
      "authors": [
        "M Mcevoy",
        "M Williams",
        "T Olds"
      ],
      "year": 2010,
      "doi": "10.3109/0142159x.2010.494741"
    },
    {
      "title": "Development and validation of the Chinese version of the evidence-based practice profile questionnaire (EBP<sup>2</sup>Q)",
      "authors": [
        "M Hu",
        "Y Wu",
        "M Mcevoy",
        "Y Wang",
        "W Cong",
        "L Liu"
      ],
      "year": 2020
    },
    {
      "title": "Validation study of the Polish version of the Evidence-Based Practice Profile Questionnaire",
      "authors": [
        "M Panczyk",
        "J Belowska",
        "A Zarzeka",
        "L Samolinski",
        "H Zmuda-Trzebiatowska",
        "J Gotlib"
      ],
      "year": 2017
    },
    {
      "title": "Translation, cross-cultural adaption and measurement properties of the evidence-based practice profile",
      "authors": [
        "K Titlestad",
        "A Snibsoer",
        "H Stromme",
        "M Nortvedt",
        "B Graverholt",
        "B Espehaug"
      ],
      "year": 2017,
      "doi": "10.1186/s13104-017-2373-7"
    },
    {
      "title": "Evidencebased practice implementation within a theory of planned behavior framework",
      "authors": [
        "A Burgess",
        "J Chang",
        "B Nakamura",
        "S Izmirian",
        "K Okamura"
      ],
      "year": 2017
    },
    {
      "title": "A Psychometric evaluation of the intention scale for providers-direct items",
      "authors": [
        "A Mah",
        "K Hill",
        "D Cicero",
        "B Nakamura"
      ],
      "year": 2020,
      "doi": "10.1007/s11414-019-09675-3"
    },
    {
      "title": "Attitude towards Evidence-Based Nursing Questionnaire: development and psychometric testing in Spanish community nurses",
      "authors": [
        "M Ruzafa-Martinez",
        "L Lopez-Iborra",
        "M Madrigal-Torres"
      ],
      "year": 2011,
      "doi": "10.1111/j.1365-2753.2011.01677.x"
    },
    {
      "title": "Evidence-based practice in physical therapy in Austria: current state and factors associated with EBP engagement",
      "authors": [
        "G Diermayr",
        "H Schachner",
        "M Eidenberger",
        "M Lohkamp",
        "N Salbach"
      ],
      "year": 2015,
      "doi": "10.1111/jep.12415"
    },
    {
      "title": "Translation and validation of the evidence-based practice attitude scale (EBPAS-15) to Brazilian Portuguese: Examining providers' perspective about evidence-based parent intervention",
      "authors": [
        "A Baumann",
        "A Vazquez",
        "A Macchione",
        "A Lima",
        "A Coelho",
        "M Juras"
      ],
      "year": 2022,
      "doi": "10.1016/j.childyouth.2022.106421"
    },
    {
      "title": "Adaptation of the evidence-based practices attitude scale-15 in Turkish family medicine residents",
      "authors": [
        "Ayhan Baser",
        "D Agadayi",
        "Gonderen Cakmak",
        "S Kahveci"
      ],
      "year": 2021
    },
    {
      "title": "Cross-cultural adaption and psychometric investigation of the German version of the Evidence Based Practice Attitude Scale (EBPAS-36D)",
      "authors": [
        "N Van Giang",
        "Lin Sy",
        "D Thai",
        "K Thielemann",
        "Jfb Christiansen",
        "H Rye",
        "M Aarons",
        "G Barke"
      ],
      "year": 2021
    },
    {
      "title": "The indonesia version of evidencebased practice questionnaire (EBPQ): Translation and Reliability",
      "authors": [
        "M Fajarini",
        "S Rahayu",
        "A Setiawan"
      ],
      "year": 2021
    },
    {
      "title": "Psychometric Properties of the Short Versions of the EBP Beliefs Scale, the EBP Implementation Scale, and the EBP Organizational Culture and Readiness Scale",
      "authors": [
        "B Melnyk",
        "A Hsieh",
        "L Gallagher-Ford",
        "B Thomas",
        "J Guo",
        "A Tan"
      ],
      "year": 2021,
      "doi": "10.1111/wvn.12525"
    },
    {
      "title": "Evidence-based practice questionnaire for physical therapists: Portuguese translation, adaptation, validity, and reliability",
      "authors": [
        "R Ferreira",
        "P Ferreira",
        "L Cavalheiro",
        "J Duarte",
        "R Gon\u00e7alves"
      ],
      "year": 2019
    },
    {
      "title": "Promoting evidence-based practice -perceived knowledge, behaviours and attitudes of Polish nurses: a cross-sectional validation study",
      "authors": [
        "J Belowska",
        "M Panczyk",
        "A Zarzeka",
        "L Iwanow",
        "I Cieslak",
        "J Gotlib"
      ],
      "year": 2020
    },
    {
      "title": "Questionnaire to Evaluate the Competency in Evidence-Based Practice of Registered Nurses (EBP-COQ Prof\u00a9): Development and Psychometric Validation",
      "authors": [
        "M Ruzafa-Martinez",
        "Fern Salazar",
        "S Leal-Costa",
        "Ramos-Morcillo Aj"
      ],
      "year": 2020,
      "doi": "10.1111/wvn.12464"
    },
    {
      "title": "Translation and Validation of the Greek Version of the Evidence-Based Practice Competency Questionnaire for Registered Nurses (EBP-COQ Prof\u00a9)",
      "authors": [
        "S Schetaki",
        "E Patelarou",
        "K Giakoumidakis",
        "A Trivli",
        "C Kleisiaris",
        "A Patelarou"
      ],
      "year": 2022
    },
    {
      "title": "Development and content validation of a measure to assess evidence-informed decision-making competence in public health nursing",
      "authors": [
        "E Belita",
        "J Yost",
        "J Squires",
        "R Ganann",
        "M Dobbins"
      ],
      "year": 2021,
      "doi": "10.1371/journal.pone.0248330"
    },
    {
      "title": "Design and validity of an instrument to assess healthcare professionals' perceptions, behaviour, self-efficacy and attitudes towards evidence-based health practice: I-SABE",
      "authors": [
        "Asm Ruano",
        "F Motter",
        "L Lopes"
      ],
      "year": 2022,
      "doi": "10.1136/bmjopen-2021-052767"
    },
    {
      "title": "Validity and reliability of the Noor Evidence-Based Medicine Questionnaire: A cross-sectional study",
      "authors": [
        "M Norhayati",
        "Z Nawi"
      ],
      "year": 2021
    },
    {
      "title": "Development and Validation of Questionnaire Measuring Registered Nurses' Competencies, Beliefs, Facilitators, Barriers, and Implementation of Evidence-Based Practice (EBP-CBFRI)",
      "authors": [
        "M Abuadas",
        "Z Albikawi",
        "F Abuadas"
      ],
      "year": 2021
    },
    {
      "title": "Evidence-based practice and associated factors among health care providers working in public hospitals in Northwest Ethiopia During 2017",
      "authors": [
        "G Dessie",
        "D Jara",
        "G Alem",
        "H Mulugeta",
        "T Zewdu",
        "F Wagnew"
      ],
      "year": 2020
    },
    {
      "title": "Measurement in medicine: a practical guide",
      "authors": [
        "Hcw De Vet",
        "C Terwee",
        "L Mokkink",
        "D Knol"
      ],
      "year": 2011,
      "doi": "10.1017/CBO9780511996214"
    },
    {
      "title": "Thanks coefficient alpha, we'll take it from here",
      "authors": [
        "D Mcneish"
      ],
      "year": 2018
    },
    {
      "title": "Health measurement scales : a practical guide to their development and use",
      "authors": [
        "D Streiner",
        "G Norman",
        "J Cairney"
      ],
      "year": 2015
    },
    {
      "title": "Evidence based practice profiles: differences among allied health professions",
      "authors": [
        "M Mcevoy",
        "M Williams",
        "T Olds"
      ],
      "year": 2010,
      "doi": "10.1186/1472-6920-10-69"
    },
    {
      "title": "Knowledge and use of evidence-based practice by allied health and health science professionals in the United Kingdom",
      "authors": [
        "D Upton",
        "P Upton"
      ],
      "year": 2006
    },
    {
      "title": "Cosmin Study design checklist for patient-reported outecome measurement instruments",
      "authors": [
        "L Mokkink",
        "C Prinsen",
        "D Patrick",
        "J Alonso",
        "L Bouter",
        "Hcd Vet"
      ]
    },
    {
      "title": "Guidelines for the process of cross-cultural adaptation of self-report measures",
      "authors": [
        "D Beaton",
        "C Bombardier",
        "F Guillemin",
        "M Ferraz"
      ],
      "year": 1976
    },
    {
      "title": "Challenges and future directions in the measurement of evidence-based practice: Qualitative analysis of umbrella review findings",
      "authors": [
        "J Roberge-Dao",
        "L Maggio",
        "M Zaccagnini",
        "Rochette Shikako",
        "K Boruff"
      ],
      "year": 2023
    },
    {
      "title": "Dissemination and implementation research in health : translating science to practice",
      "authors": [
        "R Brownson",
        "G Colditz",
        "E Proctor"
      ],
      "year": 2017
    }
  ],
  "num_references": 119
}
