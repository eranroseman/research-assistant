{
  "paper_id": "YNNWS6CP",
  "title": "Using Clinical Trial Data to Estimate the Costs of Behavioral Interventions for Potential Adopters: A Guide for Trialists",
  "abstract": "Behavioral interventions involving electronic devices, financial incentives, gamification, and specially trained staff to encourage healthy behaviors are becoming increasingly prevalent and important in health innovation and improvement efforts. Although considerations of cost are key to their wider adoption, cost information is lacking because the resources required cannot be costed using standard administrative billing data. Pragmatic clinical trials that test behavioral interventions are potentially the best and often only source of cost information but rarely incorporate costing studies. This article provides a guide for researchers to help them collect and analyze, during the trial and with little additional effort, the information needed to inform potential adopters of the costs of adopting a behavioral intervention. A key challenge in using trial data is the separation of implementation costs, the costs an adopter would incur, from research costs. Based on experience with 3 randomized clinical trials of behavioral interventions, this article explains how to frame the costing problem, including how to think about costs associated with the control group, and describes methods for collecting data on individual costs: specifications for costing a technology platform that supports the specialized functions required, how to set up a time log to collect data on the time staff spend on implementation, and issues in getting data on device, overhead, and financial incentive costs.",
  "year": 2019,
  "date": "2019",
  "journal": "Cochrane Database Syst Rev",
  "publication": "Cochrane Database Syst Rev",
  "authors": [
    {
      "forename": "Louise",
      "surname": "Russell",
      "name": "Louise Russell",
      "email": "louisebrussell1983@gmail.com"
    },
    {
      "forename": "Laurie",
      "surname": "Norton",
      "name": "Laurie Norton"
    },
    {
      "forename": "David",
      "surname": "Pagnotti",
      "name": "David Pagnotti"
    },
    {
      "forename": "Christianne",
      "surname": "Sevinc",
      "name": "Christianne Sevinc"
    },
    {
      "forename": "Sophia",
      "surname": "Anderson",
      "name": "Sophia Anderson"
    },
    {
      "forename": "Darra",
      "surname": "Finnerty Bigelow",
      "name": "Darra Finnerty Bigelow"
    },
    {
      "forename": "Lauren",
      "surname": "Iannotte",
      "name": "Lauren Iannotte"
    },
    {
      "forename": "Michael",
      "surname": "Josephs",
      "name": "Michael Josephs"
    },
    {
      "forename": "Ryan",
      "surname": "Mcgilloway",
      "name": "Ryan Mcgilloway"
    },
    {
      "forename": "Iwan",
      "surname": "Barankay",
      "name": "Iwan Barankay"
    },
    {
      "forename": "Mary",
      "surname": "Putt",
      "name": "Mary Putt"
    },
    {
      "forename": "Peter",
      "surname": "Reese",
      "name": "Peter Reese"
    },
    {
      "forename": "David",
      "surname": "Asch",
      "name": "David Asch"
    },
    {
      "forename": "Lee",
      "surname": "Goldberg",
      "name": "Lee Goldberg"
    },
    {
      "forename": "Shivan",
      "surname": "Mehta",
      "name": "Shivan Mehta"
    },
    {
      "forename": "Monique",
      "surname": "Tanna",
      "name": "Monique Tanna"
    },
    {
      "forename": "Andrea",
      "surname": "Troxel",
      "name": "Andrea Troxel"
    },
    {
      "forename": "Kevin",
      "surname": "Volpp",
      "name": "Kevin Volpp"
    },
    {
      "affiliation": "Department of Medical Ethics and Health Policy , Perelman School of Medicine , and Center for Health Incentives and Behavioral Economics , University of Pennsylvania , 423 Guardian Drive , Philadelphia , PA 19104 , USA \n\t\t\t\t\t\t\t\t Department of Medical Ethics and Health Policy \n\t\t\t\t\t\t\t\t Perelman School of Medicine \n\t\t\t\t\t\t\t\t Center for Health Incentives and Behavioral Economics \n\t\t\t\t\t\t\t\t University of Pennsylvania \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t 423 Guardian Drive \n\t\t\t\t\t\t\t\t\t 19104 \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "Department of Medical Ethics and Health Policy , Perelman School of Medicine , and Center for Health Incentives and Behavioral Economics , University of Pennsylvania , Philadelphia , PA , USA \n\t\t\t\t\t\t\t\t Department of Medical Ethics and Health Policy \n\t\t\t\t\t\t\t\t Perelman School of Medicine \n\t\t\t\t\t\t\t\t Center for Health Incentives and Behavioral Economics \n\t\t\t\t\t\t\t\t University of Pennsylvania \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "LRG , SJM , KGV) ; The Leonard Davis Institute of Health Economics (LDI) , \n\t\t\t\t\t\t\t\t The Leonard Davis Institute of Health Economics (LDI) \n\t\t\t\t\t\t\t\t LRG \n\t\t\t\t\t\t\t\t SJM \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t KGV"
    },
    {
      "affiliation": "University of Pennsylvania , Philadelphia , PA , USA DAA , \n\t\t\t\t\t\t\t\t DAA \n\t\t\t\t\t\t\t\t University of Pennsylvania \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "SJM , KGV) ; Penn Medicine Center for Health Care Innovation , \n\t\t\t\t\t\t\t\t Penn Medicine Center for Health Care Innovation \n\t\t\t\t\t\t\t\t SJM \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t KGV"
    },
    {
      "affiliation": "University of Pennsylvania , Philadelphia , PA , USA Department of Management and Department of Business Economics and Public Policy , \n\t\t\t\t\t\t\t\t Department of Management and Department of Business Economics and Public Policy \n\t\t\t\t\t\t\t\t University of Pennsylvania \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "The Wharton School , University of Pennsylvania , Philadelphia , PA , USA ( \n\t\t\t\t\t\t\t\t The Wharton School \n\t\t\t\t\t\t\t\t University of Pennsylvania \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "IB) ; Department of Biostatistics, Epidemiology & Informatics , Perelman School of Medicine , University of Pennsylvania , Philadelphia , PA , USA \n\t\t\t\t\t\t\t\t IB) \n\t\t\t\t\t\t\t\t Department of Biostatistics, Epidemiology & Informatics \n\t\t\t\t\t\t\t\t Perelman School of Medicine \n\t\t\t\t\t\t\t\t University of Pennsylvania \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "PPR) ; Renal Division , Department of Medicine , Perelman School of Medicine , University of Pennsylvania , Philadelphia , PA , USA ( \n\t\t\t\t\t\t\t\t PPR) \n\t\t\t\t\t\t\t\t Department of Medicine \n\t\t\t\t\t\t\t\t Perelman School of Medicine \n\t\t\t\t\t\t\t\t Renal Division \n\t\t\t\t\t\t\t\t University of Pennsylvania \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "PPR) ; Department of Medicine , University of Pennsylvania Perelman School of Medicine , Philadelphia , PA , USA ( \n\t\t\t\t\t\t\t\t PPR) \n\t\t\t\t\t\t\t\t Department of Medicine \n\t\t\t\t\t\t\t\t University of Pennsylvania Perelman School of Medicine \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "DAA , LRG , SJM , MST , KGV) ; \n\t\t\t\t\t\t\t\t DAA \n\t\t\t\t\t\t\t\t LRG \n\t\t\t\t\t\t\t\t SJM \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t MST KGV"
    },
    {
      "affiliation": "The Corporal Michael J Crescenz VA Medical Center , Philadelphia , PA , USA ( \n\t\t\t\t\t\t\t\t The Corporal Michael J Crescenz VA Medical Center \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Philadelphia \n\t\t\t\t\t\t\t\t\t PA \n\t\t\t\t\t\t\t\t\t USA"
    },
    {
      "affiliation": "DAA , KGV) ; Division of Biostatistics , \n\t\t\t\t\t\t\t\t Division of Biostatistics \n\t\t\t\t\t\t\t\t DAA \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t KGV"
    },
    {
      "affiliation": "Department of Population Health , New York University School of Medicine , New York , NY , USA \n\t\t\t\t\t\t\t\t Department of Population Health \n\t\t\t\t\t\t\t\t New York University School of Medicine \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t New York \n\t\t\t\t\t\t\t\t\t NY \n\t\t\t\t\t\t\t\t\t USA"
    }
  ],
  "doi": "https://doi.org/10.13039/100000049",
  "keywords": [
    "behavioral intervention",
    "clinical trial",
    "costing",
    "medication adherence",
    "outcomes research",
    "pragmatic trial",
    "preventive services",
    "translating research into practice"
  ],
  "sections": [
    {
      "text": "Date received: May 27, 2020; accepted: October 19, 2020\n\nBehavioral interventions involving the use of electronic devices, financial incentives, gamification, and specially trained staff to encourage healthy behaviors are becoming increasingly prevalent and important in health innovation and improvement efforts. Systematic reviews have found them to be effective for some behaviors in some populations.  [1] [2] 2] [3]  Potential adopters of these interventionsclinical practices, health plans, employers, and hospitalsneed information not only about their effectiveness but also about the costs of implementing them.\n\nBehavioral interventions differ from many interventions evaluated in costing, cost-effectiveness, and budget impact analyses. Interventions such as screening for cancer or surgery for heart disease make use of established clinical services and tests and can be costed using sources such as the Healthcare Cost and Utilization Project (HCUP)  4  and Medicare payment rates.  5 In contrast, behavioral interventions often involve specialized staff and technology that are not part of standard clinical workflows or billing processes. As a result, the costs of the resources required, which can include electronic devices, integrated technology platforms, secure messaging, and staff to advise patients on their use, are not readily available from HCUP, Medicare, or health insurers' administrative data systems.\n\nPragmatic clinical trials that test behavioral interventions are a potential source for this cost information. A key challenge in using trial data, however, is the separation of implementation costs from research costs; research costs are often substantial, sometimes dwarfing the costs of the intervention itself, and must be separated out to give potential adopters an accurate idea of the costs they are likely to incur when using the intervention outside of a research setting. In addition, because some of the resources required, such as secure messaging systems, are nonstandard, methods for costing them may be underdeveloped.  6, 7 his article describes how to estimate a potential adopter's costs from the trial costs of a behavioral intervention. Although the fundamental costing principles are the same as for other interventions, behavioral interventions involve nonstandard costs, the need for more primary data collection than is typically the case, and careful work to separate research costs from costs that would be incurred by an organization that adopts the intervention. We use 3 randomized controlled trials as case studies to illustrate how behavioral interventions can be costed, 1 straightforward 2-arm trial and 2 trials, each with several interventions, which were run by the same staff and so presented more complex costing issues."
    },
    {
      "title": "The Trials"
    },
    {
      "title": "Empower",
      "text": "The EMPOWER trial, which ended in April 2020, enrolled 566 patients within 30 d of hospital discharge with a heart failure diagnosis in a pragmatic, randomized clinical trial to compare usual care with an ''automated hovering'' intervention that included patient incentives for daily weight monitoring and diuretic adherence. Electronic scales and pill bottles issued to each participant sent automated feedback into the clinical care pathway, enabling real-time response to concerning clinical symptoms.  8 The intervention lasted 12 mo. The primary trial outcome was time to hospital readmission with the primary aim to reduce readmissions. EMPOWER was a 2-arm trial with staff dedicated to that trial only."
    },
    {
      "title": "Habit Formation and Process versus Outcomes",
      "text": "Habit Formation (HF) and Process versus Outcomes (PvO), completed in 2018 and 2019, respectively, were separate randomized controlled trials, each aimed at using financial incentives to improve adherence to statin medication. The goal was to reduce low-density lipoprotein cholesterol (LDL-c), the primary outcome, in patients with elevated LDL-c. Each trial tested different financial incentive interventions against control. Because of their similar patient populations, goals, and resource requirements, the same staff worked on both trials.\n\nHF. HF enrolled 805 patients in a 6-mo trial that offered financial incentives for statin adherence, monitored daily by means of an electronic pill bottle provided to each participant.  9 Each day the pill bottle was opened, a signal was transmitted to the study team, and the participant was deemed adherent. Participants in all arms, including control, received daily reminders to take their statin. The intervention arms tested 3 alternative incentive designs to improve adherence. In all 3 designs, participants were eligible to win a daily sweepstakes, based on a personal 2digit number selected at enrollment, if the subject was adherent the previous day. Arm 1 was a simple sweepstakes. Arm 2 (''Deadline Sweepstakes'') paid participants their full winnings only if they took their statin before receiving the daily reminder, half if they took it after the reminder. Arm 3 (''Sweepstakes plus Deposit'') also halved the daily winnings, compared with the simple sweepstakes, and put the other half in a monthly deposit, which was reduced each day the participant failed to take statins; the balance was paid at the end of the month. Expected winnings for a fully adherent participant averaged $2.80 per day in all 3 interventions. LDL-c levels were measured at months 6 and 12.\n\nPvO. PvO enrolled 764 patients in a 12-mo trial of financial incentives. The Process arm rewarded statin adherence by making participants eligible for sweepstakesbased incentives each day they were adherent (as indicated by opening an electronic pill bottle). The Outcome arm paid participants if lab tests every 3 mo showed that they had lowered their LDL-c by .10 mg/ dL since the last test or remained stable below 100 mg/ dL. The Process+Outcome arm combined daily sweepstakes incentives for adherence with 3-mo incentives based on lab tests that confirmed declines in LDL-c or stability below 100 mg/dL. As in the HF trial, adherence in all arms, including control, was monitored through electronic pill bottles and the expected average payment for a fully adherent participant was the same in each arm."
    },
    {
      "title": "The Costing Problem",
      "text": "What costs would potential adopters incur for one of these interventions, and what do they need to know about those costs? To estimate the costs of an intervention that is not just a rearrangement of well-established tests and procedures, it is necessary to identify in detail the tasks involved and to cost the resources required to carry out those tasks. This process is known as microcosting because it ''starts with the detailed inventory and measurement of resources.''  10 By contrast, gross costing focuses on ''units of input and output that are large relative to the intervention being analyzed,'' such as complete hospital stays.\n\nBecause the costs of a trial include both intervention and research, the first step is to identify all tasks and resources involved, then the subset required by the intervention. Trial staff and investigators, the people most knowledgeable about the trial's day-to-day activities, should create a detailed list of tasks and resources, including any that do not require payment; for example, if volunteers help with the intervention, their time must be included to give an accurate picture of resource requirements. Breaking tasks down in detail helps ensure that all tasks are identified and that implementation tasks can be distinguished from research tasks. It also helps potential adopters understand why the resources are needed.\n\nTable  1 , based on the HF/PvO trials, lists all the tasks involved in those 2 trials in the first column, the subset of tasks required to implement the interventions in the second column, and the types of resources needed for each task in the third column. Thus, it shows which tasks need to be included and which excluded, to measure only those costs involved in the intervention.\n\nResearch and other tasks unrelated to the intervention (e.g., departmental staff meetings) will clearly be excluded, but deciding how to handle some research components can be complicated. An important example is the resources required by the control group, the group of people who are enrolled in and tracked by the trial, but who do not receive the intervention. If the control group is not contacted and does not receive anything from the trial after enrollment, there are no intervention costs for them (other than enrollment). More typically, the control group is contacted and surveyed during the trial to facilitate comparison with the intervention groups. In the HF/ PvO trials, for example, the primary outcome, change in LDL-c, was measured by lab test, and the secondary outcome, adherence to statins, was measured by the frequency with which participants opened an electronic pill bottle. To determine the effects of the incentive payments, control as well as intervention participants had their LDL-c measured and received electronic pill bottles, as well as staff help in the use of the pill bottle and computer platform. Thus the trial incurred some of the same costs for control as for intervention participants, costs that would not be involved in implementing an intervention in a clinical practice or workplace setting. Removing those costs would require separating out and costing contacts with control subjects, a time-consuming and costly process. Instead, calculating costs per trial participant, which adjusts for the total number of people served, provides an approximate and inexpensive correction. The correction will be most accurate if control and intervention participants use resources similarly (e.g., with similar levels of staff assistance).\n\nOnce the subset of tasks involved in implementing the intervention(s) is identified, the quantities of resources required will be measured in natural units, such as types of staff and hours of time spent by each type. The following sections discuss how to do this for the resources required by the 3 example trials. The quantities of resources must then be valued in dollar terms; for example, staff time will be valued by wages plus fringe benefits. At this final stage, all component costs need to be adjusted to the price level of a specific year so that they can be summed. The Consumer Price Index can be used to adjust costs to the selected year.\n\nTo recap, the goal of the process is to estimate the total costs of the intervention, exclusive of research and other costs, and to present those costs in ways that are most useful to potential adopters. Expressing all costs in dollars of the same year allows potential adopters to adjust them to the price level of the year they expect to run the intervention. Expressing costs per participant lets them estimate costs for sites with more or fewer enrollees. Implementing an intervention also involves the initial costs needed for any new effort, such as meetings to discuss and plan the implementation. These efforts involve additional costs, but the research costs of a clinical trial are not a useful guide for estimating them."
    },
    {
      "title": "Micro-costing Each Resource",
      "text": "To help guide the costing process, Figure  1  provides an overview of the steps and methods involved in costing a The purpose of this table is to identify the tasks involved in a trial (column 1), the subset of those tasks that a potential adopter would need to do (column 2), and the resources needed for the task. The table is an example based on the Habit Formation (HF) and Process versus Outcomes (PvO) trials. Tasks are listed in chronological order, and research tasks are shown in italics.\n\nbehavioral intervention using trial data. The following sections describe the methods in detail."
    },
    {
      "title": "Technology Platform",
      "text": "Behavioral intervention trials conducted at the University of Pennsylvania use Way to Health, one example of a specially designed technology platform that automates tasks required for interventions that involve remote monitoring. Way to Health supports implementation through automated communication with trial participants; data collection from Bluetooth-enabled devices such as pill bottles, scales, and blood pressure cuffs; application of complex rules that evaluate data from those devices to tailor communications and monitor and adjust patient care; and automated payments of financial incentives. To serve the research needs of trials, Way to Health includes additional capabilities, such as the ability to administer and summarize surveys at enrollment and during the trial, to randomize participants to trial arms, and to store and organize data on each trial participant securely for subsequent analysis.\n\nAdopting organizations could develop their own technology platforms, adapt one they might already have developed, or purchase a subscription to a commercially available platform. Subscription prices may be per user, per feature or module, a flat rate per year, or some combination. Depending on how platform services are acquired, the cost can be variable (based on, say, the number of participants each month) or largely fixed (because of development costs or because the charge is a flat rate per year).\n\nTo estimate the cost of a platform with only the functions needed by potential adopters, we used information from the HF and PvO trials to specify how long the platform would be required, the likely number of enrollees, the design of the intervention, and the tasks the platform would be expected to automate; some, such as text messages, involve per-participant fees. In those 2 trials, an average of 49 people were enrolled per month and required all the functions of the platform for the duration of the intervention. We used the average enrollment for both trials, rather than only one, because both trials drew from the same patient population, and, during the period when both were recruiting, eligible patients were divided between them. Another 73 people per month started to enroll but were determined through the enrollment surveys to be ineligible, thus incurring only the costs of enrollment; the cost of enrolling ineligible patients are relevant if adopters would follow a similar path to determine who is eligible for the intervention. Based on their clinical experience, the clinicians on the trial team estimated that a new program would be offered for at least 3 y before it was reviewed and perhaps discontinued. Thus, to allow the last enrollees to complete the intervention, the platform would be needed for 3.5 y for a 6-mo intervention or 4 y for a 12-mo intervention. Over that period, the total number of participants would be 1764 (49 enrollees per month times 36 mo).\n\nBecause the details of the financial incentives had little effect on platform costs, we distinguished 3 types of interventions based on duration and types of messaging required.\n\n1. All 3 HF interventions offered daily sweepstakes incentives for 6 mo based on medication adherence and required 1 reminder and 1 feedback message each day (telling the participant whether and what she or he won) for a total of 362 messages over the course of the intervention. 2. The Process and Process+Outcome arms of PvO offered daily sweepstakes incentives for 12 mo and required 1 feedback message each day based on medication adherence, plus messages to remind participants to get lab tests in the Process+Outcome arm, for a total of 360 to 385 messages over the course of the intervention. 3. The Outcome arm of PvO offered payments based on LDL lab tests given every 3 mo over the course of a 12-mo intervention and required a minimum of 8 messages for participants who completed their labs after the first request (one to alert them that it is time for the lab and the second to let them know the results and any payments) and a maximum of 24 messages for participants who needed 4 reminders for each lab test.\n\nOther messages served the research aims of the trials and were not included in the specifications used to estimate the costs of a potential adopter.\n\nBased on these specifications, total platform costs for the period during which each intervention was active included implementation support, hosting and security in compliance with HIPAA requirements, ongoing support maintenance, integration with a payment service that issues debit cards, and per-participant messaging fees. Estimated total costs came to $156,920 for the 6-mo daily sweepstakes intervention, $173,744 for the 12-mo daily sweepstakes intervention, and $155,648 for the 12mo lab-based incentives intervention (in 2017 dollars). For the specified 1764 participants, the total cost per participant was thus $89, $98, and $88, respectively. The cost estimates assume that the adopter would not conduct research during or after the intervention and so would not need the additional secure data storage and staff required for such research and that the intervention is offered by a health plan or employer with a patient population similar in size and composition to that of the clinical practices that participated in the trials. Because the trials are the sources of the cost data, it would be difficult to avoid this assumption."
    },
    {
      "title": "Staff",
      "text": "Staff time is a key resource for implementation and can be a substantial expense. Potential adopters need to know what kind of personnel, with what kind of training, are needed to implement the intervention and how much time, on average, will be required per participant. The list of tasks developed to guide the costing (Table  1 ) helps them understand these requirements.\n\nThe starting point for estimating staff cost is a time log or diary, in which staff record the time spent on each task. Studies of the collection of time data indicate that the best way to obtain accurate estimates is to use a log that covers an entire day, which means that it would cover both research and implementation tasks, and that is filled out during the day, not at some later point.  11 his design ensures that the time reported for individual tasks is controlled by the total number of hours in the workday. When tasks are identified in sufficient detail, it is straightforward to separate time spent on implementation from time spent on research. In trials with more than 1 intervention, such as the HF/PvO trials, defining detailed tasks also allows tasks associated with different interventions to be distinguished so that each intervention can be costed accurately. It is essential to involve staff in identifying which tasks are part of implementation, for which intervention, and which are part of research or other work.\n\nTime can be recorded daily for individual days scattered throughout the trial or for a week at a time, with several weeks of data collected during the trial. Collecting for an entire week is especially useful if tasks vary by day of the week. Data collection should be scheduled to take place once the trial is underway and the work has become routine, to avoid overestimating costs due to startup activities. To support estimates of staff time per intervention participant, the cost relevant to an adopter, the time log should include measures of activity, particularly the number of active trial participants that day or week, but also the number of devices prepared and sent or the number of people who called for help with problems.\n\nTable  2  shows a worksheet from the time log created for the EMPOWER trial. The EMPOWER trial's design made collecting staff time particularly straightforward. There was only 1 intervention arm, so no need to sort tasks by intervention, and the control arm was not contacted after enrollment, so there were no ongoing tasks associated with it (and control participants were not included in the count of active participants). The time log was set up to collect data for a full week at a time. It was completed 5 times-a total of 5 wk-over the course of the trial. Four of those weeks fell during the period when participants were still being recruited, the fifth after recruitment was finished and the last participants were completing their year in the trial.\n\nDuring recruitment, 3 full-time trial staff rotated tasks each week, focusing alternately on recruiting trial participants, monitoring electronic devices and alerts, or helping with either set of tasks as needed. After some experimentation, all 3 worksheets were set up the same way because even the recruiter and monitor sometimes helped with tasks that were not their primary responsibility that week.\n\nIn addition to the time reported by the 3 full-time staff, the project manager reported time spent managing the nonresearch tasks involved in the intervention, which averaged 4 to 5 h per week, and the hours spent each week by the part-time screener, a medical student, who regularly reviewed selected medical records to identify eligible patients. To relate the screener's hours to the workload, the project manager recorded the number of charts the screener reviewed, the number of patients entered into the computer system for invitations, and the number enrolled that week, allowing screening time to be calculated per chart, per potentially eligible person, or per person enrolled.\n\nThe part-time screener reviewed electronic medical records that had been automatically selected by means of a ''reporting tool'' that pulled the records of patients aged 18 to 80 y who had recently been discharged from the hospital with a primary diagnosis of heart failure or a secondary diagnosis of heart failure and intravenous diuretics during the hospital stay. Reporting tools are an essential component of electronic record systems, and many systems include built-in options that make it possible to set one up in a few hours. Reports can then be created daily, every few days, or weekly, depending on how quickly the practice wants to identify patients and start them in the intervention. In EMPOWER, the part-time screener then reviewed the records pulled by the tool to determine more carefully whether the patient was a good candidate for the intervention, excluding, for example, those with terminal cancer.\n\nTable  3  is a worksheet from the time log for the HF/ PvO trials. These trials involved several complexities. Like intervention participants, control participants were sent electronic pill bottles and, in the HF trial, daily reminders to take their statins, so staff spent time answering their questions. In addition, there were a total of 8 trial arms: HF involved a control arm and 3 intervention arms, each intervention lasting 6 mo; PvO involved a control arm and 3 intervention arms, each intervention lasting 12 mo. Together, the trials had 2 full-time staff members, a project manager, part-time help from a programmer who screened medical records to identify potentially eligible people, and several part-time student workers who assisted with research tasks.\n\nThe time log was set up to collect time for a full day and was completed on 11 separate days. The 11 d covered a representative sample of days and tasks, but experience with this trial suggested that, in general, collecting data for a week at a time might better reflect the time spent on implementation tasks, because tasks varied over the week and some were even allocated to specific days. Again, time for specific tasks was related, where possible, to a measure of that task (number of phone calls with participants, number of devices prepared and mailed, etc.) to allow potential adopters to understand staff needs and adjust staffing to their own circumstances.\n\nFor research purposes, all participants in all trial arms submitted lab tests at specified intervals to allow evaluation of changes in LDL-c. Only 2 interventions, however, required lab tests for their implementation: the Outcomes and Process+Outcomes interventions in the PvO trial. In those 2 interventions, lab test results were the basis for the payments patients received. Thus, costs for lab tests would be necessary to implement those 2 interventions. The time log was set up to separate time spent dealing with lab tests from tasks that were common to all the interventions, to allow cost estimates specific to each intervention.\n\nIn addition, one-time estimates were made for the HF/ PvO trials of the time spent screening medical records to identify potentially eligible patients and recruiting them to participate by letter or email, tasks that were handled by the project manager and a programmer, not by the full-time staff. Those tasks are not included in the time log worksheet shown in Table  3  but are shown in Table  4 , the numerical example that pulls together all of the cost information.\n\nOnce staff time has been measured, the hours are costed by applying an hourly wage plus fringe benefit rate. The appropriate hourly rate depends on the skills of the staff and will likely vary across potential adopters, depending on the type of people they ordinarily hire and whether existing staff are able to take on some of the work. The estimates should thus be presented in a way that allows potential adopters to use their own wage and fringe benefit rates. For purposes of illustration in this article, we used the national average hourly wage plus fringe benefits for private industry workers in March 2017, $33.11. 12 In all 3 trials, because of the exploratory nature of the research, the investigators chose not to automate some tasks that could have been handled by the technology platform. For example, the HF/PvO research team ordered lab tests and uploaded results manually. Similarly, EMPOWER staff reviewed weights recorded by the electronic scales before forwarding them to clinicians, rather than have the technology platform automatically filter out unlikely values and alert clinicians to the rest through the electronic health record. If adopters automate these tasks staff costs would be lower (and technology costs higher)."
    },
    {
      "title": "Overhead",
      "text": "Overhead includes the costs of the physical facilities in which and with which the staff works: computers, office space, office furniture, and supplies. It also includes the costs of functions that the organization performs for all staff and all projects, such as human resources, legal consultants, administration, and so forth. Potential adopters, and even private-sector trial partners such as employers, are often unwilling to share their overhead rates. Cost estimates should therefore include overhead as an explicit and separate element so that potential adopters can recalculate costs internally using their own overhead rate. We used Penn's National Institutes of Health-approved overhead rate, 62%, for purposes of illustration."
    },
    {
      "title": "Devices",
      "text": "Electronic devices that monitor and report health-related information are a rapidly growing industry, and the devices available change frequently. Their cost consists of the device itself and the cost of its integration with the technology platform. The obvious choice is to use the price the trial paid for the device and its integration, but manufacturers frequently offer devices to researchers at lower prices than they would charge other buyers, and the contracts may specify, as they did for these 3 trials, that the price cannot be published. The prices paid by adopters will also vary depending on volume discounts and other contract features. The only published price we could find for an electronic pill bottle, the Wisepill, was published in 2010 and is used here for illustration.  13 gain, this element should be shown separately so that potential adopters can replace it with the price quoted by their supplier."
    },
    {
      "title": "Incentive Payments",
      "text": "In the HV/PvO trials, a key design feature, described earlier, was that participants were offered payments of various kinds to induce them to lower their LDL-c by taking their statins consistently. The payments earned by each participant were automatically calculated by the Way to Health platform, as they would be by an adopter's platform. In the Habit Formation trial, for example, payments based on reported adherence averaged $2.35 per participant-day across the 3 interventions, for a total of $424.45 per participant over the course of the 6-mo trial."
    },
    {
      "title": "Putting It Together: An Example",
      "text": "What does a complete costing analysis done along these lines look like? How can it be presented to be most useful to potential adopters? Table  4  presents an example based on the 6-mo daily sweepstakes incentives intervention tested in the Habit Formation trial. The information is presented item by item so that potential adopters can replace items with their own estimates, experiment with their own wage and fringe rates, and so on, to see how costs might differ in their circumstances.\n\nIn this example, total implementation costs per trial participant came to $903.54 in 2017 dollars. Incentive payments, at $424 per participant over the 6 mo, were the largest component. Staff time including overhead, $189 per participant, and the cost of the device, $185 per device, were next and close in magnitude. The technology platform, although a large fixed up-front cost, averaged less per participant, $89.\n\nWhere the data come from the trial, and the potential adopter has no other source of information, it can be helpful to provide information about the possible variation in some of the items. For example, Table  4  shows the variation in the hours of staff time required per participant: the mean time was 3.524 h per participant with a standard deviation of 0.752 h over the 11 d for which time data were collected.\n\nParticipants may incur costs for participating in an intervention (e.g., time required to complete forms, time to get lab tests, and the cost of transportation to the lab). These costs may not be of concern to the potential adopter, a health plan or employer, but it is worth keeping them in mind because they will affect individuals' willingness to participate and their success in the intervention."
    },
    {
      "title": "Discussion",
      "text": "Behavioral interventions differ from the clinical interventions typically evaluated in costing, cost-effectiveness, and budget impact analyses. The resources required, which can include electronic devices, integrated technology platforms, secure messaging, and staff to advise patients on their use, are not readily costed using standard health utilization or billing databases. The best and often only sources of cost data are the clinical trials that test behavioral interventions for effectiveness.\n\nThis article describes issues that researchers need to consider in order to provide potential adopters with estimates of the costs they will likely incur if they implement a behavioral intervention that has been tested in a clinical trial. The key issue is to separate implementation costs, the costs an adopter would incur, from the costs associated with the research purposes of the trial. We have described how to frame the costing problem, including how to think about costs associated with the control group, and methods for collecting data on individual costs: the information the technology staff needs to estimate the costs of a technology platform that supports the specialized functions of the intervention, how to set up a time log to collect data on the time staff spend on various tasks and separate out the tasks involved in implementation, and issues in getting data on device, overhead, and financial incentives costs.\n\nClinical trials are a good source of information about implementing an intervention, but every implementation must be adapted to the circumstances of the adopting workplace, health plan, or clinical practice. We have described how to present the cost information from a trial so that potential adopters can explore how their circumstances may affect costs. In addition, there are costs for which clinical trials are a less useful source. For example, there are start-up costs to any new activity that involve reviewing information to decide whether the activity is likely to be a worthwhile investment, and, if so, how to introduce it successfully. Adopters may also want to evaluate the behavioral intervention after implementation to determine its acceptance by their patients or employees and their satisfaction with it. Clinical trials, with their goal of conducting research to determine the effectiveness of an intervention, are not good guides for these costs, as their start-up and evaluation costs address very different objectives. In addition to planning the intervention and its implementation, trial start-up costs include planning how to implement the sampling strategy, randomize participants, and measure and adjudicate outcomes.\n\nIt should also be noted that although costs are generally estimated and reported as though they are constant per participant, this may not be true for technology platforms. If the platform is built in-house, it will constitute a fixed cost, incurred at the beginning of the implementation process, and the cost per participant will decline with more participants. If platform services are purchased, the cost may be fixed or vary with the number of participants, depending on the terms of the purchase. Other implementation costs may involve economies of scale, with potential adopters that have a small eligible population incurring higher costs for the same tasks than adopters with large eligible populations. Wages are different in different parts of the country, and in different countries, and staff costs will be higher or lower as a result; showing the estimates of staff time separately makes it easy for potential adopters to apply their wage and fringe rates. Although staff might be asked to work overtime for a while, thus increasing costs initially, it seems unlikely that that solution would be used for an intervention that lasts several years. Costs may also vary with the demographic composition of the eligible population. Researchers can only provide estimates for populations reasonably close in size and composition to the ones from which the trial participants were drawn.\n\nIf, however, the intervention was offered through one of the growing number of companies that specialize in providing behavioral interventions, the size of the adopting organization, an employer or a health plan, and the expected number of participants, might not matter. The adopter would be only one of many customers, and the specialist company would be able to spread costs over all of them.\n\nAlthough validation is beyond the scope of this article, a useful next step would be to study the costs incurred by organizations that adopt behavioral interventions to determine whether estimates based on trial data accurately predict adopters' costs.\n\nOur primary purpose in this article has been to describe how to estimate the costs that would be incurred by potential adopters of an intervention. The same costs and methods could provide some of the information needed for the cost-effectiveness analyses used by policy makers to decide whether to approve new medical services for coverage. Micro-costing is often used to cost interventions, so the detailed enumeration of resources and estimates of staff time would be particularly useful. The final cost estimates would, however, need to reflect the circumstances and perspective of the decision maker. For example, insurers would want to adjust for any costsharing by participants, while analyses from the perspective of the health care system as a whole, recommended by the Second Panel on Cost-Effectiveness in Health and Medicine,  14  would need to include the full costs of the intervention, regardless of payer, including any additional costs incurred by participants not measured in the trial.\n\nIn summary, behavioral health interventions are increasingly prevalent but are not easily costed using standard administrative and billing data sources. Employers and health plans may be interested in offering behavioral interventions but want to know more about the likely costs of implementing them. Clinical researchers can help prospective adopters understand these costs, increasing the potential for translation of their research into practice, by undertaking a few special efforts during the trial to cost implementation tasks and publishing their cost estimates."
    },
    {
      "text": "Figure 1 Flow chart of steps and methods."
    },
    {
      "text": "Identify Tasks and Resources Required a"
    },
    {
      "text": "EMPOWER Time Log"
    },
    {
      "text": "Habit Formation (HF) and Process versus Outcomes (PvO) time log, 2 d. Two full-time staff members each filled out worksheets such as the one below, which were then combined. Only time spent on the implementation tasks is shown. The lab tasks, highlighted in gray, were involved in only some of the interventions (see text). Early in the trial (June 15, 2015), participant communication and payment reconciliation took more time. Later (October 12, 2016), ordering, following up, and checking lab results took more time"
    },
    {
      "text": "Costs for a 6-mo Daily Incentive Intervention, 2017 Dollars a"
    }
  ],
  "references": [
    {
      "title": "investigators on the Habit Formation and Process v Outcomes trials",
      "authors": [
        "Peter Putt",
        "Kevin Reese",
        "Volpp"
      ],
      "doi": "10.1177/1740774519846852"
    },
    {
      "authors": [
        "David Asch",
        "Lee Goldberg",
        "Shivan Mehta",
        "Monique Tanna",
        "Andrea Troxel",
        "Kevin Volpp"
      ]
    },
    {
      "title": "Incentives for smoking cessation",
      "authors": [
        "C Notley",
        "S Gentry",
        "J Livingstone-Banks",
        "L Bauld",
        "R Perera",
        "J Hartmann-Boyce"
      ],
      "year": 2019,
      "doi": "10.1002/14651858.cd004307.pub6"
    },
    {
      "title": "The uncertain effect of financial incentives to improve health behaviors",
      "authors": [
        "H Thirumurthy",
        "D Asch",
        "K Volpp"
      ],
      "year": 2019,
      "doi": "10.1001/jama.2019.2560"
    },
    {
      "title": "Interventions to improve adherence to lipid-lowering medication",
      "authors": [
        "M Van Driel",
        "M Morledge",
        "R Ulep",
        "J Shaffer",
        "P Davies",
        "R Deichmann"
      ],
      "year": 2016,
      "doi": "10.1002/14651858.cd004371.pub4"
    },
    {
      "title": "Healthcare Cost and Utilization Project (HCUP)",
      "year": 2019,
      "doi": "10.4135/9781412971942.n164"
    },
    {
      "title": "Fee schedules: general information",
      "year": 2019,
      "doi": "10.1037/e514792018-001"
    },
    {
      "title": "Costs for a health coaching intervention for chronic care management",
      "authors": [
        "T Wagner",
        "R Willard-Grace",
        "Chen Bodenheimer",
        "T Thom"
      ],
      "year": 2016
    },
    {
      "title": "Estimating the cost of an intervention. Veterans Administration Health Services Research and Development, Health Economics Resource Centers webinar",
      "authors": [
        "T Wagner"
      ],
      "year": 2018
    },
    {
      "title": "Rationale and design of EMPOWER, a pragmatic randomized trial of automated hovering in patients with congestive heart failure",
      "authors": [
        "S Mehta",
        "K Volpp",
        "D Asch"
      ],
      "year": 2019,
      "doi": "10.1161/circoutcomes.118.005126"
    },
    {
      "title": "The Habit Formation trial of behavioral economic interventions to improve statin use and reduce the risk of cardiovascular disease: rationale, design & methodologies",
      "authors": [
        "M Putt",
        "P Reese",
        "K Volpp"
      ],
      "year": 2019,
      "doi": "10.1177/1740774519846852"
    },
    {
      "title": "Estimating costs in cost-effectiveness analysis",
      "authors": [
        "B Luce",
        "W Manning",
        "J Siegel",
        "J Lipscomb"
      ],
      "year": 1996,
      "doi": "10.1093/oso/9780195108248.003.0006"
    },
    {
      "title": "The allocation of time: empirical findings, behavioral models, and problems of measurement",
      "authors": [
        "F Juster",
        "F Stafford"
      ],
      "year": 1991
    },
    {
      "title": "News release: employer costs for employee compensation-March 2017",
      "year": 2017,
      "doi": "10.3886/icpsr04217.v2"
    },
    {
      "title": "Real-time adherence monitoring for HIV antiretroviral therapy",
      "authors": [
        "J Haberer",
        "J Kahane",
        "I Kigozi"
      ],
      "year": 2010,
      "doi": "10.1007/s10461-010-9799-4"
    },
    {
      "title": "Recommendations for conduct, methodological practices, and reporting of cost-effectiveness analyses in health and medicine: the Second Panel on Cost-Effectiveness in Health and Medicine",
      "authors": [
        "G Sanders",
        "P Neumann",
        "A Basu"
      ],
      "year": 2016,
      "doi": "10.1001/jama.2016.12195"
    }
  ],
  "num_references": 16
}
