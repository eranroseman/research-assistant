{
  "paper_id": "X47RRM9A",
  "title": "Machine learning model to predict mental health crises from electronic health records",
  "abstract": "early 1 billion people worldwide live with a mental disorder  1  . With the global mental health emergency considerably exacerbated by the Coronavirus Disease 2019 pandemic, healthcare systems face a growing demand for mental health services coupled with a shortage of skilled personnel  [2] [3] [4] [5]  . In clinical practice, considerable demand arises from mental health crisesthat is, situations in which patients can neither care for themselves nor function effectively in the community and situations in which patients may hurt themselves or others  6, 7  . Timely treatment can prevent exacerbating the symptoms that lead to such crises and subsequent hospitalization 8 . However, patients are frequently already experiencing a mental health crisis when they access urgent care pathways as their primary entry point to a hospital or psychiatric facility. By this point, it is too late to apply preventative strategies, limiting the ability of psychiatric services to properly allocate their limited resources ahead of time. Therefore, identifying patients at risk of experiencing a crisis before its occurrence is central to improving patient outcomes and managing caseloads  9  . In busy clinical settings, the manual review of large quantities of data across many patients to make proactive care decisions is impractical, unsustainable and error-prone 10 . Thus, shifting such tasks to the automated analysis of electronic health records (EHRs) holds great promise to revolutionize health services by enabling large-scale continuous data review. Research has already demonstrated the feasibility of predicting critical events associated with a wide range of healthcare problems, including hypertension, diabetes, circulatory failure, hospital readmission and in-hospital death  [11] [12] [13] [14] [15] [16] [17]  . However, the mental health literature is limited to predicting specific types of events-such as suicide, self-harm and first episode psychosis 18-28 -rather than continuously predicting the breadth of mental health crises that require urgent care or hospitalization. Much remains unknown about the feasibility of querying machine learning models continuously to estimate the risk of an imminent mental health crisis. This would enable optimizing healthcare staff allocation and preventing crisis onset. Furthermore, even a highly accurate predictive model does not guarantee improved mental health outcomes or long-term cost savings  29, 30  ; therefore, it remains unclear whether new predictive technologies could provide tools that are useful to mental healthcare practitioners  31, 32  . This research explores the feasibility of predicting any mental health crisis event, regardless of its cause or the underlying mental disorder, and we investigate whether such predictions can provide added value to clinical practice. The underpinning assumption is that there are historical patterns that predict future mental health crises and that such patterns can be identified in real-world EHR data, despite its sparseness, noise, errors and systematic bias 33 . To this end, we developed a mental crisis risk model by inputting EHR data collected over 7 years (2012-2018) from 17,122 patients into a machine learning algorithm. We evaluated how accurately the model continuously predicted the risk of a mental health crisis within the next 28 days from an arbitrary point in time, with a view to supporting dynamic care decisions in clinical practice. We also analyzed how the model's performance varied across a range of mental health disorders, across different ethnic, age and gender groups and across variations in data availability. Furthermore, we conducted a prospective cohort study to evaluate the crisis prediction algorithm in clinical practice from 26 November 2018 to 12 May 2019. The crisis predictions were delivered on a biweekly basis to four different groups of clinicians (in total, 60 clinicians attending 1,011 cases over 6 months), who evaluated whether and how such predictions helped them manage caseload priorities and mitigate the risk of crisis. \n Results Prediction target. As our main goal was to develop a predictive tool that could help healthcare workers manage caseload priorities and pre-emptively intervene to mitigate the risk of crisis, we established the prediction target to align with the service-oriented approach to defining crisis 7 -that is, the onset of severe symptoms that require substantial healthcare resources.",
  "year": 2022,
  "date": "2022-05-16",
  "journal": "Lancet",
  "publication": "Lancet",
  "authors": [
    {
      "forename": "Roger",
      "surname": "Garriga",
      "name": "Roger Garriga",
      "affiliation": "1  Koa Health , Barcelona , Spain. \n\t\t\t\t\t\t\t\t Koa Health \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Barcelona \n\t\t\t\t\t\t\t\t\t Spain",
      "email": "roger.garrigacalleja@koahealth.com"
    },
    {
      "forename": "Javier",
      "surname": "Mas",
      "name": "Javier Mas",
      "affiliation": "1  Koa Health , Barcelona , Spain. \n\t\t\t\t\t\t\t\t Koa Health \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Barcelona \n\t\t\t\t\t\t\t\t\t Spain",
      "orcid": "0000-0002-5836-5534"
    },
    {
      "forename": "Semha",
      "surname": "Abraha",
      "name": "Semha Abraha",
      "affiliation": "4  Birmingham and Solihull Mental Health NHS Foundation Trust , Birmingham , UK. \n\t\t\t\t\t\t\t\t Birmingham and Solihull Mental Health NHS Foundation Trust \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Birmingham \n\t\t\t\t\t\t\t\t\t UK",
      "orcid": "0000-0002-8752-4098"
    },
    {
      "forename": "Jon",
      "surname": "Nolan",
      "name": "Jon Nolan",
      "affiliation": "4  Birmingham and Solihull Mental Health NHS Foundation Trust , Birmingham , UK. \n\t\t\t\t\t\t\t\t Birmingham and Solihull Mental Health NHS Foundation Trust \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Birmingham \n\t\t\t\t\t\t\t\t\t UK"
    },
    {
      "forename": "Oliver",
      "surname": "Harrison",
      "name": "Oliver Harrison",
      "affiliation": "1  Koa Health , Barcelona , Spain. \n\t\t\t\t\t\t\t\t Koa Health \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Barcelona \n\t\t\t\t\t\t\t\t\t Spain",
      "orcid": "0000-0001-6947-2581"
    },
    {
      "forename": "George",
      "surname": "Tadros",
      "name": "George Tadros",
      "affiliation": "4  Birmingham and Solihull Mental Health NHS Foundation Trust , Birmingham , UK. \n\t\t\t\t\t\t\t\t Birmingham and Solihull Mental Health NHS Foundation Trust \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Birmingham \n\t\t\t\t\t\t\t\t\t UK"
    },
    {
      "forename": "Aleksandar",
      "surname": "Matic",
      "name": "Aleksandar Matic",
      "affiliation": "1  Koa Health , Barcelona , Spain. \n\t\t\t\t\t\t\t\t Koa Health \n\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t Barcelona \n\t\t\t\t\t\t\t\t\t Spain",
      "email": "aleksandar.matic@koahealth.com",
      "orcid": "0000-0002-8752-4098"
    }
  ],
  "doi": "10.1038/s41591-022-01811-5",
  "keywords": [
    "Any methods",
    "additional references",
    "Nature Research reporting summaries",
    "source data",
    "extended data",
    "supplementary information"
  ],
  "sections": [
    {
      "title": "Articles",
      "text": "Nature MediciNe of approaches to defining a mental health crisis in the literature (namely service-oriented, risk-focused, self-defined and negotiated definitions  7  ), these definitions consistently describe an event that substantially affects the life of a patient and the load on healthcare services. Correspondingly, our dataset included crisis events, which were registered every time a patient urgently needed mental health crisis services, such as emergency assessment, inpatient admission, home treatment assessment or hospitalization. Because crisis events frequently occur in succession when a patient is undergoing a crisis, predicting each singular crisis event registered in the EHR would be of little clinical relevance because patients who experience one crisis event receive close clinical attention over successive days. Therefore, we defined the prediction target as the onset of a crisis episode, which contains one or more crisis events, preceded by at least one full stable week without any crisis event (Fig.  1 ). Accordingly, we trained the machine learning model to predict the onset of a crisis episode-that is, the first crisis event in an episode-within the next 28 days. The time horizon of 28 days was selected based on input from clinicians to support the management of caseload priorities and to enable pre-emptive interventions. Notably, using different time horizons (that is, other than 28 days) or defining a stable period before a relapse other than 7 days did not substantially affect the model's performance (Supplementary Table  9 ).\n\nDataset. Upon applying the exclusion criteria (Methods), the study cohort data contained 5,816,586 records collected between September 2012 and November 2018 from 17,122 unique patients aged between 16 and 102 years. This included patients with a wide range of diagnosed disorders, including mood, psychotic, organic, neurotic and personality disorders. The two genders and the full range of ethnic groups were well represented in the dataset (51.5% males and 48.6% females; 66% White, 15% Asian, 9% Black and 7% Mixed). No major deviations were observed in the crisis distribution according to gender or ethnicity or disability (see Extended Data Fig.  1  for the complete summary). In total, 60,388 crisis episodes were included in the analysis, with a mean of 24 crisis events per episode. Among the 1,448,542 crisis events that were recorded, 942,017 corresponded to hospitalizations. The rest of the EHR data included phone and in-person contact with patients (2,239,632 records), referrals (250,864 records) and well-being and risk assessments (118,255 and 248,629 records) (see Supplementary Table  1  for more details). Our prediction target variable had a prevalence of 4.0% on average across the entire dataset, varying from 1.9% (organic disorders) to 7.2% (disorders of adult personality and behavior) (see Extended Data Fig.  2  for a detailed breakdown by diagnosis, training and test sets)."
    },
    {
      "title": "Development of a mental health crisis prediction model.",
      "text": "The model was designed to be queried weekly to infer each patient's risk of experiencing a crisis episode during the upcoming 28-day period. To build the model, we extracted three feature categories: (1) static or semi-static patient information (such as age, gender and International Classification of Diseases 10 (ICD-10)  34  coded diagnoses); (2) latest available assessments and interactions with the hospital (for example, most recent risk assessments or well-being indicators and severity and number of crisis events in the last episode and similar); and (3) variables representing the time elapsed since the registered events (for example, crisis episodes, contacts and referrals). In total, we extracted 198 features (Supplementary Table  5 ). When the system was implemented, instead of a binary outcome, the model was generating a predicted risk score (PRS) between 0 and 1 for each patient. Figure  2  presents the end-toend process.\n\nWe tested a range of machine learning techniques, including decision trees, probabilistic, ensembles and deep learning-based classifiers. Consistent with similar studies  11, 16, 35  , XGBoost (eXtreme Gradient Boosting) outperformed most of the other methods evaluated (although, in some cases, only by small margins). The Mann-Whitney U-test suggested a significantly better performance of XGBoost (P < 0.01) when compared to the other methods, except for a feed-forward neural network (Extended Data Figs.  3  and  4 ). The XGBoost model relied on an automatically selected subset of 104 features to predict mental health crises for all patients in our dataset (referred to as the general model). We benchmarked this model against two baseline classifiers: (1) the clinical-practice-based baseline model, developed to emulate a doctor's decisions (specifically, a decision tree using a selection of patient status indicators that doctors in our clinical setting use to assess the risk of relapse); and (2) the diagnosis-based baseline model, developed as a logistic regression that relies solely on diagnosis and time elapsed since the last crisis, resembling a threshold-based rule system (see Extended Data Fig.  5  for each baseline's list of features). The area under the receiver operating characteristic (AUROC) curves of the general model, the clinical-practice-based baseline and the diagnosis-based baseline were 0.797 (95% confidence interval (CI) 0.793-0.802), 0.736 (95% CI 0.733-0.740) and 0.746 (95% CI 0.741-0.750) (Fig.  3 ). For unbalanced datasets, as in our case, the average precision (AP)  36  represents a more informative metric  37  , and the APs obtained for the general model, the clinical-practice-based baseline and the diagnosis-based baseline were 0.159 (95% CI 0.154-0.165), 0.092 (95% CI 0.090-0.094) and 0.092 (95% CI 0.089-0.094). The general model significantly outperformed the two baseline models (P < 0.0001 for both AUROC and AP). We calibrated the predictions using isotonic regression  38  (Extended Data Fig.  6 ), ensuring that the predicted risk reflected the actual expected risk of experiencing a crisis episode  39  , and obtained a Brier score 40 of 0.028 (95% CI 0.028-0.029). Additionally, the general model demonstrated a more substantial net benefit in the decision curve analysis  41  than the baseline models and default strategies (Extended Data Fig.  6 ).\n\nModel performance for different disorders. We evaluated the performance of the prediction model in patients with mental health disorders grouped according to the first-level categorization of the ICD-10 (ref.  34 ). We relied solely on AUROC to evaluate the model performance of each disorder because the AP is an inappropriate metric for comparing groups with different prevalence values  37  . The general model performed considerably better for organic disorders, with an AUROC of 0.890 (95% CI 0.852-0.928) compared to the overall performance of 0.797 (95% CI 0.793-0.802). For other diagnostic groups, the performance ranged between 0.770 (95% CI 0.760-0.779)\n\nWeek 1 Week 2 Week 3 Week 4 Week 5 Crisis episode Crisis onset Crisis event\n\nFig.  1  | crisis episode example. example of a crisis episode timeline: crisis onset is the first crisis event of a crisis episode that follows a stable week (that is, a week without crisis events).\n\nand 0.814 (95% 0.796-0.831). The lowest performance was observed for mood-affective disorders, followed by schizophrenia and schizotypal and delusional disorders. Separate models for each diagnosis subgroup were developed and compared to the general model. The general model consistently outperformed the baseline models, and no disorder-specific model performed significantly better than the general model (Fig.  3c  and Extended Data Fig.  7 )."
    },
    {
      "title": "Model performance for different age groups.",
      "text": "We evaluated the general model in subgroups of patients across different age groups.\n\nThe model performance dropped to 0.743 (95% CI 0.718-0.767) for patients younger than 18 years and increased to 0.840 (95% CI 0.820-0.859) for patients aged between 65 and 74 years. For the other age groups, the model performed similarly, with an AUROC between 0.782 (95% CI 0.771-0.793) and 0.796 (95% CI 0.786-0.806) (Fig.  3d  and Extended Data Fig.  8 ).\n\nEffect of data availability on model performance. Data availability substantially affected model performance. For example, if there was no information about a patient for 1 year or more, the AUROC dropped to 0.617 (95% CI 0.592-0.641). Meanwhile, for patients who had at least one record within the previous month, the AUROC\n\nRaw data Data Modeling Medical application Crisis events Diagnoses Contacts CrisisType Medical event (e.g., crisis event) Patient Crisis Referral Contact IP bed Diagnosis Care cluster Risk screening Others Month 1 Feature store Predicting risk of crisis event Model selection and training 2012 Medical dashboard Patients at high risk Patient notes Questionnaire about patient Previous risk assessment Current risk assessment Planned contacts Indicators Medical contact within 28 days Older than 65 False Stated that the residents all use drugs and he is anxious around them. Was offered new accomodation in another residence but refused as the residents appeared \"dodgy\". False True True Dual diagnosis Crisis event within last 4 weeks PatientsID 98,765 12,345 98,450 34,567 2019 1 week High Low Mid Patient features Weekly features Month 2 Month 3 Month 4 Month 5 Month 6 Month 7 Month 8 Month 9 Month 10 Month 11 Month 12 Static information about the patient Diagnosis, carries over time PatientID Date 0 Date_0 Date_1 Date_N 0 N ... ... 0 Date_0 Date_1 Date_N 0 N ... ... ... Diagnose PatientID Date ... 0 Date_0 Date_1 Date_N 0 N ... ... ContactType PatientID Date ... Articles Nature MediciNe 1.0 a b c d e f 0.8 0.6 True-positive rate 0.4 0.2 0 1.0 0.8 0.6 Precision 0.4 0.2 0 0 0.2 0.4 0.6 XGBoost general (AUROC = 0.797 \u00b1 0.005) XGBoost per diagnosis (AUROC = 0.764 \u00b1 0.004) Clinical baseline (AUROC = 0.736 \u00b1 0.004) Diagnosis baseline (AUROC = 0.746 \u00b1 0.005) Random XGBoost general (AP = 0.159 \u00b1 0.005) XGBoost per diagnosis (AP = 0.123 \u00b1 0.005) Clinical baseline (AP = 0.092 \u00b1 0.002) Diagnosis baseline (AP = 0.092 \u00b1 0.002) Random False-positive rate F3 mood F2 schizophrenia and psychotic F6 personality and behavior F1 substance misuse Primary diagnosis Other diagnosis Not diagnosed F4 neurotic stress and anxiety F0 organic 0.8 0.60 0.85 0.83 0.80 0.78 0.75 0.73 0.70 Age group 0.77 0.60 0.65 0.70 0.75 Time since first visit (mth = months), (yr = years) < 6 m t h 6 -1 2 m t h 1 -2 y r 2 -5 y r > 5 y r Time since last visit (mth = months), (yr = years) < 1 m t h 1 -3 m t h 3 -6 m t h 6 -1 2 m t h 1 -2 y r > 2 y r 0.78 0.79 0.80 0.81 0.82 < 1 8 1 8 -2 4 2 5 -3 4 3 5 -4 4 4 5 -5 4 5 5 -6 4 6 5 -7 4 > 7 5 0.65 0.70 0.75 0.80 AUROC AUROC AUROC AUROC 0.85 0.90 0.95 1.00 1.0 0 0.2 0.4 0.6 Recall Model XGBoost general XGBoost per diagnosis Clinical baseline Diagnosis baseline 0.8 1.0 was 0.765 (95% CI 0.761-0.771). A longer history of patient data in the EHR of the hospital improved the model's performance, with AUROCs ranging from 0.794 (95% CI 0.772-0.817) for patients who had first visited within the previous 6 months to 0.816 (95% CI 0.805-0.827) for patients whose first record dated back 5 years or more (Fig.  3e ,f and Extended Data Fig.  8 )."
    },
    {
      "title": "Analysis of the most predictive features.",
      "text": "We analyzed the relative effect of the top 20 features on the model at each data point in the test set according to the mean absolute SHAP (SHapley Additive exPlanations)  42  value (Fig.  4 ). The historical severity of symptoms (specifically, the total number of crisis episodes and the duration of the last episode), interactions with the hospital (including unplanned contacts, missed appointments or a recent crisis), patient characteristics (including age and individual risk indices) and total time since the patient's first hospital visit carried most of the general model's predictive power (Fig.  4a , b ).\n\nTo further examine the effect of each variable, we also analyzed the SHAP values of the top 20 features separately (Fig.  4c-h  and Supplementary Fig.  1 ). The recency of records (especially important events such as crises and unplanned contacts) had a major effect on the PRS, positively contributing to the risk score up to a threshold value beyond which they began driving the risk score down. However, the effect of different events varied over time, with some having a long-lasting effect and others affecting the risk score only during the first weeks after their incidence. Unplanned contacts with a patient had the biggest short-term effect, but their effect disappeared almost completely after only 2 weeks. Longer-lasting effects were observed for events encoding contacts with the carer and missed appointments, which produced sustained effects on the PRS for 10 weeks and 16 weeks. In turn, referrals and crises considerably affected the PRS both positively (for approximately 6 months or 25 weeks and 29 weeks, respectively) and negatively (thereafter). The variables reflecting severe symptoms generally demonstrated the longest-lasting effects on the PRS. For example, referrals from acute services, positive suicide risk assessments and positive substance misuse assessments affected the PRS for 1-2 years. In most cases, the presence of important events was associated with a previous clinical deterioration, which means that the absence of certain types of events-denoted in the features by NaN (not a number) values-suggests less severe symptoms in the patient's history and negatively affected the PRS. Consider, for instance, a patient who had never been hospitalized. This had a negative influence on the PRS. In contrast, a positive influence would be observed for a patient who had been hospitalized at least once before. Finally, to investigate the complexity of the interactions among features that drive the PRS, we used the force plots of positive and negative predictions (Extended Data Fig.  9 ). The sign and magnitude of each variable's contribution differed according to the value of the other variables and its own value, thus demonstrating the model's complex and non-linear nature.\n\nClinical evaluation. To assess the added value of the algorithm in clinical practice, we conducted a prospective study in which crisis predictions were delivered to clinicians every 2 weeks. We queried our prediction model to rank patients in descending order based on the PRS. Four multidisciplinary clinical teams (Community Mental Health Teams (CMHTs); see Table  1  for the team composition) each received a dashboard displaying the 25 patients with the highest PRS. Before exploring the algorithm's practical value, we asked the CMHTs to assess the risk of crisis for each patient and rate their agreement with each prediction. Disagreement was recorded in 7% (n = 65) of all the presented predictions provided over 6 months, ranging from 3% (n = 6) to 12% (n = 27) across the four CMHTs. Overall, CMHTs rated 38% (n = 351) of the cases as low risk, 44% (n = 407) as medium risk, 13% (n = 119) as high risk and less than 0.1% (n = 3) as being at imminent risk of experiencing a mental health crisis. Meanwhile, 6% (n = 55) of the reviewed cases were patients already experiencing a crisis. Upon reviewing the predictions, CMHTs responded that they would make contact either by telephone (5% of cases) or in person (8% of cases; average percentage calculated based on the responses of the four teams; see F1 in Table  2 ). This corresponds to patients who otherwise would not have been attended to. Although the predictions were accurate in most other cases, no further action was required because the CMHTs were already managing the risk.\n\nThe risk assessment was part of the feedback form delivered after an initial review of the presented cases (F1 in Table  2 ) with a completion rate of 92% (n = 935). One week after reviewing the patients flagged by the algorithm, CMHTs reassessed each case's risk level. Their assessment of patient risk of crisis reduced in 17% of cases.\n\nMeanwhile, their perception of risk increased in 8% of cases. Clinicians rated the value of the risk predictions for mitigating the risk of crisis and for managing the caseload priority on the second feedback form (F2 in Table  2 ). The completion rate for F2 was 84% (n = 846) (see Table  1  for a detailed breakdown of the teams). Five months after the study started, semi-structured interviews were conducted to obtain additional insights into the algorithm's implementation and the effect on decision-making in clinical practice (see the qualitative report in Supplementary Materials-Qualitative Evaluation).\n\nMitigating the risk of crisis. We evaluated the opportunity to mitigate the risk of a crisis using two questions that probed whether the algorithm helped identify patient deterioration and enabled a pre-emptive intervention to prevent a crisis. Predictions were rated useful in 64% (n = 602) of the presented cases overall and in more than 70% of cases in three of the four CMHTs. Only one CMHT (Team 4 in Table  2 ) reported no added value at a high percentage (71%; n = 145), with all other teams reporting percentages below 30%. Notably, CMHTs reported that the model was clinically valuable in terms of preventing a crisis in 19% (n = 175) of cases and in terms of identifying the deterioration of patient conditions in 17% (n = 159) of cases.\n\nManaging the caseload. The value of our tool for managing caseload priorities was indirectly captured by analyzing whether risk predictions helped clinicians identify patient deterioration and decide which patients to contact. Managing caseload priorities is a complex task (especially in high-demand settings), and clinicians often rely on various parameters to prioritize caseloads, including prior knowledge about individual patients, subjective views about risks and diagnosis severity. Accordingly, we opted to capture the value of risk predictions using a general question that prompts clinicians to directly rate the value of the predictive tool for managing their caseload, with the responses indicating that the model output was used to manage caseload priorities in 28% (n = 268) of cases (see Table  2  for a detailed summary)."
    },
    {
      "title": "discussion",
      "text": "We have demonstrated the feasibility of predicting mental health crises by applying machine learning techniques to longitudinally collected EHR data, obtaining an AUROC of 0.797 for the general model. Despite the data availability concerns associated with the EHR (related to periods with no patient records), querying the prediction model continuously-that is, in a rolling window mannerproduced a better performance than that obtained by the baseline models. The lack of records for more than 3 months resulted in a 7%\n\nTable 1 | Prospective study participants and completion rate (grouped by team)\n\nNo. (%) Team 1 Team 2\n\nTeam 3 Team 4 Total Clinicians n = 13 n = 19 n = 14 n = 14 n = 60 Male 5 (38) 6 (32) 5 (36) 4 (29) 20 (33) Female 8 (62) 13 (68) 9 (64) 10 (71) 40 (67) Nurses 12 (92) 15 (79) 11 (79) 13 (93) 51 (85) Doctors 1 (8) 2 (11) 0 (0) 1 (7) 4\n\nOccupational therapists 0 (0) 1 (  5 ) 1 (  7 )\n\nDuty workers 0 (0) 1 (  5 ) 1 (  7 )\n\nSocial workers 0 (0) 0 (0) 1 (  7 ) 0 (  0 ) 1 (2) Form completion n = 292 n = 279 n = 196 n = 244 n = 1,011 F1 292 (100) 246 (87) 177 (90) 220 (89) 935 (92) F2 274 (94) 221 (78) 159 (81) 202 (80) 856 (84)\n\ndrop in AUROC. Meanwhile, having no records about a patient for more than 6 months or 1 year contributed to drops of 13% and 20%, respectively. Unsurprisingly, having a longer data history improved the risk prediction performance for a given patient. Among the machine learning models evaluated, XGBoost demonstrated the best overall performance. Nonetheless, in a few cases, there were only marginal or no significant improvements in comparison to other techniques (Extended Data Figs.  3  and  4 ). Training different models for each group of disorders to leverage the specificity of mental health disorders did not prove superior to the general model despite the differences in the performance of the general model for different disorders (Fig.  3c ). No significant difference in performance was observed across different diagnostic groups, except for increased performance for organic disorders (likely due to their lower prevalence). We further expanded the subgroup analysis to assess the algorithm's fairness. Among the common protected attributes (namely, gender, age, ethnic groups and disability), we observed a 5% increase in the AUROC for patients aged 65-74 years (likely a consequence of the considerably lower prevalence of this group) and a 7% lower AUROC for the 'Black' ethnic subgroup compared to the 'White' ethnic subgroup. We refrained from unpacking the potential causes of this disparate effect due to the complexity of known and unknown biases and factors that could not be controlled for (see Supplementary Materials-Fairness Analysis).\n\nWe evaluated whether a tool predicting and presenting risk of mental health crisis provides added value for clinical practice in terms of managing caseloads and mitigating the risk of crisis. On average, the CMHTs disagreed with only 7% of the model predictions, with the model outputs found to be clinically useful in 64% of individual cases. We did not successfully identify why considerably lower scores were observed in the responses from one of the four CMHTs, with neither the study process nor team and patient selection introducing any known bias. However, crucially, risk predictions were relevant to preventing crises in 19% of cases, to identifying the deterioration of a patient's condition in 17% of cases and to managing caseload priorities in 28% (n = 268) of cases. Notably, the importance of the algorithm for identifying at-risk patients who would otherwise have been missed emerged from the semi-structured interviews conducted with the clinicians as part of the qualitative evaluation (see Supplementary Materials-Qualitative Evaluation). The relatively high percentage of cases (36%) in which predictions were not perceived as useful was substantially affected by the number of serious cases that were already being recognized and managed by the CMHTs. Nevertheless, the clinicians opted to receive the list of patients at the highest risk of experiencing a crisis even if doing so would mean including patients whom they were already monitoring. It is reasonable to expect that the requirements for the practical implementation would not be considerably different in other clinical settings. That is, broadening the prediction list to all patients registered in the hospital system would reduce the value of each prediction relative to clinician caseload, thus having little benefit.\n\nOur study's main limitation concerns the known and potentially unknown specificity of the single-center cohort. Given that EHRs are characterized by high dimensionality and heterogeneity, risk prediction algorithms suffer from overfitting the model to the data, Table  2  | Responses to the feedback forms F1 and F2 from each team of clinicians involved in the prospective study No. (%) Team 1 Team 2"
    },
    {
      "title": "Team 3 Team 4 Total"
    },
    {
      "title": "F1 responses n = 292 n = 246 n = 177 n = 220 n = 935",
      "text": "Assessment of patient's risk of crisis Low risk 99 (  34 ) 89 (  36 ) 48 (  27 ) 115 (  52 ) 351 (38)   Medium risk 136 (  47 ) 96 (  39 ) 92 (  52 ) 83 (  38 ) 407 (44)   High risk 29 (10)  59 (  24 ) 21 (  12 ) 10 (  5 ) 119 (  13 )\n\nAlready in crisis 26 (  9 ) 2 (  1 ) 15 (  8 ) 12 (  5 ) 55 (6)   Have you taken / do you intend to take any actions as a result of this notification?\n\nYes, contact to be made (Telf) 9 (  3 ) 15 (  6 ) 11 (  6 ) 8 (  4 ) 43 (5)   Yes, contact to be made (F2F) 12 (  4 ) 38 (  15 ) 11 (  6 ) 10 (  5 ) 71 (  8 )\n\nNo, contact made in last 7 days 46 (  16 ) 28 (  11 ) 41 (  23 ) 29 (  13 ) 144 (15)   No, risk already being managed 202 (69) 156 (63) 109 (61) 146 (66) 613 (65) No, do not agree with assessment 23 (8) 9 (4) 6 (3) 27 (12) 65 (7) F2 responses n = 274 n = 221 n = 159 n = 202 n = 856 What is your current assessment of this patient's condition? Low risk 110 (40) 102 (  46 ) 47 (  30 ) 110 (  54 ) 369 (43)   Medium risk 124 (  45 ) 72 (  33 ) 83 (  52 ) 73 (  36 ) 352 (41)   High risk 25 (  9 ) 42 (  19 ) 16 (  10 ) 7 (  3 ) 90 (  11 )\n\nAlready in crisis 14 (  5 ) 3 (  1 ) 13 (  8 ) 12 (  6 ) 42 (  5 ) Do you think that this additional information has helped you with \u2026?\n\nMitigating the risk of crisis -Trying to prevent a crisis 36 (  12 ) 75 (  28 ) 45 (  26 ) 19 (  9 ) 175 (  19 )\n\n-Identifying patient's deterioration 57 (20)  62 (  23 ) 32 (  18 ) 8 (  4 ) 159 (17)   Managing caseload priorities 125 (  43 ) 62 (  23 ) 48 (  27 ) 33 (  16 ) 268 (28)   Nothing, it was not useful 73 (  25 ) 72 (  27 ) 50 (  29 ) 145 (71) 340 (36)  which limits the generalizability of the results and undermines most predictive features. However, many data fields are expected to be routinely captured by typical mental health centers, even if they only register crisis emergencies, visits and hospitalizations. Based on this understanding, we selected only eight of the top 20 features derived solely from events related to crises, contacts and hospitalization (see the list in Supplementary Material-Crisis Prediction Model) and evaluated the corresponding model. The resulting AUROC was 0.781 (compared to 0.797 for the general model). Furthermore, we limited our algorithm's applicability to patients with a history of relapse, a decision that was based on healthcare demand: patients prone to relapse require a considerable proportion of healthcare resources because they frequently need urgent and unplanned support, which engenders major challenges for optimizing healthcare resources. Thus, further research should probe the feasibility of developing an algorithm to detect first crises. Finally, although the clinicians reported that the prediction model helped to prevent a crisis in 19% of cases, this eventuality was not witnessed because it would have implied that the clinicians did not react to the predictions, which would have been ethically and legally unacceptable. Machine learning techniques trained on historical patient records have demonstrated considerable potential to predict critical events in different medical domains (for example, circulatory failure, diabetes and cardiovascular disorders)  [11] [12] [13] [14] [15]  . In the mental health domain, prediction algorithms have typically focused on detecting individual propensity to die by suicide or develop psychosis, with no extant studies attempting to continuously detect important mental health events or those that would require readmission for urgent care or hospitalization. Nonetheless, several studies have considered predictions of unplanned hospital readmissions regardless of their underpinning reason  17, [43] [44] [45] [46]  and obtained AUROCs between 0.750 and 0.791 for predicting the risk of readmission within 30 days (similar to our results of 0.797 within 28 days). Although such algorithms can importantly benefit healthcare, their potential to improve caseload management or prevent unwanted health outcomes is limited by (1) the timing of queries (only at discharge rather than continuously) and (2) the nature of readmissions (not specific to any disorder in particular; as highlighted by the authors  46  and the literature  47  , most such readmissions are not preventable). Running predictions continuously  13, 14  provides an updated risk score based on the latest available data, which typically contains the most predictive information, which is, in the case of mental health, crucial to improving healthcare management and outcomes.\n\nThe rising demand for mental healthcare is increasingly prompting hospitals to actively work on identifying novel methods of anticipating demand and better deploying their limited resources to improve patient outcomes and decrease long-term costs  9, 48  . Evaluating technical feasibility and clinical value are critical steps before integrating prediction models into routine care models  32  . From this perspective, our study paves the way for better resource optimization in mental healthcare and enabling the long-awaited shift in the mental health paradigm from reactive care (delivered in the emergency room) to preventative care (delivered in the community)."
    },
    {
      "title": "Articles"
    },
    {
      "title": "Nature MediciNe"
    },
    {
      "title": "Methods",
      "text": "Study design and setting. This study comprised two phases. The first phase involved a retrospective cohort study designed to build and evaluate a mental health crisis prediction model reliant on EHR data. The second phase implemented this model in clinical practice as part of a prospective cohort study to explore the added value it provides in the clinical context. Added value was defined as the extent to which the predictive algorithm could support clinicians in managing caseload priorities and mitigating the risk of crisis.\n\nThe retrospective and prospective studies were both conducted at Birmingham and Solihull Mental Health NHS Foundation Trust (BSMHFT). One of the largest mental health trusts in the UK, BSMHFT operates over 40 sites and serves a culturally and socially diverse population of over 1 million patients. The retrospective study used data collected between September 2012 and November 2018; the prospective study began on 26 November 2018 and ran until 12 May 2019.\n\nEthical approval and consent. The Health Research Authority (HRA) approved the study. The HRA ensures that all NHS research governance requirements are met and that patients and public interests are protected. For the historical data used in the retrospective study, the need to obtain consent was waived on the basis of the use of anonymized data that cannot be linked to any individual patient. Furthermore, the consent form that had already been signed by patients upon joining the corresponding mental health service within the NHS included the potential purpose of using patient records for predictive risk analyses. Meanwhile, the participants in the prospective study were the healthcare staff members who consented to participation in the research and who had been trained in the use of the algorithm and its outputs in support of their clinical practice.\n\nDataset. The dataset comprised anonymized clinical records extracted from a retrospective cohort of patients who had been admitted to BSMHFT. The data included demographic information, hospital contact details, referrals, diagnoses, hospitalizations, risk and well-being assessments and crisis events for all inpatients and outpatients. No exclusion criteria based on age or diagnosed disorder were applied, meaning that patient age ranged from 16 to 102 years and that a wide range of disorders was included. However, to include only patients with a history of relapse, patients who had no crisis episode in their records were excluded. This decision was made because detecting first crises and detecting relapse events correspond to different ground truth labels and different data. Furthermore, given that detecting relapse events can leverage information about the previous crisis, patients with only one crisis episode were excluded because their records were not suitable for the training and testing phases. Additionally, patients with three or fewer months of records in the system were excluded because their historical data were insufficient for the algorithm to learn from. For the remaining patients, predictions were queried and evaluated for the period after two crisis episodes and after having the first record at least 3 months before querying the model. This produced a total of 5,816,586 electronic records from 17,122 patients in the database used for this study. Supplementary Table  1  breaks down the number of records per type, and Supplementary Table  2  compares the representation of different ethnic groups and genders in the study cohort, the original hospital cohort and the Birmingham and Solihull area.\n\nFeatures and labels generation. With the exception of the static information, all EHR data included the associated date and time. The date and time refer to the moment when the specific event or assessment occurred-that is, the date and time that a patient was admitted to hospital or assigned a diagnosis. To prepare the data for the modeling task, each patient's records were consolidated at a weekly level according to the date associated with the record. Following this process, we generated evenly spaced time series for each patient that spanned from the patient's first interaction with the hospital to the study's final week. The features and labels generated for each week were computed using the data with a date prior to that week. Static data susceptible to change over time (for example, marital status) were removed to mitigate the risk of retrospective leakage.\n\nLabel generation. To construct the binary prediction target, each patient-week was assigned a positive label whenever there was a relapse during the following 4 weeks (if the patient had not had a crisis during the current week) and a negative label otherwise. To assess the extent to which the model was sensitive to such a definition of the main label, we built 47 additional labels by varying three parameters:\n\n\u2022 The number of stable weeks (without crisis) necessary to consider a crisis episode concluded: from 1 to 4 weeks. \u2022 The prediction time window length (that is, the time window in which the algorithm assesses the risk of crisis): from 1 to 4 weeks. \u2022 The number of weeks between the time of querying the algorithm and the start of the prediction time window: from 0 to 2 weeks.\n\nFeatures generation. We extracted a total of 198 features from the ten data tables (Supplementary Table  5 ). Each data table was processed separately, and no imputation that could add noise to the data was performed. Feature extraction was performed according to six procedures:\n\n\u2022 Static or semi-static features. Demographics data were represented as constant values attributed to each patient, with age treated as a special case that changed each year. \u2022 Diagnosis features. Patients were assigned their latest valid diagnosed disorder or a 'non-diagnosed' label and then separated into diagnostic groups according to the latest valid diagnosed disorder at the last week of the training set to avoid leakage into the validation and test sets. Each diagnosed disorder was mapped to its corresponding first-level category according to the ICD-10 (ref.  34 ) code system. For instance, F200 paranoid schizophrenia disorder was mapped to the F2 Schizophrenia and Psychotic category. We shortened the names of the first-level ICD-10 categories for brevity and to improve figure layouts: \u2022 F0 Organic: organic, including symptomatic, mental disorders (ICD-10 codes F00-F09). \u2022 F1 Substance Misuse: mental and behavioral disorders caused by psychoactive substance use (ICD-10 codes F10-F19). \u2022 F2 Schizophrenia and Psychotic: schizophrenia and schizotypal and delusional disorders (ICD-10 codes F20-F29). \u2022 F3 Mood: mood (affective) disorders (ICD-10 codes F30-F39).\n\n\u2022 F4 Neurotic, Stress and Anxiety: neurotic, stress-related and somatoform disorders (ICD-10 codes F40-49). \u2022 F6 Personality and Behavior: disorders of adult personality and behavior (ICD-10 codes F60-69). \u2022 Other Diagnosis: any other disorder not contemplated by the previous categories (ICD-10 codes F50-59 and F70-99). \u2022 Not Diagnosed: no diagnosed disorder available in the EHR.\n\n\u2022 EHR weekly aggregations. EHRs related to patient-hospital interactions were aggregated on a weekly basis for each patient. The resulting features constituted counts per type of interaction, one-hot encoded according to their categorization. If a specific type of event did not occur in a given week, a value of '0' was assigned to the feature related to the corresponding type of event for the corresponding week. \u2022 Time-elapsed features. At each patient-week, for each type of interaction and category, we constructed a feature that counted the number of weeks elapsed since the last occurrence of the corresponding event. If the patient had never experienced such an event type up to that point in time, NaN values were used. \u2022 Last crisis episode descriptors. For each crisis episode, a set of descriptors summarizing the length and severity of the crisis episode was built. These descriptors were used to build features for the subsequent weeks until the next crisis occurred. If the patient had never had a crisis episode up to that point in time, NaN values were used. \u2022 Status features. For specific EHRs that are characterized by the start-end date, features for the corresponding weeks were built by assigning their corresponding value (or category); otherwise, they were set to NaN.\n\nIn addition to EHR-based features, we also added the week number (of a year, 1-52) to account for seasonality effects. Given the cyclical nature of the feature, we encoded the information using the trigonometric transformations sine and cosine: sin(2\u03c0 week 52 ) and cos(2\u03c0 week 52 ).\n\nCrisis prediction modeling and evaluation. We defined the crisis prediction task as a binary classification problem to be performed on a weekly basis. For each week, the model predicts the risk of crisis onset during the upcoming 28 days. Applying a rolling window approach allows for a periodic update of the predicted risk by incorporating the newly available data (or the absence of it) at the beginning of each week. This approach is very common in settings where the predictions are used in real time and when the data are updated continuously, such as for predicting circulatory failure or sepsis intensive care units  13, 14  . We applied a time-based 80%/10%/10% training/validation/test split: Performance evaluations were conducted on a weekly basis, and each week's results were used to build CIs on the evaluated metrics. All reported results were computed using the test set if not otherwise indicated.\n\nMachine learning classifiers. For our final models, we used XGBoost  49  , an implementation of gradient boosting machines (GBMs)  50  , and the best-performing algorithm. GBMs are algorithms that build a sequence of decision trees such that every new tree improves upon the performance of previous iterations. Given that XGBoost effectively handles missing data and is not sensitive to scaling factors, no imputation or scaling techniques were applied. For comparison, we also evaluated Extended Data Fig.  1  | demographics and patient's characteristics. Summary of the retrospective cohort per gender, age group, ethnic group, marital status and primary diagnosed disorder category; including the number and percentage of patients, crisis episodes in train and test per each group category. No major differences in the distribution of crisis episodes in train and test were observed between group categories."
    },
    {
      "title": "Extended"
    },
    {
      "text": "Fig. 2 | System diagram. Time series of events are represented with the timestamps and event characteristics in different SQL tables in the hospital's database. These tables are processed and converted into features for the modeling task. Models are trained, tuned and selected based on the data for the period 2012-2019. The system predicts the risk of crisis onset within the next 28 days (whereby the algorithm is queried every week for every patient). The patients with the highest predicted risk are displayed on the dashboard delivered to clinicians alongside key indicators, patient notes and a questionnaire form about each patient, which the clinician fills out. The icons in this figure were made by Freepik from www.flaticon.com. IP, inpatient."
    },
    {
      "text": "Fig. 3 | Final model performance. a, ROC curve for the crisis prediction task. Comparison among the proposed final model (XGBoost general), a proposed diagnosis-specific model (XGBoost per diagnosis) and two baseline models.The solid lines and lighter-colored envelopes around each line were derived from the test evaluations (n = 25) as the mean and 95% CI, respectively. b, Precision-recall curve for the crisis prediction task with the same characteristics as a. c, Box plot of the AUROC curve evaluated per diagnosis. Comparison among the four models considered as in a and b. The solid line corresponds to the median value; the box limits correspond to the first Q1 (left limit) and third Q3 (right limit) quartiles; the whiskers denote the rest of the distribution range from Q1-1.5 (Q3-Q1) (left whisker) to Q3 + 1.5 (Q3-Q1) (right whisker); and the points displayed correspond to the outliers. d-f, AUROC curve evaluated for different subsets of the study cohort based on age group (d), time since the patient first visited the hospital (e) and time since the patient's last crisis episode (f). The dots and bars derive from the test evaluations (n = 25) as the mean and 95% CI, respectively."
    },
    {
      "text": "Fig. 4 | Most predictive features. a, Complete distribution of the SHAP values for the top 20 features based on the highest mean absolute SHAP value. each sample of the test set is represented as a data point per feature, and the x axis shows the positive or negative effect on the model's prediction of the feature. The color coding depicts the value of the feature and is scaled independently based on the range observed in the data. b, Absolute feature contribution of the 20 features with the highest mean absolute SHAP value. c-h, Six examples of dependence plots showing the effect on the PRS with respect to the feature value. each data point (n = 371,010) represents a sample in the test set, with the solid lines and the lighter-colored envelopes representing, respectively, the mean effect and its standard deviation per feature value. The variability at each feature value corresponds to interaction with the rest of the features. Missing values (representing the absence of events) are colored gray. GP, general practitioner; MHA, mental health act; std, standard deviation."
    },
    {
      "text": "Training data started in the first week of September 2012 and ended in the last week of December 2017. \u2022 Validation data started in the first week of January 2018 and ended in the last week of June 2018. \u2022 Test data started in the first week of July 2018 and ended in the third week of November 2018."
    },
    {
      "text": "Prevalence of the target variable (a crisis episode within the next 28 days) per disorder type. Prevalence of the target variable for different disorders. Adult personality and behaviour (F6 in ICD-10 categorisation) and Psychoactive substance use (F1 in ICD-10 categorisation) show a slightly greater prevalence of crisis episodes, whereas the prevalence was lower for Organic including symptomatic mental disorders (F0 in ICD-10 categorisation) and Not diagnosed patients. A small difference was observed between train and test, with a lower prevalence in the test set overall.Extended Data Fig.Statistical significance analysis comparing theAuROc of XGBoost to the other models. Statistical significance analysis was done using the Mann-Whitney U test. The two-stage step-up method of Benjamini, Krieger and Yekutieli was used to correct the p-values of the multiple tests performed. Model calibration and net benefit. (a), (b) Calibration curves of the general model (XGBoost general) and diagnosis specific models (XGBoost per diagnosis). Yellow and blue lines represent the non calibrated and calibrated curves for both models, respectively. The diagonal dotted line shows the ideal calibration reference curve. (c) Decision curve shows the net benefit versus the threshold probability, for the proposed models and baselines. The general model (XGBoost general) outperforms the baselines and the diagnosis specific model (XGBoost per diagnosis) at all thresholds. The solid lines and lighter-coloured envelopes around each line were derived from the test evaluations (n = 25) as the mean and 95% confidence interval respectively. Extended Data Fig. 8 | Precision and recall per cohort. Precision evaluated with respect to (a) different age groups; (b) time since the first hospital visit; (c) time since the last hospital visit; with a threshold corresponding to 15% of false positive rate. Recall evaluated with respect to (d) different age groups; (e) time since the first hospital visit; (f) time since the last visit; with a threshold corresponding to 15% of false positive rate obtained with evaluations in the test set (n=25). The dots and bars were derived from the test evaluations (n = 25) as the mean and 95% confidence interval respectively."
    }
  ],
  "references": [
    {
      "title": "Global, regional, and national incidence, prevalence, and years lived with disability for 354 diseases and injuries for 195 countries and territories, 1990-2017: a systematic analysis for the Global Burden of Disease Study 2017",
      "authors": [
        "S James"
      ],
      "year": 2018
    },
    {
      "title": "Challenges and opportunities in global mental health: a research-to-practice perspective",
      "authors": [
        "M Wainberg"
      ],
      "year": 2017,
      "doi": "10.1007/s11920-017-0780-z"
    },
    {
      "title": "The consequences of the COVID-19 pandemic on mental health and implications for clinical practice",
      "authors": [
        "A Fiorillo",
        "P Gorwood"
      ],
      "year": 2020
    },
    {
      "title": "Psychological interventions for people affected by the COVID-19 epidemic",
      "authors": [
        "L Duan",
        "G Zhu"
      ],
      "year": 2020
    },
    {
      "title": "Mental health and the Covid-19 pandemic",
      "authors": [
        "B Pfefferbaum",
        "C North"
      ],
      "year": 2020
    },
    {
      "title": "Navigating a Mental Health Crisis: A NAMI Resource Guide for Those Experiencing a Mental Health Emergency",
      "year": 2018
    },
    {
      "title": "Improving outcomes for people in mental health crisis: a rapid synthesis of the evidence for available models of care",
      "authors": [
        "F Paton"
      ],
      "year": 2016,
      "doi": "10.3310/hta20030"
    },
    {
      "title": "A role for occupational therapy in crisis intervention and prevention",
      "authors": [
        "V Miller",
        "S Robertson"
      ],
      "year": 1991
    },
    {
      "title": "Creating a learning health system through rapid-cycle, randomized testing",
      "authors": [
        "L Horwitz",
        "M Kuznetsova",
        "S Jones"
      ],
      "year": 2019
    },
    {
      "title": "Risk prediction using natural language processing of electronic mental health records in an inpatient forensic psychiatry setting",
      "authors": [
        "D Van Le",
        "J Montgomery",
        "K Kirkby",
        "J Scanlan"
      ],
      "year": 2018
    },
    {
      "title": "Prediction of incident hypertension within the next year: prospective study using statewide electronic health records and machine learning",
      "authors": [
        "C Ye"
      ],
      "year": 2018
    },
    {
      "title": "Deep learning algorithm predicts diabetic retinopathy progression in individual patients",
      "authors": [
        "F Arcadu"
      ],
      "year": 2019
    },
    {
      "title": "Early prediction of circulatory failure in the intensive care unit using machine learning",
      "authors": [
        "S Hyland"
      ],
      "year": 2020,
      "doi": "10.1038/s41591-020-0789-4"
    },
    {
      "title": "A time-phased machine learning model for real-time prediction of sepsis in critical care",
      "authors": [
        "X Li"
      ],
      "year": 2020,
      "doi": "10.1097/ccm.0000000000004494"
    },
    {
      "title": "Early sepsis prediction using ensemble learning with deep features and artificial features extracted from clinical electronic health records",
      "authors": [
        "Z He"
      ],
      "year": 2020
    },
    {
      "title": "Predicting readmission at early hospitalization using electronic health data: a customized model development",
      "authors": [
        "H Lin",
        "I.-H Tan",
        "I Lee",
        "P Wu",
        "H Chong"
      ],
      "year": 2017,
      "doi": "10.5334/ijic.3826/"
    },
    {
      "title": "Scalable and accurate deep learning for electronic health records",
      "authors": [
        "A Rajkomar"
      ],
      "year": 2018
    },
    {
      "title": "Predicting risk of suicide attempts over time through machine learning",
      "authors": [
        "C Walsh",
        "J Ribeiro",
        "J Franklin"
      ],
      "year": 2017,
      "doi": "10.1177/2167702617691560"
    },
    {
      "title": "Predicting suicide attempts and suicide deaths following outpatient visits using electronic health records",
      "authors": [
        "G Simon"
      ],
      "year": 2018
    },
    {
      "title": "Predicting suicidal behavior from longitudinal electronic health records",
      "authors": [
        "Y Barak-Corren"
      ],
      "year": 2017
    },
    {
      "title": "Predicting suicide attempt or suicide death following a visit to psychiatric specialty care: a machine learning study using Swedish national registry data",
      "authors": [
        "Q Chen"
      ],
      "year": 2020
    },
    {
      "title": "Predicting suicides after psychiatric hospitalization in US Army soldiers: the Army Study To Assess Risk and Resilience in Servicemembers (Army STARRS)",
      "authors": [
        "R Kessler"
      ],
      "year": 2015
    },
    {
      "title": "Predicting the risk of suicide by analyzing the text of clinical notes",
      "authors": [
        "C Poulin"
      ],
      "year": 2014,
      "doi": "10.1371/journal.pone.0085733"
    },
    {
      "title": "Machine learning for suicide risk prediction in children and adolescents with electronic health records",
      "authors": [
        "C Su"
      ],
      "year": 2020,
      "doi": "10.1038/s41398-020-01100-0"
    },
    {
      "title": "Identifying suicide ideation and suicidal attempts in a psychiatric clinical research database using natural language processing",
      "authors": [
        "A Fernandes"
      ],
      "year": 2018,
      "doi": "10.1038/s41598-018-25773-2"
    },
    {
      "title": "Emergency department recognition of mental disorders and short-term outcome of deliberate self-harm",
      "authors": [
        "M Olfson",
        "S Marcus",
        "J Bridge"
      ],
      "year": 2013,
      "doi": "10.1176/appi.ajp.2013.12121506"
    },
    {
      "title": "Dynamic Electronic Health Record Detection (DETECT) of individuals at risk of a first episode of psychosis: a case-control development and validation study",
      "authors": [
        "L Raket"
      ],
      "year": 2020,
      "doi": "10.1016/s2589-7500(20)30024-8"
    },
    {
      "title": "A data science approach to predicting patient aggressive events in a psychiatric hospital",
      "authors": [
        "R Suchting",
        "C Green",
        "S Glazier",
        "S Lane"
      ],
      "year": 2018,
      "doi": "10.1016/j.psychres.2018.07.004"
    },
    {
      "title": "A solution-focused research approach to achieve an implementable revolution in digital mental health",
      "authors": [
        "D Mohr",
        "H Riper",
        "S Schueller"
      ],
      "year": 2018
    },
    {
      "title": "Lessons learned from service design of a trial of a digital mental health service: informing implementation in primary care clinics",
      "authors": [
        "A Graham"
      ],
      "year": 2020
    },
    {
      "title": "A decade of ubiquitous computing research in mental health",
      "authors": [
        "J Bardram",
        "A Matic"
      ],
      "year": 2020,
      "doi": "10.1109/mprv.2019.2925338"
    },
    {
      "title": "Implementing precision psychiatry: a systematic review of individualized prediction models for clinical practice",
      "authors": [
        "G Salazar De Pablo"
      ],
      "year": 2021
    },
    {
      "title": "Deep patient: an unsupervised representation to predict the future of patients from the electronic health records",
      "authors": [
        "R Miotto",
        "L Li",
        "B Kidd",
        "J Dudley"
      ],
      "year": 2016
    },
    {
      "title": "ICD-10: International Statistical Classification of Diseases and Related Health Problems",
      "year": 2004,
      "doi": "10.1177/183335839702700110"
    },
    {
      "title": "Tree Boosting with XGBoost: Why Does XGBoost Win 'Every' Machine Learning Competition?",
      "authors": [
        "D Nielsen"
      ],
      "year": 2016,
      "doi": "10.7717/peerj-cs.586/table-2"
    },
    {
      "title": "Area under the precision-recall curve: point estimates and confidence intervals",
      "authors": [
        "K Boyd",
        "K Eng",
        "C Page"
      ],
      "year": 2013,
      "doi": "10.1007/978-3-642-40994-3_29"
    },
    {
      "title": "The precision-recall curve overcame the optimism of the receiver operating characteristic curve in rare diseases",
      "authors": [
        "B Ozenne",
        "F Subtil",
        "D Maucort-Boulch"
      ],
      "year": 2015
    },
    {
      "title": "Transforming classifier scores into accurate multiclass probability estimates",
      "authors": [
        "B Zadrozny",
        "C Elkan"
      ],
      "year": 2002,
      "doi": "10.1145/775107.775151"
    },
    {
      "title": "Assessing the performance of prediction models a framework for traditional and novel measures",
      "authors": [
        "E Steyerberg"
      ],
      "year": 2010,
      "doi": "10.1097/ede.0b013e3181c30fb2"
    },
    {
      "title": "Verification of forecasts expressed in terms of probability",
      "authors": [
        "G Brier"
      ],
      "year": 1950
    },
    {
      "title": "Decision curve analysis: a novel method for evaluating prediction models",
      "authors": [
        "A Vickers",
        "E Elkin"
      ],
      "year": 2006,
      "doi": "10.1177/0272989x06295361"
    },
    {
      "title": "A unified approach to interpreting model predictions",
      "authors": [
        "S Lundberg",
        "S.-I Lee"
      ],
      "year": 2017
    },
    {
      "title": "Predicting all-cause risk of 30-day hospital readmission using artificial neural networks",
      "authors": [
        "M Jamei",
        "A Nisnevich",
        "E Wetchler",
        "S Sudat",
        "E Liu"
      ],
      "year": 2017
    },
    {
      "title": "Readmission prediction using deep learning on electronic health records",
      "authors": [
        "A Ashfaq",
        "A Sant' Anna",
        "M Lingman",
        "S Nowaczyk"
      ],
      "year": 2019,
      "doi": "10.1016/j.jbi.2019.103256"
    },
    {
      "title": "Analysis and prediction of unplanned intensive care unit readmission using recurrent neural networks with long short-term memory",
      "authors": [
        "Y.-W Lin",
        "Y Zhou",
        "F Faghri",
        "M Shaw",
        "R Campbell"
      ],
      "year": 2019
    },
    {
      "title": "Assessment of machine learning vs standard prediction rules for predicting hospital readmissions",
      "authors": [
        "D Morgan"
      ],
      "year": 2019,
      "doi": "10.1001/jamanetworkopen.2019.0348"
    },
    {
      "title": "Proportion of hospital readmissions deemed avoidable: a systematic review",
      "authors": [
        "C Van Walraven",
        "C Bennett",
        "A Jennings",
        "P Austin",
        "A Forster"
      ],
      "year": 2011
    },
    {
      "title": "Implementation strategies for digital mental health interventions in health care settings",
      "authors": [
        "A Graham"
      ],
      "year": 2020,
      "doi": "10.1037/amp0000686"
    },
    {
      "title": "XGBoost: a scalable tree boosting system",
      "authors": [
        "T Chen",
        "C Guestrin"
      ],
      "year": 2016,
      "doi": "10.1145/2939672.2939785"
    },
    {
      "title": "Greedy function approximation: a gradient boosting machine",
      "authors": [
        "J Friedman"
      ],
      "year": 2001,
      "doi": "10.1214/aos/1013203451"
    },
    {
      "title": "Deep learning in mental health outcome research: a scoping review",
      "authors": [
        "C Su",
        "Z Xu",
        "J Pathak",
        "F Wang"
      ],
      "year": 2020,
      "doi": "10.1038/s41398-020-0780-3"
    },
    {
      "title": "Making a science of model search: hyperparameter optimization in hundreds of dimensions for vision architectures",
      "authors": [
        "J Bergstra",
        "D Yamins",
        "D Cox"
      ],
      "year": 2013,
      "doi": "10.1088/1749-4699/8/1/014008"
    },
    {
      "title": "Algorithms for hyper-parameter optimization",
      "authors": [
        "J Bergstra",
        "R Bardenet",
        "Y Bengio",
        "B K\u00e9gl"
      ],
      "year": 2011
    },
    {
      "title": "Explainable AI for trees: from local explanations to global understanding",
      "authors": [
        "S Lundberg"
      ],
      "year": 2020,
      "doi": "10.1038/s42256-019-0138-9"
    },
    {
      "title": "Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach",
      "authors": [
        "E Delong",
        "D Delong",
        "D Clarke-Pearson"
      ],
      "year": 1988,
      "doi": "10.2307/2531595"
    },
    {
      "title": "Adaptive linear step-up procedures that control the false discovery rate",
      "authors": [
        "Y Benjamini",
        "A Krieger",
        "D Yekutieli"
      ],
      "year": 2006,
      "doi": "10.1093/biomet/93.3.491"
    }
  ],
  "num_references": 56
}
