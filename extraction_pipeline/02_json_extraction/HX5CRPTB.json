{
  "paper_id": "HX5CRPTB",
  "title": "The False Choice Between Theory-Based Evaluation and Experimentation",
  "abstract": "This chapter highlights the continued salience of Weiss' s questions about theory-based evaluation, especially given the often simplistic uses of program theory in evaluation.",
  "year": 2000,
  "date": "2000",
  "journal": "Critical Public Health",
  "publication": "Critical Public Health",
  "doi": "10.1002/ev.225",
  "sections": [
    {
      "text": "Theory-based evaluation has developed significantly since Carol Weiss's chapter was published ten years ago. In 1997 Weiss pointed to theory-based evaluation being mostly used in the areas of health promotion and risk prevention. The use of program theory is now commonplace and Weiss's chapter has been cited in a wide range of program areas, including evaluations of energy conservation  (New York State Energy Research and Development Authority, 2006) , comprehensive community-based initiatives  (Judge and Bauld, 2001) , supported housing  (Rog and Randolph, 2002) , gaming and simulation  (Kriz and Hense, 2006) , and anticorruption activities  (Marra, 2000) .\n\nAlong with this proliferation of activity has come a proliferation of terminology. Weiss referred to the use of the different terms program theory and logic models to refer to essentially similar concepts. Now there is an ever-longer list of labels that have been used, not with consistently distinct definitions, including theory-based, theory-driven, theory-oriented, theoryanchored, theory-of-change, intervention theory, outcomes hierarchies, program theory, and program logic. Despite the use of the term program, the method has been used for planning and evaluating interventions ranging from small projects to multisite projects, multiyear strategies and even whole-of-government processes  (for example, Public Service Commission, South Africa, 2003) .\n\nOne of the biggest changes in the use of program theory since 1997 has been its increasing incorporation in program management processes. This phenomenon had occurred earlier in some places and program areas-for 63 5 65 THEORY-BASED EVALUATION: REFLECTIONS TEN YEARS ON\n\nThe second issue raised in Weiss' s chapter is the quality of the program theory. Weiss found that many of the program theories were based only on practitioners' assumptions and logical reasoning and hence were \"simplistic, partial, or even downright wrong.\" Although it can be useful to articulate practitioners' assumptions about how a program is intended to work, this is often not an adequate program theory for planning and evaluating the program. Unfortunately, many examples of program theory evaluation are still based on poor theories-for example, health promotion programs based solely on the discredited theory that improved knowledge will change attitudes and hence behavior. But how realistic is it to expect an evaluation to include the development of a better program theory or a full-fledged research theory? This is the issue behind  Stufflebeam's (2001)  trenchant criticism: \"There really is not much to recommend theory-based evaluation, since doing it right is usually not feasible and since failed or misrepresented attempts can be counterproductive\" (p. 80).\n\nExamples of good practice in program theory evaluation demonstrate that it can be feasible and useful to improve the quality of the theory by better logical analysis of alternative causal explanations, better use of existing research theories, and better use of alternative perspectives on how programs work, including understanding how program clients or intended beneficiaries understand it, and through a process of competitive elaboration and testing against the data. For example, the Centre for Communication Programs at Johns Hopkins University has moved on from using a \"Knowledge-Attitudes-Practice\" model of behavior change to underpin its evaluations, and now uses the frameworks of \"Ideation and Communication for Participatory Development.\" Murray-Johnson and others (2000-01)  compared the utility of four different theories: health belief model, theory of reasoned action, extended parallel process model, and social cognitive theory.\n\nThe third issue is how program theory is used in evaluations. In 1997 Weiss observed that many evaluators developed program theory but then did not use it at all to guide the evaluation. This seems less of a problem now, in part because of the widespread practice of developing performance measurement or operationalizing variables on the basis of the program theory. However, the ways program theory are used to guide evaluation are often simplistic. In many cases the evaluation consists only of gathering evidence about each of the components in the logic model, and answering the question \"Did this happen?\" about each one. Although this can be useful in reporting on the program or policy in terms of a coherent performance story, it does not use the full potential of program theory evaluation, including its ability to address the issue of causal attribution.\n\nThere are three possible responses to the challenge of causal attribution. One is to give up the attempt to use program theory evaluation for this purpose, deciding to use it only \"to improve, not to prove.\" Another option is to combine program theory with other methods for causal\n\nNEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002/ev 66 ENDURING ISSUES IN EVALUATION attribution-for example,  Cook (2000)  discussed \"the false dichotomy\" between experimental designs and program theory, and how program theory could be used to design better experiments. Alternatively, a Popperian approach can be taken, and program theory can be used to develop \"testable hypotheses,\" which are then investigated using nonexperimental methods  (Pawson and Tilley, 1997; Tilley, 2000) . Using the variations among different levels of implementation and different contexts for implementation not as \"noise\" to be screened out but rather as opportunities to test hypotheses, one can build a stronger case that the program not only contributes to the observed outcomes but also to explaining how. In an international climate of increasing focus on rigorous methods for impact evaluation, this may be the aspect of program theory evaluation most deserving of further development."
    },
    {
      "title": "Theory-based evaluation examines conditions of program implementation and mechanisms that mediate between processes and outcomes as a means to understand when and how programs work.",
      "text": "Theory-Based Evaluation: Past, Present, and Future"
    },
    {
      "title": "Carol H. Weiss",
      "text": "Theory-based evaluation has surged to attention in recent years. Evaluators are writing about it, and evaluations structured around theory are beginning to appear in numbers in the literature."
    },
    {
      "title": "The Past",
      "text": "The concept of theory-based evaluation has been around for over twentyfive years. In the spring-summer 1996 issue Evaluation Practice published two early papers-an excerpt from my 1972 book Evaluation Research and Fitz-Gibbon and Morris (1975)-along with a historical introduction by Blaine  Worthen (1996) . I have been trying to go back further. In his 1967 book Evaluative Research, Edward Suchman referred several times to the notion of programs' theories. Suchman discussed two kinds of reasons for an unsuccessful program: failure of the program to put the intended activities into operation (implementation failure) and failure of the activities to bring about the desired effects (theory failure). My 1972 book offered the first discussion Worthen and I have found of the central idea of basing evaluation on the program' s theory. I included a diagram of several alternative theories on which a program of teachers' home visiting might be based. See Figure  5 .1. I called the subject a \"process model,\" and I urged that the evaluator collect data on the posited links.\n\nIn succeeding years there were a few papers on the subject. Joe Wholey' s work on evaluability assessment stressed the need to find out whether the implicit theory underlying a program made sense  (Wholey, 1979 (Wholey, , 1983 ). Wholey's idea was that prior to the start of a formal study, the evaluator should analyze the logical reasoning that connected program inputs to 69 THEORY-BASED EVALUATION: PAST, PRESENT, AND FUTURE desired outcomes to see whether there was a reasonable likelihood that goals could be achieved.\n\nHuey-Tsyh Chen and Peter Rossi discussed the idea in a series of publications  (Chen and Rossi, 1980 , 1983 , 1987; Chen, 1990 Chen, , 1994)) . Their addition to the discussion included the idea that the theory should be a social science theory, not just a series of ad hoc logical premises. Chen (1990)  also distinguished between normative theory and causal theory. Normative theory \"provides guidance on what goals and outcomes should be pursued or examined\" (p. 43), whereas causal theory was the set of assumptions about how the program works. Causal theory is what most of the previous authors and most of the subsequent ones have talked about. By the late 1980s, program-based evaluation was becoming a popular idea. Although not many examples of theory-based evaluation were yet published, the ideas were becoming increasingly visible. Leonard Bickman edited two issues of New Directions for Program Evaluation (1987 and 1990) NEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002/ev Visits by teachers to pupils' homes Sharing of views by parent and teacher Teachers' understanding of the home culture Teachers' sympathy with children and their view of the world Teaching in terms comfortable and understandable to pupils Conscientiousness of work by pupils Pupil attendance Child' s receipt of special help Pupil morale Improvement of (health, emotional) condition Achievement in reading Parental support and encouragement with child' s homework and school assignments Parental support for better attendance at school Referral to sources of help in school or outside school Parents' knowledge of schools' expectations for pupils Identification of special problems that retard child' s achievement (health, emotional, and so on) Source: Weiss, 1972, p. 50.\n\nthat elaborated and advocated the strategy, and Lee Sechrest and A. G. Scott edited one in 1993. Lipsey wrote several articles, one explicating four different versions of program theory. Several articles addressed the subject of how to analyze data that followed the underlying assumptions of a program through time  ( Judd and Kenny, 1981; Smith, 1990; Marquart, 1990; Trochim, 1985) . Dozens of papers appeared.\n\nAt the same time, other writers were writing about logic models. Logic models seem to be similar to program theories; at least they are if the word theory does not overwhelm us. If we take the word theory to mean the professional logic that underlies a program, then the two concepts appear to be much the same.\n\nI wrote a paper on theory-based evaluation in 1995, published in what I thought would be an obscure book, that has received considerable attention  (Weiss, 1995) . The idea of basing evaluation on programs' theories of change in community-based programs received a warm welcome among evaluators and sponsors of these kinds of programs. One reason seems to be that it promised (or at least hinted at a promise) that theory-based evaluation could strengthen the validity of evaluations when random assignment is impossible, as it is in place-based programming. If the evaluation can show the series of micro-steps that lead from inputs to outcomes, then causal attribution for all practical purposes seems to be within reach. Although such an evaluation cannot rule out all the threats to validity we have come to know and love, it has the advantage (if things go well) of showing what processes lead to the outcomes observed; if some of the posited steps are not borne out by the data, then the study can show where the expected sequence of steps breaks down."
    },
    {
      "title": "The Present",
      "text": "If the past was a period when people developed and elaborated the idea of theory-based evaluation, the present is a time when evaluators are putting the ideas into practice. A graduate student, Jo Birckmayer, and I have done a search for theory-based evaluations in the periodical literature. Mark Lipsey sent me abstracts of studies that his staff coded as \"integrated theory\" in the evaluation data base he collected some years ago. We are still collecting cases, and I will welcome your papers and articles. We have now inspected about thirty studies that have at least a modicum of theory orientation. This preliminary inspection has given rise to some tentative ideas.\n\nFirst, quite a number of articles claim that the programs are theorybased, and depending on how forgiving our definition of theory is, many of them do seem to have a set of coherent ideas providing the basis for intervention. But many of the evaluations do not follow through on that theory; they do not collect data on crucial theoretical constructs. For example,  Gottfredson (1987)  reports the evaluation of a program to reduce school disorder that is reportedly based on organization development principles. NEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002/ev 1534875x, 2007, 114, Downloaded from  https://onlinelibrary.wiley.com/doi/10.1002/ev.225 by University Of Iowa Libraries -Serials Acquisitions, Wiley Online Library on [14/08/2025]. See the Terms and Conditions ( https://onlinelibrary.wiley.com/terms-and-conditions ) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 71 THEORY-BASED EVALUATION: PAST, PRESENT, AND FUTURE\n\nHowever, the evaluation as published does not look at the organizational development aspects. Similarly,  Campbell and Ramey (1995)  evaluate an early childhood intervention program that is based on theories about enhancement of early cognitive development and the ensuing development of academic confidence, motivation, and success. The evaluation does not track the steps of the program theory.\n\nBut a number of evaluations trace the emergence of the stages posited in theory. A few of the theories are simple, what  Lipsey and Pollard (1989)  would probably call \"one-step\" theories. Sheard, Marini, Bridges, and Wagner (1976)  report on lithium given to aggressive prisoners in a medium-security institution. The evaluation examined the number of violent infractions that they committed. The evaluation thus tested the theory that lithium would reduce violence. The mechanism in this case is the physiological effects that lithium produces in the body. (Actually, the theory is not simple. But the physiological element, the action of lithium in the body, has been investigated by bio-medical researchers and need not form part of the evaluation.)\n\nBut some are more complex theories, and the evaluations make valiant efforts to follow them along their course. Cohen and Rice (1995)  trace the effects of involving parents in prevention of adolescent substance abuse prevention. They find that parents were difficult to engage, and even when parents attended the program, they did not believe that their children's friends used drugs and so did not monitor their friendships.\n\nMuch of the work in theory-based evaluation is going on in the fields of health promotion and risk prevention. Evaluators are using theory-based approaches in programs to reduce smoking, stress, risky sexual behavior, drug abuse, adolescent pregnancy, and similar ills (for example, see  Goodman and others, 1996) . We have also located a few theory-based studies in mental health and health care (for example, see  Bickman, 1983) .\n\nAnother interesting thing is that evaluators abroad are embracing the approach. When I posted a request for examples of theory-based evaluation on the listserver EVALTALK, I received papers from  Rush and Ogborne (1991) , in Canada;  Torvatn (1995) , in Norway;  Kelly and Maloney (1992), in Scotland; and Milne (1995) , in Australia.\n\nConditions Conducive to Theory-Based Evaluation. When evaluators adopt a theory-based approach, it is often for one of two reasons. The first is that the evaluator is also the program developer. A program designer, usually an academic, is engaged in a cycle of program development to deal with a particular problem. He or she develops theory, operationalizes the theory in a set of program activities, tests the program and therefore the underlying theory through evaluation, and revises the intervention. Such a cycle has a long and honorable history in several fields of what I would call applied social psychology. Evaluation is part of the ongoing series of activities by which the intervention takes shape. When the work is not that of a single individual but part of the work of an academic center devoted to the ENDURING ISSUES IN EVALUATION design of interventions, evaluation becomes a key feature of both theory development and program modification.\n\nA good example is the work of  Sandler and others (1992)  at the Program for Prevention Research at Arizona State University who iteratively developed a family bereavement program for youngsters who had lost a parent. Based on prior research and pilot testing, the program aimed to influence four \"mediators,\" hypothesized as implicated in child symptomatology: parental demoralization, parental warmth, discussion of grief, and stable positive events. The evaluation then collected data on the extent to which the program was associated with changes in the four mediators, and went on to study the child' s psychosocial symptomatology.\n\nAnother condition that promotes a theory-based approach to evaluation is conscientious theory-based development of the program. Health promotion and risk prevention are fields where program planning is well developed: designers tend to spell out their theoretical assumptions in thoughtful detail and build programs on that foundation. Therefore it becomes easy for the evaluator to follow the tracks of theory in the evaluation. Some of the theories are relatively traditional and well established, such as social learning theory. A health promotion program provides knowledge (for example, about methods to break the smoking habit), which leads to a change in motivation and intention (willingness to try to reduce smoking), which leads to a change in practice (cessation of smoking). The change in practice is assumed to lead to the ultimate outcome, which may be reduction in cardiovascular disease. In addition, social-reinforcement theory may call for provision of social supports to encourage and sustain smoking cessation.\n\nSocial-cognitive theories of several kinds are prevalent in risk prevention. The operative mechanisms are expected to be change in knowledge, change in attitude, increase in feelings of self-efficacy, higher motivation, mastery of skills, and heightened sense of responsibility, which lead to intentions to change behavior and so on to the desired outcomes. Evaluations follow the anticipated sequence of changes over time.\n\nProgram Theory and Implementation Theory. While the subject of theory-based evaluation has been gaining adherents and attention, it has also gained confusion. As is frequently the case with an emerging idea, people have attached their own understandings to the same words. Therefore we have different varieties of recommended evaluation strategies all sailing under the flag of \"theory-based evaluation.\" Figures 5.2-5.5 are diagrams that evaluators have used to explain how their studies have been guided by theory. The diagrams differ in level of specificity, complexity, and type of pictorial display. They also incorporate two different elements of theory, what I will call implementation theory and programmatic theory.\n\nImplementation theory focuses on how the program is carried out. The theoretical assumption it tests is that if the program is conducted as planned, with sufficient quality, intensity, and fidelity to plan, the desired results will be forthcoming.\n\nThe emphasis is on what Suchman would have NEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002/ev 1534875x, 2007, 114, Downloaded from  https://onlinelibrary.wiley.com/doi/10.1002/ev.225 by University Of Iowa Libraries -Serials Acquisitions, Wiley Online Library on [14/08/2025]. See the Terms and Conditions ( https://onlinelibrary.wiley.com/terms-and-conditions ) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 73 THEORY-BASED EVALUATION: PAST, PRESENT, AND FUTURE\n\ncalled implementation failure/success. Programmatic theory, on the other hand, deals with the mechanisms that intervene between the delivery of program service and the occurrence of outcomes of interest. It focuses on participants' responses to program service. The mechanism of change is not the program activities per se but the response that the activities generate. For example, in a contraceptive counseling program, if counseling is associated with reduction in pregnancy, the cause of change might seem to be the counseling. But the mechanism is not the counseling; that is the program activity, the program process. The mechanism might be the knowledge that participants gain from the counseling. Or it might be that the existence of the counseling program helps to overcome cultural taboos against family planning; it might give women confidence and bolster their assertiveness in sexual relationships; it might trigger a shift in the power relations between men and women. These or any of several other cognitive, affective, social responses could be the mechanisms leading to desired outcomes.\n\nSimilarly, programs that aim to teach students understanding of other cultures may assume that any good results observed are due to the teaching. But teaching is not the mechanism. The mechanism is what students get from the teaching-knowledge or heightened interest, motivation, even anxiety. An evaluation that attempts to track the theoretical underpinnings of the program has to devise ways to define and measure the psychosocial, physiological, economic, sociological, organizational, or other processes that intervene between exposure to the program and participant outcomes.\n\nMuch evaluation that is purportedly theory-based actually examines outcomes in terms of implementation variables."
    },
    {
      "title": ". Theory of an Antismoking Program",
      "text": "Source:  Chen, Quane, Garland, and Marcin, 1988. 1534875x, 2007, 114, Downloaded from   https://onlinelibrary.wiley.com/doi/10.1002/ev.225 by University Of Iowa Libraries -Serials Acquisitions, Wiley Online Library on [14/08/2025]. See the Terms and Conditions ( https://onlinelibrary.wiley.com/terms-and-conditions ) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License No single influence causes alcohol and other drug (AOD) abuse. Therefore Such as Alcohol and other drug abuse is a consequence of several risk factors. People are influenced by interacting personality, family, and community influences: \u2022 Risk/protective factors \u2022 life events For instance \u2022 Personality/disposition \u2022 Coping skills \u2022 Self-esteem \u2022 Sense of control \u2022 Communication skills \u2022 Need for social support \u2022 Social responsibility \u2022 toward alcohol and other drug abuse \u2022 Achievement orientation \u2022 Assertiveness \u2022 Humor \u2022 Thinking skills \u2022 Self-discipline \u2022 Effectiveness in work, play, and love \u2022 Family environment \u2022 Structure/relationship rules \u2022 Cohesiveness/support \u2022 Communication patterns \u2022 Conflict patterns \u2022 Parental modeling \u2022 Family values/priorities \u2022 Parenting skills, quality of child care \u2022 Community environment \u2022 Presence of other supports \u2022 Community attitudes and values \u2022 Community resources \u2022 Community cohesiveness \u2022 Economic factors \u2022 Availability of alcohol and other drugs Information seeking Problem solving Positive focusing Relaxation Success/mastery Self-sufficiency \u2022 High value of social/academic achievement \u2022 Positive attitudes toward society/school/work \u2022 Positive social behaviors Which leads to If positive Increased resistance to alcohol and other drug abuse Decreased resistance to AOD \u2022 Low value of academic/social achievement \u2022 Negative attitudes toward school/society/work \u2022 Negative social behaviors Which contributes to Teachers Ministers Friends Relatives Employers Peers Neighbors Which contributes to If positive If negative Figure 5.3. Community Prevention Model for Alcohol and Other Drug Abuse Source: Goodman and Wandersman, 1994, p. 10. This is a working model. There are other factors, such as genetics.\n\nothers (1996) evaluated a program to change students' dietary knowledge and food choices so as to reduce the risk of cardiovascular disease. The program, called CATCH (Child and Adolescent Trial for Cardiovascular Health), had several components, but the part recently reported related to the classroom component. The outcomes studied were the children' s dietary knowledge, self-confidence that they could select better foods, and intentions to eat more wisely. See Figure  5 .6. These were analyzed against input variables (mainly student and teacher characteristics) and program processes, such as the extent to which teachers completed the full course of CATCH classroom activities (\"dose\") and the degree to which teachers modified the activities (\"fidelity\" to plan). The analysis led to conclusions about\n\nNEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002/ev Treatment planning: Comprehensive initial assessment Client and family participation Multidisciplinary team participation Linkage: Regular communication among client, family, providers, and case manager Assistance with arrangements for treatment Single point of contact for family and providers Monitoring: Review of treatment progress Review of restrictiveness of care Advocacy: Assistance to family in negotiating with providers Respect for child and family rights Improved mental health outcomes Quicker recovery Increased client and family satisfaction Reduced costs More appropriate treatment plan Plan that is acceptable to child and family Timely review of treatment process Accessible services Efficient use of services Case management activities Intermediate outcomes Distal outcomes Figure 5.4. Case Management Theory\n\nSource:  Bryant and Bickman, 1996, p. 123.  the extent to which program activities as defined by CATCH planners led to desired health knowledge and intended behaviors. This study makes excellent use of process measures in analyzing outcomes. But it does not provide a test of the programmatic theory of the program, at least in this paper. The theory underlying the program is described as \"modification of psychosocial factors \u2026 lead  [ing]  to changes in risk-factor behaviors\"  (McGraw and others, 1996, p. 292) . The evaluation reported does not address the modification of psychosocial factorsthat is, it does not inquire into the mechanisms by which change is brought about. Similarly,  Pentz and others (1990)  do an excellent analysis of the relation between the implementation of a drug abuse prevention program and outcomes for adolescents. The emphasis is strictly on the extent to which implementation variables (such as exposure, adherence, fidelity, and amount of implementation) were associated with outcomes.\n\nThe difference between program theory and implementation theory is analogous to the distinction between mediator and moderator variables  (Baron and Kenny, 1986) . Mediator and moderator variables are both third variables that affect the relation between an independent and a dependent\n\nNEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002/ev Program components Outputs Immediate objectives/ effects Intermediate objectives/ effects Ultimate objectives/ effects On-the-job training Training courses Training allowance On-the-job training placements Satisfy needs of industry for skilled workers Reduced unemployment levels Persons trained Payments Development of skills Increased employability Economic growth Increased earnings Permit people to take training Wage subsidy (unintended effect) Source: Auditor General of Canada, 1981, p. 14.\n\nvariable. The moderator variable is a characteristic, such as gender or frequency of exposure, the subcategories of which have different associations with the outcome variable. Girls do better than boys, or those who attend the program regularly do better than those who attend infrequently. On the other hand, a mediator variable \"represents the generative mechanism through which the focal independent variable is able to influence the dependent variable of interest\"  (Baron and Kenny, 1986, p. 1173) . That is, the moderator helps to explain which features of persons or situations have the strongest relationship to the outcome; mediators help to explain how the process works. The concepts are similar to the concepts of implementation theory and programmatic theory.\n\nIn most programs both kinds of theories will be implicated. Elsewhere I have used the term theories of change evaluation for evaluations that explore both elements."
    },
    {
      "title": "The Future",
      "text": "Theory-based evaluation is demonstrating its capacity to help readers understand how and why a program works or fails to work. Knowing only outcomes, even if we know them with irreproachable validity, does not tell us enough to inform program improvement or policy revision. Evaluation needs to get inside the black box and to do so systematically.\n\nNEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002/ev Student characteristics Student participation Student outcomes Training and support of school staff Intervention elements External and competing programs Student and staff characteristics Implementation Behavior Environment Person Figure 5.6. Program Theory Model of the Child and Adolescent Trial for Cardiovascular Health Source: McGraw and others, 1996, p. 294. 1534875x, 2007, 114, Downloaded from  https://onlinelibrary.wiley.com/doi/10.1002/ev.225 by University Of Iowa Libraries -Serials Acquisitions, Wiley Online Library on [14/08/2025]. See the Terms and Conditions ( https://onlinelibrary.wiley.com/terms-and-conditions ) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 78 ENDURING ISSUES IN EVALUATION\n\nOne of the side benefits of this kind of evaluation is its contribution to wiser program planning even before the evaluation gets under way. When evaluators are involved in the planning phase, they have the opportunity to elicit program designers' own theories about how the program is expected to work. They can help designers to disaggregate the assumptions into the mini-steps that are implied and to confront the leaps of faith and questionable reasoning that are often involved. Evaluators can also offer theories and promising hypotheses based in the social sciences and evidence from prior evaluations that show which kinds of theories hold up in practice. In all these ways, evaluators can become profitably engaged in helping to plan programs that are rooted in better conceived premises.\n\nChallenges lie ahead. One of the immediate needs is for better measures. Through repeated tests, evaluators have made great strides in developing valid measures of outcomes. But measurement of mediating variables is a relatively recent activity in most academic subfields, and evaluators will have to learn how to do it better. Given the large number of variables that are implied in many theories of change, measurement error makes it difficult to identify significant associations among variables-even when they are present. The field needs advances in measurement of mediating mechanisms.\n\nProbably the central need is for better program theories. Evaluators are currently making do with the assumptions that they are able to elicit from program planners and practitioners or with the logical reasoning that they bring to the table. Many of these theories are elementary, simplistic, partial, or even outright wrong. Evaluators need to look to the social sciences, including social psychology, economics, and organization studies, for clues to more valid formulations, and they have to become better versed in theory development themselves. Better theories are important to evaluators as the backbone for their studies. Better theories are even more essential for program designers, so that social interventions have a greater likelihood of achieving the kind of society we hope for in the twenty-first century.\n\nTheory-based evaluation can pursue two different strands in the coming years. One path is to build more detailed program theories, so that evaluations can trace micro-steps of process all along the pathways that lead to program effects. This is like the theory of the teacher home visiting program in Figure  5 .1. Each program activity and each participant response is followed along the hypothesized chain of events. There is real promise in this direction, if we iteratively test the linkages between steps and substantially improve our knowledge of how processes work. This kind of evaluation will have much to tell program funders, managers, and practitioners about what works and what does not work under a range of different conditions.\n\nThe other path that theory-based evaluation can pursue is to limit the theory to the one or two central assumptions embedded in each program. They should be premises that are significant for program success, common across a range of programs, and particularly problematic. For example, many interventions are now based on the assumption that empowering THEORY-BASED EVALUATION: PAST, PRESENT, AND FUTURE residents of low-income neighborhoods to help plan social, economic, and educational programs for their community will improve the nature of services. A central premise of these kinds of programs is that residents of the community will plan and allocate resources in ways that are more responsive to need than the professional systems of the past.\n\nOr consider programs to make major alterations in teachers' behavior through staff development programs. The assumption here is that short-term training will be able to modify teaching patterns developed over years of education and professional practice. Another example would be efforts to change such behaviors as low school grades, delinquency, and domestic violence through programs that seek to raise self-esteem and self-confidence. Theory-based evaluation could be directed at investigating the viability of such central theoretical premises.\n\nEvaluations that test such macro-theoretical assumptions will require multiple cases and will be difficult to do. The quest will probably be more appropriate for meta-analysis than for single studies  (Cook and others, 1992) . Furthermore, they will not hold much interest for funders or practitioners who are wedded to the premise being scrutinized. Professionals who run short-term staff development programs for teachers are not going to be receptive to studies that question whether staff development is a sensible approach to changing teacher behavior.\n\nStill there are audiences who want to know. Program sponsors and funders, whether foundations or government agencies, should have an intense interest in whether the strategy in which they are investing is feasible across a range of conditions. A meta-analysis of evaluations that have measured and examined the same central assumption should have important news to report. It should be able to give insight into the conditions under which these hypotheses hold and the conditions under which they result in shortfalls of varying dimensions. My long-range hope is that evaluation will not only be based on theory but also contribute to the cumulation of theoretical knowledge."
    },
    {
      "title": "Conclusion",
      "text": "Looking at conditions of program implementation that are associated with better outcomes is a real contribution to the improvement of programs. These process/outcome evaluations show which program processes yield positive benefits. Evaluations will provide even more valuable information when they address the mechanisms that mediate between processes and outcomes. Theory-based evaluations attend not only to what programs do but also to how participants respond. Such evaluations are not easy to do, but there are circumstances in which evaluators should proceed in that direction. As a starting point, we need plausible theories. We need to make the maximum use of logical reasoning, practitioner wisdom, prior evaluations, and social science research to generate program theories and then use our collective evaluation work to test them under realistic operating conditions."
    },
    {
      "text": "Figure 5.1. Theory of a Program of Teacher Home Visits"
    },
    {
      "text": "For example, McGraw and NEW DIRECTIONS FOR EVALUATION \u2022 DOI: 10.1002"
    },
    {
      "text": "Figure 5.2. Theory of an Antismoking Program"
    },
    {
      "text": "Figure 5.5. Program Theory of an Employment Training Program"
    }
  ],
  "references": [
    {
      "title": "The False Choice Between Theory-Based Evaluation and Experimentation",
      "authors": [
        "T Cook"
      ],
      "year": 2000
    },
    {
      "title": "Developments in the New South Wales Approach to Analysing Program Logic",
      "authors": [
        "S Funnell"
      ],
      "year": 1990
    },
    {
      "title": "Strong Theory, Flexible Methods: Evaluating Comprehensive Community-Based Initiatives",
      "authors": [
        "K Judge",
        "L Bauld"
      ],
      "year": 2001
    },
    {
      "title": "Theory-Oriented Evaluation for the Design of and Research in Gaming and Simulation",
      "authors": [
        "W Kriz",
        "J Hense"
      ],
      "year": 2006,
      "doi": "10.1177/1046878106287950"
    },
    {
      "title": "An unpublished approach and training materials developed for",
      "year": 1971,
      "doi": "10.1007/1-4020-4631-6_11"
    },
    {
      "title": "How Much Does Evaluation Matter? Some Examples of the Utilization of the Evaluation of the World Bank's Anti-Corruption Activities",
      "authors": [
        "M Marra"
      ],
      "year": 2000,
      "doi": "10.1177/13563890022209091"
    },
    {
      "title": "Using Health Education Theories to Explain Behaviour Change: A Cross-Country Analysis",
      "authors": [
        "L Murray-Johnson"
      ],
      "year": 2000
    },
    {
      "title": "Realistic Evaluation",
      "authors": [
        "R Pawson",
        "N Tilley"
      ],
      "year": 1997
    },
    {
      "title": "The PSC's Public Administration Monitoring and Evaluation System: First Consolidated Report",
      "year": 2003,
      "doi": "10.1596/14764"
    },
    {
      "title": "A Multisite Evaluation of Supported Housing: Lessons Learned from Cross-Site Collaboration",
      "authors": [
        "D Rog",
        "F Randolph"
      ],
      "year": 2002
    },
    {
      "title": "Evaluation Models",
      "authors": [
        "D Stufflebeam"
      ],
      "year": 2001,
      "doi": "10.1002/ev.3"
    },
    {
      "title": "Realistic Evaluation: An Overview",
      "authors": [
        "N Tilley"
      ],
      "year": 2000
    },
    {
      "title": "Theory-Based Evaluation: Past, Present and Future",
      "authors": [
        "Theory-Based Evaluation ; Reflections Ten Years On Weiss"
      ],
      "year": 1997
    },
    {
      "title": "ROGERS is associate professor in public sector evaluation and director of CIRCLE",
      "year": 1981,
      "doi": "10.1093/ww/9780199540884.013.u9544"
    },
    {
      "title": "The Moderator-Mediator Variable Distinction in Social Psychological Research: Conceptual, Strategic, and Statistical Considerations",
      "authors": [
        "R Baron",
        "D Kenny"
      ],
      "year": 1986
    },
    {
      "title": "The Evaluation of Prevention Programs",
      "authors": [
        "L Bickman"
      ],
      "year": 1983,
      "doi": "10.1111/j.1540-4560.1983.tb00135.x"
    },
    {
      "title": "Using Program Theory in Evaluation",
      "year": 1987,
      "doi": "10.1002/ev.1443"
    },
    {
      "title": "Advances in Program Theory",
      "year": 1990,
      "doi": "10.1002/ev.1555"
    },
    {
      "title": "Methodology for Evaluating Mental Health Case Management",
      "authors": [
        "D Bryant",
        "L Bickman"
      ],
      "year": 1996
    },
    {
      "title": "Cognitive and School Outcomes for High-Risk African-American Students at Middle Adolescence: Positive Effects of Early Intervention",
      "authors": [
        "F Campbell",
        "C Ramey"
      ],
      "year": 1995,
      "doi": "10.3102/00028312032004743"
    },
    {
      "title": "Theory-Driven Evaluation: A Comprehensive Perspective",
      "authors": [
        "H.-T Chen"
      ],
      "year": 1990
    },
    {
      "title": "Theory-Driven Evaluations: Needs, Difficulties, and Options",
      "authors": [
        "H.-T Chen"
      ],
      "year": 1994
    },
    {
      "title": "Evaluating an Antismoking Program: Diagnostics of Underlying Causal Mechanisms",
      "authors": [
        "H.-T Chen",
        "J Quane",
        "T Garland",
        "P Marcin"
      ],
      "year": 1988
    },
    {
      "title": "The Multi-Goal, Theory-Driven Approach to Evaluation: A Model Linking Basic and Applied Social Science",
      "authors": [
        "H.-T Chen",
        "P Rossi"
      ],
      "year": 1980,
      "doi": "10.2307/2577835"
    },
    {
      "title": "Evaluating with Sense: The Theory-Driven Approach",
      "authors": [
        "H.-T Chen",
        "P Rossi"
      ],
      "year": 1983,
      "doi": "10.1177/0193841x8300700301"
    },
    {
      "title": "The Theory-Driven Approach to Validity",
      "authors": [
        "H.-T Chen",
        "P Rossi"
      ],
      "year": 1987,
      "doi": "10.1016/0149-7189(87)90025-5"
    },
    {
      "title": "A Parent-Targeted Intervention for Adolescent Substance Use Prevention: Lessons Learned",
      "authors": [
        "D Cohen",
        "J Rice"
      ],
      "year": 1995
    },
    {
      "title": "Meta-Analysis for Explanation: A Casebook",
      "authors": [
        "T Cook",
        "H Cooper",
        "D Cordray",
        "H Hartmann",
        "L Hedges",
        "R Light",
        "T Louis",
        "F Mosteller"
      ],
      "year": 1992
    },
    {
      "title": "Theory-Based Evaluation",
      "authors": [
        "C Fitz-Gibbon",
        "L Morris"
      ],
      "year": 1975,
      "doi": "10.1016/s0886-1633(96)90024-0"
    },
    {
      "title": "FORECAST: A Formative Approach to Evaluating Community Coalitions and Community-Based Initiatives",
      "authors": [
        "R Goodman",
        "A Wandersman"
      ],
      "year": 1994,
      "doi": "10.1007/bf02511882"
    },
    {
      "title": "An Ecological Assessment of Community-Based Interventions for Prevention and Health Promotion: Approaches to Measuring Community Coalitions",
      "authors": [
        "R Goodman",
        "A Wandersman",
        "M Chinman",
        "P Imm",
        "E Morrissey"
      ],
      "year": 1996
    },
    {
      "title": "An Evaluation of an Organization Development Approach to Reducing School Disorder",
      "authors": [
        "D Gottfredson"
      ],
      "year": 1997
    },
    {
      "title": "Process Analysis: Estimating Mediation in Treatment Evaluations",
      "authors": [
        "C Judd",
        "D Kenny"
      ],
      "year": 1981
    },
    {
      "title": "A Behavioural Modelling Approach to Curriculum Development and Evaluation of Health Promotion for Nurses",
      "authors": [
        "M Kelly",
        "W Maloney"
      ],
      "year": 1992,
      "doi": "10.1111/j.1365-2648.1992.tb02830.x"
    },
    {
      "title": "Driving Toward Theory in Program Evaluation: More Models to Choose From",
      "authors": [
        "M Lipsey",
        "J Pollard"
      ],
      "year": 1989,
      "doi": "10.1016/0149-7189(89)90048-7"
    },
    {
      "authors": [
        "Present Past"
      ]
    },
    {
      "title": "A Pattern-Matching Approach to Link Program Theory and Evaluation Data",
      "authors": [
        "J Marquart"
      ],
      "year": 1990,
      "doi": "10.1002/ev.1557"
    },
    {
      "title": "Using Process Data to Explain Outcomes: An Illustration from the Child and Adolescent Trial for Cardiovascular Health (CATCH)",
      "authors": [
        "S Mcgraw",
        "D Sellers",
        "E Stone",
        "J Bebchuk",
        "E Edmundson",
        "C Johnson",
        "K Bachman",
        "R Luepker"
      ],
      "year": 1996
    },
    {
      "title": "Using Program Logic as a Practical Evaluation Tool: Case Studies from an Australian Evaluator",
      "authors": [
        "C Milne"
      ],
      "year": 1995
    },
    {
      "title": "Effects of Program Implementation on Adolescent Drug Use: The Midwestern Prevention Project (MPP)",
      "authors": [
        "M Pentz",
        "E A Trebow",
        "W Hansen",
        "D Mackinnon",
        "J Dwyer",
        "C Johnson",
        "B Flay",
        "S Daniels",
        "C Cormack"
      ],
      "year": 1990,
      "doi": "10.1177/0193841x9001400303"
    },
    {
      "title": "Back from the Future: Can Evaluation Survive Dissension in the Ranks",
      "authors": [
        "J Quane"
      ],
      "year": 1994
    },
    {
      "title": "Program Logic Models: Expanding Their Role and Structure for Program Planning and Evaluation",
      "authors": [
        "B Rush",
        "A Ogborne"
      ],
      "year": 1991
    },
    {
      "title": "Linking Empirically Based Theory and Evaluation: The Family Bereavement Program",
      "authors": [
        "I Sandler",
        "S West",
        "L Baca",
        "D Pillow",
        "J Gersten",
        "F Rogosch",
        "L Virdin",
        "J Beak",
        "K Reynolds",
        "C Kallgren",
        "J-Y Tein",
        "G Kriege",
        "E Cole",
        "R Ramirez"
      ],
      "year": 1992
    },
    {
      "title": "Understanding Causes and Generalizing About Them",
      "year": 1993,
      "doi": "10.1002/ev.1636"
    },
    {
      "title": "The Effect of Lithium on Impulsive Aggressive Behavior in Man",
      "authors": [
        "M Sheard",
        "J Marini",
        "C Bridges",
        "E Wagner"
      ],
      "year": 1976
    },
    {
      "title": "Using Path Analysis to Develop and Evaluate Program Theory",
      "authors": [
        "N Smith"
      ],
      "year": 1990,
      "doi": "10.1002/ev.1554"
    },
    {
      "authors": [
        "E Suchman",
        "Research"
      ],
      "year": 1967
    },
    {
      "title": "Chains of Reasoning: An Evaluation Tool",
      "authors": [
        "H Torvatn"
      ],
      "year": 1995
    },
    {
      "title": "Pattern Matching, Validity, and Conceptualization in Program Evaluation",
      "authors": [
        "W Trochim"
      ],
      "year": 1985,
      "doi": "10.1177/0193841x8500900503"
    },
    {
      "title": "Evaluation Research: Methods for Assessing Program Effectiveness",
      "authors": [
        "C Weiss"
      ],
      "year": 1972
    },
    {
      "title": "Nothing as Practical as Good Theory",
      "authors": [
        "C Weiss"
      ],
      "year": 1995
    },
    {
      "title": "Evaluation: Promise and Performance",
      "authors": [
        "J Wholey"
      ],
      "year": 1979
    },
    {
      "title": "Evaluation and Effective Public Management",
      "authors": [
        "J Wholey"
      ],
      "year": 1983
    },
    {
      "title": "Editor's Note: The Origins of Theory-Based Evaluation",
      "authors": [
        "B Worthen"
      ],
      "year": 1996
    }
  ],
  "num_references": 54
}
