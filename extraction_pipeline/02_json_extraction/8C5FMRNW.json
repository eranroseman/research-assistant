{
  "paper_id": "8C5FMRNW",
  "title": "Can AI Help Reduce Disparities in General Medical and Mental Health Care?",
  "abstract": "Background: As machine learning becomes increasingly common in health care applications, concerns have been raised about bias in these systems' data, algorithms, and recommendations. Simply put, as health care improves for some, it might not improve for all. Methods: Two case studies are examined using a machine learning algorithm on unstructured clinical and psychiatric notes to predict intensive care unit (ICU) mortality and 30-day psychiatric readmission with respect to race, gender, and insurance payer type as a proxy for socioeconomic status. Results: Clinical note topics and psychiatric note topics were heterogenous with respect to race, gender, and insurance payer type, which reflects known clinical findings. Differences in prediction accuracy and therefore machine bias are shown with respect to gender and insurance type for ICU mortality and with respect to insurance policy for psychiatric 30-day readmission. Conclusions: This analysis can provide a framework for assessing and identifying disparate impacts of artificial intelligence in health care. \n Bias in Machine Learning Models While health care is an inherently data-driven field, most clinicians operate with limited evidence guiding their decisions. Randomized trials estimate average treatment effects for a trial population, but participants in clinical trials often aren't representative of the patient population that ultimately receives the treatment with respect to race and gender.  1, 2 As a result, drugs and interventions are not tailored to historically mistreated groups; for example, women, minority groups, and obese patients tend to have generally poorer treatment options and longitudinal health outcomes.  [3] [4] [5] [6] [7] [8] [9]  Advances in artificial intelligence (AI) and machine learning offer the potential to provide personalized care by taking into account granular patient differences. Machine learning using images, clinical notes, and other electronic health record (EHR) data has been",
  "year": 2015,
  "date": "2015",
  "journal": "PLoS Med",
  "publication": "PLoS Med",
  "authors": [
    {
      "forename": "Irene",
      "surname": "Chen",
      "name": "Irene Chen"
    },
    {
      "forename": "Peter",
      "surname": "Szolovits",
      "name": "Peter Szolovits"
    },
    {
      "forename": "Marzyeh",
      "surname": "Ghassemi",
      "name": "Marzyeh Ghassemi"
    }
  ],
  "doi": "https://doi.org/10.13039/100000025",
  "sections": [
    {
      "text": "successful in several clinical tasks such as detection of diabetic retinopathy  10  and distinguishing between malignant and nonmalignant skin lesions in dermatoscopic images.  11 Prior research has established that machine learning using clinical notes to supplement lab tests and other structured data is more accurate than an algorithm using structured data alone in classifying patients with rheumatoid arthritis  12  and in predicting mortality  13  and the onset of critical care interventions  14  in intensive care settings. This same ability to discern among patients brings with it the risk of amplifying existing biases, which can be especially concerning in sensitive areas like health care.  15, 16 Because machine learning models are powered by data, bias can be encoded by modeling choices or even within the data itself.  17 Ideally, algorithms would have access to exhaustive sources of population EHR data to create representative models for diagnosing diseases, predicting adverse effects, and recommending ongoing treatments.  18 However, such comprehensive data sources are not often available, and recent work has demonstrated bias in critical care interventions. For example, recent Canadian immigrants are more likely to receive aggressive care in the ICU than other Canadian residents.  19  contrast to critical care, psychiatry relies more heavily on analysis of clinical notes for patient assessment and treatment. Text is a rich source of unstructured information for machine learning models, but the subjective and expressive nature of the data also makes text a strong potential source of bias.  20, 21 Racism has established impacts on chronic and acute health,  22  which would affect EHR data. In addition, mental health problems of racial groups often depend heavily on the larger social context in which the group is embedded,  22  which would also influence clinical prediction based on EHR data.\n\nIn prior work, the first author and colleagues formalized a framework for decomposing sources of unfairness in prediction tasks, including an analysis of racial bias for prediction of hospital mortality from clinical notes.  23 In contrast to human bias, algorithmic bias occurs when an AI model, trained on a given data set, produces results that may be completely unintended by the model creators. The authors used the publicly available Medical Information Mart for Intensive Care (MIMIC-III) v1.4,  24 which contains deidentified electronic health record data from 53 423 intensive care unit (ICU) admissions for 38 597 adult patients from Beth Israel Deaconess Medical Center from 2001 to 2012. After restricting the data set to ICU admissions lasting over 48 hours and excluding discharge summaries, the researchers created a final data set of 25 879 patient stay notes and demonstrated that prediction errors for patient mortality differ between races.  23  this paper, we explore the potential impacts of bias in 2 algorithms, one for predicting patient mortality in an ICU and the other for predicting 30-day psychiatric readmission in an inpatient psychiatric unit. We expand on the first author's previous research, discussed above, on bias in ICU patient mortality prediction using the same MIMIC-III data set cohort with gender and insurance type in addition to race as demographic groups. We also analyzed potential bias in 30-day psychiatric readmission prediction for the same demographic groups.\n\nBecause unstructured clinical notes from the EHR contain valuable information for prediction tasks-including information about the patient's race, gender, and insurance type-we focus on clinical narrative notes in EHR data available for each stay. We examine bias, as measured by differences in model error rates in patient outcomes between groups, and show that in the ICU data set, differences in error rates in mortality for gender and insurance type are statistically significant and that in the psychiatric data set, only the difference in error rates in 30-day readmission for insurance type is statistically significant."
    },
    {
      "title": "Data and Methods",
      "text": "Data. We analyze prediction error in psychiatric readmissions at a New England hospital in a data set containing 4214 deidentified notes from 3202 patients, collected from stays between 2011 and 2015. We extracted notes, patient race, gender, insurance payer type, and 30-day psychiatric readmission from every patient stay. The data set is racially imbalanced but has relative gender parity. We use the insurance payer typepublic, private, and other insurance-as a proxy for socioeconomic status. (See Supplementary Appendix Table  S1  for demographic information.)\n\nWe also examine prediction error in ICU mortality using the MIMIC-III v1.4 data set with the cohort selection explained earlier. (See Supplementary Appendix Table  S2  for demographic information.) For race, gender, and insurance payer type, we compare error rates for psychiatric readmission with error rates for ICU mortality in order to examine unfairness across different data sets and the clinical generalizability of our methods.\n\nMethods. We use topic modeling with latent Dirichlet allocation  25  (LDA) to uncover 50 topics (eg, depression, pulmonary disease; see Supplementary Appendix Tables  S3  and  S4  for example topics) and corresponding enrichment values for race, gender,  17, 26  and insurance type. We used 1500 iterations of Gibbs sampling to learn the 50 topics of the LDA for each data set. For the psychiatric data set, topics were learned using the LDA Python package  27  whereas for the ICU clinical notes, topics were learned using Mallet.  28 This difference in software arose from restrictions on the servers hosting the respective data sources.) Following prior work on enrichment of topics in clinical notes,  13, 26  we computed enrichment values for topics for race, gender, and insurance type.\n\nWe predict hospital mortality with ICU notes and 30-day psychiatric readmission with psychiatric notes using logistic regression with L1 regularization (implemented by Python package scikit-learn  29  with a hyperparameter of C = 1) using an 80/20 split for training and testing data over 50 trials. For both hospital mortality and psychiatric readmission, we report the error rate (zero-one loss) of the learned model for each demographic group and the 95% confidence interval. Text was vectorized using TF-IDF  30  on the most frequent 5000 words for each data set. We report the area under the receiver operator curve (AUC)  31  for overall model performance as well as the generalized zero-one loss as a performance metric.  32 Following prior work,  23  we use the Tukey range test,  33  which allows for pairwise comparisons among more than two groups, to test whether differences in error rates between groups are statistically significant. All Tukey range test error rate comparisons were performed using the Python package statsmodels.  34 r cohort selection code for MIMIC-III v1.4 and our analysis code are made publicly available to enable reproducibility and further study.  35 sults: Enrichment of Topic Modeled Notes Psychiatric note topics. White patients had higher topic enrichment values for the anxiety  36  and chronic pain topics, while black, Hispanic, and Asian patients had higher topic enrichment values for the psychosis topic.  37 Male patients had higher topic enrichment values than female patients for substance abuse (0.024 v 0.015), whereas female patients had higher topic enrichment values than male patients for general depression (0.021 v 0.019) and treatment resistant depression (0.025 v 0.015), reflecting known clinical findings.  38, 39 Previous work has shown that those with serious mental illness are more likely to have public insurance than private  39  ; we similarly find that private insurance patients have higher topic enrichment values than public insurance patients for anxiety (0.029 v 0.0156) and general depression (0.026 v 0.017). However, public insurance patients have higher topic enrichment values than private insurance patients for substance abuse (0.022 v 0.016). S3 ) and more refined topics than psychiatric notes due to the larger data source (25 879 v 4214 patients). As in the psychiatric data set, male patients have higher topic enrichment values for substance use than female patients (0.027 v 0.011), whereas female patients have higher topic enrichment values for pulmonary disease than male patients (0.026 v 0.016), potentially reflecting known underdiagnosis of chronic obstructive pulmonary disease in women.  40, 41 Verifying known clinical trends, Asian patients have the highest topic enrichment values for cancer (0.036), followed by white patients (0.021), other patients (0.016), and black and Hispanic patients (0.015).  42 Black patients have the highest topic enrichment values for kidney problems (0.061), followed by Hispanic patients (0.027), Asian patients (0.022), white patients (0.015), and other patients (0.014).  42 Hispanic patients have the highest topic enrichment values for liver concerns (0.034), followed by other patients (0.024), Asian patients (0.023), white patients (0.019), and black patients (0.014).  43 Finally, white patients have the highest topic enrichment values for atrial fibrillation (0.022), followed by other patients (0.017), Asian patients (0.015), black patients (0.013), and Hispanic patients (0.011).  44 blic and private insurance patients vary mainly in the severity of conditions they are being treated for. Those with public insurance often have multiple chronic conditions that require regular care.  45 In particular, compared with private insurance patients, public insurance patients have higher topic enrichment values for atrial fibrillation (0.24 v 0.013), pacemakers (0.023 v 0.014), and dialysis (0.023 v 0.013). However, compared with public insurance patients, private insurance patients have higher topic enrichment values for fractures (0.035 v 0.012), lymphoma (0.030 v 0.015), and aneurysms (0.028 v 0.016)."
    },
    {
      "title": "ICU note topics. Intensive care unit clinical notes have a different range of topics (see Supplementary Appendix Table",
      "text": "In sum, our results for gender and race reflect known specific clinical findings, whereas our results for insurance type reflect known differences in patterns of ICU usage between public insurance patients and private insurance patients."
    },
    {
      "title": "Results: Quantifying Disparities in Care With AI",
      "text": "After establishing that findings from the clinical notes reflect known disparities in patient population and experience, we evaluated whether predictions made from such notes are fair. There are multiple definitions of algorithmic fairness  [46] [47] [48] [49]  ; here we compare differences in error rates in ICU mortality and 30-day psychiatric readmission for race, gender, and insurance type.\n\nPrediction error in the ICU model. Unstructured clinical notes are a powerful source of information in predicting patient mortality-our models achieve an AUC 31 of 0.84 using only the ICU notes. Adding demographic information (age, race, gender, insurance type), improves AUC slightly, to 0.85. As shown in Figures  1  and  2 , error rates for gender and insurance type all have nonoverlapping confidence intervals. For gender, female patients have a higher model error rate than male patients; for insurance type, public insurance patients have a much higher model error rate than private insurance patients. All results are statistically significant at the 95% confidence level. Figure 2. 95% Confidence Intervals for Error Rate (Zero-One Loss) in ICU Mortality for Insurance Type\n\nPrediction in the psychiatric setting. In contrast to ICU mortality, predicting 30-day psychiatric readmission is significantly more challenging, leading to lower model accuracy.  50 One potential cause could be the importance of unmeasured residential, employment, and environmental factors in predicting short-term psychiatric readmission.  51 Another factor could be the level of hospital intervention, such as outpatient appointments.  52 mparison of prediction errors in ICU and psychiatric models. We compare differences in error rates in 30-day psychiatric readmission and ICU mortality for race, gender, and insurance type. Figure  3  shows differences in error rates in psychiatric readmission between racial groups, which were not statistically significant, with black patients having the highest error rate for psychiatric readmission. Differences in error rates in ICU mortality were also observed between racial groups. 23 We show consistent gender differences across data sets in Figures  1  and  4 , with the highest error rates for female patients, although the difference in error rates between genders was only statistically significant for ICU mortality. Note that because of the smaller size of the psychiatric notes data set, the confidence intervals overlap; however, the heterogeneity in topic enrichment values aligns with the higher error rates for female patients. Interestingly, model prediction errors for insurance type were statistically significant for both data sets (Figures  2  and  5 ), but the group with highest error rate changes. While public insurance patients have the highest error rate for ICU mortality, private insurance patients have the highest error rate for psychiatric readmission. These differences in error rates for insurance type may indicate that insurance type affects patient care in ICU and psychiatric settings differently. We note that public insurance patients have higher baseline hospital mortality rates, whereas private insurance patients have higher baseline 30-day psychiatric readmission (see Supplementary Appendix Table  S1 ). Such variation in baseline rates could be due to the previously noted prevalence of chronic conditions in public insurance patients,  45  making these patients more likely to need the ICU for regular care of multiple chronic conditions. Public insurance patients are also more likely to have serious mental illness than private insurance patients,  39  indicating that they may not come into a psychiatric hospital unless the situation is dire. In both data sets, predictions are better captured by notes for patients in the group that uses the care setting more regularly (ie, public insurance patients in the ICU and private insurance patients in the psychiatric hospital)."
    },
    {
      "title": "Responding to Algorithmic Biases in Machine Learning",
      "text": "AI and machine learning may enable faster, more accurate, and more comprehensive health care. We believe a closely cooperative relationship between clinicians and AIrather than a competitive one 53 -is necessary for illuminating areas of disparate health care impact.  51 For example, a clinician should be able to provide feedback for the algorithm to implement, and the algorithm could actively query the clinician about uncertain cases. Indeed, algorithmic scrutiny is vital to both the short-term and longterm robustness of the health care system.\n\nIn this paper, we have considered questions related to the disparate impact that AI may have in health care-in particular, on ICU mortality and 30-day psychiatric readmissions. Based on clinical notes, we demonstrated heterogeneity in the topics emphasized across race, gender, and insurance type, which tracks with known health disparities. We also showed statistically significant differences in error rates in ICU mortality for race, gender, and insurance type and in 30-day psychiatric readmission for insurance type.\n\nIn light of known clinical biases, how can AI assist in improving patient care? With increasing involvement of machine learning in health care decisions, it is crucial to assess any algorithmic biases introduced  54  by comparing prediction accuracy between demographic groups. Once algorithmic bias is uncovered, clinicians and AI must work together to identify the sources of algorithmic bias and improve models through better data collection and model improvements."
    },
    {
      "text": "Figure 1. 95% Confidence Intervals for Error Rate (Zero-One Loss) in ICU Mortality for Gender"
    },
    {
      "text": "Figure 3. 95% Confidence Intervals for Error Rate (Zero-One Loss) in Psychiatric Readmission for Racial Groups"
    },
    {
      "text": "Figure 4. 95% Confidence Intervals for Error Rate (Zero-One Loss) in Psychiatric Readmission for Gender"
    },
    {
      "text": "Figure 5. 95% Confidence Intervals for Error Rate (Zero-One Loss) in Psychiatric Readmission for Insurance Type"
    }
  ],
  "references": [
    {
      "title": "Diversity in clinical and biomedical research: a promise yet to be fulfilled",
      "authors": [
        "S Oh",
        "J Galanter",
        "N Thakur"
      ],
      "year": 2015,
      "doi": "10.1371/journal.pmed.1001918"
    },
    {
      "title": "Women's Hospital. Ten years of health advancements for women of all ages, ethnicities and nations",
      "authors": [
        "Mary Horrigan",
        "Connors Center",
        "Brigham"
      ],
      "year": 2018
    },
    {
      "title": "Racial and ethnic disparities in palliative care",
      "authors": [
        "K Johnson"
      ],
      "year": 2013,
      "doi": "10.1089/jpm.2013.9468"
    },
    {
      "title": "Impact of weight bias and stigma on quality of care and outcomes for patients with obesity",
      "authors": [
        "S Phelan",
        "D Burgess",
        "M Yeazel",
        "W Hellerstedt",
        "J Griffin",
        "M Van Ryn"
      ],
      "year": 2015
    },
    {
      "title": "The influence of gender on the frequency of pain and sedative medication administered to postoperative patients",
      "authors": [
        "K Calderone"
      ],
      "year": 1990
    },
    {
      "title": "Sex differences in pain: a brief review of clinical and experimental findings",
      "authors": [
        "E Bartley",
        "R Fillingim"
      ],
      "year": 2013,
      "doi": "10.1093/bja/aet127"
    },
    {
      "title": "The girl who cried pain: a bias against women in the treatment of pain",
      "authors": [
        "D Hoffmann",
        "A Tarzian"
      ],
      "year": 2001
    },
    {
      "title": "The black-white disparity in pregnancyrelated mortality from 5 conditions: differences in prevalence and case-fatality rates",
      "authors": [
        "M Tucker",
        "C Berg",
        "W Callaghan",
        "J Hsia"
      ],
      "year": 2007
    },
    {
      "title": "Reducing disparities in severe maternal morbidity and mortality",
      "authors": [
        "E Howell"
      ],
      "year": 2018,
      "doi": "10.1097/grf.0000000000000349"
    },
    {
      "title": "Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs",
      "authors": [
        "V Gulshan",
        "L Peng",
        "M Coram"
      ],
      "year": 2016
    },
    {
      "title": "Dermatologist-level classification of skin cancer with deep neural networks",
      "authors": [
        "A Esteva",
        "B Kuprel",
        "R Novoa"
      ],
      "year": 2017
    },
    {
      "title": "Electronic medical records for discovery research in rheumatoid arthritis",
      "authors": [
        "K Liao",
        "T Cai",
        "V Gainer"
      ],
      "year": 2010
    },
    {
      "title": "Unfolding physiological state: mortality modelling in intensive care units",
      "authors": [
        "M Ghassemi",
        "T Naumann",
        "F Doshi-Velez"
      ],
      "year": 2014
    },
    {
      "title": "Predicting intervention onset in the ICU with switching state space models",
      "authors": [
        "M Ghassemi",
        "M Wu",
        "M Hughes",
        "P Szolovits",
        "F Doshi-Velez"
      ],
      "year": 2017
    },
    {
      "title": "Fairness in precision medicine",
      "authors": [
        "K Ferryman",
        "M Pitcan"
      ],
      "year": 2018
    },
    {
      "title": "Machine bias",
      "authors": [
        "J Angwin",
        "J Larson",
        "S Mattu",
        "L Kirchner"
      ],
      "year": 2016
    },
    {
      "title": "AI can be sexist and racist-it's time to make it fair",
      "authors": [
        "J Zou",
        "L Schiebinger"
      ],
      "year": 2018,
      "doi": "10.1038/d41586-018-05707-8"
    },
    {
      "title": "Opportunities in machine learning for healthcare",
      "authors": [
        "M Ghassemi",
        "T Naumann",
        "P Schulam",
        "A Beam",
        "R Ranganath"
      ],
      "year": 2018
    },
    {
      "title": "Association between immigrant status and endof-life care in Ontario, Canada",
      "authors": [
        "C Yarnell",
        "L Fu",
        "D Manuel"
      ],
      "year": 2017,
      "doi": "10.1001/jama.2017.14418"
    },
    {
      "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes",
      "authors": [
        "N Garg",
        "L Schiebinger",
        "D Jurafsky",
        "J Zou"
      ],
      "year": 2018
    },
    {
      "title": "Race and mental health: patterns and challenges",
      "authors": [
        "D Williams",
        "M Costa",
        "J Leavell"
      ],
      "year": 2017,
      "doi": "10.1017/cbo9780511984945.018"
    },
    {
      "title": "Racial discrimination and racial disparities in health",
      "authors": [
        "N Priest",
        "D Williams"
      ],
      "year": 2017
    },
    {
      "title": "Why is my classifier discriminatory? arXiv",
      "authors": [
        "I Chen",
        "F Johansson",
        "D Sontag"
      ],
      "year": 2018
    },
    {
      "title": "MIMIC-III, a freely accessible critical care database",
      "authors": [
        "Aew Johnson",
        "T Pollard",
        "L Shen"
      ],
      "year": 2016,
      "doi": "10.1038/sdata.2016.35"
    },
    {
      "title": "Latent Dirichlet allocation",
      "authors": [
        "D Blei",
        "A Ng",
        "M Jordan"
      ],
      "year": 2003,
      "doi": "10.7551/mitpress/1120.003.0082"
    },
    {
      "title": "Unsupervised pattern discovery in electronic health care data using probabilistic clustering models",
      "authors": [
        "B Marlin",
        "D Kale",
        "R Khemani",
        "R Wetzel"
      ],
      "year": 2012,
      "doi": "10.1145/2110363.2110408"
    },
    {
      "title": "topic modeling with latent Dirichlet allocation",
      "authors": [
        "A Riddell",
        "Ida"
      ],
      "year": 2018
    },
    {
      "title": "Mallet: a machine learning for language toolkit",
      "authors": [
        "A Mccallum"
      ],
      "year": 2002
    },
    {
      "title": "Scikit-learn: machine learning in python",
      "authors": [
        "F Pedregosa",
        "G Varoquaux",
        "A Gramfort"
      ],
      "year": 2011
    },
    {
      "title": "A statistical interpretation of term specificity and its application in retrieval",
      "authors": [
        "K Jones"
      ],
      "year": 1972
    },
    {
      "title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms",
      "authors": [
        "A Bradley"
      ],
      "year": 1997
    },
    {
      "title": "On fairness and calibration",
      "authors": [
        "G Pleiss",
        "M Raghavan",
        "F Wu",
        "J Kleinberg",
        "K Weinberger"
      ],
      "year": 2017
    },
    {
      "title": "Comparing individual means in the analysis of variance",
      "authors": [
        "J Tukey"
      ],
      "year": 1949,
      "doi": "10.2307/3001913"
    },
    {
      "title": "Statsmodels: econometric and statistical modeling with python",
      "authors": [
        "S Seabold",
        "J Perktold"
      ],
      "year": 2010,
      "doi": "10.25080/majora-92bf1922-011"
    },
    {
      "title": "Reproducibility in critical care: a mortality prediction case study",
      "authors": [
        "Aew Johnson",
        "T Pollard",
        "R Mark"
      ],
      "year": 2017
    },
    {
      "title": "Gender differences in primary substance of abuse across age groups",
      "authors": [
        "K Smith"
      ],
      "year": 2014,
      "doi": "10.1037/e476292006-001"
    },
    {
      "title": "Why is depression more common among women than among men?",
      "authors": [
        "C Kuehner"
      ],
      "year": 2017,
      "doi": "10.1016/s2215-0366(16)30263-2"
    },
    {
      "title": "Contemporary use and practice of electroconvulsive therapy worldwide",
      "authors": [
        "K Leiknes",
        "Jarosh-Von Schweder",
        "L H\u00f8ie"
      ],
      "year": 2012
    },
    {
      "title": "Access and cost barriers to mental health care, by insurance status, 1999-2010",
      "authors": [
        "K Rowan",
        "D Mcalpine",
        "L Blewett"
      ],
      "year": 2013,
      "doi": "10.1377/hlthaff.2013.0133"
    },
    {
      "title": "Gender bias in the diagnosis of COPD",
      "authors": [
        "K Chapman",
        "D Tashkin",
        "D Pye"
      ],
      "year": 2001,
      "doi": "10.1378/chest.119.6.1691"
    },
    {
      "title": "Gender and chronic obstructive pulmonary disease: why it matters",
      "authors": [
        "M Han",
        "D Postma",
        "D Mannino"
      ],
      "year": 2007,
      "doi": "10.1164/rccm.200704-553cc"
    },
    {
      "title": "The burden of cancer in Asian Americans: a report of national mortality trends by Asian ethnicity",
      "authors": [
        "C Thompson",
        "S Gomez",
        "K Hastings"
      ],
      "year": 2016
    },
    {
      "title": "Chronic liver disease in the Hispanic population of the United States",
      "authors": [
        "A Carrion",
        "R Ghanta",
        "O Carrasquillo",
        "P Martin"
      ],
      "year": 2011
    },
    {
      "title": "Racial/ethnic differences in the prevalence of atrial fibrillation among older adults-a cross-sectional study",
      "authors": [
        "A Shen",
        "R Contreras",
        "S Sobnosky"
      ],
      "year": 2010
    },
    {
      "title": "Disability, health, and multiple chronic conditions among people eligible for both Medicare and Medicaid, 2005-2010",
      "authors": [
        "M Fox",
        "A Reichard"
      ],
      "year": 2013
    },
    {
      "title": "Fairness through awareness",
      "authors": [
        "C Dwork",
        "M Hardt",
        "T Pitassi",
        "O Reingold",
        "R Zemel"
      ],
      "year": 2012,
      "doi": "10.1145/2090236.2090255"
    },
    {
      "title": "Equality of opportunity in supervised learning",
      "authors": [
        "M Hardt",
        "E Price",
        "N Srebro"
      ],
      "year": 2016
    },
    {
      "title": "Learning fair representations",
      "authors": [
        "R Zemel",
        "Y Wu",
        "K Swersky",
        "T Pitassi",
        "C Dwork"
      ],
      "year": 2013
    },
    {
      "title": "Preventing fairness gerrymandering: auditing and learning for subgroup fairness",
      "authors": [
        "M Kearns",
        "S Neel",
        "A Roth",
        "Z Wu"
      ],
      "year": 2018
    },
    {
      "title": "Predicting early psychiatric readmission with natural language processing of narrative discharge summaries",
      "authors": [
        "A Rumshisky",
        "M Ghassemi",
        "T Naumann"
      ],
      "year": 2016,
      "doi": "10.1038/tp.2015.182"
    },
    {
      "title": "Predicting time to readmission in patients with recent histories of recurrent psychiatric hospitalization: a matched-control survival analysis",
      "authors": [
        "T Schmutte",
        "C Dunn",
        "W Sledge"
      ],
      "year": 2010,
      "doi": "10.1097/nmd.0b013e3181fe726b"
    },
    {
      "title": "Effects of discharge planning and compliance with outpatient appointments on readmission rates",
      "authors": [
        "E Nelson",
        "M Maruish",
        "J Axler"
      ],
      "year": 2000
    },
    {
      "title": "AI versus doctors [news",
      "year": 2017
    },
    {
      "title": "Want less-biased decisions? Use algorithms",
      "authors": [
        "A Miller"
      ],
      "year": 2018
    }
  ],
  "num_references": 54
}
